nohup: ignoring input
INFO:tensorflow:Loading config from /home/ubuntu/NeuralCodeTranslator/seq2seq/configs/small_10.yml
INFO:tensorflow:Loading config from /home/ubuntu/NeuralCodeTranslator/seq2seq/example_configs/train_seq2seq_optimized.yml
INFO:tensorflow:Loading config from /home/ubuntu/NeuralCodeTranslator/seq2seq/example_configs/text_metrics.yml
INFO:tensorflow:Final Config:
buckets: 10,20,30,40
default_params:
- {separator: ' '}
- {postproc_fn: seq2seq.data.postproc.strip_bpe}
hooks:
- {class: PrintModelAnalysisHook}
- {class: MetadataCaptureHook}
- {class: SyncReplicasOptimizerHook}
- class: TrainSampleHook
  params: {every_n_steps: 10000}
metrics:
- {class: LogPerplexityMetricSpec}
- class: BleuMetricSpec
  params: {postproc_fn: seq2seq.data.postproc.strip_bpe, separator: ' '}
model: AttentionSeq2Seq
model_params:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  optimizer.learning_rate: 0.0001
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50

WARNING:tensorflow:Ignoring config flag: default_params
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=train
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: null
  shuffle: true
  source_delimiter: ' '
  source_files: [../dataset/dataset//train/buggy.txt]
  target_delimiter: ' '
  target_files: [../dataset/dataset//train/fixed.txt]

INFO:tensorflow:Creating ParallelTextInputPipeline in mode=eval
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//eval/buggy.txt]
  target_delimiter: ' '
  target_files: [../dataset/dataset//eval/fixed.txt]

INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb0d1a61198>, '_master': '', '_num_ps_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_save_checkpoints_steps': 10000, '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 4}
INFO:tensorflow:Creating PrintModelAnalysisHook in mode=train
INFO:tensorflow:
PrintModelAnalysisHook: {}

INFO:tensorflow:Creating MetadataCaptureHook in mode=train
INFO:tensorflow:
MetadataCaptureHook: {step: 10}

INFO:tensorflow:Creating SyncReplicasOptimizerHook in mode=train
INFO:tensorflow:
SyncReplicasOptimizerHook: {}

INFO:tensorflow:Creating TrainSampleHook in mode=train
INFO:tensorflow:
TrainSampleHook: {every_n_secs: null, every_n_steps: 10000, source_delimiter: ' ',
  target_delimiter: ' '}

INFO:tensorflow:Creating LogPerplexityMetricSpec in mode=eval
INFO:tensorflow:
LogPerplexityMetricSpec: {}

INFO:tensorflow:Creating BleuMetricSpec in mode=eval
INFO:tensorflow:
BleuMetricSpec: {eos_token: SEQUENCE_END, postproc_fn: seq2seq.data.postproc.strip_bpe,
  separator: ' ', sos_token: SEQUENCE_START}

INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 2003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 2.1536362, step = 2003
INFO:tensorflow:Prediction followed by Target @ Step 2003
====================================================================================================
public void METHOD_1 ( ) VAR_1 ) { return ( . ( ) ; METHOD_2 ( ) ; METHOD_2 ( ) ) ; ; } SEQUENCE_END
public void remove ( String id ) { execute ( METHOD_1 ( ) . METHOD_2 ( ) . delete ( id ) ) ; } SEQUENCE_END

public void METHOD_1 ( ) { METHOD_2 ( ) ) ; return ( ) ; METHOD_2 ( ) ) } ( ( ( ) ) ) ) ) ( ) ) METHOD_2 ( ) ) METHOD_2 ( ) ) } ( ) ) METHOD_3 ) ; } SEQUENCE_END
public void METHOD_1 ( ) { METHOD_2 ( VAR_1 ) ; METHOD_3 ( ) . METHOD_4 ( ) ; Map < String , TYPE_1 > returnValue = METHOD_3 ( ) . METHOD_5 ( ) . getReturnValue ( ) ; assertEquals ( returnValue , null ) ; } SEQUENCE_END

public void METHOD_1 ( TYPE_1 { VAR_1 { return VAR_1 . VAR_1 TYPE_1 ( , , VAR_2 , VAR_2 , VAR_2 , ; } . ; VAR_1 , METHOD_2 ( VAR_1 , VAR_2 , VAR_2 ) VAR_2 ) ; } ( ) , VAR_2 ) ; } SEQUENCE_END
public void METHOD_1 ( ) throws Exception { TYPE_1 VAR_1 = new TYPE_1 ( VAR_2 , 0 , 0 , VAR_5 ) ; TYPE_1 VAR_3 = TYPE_1 . METHOD_2 ( VAR_2 , VAR_4 , 0 , VAR_5 ) ; assertEquals ( VAR_1 , VAR_3 ) ; } SEQUENCE_END

public void METHOD_1 ( TYPE_1 VAR_1 ) TYPE_1 VAR_1 ) { return ( VAR_1 . VAR_1 . METHOD_2 ( ) ) ; . . METHOD_2 ( ( ) ; } SEQUENCE_END . SEQUENCE_END . METHOD_2 ( VAR_1 ) METHOD_2 ( ) ) VAR_2 ) ; } SEQUENCE_END SEQUENCE_END
public void METHOD_1 ( TYPE_1 VAR_1 , int index ) { if ( index > VAR_2 . METHOD_2 ( ) ) { VAR_2 . add ( VAR_1 ) ; } else { VAR_2 . insert ( VAR_1 . METHOD_3 ( ) , index ) ; } } SEQUENCE_END

protected void METHOD_1 ( ) { METHOD_2 METHOD_2 METHOD_2 ( ) ) . ( ) . ( ) ) METHOD_2 ( ) ) METHOD_2 ( ) ( ) ) ) } ( ; } SEQUENCE_END SEQUENCE_END SEQUENCE_END SEQUENCE_END
private void METHOD_1 ( ) { TYPE_1 . METHOD_2 ( ( ) - > { METHOD_4 ( ) . METHOD_5 ( ) . METHOD_6 ( METHOD_7 ( ) ) ; return null ; } ) ; } SEQUENCE_END

public TYPE_1 TYPE_1 < TYPE_1 > METHOD_1 ( TYPE_1 TYPE_1 > , TYPE_1 TYPE_1 > ) TYPE_1 TYPE_1 > ) { return VAR_1 ( VAR_1 , VAR_2 , VAR_2 , , ( ) , VAR_1 ) ; VAR_2 ) VAR_2 ) ; } SEQUENCE_END
public static List < TYPE_1 > METHOD_1 ( final TYPE_2 VAR_1 , final String VAR_2 , final String query ) { return METHOD_2 ( VAR_1 , VAR_2 , TYPE_3 . METHOD_3 ( query , VAR_3 ) , false , false ) ; } SEQUENCE_END

public void METHOD_1 ( VAR_1 ) { VAR_1 . VAR_1 . ; return ( ( ( ) ) ) ) ) ) ; } VAR_1 VAR_1 VAR_1 , , ) ) VAR_2 ) VAR_2 ( ; ; METHOD_3 ( ) ; } SEQUENCE_END
public VAR_1 VAR_2 ( VAR_3 VAR_4 = VAR_5 ( VAR_6 ) ; Set < VAR_7 > VAR_8 = VAR_9 ( VAR_4 ) ; return ok ( VAR_10 ( VAR_11 class , STRING_1 , VAR_8 ) ) . build ( ) ; } SEQUENCE_END

public void METHOD_1 ( ) { VAR_1 { . ( . . METHOD_1 ( ) ) METHOD_2 ( ) ) METHOD_2 ( ) ) METHOD_2 ( ) ) ) METHOD_3 ( ) ) ; } . ( ( ( ) ) ; } SEQUENCE_END
public void METHOD_1 ( ) throws Exception { File VAR_1 = new File ( this . METHOD_2 ( ) . METHOD_3 ( ) . METHOD_4 ( STRING_1 ) . METHOD_5 ( ) ) ; VAR_2 = new TYPE_1 ( VAR_1 ) ; } SEQUENCE_END

public TYPE_1 < TYPE_1 > METHOD_1 ( ) { return ( ) . METHOD_2 ( ) ) ; return . ( ( ) ; METHOD_2 ( ) ( ) ) METHOD_3 ( ) ) ; } SEQUENCE_END . ; } SEQUENCE_END
protected List < TYPE_1 > METHOD_1 ( ) { if ( VAR_1 . isEmpty ( ) ) { VAR_1 = METHOD_2 ( ) . METHOD_3 ( METHOD_4 ( ) . getId ( ) ) ; } return VAR_1 ; } SEQUENCE_END

public void METHOD_1 ( ) { return VAR_1 . VAR_1 . . ; . METHOD_2 ( ) ) ; } ( ) ) ) METHOD_2 ( ) ) ; ; return . METHOD_2 ( ) ) VAR_1 ) ; } . ; } SEQUENCE_END . ; } SEQUENCE_END
public boolean METHOD_1 ( ) { TYPE_1 VAR_1 = ( TYPE_1 ) mContext . METHOD_2 ( VAR_2 ) ; if ( ! VAR_1 . METHOD_3 ( VAR_3 ) ) { VAR_4 . METHOD_4 ( true , null ) ; return true ; } return false ; } SEQUENCE_END

private void METHOD_1 ( TYPE_1 TYPE_1 VAR_1 ) TYPE_1 TYPE_1 VAR_1 ) { METHOD_2 METHOD_2 . VAR_1 TYPE_1 ( ( ) VAR_1 ) METHOD_2 ( ) ) ; } . METHOD_2 ( ) ) ; } . METHOD_2 ( ) ) ; } SEQUENCE_END
private void METHOD_1 ( final TYPE_1 response , final TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = new TYPE_3 ( VAR_3 , VAR_1 . toString ( ) ) ; VAR_2 . METHOD_3 ( VAR_4 ) ; response . METHOD_4 ( VAR_2 ) ; } SEQUENCE_END

public TYPE_1 < TYPE_1 > METHOD_1 ( ) { return ( VAR_1 . VAR_1 ) ; return . ( . METHOD_2 ( ) ( ) ) METHOD_2 ( ) ) METHOD_2 ( ) ; METHOD_3 ) VAR_2 ) ; } SEQUENCE_END ( ; } SEQUENCE_END
private TYPE_1 < DiskImage > METHOD_1 ( ) { if ( VAR_1 == null ) { VAR_1 = TYPE_2 . METHOD_2 ( getVm ( ) . METHOD_3 ( ) . values ( ) , true , false ) ; } return VAR_1 ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( TYPE_1 VAR_1 , TYPE_1 , TYPE_1 > , , TYPE_1 , TYPE_1 > , { return VAR_1 TYPE_1 ( VAR_1 , VAR_2 , VAR_2 , , , VAR_2 , , , , , , , , , ; } SEQUENCE_END
public TYPE_1 create ( TYPE_2 VAR_1 , TYPE_3 < TYPE_4 > VAR_2 , Account . Id id ) { return new TYPE_1 ( VAR_1 , VAR_3 , VAR_4 , VAR_5 , VAR_6 , VAR_7 , VAR_2 , null , id ) ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( ) { return ( . VAR_1 VAR_1 METHOD_2 ( ) ( ) ) VAR_2 ) VAR_2 ) ; } VAR_1 ; METHOD_2 ( VAR_1 , ; VAR_2 , VAR_1 ; , ; VAR_1 ; VAR_1 ; } SEQUENCE_END
private String METHOD_1 ( ) { String VAR_1 = TYPE_1 . METHOD_2 ( METHOD_3 ( ) , STRING_1 , VAR_2 ) ; return VAR_1 . equals ( STRING_2 ) ? VAR_1 + VAR_3 : VAR_1 + VAR_4 + VAR_3 ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( ) VAR_1 ) { return . ( ) ) METHOD_2 ( ) ) ; METHOD_2 ( ) ) METHOD_3 ) METHOD_3 ( ) ) ; } SEQUENCE_END
protected TYPE_1 METHOD_1 ( TYPE_2 VAR_1 ) { return METHOD_2 ( VAR_1 . getId ( ) ) . METHOD_3 ( VAR_2 , VAR_1 . METHOD_4 ( ) ) ; } SEQUENCE_END

public TYPE_1 TYPE_1 METHOD_1 ( TYPE_1 VAR_1 , TYPE_1 VAR_1 ) { return VAR_1 = VAR_1 ; ( , , VAR_2 , ; } SEQUENCE_END
public static int METHOD_1 ( String id , String VAR_1 ) { return ( Integer ) METHOD_2 ( id , VAR_1 ) ; } SEQUENCE_END

public void METHOD_1 ( TYPE_1 VAR_1 ) TYPE_1 VAR_1 ) { VAR_1 { return ( VAR_1 VAR_1 ) . ) METHOD_2 ( ) ) ; return ( ) ) ; } ( ) VAR_1 ) ; . METHOD_2 ( ) ; } ( ) ) VAR_2 ) ; }
public void METHOD_1 ( Expr e , boolean VAR_1 ) throws AnalysisException { for ( Expr VAR_2 : e . METHOD_2 ( ) ) { METHOD_3 ( VAR_2 ) ; if ( ! VAR_1 ) VAR_2 . METHOD_4 ( ) ; METHOD_5 ( VAR_2 , VAR_1 ) ; }

public void METHOD_1 ( TYPE_1 { VAR_1 ) TYPE_1 { { ( . VAR_1 ( ) ; } ( ) ) VAR_1 ) ; } . . . ( ) ) ; } ( ) ) METHOD_2 ( ; } SEQUENCE_END
public void METHOD_1 ( ) throws TYPE_1 , IOException { String changeId = METHOD_2 ( ) ; METHOD_3 ( changeId , STRING_1 ) ; ChangeInfo c = METHOD_4 ( changeId ) ; METHOD_5 ( c . messages ) ; } SEQUENCE_END

public void METHOD_1 ( TYPE_1 { METHOD_2 ( TYPE_1 ( ) ) VAR_1 . METHOD_2 ( ) ) METHOD_2 ( ) ) . } ( ) ) ; } ( ( ) METHOD_2 ( ) ) ; } SEQUENCE_END
public void METHOD_1 ( ) { List < TYPE_1 > result = VAR_1 . METHOD_2 ( Guid . METHOD_3 ( ) ) ; METHOD_4 ( result ) ; assertTrue ( result . isEmpty ( ) ) ; } SEQUENCE_END

private void METHOD_1 ( TYPE_1 VAR_1 ) { return ( ) ) TYPE_1 ( ) ) . . METHOD_2 ( ) ) ; . ( ) . METHOD_2 ( ) ) METHOD_2 ( ) ) VAR_1 ) ; . ( ; } SEQUENCE_END SEQUENCE_END . ; } SEQUENCE_END
private boolean METHOD_1 ( TYPE_1 expr ) { for ( TYPE_2 < TYPE_3 > r : expr . METHOD_2 ( ) ) { if ( r . METHOD_3 ( ) . METHOD_4 ( ) != VAR_1 ) { return false ; } } return true ; } SEQUENCE_END

public TYPE_1 < < > METHOD_1 ( ) { return ( TYPE_1 VAR_1 ) { , TYPE_1 ( TYPE_1 ( ) ) VAR_1 ) TYPE_1 ( TYPE_1 ) { . VAR_2 , VAR_2 , VAR_2 ) } } } SEQUENCE_END VAR_1 ; } SEQUENCE_END
public ArrayList < TYPE_1 > METHOD_1 ( ) { ArrayList < TYPE_1 > VAR_1 = new ArrayList < TYPE_1 > ( Arrays.asList ( new TYPE_1 [ ] { VAR_2 , VAR_3 , VAR_4 , VAR_5 } ) ) ; return VAR_1 ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( TYPE_1 VAR_1 ) TYPE_1 VAR_1 ) { return VAR_1 ( VAR_1 ) VAR_2 ) VAR_2 ) VAR_2 ) ; } SEQUENCE_END
public DiskImage METHOD_1 ( Guid VAR_1 , Guid VAR_2 ) { return METHOD_1 ( VAR_1 , VAR_2 , null , false ) ; } SEQUENCE_END

private void METHOD_1 ( ) VAR_1 ) { METHOD_2 VAR_1 . METHOD_2 ( VAR_1 ( ) ) VAR_2 ) VAR_1 ( ) ) METHOD_2 ( ) ) VAR_2 ) VAR_2 ) VAR_2 ) VAR_2 ) METHOD_3 ( ) ) VAR_2 ) VAR_2 ( ) ) ) ; } SEQUENCE_END
protected boolean METHOD_1 ( Guid VAR_1 ) { return TYPE_1 . METHOD_2 ( METHOD_3 ( ) , VAR_1 , getReturnValue ( ) . getValidationMessages ( ) , false , true , true , VAR_2 . isEmpty ( ) , VAR_3 . get ( VAR_1 ) ) ; } SEQUENCE_END

public void METHOD_1 ( ) { return ( ) VAR_1 ) ; return . ; return return ( VAR_1 VAR_1 ) METHOD_2 ( ) ) ; return . ; } SEQUENCE_END . ; METHOD_2 ( ) ; } SEQUENCE_END
public boolean METHOD_1 ( ) { if ( ! VAR_1 ) { return true ; } if ( ! VAR_2 . METHOD_2 ( ) ) { return false ; } return VAR_3 . METHOD_2 ( ) ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( ) VAR_1 ) { METHOD_2 METHOD_2 . ( . ( ; METHOD_2 ( ) . METHOD_2 ( ) ) METHOD_2 ( ) ) METHOD_3 ( ) ) METHOD_3 ( ) ) ; } . ; METHOD_3 ( ) ; } SEQUENCE_END
protected TYPE_1 METHOD_1 ( TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = factory ( ) . METHOD_2 ( ) . METHOD_3 ( ) . METHOD_4 ( VAR_1 . METHOD_5 ( ) . id ( ) ) ; return VAR_2 . build ( ) ; } SEQUENCE_END

public void void METHOD_1 ( TYPE_1 VAR_1 , TYPE_1 VAR_1 ) String VAR_1 ) String VAR_1 ) { return ( . VAR_1 ( , , VAR_2 ) VAR_2 ) ) } . METHOD_2 ( VAR_1 , VAR_2 ( ) ) ; } SEQUENCE_END
private static void METHOD_1 ( TYPE_1 VAR_1 , String VAR_2 , String VAR_3 , String VAR_4 ) { Guid VAR_5 = METHOD_2 ( VAR_3 , VAR_2 , VAR_4 ) ; VAR_1 . METHOD_3 ( VAR_5 . toString ( ) ) ; } SEQUENCE_END

public TYPE_1 TYPE_1 METHOD_1 ( ) VAR_1 ) { VAR_1 { METHOD_2 . ( . . ( TYPE_1 ( ) ) ; . . ( . METHOD_2 ( ) ; METHOD_2 ( ) ) METHOD_2 ( ) ) ; } SEQUENCE_END SEQUENCE_END
public static String METHOD_1 ( File VAR_1 ) throws IOException { try ( TYPE_1 VAR_2 = new TYPE_1 ( VAR_1 ) ) { return VAR_2 . METHOD_2 ( ) . METHOD_3 ( ) . METHOD_4 ( STRING_1 ) ; } } SEQUENCE_END

public void METHOD_1 ( ) { return ( VAR_1 ( ) ) VAR_1 ) ; return ( } return ( ( . VAR_1 METHOD_3 ( ) ) ; } ( ) ) ; } ( ) ) ; } SEQUENCE_END
public void METHOD_1 ( ) { if ( METHOD_2 ( ) != null ) { return ; } final TYPE_1 VAR_1 = new TYPE_2 ( this ) ; METHOD_3 ( VAR_1 ) ; METHOD_5 ( VAR_1 ) ; } SEQUENCE_END

public void METHOD_1 ( ) { return ( VAR_1 ; METHOD_2 ( ) ) ) ) ) ; } . return ( ( ; METHOD_2 ( ) ) ; } ( ) ; } SEQUENCE_END ( ) ) ) ; } SEQUENCE_END SEQUENCE_END
public void METHOD_1 ( ) { Collections.emptyList ( ) . METHOD_2 ( VAR_1 - > 1 ) ; try { Collections.emptyList ( ) . METHOD_2 ( null ) ; fail ( ) ; } catch ( TYPE_1 expected ) { } } SEQUENCE_END

public TYPE_1 METHOD_1 ( ) { return VAR_1 VAR_1 METHOD_2 ( VAR_1 ) VAR_2 ) VAR_2 ) ; } SEQUENCE_END
public int METHOD_1 ( ) { return TYPE_1 . hash ( message , VAR_2 , VAR_3 ) ; } SEQUENCE_END

public void METHOD_1 ( ) VAR_1 ) { return ( VAR_1 ( ) ; METHOD_2 ( ) ) METHOD_2 ( ) ) ; ; } SEQUENCE_END
public void METHOD_1 ( String id ) { execute ( METHOD_2 ( ) . METHOD_3 ( ) . delete ( id ) ) ; } SEQUENCE_END

public void METHOD_1 ( ) { return METHOD_2 METHOD_2 ( ) . return ( ) ) ) ) ) ( ) ) ; METHOD_2 ( ) ; METHOD_3 ( ) ) ( ) ) ) METHOD_3 ( ) ) ; ; } SEQUENCE_END
public void METHOD_1 ( ) { super . METHOD_1 ( ) ; METHOD_2 ( ( ( TYPE_1 ) METHOD_3 ( ) ) . METHOD_4 ( ) . METHOD_5 ( e - > VAR_1 this . METHOD_6 ( ) ) ) ; } SEQUENCE_END

====================================================================================================


INFO:tensorflow:Performing full trace on next step.
INFO:tensorflow:Captured full trace at step 2004
INFO:tensorflow:Saved run_metadata to /home/ubuntu/NeuralCodeTranslator/model/dataset/run_meta
INFO:tensorflow:Saved timeline to /home/ubuntu/NeuralCodeTranslator/model/dataset/timeline.json
INFO:tensorflow:Saved op log to /home/ubuntu/NeuralCodeTranslator/model/dataset
INFO:tensorflow:global_step/sec: 0.1468
INFO:tensorflow:loss = 1.3722614, step = 2103
INFO:tensorflow:global_step/sec: 0.164004
INFO:tensorflow:loss = 1.9820127, step = 2203
INFO:tensorflow:global_step/sec: 0.163338
INFO:tensorflow:loss = 2.1143477, step = 2303
INFO:tensorflow:global_step/sec: 0.16613
INFO:tensorflow:loss = 2.0288436, step = 2403
INFO:tensorflow:global_step/sec: 0.168178
INFO:tensorflow:loss = 1.224529, step = 2503
INFO:tensorflow:global_step/sec: 0.164164
INFO:tensorflow:loss = 1.5304393, step = 2603
INFO:tensorflow:global_step/sec: 0.164264
INFO:tensorflow:loss = 1.3189924, step = 2703
INFO:tensorflow:global_step/sec: 0.164994
INFO:tensorflow:loss = 1.907493, step = 2803
INFO:tensorflow:global_step/sec: 0.165944
INFO:tensorflow:loss = 1.2938732, step = 2903
INFO:tensorflow:Saving checkpoints for 3002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 1.7249681.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-06-11:25:34
INFO:tensorflow:Finished evaluation at 2019-05-06-11:26:20
INFO:tensorflow:Saving dict for global step 3002: bleu = 32.29, global_step = 3002, log_perplexity = 1.5974754, loss = 1.4972562
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 3003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 1.93851, step = 3003
INFO:tensorflow:global_step/sec: 0.149384
INFO:tensorflow:loss = 1.4875139, step = 3103
INFO:tensorflow:global_step/sec: 0.166607
INFO:tensorflow:loss = 1.8482533, step = 3203
INFO:tensorflow:global_step/sec: 0.164653
INFO:tensorflow:loss = 1.3484911, step = 3303
INFO:tensorflow:global_step/sec: 0.16626
INFO:tensorflow:loss = 1.3011743, step = 3403
INFO:tensorflow:global_step/sec: 0.164694
INFO:tensorflow:loss = 1.4111716, step = 3503
INFO:tensorflow:global_step/sec: 0.163467
INFO:tensorflow:loss = 0.9930879, step = 3603
INFO:tensorflow:global_step/sec: 0.165843
INFO:tensorflow:loss = 1.6071088, step = 3703
INFO:tensorflow:global_step/sec: 0.162871
INFO:tensorflow:loss = 1.4773638, step = 3803
INFO:tensorflow:global_step/sec: 0.167654
INFO:tensorflow:loss = 1.218847, step = 3903
INFO:tensorflow:Saving checkpoints for 4002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 1.3538678.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-06-13:08:31
INFO:tensorflow:Finished evaluation at 2019-05-06-13:09:18
INFO:tensorflow:Saving dict for global step 4002: bleu = 35.19, global_step = 4002, log_perplexity = 1.4212327, loss = 1.332282
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 4003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 1.691547, step = 4003
INFO:tensorflow:global_step/sec: 0.147516
INFO:tensorflow:loss = 0.84582394, step = 4103
INFO:tensorflow:global_step/sec: 0.1658
INFO:tensorflow:loss = 1.5886207, step = 4203
INFO:tensorflow:global_step/sec: 0.166712
INFO:tensorflow:loss = 1.2580144, step = 4303
INFO:tensorflow:global_step/sec: 0.164396
INFO:tensorflow:loss = 0.9411419, step = 4403
INFO:tensorflow:global_step/sec: 0.164502
INFO:tensorflow:loss = 1.0349995, step = 4503
INFO:tensorflow:global_step/sec: 0.165435
INFO:tensorflow:loss = 1.7370235, step = 4603
INFO:tensorflow:global_step/sec: 0.168669
INFO:tensorflow:loss = 1.2780225, step = 4703
INFO:tensorflow:global_step/sec: 0.164691
INFO:tensorflow:loss = 0.9661005, step = 4803
INFO:tensorflow:global_step/sec: 0.163086
INFO:tensorflow:loss = 1.5371927, step = 4903
INFO:tensorflow:Saving checkpoints for 5002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 1.2591934.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-06-14:51:26
INFO:tensorflow:Finished evaluation at 2019-05-06-14:52:12
INFO:tensorflow:Saving dict for global step 5002: bleu = 38.93, global_step = 5002, log_perplexity = 1.3074583, loss = 1.225735
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 5003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 1.329521, step = 5003
INFO:tensorflow:global_step/sec: 0.148703
INFO:tensorflow:loss = 1.2776027, step = 5103
INFO:tensorflow:global_step/sec: 0.16744
INFO:tensorflow:loss = 1.327553, step = 5203
INFO:tensorflow:global_step/sec: 0.167036
INFO:tensorflow:loss = 1.3914838, step = 5303
INFO:tensorflow:global_step/sec: 0.16288
INFO:tensorflow:loss = 1.3547475, step = 5403
INFO:tensorflow:global_step/sec: 0.166355
INFO:tensorflow:loss = 1.1139796, step = 5503
INFO:tensorflow:global_step/sec: 0.162872
INFO:tensorflow:loss = 1.1735163, step = 5603
INFO:tensorflow:global_step/sec: 0.168826
INFO:tensorflow:loss = 1.2912265, step = 5703
INFO:tensorflow:global_step/sec: 0.165679
INFO:tensorflow:loss = 1.4090464, step = 5803
INFO:tensorflow:global_step/sec: 0.166409
INFO:tensorflow:loss = 1.485604, step = 5903
INFO:tensorflow:Saving checkpoints for 6002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.73028743.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-06-16:33:53
INFO:tensorflow:Finished evaluation at 2019-05-06-16:34:39
INFO:tensorflow:Saving dict for global step 6002: bleu = 41.59, global_step = 6002, log_perplexity = 1.2165635, loss = 1.1403803
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 6003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 1.2363914, step = 6003
INFO:tensorflow:global_step/sec: 0.1495
INFO:tensorflow:loss = 0.65719044, step = 6103
INFO:tensorflow:global_step/sec: 0.16621
INFO:tensorflow:loss = 1.0995985, step = 6203
INFO:tensorflow:global_step/sec: 0.166202
INFO:tensorflow:loss = 1.1525009, step = 6303
INFO:tensorflow:global_step/sec: 0.167442
INFO:tensorflow:loss = 0.72214794, step = 6403
INFO:tensorflow:global_step/sec: 0.165005
INFO:tensorflow:loss = 0.7374491, step = 6503
INFO:tensorflow:global_step/sec: 0.164938
INFO:tensorflow:loss = 1.3196416, step = 6603
INFO:tensorflow:global_step/sec: 0.164279
INFO:tensorflow:loss = 1.1429557, step = 6703
INFO:tensorflow:global_step/sec: 0.166241
INFO:tensorflow:loss = 1.3056674, step = 6803
INFO:tensorflow:global_step/sec: 0.170464
INFO:tensorflow:loss = 0.9005016, step = 6903
INFO:tensorflow:Saving checkpoints for 7002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 1.079701.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-06-18:16:16
INFO:tensorflow:Finished evaluation at 2019-05-06-18:17:02
INFO:tensorflow:Saving dict for global step 7002: bleu = 43.44, global_step = 7002, log_perplexity = 1.1408387, loss = 1.069083
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 7003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 1.0705498, step = 7003
INFO:tensorflow:global_step/sec: 0.145125
INFO:tensorflow:loss = 1.2942786, step = 7103
INFO:tensorflow:global_step/sec: 0.16345
INFO:tensorflow:loss = 1.1852099, step = 7203
INFO:tensorflow:global_step/sec: 0.164487
INFO:tensorflow:loss = 0.9664902, step = 7303
INFO:tensorflow:global_step/sec: 0.166227
INFO:tensorflow:loss = 0.8788658, step = 7403
INFO:tensorflow:global_step/sec: 0.163238
INFO:tensorflow:loss = 0.98281914, step = 7503
INFO:tensorflow:global_step/sec: 0.163125
INFO:tensorflow:loss = 1.0275853, step = 7603
INFO:tensorflow:global_step/sec: 0.163582
INFO:tensorflow:loss = 0.9411496, step = 7703
INFO:tensorflow:global_step/sec: 0.165189
INFO:tensorflow:loss = 1.1468899, step = 7803
INFO:tensorflow:global_step/sec: 0.157162
INFO:tensorflow:loss = 0.60205525, step = 7903
INFO:tensorflow:Saving checkpoints for 8002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.7620146.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-06-20:00:35
INFO:tensorflow:Finished evaluation at 2019-05-06-20:01:21
INFO:tensorflow:Saving dict for global step 8002: bleu = 45.95, global_step = 8002, log_perplexity = 1.0791752, loss = 1.0113161
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 8003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 1.1603099, step = 8003
INFO:tensorflow:global_step/sec: 0.147216
INFO:tensorflow:loss = 0.8948469, step = 8103
INFO:tensorflow:global_step/sec: 0.161051
INFO:tensorflow:loss = 0.48170698, step = 8203
INFO:tensorflow:global_step/sec: 0.164541
INFO:tensorflow:loss = 0.80771434, step = 8303
INFO:tensorflow:global_step/sec: 0.161727
INFO:tensorflow:loss = 1.1229279, step = 8403
INFO:tensorflow:global_step/sec: 0.162995
INFO:tensorflow:loss = 0.63072413, step = 8503
INFO:tensorflow:global_step/sec: 0.160923
INFO:tensorflow:loss = 0.87294424, step = 8603
INFO:tensorflow:global_step/sec: 0.163581
INFO:tensorflow:loss = 0.88956743, step = 8703
INFO:tensorflow:global_step/sec: 0.163119
INFO:tensorflow:loss = 1.0037159, step = 8803
INFO:tensorflow:global_step/sec: 0.166579
INFO:tensorflow:loss = 0.42027888, step = 8903
INFO:tensorflow:Saving checkpoints for 9002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.9568123.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-06-21:44:58
INFO:tensorflow:Finished evaluation at 2019-05-06-21:45:47
INFO:tensorflow:Saving dict for global step 9002: bleu = 48.2, global_step = 9002, log_perplexity = 1.0131812, loss = 0.9492931
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 9003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 1.0773585, step = 9003
INFO:tensorflow:global_step/sec: 0.144781
INFO:tensorflow:loss = 1.0506283, step = 9103
INFO:tensorflow:global_step/sec: 0.162144
INFO:tensorflow:loss = 0.6721676, step = 9203
INFO:tensorflow:global_step/sec: 0.164725
INFO:tensorflow:loss = 0.49440697, step = 9303
INFO:tensorflow:global_step/sec: 0.164326
INFO:tensorflow:loss = 0.57325846, step = 9403
INFO:tensorflow:global_step/sec: 0.162736
INFO:tensorflow:loss = 0.798534, step = 9503
INFO:tensorflow:global_step/sec: 0.165688
INFO:tensorflow:loss = 0.71598446, step = 9603
INFO:tensorflow:global_step/sec: 0.165011
INFO:tensorflow:loss = 0.578533, step = 9703
INFO:tensorflow:global_step/sec: 0.162229
INFO:tensorflow:loss = 0.58967346, step = 9803
INFO:tensorflow:global_step/sec: 0.164322
INFO:tensorflow:loss = 1.0576969, step = 9903
INFO:tensorflow:Saving checkpoints for 10002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.8835949.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-06-23:28:57
INFO:tensorflow:Finished evaluation at 2019-05-06-23:29:44
INFO:tensorflow:Saving dict for global step 10002: bleu = 51.27, global_step = 10002, log_perplexity = 0.95245725, loss = 0.8926453
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 10003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.89860874, step = 10003
INFO:tensorflow:global_step/sec: 0.14734
INFO:tensorflow:loss = 0.4191843, step = 10103
INFO:tensorflow:global_step/sec: 0.160164
INFO:tensorflow:loss = 0.6796383, step = 10203
INFO:tensorflow:global_step/sec: 0.166196
INFO:tensorflow:loss = 0.57425714, step = 10303
INFO:tensorflow:global_step/sec: 0.161097
INFO:tensorflow:loss = 1.0239272, step = 10403
INFO:tensorflow:global_step/sec: 0.162572
INFO:tensorflow:loss = 0.42087638, step = 10503
INFO:tensorflow:global_step/sec: 0.16405
INFO:tensorflow:loss = 0.83649254, step = 10603
INFO:tensorflow:global_step/sec: 0.164166
INFO:tensorflow:loss = 0.96469414, step = 10703
INFO:tensorflow:global_step/sec: 0.167011
INFO:tensorflow:loss = 0.9039929, step = 10803
INFO:tensorflow:global_step/sec: 0.164202
INFO:tensorflow:loss = 0.55408746, step = 10903
INFO:tensorflow:Saving checkpoints for 11002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.4057298.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-01:12:36
INFO:tensorflow:Finished evaluation at 2019-05-07-01:13:22
INFO:tensorflow:Saving dict for global step 11002: bleu = 54.58, global_step = 11002, log_perplexity = 0.8910795, loss = 0.8348394
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 11003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.8670498, step = 11003
INFO:tensorflow:global_step/sec: 0.148703
INFO:tensorflow:loss = 0.57230926, step = 11103
INFO:tensorflow:global_step/sec: 0.161458
INFO:tensorflow:loss = 0.8729099, step = 11203
INFO:tensorflow:global_step/sec: 0.163736
INFO:tensorflow:loss = 0.710702, step = 11303
INFO:tensorflow:global_step/sec: 0.167121
INFO:tensorflow:loss = 0.3710516, step = 11403
INFO:tensorflow:global_step/sec: 0.164994
INFO:tensorflow:loss = 0.66771764, step = 11503
INFO:tensorflow:global_step/sec: 0.165901
INFO:tensorflow:loss = 0.59590197, step = 11603
INFO:tensorflow:global_step/sec: 0.16599
INFO:tensorflow:loss = 0.4085273, step = 11703
INFO:tensorflow:global_step/sec: 0.163926
INFO:tensorflow:loss = 0.52205014, step = 11803
INFO:tensorflow:global_step/sec: 0.165645
INFO:tensorflow:loss = 0.42782724, step = 11903
INFO:tensorflow:Saving checkpoints for 12002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.46652997.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-02:55:46
INFO:tensorflow:Finished evaluation at 2019-05-07-02:56:33
INFO:tensorflow:Saving dict for global step 12002: bleu = 57.73, global_step = 12002, log_perplexity = 0.83627564, loss = 0.7839298
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 12003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.75758904, step = 12003
INFO:tensorflow:Prediction followed by Target @ Step 12004
====================================================================================================
protected String METHOD_1 ( TYPE_1 VAR_1 ) { return METHOD_2 ( VAR_1 != null ? VAR_1 . METHOD_3 ( ) ) null ) ; } SEQUENCE_END
public String METHOD_1 ( TYPE_1 VAR_1 ) { return METHOD_2 ( VAR_1 != null ? VAR_1 . METHOD_3 ( ) : null ) ; } SEQUENCE_END

private static void METHOD_1 ( TYPE_1 VAR_1 , TYPE_1 VAR_2 ) { VAR_3 . METHOD_2 ( METHOD_2 ( VAR_1 ) , METHOD_2 ( VAR_2 ) ) ; } SEQUENCE_END
public static void METHOD_1 ( TYPE_1 VAR_1 , TYPE_1 VAR_2 ) { VAR_3 . put ( METHOD_2 ( VAR_1 ) , METHOD_2 ( VAR_2 ) ) ; } SEQUENCE_END

protected void METHOD_1 ( ) { METHOD_2 ( ) ; METHOD_3 ( ) ; } SEQUENCE_END
protected void METHOD_1 ( ) { METHOD_2 ( ) ; METHOD_3 ( ) ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( ) { return ( ( ) ; new TYPE_2 ( ) : null ; } SEQUENCE_END
public TYPE_1 METHOD_1 ( ) { return METHOD_2 ( ) ? new TYPE_2 ( ) : null ; } SEQUENCE_END

private void METHOD_1 ( final e ) { throw new TYPE_1 ( VAR_1 , e ) getMessage ( ) ) ; } SEQUENCE_END
private void METHOD_1 ( Throwable e ) { throw new TYPE_1 ( VAR_1 , e . getMessage ( ) ) ; } SEQUENCE_END

public void METHOD_1 ( ) { TYPE_1 . METHOD_2 ( ) ; } SEQUENCE_END
public void METHOD_1 ( ) { TYPE_1 . METHOD_2 ( ) ; } SEQUENCE_END

private static TYPE_1 create ( return id ) { return new TYPE_1 ( STRING_1 ) . id ( id . valueOf ( id ) ) ; } SEQUENCE_END
public static TYPE_1 change ( int id ) { return new TYPE_1 ( STRING_1 ) . id ( String . valueOf ( id ) ) ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( ) { return VAR_1 TYPE_2 ( VAR_1 ) ; } SEQUENCE_END
public TYPE_1 METHOD_1 ( ) { return new TYPE_2 ( VAR_2 ) ; } SEQUENCE_END

protected TYPE_1 remove ( ) . ) { TYPE_1 e ; new TYPE_1 ( STRING_1 ) ; e . METHOD_1 ( e ) ; return e ; } SEQUENCE_END
private TYPE_1 fail ( Throwable t ) { TYPE_1 e = new TYPE_1 ( STRING_1 ) ; e . METHOD_1 ( t ) ; return e ; } SEQUENCE_END

public int METHOD_1 ( ) { return TYPE_1 . hash ( type , VAR_2 , VAR_4 , ; } SEQUENCE_END
public int METHOD_1 ( ) { return TYPE_1 . hash ( type , VAR_4 , VAR_5 ) ; } SEQUENCE_END

protected void METHOD_1 ( ) { super . METHOD_1 ( ) ; METHOD_2 . METHOD_1 ( ) ; } SEQUENCE_END
protected void METHOD_1 ( ) { super . METHOD_1 ( ) ; VAR_1 . METHOD_3 ( ) ; } SEQUENCE_END

public int METHOD_1 ( ) { return TYPE_1 . hash ( name , VAR_2 , VAR_2 ) ; } SEQUENCE_END
public int METHOD_1 ( ) { return TYPE_1 . hash ( name , VAR_1 , VAR_3 ) ; } SEQUENCE_END

public List < TYPE_1 > METHOD_1 ( boolean VAR_1 ) { throw new TYPE_2 ( ) ; } SEQUENCE_END
public List < TYPE_1 > METHOD_1 ( boolean VAR_1 ) { throw new TYPE_2 ( ) ; } SEQUENCE_END

public static TYPE_1 create ( ) { return new TYPE_1 ( ) ; } SEQUENCE_END
public static TYPE_1 create ( ) { return new TYPE_1 ( ) ; } SEQUENCE_END

public Map < String , TYPE_1 > METHOD_1 ( ) { throw new TYPE_2 ( ) ; } SEQUENCE_END
public Map < String , TYPE_1 > METHOD_1 ( ) { throw new TYPE_2 ( ) ; } SEQUENCE_END

protected void METHOD_1 ( ) { super . METHOD_1 ( ) ; METHOD_2 ( VAR_1 ) ; addValidationMessage ( VAR_2 ) ; } SEQUENCE_END
protected void METHOD_1 ( ) { super . METHOD_1 ( ) ; addValidationMessage ( VAR_1 ) ; addValidationMessage ( VAR_2 ) ; } SEQUENCE_END

public void METHOD_1 ( TYPE_1 VAR_1 ) throws IOException { super . METHOD_1 ( VAR_1 ) ; METHOD_2 ( ) ; } SEQUENCE_END
public void METHOD_1 ( TYPE_1 query ) throws IOException { super . METHOD_1 ( query ) ; METHOD_2 ( ) ; } SEQUENCE_END

public Map < String , Map < String , String > > METHOD_1 ( ) { return VAR_1 ; } SEQUENCE_END
public Map < TYPE_1 , Map < String , String > > METHOD_1 ( ) { return VAR_1 ; } SEQUENCE_END

public void METHOD_1 ( ) { METHOD_2 ( VAR_1 , VAR_2 , VAR_4 , true ) ; } SEQUENCE_END
public void METHOD_1 ( ) { METHOD_2 ( VAR_1 , VAR_2 , VAR_4 , true ) ; } SEQUENCE_END

private void METHOD_1 ( int response , String VAR_1 ) { METHOD_2 ( STRING_1 + METHOD_3 ( STRING_1 + + VAR_1 + VAR_1 + ; } SEQUENCE_END
protected void METHOD_1 ( int response , String VAR_1 ) { METHOD_2 ( STRING_1 + METHOD_3 ( response ) + STRING_2 + VAR_1 ) ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( TYPE_1 VAR_1 ) throws HyracksDataException { VAR_2 . add ( VAR_1 ) ; return VAR_1 ; } SEQUENCE_END
public TYPE_1 delete ( TYPE_1 VAR_1 ) throws HyracksDataException { VAR_3 . add ( VAR_1 ) ; return VAR_1 ; } SEQUENCE_END

protected List < Guid > METHOD_1 ( ) { return METHOD_2 ( ) . METHOD_1 ( ) ; } SEQUENCE_END
private ArrayList < Guid > METHOD_1 ( ) { return METHOD_2 ( ) . METHOD_1 ( ) ; } SEQUENCE_END

protected void METHOD_1 ( ) { VAR_1 . METHOD_2 ( VAR_2 ) ; } SEQUENCE_END
protected void METHOD_1 ( ) { VAR_1 . METHOD_2 ( VAR_2 ) ; } SEQUENCE_END

boolean VAR_1 ( ) ! VAR_2 ( ) && VAR_4 ( ) != VAR_3 && VAR_4 ( ) != VAR_3 ; } SEQUENCE_END
boolean VAR_1 ( return ! VAR_2 ( ) && VAR_3 ( ) != VAR_4 && VAR_3 ( ) != VAR_5 ; } SEQUENCE_END

public void METHOD_1 ( final VAR_1 , TYPE_1 . ) { METHOD_2 ( VAR_1 , false ) ; } SEQUENCE_END
public void METHOD_1 ( Branch.NameKey VAR_1 , TYPE_1 event ) { METHOD_2 ( VAR_1 , event ) ; } SEQUENCE_END

public void METHOD_1 ( ) { METHOD_2 . METHOD_2 ( STRING_1 ) ; METHOD_3 ( METHOD_3 ( ) , METHOD_3 ( VAR_1 ) ) ; } ( VAR_1 ( ) ) VAR_1 ( VAR_1 ) ) ; } SEQUENCE_END
public void METHOD_1 ( ) { vm . METHOD_2 ( STRING_1 ) ; assertThat ( filter ( ) , METHOD_3 ( VAR_1 ) ) ; assertThat ( messages ( ) , hasSize ( 1 ) ) ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( return type ) { this = METHOD_1 ( STRING_1 , VAR_2 ) ; return this ; } SEQUENCE_END
public TYPE_1 METHOD_1 ( String mode ) { VAR_1 . args ( STRING_1 , mode ) ; return this ; } SEQUENCE_END

protected void METHOD_1 ( TYPE_1 VAR_1 ) { TYPE_2 . METHOD_2 ( VAR_2 , ; } SEQUENCE_END
protected void METHOD_1 ( TYPE_1 VAR_1 ) { TYPE_2 . METHOD_2 ( VAR_2 ) ; } SEQUENCE_END

protected boolean METHOD_1 ( ) { return . METHOD_2 ( STRING_1 ) ; return this ; } SEQUENCE_END
protected boolean METHOD_1 ( ) { log . debug ( STRING_1 ) ; return true ; } SEQUENCE_END

public boolean METHOD_1 ( TYPE_1 VAR_1 , String VAR_2 , List < TYPE_2 > VAR_3 ) { throw new TYPE_3 ( STRING_1 ) ; } SEQUENCE_END
public boolean METHOD_1 ( DeviceId deviceId , String VAR_2 , List < TYPE_2 > VAR_3 ) { throw new TYPE_3 ( STRING_1 ) ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( TYPE_1 VAR_1 ) throws TYPE_2 { return ( TYPE_3 ) VAR_1 ; } SEQUENCE_END
public TYPE_1 METHOD_1 ( Object VAR_1 ) throws TYPE_2 { return ( TYPE_1 ) VAR_1 ; } SEQUENCE_END

public < TYPE_1 > METHOD_1 ( ) { return VAR_1 ; METHOD_2 ( ) ; } SEQUENCE_END
List < TYPE_1 > METHOD_1 ( ) { return VAR_1 . get ( ) ; } SEQUENCE_END

====================================================================================================


INFO:tensorflow:global_step/sec: 0.14846
INFO:tensorflow:loss = 0.70530295, step = 12103
INFO:tensorflow:global_step/sec: 0.164823
INFO:tensorflow:loss = 0.45323667, step = 12203
INFO:tensorflow:global_step/sec: 0.162834
INFO:tensorflow:loss = 0.5981753, step = 12303
INFO:tensorflow:global_step/sec: 0.163072
INFO:tensorflow:loss = 0.569172, step = 12403
INFO:tensorflow:global_step/sec: 0.164759
INFO:tensorflow:loss = 0.4911795, step = 12503
INFO:tensorflow:global_step/sec: 0.16391
INFO:tensorflow:loss = 0.38518527, step = 12603
INFO:tensorflow:global_step/sec: 0.165749
INFO:tensorflow:loss = 0.45139885, step = 12703
INFO:tensorflow:global_step/sec: 0.167154
INFO:tensorflow:loss = 0.4521635, step = 12803
INFO:tensorflow:global_step/sec: 0.163313
INFO:tensorflow:loss = 0.51681596, step = 12903
INFO:tensorflow:Saving checkpoints for 13002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.67625815.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-04:39:09
INFO:tensorflow:Finished evaluation at 2019-05-07-04:39:56
INFO:tensorflow:Saving dict for global step 13002: bleu = 60.57, global_step = 13002, log_perplexity = 0.77694833, loss = 0.72810924
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 13003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.45979083, step = 13003
INFO:tensorflow:global_step/sec: 0.145152
INFO:tensorflow:loss = 0.6113895, step = 13103
INFO:tensorflow:global_step/sec: 0.164698
INFO:tensorflow:loss = 0.41266474, step = 13203
INFO:tensorflow:global_step/sec: 0.165938
INFO:tensorflow:loss = 0.59072715, step = 13303
INFO:tensorflow:global_step/sec: 0.168297
INFO:tensorflow:loss = 0.64004225, step = 13403
INFO:tensorflow:global_step/sec: 0.162969
INFO:tensorflow:loss = 0.48131713, step = 13503
INFO:tensorflow:global_step/sec: 0.169154
INFO:tensorflow:loss = 0.38318568, step = 13603
INFO:tensorflow:global_step/sec: 0.166556
INFO:tensorflow:loss = 0.42312723, step = 13703
INFO:tensorflow:global_step/sec: 0.164361
INFO:tensorflow:loss = 0.5900797, step = 13803
INFO:tensorflow:global_step/sec: 0.166411
INFO:tensorflow:loss = 0.54203916, step = 13903
INFO:tensorflow:Saving checkpoints for 14002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.21255569.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-06:21:59
INFO:tensorflow:Finished evaluation at 2019-05-07-06:22:46
INFO:tensorflow:Saving dict for global step 14002: bleu = 64.79, global_step = 14002, log_perplexity = 0.7233322, loss = 0.67741024
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 14003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.557981, step = 14003
INFO:tensorflow:global_step/sec: 0.148077
INFO:tensorflow:loss = 0.31944495, step = 14103
INFO:tensorflow:global_step/sec: 0.16196
INFO:tensorflow:loss = 0.44189557, step = 14203
INFO:tensorflow:global_step/sec: 0.164166
INFO:tensorflow:loss = 0.54019994, step = 14303
INFO:tensorflow:global_step/sec: 0.166162
INFO:tensorflow:loss = 0.4237229, step = 14403
INFO:tensorflow:global_step/sec: 0.164796
INFO:tensorflow:loss = 0.3297575, step = 14503
INFO:tensorflow:global_step/sec: 0.167781
INFO:tensorflow:loss = 0.38141707, step = 14603
INFO:tensorflow:global_step/sec: 0.165218
INFO:tensorflow:loss = 0.40972635, step = 14703
INFO:tensorflow:global_step/sec: 0.166361
INFO:tensorflow:loss = 0.3645819, step = 14803
INFO:tensorflow:global_step/sec: 0.166738
INFO:tensorflow:loss = 0.25522962, step = 14903
INFO:tensorflow:Saving checkpoints for 15002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.36740136.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-08:04:59
INFO:tensorflow:Finished evaluation at 2019-05-07-08:05:45
INFO:tensorflow:Saving dict for global step 15002: bleu = 67.26, global_step = 15002, log_perplexity = 0.68412715, loss = 0.6405876
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 15003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.48215386, step = 15003
INFO:tensorflow:global_step/sec: 0.149102
INFO:tensorflow:loss = 0.398542, step = 15103
INFO:tensorflow:global_step/sec: 0.162597
INFO:tensorflow:loss = 0.2514734, step = 15203
INFO:tensorflow:global_step/sec: 0.164418
INFO:tensorflow:loss = 0.51728487, step = 15303
INFO:tensorflow:global_step/sec: 0.164598
INFO:tensorflow:loss = 0.26235747, step = 15403
INFO:tensorflow:global_step/sec: 0.165166
INFO:tensorflow:loss = 0.17495699, step = 15503
INFO:tensorflow:global_step/sec: 0.165034
INFO:tensorflow:loss = 0.33280963, step = 15603
INFO:tensorflow:global_step/sec: 0.165437
INFO:tensorflow:loss = 0.20291193, step = 15703
INFO:tensorflow:global_step/sec: 0.162998
INFO:tensorflow:loss = 0.37605262, step = 15803
INFO:tensorflow:global_step/sec: 0.166051
INFO:tensorflow:loss = 0.34596327, step = 15903
INFO:tensorflow:Saving checkpoints for 16002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.2329396.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-09:48:06
INFO:tensorflow:Finished evaluation at 2019-05-07-09:48:53
INFO:tensorflow:Saving dict for global step 16002: bleu = 69.9, global_step = 16002, log_perplexity = 0.6608959, loss = 0.6191628
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 16003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.3962966, step = 16003
INFO:tensorflow:global_step/sec: 0.150744
INFO:tensorflow:loss = 0.13167892, step = 16103
INFO:tensorflow:global_step/sec: 0.163197
INFO:tensorflow:loss = 0.31448138, step = 16203
INFO:tensorflow:global_step/sec: 0.167194
INFO:tensorflow:loss = 0.37084886, step = 16303
INFO:tensorflow:global_step/sec: 0.166593
INFO:tensorflow:loss = 0.37779957, step = 16403
INFO:tensorflow:global_step/sec: 0.165957
INFO:tensorflow:loss = 0.32436836, step = 16503
INFO:tensorflow:global_step/sec: 0.165476
INFO:tensorflow:loss = 0.32583508, step = 16603
INFO:tensorflow:global_step/sec: 0.163718
INFO:tensorflow:loss = 0.35243884, step = 16703
INFO:tensorflow:global_step/sec: 0.166198
INFO:tensorflow:loss = 0.3430061, step = 16803
INFO:tensorflow:global_step/sec: 0.165329
INFO:tensorflow:loss = 0.31551197, step = 16903
INFO:tensorflow:Saving checkpoints for 17002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.2691065.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-11:30:41
INFO:tensorflow:Finished evaluation at 2019-05-07-11:31:27
INFO:tensorflow:Saving dict for global step 17002: bleu = 70.9, global_step = 17002, log_perplexity = 0.64035136, loss = 0.5985083
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 17003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.2695103, step = 17003
INFO:tensorflow:global_step/sec: 0.149874
INFO:tensorflow:loss = 0.2796761, step = 17103
INFO:tensorflow:global_step/sec: 0.164989
INFO:tensorflow:loss = 0.24383931, step = 17203
INFO:tensorflow:global_step/sec: 0.16429
INFO:tensorflow:loss = 0.1503384, step = 17303
INFO:tensorflow:global_step/sec: 0.165485
INFO:tensorflow:loss = 0.25199503, step = 17403
INFO:tensorflow:global_step/sec: 0.16531
INFO:tensorflow:loss = 0.16237117, step = 17503
INFO:tensorflow:global_step/sec: 0.163309
INFO:tensorflow:loss = 0.3511653, step = 17603
INFO:tensorflow:global_step/sec: 0.163828
INFO:tensorflow:loss = 0.27362224, step = 17703
INFO:tensorflow:global_step/sec: 0.165917
INFO:tensorflow:loss = 0.27462298, step = 17803
INFO:tensorflow:global_step/sec: 0.166811
INFO:tensorflow:loss = 0.26586732, step = 17903
INFO:tensorflow:Saving checkpoints for 18002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.14572388.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-13:13:29
INFO:tensorflow:Finished evaluation at 2019-05-07-13:14:15
INFO:tensorflow:Saving dict for global step 18002: bleu = 73.67, global_step = 18002, log_perplexity = 0.6343135, loss = 0.59376776
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 18003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.26124895, step = 18003
INFO:tensorflow:global_step/sec: 0.149298
INFO:tensorflow:loss = 0.12865546, step = 18103
INFO:tensorflow:global_step/sec: 0.163713
INFO:tensorflow:loss = 0.12978873, step = 18203
INFO:tensorflow:global_step/sec: 0.163468
INFO:tensorflow:loss = 0.2787444, step = 18303
INFO:tensorflow:global_step/sec: 0.167162
INFO:tensorflow:loss = 0.11889043, step = 18403
INFO:tensorflow:global_step/sec: 0.162709
INFO:tensorflow:loss = 0.16424184, step = 18503
INFO:tensorflow:global_step/sec: 0.164949
INFO:tensorflow:loss = 0.15740608, step = 18603
INFO:tensorflow:global_step/sec: 0.163827
INFO:tensorflow:loss = 0.24912275, step = 18703
INFO:tensorflow:global_step/sec: 0.165597
INFO:tensorflow:loss = 0.14448528, step = 18803
INFO:tensorflow:global_step/sec: 0.164388
INFO:tensorflow:loss = 0.1291538, step = 18903
INFO:tensorflow:Saving checkpoints for 19002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.13142926.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-14:56:41
INFO:tensorflow:Finished evaluation at 2019-05-07-14:57:27
INFO:tensorflow:Saving dict for global step 19002: bleu = 73.11, global_step = 19002, log_perplexity = 0.6375531, loss = 0.59587854
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 19003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.24874252, step = 19003
INFO:tensorflow:global_step/sec: 0.147303
INFO:tensorflow:loss = 0.13346608, step = 19103
INFO:tensorflow:global_step/sec: 0.166011
INFO:tensorflow:loss = 0.29232734, step = 19203
INFO:tensorflow:global_step/sec: 0.163057
INFO:tensorflow:loss = 0.19458692, step = 19303
INFO:tensorflow:global_step/sec: 0.166342
INFO:tensorflow:loss = 0.118473366, step = 19403
INFO:tensorflow:global_step/sec: 0.163208
INFO:tensorflow:loss = 0.13201419, step = 19503
INFO:tensorflow:global_step/sec: 0.166034
INFO:tensorflow:loss = 0.16917613, step = 19603
INFO:tensorflow:global_step/sec: 0.16661
INFO:tensorflow:loss = 0.08049935, step = 19703
INFO:tensorflow:global_step/sec: 0.165922
INFO:tensorflow:loss = 0.08269637, step = 19803
INFO:tensorflow:global_step/sec: 0.161962
INFO:tensorflow:loss = 0.24244921, step = 19903
INFO:tensorflow:Saving checkpoints for 20002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.12594402.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-16:39:46
INFO:tensorflow:Finished evaluation at 2019-05-07-16:40:32
INFO:tensorflow:Saving dict for global step 20002: bleu = 75.31, global_step = 20002, log_perplexity = 0.62239164, loss = 0.58209044
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 20003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.17906801, step = 20003
INFO:tensorflow:global_step/sec: 0.148059
INFO:tensorflow:loss = 0.1093618, step = 20103
INFO:tensorflow:global_step/sec: 0.168331
INFO:tensorflow:loss = 0.20530911, step = 20203
INFO:tensorflow:global_step/sec: 0.166972
INFO:tensorflow:loss = 0.12719361, step = 20303
INFO:tensorflow:global_step/sec: 0.164527
INFO:tensorflow:loss = 0.08674077, step = 20403
INFO:tensorflow:global_step/sec: 0.166285
INFO:tensorflow:loss = 0.100093886, step = 20503
INFO:tensorflow:global_step/sec: 0.167954
INFO:tensorflow:loss = 0.08008283, step = 20603
INFO:tensorflow:global_step/sec: 0.16503
INFO:tensorflow:loss = 0.13362728, step = 20703
INFO:tensorflow:global_step/sec: 0.165596
INFO:tensorflow:loss = 0.091920055, step = 20803
INFO:tensorflow:global_step/sec: 0.163468
INFO:tensorflow:loss = 0.14318183, step = 20903
INFO:tensorflow:Saving checkpoints for 21002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.19608703.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-18:22:29
INFO:tensorflow:Finished evaluation at 2019-05-07-18:23:16
INFO:tensorflow:Saving dict for global step 21002: bleu = 75.39, global_step = 21002, log_perplexity = 0.63037705, loss = 0.5896735
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 21003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.082257085, step = 21003
INFO:tensorflow:global_step/sec: 0.145945
INFO:tensorflow:loss = 0.13523789, step = 21103
INFO:tensorflow:global_step/sec: 0.167325
INFO:tensorflow:loss = 0.15055169, step = 21203
INFO:tensorflow:global_step/sec: 0.164321
INFO:tensorflow:loss = 0.12690589, step = 21303
INFO:tensorflow:global_step/sec: 0.164835
INFO:tensorflow:loss = 0.105700664, step = 21403
INFO:tensorflow:global_step/sec: 0.163988
INFO:tensorflow:loss = 0.1028062, step = 21503
INFO:tensorflow:global_step/sec: 0.166306
INFO:tensorflow:loss = 0.123560704, step = 21603
INFO:tensorflow:global_step/sec: 0.164371
INFO:tensorflow:loss = 0.1306301, step = 21703
INFO:tensorflow:global_step/sec: 0.163058
INFO:tensorflow:loss = 0.13842738, step = 21803
INFO:tensorflow:global_step/sec: 0.167403
INFO:tensorflow:loss = 0.12325956, step = 21903
INFO:tensorflow:Saving checkpoints for 22002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.08710147.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-20:05:31
INFO:tensorflow:Finished evaluation at 2019-05-07-20:06:18
INFO:tensorflow:Saving dict for global step 22002: bleu = 75.19, global_step = 22002, log_perplexity = 0.64079326, loss = 0.59869325
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 22003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.10802163, step = 22003
INFO:tensorflow:Prediction followed by Target @ Step 22004
====================================================================================================
public boolean METHOD_1 ( TYPE_1 ctx ) throws OrmException , TYPE_2 , IOException { boolean ret = VAR_1 . METHOD_1 ( ctx ) ; VAR_2 = VAR_1 . METHOD_2 ( ) ; return true ; } SEQUENCE_END
public boolean METHOD_1 ( TYPE_1 ctx ) throws OrmException , TYPE_2 , IOException { boolean ret = VAR_1 . METHOD_1 ( ctx ) ; VAR_2 = VAR_1 . METHOD_2 ( ) ; return ret ; } SEQUENCE_END

private void METHOD_1 ( Throwable e ) { if ( e instanceof RuntimeException ) { throw ( RuntimeException ) e ; } else { throw new RuntimeException ( e ) ; } } SEQUENCE_END
private void METHOD_1 ( Exception e ) { if ( e instanceof RuntimeException ) { throw ( RuntimeException ) e ; } else { throw new RuntimeException ( e ) ; } } SEQUENCE_END

public TYPE_1 < TYPE_2 > METHOD_1 ( String project , String revision , String file ) { return METHOD_2 ( VAR_1 , VAR_2 - > VAR_2 . METHOD_3 ( project , revision , file ) ) ; } SEQUENCE_END
public List < TYPE_2 > METHOD_1 ( String project , String revision , String file ) { return METHOD_2 ( VAR_1 , VAR_2 - > VAR_2 . METHOD_3 ( project , revision , file ) ) ; } SEQUENCE_END

private void METHOD_1 ( int response , Object ret ) { METHOD_2 ( STRING_1 + METHOD_3 ( response ) + STRING_2 + METHOD_4 ( response , ret ) ) ; } SEQUENCE_END
protected void METHOD_1 ( int response , Object ret ) { METHOD_2 ( STRING_1 + METHOD_3 ( response ) + STRING_2 + METHOD_4 ( response , ret ) ) ; } SEQUENCE_END

private void METHOD_1 ( ) { VAR_1 . METHOD_2 ( new - > VAR_2 . execute ( ) ) ; } SEQUENCE_END
private void METHOD_1 ( ) { VAR_1 . METHOD_2 ( event - > VAR_2 . execute ( ) ) ; } SEQUENCE_END

public boolean METHOD_1 ( TYPE_1 vm ) { return vm != null && ( Boolean ) METHOD_2 ( VAR_1 , vm . METHOD_3 ( ) . toString ( ) ) ; } SEQUENCE_END
public boolean METHOD_1 ( TYPE_1 vm ) { return vm != null && ( Boolean ) METHOD_2 ( VAR_1 , vm . METHOD_3 ( ) . toString ( ) ) ; } SEQUENCE_END

public synchronized void METHOD_1 ( TYPE_1 VAR_1 ) throws VAR_1 . METHOD_2 ( this ) ; if ( ! VAR_1 . METHOD_3 ( ) ) { VAR_2 . add ( VAR_1 ) ; } } SEQUENCE_END
public synchronized void METHOD_1 ( TYPE_1 VAR_1 ) { VAR_1 . METHOD_2 ( this ) ; if ( ! VAR_1 . METHOD_3 ( ) ) { VAR_2 . add ( VAR_1 ) ; } } SEQUENCE_END

static boolean METHOD_1 ( TYPE_1 config , final name , boolean VAR_1 ) { return config . METHOD_2 ( STRING_1 , name , VAR_1 ) ; } SEQUENCE_END
static boolean METHOD_1 ( TYPE_1 config , String name , boolean VAR_1 ) { return config . METHOD_2 ( STRING_1 , name , VAR_1 ) ; } SEQUENCE_END

protected TYPE_1 TYPE_1 METHOD_1 ( TYPE_2 type ) throws OrmException { TYPE_3 VAR_1 = new TYPE_3 ( args , project , VAR_2 , VAR_3 ) ; return VAR_1 . METHOD_1 ( type ) ; } SEQUENCE_END
protected final TYPE_1 METHOD_1 ( TYPE_2 type ) throws OrmException { TYPE_3 VAR_1 = new TYPE_3 ( args , project , VAR_2 , VAR_3 ) ; return VAR_1 . METHOD_1 ( type ) ; } SEQUENCE_END

static boolean METHOD_1 ( String VAR_1 ) { return ( VAR_1 . METHOD_2 ( STRING_1 ) || VAR_1 . METHOD_2 ( STRING_2 ) || VAR_1 . METHOD_2 ( STRING_2 ) ) { return true ; } return true ; } SEQUENCE_END
static boolean METHOD_1 ( String VAR_1 ) { if ( VAR_1 . METHOD_2 ( STRING_1 ) || VAR_1 . METHOD_2 ( STRING_2 ) || VAR_1 . METHOD_2 ( STRING_3 ) ) { return true ; } return false ; } SEQUENCE_END

public static METHOD_1 ( TYPE_2 VAR_1 ) { if ( VAR_2 == null ) { return null ; } return VAR_2 . get ( VAR_1 ) ; } SEQUENCE_END
public TYPE_1 METHOD_1 ( TYPE_2 VAR_1 ) { if ( VAR_2 == null ) { return null ; } return VAR_2 . get ( VAR_1 ) ; } SEQUENCE_END

public String toString ( ) { return METHOD_1 ( this ) . add ( STRING_1 , order ) . add ( STRING_2 , value ) . toString ( ) ; } SEQUENCE_END
public String toString ( ) { return METHOD_1 ( this ) . add ( STRING_1 , order ) . add ( STRING_2 , value ) . toString ( ) ; } SEQUENCE_END

public static TYPE_1 getParameters ( TYPE_2 VAR_1 ) { return ( TYPE_1 ) VAR_1 . METHOD_1 ( ) != get ( VAR_2 ) ; } SEQUENCE_END
public static TYPE_1 getParameters ( TYPE_2 VAR_1 ) { return ( TYPE_1 ) VAR_1 . METHOD_1 ( ) . get ( VAR_2 ) ; } SEQUENCE_END

TYPE_1 < ? > insert ( final TYPE_2 VAR_1 ) { return METHOD_1 ( ( ) - > VAR_2 . METHOD_2 ( VAR_1 ) ) ; } SEQUENCE_END
TYPE_1 < ? > insert ( final TYPE_2 VAR_1 ) { return METHOD_1 ( ( ) - > VAR_2 . METHOD_2 ( VAR_1 ) ) ; } SEQUENCE_END

void METHOD_1 ( int seq , TYPE_1 item , int VAR_1 , int VAR_2 , int VAR_3 ) throws RemoteException { VAR_4 . METHOD_1 ( seq , TYPE_2 . METHOD_2 ( item ) , VAR_1 , VAR_2 , VAR_3 ) ; } SEQUENCE_END
void METHOD_1 ( int seq , TYPE_1 item , int VAR_1 , int VAR_2 , int VAR_3 ) throws RemoteException { VAR_4 . METHOD_1 ( seq , TYPE_2 . METHOD_2 ( item ) , VAR_1 , VAR_2 , VAR_3 ) ; } SEQUENCE_END

public static String METHOD_1 ( String VAR_1 ) { VAR_1 = TYPE_1 . METHOD_2 ( VAR_1 ) ; VAR_1 = TYPE_2 . METHOD_3 ( VAR_1 ) ; return VAR_1 ; } SEQUENCE_END
public static String METHOD_1 ( String VAR_1 ) { VAR_1 = TYPE_1 . METHOD_2 ( VAR_1 ) ; VAR_1 = TYPE_2 . METHOD_3 ( VAR_1 ) ; return VAR_1 ; } SEQUENCE_END

protected void METHOD_1 ( ) { VAR_1 = true ; TYPE_1 VAR_2 = METHOD_2 ( ) ; if ( VAR_2 != null ) { VAR_2 . METHOD_1 ( ) ; } } SEQUENCE_END
protected void METHOD_1 ( ) { VAR_1 = true ; TYPE_1 VAR_2 = METHOD_2 ( ) ; if ( VAR_2 != null ) { VAR_2 . METHOD_1 ( ) ; } } SEQUENCE_END

public static void METHOD_1 ( change ( project , id ) . view ( STRING_1 ) . delete ( VAR_1 ) ; } SEQUENCE_END
public static void METHOD_1 ( change ( project , id ) . view ( STRING_1 ) . delete ( VAR_1 ) ; } SEQUENCE_END

public void METHOD_1 ( TYPE_1 VAR_1 , int flags ) { VAR_1 . METHOD_2 ( VAR_2 ) ; VAR_1 . METHOD_3 ( VAR_3 ) ; } SEQUENCE_END
public void METHOD_1 ( TYPE_1 VAR_1 , int flags ) { VAR_1 . METHOD_2 ( VAR_2 ) ; VAR_1 . METHOD_3 ( VAR_3 ) ; } SEQUENCE_END

public void METHOD_1 ( List < TYPE_1 > VAR_1 ) { if ( VAR_1 == null ) { if ( TYPE_1 VAR_2 : VAR_1 ) { VAR_2 . METHOD_2 ( METHOD_3 ( ) ) ; } } } SEQUENCE_END
public void METHOD_1 ( List < TYPE_1 > VAR_1 ) { if ( VAR_1 != null ) { for ( TYPE_1 VAR_2 : VAR_1 ) { VAR_2 . METHOD_2 ( METHOD_3 ( ) ) ; } } } SEQUENCE_END

void METHOD_1 ( int VAR_1 ) { VAR_2 = VAR_1 ; if ( view != null ) { view . METHOD_2 ( VAR_1 + STRING_1 ) ; view . METHOD_3 ( VAR_3 , false ) ; } } SEQUENCE_END
void METHOD_1 ( int VAR_1 ) { VAR_2 = VAR_1 ; if ( view != null ) { view . METHOD_2 ( VAR_1 + STRING_1 ) ; view . METHOD_3 ( VAR_3 , true ) ; } } SEQUENCE_END

public boolean METHOD_1 ( TYPE_1 object ) throws OrmException { Change change = object . change ( ) ; return change != null && status ( status , change . getStatus ( ) ) ; } SEQUENCE_END
public boolean METHOD_1 ( TYPE_1 object ) throws OrmException { Change change = object . change ( ) ; return change != null && Objects.equals ( status , change . getStatus ( ) ) ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( ) { return METHOD_2 ( METHOD_3 ( ) . METHOD_4 ( ) . METHOD_5 ( network . getId ( ) ) , VAR_1 ) ; } SEQUENCE_END
public TYPE_1 METHOD_1 ( ) { return METHOD_2 ( METHOD_3 ( ) . METHOD_4 ( ) . METHOD_5 ( network . getId ( ) ) , VAR_1 ) ; } SEQUENCE_END

protected boolean validate ( ) { boolean result = true ; if ( METHOD_1 ( ) == null ) { addValidationMessage ( VAR_1 ) ; result = false ; } return result ; } SEQUENCE_END
protected boolean validate ( ) { boolean result = true ; if ( METHOD_1 ( ) == null ) { addValidationMessage ( VAR_1 ) ; result = false ; } return result ; } SEQUENCE_END

public String METHOD_1 ( ) { return STRING_1 + TYPE_1 . toString ( this . VAR_2 ) + STRING_2 + id ) ) . toString ( ) ; } SEQUENCE_END
public String METHOD_1 ( ) { return STRING_1 + TYPE_1 . toString ( this . VAR_2 ) + STRING_2 + id ( ) . toString ( ) ; } SEQUENCE_END

public static String METHOD_1 ( TYPE_1 VAR_1 ) { String VAR_2 = VAR_1 . get ( VAR_3 ) ; return VAR_2 != null ? VAR_2 : VAR_4 . get ( VAR_5 ) ; } SEQUENCE_END
public static String METHOD_1 ( TYPE_1 VAR_1 ) { String VAR_2 = VAR_1 . get ( VAR_3 ) ; return VAR_2 != null ? VAR_2 : VAR_1 . get ( VAR_5 ) ; } SEQUENCE_END

private void METHOD_1 ( TYPE_1 VAR_1 , TYPE_2 parameters ) { log . debug ( STRING_1 , VAR_1 , parameters ) ; } SEQUENCE_END
private void METHOD_1 ( TYPE_1 VAR_1 , TYPE_2 parameters ) { log . debug ( STRING_1 , VAR_1 , parameters ) ; } SEQUENCE_END

private void METHOD_1 ( String iface ) { METHOD_2 ( VAR_1 - > VAR_1 . METHOD_3 ( iface ) ) ; } SEQUENCE_END
private void METHOD_1 ( String iface ) { METHOD_2 ( VAR_1 - > VAR_1 . METHOD_3 ( iface ) ) ; } SEQUENCE_END

public void METHOD_1 ( ) throws TYPE_1 { if ( ! VAR_2 ) { METHOD_2 ( true ) ; } VAR_3 . METHOD_3 ( true ) ; } SEQUENCE_END
public void METHOD_1 ( ) throws TYPE_1 { if ( ! VAR_2 ) { METHOD_2 ( null ) ; } VAR_3 . METHOD_3 ( true ) ; } SEQUENCE_END

public static void METHOD_1 ( METHOD_2 ( project , id ) . METHOD_3 ( STRING_1 ) . get ( VAR_1 ) ; } SEQUENCE_END
public static void METHOD_1 ( METHOD_2 ( project , id ) . METHOD_3 ( STRING_1 ) . get ( VAR_1 ) ; } SEQUENCE_END

public void METHOD_1 ( Guid VAR_1 ) { if ( VAR_1 , false ) ; } SEQUENCE_END
public void METHOD_1 ( Guid VAR_1 ) { METHOD_1 ( VAR_1 , false ) ; } SEQUENCE_END

private TYPE_1 METHOD_1 ( RevCommit c , TYPE_2 key ) throws OrmException { return new TYPE_1 ( c , key , VAR_1 . get ( ) . METHOD_2 ( VAR_2 , key ) ) ; } SEQUENCE_END
private TYPE_1 METHOD_1 ( RevCommit c , TYPE_2 key ) throws OrmException { return new TYPE_1 ( c , key , VAR_1 . get ( ) . METHOD_2 ( VAR_2 , key ) ) ; } SEQUENCE_END

====================================================================================================


INFO:tensorflow:global_step/sec: 0.149814
INFO:tensorflow:loss = 0.12593544, step = 22103
INFO:tensorflow:global_step/sec: 0.163718
INFO:tensorflow:loss = 0.14154047, step = 22203
INFO:tensorflow:global_step/sec: 0.164767
INFO:tensorflow:loss = 0.050870754, step = 22303
INFO:tensorflow:global_step/sec: 0.164674
INFO:tensorflow:loss = 0.1287113, step = 22403
INFO:tensorflow:global_step/sec: 0.166676
INFO:tensorflow:loss = 0.08891767, step = 22503
INFO:tensorflow:global_step/sec: 0.164096
INFO:tensorflow:loss = 0.12582088, step = 22603
INFO:tensorflow:global_step/sec: 0.167922
INFO:tensorflow:loss = 0.07596865, step = 22703
INFO:tensorflow:global_step/sec: 0.165772
INFO:tensorflow:loss = 0.07989035, step = 22803
INFO:tensorflow:global_step/sec: 0.167289
INFO:tensorflow:loss = 0.12238209, step = 22903
INFO:tensorflow:Saving checkpoints for 23002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.1050679.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-21:48:06
INFO:tensorflow:Finished evaluation at 2019-05-07-21:48:52
INFO:tensorflow:Saving dict for global step 23002: bleu = 75.38, global_step = 23002, log_perplexity = 0.6692117, loss = 0.6255759
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 23003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.10240961, step = 23003
INFO:tensorflow:global_step/sec: 0.148935
INFO:tensorflow:loss = 0.07621895, step = 23103
INFO:tensorflow:global_step/sec: 0.165801
INFO:tensorflow:loss = 0.104665294, step = 23203
INFO:tensorflow:global_step/sec: 0.165289
INFO:tensorflow:loss = 0.0604727, step = 23303
INFO:tensorflow:global_step/sec: 0.16177
INFO:tensorflow:loss = 0.06645872, step = 23403
INFO:tensorflow:global_step/sec: 0.16331
INFO:tensorflow:loss = 0.12602141, step = 23503
INFO:tensorflow:global_step/sec: 0.164862
INFO:tensorflow:loss = 0.06211605, step = 23603
INFO:tensorflow:global_step/sec: 0.164587
INFO:tensorflow:loss = 0.09478969, step = 23703
INFO:tensorflow:global_step/sec: 0.163906
INFO:tensorflow:loss = 0.06204342, step = 23803
INFO:tensorflow:global_step/sec: 0.163287
INFO:tensorflow:loss = 0.058130097, step = 23903
INFO:tensorflow:Saving checkpoints for 24002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.08117418.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-07-23:31:37
INFO:tensorflow:Finished evaluation at 2019-05-07-23:32:24
INFO:tensorflow:Saving dict for global step 24002: bleu = 76.2, global_step = 24002, log_perplexity = 0.6639826, loss = 0.620006
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 24003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.11277927, step = 24003
INFO:tensorflow:global_step/sec: 0.146793
INFO:tensorflow:loss = 0.10303908, step = 24103
INFO:tensorflow:global_step/sec: 0.164315
INFO:tensorflow:loss = 0.107816406, step = 24203
INFO:tensorflow:global_step/sec: 0.163624
INFO:tensorflow:loss = 0.07484074, step = 24303
INFO:tensorflow:global_step/sec: 0.1656
INFO:tensorflow:loss = 0.08760816, step = 24403
INFO:tensorflow:global_step/sec: 0.166636
INFO:tensorflow:loss = 0.069675975, step = 24503
INFO:tensorflow:global_step/sec: 0.165701
INFO:tensorflow:loss = 0.07205161, step = 24603
INFO:tensorflow:global_step/sec: 0.164838
INFO:tensorflow:loss = 0.06110784, step = 24703
INFO:tensorflow:global_step/sec: 0.165423
INFO:tensorflow:loss = 0.06650237, step = 24803
INFO:tensorflow:global_step/sec: 0.165066
INFO:tensorflow:loss = 0.07864247, step = 24903
INFO:tensorflow:Saving checkpoints for 25002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.04762419.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-01:14:29
INFO:tensorflow:Finished evaluation at 2019-05-08-01:15:16
INFO:tensorflow:Saving dict for global step 25002: bleu = 76.2, global_step = 25002, log_perplexity = 0.67389625, loss = 0.6299408
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 25003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.036599014, step = 25003
INFO:tensorflow:global_step/sec: 0.147113
INFO:tensorflow:loss = 0.06564051, step = 25103
INFO:tensorflow:global_step/sec: 0.166535
INFO:tensorflow:loss = 0.11975699, step = 25203
INFO:tensorflow:global_step/sec: 0.165425
INFO:tensorflow:loss = 0.05822311, step = 25303
INFO:tensorflow:global_step/sec: 0.163204
INFO:tensorflow:loss = 0.077969186, step = 25403
INFO:tensorflow:global_step/sec: 0.165233
INFO:tensorflow:loss = 0.03811306, step = 25503
INFO:tensorflow:global_step/sec: 0.165102
INFO:tensorflow:loss = 0.05293527, step = 25603
INFO:tensorflow:global_step/sec: 0.1619
INFO:tensorflow:loss = 0.06298488, step = 25703
INFO:tensorflow:global_step/sec: 0.167383
INFO:tensorflow:loss = 0.05367961, step = 25803
INFO:tensorflow:global_step/sec: 0.165922
INFO:tensorflow:loss = 0.043897502, step = 25903
INFO:tensorflow:Saving checkpoints for 26002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.060965076.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-02:57:44
INFO:tensorflow:Finished evaluation at 2019-05-08-02:58:30
INFO:tensorflow:Saving dict for global step 26002: bleu = 76.89, global_step = 26002, log_perplexity = 0.6756496, loss = 0.63153845
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 26003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.06316101, step = 26003
INFO:tensorflow:global_step/sec: 0.146397
INFO:tensorflow:loss = 0.034174375, step = 26103
INFO:tensorflow:global_step/sec: 0.166074
INFO:tensorflow:loss = 0.03584092, step = 26203
INFO:tensorflow:global_step/sec: 0.16469
INFO:tensorflow:loss = 0.033028603, step = 26303
INFO:tensorflow:global_step/sec: 0.162876
INFO:tensorflow:loss = 0.04130766, step = 26403
INFO:tensorflow:global_step/sec: 0.161155
INFO:tensorflow:loss = 0.09216071, step = 26503
INFO:tensorflow:global_step/sec: 0.162741
INFO:tensorflow:loss = 0.05981687, step = 26603
INFO:tensorflow:global_step/sec: 0.164773
INFO:tensorflow:loss = 0.045962587, step = 26703
INFO:tensorflow:global_step/sec: 0.162168
INFO:tensorflow:loss = 0.04396382, step = 26803
INFO:tensorflow:global_step/sec: 0.162121
INFO:tensorflow:loss = 0.037552994, step = 26903
INFO:tensorflow:Saving checkpoints for 27002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.03630115.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-04:41:45
INFO:tensorflow:Finished evaluation at 2019-05-08-04:42:32
INFO:tensorflow:Saving dict for global step 27002: bleu = 77.59, global_step = 27002, log_perplexity = 0.67416954, loss = 0.62996566
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 27003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.05187307, step = 27003
INFO:tensorflow:global_step/sec: 0.147097
INFO:tensorflow:loss = 0.062207647, step = 27103
INFO:tensorflow:global_step/sec: 0.1643
INFO:tensorflow:loss = 0.07146078, step = 27203
INFO:tensorflow:global_step/sec: 0.162198
INFO:tensorflow:loss = 0.045246813, step = 27303
INFO:tensorflow:global_step/sec: 0.162424
INFO:tensorflow:loss = 0.036128476, step = 27403
INFO:tensorflow:global_step/sec: 0.161748
INFO:tensorflow:loss = 0.02166723, step = 27503
INFO:tensorflow:global_step/sec: 0.163157
INFO:tensorflow:loss = 0.04375233, step = 27603
INFO:tensorflow:global_step/sec: 0.162758
INFO:tensorflow:loss = 0.029512586, step = 27703
INFO:tensorflow:global_step/sec: 0.160735
INFO:tensorflow:loss = 0.048285294, step = 27803
INFO:tensorflow:global_step/sec: 0.162869
INFO:tensorflow:loss = 0.018909438, step = 27903
INFO:tensorflow:Saving checkpoints for 28002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.037915308.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-06:26:10
INFO:tensorflow:Finished evaluation at 2019-05-08-06:26:57
INFO:tensorflow:Saving dict for global step 28002: bleu = 77.07, global_step = 28002, log_perplexity = 0.71366554, loss = 0.6663362
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 28003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.05529572, step = 28003
INFO:tensorflow:global_step/sec: 0.147257
INFO:tensorflow:loss = 0.02951438, step = 28103
INFO:tensorflow:global_step/sec: 0.162589
INFO:tensorflow:loss = 0.073514946, step = 28203
INFO:tensorflow:global_step/sec: 0.162453
INFO:tensorflow:loss = 0.036639545, step = 28303
INFO:tensorflow:global_step/sec: 0.162819
INFO:tensorflow:loss = 0.033126365, step = 28403
INFO:tensorflow:global_step/sec: 0.162793
INFO:tensorflow:loss = 0.0365866, step = 28503
INFO:tensorflow:global_step/sec: 0.164258
INFO:tensorflow:loss = 0.024947565, step = 28603
INFO:tensorflow:global_step/sec: 0.162272
INFO:tensorflow:loss = 0.059969403, step = 28703
INFO:tensorflow:global_step/sec: 0.162389
INFO:tensorflow:loss = 0.05189247, step = 28803
INFO:tensorflow:global_step/sec: 0.166321
INFO:tensorflow:loss = 0.070686325, step = 28903
INFO:tensorflow:Saving checkpoints for 29002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.020552896.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-08:10:09
INFO:tensorflow:Finished evaluation at 2019-05-08-08:10:56
INFO:tensorflow:Saving dict for global step 29002: bleu = 77.98, global_step = 29002, log_perplexity = 0.70400333, loss = 0.6576596
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 29003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.031128097, step = 29003
INFO:tensorflow:global_step/sec: 0.148466
INFO:tensorflow:loss = 0.027768126, step = 29103
INFO:tensorflow:global_step/sec: 0.162385
INFO:tensorflow:loss = 0.057237152, step = 29203
INFO:tensorflow:global_step/sec: 0.163498
INFO:tensorflow:loss = 0.029709106, step = 29303
INFO:tensorflow:global_step/sec: 0.164587
INFO:tensorflow:loss = 0.026856836, step = 29403
INFO:tensorflow:global_step/sec: 0.163477
INFO:tensorflow:loss = 0.026368676, step = 29503
INFO:tensorflow:global_step/sec: 0.165375
INFO:tensorflow:loss = 0.015481652, step = 29603
INFO:tensorflow:global_step/sec: 0.161611
INFO:tensorflow:loss = 0.028892545, step = 29703
INFO:tensorflow:global_step/sec: 0.161118
INFO:tensorflow:loss = 0.050796848, step = 29803
INFO:tensorflow:global_step/sec: 0.163433
INFO:tensorflow:loss = 0.044225574, step = 29903
INFO:tensorflow:Saving checkpoints for 30002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.03772212.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-09:54:15
INFO:tensorflow:Finished evaluation at 2019-05-08-09:55:01
INFO:tensorflow:Saving dict for global step 30002: bleu = 78.24, global_step = 30002, log_perplexity = 0.7086721, loss = 0.6634909
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 30003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.024454746, step = 30003
INFO:tensorflow:global_step/sec: 0.14491
INFO:tensorflow:loss = 0.018760676, step = 30103
INFO:tensorflow:global_step/sec: 0.165122
INFO:tensorflow:loss = 0.013455792, step = 30203
INFO:tensorflow:global_step/sec: 0.16235
INFO:tensorflow:loss = 0.025335357, step = 30303
INFO:tensorflow:global_step/sec: 0.16361
INFO:tensorflow:loss = 0.036677822, step = 30403
INFO:tensorflow:global_step/sec: 0.165115
INFO:tensorflow:loss = 0.028366227, step = 30503
INFO:tensorflow:global_step/sec: 0.166106
INFO:tensorflow:loss = 0.040336233, step = 30603
INFO:tensorflow:global_step/sec: 0.164744
INFO:tensorflow:loss = 0.012565492, step = 30703
INFO:tensorflow:global_step/sec: 0.163618
INFO:tensorflow:loss = 0.023291878, step = 30803
INFO:tensorflow:global_step/sec: 0.162145
INFO:tensorflow:loss = 0.024734708, step = 30903
INFO:tensorflow:Saving checkpoints for 31002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.022390528.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-11:38:03
INFO:tensorflow:Finished evaluation at 2019-05-08-11:38:50
INFO:tensorflow:Saving dict for global step 31002: bleu = 77.94, global_step = 31002, log_perplexity = 0.73000187, loss = 0.6822435
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 31003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.036145933, step = 31003
INFO:tensorflow:global_step/sec: 0.14676
INFO:tensorflow:loss = 0.032196343, step = 31103
INFO:tensorflow:global_step/sec: 0.165121
INFO:tensorflow:loss = 0.024811966, step = 31203
INFO:tensorflow:global_step/sec: 0.163263
INFO:tensorflow:loss = 0.03503992, step = 31303
INFO:tensorflow:global_step/sec: 0.162651
INFO:tensorflow:loss = 0.04030867, step = 31403
INFO:tensorflow:global_step/sec: 0.163763
INFO:tensorflow:loss = 0.024233788, step = 31503
INFO:tensorflow:global_step/sec: 0.162543
INFO:tensorflow:loss = 0.02480667, step = 31603
INFO:tensorflow:global_step/sec: 0.163321
INFO:tensorflow:loss = 0.016836576, step = 31703
INFO:tensorflow:global_step/sec: 0.162899
INFO:tensorflow:loss = 0.025392212, step = 31803
INFO:tensorflow:global_step/sec: 0.167928
INFO:tensorflow:loss = 0.027598295, step = 31903
INFO:tensorflow:Saving checkpoints for 32002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.0180697.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-13:22:05
INFO:tensorflow:Finished evaluation at 2019-05-08-13:22:53
INFO:tensorflow:Saving dict for global step 32002: bleu = 78.18, global_step = 32002, log_perplexity = 0.73627776, loss = 0.68896097
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 32003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.017262692, step = 32003
INFO:tensorflow:Prediction followed by Target @ Step 32004
====================================================================================================
private void METHOD_1 ( Guid VAR_1 , Guid vmId , Set < TYPE_1 > VAR_2 ) { for ( TYPE_1 VAR_3 : VAR_2 ) { VAR_4 . METHOD_2 ( VAR_3 . getId ( ) , vmId ) ; } } SEQUENCE_END
private void METHOD_1 ( Guid VAR_1 , Guid vmId , Set < TYPE_1 > VAR_2 ) { for ( TYPE_1 VAR_3 : VAR_2 ) { VAR_4 . METHOD_2 ( VAR_3 . getId ( ) , vmId ) ; } } SEQUENCE_END

public void METHOD_1 ( ) { super . METHOD_1 ( ) ; METHOD_3 ( ) ; METHOD_4 ( ) ; doReturn ( VAR_1 ) . METHOD_5 ( VAR_2 ) . METHOD_6 ( ) ; } SEQUENCE_END
public void METHOD_1 ( ) { super . METHOD_1 ( ) ; METHOD_3 ( ) ; METHOD_4 ( ) ; doReturn ( VAR_1 ) . METHOD_5 ( VAR_2 ) . METHOD_6 ( ) ; } SEQUENCE_END

private TYPE_1 METHOD_1 ( ) { final String VAR_2 = String.format ( STRING_1 , METHOD_2 ( ) , METHOD_3 ( ) . VAR_3 ) ; return METHOD_4 ( VAR_2 ) ; return SEQUENCE_END
private TYPE_1 METHOD_1 ( ) { final String VAR_2 = String.format ( STRING_1 , METHOD_2 ( ) , METHOD_3 ( ) . VAR_3 ) ; return METHOD_4 ( VAR_2 ) ; } SEQUENCE_END

protected METHOD_1 ( ) { TYPE_1 VAR_1 = TYPE_2 . METHOD_2 ( VAR_2 . METHOD_3 ( ) ) ; TYPE_1 VAR_3 = TYPE_2 . METHOD_2 ( VAR_4 . METHOD_3 ( ) ) ; return TYPE_3 . METHOD_4 ( VAR_1 , VAR_3 ) ; } SEQUENCE_END
boolean METHOD_1 ( ) { TYPE_1 VAR_1 = TYPE_2 . METHOD_2 ( VAR_2 . METHOD_3 ( ) ) ; TYPE_1 VAR_3 = TYPE_2 . METHOD_2 ( VAR_4 . METHOD_3 ( ) ) ; return TYPE_3 . METHOD_4 ( VAR_1 , VAR_3 ) ; } SEQUENCE_END

private TYPE_1 METHOD_1 ( TYPE_2 VAR_1 ) { TYPE_1 VAR_2 = METHOD_2 ( VAR_1 ) ; if ( VAR_2 == null ) { VAR_2 = new TYPE_1 ( VAR_1 , null , null , null ) ; } return VAR_2 ; } SEQUENCE_END
private TYPE_1 METHOD_1 ( TYPE_2 VAR_1 ) { TYPE_1 VAR_2 = METHOD_2 ( VAR_1 ) ; if ( VAR_2 == null ) { VAR_2 = new TYPE_1 ( VAR_1 , null , null , null ) ; } return VAR_2 ; } SEQUENCE_END

public TYPE_1 METHOD_1 ( TYPE_2 VAR_1 , TYPE_2 VAR_2 ) { TYPE_3 VAR_3 = new TYPE_3 ( VAR_1 ) ; VAR_3 . METHOD_2 ( VAR_4 ) ; return METHOD_3 ( VAR_5 , VAR_1 , VAR_3 , VAR_2 , false ) ; } SEQUENCE_END
public TYPE_1 METHOD_1 ( TYPE_2 VAR_1 , TYPE_2 VAR_2 ) { TYPE_3 VAR_3 = new TYPE_3 ( VAR_1 ) ; VAR_3 . METHOD_2 ( VAR_4 ) ; return METHOD_3 ( VAR_5 , VAR_1 , VAR_3 , VAR_2 , false ) ; } SEQUENCE_END

private void METHOD_1 ( TYPE_1 request , TYPE_2 response ) throws HyracksDataException { final String node = request . METHOD_2 ( STRING_1 ) ; VAR_1 . METHOD_3 ( ) . METHOD_4 ( node ) ; response . METHOD_5 ( VAR_2 ) ; } SEQUENCE_END
private void METHOD_1 ( TYPE_1 request , TYPE_2 response ) throws HyracksDataException { final String node = request . METHOD_2 ( STRING_1 ) ; VAR_1 . METHOD_3 ( ) . METHOD_4 ( node ) ; response . METHOD_5 ( VAR_2 ) ; } SEQUENCE_END

private void METHOD_1 ( ) { METHOD_2 ( ) . METHOD_4 ( ) . METHOD_5 ( this ) ; } SEQUENCE_END
private void METHOD_1 ( ) { METHOD_2 ( ) . METHOD_4 ( ) . METHOD_5 ( this ) ; } SEQUENCE_END

public void METHOD_1 ( ) { parameters . METHOD_2 ( TYPE_1 ) ; METHOD_3 ( VAR_1 . get ( TYPE_1 . getId ( ) ) ) . thenReturn ( null ) ; TYPE_2 . METHOD_5 ( command , VAR_2 ) ; } SEQUENCE_END
public void METHOD_1 ( ) { parameters . METHOD_2 ( TYPE_1 ) ; METHOD_3 ( VAR_1 . get ( TYPE_1 . getId ( ) ) ) . thenReturn ( null ) ; TYPE_2 . METHOD_5 ( command , VAR_2 ) ; } SEQUENCE_END

private void METHOD_1 ( Account . Id id ) { TYPE_1 VAR_1 = session . get ( id ; VAR_1 . METHOD_2 ( id , ; VAR_1 . METHOD_3 ( VAR_2 , true ) ; VAR_1 . METHOD_3 ( VAR_3 , true ) ; } SEQUENCE_END
private void METHOD_1 ( Account . Id id ) { TYPE_1 VAR_1 = session . get ( ) ; VAR_1 . METHOD_2 ( id ) ; VAR_1 . METHOD_3 ( VAR_2 , true ) ; VAR_1 . METHOD_3 ( VAR_3 , true ) ; } SEQUENCE_END

private TYPE_1 METHOD_1 ( final TYPE_2 VAR_1 ) throws TYPE_3 , TYPE_4 { return VAR_2 . create ( VAR_1 , db , repo , rw , VAR_3 , VAR_4 , METHOD_2 ( VAR_5 ) , VAR_7 ) ; } SEQUENCE_END
private TYPE_1 METHOD_1 ( final TYPE_2 VAR_1 ) throws TYPE_3 , TYPE_4 { return VAR_2 . create ( VAR_1 , db , repo , rw , VAR_3 , VAR_4 , METHOD_2 ( VAR_5 ) , VAR_7 ) ; } SEQUENCE_END

public String METHOD_1 ( ) { if ( VAR_1 != null ) return TYPE_1 ; if ( VAR_1 != null ) return TYPE_1 . METHOD_2 ( VAR_1 ) ; return STRING_1 + Integer . toString ( VAR_3 . getId ( ) . METHOD_3 ( ) ) + STRING_2 ;
public String METHOD_1 ( ) { if ( VAR_2 != null ) return VAR_2 ; if ( VAR_1 != null ) return TYPE_1 . METHOD_2 ( VAR_1 ) ; return STRING_1 + Integer . toString ( VAR_3 . getId ( ) . METHOD_3 ( ) ) + STRING_2 ;

private void METHOD_1 ( final TYPE_1 server ) { TYPE_2 . METHOD_2 ( ( ) - > { METHOD_4 ( server ) ; METHOD_5 ( server ) ; METHOD_6 ( server ) ; return null ; } ) ; } SEQUENCE_END
private void METHOD_1 ( final TYPE_1 server ) { TYPE_2 . METHOD_2 ( ( ) - > { METHOD_4 ( server ) ; METHOD_5 ( server ) ; METHOD_6 ( server ) ; return null ; } ) ; } SEQUENCE_END

private static VAR_1 VAR_2 ( VAR_3 < String , String > config , VAR_4 VAR_5 = VAR_6 ( ) ; VAR_9 ( VAR_5 , config , req ) ; VAR_10 ( VAR_5 , config ) ; return VAR_5 . create ( ) ; } SEQUENCE_END
private static VAR_1 VAR_2 ( VAR_3 < String , String > config , VAR_4 VAR_5 = VAR_6 ( ) ; VAR_9 ( VAR_5 , config , req ) ; VAR_10 ( VAR_5 , config ) ; return VAR_5 . create ( ) ; } SEQUENCE_END

public void METHOD_1 ( Account . Id accountId ) throws IOException { if ( accountId != null ) { VAR_1 . METHOD_2 ( accountId ) ; index ( accountId ) ; } } SEQUENCE_END
public void METHOD_1 ( Account . Id accountId ) throws IOException { if ( accountId != null ) { VAR_1 . METHOD_2 ( accountId ) ; index ( accountId ) ; } } SEQUENCE_END

public static TYPE_1 METHOD_1 ( TYPE_2 VAR_1 , TYPE_3 VAR_2 , TYPE_4 . Type < , TYPE_5 factory , TYPE_6 < TYPE_7 > VAR_3 , TYPE_6 < TYPE_8 > VAR_4 ) { return new TYPE_1 ( VAR_1 , VAR_2 , type , factory , VAR_3 , VAR_4 ) ;
public static TYPE_1 METHOD_1 ( TYPE_2 VAR_1 , TYPE_3 VAR_2 , TYPE_4 . Type type , TYPE_5 factory , TYPE_6 < TYPE_7 > VAR_3 , TYPE_6 < TYPE_8 > VAR_4 ) { return new TYPE_1 ( VAR_1 , VAR_2 , type , factory , VAR_3 , VAR_4 ) ;

public static void VAR_1 ( VAR_2 ( new VAR_3 ( VAR_4 , VAR_5 ) ) ; VAR_6 ( new VAR_7 ( ) { public void VAR_8 ( View v ) { VAR_9 ( VAR_10 , VAR_5 ) ; } } ) ; } SEQUENCE_END
public static void VAR_1 ( VAR_2 ( new VAR_3 ( VAR_4 , VAR_5 ) ) ; VAR_6 ( new VAR_7 ( ) { public void VAR_8 ( View v ) { VAR_9 ( VAR_10 , VAR_5 ) ; } } ) ; } SEQUENCE_END

protected boolean METHOD_1 ( ) { if ( getParameters ( ) . METHOD_2 ( ) == VAR_1 ) { return true ; } return validate ( TYPE_1 . METHOD_1 ( getParameters ( ) . METHOD_3 ( ) , getVm ( ) . METHOD_5 ( ) ) ) ; }
protected boolean METHOD_1 ( ) { if ( getParameters ( ) . METHOD_2 ( ) == VAR_1 ) { return true ; } return validate ( TYPE_1 . METHOD_1 ( getParameters ( ) . METHOD_3 ( ) , getVm ( ) . METHOD_5 ( ) ) ) ; }

public boolean METHOD_1 ( ) { TYPE_1 VAR_1 = ( TYPE_1 ) mContext . METHOD_2 ( VAR_2 ) ; if ( ! VAR_1 . METHOD_3 ( VAR_3 ) ) { VAR_4 . METHOD_4 ( true , true ) ; return true ; } return false ; } SEQUENCE_END
public boolean METHOD_1 ( ) { TYPE_1 VAR_1 = ( TYPE_1 ) mContext . METHOD_2 ( VAR_2 ) ; if ( ! VAR_1 . METHOD_3 ( VAR_3 ) ) { VAR_4 . METHOD_4 ( true , null ) ; return true ; } return false ; } SEQUENCE_END

public void METHOD_1 ( Analyzer analyzer ) { for ( TYPE_1 VAR_1 : VAR_2 ) { if ( VAR_1 . METHOD_2 ( STRING_1 ) ) { analyzer . METHOD_4 ( ) ; } else { analyzer . METHOD_3 ( STRING_2 + VAR_1 ) ; } } } SEQUENCE_END
public void METHOD_1 ( Analyzer analyzer ) { for ( TYPE_1 VAR_1 : VAR_2 ) { if ( VAR_1 . METHOD_2 ( STRING_1 ) ) { analyzer . METHOD_4 ( ) ; } else { analyzer . METHOD_3 ( STRING_2 + VAR_1 ) ; } } } SEQUENCE_END

public TYPE_1 create ( TYPE_2 VAR_1 , TYPE_3 < TYPE_4 > VAR_2 , Account . Id id ) { return new TYPE_1 ( VAR_1 , VAR_3 , VAR_4 , VAR_5 , VAR_6 , VAR_7 , VAR_2 , null , id ) ; } SEQUENCE_END
public TYPE_1 create ( TYPE_2 VAR_1 , TYPE_3 < TYPE_4 > VAR_2 , Account . Id id ) { return new TYPE_1 ( VAR_1 , VAR_3 , VAR_4 , VAR_5 , VAR_6 , VAR_7 , VAR_2 , null , id ) ; } SEQUENCE_END

public static String METHOD_1 ( Context context , String name ) { int id = getId ( context , VAR_1 , name ) ; if ( id == 0 ) { return null ; } return METHOD_2 ( context ) . METHOD_1 ( id ) ; } SEQUENCE_END
public static String METHOD_1 ( Context context , String name ) { int id = getId ( context , VAR_1 , name ) ; if ( id == 0 ) { return null ; } return METHOD_2 ( context ) . METHOD_1 ( id ) ; } SEQUENCE_END

public static TYPE_1 METHOD_1 ( TYPE_1 response ) { if ( response . getEntity ( ) instanceof TYPE_2 ) { TYPE_2 vm = ( TYPE_2 ) response . getEntity ( ) ; METHOD_2 ( vm ) ; } return response ; } SEQUENCE_END
public static TYPE_1 METHOD_1 ( TYPE_1 response ) { if ( response . getEntity ( ) instanceof TYPE_2 ) { TYPE_2 vm = ( TYPE_2 ) response . getEntity ( ) ; METHOD_2 ( vm ) ; } return response ; } SEQUENCE_END

public TYPE_1 commit ( TYPE_2 action ) { TYPE_3 VAR_1 = new TYPE_3 ( VAR_2 , VAR_3 ) ; TYPE_1 response = METHOD_2 ( VAR_4 , VAR_1 , action ) ; return response ; } SEQUENCE_END
public TYPE_1 commit ( TYPE_2 action ) { TYPE_3 VAR_1 = new TYPE_3 ( VAR_2 , VAR_3 ) ; TYPE_1 response = METHOD_2 ( VAR_4 , VAR_1 , action ) ; return response ; } SEQUENCE_END

public void METHOD_1 ( TYPE_1 VAR_1 ) throws RestApiException { try { VAR_2 . apply ( change , VAR_1 ) ; } catch ( OrmException | TYPE_2 e ) { throw new RestApiException ( STRING_1 , e ) ; } } SEQUENCE_END
public void METHOD_1 ( TYPE_1 VAR_1 ) throws RestApiException { try { VAR_2 . apply ( change , VAR_1 ) ; } catch ( OrmException | TYPE_2 e ) { throw new RestApiException ( STRING_1 , e ) ; } } SEQUENCE_END

public void METHOD_1 ( ) throws Exception { METHOD_2 ( VAR_1 , TYPE_1 . of ( STRING_1 , Lists.newArrayList ( VAR_2 , VAR_3 , VAR_4 ) ) ) ; METHOD_2 ( VAR_5 , VAR_6 < String , List < TYPE_2 > > METHOD_3 ( ) ) ; } SEQUENCE_END
public void METHOD_1 ( ) throws Exception { METHOD_2 ( VAR_1 , TYPE_1 . of ( STRING_1 , Lists.newArrayList ( VAR_2 , VAR_3 , VAR_4 ) ) ) ; METHOD_2 ( VAR_5 , VAR_6 < String , List < TYPE_2 > > METHOD_3 ( ) ) ; } SEQUENCE_END

public String METHOD_1 ( ) { if ( VAR_1 != null ) { final String VAR_2 = VAR_1 . getMessage ( ) ; if ( VAR_2 != null ) { return VAR_2 . METHOD_2 ( ) ; } } return STRING_1 ; } SEQUENCE_END
public String METHOD_1 ( ) { if ( VAR_1 != null ) { final String VAR_2 = VAR_1 . getMessage ( ) ; if ( VAR_2 != null ) { return VAR_2 . METHOD_2 ( ) ; } } return STRING_1 ; } SEQUENCE_END

public boolean METHOD_1 ( TYPE_1 VAR_1 ) throws IOException { VAR_2 . METHOD_2 ( new TYPE_2 ( VAR_1 ) ) ; return true ; } SEQUENCE_END
public boolean METHOD_1 ( TYPE_1 VAR_1 ) throws IOException { VAR_2 . METHOD_2 ( new TYPE_2 ( VAR_1 ) ) ; return true ; } SEQUENCE_END

private void METHOD_1 ( ) { VAR_1 = new TYPE_1 < TYPE_2 > ( false , true ) ; VAR_1 . METHOD_2 ( ) ; VAR_2 = new TYPE_3 ( ) ; VAR_5 = new TYPE_4 ( 4 ) ; } SEQUENCE_END
private void METHOD_1 ( ) { VAR_1 = new TYPE_1 < TYPE_2 > ( false , true ) ; VAR_1 . METHOD_2 ( ) ; VAR_2 = new TYPE_3 ( ) ; VAR_5 = new TYPE_4 ( 4 ) ; } SEQUENCE_END

public List < ChangeInfo > METHOD_1 ( ) throws RestApiException { try { return ( List < ChangeInfo > ) VAR_1 . apply ( change ) ; } catch ( IOException | OrmException e ) { throw new RestApiException ( STRING_1 , e ) ; } } SEQUENCE_END
public List < ChangeInfo > METHOD_1 ( ) throws RestApiException { try { return ( List < ChangeInfo > ) VAR_1 . apply ( change ) ; } catch ( IOException | OrmException e ) { throw new RestApiException ( STRING_1 , e ) ; } } SEQUENCE_END

private void METHOD_1 ( TYPE_1 VAR_1 , int VAR_2 , int VAR_3 ) { if ( ! VAR_4 . isEmpty ( ) ) { VAR_5 . METHOD_2 ( VAR_4 ) ; VAR_5 . METHOD_3 ( VAR_2 , VAR_3 ) ; VAR_5 . METHOD_4 ( ) ; } } SEQUENCE_END
private void METHOD_1 ( TYPE_1 VAR_1 , int VAR_2 , int VAR_3 ) { if ( ! VAR_4 . isEmpty ( ) ) { VAR_5 . METHOD_2 ( VAR_4 ) ; VAR_5 . METHOD_3 ( VAR_2 , VAR_3 ) ; VAR_5 . METHOD_4 ( ) ; } } SEQUENCE_END

private void METHOD_1 ( ) { TYPE_1 params = new TYPE_1 ( METHOD_2 ( ) . getId ( ) ) ; Frontend . METHOD_3 ( ) . METHOD_4 ( VAR_1 , params ) ; } SEQUENCE_END
private void METHOD_1 ( ) { TYPE_1 params = new TYPE_1 ( METHOD_2 ( ) . getId ( ) ) ; Frontend . METHOD_3 ( ) . METHOD_4 ( VAR_1 , params ) ; } SEQUENCE_END

====================================================================================================


INFO:tensorflow:global_step/sec: 0.14625
INFO:tensorflow:loss = 0.01921464, step = 32103
INFO:tensorflow:global_step/sec: 0.165083
INFO:tensorflow:loss = 0.01961067, step = 32203
INFO:tensorflow:global_step/sec: 0.165088
INFO:tensorflow:loss = 0.027166521, step = 32303
INFO:tensorflow:global_step/sec: 0.16508
INFO:tensorflow:loss = 0.024506455, step = 32403
INFO:tensorflow:global_step/sec: 0.162337
INFO:tensorflow:loss = 0.01882568, step = 32503
INFO:tensorflow:global_step/sec: 0.162762
INFO:tensorflow:loss = 0.015250114, step = 32603
INFO:tensorflow:global_step/sec: 0.163261
INFO:tensorflow:loss = 0.02878737, step = 32703
INFO:tensorflow:global_step/sec: 0.164055
INFO:tensorflow:loss = 0.025040762, step = 32803
INFO:tensorflow:global_step/sec: 0.16256
INFO:tensorflow:loss = 0.014583635, step = 32903
INFO:tensorflow:Saving checkpoints for 33002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.012114536.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-15:05:49
INFO:tensorflow:Finished evaluation at 2019-05-08-15:06:37
INFO:tensorflow:Saving dict for global step 33002: bleu = 78.46, global_step = 33002, log_perplexity = 0.7369554, loss = 0.689058
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 33003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.022900332, step = 33003
INFO:tensorflow:global_step/sec: 0.145793
INFO:tensorflow:loss = 0.022689218, step = 33103
INFO:tensorflow:global_step/sec: 0.166965
INFO:tensorflow:loss = 0.018662382, step = 33203
INFO:tensorflow:global_step/sec: 0.163843
INFO:tensorflow:loss = 0.018914975, step = 33303
INFO:tensorflow:global_step/sec: 0.162305
INFO:tensorflow:loss = 0.012073123, step = 33403
INFO:tensorflow:global_step/sec: 0.166212
INFO:tensorflow:loss = 0.022351364, step = 33503
INFO:tensorflow:global_step/sec: 0.165148
INFO:tensorflow:loss = 0.020312317, step = 33603
INFO:tensorflow:global_step/sec: 0.160498
INFO:tensorflow:loss = 0.021362917, step = 33703
INFO:tensorflow:global_step/sec: 0.164297
INFO:tensorflow:loss = 0.019745726, step = 33803
INFO:tensorflow:global_step/sec: 0.16678
INFO:tensorflow:loss = 0.014741946, step = 33903
INFO:tensorflow:Saving checkpoints for 34002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.02189317.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-16:49:21
INFO:tensorflow:Finished evaluation at 2019-05-08-16:50:08
INFO:tensorflow:Saving dict for global step 34002: bleu = 77.91, global_step = 34002, log_perplexity = 0.7592754, loss = 0.7103518
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 34003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.02945316, step = 34003
INFO:tensorflow:global_step/sec: 0.147405
INFO:tensorflow:loss = 0.018309595, step = 34103
INFO:tensorflow:global_step/sec: 0.16236
INFO:tensorflow:loss = 0.025919689, step = 34203
INFO:tensorflow:global_step/sec: 0.162075
INFO:tensorflow:loss = 0.019597795, step = 34303
INFO:tensorflow:global_step/sec: 0.165916
INFO:tensorflow:loss = 0.018346682, step = 34403
INFO:tensorflow:global_step/sec: 0.16488
INFO:tensorflow:loss = 0.014200332, step = 34503
INFO:tensorflow:global_step/sec: 0.163117
INFO:tensorflow:loss = 0.015532721, step = 34603
INFO:tensorflow:global_step/sec: 0.162265
INFO:tensorflow:loss = 0.020003723, step = 34703
INFO:tensorflow:global_step/sec: 0.161314
INFO:tensorflow:loss = 0.027971357, step = 34803
INFO:tensorflow:global_step/sec: 0.165338
INFO:tensorflow:loss = 0.01864266, step = 34903
INFO:tensorflow:Saving checkpoints for 35002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.019841675.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-18:33:32
INFO:tensorflow:Finished evaluation at 2019-05-08-18:34:19
INFO:tensorflow:Saving dict for global step 35002: bleu = 76.99, global_step = 35002, log_perplexity = 0.7978134, loss = 0.7458477
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 35003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.030199086, step = 35003
INFO:tensorflow:global_step/sec: 0.146947
INFO:tensorflow:loss = 0.017971866, step = 35103
INFO:tensorflow:global_step/sec: 0.166739
INFO:tensorflow:loss = 0.008698007, step = 35203
INFO:tensorflow:global_step/sec: 0.162236
INFO:tensorflow:loss = 0.020717932, step = 35303
INFO:tensorflow:global_step/sec: 0.161762
INFO:tensorflow:loss = 0.0074495925, step = 35403
INFO:tensorflow:global_step/sec: 0.164494
INFO:tensorflow:loss = 0.0159036, step = 35503
INFO:tensorflow:global_step/sec: 0.164344
INFO:tensorflow:loss = 0.022253228, step = 35603
INFO:tensorflow:global_step/sec: 0.162745
INFO:tensorflow:loss = 0.018359344, step = 35703
INFO:tensorflow:global_step/sec: 0.162795
INFO:tensorflow:loss = 0.014416561, step = 35803
INFO:tensorflow:global_step/sec: 0.161528
INFO:tensorflow:loss = 0.018842801, step = 35903
INFO:tensorflow:Saving checkpoints for 36002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.0139587885.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-20:17:40
INFO:tensorflow:Finished evaluation at 2019-05-08-20:18:28
INFO:tensorflow:Saving dict for global step 36002: bleu = 78.16, global_step = 36002, log_perplexity = 0.77089757, loss = 0.7213874
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 36003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.010064384, step = 36003
INFO:tensorflow:global_step/sec: 0.146224
INFO:tensorflow:loss = 0.017378666, step = 36103
INFO:tensorflow:global_step/sec: 0.163201
INFO:tensorflow:loss = 0.021859862, step = 36203
INFO:tensorflow:global_step/sec: 0.16496
INFO:tensorflow:loss = 0.007103369, step = 36303
INFO:tensorflow:global_step/sec: 0.16195
INFO:tensorflow:loss = 0.03502027, step = 36403
INFO:tensorflow:global_step/sec: 0.163848
INFO:tensorflow:loss = 0.016641041, step = 36503
INFO:tensorflow:global_step/sec: 0.163223
INFO:tensorflow:loss = 0.012449231, step = 36603
INFO:tensorflow:global_step/sec: 0.161182
INFO:tensorflow:loss = 0.024187889, step = 36703
INFO:tensorflow:global_step/sec: 0.163424
INFO:tensorflow:loss = 0.0056853066, step = 36803
INFO:tensorflow:global_step/sec: 0.164448
INFO:tensorflow:loss = 0.015216476, step = 36903
INFO:tensorflow:Saving checkpoints for 37002 into ../model/dataset/model.ckpt.
INFO:tensorflow:Loss for final step: 0.00644221.
INFO:tensorflow:Evaluating model now.
INFO:tensorflow:Creating AttentionSeq2Seq in mode=eval
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=eval
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=eval
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=eval
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=eval
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Starting evaluation at 2019-05-08-22:01:57
INFO:tensorflow:Finished evaluation at 2019-05-08-22:02:44
INFO:tensorflow:Saving dict for global step 37002: bleu = 78.89, global_step = 37002, log_perplexity = 0.78518033, loss = 0.7333023
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
INFO:tensorflow:Training model for 1000 steps
INFO:tensorflow:Creating AttentionSeq2Seq in mode=train
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=train
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=train
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=train
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=train
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Create CheckpointSaverHook.
4 ops no flops stats due to incomplete shapes. Consider passing run_meta to use run_time shapes.
INFO:tensorflow:b'_TFProfRoot (--/4.64m params)\n  model/att_seq2seq/Variable (0/0 params)\n  model/att_seq2seq/decode/attention/att_keys/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_keys/weights (512x512, 262.14k/262.14k params)\n  model/att_seq2seq/decode/attention/att_query/biases (512, 512/512 params)\n  model/att_seq2seq/decode/attention/att_query/weights (256x512, 131.07k/131.07k params)\n  model/att_seq2seq/decode/attention/v_att (512, 512/512 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/biases (256, 256/256 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/attention_mix/weights (768x256, 196.61k/196.61k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_0/lstm_cell/weights (1280x1024, 1.31m/1.31m params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/extended_multi_rnn_cell/cell_1/lstm_cell/weights (512x1024, 524.29k/524.29k params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/biases (495, 495/495 params)\n  model/att_seq2seq/decode/attention_decoder/decoder/logits/weights (256x495, 126.72k/126.72k params)\n  model/att_seq2seq/decode/target_embedding/W (495x512, 253.44k/253.44k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/biases (1024, 1.02k/1.02k params)\n  model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/fw/lstm_cell/weights (768x1024, 786.43k/786.43k params)\n  model/att_seq2seq/encode/source_embedding/W (497x512, 254.46k/254.46k params)\n'
INFO:tensorflow:Saving checkpoints for 37003 into ../model/dataset/model.ckpt.
INFO:tensorflow:loss = 0.007878013, step = 37003
./train_test.sh: line 68:   910 Killed                  python3 -m bin.train --config_paths="
      ./configs/$CONFIG.yml,
      ./example_configs/train_seq2seq_optimized.yml,
      ./example_configs/text_metrics.yml" --model_params "
      vocab_source: $VOCAB_SOURCE
      vocab_target: $VOCAB_TARGET" --input_pipeline_train "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TRAIN_SOURCES
      target_files:
        - $TRAIN_TARGETS" --input_pipeline_dev "
    class: ParallelTextInputPipeline
    params:
       source_files:
        - $DEV_SOURCES
       target_files:
        - $DEV_TARGETS" --batch_size 32 --train_steps $TRAIN_STEPS --output_dir $MODEL_DIR --save_checkpoints_steps $CHECKPOINT_STEPS --eval_every_n_steps $EVAL_STEPS --keep_checkpoint_max $MAX_CHECKPOINT
--------------- TRAINING ENDED ---------------------
------------------- TESTING GREEDY ------------------------
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 0
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
------------------- BLEU ------------------------
buggy vs fixed
BLEU = 79.71, 86.4/82.1/77.8/73.2 (BP=1.000, ratio=1.104, hyp_len=14424, ref_len=13070)
prediction vs fixed
BLEU = 76.57, 87.7/80.6/73.7/67.2 (BP=0.995, ratio=0.995, hyp_len=13010, ref_len=13070)
------------------- CLASSIFICATION ------------------------
Test Set: 470
Predictions
Perf: 70 (14.89%)
Pot : 317
Bad : 83
------------------- TESTING BEAM SEARCH ------------------------
Beam width: 5
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 5
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_5/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
---------- TIME REPORT ----------
Beam width: 5
Total seconds: 135
Total bugs: 470
Total patches: 2350
Avg patch/sec: .057446
Avg bug/sec: .287234
---------------------------------
Test Set: 470
Predictions: 470
---------- PREDICTIONS REPORT ----------
Beam width: 5
Perf: 115 (24.46%)
Pot : 2085
Bad : 150
Avg_Pot : 4.4361702127659575
Min Bad : 0
--------------------------------------
Beam width: 10
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 10
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_10/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
---------- TIME REPORT ----------
Beam width: 10
Total seconds: 221
Total bugs: 470
Total patches: 4700
Avg patch/sec: .047021
Avg bug/sec: .470212
---------------------------------
Test Set: 470
Predictions: 470
---------- PREDICTIONS REPORT ----------
Beam width: 10
Perf: 126 (26.80%)
Pot : 4398
Bad : 176
Avg_Pot : 9.357446808510637
Min Bad : 0
--------------------------------------
Beam width: 15
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 15
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_15/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
---------- TIME REPORT ----------
Beam width: 15
Total seconds: 302
Total bugs: 470
Total patches: 7050
Avg patch/sec: .042836
Avg bug/sec: .642553
---------------------------------
Test Set: 470
Predictions: 470
---------- PREDICTIONS REPORT ----------
Beam width: 15
Perf: 136 (28.93%)
Pot : 6718
Bad : 196
Avg_Pot : 14.293617021276596
Min Bad : 0
--------------------------------------
Beam width: 20
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 20
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_20/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
---------- TIME REPORT ----------
Beam width: 20
Total seconds: 378
Total bugs: 470
Total patches: 9400
Avg patch/sec: .040212
Avg bug/sec: .804255
---------------------------------
Test Set: 470
Predictions: 470
---------- PREDICTIONS REPORT ----------
Beam width: 20
Perf: 141 (30.00%)
Pot : 9047
Bad : 212
Avg_Pot : 19.248936170212765
Min Bad : 0
--------------------------------------
Beam width: 25
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 25
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_25/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
./inference.sh: line 23:  3094 Killed                  python3 -m bin.infer --tasks "
    - class: DecodeText
    - class: DumpBeams
      params:
        file: ${PRED_DIR}/beams.npz" --model_dir $MODEL_DIR --model_params "
    inference.beam_search.beam_width: $beam_width" --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TEST_SOURCES" > ${PRED_DIR}/predictions.beam.txt
---------- TIME REPORT ----------
Beam width: 25
Total seconds: 16
Total bugs: 470
Total patches: 11750
Avg patch/sec: .001361
Avg bug/sec: .034042
---------------------------------
Test Set: 470
Traceback (most recent call last):
  File "predictions//beamPredictions.py", line 32, in <module>
    data = np.load((path + '/beams.npz'))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '../model/dataset//pred_25/beams.npz'
---------- PREDICTIONS REPORT ----------
Beam width: 25
Perf: 0 (0%)
Pot : 470
Bad : 0
Avg_Pot : 1.0
Min Bad : 0
--------------------------------------
Beam width: 30
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 30
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_30/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
./inference.sh: line 23:  3317 Killed                  python3 -m bin.infer --tasks "
    - class: DecodeText
    - class: DumpBeams
      params:
        file: ${PRED_DIR}/beams.npz" --model_dir $MODEL_DIR --model_params "
    inference.beam_search.beam_width: $beam_width" --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TEST_SOURCES" > ${PRED_DIR}/predictions.beam.txt
---------- TIME REPORT ----------
Beam width: 30
Total seconds: 27
Total bugs: 470
Total patches: 14100
Avg patch/sec: .001914
Avg bug/sec: .057446
---------------------------------
Test Set: 470
Traceback (most recent call last):
  File "predictions//beamPredictions.py", line 32, in <module>
    data = np.load((path + '/beams.npz'))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '../model/dataset//pred_30/beams.npz'
---------- PREDICTIONS REPORT ----------
Beam width: 30
Perf: 0 (0%)
Pot : 470
Bad : 0
Avg_Pot : 1.0
Min Bad : 0
--------------------------------------
Beam width: 35
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 35
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_35/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
./inference.sh: line 23:  3541 Killed                  python3 -m bin.infer --tasks "
    - class: DecodeText
    - class: DumpBeams
      params:
        file: ${PRED_DIR}/beams.npz" --model_dir $MODEL_DIR --model_params "
    inference.beam_search.beam_width: $beam_width" --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TEST_SOURCES" > ${PRED_DIR}/predictions.beam.txt
---------- TIME REPORT ----------
Beam width: 35
Total seconds: 10
Total bugs: 470
Total patches: 16450
Avg patch/sec: .000607
Avg bug/sec: .021276
---------------------------------
Test Set: 470
Traceback (most recent call last):
  File "predictions//beamPredictions.py", line 32, in <module>
    data = np.load((path + '/beams.npz'))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '../model/dataset//pred_35/beams.npz'
---------- PREDICTIONS REPORT ----------
Beam width: 35
Perf: 0 (0%)
Pot : 470
Bad : 0
Avg_Pot : 1.0
Min Bad : 0
--------------------------------------
Beam width: 40
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 40
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_40/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
./inference.sh: line 23:  3764 Killed                  python3 -m bin.infer --tasks "
    - class: DecodeText
    - class: DumpBeams
      params:
        file: ${PRED_DIR}/beams.npz" --model_dir $MODEL_DIR --model_params "
    inference.beam_search.beam_width: $beam_width" --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TEST_SOURCES" > ${PRED_DIR}/predictions.beam.txt
---------- TIME REPORT ----------
Beam width: 40
Total seconds: 9
Total bugs: 470
Total patches: 18800
Avg patch/sec: .000478
Avg bug/sec: .019148
---------------------------------
Test Set: 470
Traceback (most recent call last):
  File "predictions//beamPredictions.py", line 32, in <module>
    data = np.load((path + '/beams.npz'))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '../model/dataset//pred_40/beams.npz'
---------- PREDICTIONS REPORT ----------
Beam width: 40
Perf: 0 (0%)
Pot : 470
Bad : 0
Avg_Pot : 1.0
Min Bad : 0
--------------------------------------
Beam width: 45
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 45
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_45/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
./inference.sh: line 23:  3987 Killed                  python3 -m bin.infer --tasks "
    - class: DecodeText
    - class: DumpBeams
      params:
        file: ${PRED_DIR}/beams.npz" --model_dir $MODEL_DIR --model_params "
    inference.beam_search.beam_width: $beam_width" --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TEST_SOURCES" > ${PRED_DIR}/predictions.beam.txt
---------- TIME REPORT ----------
Beam width: 45
Total seconds: 27
Total bugs: 470
Total patches: 21150
Avg patch/sec: .001276
Avg bug/sec: .057446
---------------------------------
Test Set: 470
Traceback (most recent call last):
  File "predictions//beamPredictions.py", line 32, in <module>
    data = np.load((path + '/beams.npz'))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '../model/dataset//pred_45/beams.npz'
---------- PREDICTIONS REPORT ----------
Beam width: 45
Perf: 0 (0%)
Pot : 470
Bad : 0
Avg_Pot : 1.0
Min Bad : 0
--------------------------------------
Beam width: 50
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 50
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_50/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
./inference.sh: line 23:  4210 Killed                  python3 -m bin.infer --tasks "
    - class: DecodeText
    - class: DumpBeams
      params:
        file: ${PRED_DIR}/beams.npz" --model_dir $MODEL_DIR --model_params "
    inference.beam_search.beam_width: $beam_width" --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TEST_SOURCES" > ${PRED_DIR}/predictions.beam.txt
---------- TIME REPORT ----------
Beam width: 50
Total seconds: 8
Total bugs: 470
Total patches: 23500
Avg patch/sec: .000340
Avg bug/sec: .017021
---------------------------------
Test Set: 470
Traceback (most recent call last):
  File "predictions//beamPredictions.py", line 32, in <module>
    data = np.load((path + '/beams.npz'))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '../model/dataset//pred_50/beams.npz'
---------- PREDICTIONS REPORT ----------
Beam width: 50
Perf: 0 (0%)
Pot : 470
Bad : 0
Avg_Pot : 1.0
Min Bad : 0
--------------------------------------
Beam width: 100
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 100
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_100/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
./inference.sh: line 23:  4433 Killed                  python3 -m bin.infer --tasks "
    - class: DecodeText
    - class: DumpBeams
      params:
        file: ${PRED_DIR}/beams.npz" --model_dir $MODEL_DIR --model_params "
    inference.beam_search.beam_width: $beam_width" --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TEST_SOURCES" > ${PRED_DIR}/predictions.beam.txt
---------- TIME REPORT ----------
Beam width: 100
Total seconds: 36
Total bugs: 470
Total patches: 47000
Avg patch/sec: .000765
Avg bug/sec: .076595
---------------------------------
Test Set: 470
Traceback (most recent call last):
  File "predictions//beamPredictions.py", line 32, in <module>
    data = np.load((path + '/beams.npz'))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '../model/dataset//pred_100/beams.npz'
---------- PREDICTIONS REPORT ----------
Beam width: 100
Perf: 0 (0%)
Pot : 470
Bad : 0
Avg_Pot : 1.0
Min Bad : 0
--------------------------------------
Beam width: 200
INFO:tensorflow:Creating ParallelTextInputPipeline in mode=infer
INFO:tensorflow:
ParallelTextInputPipeline:
  num_epochs: 1
  shuffle: false
  source_delimiter: ' '
  source_files: [../dataset/dataset//test/buggy.txt]
  target_delimiter: ' '
  target_files: []

INFO:tensorflow:Creating AttentionSeq2Seq in mode=infer
INFO:tensorflow:
AttentionSeq2Seq:
  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau
  attention.params: {num_units: 512}
  bridge.class: seq2seq.models.bridges.ZeroBridge
  bridge.params: {}
  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 2
  embedding.dim: 512
  embedding.init_scale: 0.04
  embedding.share: false
  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params: {num_units: 256}
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1
  inference.beam_search.beam_width: 200
  inference.beam_search.choose_successors_fn: choose_top_k
  inference.beam_search.length_penalty_weight: 0.0
  optimizer.clip_embed_gradients: 0.1
  optimizer.clip_gradients: 5.0
  optimizer.learning_rate: 0.0001
  optimizer.lr_decay_rate: 0.99
  optimizer.lr_decay_steps: 100
  optimizer.lr_decay_type: ''
  optimizer.lr_min_learning_rate: 1.0e-12
  optimizer.lr_staircase: false
  optimizer.lr_start_decay_at: 0
  optimizer.lr_stop_decay_at: 2147483647
  optimizer.name: Adam
  optimizer.params: {epsilon: 8.0e-07}
  optimizer.sync_replicas: 0
  optimizer.sync_replicas_to_aggregate: 0
  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50
  vocab_source: ../dataset/dataset//train/vocab.buggy.txt
  vocab_target: ../dataset/dataset//train/vocab.fixed.txt

INFO:tensorflow:Creating DecodeText in mode=infer
INFO:tensorflow:
DecodeText: {delimiter: ' ', postproc_fn: '', unk_mapping: null, unk_replace: false}

INFO:tensorflow:Creating DumpBeams in mode=infer
INFO:tensorflow:
DumpBeams: {file: ../model/dataset//pred_200/beams.npz}

INFO:tensorflow:Setting batch size to 1 for beam search.
INFO:tensorflow:Creating vocabulary lookup table of size 497
INFO:tensorflow:Creating vocabulary lookup table of size 495
INFO:tensorflow:Creating BidirectionalRNNEncoder in mode=infer
INFO:tensorflow:
BidirectionalRNNEncoder:
  init_scale: 0.04
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 1
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating AttentionLayerBahdanau in mode=infer
INFO:tensorflow:
AttentionLayerBahdanau: {num_units: 512}

INFO:tensorflow:Creating AttentionDecoder in mode=infer
INFO:tensorflow:
AttentionDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 0.8
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating BeamSearchDecoder in mode=infer
INFO:tensorflow:
BeamSearchDecoder:
  init_scale: 0.04
  max_decode_length: 100
  rnn_cell:
    cell_class: LSTMCell
    cell_params: {num_units: 256}
    dropout_input_keep_prob: 1.0
    dropout_output_keep_prob: 1.0
    num_layers: 2
    residual_combiner: add
    residual_connections: false
    residual_dense: false

INFO:tensorflow:Creating ZeroBridge in mode=infer
INFO:tensorflow:
ZeroBridge: {}

INFO:tensorflow:Restored model from ../model/dataset/model.ckpt-37003
./inference.sh: line 23:  4658 Killed                  python3 -m bin.infer --tasks "
    - class: DecodeText
    - class: DumpBeams
      params:
        file: ${PRED_DIR}/beams.npz" --model_dir $MODEL_DIR --model_params "
    inference.beam_search.beam_width: $beam_width" --input_pipeline "
    class: ParallelTextInputPipeline
    params:
      source_files:
        - $TEST_SOURCES" > ${PRED_DIR}/predictions.beam.txt
---------- TIME REPORT ----------
Beam width: 200
Total seconds: 22
Total bugs: 470
Total patches: 94000
Avg patch/sec: .000234
Avg bug/sec: .046808
---------------------------------
Test Set: 470
Traceback (most recent call last):
  File "predictions//beamPredictions.py", line 32, in <module>
    data = np.load((path + '/beams.npz'))
  File "/home/ubuntu/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py", line 372, in load
    fid = open(file, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '../model/dataset//pred_200/beams.npz'
---------- PREDICTIONS REPORT ----------
Beam width: 200
Perf: 0 (0%)
Pot : 470
Bad : 0
Avg_Pot : 1.0
Min Bad : 0
--------------------------------------
