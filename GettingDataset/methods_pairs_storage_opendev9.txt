181
#method_before
@Override
@SuppressWarnings("unchecked")
public void run(ApiConfig config, Environment environment) throws Exception {
    /**
     * Wire services
     */
    Injector.registerModules(new MonApiModule(environment, config));
    /**
     * Configure resources
     */
    environment.jersey().register(Injector.getInstance(VersionResource.class));
    environment.jersey().register(Injector.getInstance(AlarmDefinitionResource.class));
    environment.jersey().register(Injector.getInstance(AlarmResource.class));
    environment.jersey().register(Injector.getInstance(MetricResource.class));
    environment.jersey().register(Injector.getInstance(MeasurementResource.class));
    environment.jersey().register(Injector.getInstance(StatisticResource.class));
    environment.jersey().register(Injector.getInstance(NotificationMethodResource.class));
    /**
     * Configure providers
     */
    removeExceptionMappers(environment.jersey().getResourceConfig().getSingletons());
    environment.jersey().register(new EntityExistsExceptionMapper());
    environment.jersey().register(new EntityNotFoundExceptionMapper());
    environment.jersey().register(new IllegalArgumentExceptionMapper());
    environment.jersey().register(new InvalidEntityExceptionMapper());
    environment.jersey().register(new JsonProcessingExceptionMapper());
    environment.jersey().register(new JsonMappingExceptionManager());
    environment.jersey().register(new ConstraintViolationExceptionMapper());
    environment.jersey().register(new ThrowableExceptionMapper<Throwable>() {
    });
    /**
     * Configure Jackson
     */
    environment.getObjectMapper().setPropertyNamingStrategy(PropertyNamingStrategy.CAMEL_CASE_TO_LOWER_CASE_WITH_UNDERSCORES);
    environment.getObjectMapper().enable(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY);
    environment.getObjectMapper().disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS);
    SimpleModule module = new SimpleModule("SerializationModule");
    module.addSerializer(new SubAlarmExpressionSerializer());
    environment.getObjectMapper().registerModule(module);
    /**
     * Configure CORS filter
     */
    Dynamic corsFilter = environment.servlets().addFilter("cors", CrossOriginFilter.class);
    corsFilter.addMappingForUrlPatterns(null, true, "/*");
    corsFilter.setInitParameter("allowedOrigins", "*");
    corsFilter.setInitParameter("allowedHeaders", "X-Requested-With,Content-Type,Accept,Origin,X-Auth-Token");
    corsFilter.setInitParameter("allowedMethods", "OPTIONS,GET,HEAD");
    if (config.middleware.enabled) {
        ensureHasValue(config.middleware.serverVIP, "serverVIP", "enabled", "true");
        ensureHasValue(config.middleware.serverPort, "serverPort", "enabled", "true");
        ensureHasValue(config.middleware.adminAuthMethod, "adminAuthMethod", "enabled", "true");
        if ("password".equalsIgnoreCase(config.middleware.adminAuthMethod)) {
            ensureHasValue(config.middleware.adminUser, "adminUser", "adminAuthMethod", "password");
            ensureHasValue(config.middleware.adminPassword, "adminPassword", "adminAuthMethod", "password");
        } else if ("token".equalsIgnoreCase(config.middleware.adminAuthMethod)) {
            ensureHasValue(config.middleware.adminToken, "adminToken", "adminAuthMethod", "token");
        } else {
            throw new Exception(String.format("Invalid value '%s' for adminAuthMethod. Must be either password or token", config.middleware.adminAuthMethod));
        }
        if (config.middleware.defaultAuthorizedRoles == null || config.middleware.defaultAuthorizedRoles.isEmpty()) {
            ensureHasValue(null, "defaultAuthorizedRoles", "enabled", "true");
        }
        if (config.middleware.connSSLClientAuth) {
            ensureHasValue(config.middleware.keystore, "keystore", "connSSLClientAuth", "true");
            ensureHasValue(config.middleware.keystorePassword, "keystorePassword", "connSSLClientAuth", "true");
        }
        Map<String, String> authInitParams = new HashMap<String, String>();
        authInitParams.put("ServerVIP", config.middleware.serverVIP);
        authInitParams.put("ServerPort", config.middleware.serverPort);
        authInitParams.put(AuthConstants.USE_HTTPS, String.valueOf(config.middleware.useHttps));
        authInitParams.put("ConnTimeout", config.middleware.connTimeout);
        authInitParams.put("ConnSSLClientAuth", String.valueOf(config.middleware.connSSLClientAuth));
        authInitParams.put("ConnPoolMaxActive", config.middleware.connPoolMaxActive);
        authInitParams.put("ConnPoolMaxIdle", config.middleware.connPoolMaxActive);
        authInitParams.put("ConnPoolEvictPeriod", config.middleware.connPoolEvictPeriod);
        authInitParams.put("ConnPoolMinIdleTime", config.middleware.connPoolMinIdleTime);
        authInitParams.put("ConnRetryTimes", config.middleware.connRetryTimes);
        authInitParams.put("ConnRetryInterval", config.middleware.connRetryInterval);
        authInitParams.put("AdminToken", config.middleware.adminToken);
        authInitParams.put("TimeToCacheToken", config.middleware.timeToCacheToken);
        authInitParams.put("AdminAuthMethod", config.middleware.adminAuthMethod);
        authInitParams.put("AdminUser", config.middleware.adminUser);
        authInitParams.put("AdminPassword", config.middleware.adminPassword);
        authInitParams.put("AdminProjectId", config.middleware.adminProjectId);
        authInitParams.put("AdminProjectName", config.middleware.adminProjectName);
        authInitParams.put("MaxTokenCacheSize", config.middleware.maxTokenCacheSize);
        setIfNotNull(authInitParams, AuthConstants.TRUSTSTORE, config.middleware.truststore);
        setIfNotNull(authInitParams, AuthConstants.TRUSTSTORE_PASS, config.middleware.truststorePassword);
        setIfNotNull(authInitParams, AuthConstants.KEYSTORE, config.middleware.keystore);
        setIfNotNull(authInitParams, AuthConstants.KEYSTORE_PASS, config.middleware.keystorePassword);
        /**
         * Configure auth filters
         */
        Dynamic preAuthenticationFilter = environment.servlets().addFilter("pre-auth", new PreAuthenticationFilter());
        preAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        preAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        Dynamic tokenAuthFilter = environment.servlets().addFilter("token-auth", new TokenAuth());
        tokenAuthFilter.addMappingForUrlPatterns(null, true, "/");
        tokenAuthFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        tokenAuthFilter.setInitParameters(authInitParams);
        Dynamic postAuthenticationFilter = environment.servlets().addFilter("post-auth", new PostAuthenticationFilter(config.middleware.defaultAuthorizedRoles, config.middleware.agentAuthorizedRoles));
        postAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        postAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        environment.jersey().getResourceConfig().getContainerRequestFilters().add(new RoleAuthorizationFilter());
    } else {
        Dynamic mockAuthenticationFilter = environment.servlets().addFilter("mock-auth", new MockAuthenticationFilter());
        mockAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        mockAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
    }
}
#method_after
@Override
@SuppressWarnings("unchecked")
public void run(ApiConfig config, Environment environment) throws Exception {
    /**
     * Wire services
     */
    Injector.registerModules(new MonApiModule(environment, config));
    /**
     * Configure resources
     */
    environment.jersey().register(Injector.getInstance(VersionResource.class));
    environment.jersey().register(Injector.getInstance(AlarmDefinitionResource.class));
    environment.jersey().register(Injector.getInstance(AlarmResource.class));
    environment.jersey().register(Injector.getInstance(MetricResource.class));
    environment.jersey().register(Injector.getInstance(MeasurementResource.class));
    environment.jersey().register(Injector.getInstance(StatisticResource.class));
    environment.jersey().register(Injector.getInstance(NotificationMethodResource.class));
    /**
     * Configure providers
     */
    removeExceptionMappers(environment.jersey().getResourceConfig().getSingletons());
    environment.jersey().register(new EntityExistsExceptionMapper());
    environment.jersey().register(new EntityNotFoundExceptionMapper());
    environment.jersey().register(new IllegalArgumentExceptionMapper());
    environment.jersey().register(new InvalidEntityExceptionMapper());
    environment.jersey().register(new JsonProcessingExceptionMapper());
    environment.jersey().register(new JsonMappingExceptionManager());
    environment.jersey().register(new ConstraintViolationExceptionMapper());
    environment.jersey().register(new ThrowableExceptionMapper<Throwable>() {
    });
    /**
     * Configure Jackson
     */
    environment.getObjectMapper().setPropertyNamingStrategy(PropertyNamingStrategy.CAMEL_CASE_TO_LOWER_CASE_WITH_UNDERSCORES);
    environment.getObjectMapper().enable(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY);
    environment.getObjectMapper().disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS);
    SimpleModule module = new SimpleModule("SerializationModule");
    module.addSerializer(new SubAlarmExpressionSerializer());
    environment.getObjectMapper().registerModule(module);
    /**
     * Configure CORS filter
     */
    Dynamic corsFilter = environment.servlets().addFilter("cors", CrossOriginFilter.class);
    corsFilter.addMappingForUrlPatterns(null, true, "/*");
    corsFilter.setInitParameter("allowedOrigins", "*");
    corsFilter.setInitParameter("allowedHeaders", "X-Requested-With,Content-Type,Accept,Origin,X-Auth-Token");
    corsFilter.setInitParameter("allowedMethods", "OPTIONS,GET,HEAD");
    if (config.middleware.enabled) {
        ensureHasValue(config.middleware.serverVIP, "serverVIP", "enabled", "true");
        ensureHasValue(config.middleware.serverPort, "serverPort", "enabled", "true");
        ensureHasValue(config.middleware.adminAuthMethod, "adminAuthMethod", "enabled", "true");
        if ("password".equalsIgnoreCase(config.middleware.adminAuthMethod)) {
            ensureHasValue(config.middleware.adminUser, "adminUser", "adminAuthMethod", "password");
            ensureHasValue(config.middleware.adminPassword, "adminPassword", "adminAuthMethod", "password");
        } else if ("token".equalsIgnoreCase(config.middleware.adminAuthMethod)) {
            ensureHasValue(config.middleware.adminToken, "adminToken", "adminAuthMethod", "token");
        } else {
            throw new Exception(String.format("Invalid value '%s' for adminAuthMethod. Must be either password or token", config.middleware.adminAuthMethod));
        }
        if (config.middleware.defaultAuthorizedRoles == null || config.middleware.defaultAuthorizedRoles.isEmpty()) {
            ensureHasValue(null, "defaultAuthorizedRoles", "enabled", "true");
        }
        if (config.middleware.connSSLClientAuth) {
            ensureHasValue(config.middleware.keystore, "keystore", "connSSLClientAuth", "true");
            ensureHasValue(config.middleware.keystorePassword, "keystorePassword", "connSSLClientAuth", "true");
        }
        Map<String, String> authInitParams = new HashMap<String, String>();
        authInitParams.put("ServerVIP", config.middleware.serverVIP);
        authInitParams.put("ServerPort", config.middleware.serverPort);
        authInitParams.put(AuthConstants.USE_HTTPS, String.valueOf(config.middleware.useHttps));
        authInitParams.put("ConnTimeout", config.middleware.connTimeout);
        authInitParams.put("ConnSSLClientAuth", String.valueOf(config.middleware.connSSLClientAuth));
        authInitParams.put("ConnPoolMaxActive", config.middleware.connPoolMaxActive);
        authInitParams.put("ConnPoolMaxIdle", config.middleware.connPoolMaxActive);
        authInitParams.put("ConnPoolEvictPeriod", config.middleware.connPoolEvictPeriod);
        authInitParams.put("ConnPoolMinIdleTime", config.middleware.connPoolMinIdleTime);
        authInitParams.put("ConnRetryTimes", config.middleware.connRetryTimes);
        authInitParams.put("ConnRetryInterval", config.middleware.connRetryInterval);
        authInitParams.put("AdminToken", config.middleware.adminToken);
        authInitParams.put("TimeToCacheToken", config.middleware.timeToCacheToken);
        authInitParams.put("AdminAuthMethod", config.middleware.adminAuthMethod);
        authInitParams.put("AdminUser", config.middleware.adminUser);
        authInitParams.put("AdminPassword", config.middleware.adminPassword);
        authInitParams.put(AuthConstants.ADMIN_PROJECT_ID, config.middleware.adminProjectId);
        authInitParams.put(AuthConstants.ADMIN_PROJECT_NAME, config.middleware.adminProjectName);
        authInitParams.put("MaxTokenCacheSize", config.middleware.maxTokenCacheSize);
        setIfNotNull(authInitParams, AuthConstants.TRUSTSTORE, config.middleware.truststore);
        setIfNotNull(authInitParams, AuthConstants.TRUSTSTORE_PASS, config.middleware.truststorePassword);
        setIfNotNull(authInitParams, AuthConstants.KEYSTORE, config.middleware.keystore);
        setIfNotNull(authInitParams, AuthConstants.KEYSTORE_PASS, config.middleware.keystorePassword);
        /**
         * Configure auth filters
         */
        Dynamic preAuthenticationFilter = environment.servlets().addFilter("pre-auth", new PreAuthenticationFilter());
        preAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        preAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        Dynamic tokenAuthFilter = environment.servlets().addFilter("token-auth", new TokenAuth());
        tokenAuthFilter.addMappingForUrlPatterns(null, true, "/");
        tokenAuthFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        tokenAuthFilter.setInitParameters(authInitParams);
        Dynamic postAuthenticationFilter = environment.servlets().addFilter("post-auth", new PostAuthenticationFilter(config.middleware.defaultAuthorizedRoles, config.middleware.agentAuthorizedRoles));
        postAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        postAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        environment.jersey().getResourceConfig().getContainerRequestFilters().add(new RoleAuthorizationFilter());
    } else {
        Dynamic mockAuthenticationFilter = environment.servlets().addFilter("mock-auth", new MockAuthenticationFilter());
        mockAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        mockAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
    }
}
#end_block

#method_before
@Override
public List<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, String limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    String serieNameRegex = buildSerieNameRegex(tenantId, config.region, name, dimensions);
    String timePart = InfluxV8Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String offsetPart = InfluxV8Utils.buildOffsetPart(offset);
    String query = String.format("select value " + "from /%1$s/ where 1 = 1 " + " %2$s  %3$s", serieNameRegex, timePart, offsetPart);
    logger.debug("Query string: {}", query);
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().startsWith(InfluxV8Utils.COULD_NOT_LOOK_UP_COLUMNS_EXC_MSG)) {
            return new LinkedList<>();
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    return buildMeasurementList(result);
}
#method_after
@Override
public List<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, int limit, Boolean mergeMetricsFlag) throws Exception {
    // mergeMetricsFlag is not implemented for Influxdb V8.
    // Limit is not implemented for Influxdb V8.
    String serieNameRegex = buildSerieNameRegex(tenantId, config.region, name, dimensions);
    String timePart = InfluxV8Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String offsetPart = InfluxV8Utils.buildOffsetPart(offset);
    String query = String.format("select * " + "from /%1$s/ where 1 = 1 " + " %2$s  %3$s", serieNameRegex, timePart, offsetPart);
    logger.debug("Query string: {}", query);
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().startsWith(InfluxV8Utils.COULD_NOT_LOOK_UP_COLUMNS_EXC_MSG)) {
            return new LinkedList<>();
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    return buildMeasurementList(result);
}
#end_block

#method_before
private List<Measurements> buildMeasurementList(List<Serie> result) throws Exception {
    List<Measurements> measurementsList = new LinkedList<>();
    for (Serie serie : result) {
        InfluxV8Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new InfluxV8Utils.SerieNameDecoder(serie.getName());
        } catch (InfluxV8Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Measurements measurements = new Measurements();
        measurements.setName(serieNameDecoder.getMetricName());
        measurements.setDimensions(serieNameDecoder.getDimensions());
        List<Object[]> valObjArryList = new LinkedList<>();
        for (Map<String, Object> row : serie.getRows()) {
            Object[] objArry = new Object[3];
            // sequence_number
            objArry[0] = ((Double) row.get(serie.getColumns()[1])).longValue();
            // time
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            // last id wins. ids should be in descending order.
            measurements.setId(String.valueOf(timeDouble.longValue()));
            objArry[1] = DATETIME_FORMATTER.print(timeDouble.longValue());
            // value
            objArry[2] = (Double) row.get(serie.getColumns()[2]);
            valObjArryList.add(objArry);
        }
        measurements.setMeasurements(valObjArryList);
        measurementsList.add(measurements);
    }
    return measurementsList;
}
#method_after
@SuppressWarnings("unchecked")
private List<Measurements> buildMeasurementList(List<Serie> result) throws Exception {
    List<Measurements> measurementsList = new LinkedList<>();
    for (Serie serie : result) {
        final InfluxV8Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new InfluxV8Utils.SerieNameDecoder(serie.getName());
        } catch (InfluxV8Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Measurements measurements = new Measurements();
        measurements.setName(serieNameDecoder.getMetricName());
        measurements.setDimensions(serieNameDecoder.getDimensions());
        List<Object[]> valObjArryList = new LinkedList<>();
        for (Map<String, Object> row : serie.getRows()) {
            Object[] objArry = new Object[4];
            // sequence_number
            objArry[0] = ((Double) row.get("sequence_number")).longValue();
            // time
            Double timeDouble = (Double) row.get("time");
            // last id wins. ids should be in descending order.
            measurements.setId(String.valueOf(timeDouble.longValue()));
            objArry[1] = DATETIME_FORMATTER.print(timeDouble.longValue());
            // value
            objArry[2] = (Double) row.get("value");
            final Map<String, String> valueMeta;
            final String valueMetaJsonStr = (String) row.get("value_meta");
            if (valueMetaJsonStr == null) {
                valueMeta = EMPTY_VALUE_META;
            } else {
                valueMeta = (Map<String, String>) OBJECT_MAPPER.readValue(valueMetaJsonStr, VALUE_META_TYPE);
            }
            objArry[3] = valueMeta;
            valObjArryList.add(objArry);
        }
        measurements.setMeasurements(valObjArryList);
        measurementsList.add(measurements);
    }
    return measurementsList;
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object getMetrics(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), metricRepo.find(tenantId, name, dimensions, offset, null), uriInfo);
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object getMetrics(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), metricRepo.find(tenantId, name, dimensions, offset, this.persistUtils.getLimit(limit)), uriInfo);
}
#end_block

#method_before
public void testExists() {
    assertTrue(repo.exists("bob", "90% CPU"));
    // Negative
    assertFalse(repo.exists("bob", "999% CPU"));
}
#method_after
public void testExists() {
    assertEquals(repo.exists("bob", "90% CPU"), "123");
    // Negative
    assertNull(repo.exists("bob", "999% CPU"));
}
#end_block

#method_before
public void shouldFind() {
    assertEquals(Arrays.asList(alarmDef_123, alarmDef_234), repo.find("bob", null, null, null, null));
    // Make sure it still finds AlarmDefinitions with no notifications
    handle.execute("delete from alarm_action");
    alarmDef_123.setAlarmActions(new ArrayList<String>(0));
    alarmDef_234.setAlarmActions(new ArrayList<String>(0));
    assertEquals(Arrays.asList(alarmDef_123, alarmDef_234), repo.find("bob", null, null, null, null));
    assertEquals(0, repo.find("bill", null, null, null, null).size());
}
#method_after
public void shouldFind() {
    assertEquals(Arrays.asList(alarmDef_123, alarmDef_234), repo.find("bob", null, null, null, 1));
    // Make sure it still finds AlarmDefinitions with no notifications
    handle.execute("delete from alarm_action");
    alarmDef_123.setAlarmActions(new ArrayList<String>(0));
    alarmDef_234.setAlarmActions(new ArrayList<String>(0));
    assertEquals(Arrays.asList(alarmDef_123, alarmDef_234), repo.find("bob", null, null, null, 1));
    assertEquals(0, repo.find("bill", null, null, null, 1).size());
}
#end_block

#method_before
public void shouldFindByDimension() {
    final Map<String, String> dimensions = new HashMap<>();
    dimensions.put("image_id", "888");
    assertEquals(Arrays.asList(alarmDef_123, alarmDef_234), repo.find("bob", null, dimensions, null, null));
    dimensions.clear();
    dimensions.put("device", "1");
    assertEquals(Arrays.asList(alarmDef_123), repo.find("bob", null, dimensions, null, null));
    dimensions.clear();
    dimensions.put("Not real", "AA");
    assertEquals(0, repo.find("bob", null, dimensions, null, null).size());
}
#method_after
public void shouldFindByDimension() {
    final Map<String, String> dimensions = new HashMap<>();
    dimensions.put("image_id", "888");
    assertEquals(Arrays.asList(alarmDef_123, alarmDef_234), repo.find("bob", null, dimensions, null, 1));
    dimensions.clear();
    dimensions.put("device", "1");
    assertEquals(Arrays.asList(alarmDef_123), repo.find("bob", null, dimensions, null, 1));
    dimensions.clear();
    dimensions.put("Not real", "AA");
    assertEquals(0, repo.find("bob", null, dimensions, null, 1).size());
}
#end_block

#method_before
public void shouldFindByName() {
    assertEquals(Arrays.asList(alarmDef_123), repo.find("bob", "90% CPU", null, null, null));
    assertEquals(0, repo.find("bob", "Does not exist", null, null, null).size());
}
#method_after
public void shouldFindByName() {
    assertEquals(Arrays.asList(alarmDef_123), repo.find("bob", "90% CPU", null, null, 1));
    assertEquals(0, repo.find("bob", "Does not exist", null, null, 1).size());
}
#end_block

#method_before
public void shouldDeleteById() {
    repo.deleteById("bob", "123");
    try {
        assertNull(repo.findById("bob", "123"));
        fail();
    } catch (EntityNotFoundException expected) {
    }
    assertEquals(Arrays.asList(alarmDef_234), repo.find("bob", null, null, null, null));
}
#method_after
public void shouldDeleteById() {
    repo.deleteById("bob", "123");
    try {
        assertNull(repo.findById("bob", "123"));
        fail();
    } catch (EntityNotFoundException expected) {
    }
    assertEquals(Arrays.asList(alarmDef_234), repo.find("bob", null, null, null, 1));
}
#end_block

#method_before
public int getLimit(String limit) {
    if (limit == null || limit.isEmpty()) {
        return this.maxQueryLimit;
    }
    int limitInt = Integer.parseInt(limit);
    if (limitInt <= this.maxQueryLimit) {
        return limitInt;
    } else {
        return this.maxQueryLimit;
    }
}
#method_after
public int getLimit(String limit) {
    if (limit == null || limit.isEmpty()) {
        return this.maxQueryLimit;
    }
    int limitInt;
    try {
        limitInt = Integer.parseInt(limit);
    } catch (NumberFormatException e) {
        throw new IllegalArgumentException(String.format("Found invalid Limit: '%1$s'. Limit must be an integer.", limit));
    }
    if (limitInt <= this.maxQueryLimit) {
        return limitInt;
    } else {
        return this.maxQueryLimit;
    }
}
#end_block

#method_before
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, List<String> statistics, int period, String offset, String limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    String serieNameRegex = buildSerieNameRegex(tenantId, config.region, name, dimensions);
    String statsPart = buildStatsPart(statistics);
    String timePart = InfluxV8Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String periodPart = buildPeriodPart(period);
    String query = String.format("select time %1$s from /%2$s/ where 1=1 %3$s %4$s", statsPart, serieNameRegex, timePart, periodPart);
    logger.debug("Query string: {}", query);
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().startsWith(InfluxV8Utils.COULD_NOT_LOOK_UP_COLUMNS_EXC_MSG)) {
            return new LinkedList<>();
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    return buildStatisticsList(statistics, result);
}
#method_after
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, List<String> statistics, int period, String offset, int limit, Boolean mergeMetricsFlag) throws Exception {
    // mergeMetricsFlag is not implemented for Influxdb V8.
    // Limit is not implemented for Influxdb V8.
    String serieNameRegex = buildSerieNameRegex(tenantId, config.region, name, dimensions);
    String statsPart = buildStatsPart(statistics);
    String timePart = InfluxV8Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String periodPart = buildPeriodPart(period);
    String query = String.format("select time %1$s from /%2$s/ where 1=1 %3$s %4$s", statsPart, serieNameRegex, timePart, periodPart);
    logger.debug("Query string: {}", query);
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().startsWith(InfluxV8Utils.COULD_NOT_LOOK_UP_COLUMNS_EXC_MSG)) {
            return new LinkedList<>();
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    return buildStatisticsList(statistics, result);
}
#end_block

#method_before
private List<Statistics> buildStatisticsList(List<String> statistics, List<Serie> result) throws Exception {
    List<Statistics> statisticsList = new LinkedList<Statistics>();
    for (Serie serie : result) {
        InfluxV8Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new InfluxV8Utils.SerieNameDecoder(serie.getName());
        } catch (InfluxV8Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Statistics statistic = new Statistics();
        statistic.setName(serieNameDecoder.getMetricName());
        List<String> colNamesList = new LinkedList<>(statistics);
        colNamesList.add(0, "timestamp");
        statistic.setColumns(colNamesList);
        statistic.setDimensions(serieNameDecoder.getDimensions());
        List<List<Object>> valObjArryArry = new LinkedList<List<Object>>();
        statistic.setStatistics(valObjArryArry);
        for (Map<String, Object> row : serie.getRows()) {
            List<Object> valObjArry = new ArrayList<>();
            // First column is always time.
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            valObjArry.add(DATETIME_FORMATTER.print(timeDouble.longValue()));
            for (int j = 1; j < statistics.size() + 1; j++) {
                valObjArry.add(row.get(serie.getColumns()[j]));
            }
            valObjArryArry.add(valObjArry);
        }
        statisticsList.add(statistic);
    }
    return statisticsList;
}
#method_after
private List<Statistics> buildStatisticsList(List<String> statistics, List<Serie> result) throws Exception {
    List<Statistics> statisticsList = new LinkedList<>();
    for (Serie serie : result) {
        InfluxV8Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new InfluxV8Utils.SerieNameDecoder(serie.getName());
        } catch (InfluxV8Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Statistics statistic = new Statistics();
        statistic.setName(serieNameDecoder.getMetricName());
        List<String> colNamesList = new LinkedList<>(statistics);
        colNamesList.add(0, "timestamp");
        statistic.setColumns(colNamesList);
        statistic.setDimensions(serieNameDecoder.getDimensions());
        List<List<Object>> valObjArryArry = new LinkedList<>();
        statistic.setStatistics(valObjArryArry);
        for (Map<String, Object> row : serie.getRows()) {
            List<Object> valObjArry = new ArrayList<>();
            // First column is always time.
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            valObjArry.add(DATETIME_FORMATTER.print(timeDouble.longValue()));
            for (int j = 1; j < statistics.size() + 1; j++) {
                valObjArry.add(row.get(serie.getColumns()[j]));
            }
            valObjArryArry.add(valObjArry);
        }
        statisticsList.add(statistic);
    }
    return statisticsList;
}
#end_block

#method_before
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, String limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    String serieNameRegex = buildSerieNameRegex(tenantId, config.region, name, dimensions);
    String query = String.format("list series /%1$s/", serieNameRegex);
    logger.debug("Query string: {}", query);
    List<Serie> result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.SECONDS);
    return buildMetricDefList(result, offset);
}
#method_after
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, int limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    String serieNameRegex = buildSerieNameRegex(tenantId, config.region, name, dimensions);
    String query = String.format("list series /%1$s/", serieNameRegex);
    logger.debug("Query string: {}", query);
    List<Serie> result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    return buildMetricDefList(result, offset);
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object get(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("start_time") String startTimeStr, @QueryParam("end_time") String endTimeStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    // Validate query parameters
    DateTime startTime = Validation.parseAndValidateDate(startTimeStr, "start_time", true);
    DateTime endTime = Validation.parseAndValidateDate(endTimeStr, "end_time", false);
    Validation.validateTimes(startTime, endTime);
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), repo.find(tenantId, name, dimensions, startTime, endTime, offset, limit), uriInfo);
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object get(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("start_time") String startTimeStr, @QueryParam("end_time") String endTimeStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit, @QueryParam("merge_metrics") Boolean mergeMetricsFlag) throws Exception {
    // Validate query parameters
    DateTime startTime = Validation.parseAndValidateDate(startTimeStr, "start_time", true);
    DateTime endTime = Validation.parseAndValidateDate(endTimeStr, "end_time", false);
    Validation.validateTimes(startTime, endTime);
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    return Links.paginateMeasurements(this.persistUtils.getLimit(limit), repo.find(tenantId, name, dimensions, startTime, endTime, offset, this.persistUtils.getLimit(limit), mergeMetricsFlag), uriInfo);
}
#end_block

#method_before
public void shouldFind() {
    List<NotificationMethod> nms = repo.find("444", null, null);
    assertEquals(nms, Arrays.asList(new NotificationMethod("123", "MyEmail", NotificationMethodType.EMAIL, "a@b")));
}
#method_after
public void shouldFind() {
    List<NotificationMethod> nms = repo.find("444", null, 1);
    assertEquals(nms, Arrays.asList(new NotificationMethod("123", "MyEmail", NotificationMethodType.EMAIL, "a@b")));
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
protected void setupResources() throws Exception {
    super.setupResources();
    expression = "avg(disk_read_ops{service=hpcs.compute, instance_id=937}) >= 90";
    List<String> matchBy = Arrays.asList("service", "instance_id");
    alarmItem = new AlarmDefinition("123", "Disk Exceeds 1k Operations", null, "LOW", expression, Arrays.asList("service", "instance_id"), true, null, null, null);
    alarmActions = new ArrayList<String>();
    alarmActions.add("29387234");
    alarmActions.add("77778687");
    alarm = new AlarmDefinition("123", "Disk Exceeds 1k Operations", null, "LOW", expression, matchBy, true, alarmActions, null, null);
    service = mock(AlarmDefinitionService.class);
    when(service.create(eq("abc"), eq("Disk Exceeds 1k Operations"), any(String.class), eq("LOW"), eq(expression), eq(AlarmExpression.of(expression)), eq(matchBy), any(List.class), any(List.class), any(List.class))).thenReturn(alarm);
    repo = mock(AlarmDefinitionRepo.class);
    when(repo.findById(eq("abc"), eq("123"))).thenReturn(alarm);
    when(repo.find(anyString(), anyString(), (Map<String, String>) anyMap(), anyString(), anyString())).thenReturn(Arrays.asList(alarmItem));
    addResources(new AlarmDefinitionResource(service, repo, new PersistUtils()));
}
#method_after
@Override
@SuppressWarnings("unchecked")
protected void setupResources() throws Exception {
    super.setupResources();
    expression = "avg(disk_read_ops{service=hpcs.compute, instance_id=937}) >= 90";
    List<String> matchBy = Arrays.asList("service", "instance_id");
    alarmItem = new AlarmDefinition("123", "Disk Exceeds 1k Operations", null, "LOW", expression, Arrays.asList("service", "instance_id"), true, null, null, null);
    alarmActions = new ArrayList<String>();
    alarmActions.add("29387234");
    alarmActions.add("77778687");
    alarm = new AlarmDefinition("123", "Disk Exceeds 1k Operations", null, "LOW", expression, matchBy, true, alarmActions, null, null);
    service = mock(AlarmDefinitionService.class);
    when(service.create(eq("abc"), eq("Disk Exceeds 1k Operations"), any(String.class), eq("LOW"), eq(expression), eq(AlarmExpression.of(expression)), eq(matchBy), any(List.class), any(List.class), any(List.class))).thenReturn(alarm);
    repo = mock(AlarmDefinitionRepo.class);
    when(repo.findById(eq("abc"), eq("123"))).thenReturn(alarm);
    when(repo.find(anyString(), anyString(), (Map<String, String>) anyMap(), anyString(), anyInt())).thenReturn(Arrays.asList(alarmItem));
    addResources(new AlarmDefinitionResource(service, repo, new PersistUtils()));
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldList() {
    Map lhm = (Map) client().resource("/v2.0/alarm-definitions").header("X-Tenant-Id", "abc").get(Paged.class).elements.get(0);
    AlarmDefinition ad = new AlarmDefinition((String) lhm.get("id"), (String) lhm.get("name"), (String) lhm.get("description"), (String) lhm.get("severity"), (String) lhm.get("expression"), (List<String>) lhm.get("match_by"), (boolean) lhm.get("actions_enabled"), (List<String>) lhm.get("alarm_actions"), (List<String>) lhm.get("ok_actions"), (List<String>) lhm.get("undetermined_actions"));
    List<Map<String, String>> links = (List<Map<String, String>>) lhm.get("links");
    List<Link> linksList = Arrays.asList(new Link(links.get(0).get("rel"), links.get(0).get("href")));
    ad.setLinks(linksList);
    List<AlarmDefinition> alarms = Arrays.asList(ad);
    assertEquals(alarms, Arrays.asList(alarmItem));
    verify(repo).find(eq("abc"), anyString(), (Map<String, String>) anyMap(), anyString(), anyString());
}
#method_after
@SuppressWarnings("unchecked")
public void shouldList() {
    Map lhm = (Map) client().resource("/v2.0/alarm-definitions").header("X-Tenant-Id", "abc").get(Paged.class).elements.get(0);
    AlarmDefinition ad = new AlarmDefinition((String) lhm.get("id"), (String) lhm.get("name"), (String) lhm.get("description"), (String) lhm.get("severity"), (String) lhm.get("expression"), (List<String>) lhm.get("match_by"), (boolean) lhm.get("actions_enabled"), (List<String>) lhm.get("alarm_actions"), (List<String>) lhm.get("ok_actions"), (List<String>) lhm.get("undetermined_actions"));
    List<Map<String, String>> links = (List<Map<String, String>>) lhm.get("links");
    List<Link> linksList = Arrays.asList(new Link(links.get(0).get("rel"), links.get(0).get("href")));
    ad.setLinks(linksList);
    List<AlarmDefinition> alarms = Arrays.asList(ad);
    assertEquals(alarms, Arrays.asList(alarmItem));
    verify(repo).find(eq("abc"), anyString(), (Map<String, String>) anyMap(), anyString(), anyInt());
}
#end_block

#method_before
// @SuppressWarnings("unchecked")
// public void shouldListByName() throws Exception {
// List<AlarmDefinition> alarms =
// client().resource("/v2.0/alarm-definitions?name=" + URLEncoder.encode("foo bar baz", "UTF-8"))
// .header("X-Tenant-Id", "abc").get(new GenericType<List<AlarmDefinition>>() {});
// 
// assertEquals(alarms, Arrays.asList(alarmItem));
// verify(repo).find(eq("abc"), eq("foo bar baz"), (Map<String, String>) anyMap(), anyString(), anyString());
// }
public void shouldGet() {
    assertEquals(client().resource("/v2.0/alarm-definitions/123").header("X-Tenant-Id", "abc").get(AlarmDefinition.class), alarm);
    verify(repo).findById(eq("abc"), eq("123"));
}
#method_after
public void shouldGet() {
    assertEquals(client().resource("/v2.0/alarm-definitions/123").header("X-Tenant-Id", "abc").get(AlarmDefinition.class), alarm);
    verify(repo).findById(eq("abc"), eq("123"));
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void should500OnInternalException() {
    doThrow(new RuntimeException("")).when(repo).find(anyString(), anyString(), (Map<String, String>) anyObject(), anyString(), anyString());
    try {
        client().resource("/v2.0/alarm-definitions").header("X-Tenant-Id", "abc").get(List.class);
        fail();
    } catch (Exception e) {
        assertTrue(e.getMessage().contains("500"));
    }
}
#method_after
@SuppressWarnings("unchecked")
public void should500OnInternalException() {
    doThrow(new RuntimeException("")).when(repo).find(anyString(), anyString(), (Map<String, String>) anyObject(), anyString(), anyInt());
    try {
        client().resource("/v2.0/alarm-definitions").header("X-Tenant-Id", "abc").get(List.class);
        fail();
    } catch (Exception e) {
        assertTrue(e.getMessage().contains("500"));
    }
}
#end_block

#method_before
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, DateTime endTime, List<String> statistics, int period, String offset, String limit) {
    // Todo. Use offset and limit for pagination.
    List<Statistics> listStats = new ArrayList<>();
    List<String> copyStatistics = createColumns(statistics);
    try (Handle h = db.open()) {
        Map<byte[], Statistics> byteMap = findDefIds(h, tenantId, name, dimensions, startTime, endTime);
        for (byte[] bufferId : byteMap.keySet()) {
            Query<Map<String, Object>> query = h.createQuery(createQuery(period, startTime, endTime, statistics)).bind("definition_id", bufferId).bind("start_time", startTime).bind("end_time", endTime);
            // Execute
            List<Map<String, Object>> rows = query.list();
            List<Object> statisticsRow = new ArrayList<Object>();
            for (Map<String, Object> row : rows) {
                Double sum = (Double) row.get("sum");
                Double average = (Double) row.get("avg");
                Double min = (Double) row.get("min");
                Double max = (Double) row.get("max");
                Long count = (Long) row.get("count");
                Timestamp time_stamp = (Timestamp) row.get("time_interval");
                if (time_stamp != null) {
                    statisticsRow.add(DATETIME_FORMATTER.print(time_stamp.getTime()));
                }
                if (average != null) {
                    statisticsRow.add(average);
                }
                if (count != null) {
                    statisticsRow.add(count);
                }
                if (max != null) {
                    statisticsRow.add(max);
                }
                if (min != null) {
                    statisticsRow.add(min);
                }
                if (sum != null) {
                    statisticsRow.add(sum);
                }
                byteMap.get(bufferId).addValues(statisticsRow);
                statisticsRow = new ArrayList<>();
            }
            byteMap.get(bufferId).setColumns(copyStatistics);
            listStats.add(byteMap.get(bufferId));
        }
    }
    return listStats;
}
#method_after
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, DateTime endTime, List<String> statistics, int period, String offset, int limit, Boolean mergeMetricsFlag) {
    // Todo. Use mergeMetricsFlag.
    // Todo. Use offset and limit for pagination.
    List<Statistics> listStats = new ArrayList<>();
    List<String> copyStatistics = createColumns(statistics);
    try (Handle h = db.open()) {
        Map<byte[], Statistics> byteMap = findDefIds(h, tenantId, name, dimensions, startTime, endTime);
        for (byte[] bufferId : byteMap.keySet()) {
            Query<Map<String, Object>> query = h.createQuery(createQuery(period, startTime, endTime, statistics)).bind("definition_id", bufferId).bind("start_time", startTime).bind("end_time", endTime);
            // Execute
            List<Map<String, Object>> rows = query.list();
            List<Object> statisticsRow = new ArrayList<Object>();
            for (Map<String, Object> row : rows) {
                Double sum = (Double) row.get("sum");
                Double average = (Double) row.get("avg");
                Double min = (Double) row.get("min");
                Double max = (Double) row.get("max");
                Long count = (Long) row.get("count");
                Timestamp time_stamp = (Timestamp) row.get("time_interval");
                if (time_stamp != null) {
                    statisticsRow.add(DATETIME_FORMATTER.print(time_stamp.getTime()));
                }
                if (average != null) {
                    statisticsRow.add(average);
                }
                if (count != null) {
                    statisticsRow.add(count);
                }
                if (max != null) {
                    statisticsRow.add(max);
                }
                if (min != null) {
                    statisticsRow.add(min);
                }
                if (sum != null) {
                    statisticsRow.add(sum);
                }
                byteMap.get(bufferId).addValues(statisticsRow);
                statisticsRow = new ArrayList<>();
            }
            byteMap.get(bufferId).setColumns(copyStatistics);
            listStats.add(byteMap.get(bufferId));
        }
    }
    return listStats;
}
#end_block

#method_before
@Override
protected void setupResources() throws Exception {
    super.setupResources();
    notificationMethod = new NotificationMethod("123", "Joe's Email", NotificationMethodType.EMAIL, "a@b");
    notificationMethodWebhook = new NotificationMethod("1234", "MyWh", NotificationMethodType.WEBHOOK, "http://localhost");
    notificationMethodPagerduty = new NotificationMethod("12345", "MyPd", NotificationMethodType.PAGERDUTY, "nzH2LVRdMzun11HNC2oD");
    repo = mock(NotificationMethodRepo.class);
    when(repo.create(eq("abc"), eq("MyEmail"), eq(NotificationMethodType.EMAIL), anyString())).thenReturn(notificationMethod);
    when(repo.create(eq("abc"), eq("MyWh"), eq(NotificationMethodType.WEBHOOK), anyString())).thenReturn(notificationMethodWebhook);
    when(repo.create(eq("abc"), eq("MyPd"), eq(NotificationMethodType.PAGERDUTY), anyString())).thenReturn(notificationMethodPagerduty);
    when(repo.findById(eq("abc"), eq("123"))).thenReturn(notificationMethod);
    when(repo.find(eq("abc"), anyString(), anyString())).thenReturn(Arrays.asList(notificationMethod));
    addResources(new NotificationMethodResource(repo, new PersistUtils()));
}
#method_after
@Override
protected void setupResources() throws Exception {
    super.setupResources();
    notificationMethod = new NotificationMethod("123", "Joe's Email", NotificationMethodType.EMAIL, "a@b");
    notificationMethodWebhook = new NotificationMethod("1234", "MyWh", NotificationMethodType.WEBHOOK, "http://localhost");
    notificationMethodPagerduty = new NotificationMethod("12345", "MyPd", NotificationMethodType.PAGERDUTY, "nzH2LVRdMzun11HNC2oD");
    repo = mock(NotificationMethodRepo.class);
    when(repo.create(eq("abc"), eq("MyEmail"), eq(NotificationMethodType.EMAIL), anyString())).thenReturn(notificationMethod);
    when(repo.create(eq("abc"), eq("MyWh"), eq(NotificationMethodType.WEBHOOK), anyString())).thenReturn(notificationMethodWebhook);
    when(repo.create(eq("abc"), eq("MyPd"), eq(NotificationMethodType.PAGERDUTY), anyString())).thenReturn(notificationMethodPagerduty);
    when(repo.findById(eq("abc"), eq("123"))).thenReturn(notificationMethod);
    when(repo.find(eq("abc"), anyString(), anyInt())).thenReturn(Arrays.asList(notificationMethod));
    addResources(new NotificationMethodResource(repo, new PersistUtils()));
}
#end_block

#method_before
// public void shouldList() {
// List<NotificationMethod> notificationMethods =
// client().resource("/v2.0/notification-methods").header("X-Tenant-Id", "abc")
// .get(new GenericType<List<NotificationMethod>>() {});
// 
// assertEquals(notificationMethods, Arrays.asList(notificationMethod));
// verify(repo).find(eq("abc"), anyString(), anyString());
// }
public void shouldGet() {
    assertEquals(client().resource("/v2.0/notification-methods/123").header("X-Tenant-Id", "abc").get(NotificationMethod.class), notificationMethod);
    verify(repo).findById(eq("abc"), eq("123"));
}
#method_after
public void shouldGet() {
    assertEquals(client().resource("/v2.0/notification-methods/123").header("X-Tenant-Id", "abc").get(NotificationMethod.class), notificationMethod);
    verify(repo).findById(eq("abc"), eq("123"));
}
#end_block

#method_before
public void should500OnInternalException() {
    doThrow(new RuntimeException("")).when(repo).find(anyString(), anyString(), anyString());
    try {
        client().resource("/v2.0/notification-methods").header("X-Tenant-Id", "abc").get(new GenericType<List<NotificationMethod>>() {
        });
        fail();
    } catch (Exception e) {
        assertTrue(e.getMessage().contains("500"));
    }
}
#method_after
public void should500OnInternalException() {
    doThrow(new RuntimeException("")).when(repo).find(anyString(), anyString(), anyInt());
    try {
        client().resource("/v2.0/notification-methods").header("X-Tenant-Id", "abc").get(new GenericType<List<NotificationMethod>>() {
        });
        fail();
    } catch (Exception e) {
        assertTrue(e.getMessage().contains("500"));
    }
}
#end_block

#method_before
@Override
public List<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, String limit) {
    try (Handle h = db.open()) {
        // Build sql
        StringBuilder sbWhere = new StringBuilder();
        if (name != null)
            sbWhere.append(" and def.name = :name");
        if (endTime != null)
            sbWhere.append(" and m.time_stamp <= :endTime");
        String sql = String.format(FIND_BY_METRIC_DEF_SQL, MetricQueries.buildJoinClauseFor(dimensions), sbWhere);
        // Build query
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId).bind("startTime", new Timestamp(startTime.getMillis()));
        if (name != null)
            query.bind("name", name);
        if (endTime != null)
            query.bind("endTime", new Timestamp(endTime.getMillis()));
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        // Execute query
        List<Map<String, Object>> rows = query.list();
        // Build results
        Map<ByteBuffer, Measurements> results = new LinkedHashMap<>();
        for (Map<String, Object> row : rows) {
            String metricName = (String) row.get("name");
            byte[] defIdBytes = (byte[]) row.get("definition_dimensions_id");
            byte[] dimSetIdBytes = (byte[]) row.get("dimension_set_id");
            ByteBuffer defId = ByteBuffer.wrap(defIdBytes);
            long measurementId = (Long) row.get("id");
            String timestamp = DATETIME_FORMATTER.print(((Timestamp) row.get("time_stamp")).getTime());
            double value = (double) row.get("value");
            Measurements measurements = results.get(defId);
            if (measurements == null) {
                measurements = new Measurements(metricName, MetricQueries.dimensionsFor(h, dimSetIdBytes), new ArrayList<Object[]>());
                results.put(defId, measurements);
            }
            measurements.addMeasurement(new Object[] { measurementId, timestamp, value });
        }
        return new ArrayList(results.values());
    }
}
#method_after
@Override
public List<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, int limit, Boolean mergeMetricsFlag) {
    try (Handle h = db.open()) {
        // Build sql
        StringBuilder sbWhere = new StringBuilder();
        if (name != null)
            sbWhere.append(" and def.name = :name");
        if (endTime != null)
            sbWhere.append(" and m.time_stamp <= :endTime");
        String sql = String.format(FIND_BY_METRIC_DEF_SQL, MetricQueries.buildJoinClauseFor(dimensions), sbWhere);
        // Build query
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId).bind("startTime", new Timestamp(startTime.getMillis()));
        if (name != null)
            query.bind("name", name);
        if (endTime != null)
            query.bind("endTime", new Timestamp(endTime.getMillis()));
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        // Execute query
        List<Map<String, Object>> rows = query.list();
        // Build results
        Map<ByteBuffer, Measurements> results = new LinkedHashMap<>();
        for (Map<String, Object> row : rows) {
            String metricName = (String) row.get("name");
            byte[] defIdBytes = (byte[]) row.get("definition_dimensions_id");
            byte[] dimSetIdBytes = (byte[]) row.get("dimension_set_id");
            ByteBuffer defId = ByteBuffer.wrap(defIdBytes);
            long measurementId = (Long) row.get("id");
            String timestamp = DATETIME_FORMATTER.print(((Timestamp) row.get("time_stamp")).getTime());
            double value = (double) row.get("value");
            Measurements measurements = results.get(defId);
            if (measurements == null) {
                measurements = new Measurements(metricName, MetricQueries.dimensionsFor(h, dimSetIdBytes), new ArrayList<Object[]>());
                results.put(defId, measurements);
            }
            // TODO - Really support valueMeta
            measurements.addMeasurement(new Object[] { measurementId, timestamp, value, new HashMap<String, String>() });
        }
        return new ArrayList<Measurements>(results.values());
    }
}
#end_block

#method_before
@GET
@Timed
@Path("/{alarm_id}/state-history")
@Produces(MediaType.APPLICATION_JSON)
public Object getStateHistory(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_id") String alarmId, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    return Links.paginate(this.persistUtils.getLimit(limit), stateHistoryRepo.findById(tenantId, alarmId, offset, limit), uriInfo);
}
#method_after
@GET
@Timed
@Path("/{alarm_id}/state-history")
@Produces(MediaType.APPLICATION_JSON)
public Object getStateHistory(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_id") String alarmId, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    return Links.paginate(this.persistUtils.getLimit(limit), stateHistoryRepo.findById(tenantId, alarmId, offset, this.persistUtils.getLimit(limit)), uriInfo);
}
#end_block

#method_before
@GET
@Timed
@Path("/state-history")
@Produces(MediaType.APPLICATION_JSON)
public Object listStateHistory(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("dimensions") String dimensionsStr, @QueryParam("start_time") String startTimeStr, @QueryParam("end_time") String endTimeStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    // Validate query parameters
    DateTime startTime = Validation.parseAndValidateDate(startTimeStr, "start_time", false);
    DateTime endTime = Validation.parseAndValidateDate(endTimeStr, "end_time", false);
    if (startTime != null)
        Validation.validateTimes(startTime, endTime);
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateDimensions(dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), stateHistoryRepo.find(tenantId, dimensions, startTime, endTime, offset, limit), uriInfo);
}
#method_after
@GET
@Timed
@Path("/state-history")
@Produces(MediaType.APPLICATION_JSON)
public Object listStateHistory(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("dimensions") String dimensionsStr, @QueryParam("start_time") String startTimeStr, @QueryParam("end_time") String endTimeStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    // Validate query parameters
    DateTime startTime = Validation.parseAndValidateDate(startTimeStr, "start_time", false);
    DateTime endTime = Validation.parseAndValidateDate(endTimeStr, "end_time", false);
    if (startTime != null)
        Validation.validateTimes(startTime, endTime);
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateDimensions(dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), stateHistoryRepo.find(tenantId, dimensions, startTime, endTime, offset, this.persistUtils.getLimit(limit)), uriInfo);
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object list(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("alarm_definition_id") String alarmDefId, @QueryParam("metric_name") String metricName, @QueryParam("metric_dimensions") String metricDimensionsStr, @QueryParam("state") AlarmState state, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    Map<String, String> metricDimensions = Strings.isNullOrEmpty(metricDimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(metricName, metricDimensionsStr);
    final List<Alarm> alarms = repo.find(tenantId, alarmDefId, metricName, metricDimensions, state, offset, limit, true);
    for (final Alarm alarm : alarms) {
        Links.hydrate(alarm.getAlarmDefinition(), uriInfo, AlarmDefinitionResource.ALARM_DEFINITIONS_PATH);
    }
    return Links.paginate(this.persistUtils.getLimit(limit), Links.hydrate(alarms, uriInfo), uriInfo);
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object list(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("alarm_definition_id") String alarmDefId, @QueryParam("metric_name") String metricName, @QueryParam("metric_dimensions") String metricDimensionsStr, @QueryParam("state") AlarmState state, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    Map<String, String> metricDimensions = Strings.isNullOrEmpty(metricDimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(metricName, metricDimensionsStr);
    final List<Alarm> alarms = repo.find(tenantId, alarmDefId, metricName, metricDimensions, state, offset, this.persistUtils.getLimit(limit), true);
    for (final Alarm alarm : alarms) {
        Links.hydrate(alarm.getAlarmDefinition(), uriInfo, AlarmDefinitionResource.ALARM_DEFINITIONS_PATH);
    }
    return Links.paginate(this.persistUtils.getLimit(limit), Links.hydrate(alarms, uriInfo), uriInfo);
}
#end_block

#method_before
@Test(groups = "database")
public void shouldFind() {
    checkList(repo.find("Not a tenant id", null, null, null, null, null, null, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, null, null, null, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, null, null, null, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", null, null, null, null, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("flavor_id", "222").build(), null, null, null, false), alarm1, alarm3);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").put("hostname", "roland").build(), null, null, null, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, null, null, AlarmState.UNDETERMINED, null, null, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), null, null, null, false), alarm1, alarm2);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", null, null, null, null, false), alarm1, alarm2, alarm3);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, AlarmState.UNDETERMINED, null, null, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, AlarmState.UNDETERMINED, null, null, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, false), alarm2);
}
#method_after
@Test(groups = "database")
public void shouldFind() {
    checkList(repo.find("Not a tenant id", null, null, null, null, null, 1, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("flavor_id", "222").build(), null, null, 1, false), alarm1, alarm3);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").put("hostname", "roland").build(), null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, null, null, AlarmState.UNDETERMINED, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), null, null, 1, false), alarm1, alarm2);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", null, null, null, 1, false), alarm1, alarm2, alarm3);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, AlarmState.UNDETERMINED, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, AlarmState.UNDETERMINED, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, 1, false), alarm2);
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object get(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("start_time") String startTimeStr, @QueryParam("end_time") String endTimeStr, @QueryParam("statistics") String statisticsStr, @DefaultValue("300") @QueryParam("period") String periodStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    // Validate query parameters
    DateTime startTime = Validation.parseAndValidateDate(startTimeStr, "start_time", true);
    DateTime endTime = Validation.parseAndValidateDate(endTimeStr, "end_time", false);
    Validation.validateTimes(startTime, endTime);
    Validation.validateNotNullOrEmpty(statisticsStr, "statistics");
    int period = Validation.parseAndValidateNumber(periodStr, "period");
    List<String> statistics = Validation.parseValidateAndNormalizeStatistics(COMMA_SPLITTER.split(statisticsStr));
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), repo.find(tenantId, name, dimensions, startTime, endTime, statistics, period, offset, limit), uriInfo);
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object get(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("start_time") String startTimeStr, @QueryParam("end_time") String endTimeStr, @QueryParam("statistics") String statisticsStr, @DefaultValue("300") @QueryParam("period") String periodStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit, @QueryParam("merge_metrics") Boolean mergeMetricsFlag) throws Exception {
    // Validate query parameters
    DateTime startTime = Validation.parseAndValidateDate(startTimeStr, "start_time", true);
    DateTime endTime = Validation.parseAndValidateDate(endTimeStr, "end_time", false);
    Validation.validateTimes(startTime, endTime);
    Validation.validateNotNullOrEmpty(statisticsStr, "statistics");
    int period = Validation.parseAndValidateNumber(periodStr, "period");
    List<String> statistics = Validation.parseValidateAndNormalizeStatistics(COMMA_SPLITTER.split(statisticsStr));
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    return Links.paginateStatistics(this.persistUtils.getLimit(limit), repo.find(tenantId, name, dimensions, startTime, endTime, statistics, period, offset, this.persistUtils.getLimit(limit), mergeMetricsFlag), uriInfo);
}
#end_block

#method_before
public void shouldFindWithoutDimensions() throws Exception {
    List<MetricDefinition> defs = repo.find("bob", "cpu_utilization", null, null, null);
    assertEquals(defs.size(), 3);
}
#method_after
public void shouldFindWithoutDimensions() throws Exception {
    List<MetricDefinition> defs = repo.find("bob", "cpu_utilization", null, null, 1);
    assertEquals(defs.size(), 3);
}
#end_block

#method_before
public void shouldFindWithDimensions() throws Exception {
    Map<String, String> dims = new HashMap<>();
    dims.put("service", "compute");
    dims.put("instance_id", "123");
    List<MetricDefinition> defs = repo.find("bob", "cpu_utilization", dims, null, null);
    assertEquals(defs.size(), 2);
    dims.put("flavor_id", "2");
    defs = repo.find("bob", "cpu_utilization", dims, null, null);
    assertEquals(defs.size(), 1);
}
#method_after
public void shouldFindWithDimensions() throws Exception {
    Map<String, String> dims = new HashMap<>();
    dims.put("service", "compute");
    dims.put("instance_id", "123");
    List<MetricDefinition> defs = repo.find("bob", "cpu_utilization", dims, null, 1);
    assertEquals(defs.size(), 2);
    dims.put("flavor_id", "2");
    defs = repo.find("bob", "cpu_utilization", dims, null, 1);
    assertEquals(defs.size(), 1);
}
#end_block

#method_before
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, List<String> statistics, int period, String offset, String limit) throws Exception {
    int startIndex = this.influxV9Utils.startIndex(offset);
    String q = String.format("select %1$s %2$s " + "where %3$s %4$s %5$s %6$s %7$s %8$s %9$s %10$s", funcPart(statistics), this.influxV9Utils.namePart(name, true), this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.regionPart(this.region), this.influxV9Utils.startTimePart(startTime), this.influxV9Utils.dimPart(dimensions), this.influxV9Utils.endTimePart(endTime), this.influxV9Utils.periodPart(period), this.influxV9Utils.limitPart(limit), this.influxV9Utils.offsetPart(startIndex));
    // Todo. Need Influxdb 9 to support limit on points.
    logger.debug("Measurements query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<Statistics> statisticsList = statisticslist(series, startIndex);
    logger.debug("Found {} metric definitions matching query", statisticsList.size());
    return statisticsList;
}
#method_after
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, List<String> statistics, int period, String offset, int limit, Boolean mergeMetricsFlag) throws Exception {
    String q = buildQuery(tenantId, name, dimensions, startTime, endTime, statistics, period, offset, limit, mergeMetricsFlag);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<Statistics> statisticsList = statisticslist(series);
    logger.debug("Found {} metric definitions matching query", statisticsList.size());
    return statisticsList;
}
#end_block

#method_before
private List<Statistics> statisticslist(Series series, int startIndex) {
    List<Statistics> statisticsList = new LinkedList<>();
    if (!series.isEmpty()) {
        int index = startIndex;
        // Influxdb is returning all series back in one series.
        for (Serie serie : series.getSeries()) {
            Statistics statistics = new Statistics(serie.getName(), new HashMap<String, String>(), Arrays.asList(translateNames(serie.getColumns())));
            statistics.setId(String.valueOf(index++));
            for (Object[] values : serie.getValues()) {
                statistics.addStatistics(Arrays.asList(values));
            }
            statisticsList.add(statistics);
        }
    }
    return statisticsList;
}
#method_after
private List<Statistics> statisticslist(Series series) {
    List<Statistics> statisticsList = new LinkedList<>();
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            Statistics statistics = new Statistics(serie.getName(), serie.getTags(), Arrays.asList(translateNames(serie.getColumns())));
            for (Object[] values : serie.getValues()) {
                statistics.addStatistics(Arrays.asList(values));
            }
            statisticsList.add(statistics);
        }
    }
    return statisticsList;
}
#end_block

#method_before
public void shouldCreate() throws Exception {
    dimensions = new HashMap<String, String>();
    dimensions.put("instance_id", "937");
    dimensions.put("az", "2");
    dimensions.put("instance_uuid", "abc123");
    long timestamp = System.currentTimeMillis() / 1000;
    ClientResponse response = client().resource("/v2.0/metrics").header("X-Tenant-Id", TENANT_ID).header("Content-Type", MediaType.APPLICATION_JSON).post(ClientResponse.class, new CreateMetricCommand("test_namespace", dimensions, timestamp, 22.0));
    assertEquals(response.getStatus(), 204);
}
#method_after
public void shouldCreate() throws Exception {
    dimensions = new HashMap<String, String>();
    dimensions.put("instance_id", "937");
    dimensions.put("az", "2");
    dimensions.put("instance_uuid", "abc123");
    valueMeta = new HashMap<String, String>();
    valueMeta.put("rc", "404");
    valueMeta.put("errMsg", "Not Found");
    long timestamp = System.currentTimeMillis();
    ClientResponse response = client().resource("/v2.0/metrics").header("X-Tenant-Id", TENANT_ID).header("Content-Type", MediaType.APPLICATION_JSON).post(ClientResponse.class, new CreateMetricCommand("test_namespace", dimensions, timestamp, 22.0, valueMeta));
    assertEquals(response.getStatus(), 204);
}
#end_block

#method_before
@Override
public List<AlarmStateHistory> findById(String tenantId, String alarmId, String offset, String limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    // InfluxDB orders queries by time stamp desc by default.
    String query = buildQueryForFindById(tenantId, alarmId, offset);
    return queryInfluxDBForAlarmStateHistory(query);
}
#method_after
@Override
public List<AlarmStateHistory> findById(String tenantId, String alarmId, String offset, int limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    // InfluxDB orders queries by time stamp desc by default.
    String query = buildQueryForFindById(tenantId, alarmId, offset);
    return queryInfluxDBForAlarmStateHistory(query);
}
#end_block

#method_before
@Override
public List<AlarmStateHistory> find(String tenantId, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, String offset, String limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    List<String> alarmIdList = findAlarmIds(this.mysql, tenantId, dimensions);
    if (alarmIdList == null || alarmIdList.isEmpty()) {
        logger.debug("AlarmStateHistory no alarmIds");
        return Collections.emptyList();
    }
    logger.debug("AlarmStateHistory alarmIds {}", alarmIdList);
    String timePart = buildTimePart(startTime, endTime);
    String alarmsPart = buildAlarmsPart(alarmIdList);
    String offsetPart = InfluxV8Utils.buildOffsetPart(offset);
    String query = buildQueryForFind(tenantId, timePart, alarmsPart, offsetPart);
    logger.debug("AlarmStateHistory query for influxdb '{}'", query);
    return queryInfluxDBForAlarmStateHistory(query);
}
#method_after
@Override
public List<AlarmStateHistory> find(String tenantId, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, String offset, int limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    List<String> alarmIdList = findAlarmIds(this.mysql, tenantId, dimensions);
    if (alarmIdList == null || alarmIdList.isEmpty()) {
        logger.debug("AlarmStateHistory no alarmIds");
        return Collections.emptyList();
    }
    logger.debug("AlarmStateHistory alarmIds {}", alarmIdList);
    String timePart = buildTimePart(startTime, endTime);
    String alarmsPart = buildAlarmsPart(alarmIdList);
    String offsetPart = InfluxV8Utils.buildOffsetPart(offset);
    String query = buildQueryForFind(tenantId, timePart, alarmsPart, offsetPart);
    logger.debug("AlarmStateHistory query for influxdb '{}'", query);
    return queryInfluxDBForAlarmStateHistory(query);
}
#end_block

#method_before
public static Object paginate(int limit, List<? extends AbstractEntity> elements, UriInfo uriInfo) {
    Paged paged = new Paged();
    Link selfLink = new Link();
    selfLink.rel = "self";
    selfLink.href = uriInfo.getRequestUri().toString();
    paged.links.add(selfLink);
    if (elements != null) {
        if (elements.size() > limit) {
            Link nextLink = new Link();
            nextLink.rel = "next";
            // Create a new URL with the new offset.
            nextLink.href = uriInfo.getAbsolutePath().toString() + "?offset=" + elements.get(limit - 1).getId();
            // Add the query parms back to the URL without the original offset.
            for (String parmKey : uriInfo.getQueryParameters().keySet()) {
                if (!parmKey.equalsIgnoreCase("offset")) {
                    List<String> parmValList = uriInfo.getQueryParameters().get(parmKey);
                    for (String parmVal : parmValList) {
                        nextLink.href += "&" + parmKey + "=" + parmVal;
                    }
                }
            }
            paged.links.add(nextLink);
            // Truncate the list. Normally this will just truncate one extra element.
            elements = elements.subList(0, limit);
        }
        paged.elements = elements;
    } else {
        paged.elements = new ArrayList();
    }
    return paged;
}
#method_after
public static Object paginate(int limit, List<? extends AbstractEntity> elements, UriInfo uriInfo) {
    // Check for paging turned off. Happens if maxQueryLimit is not set. Used for V8 compatibility.
    if (limit == 0) {
        Paged paged = new Paged();
        paged.elements = elements != null ? elements : new ArrayList<>();
        return paged;
    }
    Paged paged = new Paged();
    paged.links.add(getSelfLink(uriInfo));
    if (elements != null) {
        if (elements.size() > limit) {
            String offset = elements.get(limit - 1).getId();
            paged.links.add(getNextLink(offset, uriInfo));
            // Truncate the list. Normally this will just truncate one extra element.
            elements = elements.subList(0, limit);
        }
        paged.elements = elements;
    } else {
        paged.elements = new ArrayList();
    }
    return paged;
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
protected void setupResources() throws Exception {
    super.setupResources();
    dimensions = new HashMap<String, String>();
    dimensions.put("instance_id", "937");
    dimensions.put("service", "foo.compute");
    timestamp = System.currentTimeMillis() / 1000L;
    service = mock(MetricService.class);
    doNothing().when(service).create(any(List.class), anyString(), anyString());
    metricRepo = mock(MetricDefinitionRepo.class);
    addResources(new MetricResource(service, metricRepo, new PersistUtils()));
}
#method_after
@Override
@SuppressWarnings("unchecked")
protected void setupResources() throws Exception {
    super.setupResources();
    dimensions = new HashMap<String, String>();
    dimensions.put("instance_id", "937");
    dimensions.put("service", "foo.compute");
    valueMeta = new HashMap<String, String>();
    valueMeta.put("rc", "404");
    valueMeta.put("errorMsg", "Not Found");
    timestamp = System.currentTimeMillis();
    service = mock(MetricService.class);
    doNothing().when(service).create(any(List.class), anyString(), anyString());
    metricRepo = mock(MetricDefinitionRepo.class);
    addResources(new MetricResource(service, metricRepo, new PersistUtils()));
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldCreate() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, timestamp, 22.0));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#method_after
@SuppressWarnings("unchecked")
public void shouldCreate() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, timestamp, 22.0, valueMeta));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldCreateWithNonNumericAZ() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("instance_id", "937");
    dims.put("service", "foo.compute");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#method_after
@SuppressWarnings("unchecked")
public void shouldCreateWithNonNumericAZ() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("instance_id", "937");
    dims.put("service", "foo.compute");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0, valueMeta));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldCreateWithoutDimensions() throws Exception {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", null, timestamp, 22.0));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#method_after
@SuppressWarnings("unchecked")
public void shouldCreateWithoutDimensions() throws Exception {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", null, timestamp, 22.0, valueMeta));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldCreateWithZeroValue() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, timestamp, 0.0));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#method_after
@SuppressWarnings("unchecked")
public void shouldCreateWithZeroValue() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, timestamp, 0.0, valueMeta));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldCreateWithNegativeValue() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, timestamp, -1.0));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#method_after
@SuppressWarnings("unchecked")
public void shouldCreateWithNegativeValue() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, timestamp, -1.0, valueMeta));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldCreateWithZeroTimestamp() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, 0L, 0.0));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#method_after
@SuppressWarnings("unchecked")
public void shouldCreateWithZeroTimestamp() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, 0L, 0.0, valueMeta));
    assertEquals(response.getStatus(), 204);
    verify(service).create(any(List.class), eq("abc"), anyString());
}
#end_block

#method_before
public void shouldErrorOnPostWithCrossTenant() {
    ClientResponse response = createResponseForCrossTenant(new CreateMetricCommand("test_metrictype", dimensions, timestamp, 22.0), "def");
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("forbidden", 403, "Project abc cannot POST cross tenant");
}
#method_after
public void shouldErrorOnPostWithCrossTenant() {
    ClientResponse response = createResponseForCrossTenant(new CreateMetricCommand("test_metrictype", dimensions, timestamp, 22.0, valueMeta), "def");
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("forbidden", 403, "Project abc cannot POST cross tenant");
}
#end_block

#method_before
public void shouldErrorOnCreateWithIllegalCharsInName() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("hpcs@.compute%", dimensions, timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Metric name hpcs@.compute% may only contain: a-z A-Z 0-9 _ - .");
}
#method_after
public void shouldErrorOnCreateWithIllegalCharsInName() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("hpcs@.compute%", dimensions, timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Metric name hpcs@.compute% may only contain: a-z A-Z 0-9 _ - .");
}
#end_block

#method_before
public void shouldErrorOnCreateWithTooLongName() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("1234567890123456789012345678901234567890123456789012345678901234567890" + "1234567890123456789012345678901234567890123456789012345678901234567890" + "1234567890123456789012345678901234567890123456789012345678901234567890" + "1234567890123456789012345678901234567890123456789012345678901234567890" + "1234567890123456789012345678901234567890123456789012345678901234567890", dimensions, timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, String.format("[name size must be between 1 and %d", CreateMetricCommand.MAX_NAME_LENGTH));
}
#method_after
public void shouldErrorOnCreateWithTooLongName() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("1234567890123456789012345678901234567890123456789012345678901234567890" + "1234567890123456789012345678901234567890123456789012345678901234567890" + "1234567890123456789012345678901234567890123456789012345678901234567890" + "1234567890123456789012345678901234567890123456789012345678901234567890" + "1234567890123456789012345678901234567890123456789012345678901234567890", dimensions, timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, String.format("[name size must be between 1 and %d", CreateMetricCommand.MAX_NAME_LENGTH));
}
#end_block

#method_before
public void shouldErrorOnCreateWithReservedService() {
    Map<String, String> dims = new HashMap<>();
    dims.put("instance_id", "937");
    dims.put("service", "hpcs.compute");
    ClientResponse response = createResponseFor(new CreateMetricCommand("foo", dims, timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("forbidden", 403, "Project abc cannot POST metrics for the hpcs service");
}
#method_after
public void shouldErrorOnCreateWithReservedService() {
    Map<String, String> dims = new HashMap<>();
    dims.put("instance_id", "937");
    dims.put("service", "hpcs.compute");
    ClientResponse response = createResponseFor(new CreateMetricCommand("foo", dims, timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("forbidden", 403, "Project abc cannot POST metrics for the hpcs service");
}
#end_block

#method_before
public void shouldErrorOnCreateWithBadValue() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("blah", "");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Dimension blah cannot have an empty value");
}
#method_after
public void shouldErrorOnCreateWithBadValue() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("blah", "");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Dimension blah cannot have an empty value");
}
#end_block

#method_before
public void shouldErrorOnCreateWithMissingDimensionValue() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("instance_id", "937");
    dims.put("az", "2");
    dims.put("instance_uuid", "abc123");
    dims.put("flavor_id", "");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Dimension flavor_id cannot have an empty value");
}
#method_after
public void shouldErrorOnCreateWithMissingDimensionValue() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("instance_id", "937");
    dims.put("az", "2");
    dims.put("instance_uuid", "abc123");
    dims.put("flavor_id", "");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Dimension flavor_id cannot have an empty value");
}
#end_block

#method_before
public void shouldErrorOnCreateWithTooLongDimensionName() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("instance_id", "937");
    dims.put("az", "2");
    dims.put("instance_uuid", "abc123");
    dims.put("0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789" + "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789" + "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789abc", "abc123");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Dimension name 012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789abc must be 255 characters or less");
}
#method_after
public void shouldErrorOnCreateWithTooLongDimensionName() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("instance_id", "937");
    dims.put("az", "2");
    dims.put("instance_uuid", "abc123");
    dims.put("0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789" + "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789" + "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789abc", "abc123");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Dimension name 012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789abc must be 255 characters or less");
}
#end_block

#method_before
public void shouldErrorOnCreateWithTooLongDimensionValue() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("instance_id", "937");
    dims.put("az", "2");
    dims.put("instance_uuid", "abc123");
    dims.put("abc", "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789" + "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789" + "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789abc");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Dimension value 012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789abc must be 255 characters or less");
}
#method_after
public void shouldErrorOnCreateWithTooLongDimensionValue() {
    Map<String, String> dims = new HashMap<String, String>();
    dims.put("instance_id", "937");
    dims.put("az", "2");
    dims.put("instance_uuid", "abc123");
    dims.put("abc", "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789" + "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789" + "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789abc");
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dims, timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Dimension value 012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789abc must be 255 characters or less");
}
#end_block

#method_before
public void shouldErrorOnCreateWithHighTimestamp() {
    long local_timestamp = timestamp + 1000;
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, local_timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Timestamp " + local_timestamp + " is out of legal range");
}
#method_after
public void shouldErrorOnCreateWithHighTimestamp() {
    long local_timestamp = timestamp + 1000000;
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, local_timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Timestamp " + local_timestamp + " is out of legal range");
}
#end_block

#method_before
public void shouldErrorOnCreateWithLowTimestamp() {
    long local_timestamp = timestamp - 1309600;
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, local_timestamp, 22.0));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Timestamp " + local_timestamp + " is out of legal range");
}
#method_after
public void shouldErrorOnCreateWithLowTimestamp() {
    long local_timestamp = timestamp - 1309600000;
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, local_timestamp, 22.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "Timestamp " + local_timestamp + " is out of legal range");
}
#end_block

#method_before
@Override
public List<AlarmStateHistory> findById(String tenantId, String alarmId, String offset, String limit) throws Exception {
    // Need Influxdb 9 to support limit on points.
    String q = String.format("select alarm_id, metrics, old_state, new_state, reason, reason_data " + "from alarm_state_history " + "where %1$s %2$s %3$s %4$s", this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.alarmIdPart(alarmId), this.influxV9Utils.timeOffsetPart(offset), this.influxV9Utils.limitPart(limit));
    logger.debug("Alarm state history query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<AlarmStateHistory> alarmStateHistoryList = alarmStateHistoryList(series);
    logger.debug("Found {} alarm state transitions matching query", alarmStateHistoryList.size());
    return alarmStateHistoryList;
}
#method_after
@Override
public List<AlarmStateHistory> findById(String tenantId, String alarmId, String offset, int limit) throws Exception {
    String q = String.format("select alarm_id, metrics, old_state, new_state, reason, reason_data " + "from alarm_state_history " + "where %1$s %2$s %3$s %4$s", this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.alarmIdPart(alarmId), this.influxV9Utils.timeOffsetPart(offset), this.influxV9Utils.limitPart(limit));
    logger.debug("Alarm state history query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<AlarmStateHistory> alarmStateHistoryList = alarmStateHistoryList(series);
    logger.debug("Found {} alarm state transitions matching query", alarmStateHistoryList.size());
    return alarmStateHistoryList;
}
#end_block

#method_before
@Override
public List<AlarmStateHistory> find(String tenantId, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, String limit) throws Exception {
    // Need Influxdb 9 to support limit on points.
    List<String> alarmIdList = findAlarmIds(this.mysql, tenantId, dimensions);
    if (alarmIdList == null || alarmIdList.isEmpty()) {
        return new ArrayList<>();
    }
    String q = String.format("select alarm_id, metrics, old_state, new_state, reason, reason_data " + "from alarm_state_history " + "where %1$s %2$s %3$s %4$s %5$s", this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.startTimeEndTimePart(startTime, endTime), this.influxV9Utils.alarmIdsPart(alarmIdList), this.influxV9Utils.timeOffsetPart(offset), this.influxV9Utils.limitPart(limit));
    logger.debug("Alarm state history list query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<AlarmStateHistory> alarmStateHistoryList = alarmStateHistoryList(series);
    logger.debug("Found {} alarm state transitions matching query", alarmStateHistoryList.size());
    return alarmStateHistoryList;
}
#method_after
@Override
public List<AlarmStateHistory> find(String tenantId, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, int limit) throws Exception {
    List<String> alarmIdList = findAlarmIds(this.mysql, tenantId, dimensions);
    if (alarmIdList == null || alarmIdList.isEmpty()) {
        return new ArrayList<>();
    }
    String q = String.format("select alarm_id, metrics, old_state, new_state, reason, reason_data " + "from alarm_state_history " + "where %1$s %2$s %3$s %4$s %5$s", this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.startTimeEndTimePart(startTime, endTime), this.influxV9Utils.alarmIdsPart(alarmIdList), this.influxV9Utils.timeOffsetPart(offset), this.influxV9Utils.limitPart(limit));
    logger.debug("Alarm state history list query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<AlarmStateHistory> alarmStateHistoryList = alarmStateHistoryList(series);
    logger.debug("Found {} alarm state transitions matching query", alarmStateHistoryList.size());
    return alarmStateHistoryList;
}
#end_block

#method_before
private List<AlarmStateHistory> alarmStateHistoryList(Series series) {
    List<AlarmStateHistory> alarmStateHistoryList = new LinkedList<>();
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            for (String[] values : serie.getValues()) {
                AlarmStateHistory alarmStateHistory = new AlarmStateHistory();
                Date date;
                try {
                    date = this.simpleDateFormat.parse(values[0]);
                } catch (ParseException e) {
                    logger.error("Failed to parse time", e);
                    continue;
                }
                DateTime dateTime = new DateTime(date.getTime(), DateTimeZone.UTC);
                alarmStateHistory.setTimestamp(dateTime);
                alarmStateHistory.setAlarmId(values[1]);
                List<MetricDefinition> metricDefinitionList;
                try {
                    metricDefinitionList = this.objectMapper.readValue(values[2], METRICS_TYPE);
                } catch (IOException e) {
                    logger.error("Failed to parse metrics", e);
                    continue;
                }
                alarmStateHistory.setMetrics(metricDefinitionList);
                alarmStateHistory.setOldState(AlarmState.valueOf(values[3]));
                alarmStateHistory.setNewState(AlarmState.valueOf(values[4]));
                alarmStateHistory.setReason(values[5]);
                alarmStateHistory.setReasonData(values[6]);
                alarmStateHistoryList.add(alarmStateHistory);
            }
        }
    }
    return alarmStateHistoryList;
}
#method_after
private List<AlarmStateHistory> alarmStateHistoryList(Series series) {
    List<AlarmStateHistory> alarmStateHistoryList = new LinkedList<>();
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            for (String[] values : serie.getValues()) {
                AlarmStateHistory alarmStateHistory = new AlarmStateHistory();
                Date date;
                try {
                    date = this.simpleDateFormat.parse(values[0]);
                } catch (ParseException e) {
                    logger.error("Failed to parse time", e);
                    continue;
                }
                DateTime dateTime = new DateTime(date.getTime(), DateTimeZone.UTC);
                alarmStateHistory.setTimestamp(dateTime);
                alarmStateHistory.setAlarmId(values[1]);
                List<MetricDefinition> metricDefinitionList;
                try {
                    metricDefinitionList = this.objectMapper.readValue(values[2], METRICS_TYPE);
                } catch (IOException e) {
                    logger.error("Failed to parse metrics", e);
                    continue;
                }
                alarmStateHistory.setMetrics(metricDefinitionList);
                alarmStateHistory.setOldState(AlarmState.valueOf(values[3]));
                alarmStateHistory.setNewState(AlarmState.valueOf(values[4]));
                alarmStateHistory.setReason(values[5]);
                alarmStateHistory.setReasonData(values[6]);
                List<AlarmTransitionSubAlarm> subAlarmList;
                try {
                    subAlarmList = this.objectMapper.readValue(values[7], SUBALARMS_TYPE);
                } catch (IOException e) {
                    logger.error("Failed to parse sub-alarms", e);
                    continue;
                }
                alarmStateHistory.setSubAlarms(subAlarmList);
                alarmStateHistoryList.add(alarmStateHistory);
            }
        }
    }
    return alarmStateHistoryList;
}
#end_block

#method_before
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, String limit) throws Exception {
    int startIndex = this.influxV9Utils.startIndex(offset);
    String q = String.format("show series %1$s where %2$s %3$s %4$s %5$s %6$s", this.influxV9Utils.namePart(name, false), this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.regionPart(this.region), this.influxV9Utils.dimPart(dimensions), this.influxV9Utils.limitPart(limit), this.influxV9Utils.offsetPart(startIndex));
    logger.debug("Metric definition query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<MetricDefinition> metricDefinitionList = metricDefinitionList(series, startIndex);
    logger.debug("Found {} metric definitions matching query", metricDefinitionList.size());
    return metricDefinitionList;
}
#method_after
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, int limit) throws Exception {
    int startIndex = this.influxV9Utils.startIndex(offset);
    String q = String.format("show series %1$s where %2$s %3$s %4$s %5$s %6$s", this.influxV9Utils.namePart(name, false), this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.regionPart(this.region), this.influxV9Utils.dimPart(dimensions), this.influxV9Utils.limitPart(limit), this.influxV9Utils.offsetPart(startIndex));
    logger.debug("Metric definition query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<MetricDefinition> metricDefinitionList = metricDefinitionList(series, startIndex);
    logger.debug("Found {} metric definitions matching query", metricDefinitionList.size());
    return metricDefinitionList;
}
#end_block

#method_before
@Override
public List<NotificationMethod> find(String tenantId, String offset, String limit) {
    try (Handle h = db.open()) {
        String rawQuery = "  SELECT nm.id, nm.tenant_id, nm.name, nm.type, nm.address, nm.created_at, nm.updated_at " + "FROM notification_method as nm " + "WHERE tenant_id = :tenantId %1$s order by nm.id asc limit :limit";
        String offsetPart = "";
        if (offset != null) {
            offsetPart = "and nm.id > :offset";
        }
        String query = String.format(rawQuery, offsetPart);
        Query<?> q = h.createQuery(query);
        q.bind("tenantId", tenantId);
        if (offset != null) {
            q.bind("offset", offset);
        }
        q.bind("limit", this.persistUtils.getLimit(limit) + 1);
        return (List<NotificationMethod>) q.map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
    }
}
#method_after
@Override
public List<NotificationMethod> find(String tenantId, String offset, int limit) {
    try (Handle h = db.open()) {
        String rawQuery = "  SELECT nm.id, nm.tenant_id, nm.name, nm.type, nm.address, nm.created_at, nm.updated_at " + "FROM notification_method as nm " + "WHERE tenant_id = :tenantId %1$s order by nm.id asc limit :limit";
        String offsetPart = "";
        if (offset != null) {
            offsetPart = "and nm.id > :offset";
        }
        String query = String.format(rawQuery, offsetPart);
        Query<?> q = h.createQuery(query);
        q.bind("tenantId", tenantId);
        if (offset != null) {
            q.bind("offset", offset);
        }
        q.bind("limit", limit + 1);
        return (List<NotificationMethod>) q.map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
    }
}
#end_block

#method_before
@Override
public boolean exists(String tenantId, String name) {
    try (Handle h = db.open()) {
        return h.createQuery("select exists(select 1 from alarm_definition where tenant_id = :tenantId and name = :name and deleted_at is NULL)").bind("tenantId", tenantId).bind("name", name).mapTo(Boolean.TYPE).first();
    }
}
#method_after
@Override
public String exists(String tenantId, String name) {
    try (Handle h = db.open()) {
        Map<String, Object> map = h.createQuery("select id from alarm_definition where tenant_id = :tenantId and name = :name and deleted_at is NULL").bind("tenantId", tenantId).bind("name", name).first();
        if (map != null) {
            if (map.values().size() != 0) {
                return map.get("id").toString();
            } else {
                return null;
            }
        } else {
            return null;
        }
    }
}
#end_block

#method_before
@SuppressWarnings("unchecked")
@Override
public List<AlarmDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, String limit) {
    try (Handle h = db.open()) {
        String query = "  SELECT t.id, t.tenant_id, t.name, t.description, t.expression, t.severity, t.match_by," + "     t.actions_enabled, t.created_at, t.updated_at, t.deleted_at, " + "     GROUP_CONCAT(aa.alarm_state) AS states, " + "     GROUP_CONCAT(aa.action_id) AS notificationIds " + "FROM (SELECT distinct ad.id, ad.tenant_id, ad.name, ad.description, ad.expression," + "        ad.severity, ad.match_by, ad.actions_enabled, ad.created_at, " + "        ad.updated_at, ad.deleted_at " + "      FROM alarm_definition AS ad " + "      LEFT OUTER JOIN sub_alarm_definition AS sad ON ad.id = sad.alarm_definition_id " + "      LEFT OUTER JOIN sub_alarm_definition_dimension AS dim ON sad.id = dim.sub_alarm_definition_id %1$s " + "      WHERE ad.tenant_id = :tenantId AND ad.deleted_at IS NULL %2$s limit :limit) AS t " + "LEFT OUTER JOIN alarm_action AS aa ON t.id = aa.alarm_definition_id " + "GROUP BY t.id ORDER BY t.id, t.created_at";
        StringBuilder sbWhere = new StringBuilder();
        if (name != null) {
            sbWhere.append(" and ad.name = :name");
        }
        if (offset != null) {
            sbWhere.append(" and ad.id > :offset");
        }
        String sql = String.format(query, SubAlarmDefinitionQueries.buildJoinClauseFor(dimensions), sbWhere);
        Query<?> q = h.createQuery(sql);
        q.bind("tenantId", tenantId);
        if (name != null) {
            q.bind("name", name);
        }
        if (offset != null) {
            q.bind("offset", offset);
        }
        q.bind("limit", this.persistUtils.getLimit(limit) + 1);
        q.registerMapper(new AlarmDefinitionMapper());
        q = q.mapTo(AlarmDefinition.class);
        DimensionQueries.bindDimensionsToQuery(q, dimensions);
        List<AlarmDefinition> resultSet = (List<AlarmDefinition>) q.list();
        return resultSet;
    }
}
#method_after
@SuppressWarnings("unchecked")
@Override
public List<AlarmDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, int limit) {
    try (Handle h = db.open()) {
        String query = "  SELECT t.id, t.tenant_id, t.name, t.description, t.expression, t.severity, t.match_by," + "     t.actions_enabled, t.created_at, t.updated_at, t.deleted_at, " + "     GROUP_CONCAT(aa.alarm_state) AS states, " + "     GROUP_CONCAT(aa.action_id) AS notificationIds " + "FROM (SELECT distinct ad.id, ad.tenant_id, ad.name, ad.description, ad.expression," + "        ad.severity, ad.match_by, ad.actions_enabled, ad.created_at, " + "        ad.updated_at, ad.deleted_at " + "      FROM alarm_definition AS ad " + "      LEFT OUTER JOIN sub_alarm_definition AS sad ON ad.id = sad.alarm_definition_id " + "      LEFT OUTER JOIN sub_alarm_definition_dimension AS dim ON sad.id = dim.sub_alarm_definition_id %1$s " + "      WHERE ad.tenant_id = :tenantId AND ad.deleted_at IS NULL %2$s limit :limit) AS t " + "LEFT OUTER JOIN alarm_action AS aa ON t.id = aa.alarm_definition_id " + "GROUP BY t.id ORDER BY t.id, t.created_at";
        StringBuilder sbWhere = new StringBuilder();
        if (name != null) {
            sbWhere.append(" and ad.name = :name");
        }
        if (offset != null) {
            sbWhere.append(" and ad.id > :offset");
        }
        String sql = String.format(query, SubAlarmDefinitionQueries.buildJoinClauseFor(dimensions), sbWhere);
        Query<?> q = h.createQuery(sql);
        q.bind("tenantId", tenantId);
        if (name != null) {
            q.bind("name", name);
        }
        if (offset != null) {
            q.bind("offset", offset);
        }
        q.bind("limit", limit + 1);
        q.registerMapper(new AlarmDefinitionMapper());
        q = q.mapTo(AlarmDefinition.class);
        DimensionQueries.bindDimensionsToQuery(q, dimensions);
        List<AlarmDefinition> resultSet = (List<AlarmDefinition>) q.list();
        return resultSet;
    }
}
#end_block

#method_before
public AlarmDefinition create(String tenantId, String name, @Nullable String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, List<String> alarmActions, @Nullable List<String> okActions, @Nullable List<String> undeterminedActions) {
    // Assert no alarm exists by the name
    if (repo.exists(tenantId, name))
        throw new EntityExistsException("An alarm definition already exists for project / tenant: %s named: %s", tenantId, name);
    assertActionsExist(tenantId, alarmActions, okActions, undeterminedActions);
    Map<String, AlarmSubExpression> subAlarms = new HashMap<String, AlarmSubExpression>();
    for (AlarmSubExpression subExpression : alarmExpression.getSubExpressions()) subAlarms.put(UUID.randomUUID().toString(), subExpression);
    String alarmDefId = UUID.randomUUID().toString();
    AlarmDefinition alarm = null;
    try {
        LOG.debug("Creating alarm definition {} for tenant {}", name, tenantId);
        alarm = repo.create(tenantId, alarmDefId, name, description, severity, expression, subAlarms, matchBy, alarmActions, okActions, undeterminedActions);
        // Notify interested parties of new alarm
        String event = Serialization.toJson(new AlarmDefinitionCreatedEvent(tenantId, alarmDefId, name, description, expression, subAlarms, matchBy));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
        return alarm;
    } catch (Exception e) {
        if (alarm != null)
            try {
                repo.deleteById(tenantId, alarm.getId());
            } catch (Exception ignore) {
            }
        throw Exceptions.uncheck(e, "Error creating alarm definition for project / tenant %s", tenantId);
    }
}
#method_after
public AlarmDefinition create(String tenantId, String name, @Nullable String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, List<String> alarmActions, @Nullable List<String> okActions, @Nullable List<String> undeterminedActions) {
    // Assert no alarm exists by the name
    String alarmDefID = repo.exists(tenantId, name);
    if (alarmDefID != null) {
        throw new EntityExistsException("An alarm definition already exists for project / tenant: %s named: %s", tenantId, name);
    }
    assertActionsExist(tenantId, alarmActions, okActions, undeterminedActions);
    Map<String, AlarmSubExpression> subAlarms = new HashMap<String, AlarmSubExpression>();
    for (AlarmSubExpression subExpression : alarmExpression.getSubExpressions()) subAlarms.put(UUID.randomUUID().toString(), subExpression);
    String alarmDefId = UUID.randomUUID().toString();
    AlarmDefinition alarm = null;
    try {
        LOG.debug("Creating alarm definition {} for tenant {}", name, tenantId);
        alarm = repo.create(tenantId, alarmDefId, name, description, severity, expression, subAlarms, matchBy, alarmActions, okActions, undeterminedActions);
        // Notify interested parties of new alarm
        String event = Serialization.toJson(new AlarmDefinitionCreatedEvent(tenantId, alarmDefId, name, description, expression, subAlarms, matchBy));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
        return alarm;
    } catch (Exception e) {
        if (alarm != null)
            try {
                repo.deleteById(tenantId, alarm.getId());
            } catch (Exception ignore) {
            }
        throw Exceptions.uncheck(e, "Error creating alarm definition for project / tenant %s", tenantId);
    }
}
#end_block

#method_before
public void delete(String tenantId, String alarmDefId) {
    Map<String, MetricDefinition> subAlarmMetricDefs = repo.findSubAlarmMetricDefinitions(alarmDefId);
    // Have to get information about the Alarms before they are deleted. They will be deleted
    // by the database as a cascade delete from the Alarm Definition delete
    final List<Alarm> alarms = alarmRepo.find(tenantId, alarmDefId, null, null, null, null, null, false);
    final Map<String, Map<String, AlarmSubExpression>> alarmSubExpressions = alarmRepo.findAlarmSubExpressionsForAlarmDefinition(alarmDefId);
    repo.deleteById(tenantId, alarmDefId);
    // Notify interested parties of alarm definition deletion
    String event = Serialization.toJson(new AlarmDefinitionDeletedEvent(alarmDefId, subAlarmMetricDefs));
    producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
    // wants it so Alarms don't get recreated
    for (final Alarm alarm : alarms) {
        String alarmDeletedEvent = Serialization.toJson(new AlarmDeletedEvent(tenantId, alarm.getId(), alarm.getMetrics(), alarmDefId, alarmSubExpressions.get(alarm.getId())));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), alarmDeletedEvent));
    }
}
#method_after
public void delete(String tenantId, String alarmDefId) {
    Map<String, MetricDefinition> subAlarmMetricDefs = repo.findSubAlarmMetricDefinitions(alarmDefId);
    // Have to get information about the Alarms before they are deleted. They will be deleted
    // by the database as a cascade delete from the Alarm Definition delete
    final List<Alarm> alarms = alarmRepo.find(tenantId, alarmDefId, null, null, null, null, 1, false);
    final Map<String, Map<String, AlarmSubExpression>> alarmSubExpressions = alarmRepo.findAlarmSubExpressionsForAlarmDefinition(alarmDefId);
    repo.deleteById(tenantId, alarmDefId);
    // Notify interested parties of alarm definition deletion
    String event = Serialization.toJson(new AlarmDefinitionDeletedEvent(alarmDefId, subAlarmMetricDefs));
    producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
    // wants it so Alarms don't get recreated
    for (final Alarm alarm : alarms) {
        String alarmDeletedEvent = Serialization.toJson(new AlarmDeletedEvent(tenantId, alarm.getId(), alarm.getMetrics(), alarmDefId, alarmSubExpressions.get(alarm.getId())));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), alarmDeletedEvent));
    }
}
#end_block

#method_before
public AlarmDefinition update(String tenantId, String alarmDefId, AlarmExpression alarmExpression, UpdateAlarmDefinitionCommand command) {
    final AlarmDefinition oldAlarmDefinition = assertAlarmDefinitionExists(tenantId, alarmDefId, command.alarmActions, command.okActions, command.undeterminedActions);
    final SubExpressions subExpressions = subExpressionsFor(repo.findSubExpressions(alarmDefId), alarmExpression);
    validateChangesAllowed(command.matchBy, oldAlarmDefinition, subExpressions);
    updateInternal(tenantId, alarmDefId, false, command.name, command.description, command.expression, command.matchBy, command.severity, alarmExpression, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions, subExpressions);
    return new AlarmDefinition(alarmDefId, command.name, command.description, command.severity, command.expression, command.matchBy, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions);
}
#method_after
public AlarmDefinition update(String tenantId, String alarmDefId, AlarmExpression alarmExpression, UpdateAlarmDefinitionCommand command) {
    final AlarmDefinition oldAlarmDefinition = assertAlarmDefinitionExists(tenantId, alarmDefId, command.alarmActions, command.okActions, command.undeterminedActions);
    final SubExpressions subExpressions = subExpressionsFor(repo.findSubExpressions(alarmDefId), alarmExpression);
    String alarmID = repo.exists(tenantId, command.name);
    if (alarmID != null && !alarmID.equalsIgnoreCase(alarmDefId)) {
        throw new EntityExistsException("An alarm definition with the same name already exists for project / tenant: %s named: %s", tenantId, command.name);
    }
    validateChangesAllowed(command.matchBy, oldAlarmDefinition, subExpressions);
    updateInternal(tenantId, alarmDefId, false, command.name, command.description, command.expression, command.matchBy, command.severity, alarmExpression, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions, subExpressions);
    return new AlarmDefinition(alarmDefId, command.name, command.description, command.severity, command.expression, command.matchBy, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions);
}
#end_block

#method_before
public AlarmDefinition patch(String tenantId, String alarmDefId, String name, String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, Boolean enabled, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions) {
    AlarmDefinition oldAlarmDefinition = assertAlarmDefinitionExists(tenantId, alarmDefId, alarmActions, okActions, undeterminedActions);
    name = name == null ? oldAlarmDefinition.getName() : name;
    description = description == null ? oldAlarmDefinition.getDescription() : description;
    expression = expression == null ? oldAlarmDefinition.getExpression() : expression;
    severity = severity == null ? oldAlarmDefinition.getSeverity() : severity;
    alarmExpression = alarmExpression == null ? AlarmExpression.of(expression) : alarmExpression;
    enabled = enabled == null ? oldAlarmDefinition.isActionsEnabled() : enabled;
    matchBy = matchBy == null ? oldAlarmDefinition.getMatchBy() : matchBy;
    final SubExpressions subExpressions = subExpressionsFor(repo.findSubExpressions(alarmDefId), alarmExpression);
    validateChangesAllowed(matchBy, oldAlarmDefinition, subExpressions);
    updateInternal(tenantId, alarmDefId, true, name, description, expression, matchBy, severity, alarmExpression, enabled, alarmActions, okActions, undeterminedActions, subExpressions);
    return new AlarmDefinition(alarmDefId, name, description, severity, expression, matchBy, enabled, alarmActions == null ? oldAlarmDefinition.getAlarmActions() : alarmActions, okActions == null ? oldAlarmDefinition.getOkActions() : okActions, undeterminedActions == null ? oldAlarmDefinition.getUndeterminedActions() : undeterminedActions);
}
#method_after
public AlarmDefinition patch(String tenantId, String alarmDefId, String name, String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, Boolean enabled, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions) {
    AlarmDefinition oldAlarmDefinition = assertAlarmDefinitionExists(tenantId, alarmDefId, alarmActions, okActions, undeterminedActions);
    name = name == null ? oldAlarmDefinition.getName() : name;
    String alarmID = repo.exists(tenantId, name);
    if (alarmID != null && !alarmID.equalsIgnoreCase(alarmDefId)) {
        throw new EntityExistsException("An alarm definition with the same name already exists for project / tenant: %s named: %s", tenantId, name);
    }
    description = description == null ? oldAlarmDefinition.getDescription() : description;
    expression = expression == null ? oldAlarmDefinition.getExpression() : expression;
    severity = severity == null ? oldAlarmDefinition.getSeverity() : severity;
    alarmExpression = alarmExpression == null ? AlarmExpression.of(expression) : alarmExpression;
    enabled = enabled == null ? oldAlarmDefinition.isActionsEnabled() : enabled;
    matchBy = matchBy == null ? oldAlarmDefinition.getMatchBy() : matchBy;
    final SubExpressions subExpressions = subExpressionsFor(repo.findSubExpressions(alarmDefId), alarmExpression);
    validateChangesAllowed(matchBy, oldAlarmDefinition, subExpressions);
    updateInternal(tenantId, alarmDefId, true, name, description, expression, matchBy, severity, alarmExpression, enabled, alarmActions, okActions, undeterminedActions, subExpressions);
    return new AlarmDefinition(alarmDefId, name, description, severity, expression, matchBy, enabled, alarmActions == null ? oldAlarmDefinition.getAlarmActions() : alarmActions, okActions == null ? oldAlarmDefinition.getOkActions() : okActions, undeterminedActions == null ? oldAlarmDefinition.getUndeterminedActions() : undeterminedActions);
}
#end_block

#method_before
@Override
public List<AlarmStateHistory> findById(String tenantId, String alarmId, String offset, String limit) {
    try (Handle h = vertica.open()) {
        return h.createQuery("select alarm_id, old_state, new_state, reason, reason_data, time_stamp as timestamp from MonAlarms.StateHistory where tenant_id = :tenantId and alarm_id = :alarmId order by time_stamp desc").bind("tenantId", tenantId).bind("alarmId", alarmId).map(new BeanMapper<>(AlarmStateHistory.class)).list();
    }
}
#method_after
@Override
public List<AlarmStateHistory> findById(String tenantId, String alarmId, String offset, int limit) {
    try (Handle h = vertica.open()) {
        return h.createQuery("select alarm_id, old_state, new_state, reason, reason_data, time_stamp as timestamp from MonAlarms.StateHistory where tenant_id = :tenantId and alarm_id = :alarmId order by time_stamp desc").bind("tenantId", tenantId).bind("alarmId", alarmId).map(new BeanMapper<>(AlarmStateHistory.class)).list();
    }
}
#end_block

#method_before
@Override
public List<AlarmStateHistory> find(String tenantId, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, String limit) {
    // Todo. Use offset and limit for pagination.
    List<String> alarmIds = null;
    // Find alarm Ids for dimensions
    try (Handle h = mysql.open()) {
        String sql = String.format(FIND_ALARMS_SQL, SubAlarmDefinitionQueries.buildJoinClauseFor(dimensions));
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId);
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        alarmIds = query.map(StringMapper.FIRST).list();
    }
    if (alarmIds == null || alarmIds.isEmpty())
        return Collections.emptyList();
    // Find alarm state history for alarm Ids
    try (Handle h = vertica.open()) {
        // Build sql
        StringBuilder sbWhere = new StringBuilder();
        sbWhere.append(" and alarm_id in (");
        for (int i = 0; i < alarmIds.size(); i++) {
            if (i > 0)
                sbWhere.append(", ");
            sbWhere.append('\'').append(alarmIds.get(i)).append('\'');
        }
        sbWhere.append(')');
        if (startTime != null)
            sbWhere.append(" and time_stamp >= :startTime");
        if (endTime != null)
            sbWhere.append(" and time_stamp <= :endTime");
        String sql = String.format(FIND_BY_ALARM_DEF_SQL, sbWhere);
        // Build query
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId);
        if (startTime != null)
            query.bind("startTime", new Timestamp(startTime.getMillis()));
        if (endTime != null)
            query.bind("endTime", new Timestamp(endTime.getMillis()));
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        return query.map(new BeanMapper<>(AlarmStateHistory.class)).list();
    }
}
#method_after
@Override
public List<AlarmStateHistory> find(String tenantId, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, int limit) {
    // Todo. Use offset and limit for pagination.
    List<String> alarmIds = null;
    // Find alarm Ids for dimensions
    try (Handle h = mysql.open()) {
        String sql = String.format(FIND_ALARMS_SQL, SubAlarmDefinitionQueries.buildJoinClauseFor(dimensions));
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId);
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        alarmIds = query.map(StringMapper.FIRST).list();
    }
    if (alarmIds == null || alarmIds.isEmpty())
        return Collections.emptyList();
    // Find alarm state history for alarm Ids
    try (Handle h = vertica.open()) {
        // Build sql
        StringBuilder sbWhere = new StringBuilder();
        sbWhere.append(" and alarm_id in (");
        for (int i = 0; i < alarmIds.size(); i++) {
            if (i > 0)
                sbWhere.append(", ");
            sbWhere.append('\'').append(alarmIds.get(i)).append('\'');
        }
        sbWhere.append(')');
        if (startTime != null)
            sbWhere.append(" and time_stamp >= :startTime");
        if (endTime != null)
            sbWhere.append(" and time_stamp <= :endTime");
        String sql = String.format(FIND_BY_ALARM_DEF_SQL, sbWhere);
        // Build query
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId);
        if (startTime != null)
            query.bind("startTime", new Timestamp(startTime.getMillis()));
        if (endTime != null)
            query.bind("endTime", new Timestamp(endTime.getMillis()));
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        return query.map(new BeanMapper<>(AlarmStateHistory.class)).list();
    }
}
#end_block

#method_before
@Test
public void shouldCreateAndFind() throws Exception {
    create("bob", "123", AlarmState.UNDETERMINED, AlarmState.ALARM, "foo", "bar", new DateTime());
    assertEquals(repo.findById("bob", "123", null, null).size(), 1);
}
#method_after
@Test
public void shouldCreateAndFind() throws Exception {
    create("bob", "123", AlarmState.UNDETERMINED, AlarmState.ALARM, "foo", "bar", new DateTime());
    assertEquals(repo.findById("bob", "123", null, 1).size(), 1);
}
#end_block

#method_before
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, String limit) {
    try (Handle h = db.open()) {
        // Build sql
        StringBuilder sbWhere = new StringBuilder();
        if (name != null)
            sbWhere.append(" and def.name = :name");
        String sql = String.format(FIND_BY_METRIC_DEF_SQL, MetricQueries.buildJoinClauseFor(dimensions), sbWhere);
        // Build query
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null)
            query.bind("name", name);
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        // Execute query
        List<Map<String, Object>> rows = query.list();
        // Build results
        List<MetricDefinition> metricDefs = new ArrayList<>(rows.size());
        byte[] currentId = null;
        Map<String, String> dims = null;
        for (Map<String, Object> row : rows) {
            byte[] defId = (byte[]) row.get("id");
            String metricName = (String) row.get("name");
            String dName = (String) row.get("dname");
            String dValue = (String) row.get("dvalue");
            if (defId == null || !Arrays.equals(currentId, defId)) {
                currentId = defId;
                dims = new HashMap<>();
                if (dName != null && dValue != null)
                    dims.put(dName, dValue);
                metricDefs.add(new MetricDefinition(metricName, dims));
            } else
                dims.put(dName, dValue);
        }
        return metricDefs;
    }
}
#method_after
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, int limit) {
    try (Handle h = db.open()) {
        // Build sql
        StringBuilder sbWhere = new StringBuilder();
        if (name != null)
            sbWhere.append(" and def.name = :name");
        String sql = String.format(FIND_BY_METRIC_DEF_SQL, MetricQueries.buildJoinClauseFor(dimensions), sbWhere);
        // Build query
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null)
            query.bind("name", name);
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        // Execute query
        List<Map<String, Object>> rows = query.list();
        // Build results
        List<MetricDefinition> metricDefs = new ArrayList<>(rows.size());
        byte[] currentId = null;
        Map<String, String> dims = null;
        for (Map<String, Object> row : rows) {
            byte[] defId = (byte[]) row.get("id");
            String metricName = (String) row.get("name");
            String dName = (String) row.get("dname");
            String dValue = (String) row.get("dvalue");
            if (defId == null || !Arrays.equals(currentId, defId)) {
                currentId = defId;
                dims = new HashMap<>();
                if (dName != null && dValue != null)
                    dims.put(dName, dValue);
                metricDefs.add(new MetricDefinition(metricName, dims));
            } else
                dims.put(dName, dValue);
        }
        return metricDefs;
    }
}
#end_block

#method_before
// @SuppressWarnings("unchecked")
// public void shouldQueryWithDefaultParams() throws Exception {
// client()
// .resource(
// "/v2.0/metrics/statistics?name=cpu_utilization&start_time=2013-11-20T18:43Z&dimensions=service:hpcs.compute,%20instance_id:123&statistics=avg,%20min,%20max&period=60")
// .header("X-Tenant-Id", "abc").get(ClientResponse.class);
// verify(statisticRepo).find(anyString(), anyString(), any(Map.class), any(DateTime.class),
// any(DateTime.class), any(List.class), anyInt(), null, null);
// }
public void queryShouldThrowOnInvalidDateFormat() throws Exception {
    ClientResponse response = client().resource("/v2.0/metrics/statistics?name=cpu_utilization&dimensions=service:hpcs.compute,%20instance_id:123&start_time=2013-1120&statistics=avg").header("X-Tenant-Id", "abc").get(ClientResponse.class);
    assertEquals(response.getStatus(), 422);
}
#method_after
public void queryShouldThrowOnInvalidDateFormat() throws Exception {
    ClientResponse response = client().resource("/v2.0/metrics/statistics?name=cpu_utilization&dimensions=service:hpcs.compute,%20instance_id:123&start_time=2013-1120&statistics=avg").header("X-Tenant-Id", "abc").get(ClientResponse.class);
    assertEquals(response.getStatus(), 422);
}
#end_block

#method_before
@Override
public List<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, String limit) throws Exception {
    // Todo. Need Influxdb 9 to support limit on points.
    String q = String.format("select value %1$s where %2$s %3$s %4$s %5$s %6$s %7$s %8$s", this.influxV9Utils.namePart(name, true), this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.regionPart(this.region), this.influxV9Utils.startTimePart(startTime), this.influxV9Utils.dimPart(dimensions), this.influxV9Utils.endTimePart(endTime), this.influxV9Utils.timeOffsetPart(offset), this.influxV9Utils.limitPart(limit));
    logger.debug("Measurements query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<Measurements> measurementsList = measurementsList(series);
    logger.debug("Found {} metrics matching query", measurementsList.size());
    return measurementsList;
}
#method_after
@Override
public List<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, @Nullable String offset, int limit, Boolean mergeMetricsFlag) throws Exception {
    String q = buildQuery(tenantId, name, dimensions, startTime, endTime, offset, limit, mergeMetricsFlag);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<Measurements> measurementsList = measurementsList(series);
    logger.debug("Found {} metrics matching query", measurementsList.size());
    return measurementsList;
}
#end_block

#method_before
private List<Measurements> measurementsList(Series series) {
    List<Measurements> measurementsList = new LinkedList<>();
    if (!series.isEmpty()) {
        // Influxdb is returning all series back in one series.
        for (Serie serie : series.getSeries()) {
            // Influxdb 0.9.0 does not return dimensions at this time.
            Measurements measurements = new Measurements(serie.getName(), new HashMap());
            for (String[] values : serie.getValues()) {
                measurements.addMeasurement(new Object[] { values[0], Double.parseDouble(values[1]) });
            }
            measurementsList.add(measurements);
        }
    }
    return measurementsList;
}
#method_after
private List<Measurements> measurementsList(Series series) {
    List<Measurements> measurementsList = new LinkedList<>();
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            Measurements measurements = new Measurements(serie.getName(), serie.getTags());
            for (String[] values : serie.getValues()) {
                measurements.addMeasurement(new Object[] { values[0], values[0], Double.parseDouble(values[1]), getValueMeta(values) });
            }
            measurementsList.add(measurements);
        }
    }
    return measurementsList;
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object list(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("offset") String offset, @QueryParam("limit") String limit) {
    return Links.paginate(this.persistUtils.getLimit(limit), Links.hydrate(repo.find(tenantId, offset, limit), uriInfo), uriInfo);
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object list(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("offset") String offset, @QueryParam("limit") String limit) {
    return Links.paginate(this.persistUtils.getLimit(limit), Links.hydrate(repo.find(tenantId, offset, this.persistUtils.getLimit(limit)), uriInfo), uriInfo);
}
#end_block

#method_before
public String namePart(String name, boolean isRequired) throws Exception {
    if (isRequired) {
        if (name == null || name.isEmpty()) {
            throw new Exception(String.format("Found null or empty name: %1$s", name));
        }
    }
    if (name == null || name.isEmpty()) {
        return "";
    } else {
        sanitize(name);
        return String.format(" from \"%1$s\"", name);
    }
}
#method_after
public String namePart(String name, boolean isRequired) throws Exception {
    if (isRequired) {
        if (name == null || name.isEmpty()) {
            throw new Exception(String.format("Found null or empty name: %1$s", name));
        }
    }
    if (name == null || name.isEmpty()) {
        return "";
    } else {
        return String.format(" from \"%1$s\"", sanitize(name));
    }
}
#end_block

#method_before
public String tenantIdPart(String tenantId) throws Exception {
    if (tenantId == null || tenantId.isEmpty()) {
        throw new Exception(String.format("Found null or empty tenant id: %1$s", tenantId));
    }
    sanitize(tenantId);
    return " tenant_id=" + "'" + tenantId + "'";
}
#method_after
public String tenantIdPart(String tenantId) throws Exception {
    if (tenantId == null || tenantId.isEmpty()) {
        throw new Exception(String.format("Found null or empty tenant id: %1$s", tenantId));
    }
    return " tenant_id=" + "'" + sanitize(tenantId) + "'";
}
#end_block

#method_before
public String timeOffsetPart(String offset) {
    if (offset == null || offset.isEmpty()) {
        return "";
    }
    return String.format(" and time > %1$s", offset);
}
#method_after
public String timeOffsetPart(String offset) {
    if (offset == null || offset.isEmpty()) {
        return "";
    }
    return String.format(" and time > '%1$s'", offset);
}
#end_block

#method_before
public String regionPart(String region) throws Exception {
    if (region == null || region.isEmpty()) {
        throw new Exception(String.format("Found null or empty region: %1$s", region));
    }
    sanitize(region);
    return " and region=" + "'" + region + "'";
}
#method_after
public String regionPart(String region) throws Exception {
    if (region == null || region.isEmpty()) {
        throw new Exception(String.format("Found null or empty region: %1$s", region));
    }
    return " and region=" + "'" + sanitize(region) + "'";
}
#end_block

#method_before
public String dimPart(Map<String, String> dims) throws Exception {
    StringBuilder sb = new StringBuilder();
    if (dims != null && !dims.isEmpty()) {
        for (String k : dims.keySet()) {
            String v = dims.get(k);
            if (k != null && !k.isEmpty() && v != null && !v.isEmpty()) {
                sanitize(k);
                sanitize(v);
                sb.append(" and " + k + "=" + "'" + v + "'");
            }
        }
    }
    return sb.toString();
}
#method_after
public String dimPart(Map<String, String> dims) throws Exception {
    StringBuilder sb = new StringBuilder();
    if (dims != null && !dims.isEmpty()) {
        for (String k : dims.keySet()) {
            String v = dims.get(k);
            if (k != null && !k.isEmpty() && v != null && !v.isEmpty()) {
                sb.append(" and " + sanitize(k) + "=" + "'" + sanitize(v) + "'");
            }
        }
    }
    return sb.toString();
}
#end_block

#method_before
public String limitPart(String limit) {
    // We add 1 to limit to determine if we need to insert a next link.
    return String.format(" limit %1$d", this.persistUtils.getLimit(limit) + 1);
}
#method_after
public String limitPart(int limit) {
    // We add 1 to limit to determine if we need to insert a next link.
    return String.format(" limit %1$d", limit + 1);
}
#end_block

#method_before
public int startIndex(String offset) {
    // Influxdb offsets start at 1.
    if (offset == null || offset.isEmpty()) {
        return 1;
    }
    // We've already returned up to offset, so return offset + 1.
    return Integer.parseInt(offset) + 1;
}
#method_after
public int startIndex(String offset) {
    if (offset == null || offset.isEmpty()) {
        return 0;
    }
    // We've already returned up to offset, so return offset + 1.
    return Integer.parseInt(offset) + 1;
}
#end_block

#method_before
public String periodPart(int period) {
    return period >= 1 ? String.format(" group by time(%1$ds)", period) : "";
}
#method_after
public String periodPart(int period) {
    return period > 0 ? String.format(" group by time(%1$ds) fill(0)", period) : " group by time(300s) fill(0)";
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object list(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) {
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateDimensions(dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), Links.hydrate(repo.find(tenantId, name, dimensions, offset, limit), uriInfo), uriInfo);
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object list(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) {
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateDimensions(dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), Links.hydrate(repo.find(tenantId, name, dimensions, offset, this.persistUtils.getLimit(limit)), uriInfo), uriInfo);
}
#end_block

#method_before
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, String offset, String limit, boolean enforceLimit) {
    try (Handle h = db.open()) {
        StringBuilder sbWhere = new StringBuilder();
        if (alarmDefId != null) {
            sbWhere.append("and ad.id = :alarmDefId ");
        }
        if (metricName != null) {
            sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id " + "inner join (select distinct id from metric_definition " + "            where name = :metricName) as md " + "on md.id = mdd.metric_definition_id ");
            buildJoinClauseFor(metricDimensions, sbWhere);
            sbWhere.append(")");
        }
        if (state != null) {
            sbWhere.append(" and a.state = :state");
        }
        if (offset != null) {
            sbWhere.append(" and a.id > :offset");
        }
        String limitPart = "";
        if (enforceLimit) {
            limitPart = " limit :limit";
        }
        String sql = String.format(ALARM_SQL, sbWhere, limitPart);
        final Query<Map<String, Object>> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        if (offset != null) {
            q.bind("offset", offset);
        }
        if (enforceLimit) {
            q.bind("limit", this.persistUtils.getLimit(limit) + 1);
        }
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        final long start = System.currentTimeMillis();
        final List<Map<String, Object>> rows = q.list();
        logger.debug("Query took {} milliseconds", System.currentTimeMillis() - start);
        final List<Alarm> alarms = createAlarms(tenantId, rows);
        return alarms;
    }
}
#method_after
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, String offset, int limit, boolean enforceLimit) {
    try (Handle h = db.open()) {
        StringBuilder sbWhere = new StringBuilder();
        if (alarmDefId != null) {
            sbWhere.append("and ad.id = :alarmDefId ");
        }
        if (metricName != null) {
            sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id " + "inner join (select distinct id from metric_definition " + "            where name = :metricName) as md " + "on md.id = mdd.metric_definition_id ");
            buildJoinClauseFor(metricDimensions, sbWhere);
            sbWhere.append(")");
        }
        if (state != null) {
            sbWhere.append(" and a.state = :state");
        }
        if (offset != null) {
            sbWhere.append(" and a.id > :offset");
        }
        String limitPart = "";
        if (enforceLimit) {
            limitPart = " limit :limit";
        }
        String sql = String.format(ALARM_SQL, sbWhere, limitPart);
        final Query<Map<String, Object>> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        if (offset != null) {
            q.bind("offset", offset);
        }
        if (enforceLimit) {
            q.bind("limit", limit + 1);
        }
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        final long start = System.currentTimeMillis();
        final List<Map<String, Object>> rows = q.list();
        logger.debug("Query took {} milliseconds", System.currentTimeMillis() - start);
        final List<Alarm> alarms = createAlarms(tenantId, rows);
        return alarms;
    }
}
#end_block

#method_before
public void shouldFindWithoutDimensions() throws Exception {
    Collection<Measurements> measurements = repo.find("bob", "cpu_utilization", null, new DateTime(2014, 1, 1, 0, 0, 0), null, null, null);
    assertEquals(measurements.size(), 3);
}
#method_after
public void shouldFindWithoutDimensions() throws Exception {
    Collection<Measurements> measurements = repo.find("bob", "cpu_utilization", null, new DateTime(2014, 1, 1, 0, 0, 0), null, null, 1, false);
    assertEquals(measurements.size(), 3);
}
#end_block

#method_before
public void shouldFindWithDimensions() throws Exception {
    Map<String, String> dims = new HashMap<>();
    dims.put("service", "compute");
    dims.put("instance_id", "123");
    Collection<Measurements> measurements = repo.find("bob", "cpu_utilization", dims, new DateTime(2014, 1, 1, 0, 0), null, null, null);
    assertEquals(measurements.size(), 2);
    dims.put("flavor_id", "2");
    measurements = repo.find("bob", "cpu_utilization", dims, new DateTime(2014, 1, 1, 0, 0), null, null, null);
    assertEquals(measurements.size(), 1);
}
#method_after
public void shouldFindWithDimensions() throws Exception {
    Map<String, String> dims = new HashMap<>();
    dims.put("service", "compute");
    dims.put("instance_id", "123");
    Collection<Measurements> measurements = repo.find("bob", "cpu_utilization", dims, new DateTime(2014, 1, 1, 0, 0), null, null, 1, false);
    assertEquals(measurements.size(), 2);
    dims.put("flavor_id", "2");
    measurements = repo.find("bob", "cpu_utilization", dims, new DateTime(2014, 1, 1, 0, 0), null, null, 1, false);
    assertEquals(measurements.size(), 1);
}
#end_block

#method_before
public void run() {
    String event;
    while (true) {
        try {
            event = queue.take();
            bindSocket();
            if (publisher != null) {
                try {
                    LOGGER.log(Level.FINE, String.format("Publishing ZMQ event"));
                    publisher.send(event.getBytes(), 0);
                } catch (ZMQException e) {
                    LOGGER.log(Level.INFO, "Unable to send event. " + e.toString(), e);
                }
            } else {
                LOGGER.log(Level.WARNING, String.format("ZMQ Publisher is NULL"));
            }
        }// Catch all exceptions so that this thread does not die.
         catch (Exception e) {
            LOGGER.log(Level.SEVERE, "Unhandled exception publishing ZMQ events " + e.toString(), e);
        }
    }
}
#method_after
public void run() {
    String event;
    while (true) {
        try {
            event = queue.take();
            bindSocket();
            if (publisher != null) {
                try {
                    LOGGER.log(Level.FINE, "Publishing ZMQ event: " + event);
                    publisher.send(event.getBytes(), 0);
                } catch (ZMQException e) {
                    LOGGER.log(Level.INFO, "Unable to send event. " + e.toString(), e);
                }
            } else {
                LOGGER.log(Level.WARNING, "ZMQ Publisher is NULL");
            }
        }// Catch all exceptions so that this thread does not die.
         catch (Exception e) {
            LOGGER.log(Level.SEVERE, "Unhandled exception publishing ZMQ events " + e.toString(), e);
        }
    }
}
#end_block

#method_before
@Test(expectedExceptions = EntityExistsException.class)
public void testPatchSameName() {
    final String alarmDefId = "123";
    String exprStr = EXPR1 + " or " + EXPR2;
    List<String> alarmActions = Arrays.asList("1", "2", "3");
    List<String> okActions = Arrays.asList("2", "3");
    List<String> undeterminedActions = Arrays.asList("3");
    List<String> matchBy = Arrays.asList("service", "instance_id");
    AlarmDefinition firstAlarmDef = new AlarmDefinition(alarmDefId, "91% CPU", "description1", "LOW", exprStr, matchBy, true, alarmActions, okActions, undeterminedActions);
    AlarmDefinition secondAlarmDef = new AlarmDefinition(alarmDefId, "92% CPU", "description2", "LOW", exprStr, matchBy, true, alarmActions, okActions, undeterminedActions);
    when(repo.findById(TENANT_ID, secondAlarmDef.getId())).thenReturn(secondAlarmDef);
    when(repo.findById(TENANT_ID, firstAlarmDef.getId())).thenReturn(firstAlarmDef);
    when(repo.exists(TENANT_ID, "91% CPU")).thenReturn(true);
    when(notificationMethodRepo.exists(eq(TENANT_ID), anyString())).thenReturn(true);
    service.patch(TENANT_ID, secondAlarmDef.getId(), "91% CPU", "foo", "LOW", exprStr, null, matchBy, true, alarmActions, okActions, undeterminedActions);
}
#method_after
@Test(expectedExceptions = EntityExistsException.class)
public void testPatchSameName() {
    String exprStr = EXPR1 + " or " + EXPR2;
    List<String> alarmActions = Arrays.asList("1", "2", "3");
    List<String> okActions = Arrays.asList("2", "3");
    List<String> undeterminedActions = Arrays.asList("3");
    List<String> matchBy = Arrays.asList("service", "instance_id");
    AlarmDefinition firstAlarmDef = new AlarmDefinition("123", "91% CPU", "description1", "LOW", exprStr, matchBy, true, alarmActions, okActions, undeterminedActions);
    AlarmDefinition secondAlarmDef = new AlarmDefinition("234", "92% CPU", "description2", "LOW", exprStr, matchBy, true, alarmActions, okActions, undeterminedActions);
    when(repo.findById(TENANT_ID, secondAlarmDef.getId())).thenReturn(secondAlarmDef);
    when(repo.findById(TENANT_ID, firstAlarmDef.getId())).thenReturn(firstAlarmDef);
    when(repo.exists(TENANT_ID, "91% CPU")).thenReturn("123");
    when(notificationMethodRepo.exists(eq(TENANT_ID), anyString())).thenReturn(true);
    service.patch(TENANT_ID, secondAlarmDef.getId(), firstAlarmDef.getName(), "foo", "LOW", exprStr, null, matchBy, true, alarmActions, okActions, undeterminedActions);
}
#end_block

#method_before
public AlarmDefinition create(String tenantId, String name, @Nullable String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, List<String> alarmActions, @Nullable List<String> okActions, @Nullable List<String> undeterminedActions) {
    // Assert no alarm exists by the name
    if (repo.exists(tenantId, name))
        throw new EntityExistsException("An alarm definition already exists for project / tenant: %s named: %s", tenantId, name);
    assertActionsExist(tenantId, alarmActions, okActions, undeterminedActions);
    Map<String, AlarmSubExpression> subAlarms = new HashMap<String, AlarmSubExpression>();
    for (AlarmSubExpression subExpression : alarmExpression.getSubExpressions()) subAlarms.put(UUID.randomUUID().toString(), subExpression);
    String alarmDefId = UUID.randomUUID().toString();
    AlarmDefinition alarm = null;
    try {
        LOG.debug("Creating alarm definition {} for tenant {}", name, tenantId);
        alarm = repo.create(tenantId, alarmDefId, name, description, severity, expression, subAlarms, matchBy, alarmActions, okActions, undeterminedActions);
        // Notify interested parties of new alarm
        String event = Serialization.toJson(new AlarmDefinitionCreatedEvent(tenantId, alarmDefId, name, description, expression, subAlarms, matchBy));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
        return alarm;
    } catch (Exception e) {
        if (alarm != null)
            try {
                repo.deleteById(tenantId, alarm.getId());
            } catch (Exception ignore) {
            }
        throw Exceptions.uncheck(e, "Error creating alarm definition for project / tenant %s", tenantId);
    }
}
#method_after
public AlarmDefinition create(String tenantId, String name, @Nullable String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, List<String> alarmActions, @Nullable List<String> okActions, @Nullable List<String> undeterminedActions) {
    // Assert no alarm exists by the name
    String alarmDefID = repo.exists(tenantId, name);
    if (alarmDefID != null) {
        throw new EntityExistsException("An alarm definition already exists for project / tenant: %s named: %s", tenantId, name);
    }
    assertActionsExist(tenantId, alarmActions, okActions, undeterminedActions);
    Map<String, AlarmSubExpression> subAlarms = new HashMap<String, AlarmSubExpression>();
    for (AlarmSubExpression subExpression : alarmExpression.getSubExpressions()) subAlarms.put(UUID.randomUUID().toString(), subExpression);
    String alarmDefId = UUID.randomUUID().toString();
    AlarmDefinition alarm = null;
    try {
        LOG.debug("Creating alarm definition {} for tenant {}", name, tenantId);
        alarm = repo.create(tenantId, alarmDefId, name, description, severity, expression, subAlarms, matchBy, alarmActions, okActions, undeterminedActions);
        // Notify interested parties of new alarm
        String event = Serialization.toJson(new AlarmDefinitionCreatedEvent(tenantId, alarmDefId, name, description, expression, subAlarms, matchBy));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
        return alarm;
    } catch (Exception e) {
        if (alarm != null)
            try {
                repo.deleteById(tenantId, alarm.getId());
            } catch (Exception ignore) {
            }
        throw Exceptions.uncheck(e, "Error creating alarm definition for project / tenant %s", tenantId);
    }
}
#end_block

#method_before
public AlarmDefinition update(String tenantId, String alarmDefId, AlarmExpression alarmExpression, UpdateAlarmDefinitionCommand command) {
    final AlarmDefinition oldAlarmDefinition = assertAlarmDefinitionExists(tenantId, alarmDefId, command.alarmActions, command.okActions, command.undeterminedActions);
    final SubExpressions subExpressions = subExpressionsFor(repo.findSubExpressions(alarmDefId), alarmExpression);
    validateChangesAllowed(command.matchBy, oldAlarmDefinition, subExpressions);
    updateInternal(tenantId, alarmDefId, false, command.name, command.description, command.expression, command.matchBy, command.severity, alarmExpression, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions, subExpressions);
    return new AlarmDefinition(alarmDefId, command.name, command.description, command.severity, command.expression, command.matchBy, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions);
}
#method_after
public AlarmDefinition update(String tenantId, String alarmDefId, AlarmExpression alarmExpression, UpdateAlarmDefinitionCommand command) {
    final AlarmDefinition oldAlarmDefinition = assertAlarmDefinitionExists(tenantId, alarmDefId, command.alarmActions, command.okActions, command.undeterminedActions);
    final SubExpressions subExpressions = subExpressionsFor(repo.findSubExpressions(alarmDefId), alarmExpression);
    String alarmID = repo.exists(tenantId, command.name);
    if (alarmID != null && !alarmID.equalsIgnoreCase(alarmDefId)) {
        throw new EntityExistsException("An alarm definition with the same name already exists for project / tenant: %s named: %s", tenantId, command.name);
    }
    validateChangesAllowed(command.matchBy, oldAlarmDefinition, subExpressions);
    updateInternal(tenantId, alarmDefId, false, command.name, command.description, command.expression, command.matchBy, command.severity, alarmExpression, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions, subExpressions);
    return new AlarmDefinition(alarmDefId, command.name, command.description, command.severity, command.expression, command.matchBy, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions);
}
#end_block

#method_before
public AlarmDefinition patch(String tenantId, String alarmDefId, String name, String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, Boolean enabled, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions) {
    AlarmDefinition oldAlarmDefinition = assertAlarmDefinitionExists(tenantId, alarmDefId, alarmActions, okActions, undeterminedActions);
    name = name == null ? oldAlarmDefinition.getName() : name;
    if (repo.exists(tenantId, name)) {
        throw new EntityExistsException("An alarm definition with the same name already exists for project / tenant: %s named: %s", tenantId, name);
    }
    description = description == null ? oldAlarmDefinition.getDescription() : description;
    expression = expression == null ? oldAlarmDefinition.getExpression() : expression;
    severity = severity == null ? oldAlarmDefinition.getSeverity() : severity;
    alarmExpression = alarmExpression == null ? AlarmExpression.of(expression) : alarmExpression;
    enabled = enabled == null ? oldAlarmDefinition.isActionsEnabled() : enabled;
    matchBy = matchBy == null ? oldAlarmDefinition.getMatchBy() : matchBy;
    final SubExpressions subExpressions = subExpressionsFor(repo.findSubExpressions(alarmDefId), alarmExpression);
    validateChangesAllowed(matchBy, oldAlarmDefinition, subExpressions);
    updateInternal(tenantId, alarmDefId, true, name, description, expression, matchBy, severity, alarmExpression, enabled, alarmActions, okActions, undeterminedActions, subExpressions);
    return new AlarmDefinition(alarmDefId, name, description, severity, expression, matchBy, enabled, alarmActions == null ? oldAlarmDefinition.getAlarmActions() : alarmActions, okActions == null ? oldAlarmDefinition.getOkActions() : okActions, undeterminedActions == null ? oldAlarmDefinition.getUndeterminedActions() : undeterminedActions);
}
#method_after
public AlarmDefinition patch(String tenantId, String alarmDefId, String name, String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, Boolean enabled, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions) {
    AlarmDefinition oldAlarmDefinition = assertAlarmDefinitionExists(tenantId, alarmDefId, alarmActions, okActions, undeterminedActions);
    name = name == null ? oldAlarmDefinition.getName() : name;
    String alarmID = repo.exists(tenantId, name);
    if (alarmID != null && !alarmID.equalsIgnoreCase(alarmDefId)) {
        throw new EntityExistsException("An alarm definition with the same name already exists for project / tenant: %s named: %s", tenantId, name);
    }
    description = description == null ? oldAlarmDefinition.getDescription() : description;
    expression = expression == null ? oldAlarmDefinition.getExpression() : expression;
    severity = severity == null ? oldAlarmDefinition.getSeverity() : severity;
    alarmExpression = alarmExpression == null ? AlarmExpression.of(expression) : alarmExpression;
    enabled = enabled == null ? oldAlarmDefinition.isActionsEnabled() : enabled;
    matchBy = matchBy == null ? oldAlarmDefinition.getMatchBy() : matchBy;
    final SubExpressions subExpressions = subExpressionsFor(repo.findSubExpressions(alarmDefId), alarmExpression);
    validateChangesAllowed(matchBy, oldAlarmDefinition, subExpressions);
    updateInternal(tenantId, alarmDefId, true, name, description, expression, matchBy, severity, alarmExpression, enabled, alarmActions, okActions, undeterminedActions, subExpressions);
    return new AlarmDefinition(alarmDefId, name, description, severity, expression, matchBy, enabled, alarmActions == null ? oldAlarmDefinition.getAlarmActions() : alarmActions, okActions == null ? oldAlarmDefinition.getOkActions() : okActions, undeterminedActions == null ? oldAlarmDefinition.getUndeterminedActions() : undeterminedActions);
}
#end_block

#method_before
private void updateInternal(String tenantId, String alarmDefId, boolean patch, String name, String description, String expression, List<String> matchBy, String severity, AlarmExpression alarmExpression, Boolean enabled, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions, SubExpressions subExpressions) {
    if (repo.exists(tenantId, name)) {
        throw new EntityExistsException("An alarm definition with the same name already exists for project / tenant: %s named: %s", tenantId, name);
    }
    try {
        LOG.debug("Updating alarm definition {} for tenant {}", name, tenantId);
        repo.update(tenantId, alarmDefId, patch, name, description, expression, matchBy, severity, enabled, subExpressions.oldAlarmSubExpressions.keySet(), subExpressions.changedSubExpressions, subExpressions.newAlarmSubExpressions, alarmActions, okActions, undeterminedActions);
        // Notify interested parties of updated alarm
        String event = Serialization.toJson(new AlarmDefinitionUpdatedEvent(tenantId, alarmDefId, name, description, expression, matchBy, enabled, severity, subExpressions.oldAlarmSubExpressions, subExpressions.changedSubExpressions, subExpressions.unchangedSubExpressions, subExpressions.newAlarmSubExpressions));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
    } catch (Exception e) {
        throw Exceptions.uncheck(e, "Error updating alarm definition for project / tenant %s", tenantId);
    }
}
#method_after
private void updateInternal(String tenantId, String alarmDefId, boolean patch, String name, String description, String expression, List<String> matchBy, String severity, AlarmExpression alarmExpression, Boolean enabled, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions, SubExpressions subExpressions) {
    try {
        LOG.debug("Updating alarm definition {} for tenant {}", name, tenantId);
        repo.update(tenantId, alarmDefId, patch, name, description, expression, matchBy, severity, enabled, subExpressions.oldAlarmSubExpressions.keySet(), subExpressions.changedSubExpressions, subExpressions.newAlarmSubExpressions, alarmActions, okActions, undeterminedActions);
        // Notify interested parties of updated alarm
        String event = Serialization.toJson(new AlarmDefinitionUpdatedEvent(tenantId, alarmDefId, name, description, expression, matchBy, enabled, severity, subExpressions.oldAlarmSubExpressions, subExpressions.changedSubExpressions, subExpressions.unchangedSubExpressions, subExpressions.newAlarmSubExpressions));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
    } catch (Exception e) {
        throw Exceptions.uncheck(e, "Error updating alarm definition for project / tenant %s", tenantId);
    }
}
#end_block

#method_before
@JsonProperty
public void setValueMeta(Map<String, String> valueMeta) {
    this.valueMeta = valueMeta == null || valueMeta.isEmpty() ? null : ValueMetaValidation.normalize(valueMeta);
}
#method_after
@JsonProperty
public void setValueMeta(Map<String, String> valueMeta) {
    this.valueMeta = ValueMetaValidation.normalize(valueMeta);
}
#end_block

#method_before
public static Map<String, String> normalize(Map<String, String> valueMeta) {
    if (valueMeta == null) {
        return null;
    }
    final Map<String, String> result = new HashMap<>();
    for (Map.Entry<String, String> entry : valueMeta.entrySet()) {
        final String key = CharMatcher.WHITESPACE.trimFrom(entry.getKey());
        result.put(key, entry.getValue());
    }
    return result;
}
#method_after
public static Map<String, String> normalize(Map<String, String> valueMeta) {
    if (valueMeta == null || valueMeta.isEmpty()) {
        return EMPTY_VALUE_META;
    }
    final Map<String, String> result = new HashMap<>();
    for (Map.Entry<String, String> entry : valueMeta.entrySet()) {
        final String key = CharMatcher.WHITESPACE.trimFrom(entry.getKey());
        result.put(key, entry.getValue());
    }
    return result;
}
#end_block

#method_before
public static void validate(Map<String, String> valueMetas) {
    if (valueMetas.size() > VALUE_META_MAX_NUMBER) {
        throw Exceptions.unprocessableEntity("Number of valueMeta  must be %d or less", VALUE_META_MAX_NUMBER);
    }
    // Validate valueMeta names and values
    for (Map.Entry<String, String> valueMeta : valueMetas.entrySet()) {
        // Have to check for null first because later check is for trimmed name
        if (valueMeta.getKey() == null) {
            throw Exceptions.unprocessableEntity("valueMeta name cannot be empty");
        }
        final String name = CharMatcher.WHITESPACE.trimFrom(valueMeta.getKey());
        String value = valueMeta.getValue();
        // General validations
        if (Strings.isNullOrEmpty(name)) {
            throw Exceptions.unprocessableEntity("valueMeta name cannot be empty");
        }
        if (Strings.isNullOrEmpty(value)) {
            throw Exceptions.unprocessableEntity("valueMeta %s cannot have an empty value", name);
        }
        if (name.length() > VALUE_META_NAME_MAX_LENGTH) {
            throw Exceptions.unprocessableEntity("valueMeta name %s must be %d characters or less", name, VALUE_META_NAME_MAX_LENGTH);
        }
        if (value.length() > VALUE_META_VALUE_MAX_LENGTH) {
            throw Exceptions.unprocessableEntity("valueMeta value %s must be %d characters or less", value, VALUE_META_VALUE_MAX_LENGTH);
        }
    }
}
#method_after
public static void validate(Map<String, String> valueMetas) {
    if (valueMetas.size() > VALUE_META_MAX_NUMBER) {
        throw Exceptions.unprocessableEntity("Maximum number of valueMeta key/value parirs is %d", VALUE_META_MAX_NUMBER);
    }
    // Validate valueMeta names and values
    for (Map.Entry<String, String> valueMeta : valueMetas.entrySet()) {
        // Have to check for null first because later check is for trimmed name
        if (valueMeta.getKey() == null) {
            throw Exceptions.unprocessableEntity("valueMeta name cannot be empty");
        }
        final String name = CharMatcher.WHITESPACE.trimFrom(valueMeta.getKey());
        String value = valueMeta.getValue();
        // General validations
        if (Strings.isNullOrEmpty(name)) {
            throw Exceptions.unprocessableEntity("valueMeta name cannot be empty");
        }
        if (Strings.isNullOrEmpty(value)) {
            throw Exceptions.unprocessableEntity("valueMeta %s cannot have an empty value", name);
        }
        if (name.length() > VALUE_META_NAME_MAX_LENGTH) {
            throw Exceptions.unprocessableEntity("valueMeta name %s must be %d characters or less", name, VALUE_META_NAME_MAX_LENGTH);
        }
        if (value.length() > VALUE_META_VALUE_MAX_LENGTH) {
            throw Exceptions.unprocessableEntity("valueMeta value %s must be %d characters or less", value, VALUE_META_VALUE_MAX_LENGTH);
        }
    }
}
#end_block

#method_before
private List<Measurements> buildMeasurementList(List<Serie> result) throws Exception {
    List<Measurements> measurementsList = new LinkedList<>();
    for (Serie serie : result) {
        InfluxV8Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new InfluxV8Utils.SerieNameDecoder(serie.getName());
        } catch (InfluxV8Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Measurements measurements = new Measurements();
        measurements.setName(serieNameDecoder.getMetricName());
        measurements.setDimensions(serieNameDecoder.getDimensions());
        List<Object[]> valObjArryList = new LinkedList<>();
        for (Map<String, Object> row : serie.getRows()) {
            Object[] objArry = new Object[4];
            // sequence_number
            objArry[0] = ((Double) row.get("sequence_number")).longValue();
            // time
            Double timeDouble = (Double) row.get("time");
            // last id wins. ids should be in descending order.
            measurements.setId(String.valueOf(timeDouble.longValue()));
            objArry[1] = DATETIME_FORMATTER.print(timeDouble.longValue());
            // value
            objArry[2] = (Double) row.get("value");
            final Map<String, String> valueMeta = new HashMap<String, String>();
            if (serie.getColumns().length > 3) {
                for (int i = 0; i < serie.getColumns().length; i++) {
                    final String key = serie.getColumns()[i];
                    if ("value".equals(key) || "sequence_number".equals(key) || "time".equals(key)) {
                        continue;
                    }
                    final Object valueObj = row.get(key);
                    if (valueObj != null) {
                        // Have to use String.valueOf() because Influx will store stings like "9.0" as
                        // Doubles
                        final String metaValue = String.valueOf(valueObj);
                        valueMeta.put(key, metaValue);
                    }
                }
            }
            objArry[3] = valueMeta;
            valObjArryList.add(objArry);
        }
        measurements.setMeasurements(valObjArryList);
        measurementsList.add(measurements);
    }
    return measurementsList;
}
#method_after
@SuppressWarnings("unchecked")
private List<Measurements> buildMeasurementList(List<Serie> result) throws Exception {
    List<Measurements> measurementsList = new LinkedList<>();
    for (Serie serie : result) {
        final InfluxV8Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new InfluxV8Utils.SerieNameDecoder(serie.getName());
        } catch (InfluxV8Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Measurements measurements = new Measurements();
        measurements.setName(serieNameDecoder.getMetricName());
        measurements.setDimensions(serieNameDecoder.getDimensions());
        List<Object[]> valObjArryList = new LinkedList<>();
        for (Map<String, Object> row : serie.getRows()) {
            Object[] objArry = new Object[4];
            // sequence_number
            objArry[0] = ((Double) row.get("sequence_number")).longValue();
            // time
            Double timeDouble = (Double) row.get("time");
            // last id wins. ids should be in descending order.
            measurements.setId(String.valueOf(timeDouble.longValue()));
            objArry[1] = DATETIME_FORMATTER.print(timeDouble.longValue());
            // value
            objArry[2] = (Double) row.get("value");
            final Map<String, String> valueMeta;
            final String valueMetaJsonStr = (String) row.get("value_meta");
            if (valueMetaJsonStr == null) {
                valueMeta = EMPTY_VALUE_META;
            } else {
                valueMeta = (Map<String, String>) OBJECT_MAPPER.readValue(valueMetaJsonStr, VALUE_META_TYPE);
            }
            objArry[3] = valueMeta;
            valObjArryList.add(objArry);
        }
        measurements.setMeasurements(valObjArryList);
        measurementsList.add(measurements);
    }
    return measurementsList;
}
#end_block

#method_before
@Override
protected void write() throws Exception {
    this.influxV9RepoWriter.write(getInfluxPointArry());
}
#method_after
@Override
protected void write() throws Exception {
// this.influxV9RepoWriter.write(getInfluxPointArry());
}
#end_block

#method_before
private InfluxPoint[] getInfluxPointArry() throws Exception {
    List<InfluxPoint> influxPointList = new LinkedList<>();
    for (AlarmStateTransitionedEvent event : this.alarmStateTransitionedEventList) {
        Map<String, Object> valueMap = new HashMap<>();
        valueMap.put("tenant_id", event.tenantId);
        valueMap.put("alarm_id", event.alarmId);
        valueMap.put("metrics", this.objectMapper.writeValueAsString(event.metrics));
        valueMap.put("old_state", event.oldState);
        valueMap.put("new_state", event.newState);
        valueMap.put("sub_alarms", event.subAlarms);
        valueMap.put("reason", event.stateChangeReason);
        valueMap.put("reason_data", "{}");
        DateTime dateTime = new DateTime(event.timestamp * 1000, DateTimeZone.UTC);
        String dateString = this.dateFormatter.print(dateTime);
        InfluxPoint influxPoint = new InfluxPoint(ALARM_STATE_HISTORY_NAME, new HashMap(), dateString, valueMap);
        influxPointList.add(influxPoint);
    }
    return influxPointList.toArray(new InfluxPoint[influxPointList.size()]);
}
#method_after
private InfluxPoint[] getInfluxPointArry() throws Exception {
    List<InfluxPoint> influxPointList = new LinkedList<>();
    for (AlarmStateTransitionedEvent event : this.alarmStateTransitionedEventList) {
        Map<String, Object> valueMap = new HashMap<>();
        valueMap.put("tenant_id", event.tenantId);
        valueMap.put("alarm_id", event.alarmId);
        valueMap.put("metrics", this.objectMapper.writeValueAsString(event.metrics));
        valueMap.put("old_state", event.oldState);
        valueMap.put("new_state", event.newState);
        valueMap.put("sub_alarms", this.objectMapper.writeValueAsString(event.subAlarms));
        valueMap.put("reason", event.stateChangeReason);
        valueMap.put("reason_data", "{}");
        DateTime dateTime = new DateTime(event.timestamp * 1000, DateTimeZone.UTC);
        String dateString = this.dateFormatter.print(dateTime);
        InfluxPoint influxPoint = new InfluxPoint(ALARM_STATE_HISTORY_NAME, new HashMap(), dateString, valueMap);
        influxPointList.add(influxPoint);
    }
    return influxPointList.toArray(new InfluxPoint[influxPointList.size()]);
}
#end_block

#method_before
public void shouldBuiltStateChangeReason() {
    AlarmExpression expr = new AlarmExpression("avg(hpcs.compute{instance_id=5,metric_name=cpu,device=1}, 1) > 5 times 3 OR avg(hpcs.compute{flavor_id=3,metric_name=mem}, 2) < 4 times 3");
    Alarm alarm = new Alarm();
    List<AlarmTransitionSubAlarm> transitionSubAlarms = new ArrayList<>();
    transitionSubAlarms.add(new AlarmTransitionSubAlarm(expr.getSubExpressions().get(0).getExpression(), AlarmState.UNDETERMINED, new ArrayList<Double>()));
    transitionSubAlarms.add(new AlarmTransitionSubAlarm(expr.getSubExpressions().get(1).getExpression(), AlarmState.ALARM, new ArrayList<Double>()));
    alarm.setTransitionSubAlarms(transitionSubAlarms);
    assertEquals(alarm.buildStateChangeReason(AlarmState.UNDETERMINED), "No data was present for the sub-alarms: avg(hpcs.compute{device=1, instance_id=5, metric_name=cpu}, 1) > 5.0 times 3");
    assertEquals(alarm.buildStateChangeReason(AlarmState.ALARM), "Thresholds were exceeded for the sub-alarms: avg(hpcs.compute{flavor_id=3, metric_name=mem}, 2) < 4.0 times 3 with the values: []");
}
#method_after
public void shouldBuiltStateChangeReason() {
    AlarmExpression expr = new AlarmExpression("avg(hpcs.compute{instance_id=5,metric_name=cpu,device=1}, 1) > 5 times 3 OR avg(hpcs.compute{flavor_id=3,metric_name=mem}, 2) < 4 times 3");
    Alarm alarm = new Alarm();
    List<AlarmTransitionSubAlarm> transitionSubAlarms = new ArrayList<>();
    transitionSubAlarms.add(new AlarmTransitionSubAlarm(expr.getSubExpressions().get(0), AlarmState.UNDETERMINED, new ArrayList<Double>()));
    transitionSubAlarms.add(new AlarmTransitionSubAlarm(expr.getSubExpressions().get(1), AlarmState.ALARM, new ArrayList<Double>()));
    alarm.setTransitionSubAlarms(transitionSubAlarms);
    assertEquals(alarm.buildStateChangeReason(AlarmState.UNDETERMINED), "No data was present for the sub-alarms: avg(hpcs.compute{device=1, instance_id=5, metric_name=cpu}, 1) > 5.0 times 3");
    assertEquals(alarm.buildStateChangeReason(AlarmState.ALARM), "Thresholds were exceeded for the sub-alarms: avg(hpcs.compute{flavor_id=3, metric_name=mem}, 2) < 4.0 times 3 with the values: []");
}
#end_block

#method_before
public boolean evaluate(AlarmExpression expression) {
    transitionSubAlarms.clear();
    AlarmState initialState = state;
    boolean unitialized = false;
    for (SubAlarm subAlarm : subAlarms.values()) {
        if (AlarmState.UNDETERMINED.equals(subAlarm.getState())) {
            unitialized = true;
        }
        transitionSubAlarms.add(new AlarmTransitionSubAlarm(subAlarm.getExpression().toString(), subAlarm.getState(), subAlarm.getCurrentValues()));
    }
    // Handle UNDETERMINED state
    if (unitialized) {
        if (AlarmState.UNDETERMINED.equals(initialState)) {
            return false;
        }
        state = AlarmState.UNDETERMINED;
        stateChangeReason = buildStateChangeReason(state);
        return true;
    }
    Map<AlarmSubExpression, Boolean> subExpressionValues = new HashMap<AlarmSubExpression, Boolean>();
    for (SubAlarm subAlarm : subAlarms.values()) {
        subExpressionValues.put(subAlarm.getExpression(), AlarmState.ALARM.equals(subAlarm.getState()));
    }
    // Handle ALARM state
    if (expression.evaluate(subExpressionValues)) {
        if (AlarmState.ALARM.equals(initialState)) {
            return false;
        }
        state = AlarmState.ALARM;
        stateChangeReason = buildStateChangeReason(state);
        return true;
    }
    if (AlarmState.OK.equals(initialState)) {
        return false;
    }
    state = AlarmState.OK;
    stateChangeReason = buildStateChangeReason(state);
    return true;
}
#method_after
public boolean evaluate(AlarmExpression expression) {
    transitionSubAlarms.clear();
    AlarmState initialState = state;
    boolean uninitialized = false;
    for (SubAlarm subAlarm : subAlarms.values()) {
        if (AlarmState.UNDETERMINED.equals(subAlarm.getState())) {
            uninitialized = true;
        }
        transitionSubAlarms.add(new AlarmTransitionSubAlarm(subAlarm.getExpression(), subAlarm.getState(), subAlarm.getCurrentValues()));
    }
    // Handle UNDETERMINED state
    if (uninitialized) {
        if (AlarmState.UNDETERMINED.equals(initialState)) {
            return false;
        }
        state = AlarmState.UNDETERMINED;
        stateChangeReason = buildStateChangeReason(state);
        return true;
    }
    Map<AlarmSubExpression, Boolean> subExpressionValues = new HashMap<AlarmSubExpression, Boolean>();
    for (SubAlarm subAlarm : subAlarms.values()) {
        subExpressionValues.put(subAlarm.getExpression(), AlarmState.ALARM.equals(subAlarm.getState()));
    }
    // Handle ALARM state
    if (expression.evaluate(subExpressionValues)) {
        if (AlarmState.ALARM.equals(initialState)) {
            return false;
        }
        state = AlarmState.ALARM;
        stateChangeReason = buildStateChangeReason(state);
        return true;
    }
    if (AlarmState.OK.equals(initialState)) {
        return false;
    }
    state = AlarmState.OK;
    stateChangeReason = buildStateChangeReason(state);
    return true;
}
#end_block

#method_before
boolean evaluate() {
    double[] values = stats.getViewValues();
    boolean thresholdExceeded = false;
    boolean hasEmptyWindows = false;
    subAlarm.getCurrentValues().clear();
    for (double value : values) {
        if (Double.isNaN(value)) {
            hasEmptyWindows = true;
        } else {
            subAlarm.getCurrentValues().add(value);
            emptyWindowObservations = 0;
            // Check if value is OK
            if (!subAlarm.getExpression().getOperator().evaluate(value, subAlarm.getExpression().getThreshold())) {
                if (!shouldSendStateChange(AlarmState.OK)) {
                    return false;
                }
                setSubAlarmState(AlarmState.OK);
                return true;
            } else
                thresholdExceeded = true;
        }
    }
    if (thresholdExceeded && !hasEmptyWindows) {
        if (!shouldSendStateChange(AlarmState.ALARM)) {
            return false;
        }
        setSubAlarmState(AlarmState.ALARM);
        return true;
    }
    // Window is empty at this point
    emptyWindowObservations++;
    if ((emptyWindowObservations >= emptyWindowObservationThreshold) && shouldSendStateChange(AlarmState.UNDETERMINED) && !subAlarm.isSporadicMetric()) {
        setSubAlarmState(AlarmState.UNDETERMINED);
        return true;
    }
    return false;
}
#method_after
boolean evaluate() {
    double[] values = stats.getViewValues();
    boolean thresholdExceeded = false;
    boolean hasEmptyWindows = false;
    subAlarm.clearCurrentValues();
    for (double value : values) {
        if (Double.isNaN(value)) {
            hasEmptyWindows = true;
        } else {
            subAlarm.addCurrentValue(value);
            emptyWindowObservations = 0;
            // Check if value is OK
            if (!subAlarm.getExpression().getOperator().evaluate(value, subAlarm.getExpression().getThreshold())) {
                if (!shouldSendStateChange(AlarmState.OK)) {
                    return false;
                }
                setSubAlarmState(AlarmState.OK);
                return true;
            } else
                thresholdExceeded = true;
        }
    }
    if (thresholdExceeded && !hasEmptyWindows) {
        if (!shouldSendStateChange(AlarmState.ALARM)) {
            return false;
        }
        setSubAlarmState(AlarmState.ALARM);
        return true;
    }
    // Window is empty at this point
    emptyWindowObservations++;
    if ((emptyWindowObservations >= emptyWindowObservationThreshold) && shouldSendStateChange(AlarmState.UNDETERMINED) && !subAlarm.isSporadicMetric()) {
        setSubAlarmState(AlarmState.UNDETERMINED);
        return true;
    }
    return false;
}
#end_block

#method_before
private String buildSubAlarmJson(Collection<SubAlarm> subAlarms) {
    StringBuilder stringBuilder = new StringBuilder();
    for (SubAlarm subAlarm : subAlarms) {
        if (stringBuilder.length() != 0) {
            stringBuilder.append(",");
        }
        stringBuilder.append("{\"subAlarmExpression\":\"").append(subAlarm.getExpression().getExpression()).append("\",");
        stringBuilder.append("\"subAlarmState\":\"").append(subAlarm.getState()).append("\",");
        stringBuilder.append("\"currentValues\":").append(subAlarm.getCurrentValues()).append("}");
    }
    return stringBuilder.toString();
}
#method_after
private String buildSubAlarmJson(Collection<SubAlarm> subAlarms) {
    StringBuilder stringBuilder = new StringBuilder();
    for (SubAlarm subAlarm : subAlarms) {
        if (stringBuilder.length() != 0) {
            stringBuilder.append(",");
        }
        stringBuilder.append(Serialization.toJson(subAlarm.getExpression())).setCharAt(stringBuilder.length() - 1, ',');
        stringBuilder.append("\"subAlarmState\":\"").append(subAlarm.getState()).append("\",");
        stringBuilder.append("\"currentValues\":").append(subAlarm.getCurrentValues()).append("}");
    }
    return stringBuilder.toString().replace("AlarmSubExpression", "subAlarmExpression");
}
#end_block

#method_before
@Override
public String toString() {
    return "AlarmTransitionAlarmSub [subAlarmExpression=" + subAlarmExpression + ", subAlarmState=" + subAlarmState + ", values=" + currentValues + "]";
}
#method_after
@Override
public String toString() {
    return "AlarmTransitionSubAlarm [subAlarmExpression=" + subAlarmExpression + ", subAlarmState=" + subAlarmState + ", values=" + currentValues + "]";
}
#end_block

#method_before
protected void write(final InfluxPoint[] influxPointArry) throws Exception {
    HttpPost request = new HttpPost(this.influxUrl);
    request.addHeader("content-type", "application/json");
    request.addHeader("Authorization", this.baseAuthHeader);
    InfluxWrite influxWrite = new InfluxWrite(this.influxName, this.influxRetentionPolicy, influxPointArry, new HashMap());
    StringEntity params = new StringEntity(this.objectMapper.writeValueAsString(influxWrite));
    request.setEntity(params);
    try {
        logger.debug("Writing {} points to influxdb database {} at {}", influxPointArry.length, this.influxName, this.influxUrl);
        HttpResponse response = this.httpclient.execute(request);
        int rc = response.getStatusLine().getStatusCode();
        if (rc != HttpStatus.SC_OK) {
            HttpEntity entity = response.getEntity();
            String responseString = EntityUtils.toString(entity, "UTF-8");
            logger.error("Failed to write data to Influxdb: {}", String.valueOf(rc));
            logger.error("Http response: {}", responseString);
            throw new Exception(responseString);
        }
        logger.debug("Successfully wrote {} points to influxdb database {} at {}", influxPointArry.length, this.influxName, this.influxUrl);
    } finally {
        request.releaseConnection();
    }
}
#method_after
protected void write(final InfluxPoint[] influxPointArry) throws Exception {
    HttpPost request = new HttpPost(this.influxUrl);
    request.addHeader("content-type", "application/json");
    request.addHeader("Authorization", this.baseAuthHeader);
    InfluxWrite influxWrite = new InfluxWrite(this.influxName, this.influxRetentionPolicy, influxPointArry, new HashMap());
    StringEntity params = new StringEntity(this.objectMapper.writeValueAsString(influxWrite));
    request.setEntity(params);
    try {
        logger.debug("Writing {} points to influxdb database {} at {}", influxPointArry.length, this.influxName, this.influxUrl);
        HttpResponse response = this.httpClient.execute(request);
        int rc = response.getStatusLine().getStatusCode();
        if (rc != HttpStatus.SC_OK) {
            HttpEntity entity = response.getEntity();
            String responseString = EntityUtils.toString(entity, "UTF-8");
            logger.error("Failed to write data to influx database {} at {}: {}", this.influxName, this.influxUrl, String.valueOf(rc));
            logger.error("Http response: {}", responseString);
            throw new Exception(rc + ":" + responseString);
        }
        logger.debug("Successfully wrote {} points to influx database {} at {}", influxPointArry.length, this.influxName, this.influxUrl);
    } finally {
        request.releaseConnection();
    }
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
public List<AlarmDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset) {
    try (Handle h = db.open()) {
        String query = "select distinct ad.id, ad.description, ad.tenant_id, ad.severity, ad.expression, ad.match_by, ad.name, ad.actions_enabled, ad.created_at, ad.updated_at, ad.deleted_at " + "from alarm_definition ad join sub_alarm_definition sub on ad.id = sub.alarm_definition_id " + "left outer join sub_alarm_definition_dimension dim on sub.id = dim.sub_alarm_definition_id%s " + "where tenant_id = :tenantId and deleted_at is NULL %s order by %s ad.created_at %s";
        StringBuilder sbWhere = new StringBuilder();
        if (name != null) {
            sbWhere.append(" and ad.name = :name");
        }
        if (offset != null) {
            sbWhere.append(" and ad.id > :offset");
        }
        String orderBy = offset != null ? "ad.id," : "";
        String limit = offset != null ? " limit :limit" : "";
        String sql = String.format(query, SubAlarmDefinitionQueries.buildJoinClauseFor(dimensions), sbWhere, orderBy, limit);
        Query<?> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null) {
            q.bind("name", name);
        }
        if (offset != null) {
            q.bind("offset", offset);
            q.bind("limit", Paged.LIMIT);
        }
        q = q.map(new BeanMapper<AlarmDefinition>(AlarmDefinition.class));
        DimensionQueries.bindDimensionsToQuery(q, dimensions);
        List<AlarmDefinition> alarms = (List<AlarmDefinition>) q.list();
        for (AlarmDefinition alarm : alarms) hydrateRelationships(h, alarm);
        return alarms;
    }
}
#method_after
@SuppressWarnings("unchecked")
@Override
public List<AlarmDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset) {
    try (Handle h = db.open()) {
        String query = "SELECT alarmdefinition.*, " + "GROUP_CONCAT(alarm_action.alarm_state) AS states, GROUP_CONCAT(alarm_action.action_id) AS notificationIds FROM (select distinct alarm_definition.id,tenant_id," + "name,description, severity, expression, match_by, actions_enabled, alarm_definition.created_at, alarm_definition.updated_at," + " alarm_definition.deleted_at FROM alarm_definition LEFT OUTER JOIN sub_alarm_definition sub ON alarm_definition.id = sub.alarm_definition_id" + " LEFT OUTER JOIN sub_alarm_definition_dimension dim ON sub.id = dim.sub_alarm_definition_id%s WHERE tenant_id = :tenantId" + " AND deleted_at IS NULL %s ORDER BY %s alarm_definition.created_at %s) AS alarmdefinition LEFT OUTER JOIN alarm_action ON alarmdefinition.id=alarm_action.alarm_definition_id" + " GROUP BY alarmdefinition.id";
        StringBuilder sbWhere = new StringBuilder();
        if (name != null) {
            sbWhere.append(" and alarm_definition.name = :name");
        }
        if (offset != null) {
            sbWhere.append(" and alarm_definition.id > :offset");
        }
        String orderBy = offset != null ? "alarm_definition.id," : "";
        String limit = offset != null ? " limit :limit" : "";
        String sql = String.format(query, SubAlarmDefinitionQueries.buildJoinClauseFor(dimensions), sbWhere, orderBy, limit);
        Query<?> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null) {
            q.bind("name", name);
        }
        if (offset != null) {
            q.bind("offset", offset);
            q.bind("limit", Paged.LIMIT);
        }
        q.registerMapper(new AlarmDefinitionMapper());
        q = q.mapTo(AlarmDefinition.class);
        DimensionQueries.bindDimensionsToQuery(q, dimensions);
        List<AlarmDefinition> resultSet = (List<AlarmDefinition>) q.list();
        return resultSet;
    }
}
#end_block

#method_before
@Override
public AlarmDefinition findById(String tenantId, String alarmDefId) {
    try (Handle h = db.open()) {
        AlarmDefinition alarm = h.createQuery("select * from alarm_definition where tenant_id = :tenantId and id = :id and deleted_at is NULL").bind("tenantId", tenantId).bind("id", alarmDefId).map(new BeanMapper<AlarmDefinition>(AlarmDefinition.class)).first();
        if (alarm == null)
            throw new EntityNotFoundException("No alarm definition exists for %s", alarmDefId);
        hydrateRelationships(h, alarm);
        return alarm;
    }
}
#method_after
@Override
public AlarmDefinition findById(String tenantId, String alarmDefId) {
    try (Handle h = db.open()) {
        String query = "SELECT alarm_definition.id, alarm_definition.tenant_id, alarm_definition.name, alarm_definition.description," + "alarm_definition.expression, alarm_definition.severity, alarm_definition.match_by, alarm_definition.actions_enabled," + " alarm_definition.created_at, alarm_definition.updated_at, alarm_definition.deleted_at, " + "GROUP_CONCAT(alarm_action.action_id) AS notificationIds,group_concat(alarm_action.alarm_state) AS states " + "FROM alarm_definition LEFT OUTER JOIN alarm_action ON alarm_definition.id=alarm_action.alarm_definition_id " + " WHERE alarm_definition.tenant_id=:tenantId AND alarm_definition.id=:alarmDefId AND alarm_definition.deleted_at" + " IS NULL GROUP BY alarm_definition.id";
        Query<?> q = h.createQuery(query);
        q.bind("tenantId", tenantId);
        q.bind("alarmDefId", alarmDefId);
        q.registerMapper(new AlarmDefinitionMapper());
        q = q.mapTo(AlarmDefinition.class);
        AlarmDefinition alarmDefinition = (AlarmDefinition) q.first();
        if (alarmDefinition == null) {
            throw new EntityNotFoundException("No alarm definition exists for %s", alarmDefId);
        }
        return alarmDefinition;
    }
}
#end_block

#method_before
@PATCH
@Timed
@Path("/{alarm_definition_id}")
@Consumes({ "application/json-patch+json", MediaType.APPLICATION_JSON })
@Produces(MediaType.APPLICATION_JSON)
@SuppressWarnings("unchecked")
public AlarmDefinition patch(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_definition_id") String alarmDefinitionId, @NotEmpty Map<String, Object> fields) throws JsonMappingException {
    String name = (String) fields.get("name");
    String description = (String) fields.get("description");
    String severity = (String) fields.get("severity");
    String expression = (String) fields.get("expression");
    List<String> matchBy = (List<String>) fields.get("match_by");
    Boolean enabled = (Boolean) fields.get("actions_enabled");
    List<String> alarmActions = (List<String>) fields.get("alarm_actions");
    List<String> okActions = (List<String>) fields.get("ok_actions");
    List<String> undeterminedActions = (List<String>) fields.get("undetermined_actions");
    AlarmValidation.validate(name, description, severity, alarmActions, okActions, undeterminedActions);
    AlarmExpression alarmExpression = expression == null ? null : AlarmValidation.validateNormalizeAndGet(expression);
    return Links.hydrate(service.patch(tenantId, alarmDefinitionId, name, description, severity, expression, alarmExpression, matchBy, enabled, alarmActions, okActions, undeterminedActions), uriInfo, true);
}
#method_after
@PATCH
@Timed
@Path("/{alarm_definition_id}")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
@SuppressWarnings("unchecked")
public AlarmDefinition patch(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_definition_id") String alarmDefinitionId, @NotEmpty Map<String, Object> fields) throws JsonMappingException {
    String name = (String) fields.get("name");
    String description = (String) fields.get("description");
    String severity = (String) fields.get("severity");
    String expression = (String) fields.get("expression");
    List<String> matchBy = (List<String>) fields.get("match_by");
    Boolean enabled = (Boolean) fields.get("actions_enabled");
    List<String> alarmActions = (List<String>) fields.get("alarm_actions");
    List<String> okActions = (List<String>) fields.get("ok_actions");
    List<String> undeterminedActions = (List<String>) fields.get("undetermined_actions");
    AlarmValidation.validate(name, description, severity, alarmActions, okActions, undeterminedActions);
    AlarmExpression alarmExpression = expression == null ? null : AlarmValidation.validateNormalizeAndGet(expression);
    return Links.hydrate(service.patch(tenantId, alarmDefinitionId, name, description, severity, expression, alarmExpression, matchBy, enabled, alarmActions, okActions, undeterminedActions), uriInfo, true);
}
#end_block

#method_before
public static String tenantIdPart(String tenantId) throws Exception {
    sanitize(tenantId);
    String s = "";
    if (tenantId != null && !tenantId.isEmpty()) {
        s += "tenant_id=" + "'" + tenantId + "'";
    }
    return s;
}
#method_after
public static String tenantIdPart(String tenantId) throws Exception {
    if (tenantId == null || tenantId.isEmpty()) {
        throw new Exception(String.format("Found invalid tenant id: %1$s", tenantId));
    }
    sanitize(tenantId);
    return "tenant_id=" + "'" + tenantId + "'";
}
#end_block

#method_before
protected String read(final String query) throws Exception {
    HttpGet request = new HttpGet(this.influxUrl + "?q=" + URLEncoder.encode(query, "UTF-8") + "&db=" + URLEncoder.encode(this.influxName, "UTF-8"));
    request.addHeader("content-type", "application/json");
    request.addHeader("Authorization", this.baseAuthHeader);
    try {
        logger.debug("Sending query {} to influxdb database {} at {}", query, this.influxName, this.influxUrl);
        HttpResponse response = this.httpClient.execute(request);
        int rc = response.getStatusLine().getStatusCode();
        if (rc != HttpStatus.SC_OK) {
            HttpEntity entity = response.getEntity();
            String responseString = EntityUtils.toString(entity, "UTF-8");
            logger.error("Failed to query Influxdb: {}", String.valueOf(rc));
            logger.error("Http response: {}", responseString);
            throw new Exception(responseString);
        }
        logger.debug("Successfully queried {} influxdb database {} at {}", this.influxName, this.influxUrl);
        HttpEntity entity = response.getEntity();
        return entity != null ? EntityUtils.toString(entity) : null;
    } finally {
        request.releaseConnection();
    }
}
#method_after
protected String read(final String query) throws Exception {
    HttpGet request = new HttpGet(this.influxUrl + "?q=" + URLEncoder.encode(query, "UTF-8") + "&db=" + URLEncoder.encode(this.influxName, "UTF-8"));
    request.addHeader("content-type", "application/json");
    request.addHeader("Authorization", this.baseAuthHeader);
    try {
        logger.debug("Sending query {} to influx database {} at {}", query, this.influxName, this.influxUrl);
        HttpResponse response = this.httpClient.execute(request);
        int rc = response.getStatusLine().getStatusCode();
        if (rc != HttpStatus.SC_OK) {
            HttpEntity entity = response.getEntity();
            String responseString = EntityUtils.toString(entity, "UTF-8");
            logger.error("Failed to query influx database {} at {}: {}", this.influxName, this.influxUrl, String.valueOf(rc));
            logger.error("Http response: {}", responseString);
            throw new Exception(rc + ":" + responseString);
        }
        logger.debug("Successfully queried influx database {} at {}", this.influxName, this.influxUrl);
        HttpEntity entity = response.getEntity();
        return entity != null ? EntityUtils.toString(entity) : null;
    } finally {
        request.releaseConnection();
    }
}
#end_block

#method_before
boolean isEmpty() {
    return this.results[0].rows == null ? true : false;
}
#method_after
boolean isEmpty() {
    return this.results[0].rows == null;
}
#end_block

#method_before
@PATCH
@Timed
@Path("/{alarm_id}")
@Consumes({ "application/json-patch+json", MediaType.APPLICATION_JSON })
@Produces(MediaType.APPLICATION_JSON)
public Alarm patch(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_id") String alarmId, @NotEmpty Map<String, Object> fields) throws JsonMappingException {
    String stateStr = (String) fields.get("state");
    AlarmState state = stateStr == null ? null : Validation.parseAndValidate(AlarmState.class, stateStr);
    return fixAlarmLinks(uriInfo, service.patch(tenantId, alarmId, state));
}
#method_after
@PATCH
@Timed
@Path("/{alarm_id}")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
public Alarm patch(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_id") String alarmId, @NotEmpty Map<String, Object> fields) throws JsonMappingException {
    String stateStr = (String) fields.get("state");
    AlarmState state = stateStr == null ? null : Validation.parseAndValidate(AlarmState.class, stateStr);
    return fixAlarmLinks(uriInfo, service.patch(tenantId, alarmId, state));
}
#end_block

#method_before
@Override
protected void write() throws Exception {
    this.influxV8RepoTrait.write(TimeUnit.SECONDS, getSeries());
}
#method_after
@Override
protected void write() throws Exception {
    this.influxV8RepoWriter.write(TimeUnit.SECONDS, getSeries());
}
#end_block

#method_before
private Serie[] getSeries() throws Exception {
    final List<Serie> serieList = new LinkedList<>();
    for (final Sha1HashId defDimId : this.measurementMap.keySet()) {
        final DefDim defDim = this.defDimMap.get(defDimId);
        final Def def = getDef(defDim.defId);
        final Set<Dim> dimSet = getDimSet(defDim.dimId);
        final Serie.Builder builder = new Serie.Builder(buildSerieName(def, dimSet));
        builder.columns(COL_NAMES_STRING_ARRY);
        for (final Measurement measurement : this.measurementMap.get(defDimId)) {
            final Object[] colValsObjArry = new Object[COL_NAMES_STRING_ARRY.length];
            final Date date = this.simpleDateFormat.parse(measurement.time + " UTC");
            final Long time = date.getTime() / 1000;
            colValsObjArry[0] = time;
            logger.debug("Added column value to colValsObjArry[{}] = {}", 0, colValsObjArry[0]);
            colValsObjArry[1] = measurement.value;
            logger.debug("Added column value to colValsObjArry[{}] = {}", 1, colValsObjArry[1]);
            builder.values(colValsObjArry);
            this.measurementMeter.mark();
        }
        final Serie serie = builder.build();
        if (logger.isDebugEnabled()) {
            this.influxV8RepoTrait.logColValues(serie);
        }
        serieList.add(serie);
        logger.debug("Added serie: {} to serieList", serie.getName());
    }
    return serieList.toArray(new Serie[serieList.size()]);
}
#method_after
private Serie[] getSeries() throws Exception {
    final List<Serie> serieList = new LinkedList<>();
    for (final Sha1HashId defDimId : this.measurementMap.keySet()) {
        final DefDim defDim = this.defDimMap.get(defDimId);
        final Def def = getDef(defDim.defId);
        final Set<Dim> dimSet = getDimSet(defDim.dimId);
        final Serie.Builder builder = new Serie.Builder(buildSerieName(def, dimSet));
        builder.columns(COL_NAMES_STRING_ARRY);
        for (final Measurement measurement : this.measurementMap.get(defDimId)) {
            final Object[] colValsObjArry = new Object[COL_NAMES_STRING_ARRY.length];
            final Date date = this.simpleDateFormat.parse(measurement.time + " UTC");
            final Long time = date.getTime() / 1000;
            colValsObjArry[0] = time;
            logger.debug("Added column value to colValsObjArry[{}] = {}", 0, colValsObjArry[0]);
            colValsObjArry[1] = measurement.value;
            logger.debug("Added column value to colValsObjArry[{}] = {}", 1, colValsObjArry[1]);
            builder.values(colValsObjArry);
            this.measurementMeter.mark();
        }
        final Serie serie = builder.build();
        if (logger.isDebugEnabled()) {
            this.influxV8RepoWriter.logColValues(serie);
        }
        serieList.add(serie);
        logger.debug("Added serie: {} to serieList", serie.getName());
    }
    return serieList.toArray(new Serie[serieList.size()]);
}
#end_block

#method_before
@Override
protected void write() throws JsonProcessingException {
    final Serie.Builder builder = new Serie.Builder(ALARM_STATE_HISTORY_NAME);
    logger.debug("Created serie: {}", ALARM_STATE_HISTORY_NAME);
    builder.columns(COLUMN_NAMES);
    if (logger.isDebugEnabled()) {
        this.influxV8RepoTrait.logColumnNames(COLUMN_NAMES);
    }
    for (AlarmStateTransitionedEvent alarmStateTransitionedEvent : this.alarmStateTransitionedEventList) {
        builder.values(alarmStateTransitionedEvent.tenantId, alarmStateTransitionedEvent.alarmId, this.objectMapper.writeValueAsString(alarmStateTransitionedEvent.metrics), alarmStateTransitionedEvent.oldState, alarmStateTransitionedEvent.newState, alarmStateTransitionedEvent.stateChangeReason, "{}", alarmStateTransitionedEvent.timestamp);
    }
    final Serie[] series = { builder.build() };
    if (logger.isDebugEnabled()) {
        this.influxV8RepoTrait.logColValues(series[0]);
    }
    this.influxV8RepoTrait.write(TimeUnit.SECONDS, series);
}
#method_after
@Override
protected void write() throws JsonProcessingException {
    final Serie.Builder builder = new Serie.Builder(ALARM_STATE_HISTORY_NAME);
    logger.debug("Created serie: {}", ALARM_STATE_HISTORY_NAME);
    builder.columns(COLUMN_NAMES);
    if (logger.isDebugEnabled()) {
        this.influxV8RepoWriter.logColumnNames(COLUMN_NAMES);
    }
    for (AlarmStateTransitionedEvent alarmStateTransitionedEvent : this.alarmStateTransitionedEventList) {
        builder.values(alarmStateTransitionedEvent.tenantId, alarmStateTransitionedEvent.alarmId, this.objectMapper.writeValueAsString(alarmStateTransitionedEvent.metrics), alarmStateTransitionedEvent.oldState, alarmStateTransitionedEvent.newState, alarmStateTransitionedEvent.stateChangeReason, "{}", alarmStateTransitionedEvent.timestamp);
    }
    final Serie[] series = { builder.build() };
    if (logger.isDebugEnabled()) {
        this.influxV8RepoWriter.logColValues(series[0]);
    }
    this.influxV8RepoWriter.write(TimeUnit.SECONDS, series);
}
#end_block

#method_before
@Override
protected void configure() {
    bind(PersisterConfig.class).toInstance(config);
    bind(Environment.class).toInstance(env);
    install(new FactoryModuleBuilder().implement(MetricHandler.class, MetricHandler.class).build(MetricHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionedEventHandler.class, AlarmStateTransitionedEventHandler.class).build(AlarmStateTransitionedEventHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumerRunnableBasic.class, KafkaMetricsConsumerRunnableBasic.class).build(KafkaMetricsConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumerRunnableBasic.class, KafkaAlarmStateTransitionConsumerRunnableBasic.class).build(KafkaAlarmStateTransitionConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumer.class, KafkaMetricsConsumer.class).build(KafkaMetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricPipeline.class, MetricPipeline.class).build(MetricPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionPipeline.class, AlarmStateTransitionPipeline.class).build(AlarmStateTransitionPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionConsumer.class, AlarmStateTransitionConsumer.class).build(AlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumer.class, KafkaAlarmStateTransitionConsumer.class).build(KafkaAlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricsConsumer.class, MetricsConsumer.class).build(MetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaChannel.class, KafkaChannel.class).build(KafkaChannelFactory.class));
    if (config.getDatabaseConfiguration().getDatabaseType().equalsIgnoreCase(VERTICA)) {
        bind(DBI.class).toProvider(DBIProvider.class).in(Scopes.SINGLETON);
        bind(MetricRepo.class).to(VerticaMetricRepo.class);
        bind(AlarmRepo.class).to(VerticaAlarmRepo.class);
    } else if (config.getDatabaseConfiguration().getDatabaseType().equalsIgnoreCase(INFLUXDB)) {
        // Check for null to not break existing configs. If no version, default to V8.
        if (config.getInfluxDBConfiguration().getVersion() == null || config.getInfluxDBConfiguration().getVersion().equalsIgnoreCase(INFLUXDB_V8)) {
            bind(InfluxV8RepoTrait.class);
            bind(MetricRepo.class).to(InfluxV8MetricRepo.class);
            bind(AlarmRepo.class).to(InfluxV8AlarmRepo.class);
        } else if (config.getInfluxDBConfiguration().getVersion().equalsIgnoreCase(INFLUXDB_V9)) {
            bind(InfluxV9RepoTrait.class);
            bind(MetricRepo.class).to(InfluxV9MetricRepo.class);
            bind(AlarmRepo.class).to(InfluxV9AlarmRepo.class);
        } else {
            System.out.println("Found unknown Influxdb version: " + config.getInfluxDBConfiguration().getVersion());
            System.out.println("Supported Influxdb versions are 'v8' and 'v9'");
            System.out.println("Check your config file");
            System.exit(1);
        }
    } else {
        System.out.println("Found unknown database type: " + config.getDatabaseConfiguration().getDatabaseType());
        System.out.println("Supported databases are 'vertica' and 'influxdb'");
        System.out.println("Check your config file.");
        System.exit(1);
    }
}
#method_after
@Override
protected void configure() {
    bind(PersisterConfig.class).toInstance(config);
    bind(Environment.class).toInstance(env);
    install(new FactoryModuleBuilder().implement(MetricHandler.class, MetricHandler.class).build(MetricHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionedEventHandler.class, AlarmStateTransitionedEventHandler.class).build(AlarmStateTransitionedEventHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumerRunnableBasic.class, KafkaMetricsConsumerRunnableBasic.class).build(KafkaMetricsConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumerRunnableBasic.class, KafkaAlarmStateTransitionConsumerRunnableBasic.class).build(KafkaAlarmStateTransitionConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumer.class, KafkaMetricsConsumer.class).build(KafkaMetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricPipeline.class, MetricPipeline.class).build(MetricPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionPipeline.class, AlarmStateTransitionPipeline.class).build(AlarmStateTransitionPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionConsumer.class, AlarmStateTransitionConsumer.class).build(AlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumer.class, KafkaAlarmStateTransitionConsumer.class).build(KafkaAlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricsConsumer.class, MetricsConsumer.class).build(MetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaChannel.class, KafkaChannel.class).build(KafkaChannelFactory.class));
    if (config.getDatabaseConfiguration().getDatabaseType().equalsIgnoreCase(VERTICA)) {
        bind(DBI.class).toProvider(DBIProvider.class).in(Scopes.SINGLETON);
        bind(MetricRepo.class).to(VerticaMetricRepo.class);
        bind(AlarmRepo.class).to(VerticaAlarmRepo.class);
    } else if (config.getDatabaseConfiguration().getDatabaseType().equalsIgnoreCase(INFLUXDB)) {
        // Check for null to not break existing configs. If no version, default to V8.
        if (config.getInfluxDBConfiguration().getVersion() == null || config.getInfluxDBConfiguration().getVersion().equalsIgnoreCase(INFLUXDB_V8)) {
            bind(InfluxV8RepoWriter.class);
            bind(MetricRepo.class).to(InfluxV8MetricRepo.class);
            bind(AlarmRepo.class).to(InfluxV8AlarmRepo.class);
        } else if (config.getInfluxDBConfiguration().getVersion().equalsIgnoreCase(INFLUXDB_V9)) {
            bind(InfluxV9RepoWriter.class);
            bind(MetricRepo.class).to(InfluxV9MetricRepo.class);
            bind(AlarmRepo.class).to(InfluxV9AlarmRepo.class);
        } else {
            System.err.println("Found unknown Influxdb version: " + config.getInfluxDBConfiguration().getVersion());
            System.err.println("Supported Influxdb versions are 'v8' and 'v9'");
            System.err.println("Check your config file");
            System.exit(1);
        }
    } else {
        System.err.println("Found unknown database type: " + config.getDatabaseConfiguration().getDatabaseType());
        System.err.println("Supported databases are 'vertica' and 'influxdb'");
        System.err.println("Check your config file.");
        System.exit(1);
    }
}
#end_block

#method_before
@Override
protected void write() throws Exception {
    this.influxV9RepoTrait.write(getInfluxPointArry());
}
#method_after
@Override
protected void write() throws Exception {
    this.influxV9RepoWriter.write(getInfluxPointArry());
}
#end_block

#method_before
@Override
protected void write() throws Exception {
    this.influxV9RepoTrait.write(getInfluxPointArry());
}
#method_after
@Override
protected void write() throws Exception {
    this.influxV9RepoWriter.write(getInfluxPointArry());
}
#end_block

#method_before
private String buildKey(String metricTenantId, Metric metric) {
    final StringBuilder key = new StringBuilder(metricTenantId);
    key.append('-');
    key.append(metric.name);
    // Dimensions are optional.
    if (metric.dimensions != null && !metric.dimensions.isEmpty()) {
        // in a known order
        for (final Map.Entry<String, String> dim : buildSortedDimSet(metric.dimensions)) {
            key.append('&');
            key.append(dim.getKey());
            key.append('=');
            key.append(dim.getValue());
        }
    }
    String keyValue = key.toString();
    return keyValue;
}
#method_after
private String buildKey(String metricTenantId, Metric metric) {
    final StringBuilder key = new StringBuilder(metricTenantId);
    key.append(metric.name);
    // Dimensions are optional.
    if (metric.dimensions != null && !metric.dimensions.isEmpty()) {
        // in a known order
        for (final Map.Entry<String, String> dim : buildSortedDimSet(metric.dimensions)) {
            key.append(dim.getKey());
            key.append(dim.getValue());
        }
    }
    String keyValue = key.toString();
    return keyValue;
}
#end_block

#method_before
private List<Map.Entry<String, String>> buildSortedDimSet(final Map<String, String> dimMap) {
    final List<Map.Entry<String, String>> dims = new ArrayList<>(dimMap.entrySet());
    Collections.sort(dims, new Comparator<Map.Entry<String, String>>() {

        @Override
        public int compare(Entry<String, String> o1, Entry<String, String> o2) {
            int nameCmp = String.CASE_INSENSITIVE_ORDER.compare(o1.getKey(), o2.getKey());
            return (nameCmp != 0 ? nameCmp : String.CASE_INSENSITIVE_ORDER.compare(o1.getValue(), o2.getValue()));
        }
    });
    return dims;
}
#method_after
private List<Map.Entry<String, String>> buildSortedDimSet(final Map<String, String> dimMap) {
    final List<Map.Entry<String, String>> dims = new ArrayList<>(dimMap.entrySet());
    Collections.sort(dims, new Comparator<Map.Entry<String, String>>() {

        @Override
        public int compare(Entry<String, String> o1, Entry<String, String> o2) {
            int nameCmp = o1.getKey().compareTo(o2.getKey());
            return (nameCmp != 0 ? nameCmp : o1.getValue().compareTo(o2.getValue()));
        }
    });
    return dims;
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
public List<AlarmDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset) {
    try (Handle h = db.open()) {
        String query = "select distinct ad.id, ad.description, ad.tenant_id, ad.severity, ad.expression, ad.match_by, ad.name, ad.actions_enabled, ad.created_at, ad.updated_at, ad.deleted_at " + "from alarm_definition ad join sub_alarm_definition sub on ad.id = sub.alarm_definition_id " + "left outer join sub_alarm_definition_dimension dim on sub.id = dim.sub_alarm_definition_id%s " + "where tenant_id = :tenantId and deleted_at is NULL %s order by %s ad.created_at %s";
        StringBuilder sbWhere = new StringBuilder();
        if (name != null) {
            sbWhere.append(" and ad.name = :name");
        }
        if (offset != null) {
            sbWhere.append(" and ad.id > :offset");
        }
        String orderBy = offset != null ? "ad.id," : "";
        String limit = offset != null ? " limit :limit" : "";
        String sql = String.format(query, SubAlarmQueries.buildJoinClauseFor(dimensions), sbWhere, orderBy, limit);
        Query<?> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null) {
            q.bind("name", name);
        }
        if (offset != null) {
            q.bind("offset", offset);
            q.bind("limit", Paged.limit);
        }
        q = q.map(new BeanMapper<AlarmDefinition>(AlarmDefinition.class));
        DimensionQueries.bindDimensionsToQuery(q, dimensions);
        List<AlarmDefinition> alarms = (List<AlarmDefinition>) q.list();
        for (AlarmDefinition alarm : alarms) hydrateRelationships(h, alarm);
        return alarms;
    }
}
#method_after
@Override
@SuppressWarnings("unchecked")
public List<AlarmDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset) {
    try (Handle h = db.open()) {
        String query = "select distinct ad.id, ad.description, ad.tenant_id, ad.severity, ad.expression, ad.match_by, ad.name, ad.actions_enabled, ad.created_at, ad.updated_at, ad.deleted_at " + "from alarm_definition ad join sub_alarm_definition sub on ad.id = sub.alarm_definition_id " + "left outer join sub_alarm_definition_dimension dim on sub.id = dim.sub_alarm_definition_id%s " + "where tenant_id = :tenantId and deleted_at is NULL %s order by %s ad.created_at %s";
        StringBuilder sbWhere = new StringBuilder();
        if (name != null) {
            sbWhere.append(" and ad.name = :name");
        }
        if (offset != null) {
            sbWhere.append(" and ad.id > :offset");
        }
        String orderBy = offset != null ? "ad.id," : "";
        String limit = offset != null ? " limit :limit" : "";
        String sql = String.format(query, SubAlarmQueries.buildJoinClauseFor(dimensions), sbWhere, orderBy, limit);
        Query<?> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null) {
            q.bind("name", name);
        }
        if (offset != null) {
            q.bind("offset", offset);
            q.bind("limit", Paged.LIMIT);
        }
        q = q.map(new BeanMapper<AlarmDefinition>(AlarmDefinition.class));
        DimensionQueries.bindDimensionsToQuery(q, dimensions);
        List<AlarmDefinition> alarms = (List<AlarmDefinition>) q.list();
        for (AlarmDefinition alarm : alarms) hydrateRelationships(h, alarm);
        return alarms;
    }
}
#end_block

#method_before
public static Object paginate(String offset, List<? extends AbstractEntity> elements, UriInfo uriInfo) {
    if (offset != null) {
        Paged paged = new Paged();
        Link selfLink = new Link();
        selfLink.rel = "self";
        selfLink.href = uriInfo.getRequestUri().toString();
        paged.links.add(selfLink);
        if (elements != null) {
            if (elements.size() >= Paged.limit) {
                Link nextLink = new Link();
                nextLink.rel = "next";
                // Create a new URL with the new offset.
                nextLink.href = uriInfo.getAbsolutePath().toString() + "?offset=" + elements.get(elements.size() - 1).getId();
                // Add the query parms back to the URL without the original offset.
                for (String parmKey : uriInfo.getQueryParameters().keySet()) {
                    if (!parmKey.equalsIgnoreCase("offset")) {
                        List<String> parmValList = uriInfo.getQueryParameters().get(parmKey);
                        for (String parmVal : parmValList) {
                            nextLink.href += "&" + parmKey + "=" + parmVal;
                        }
                    }
                }
                paged.links.add(nextLink);
            }
        }
        paged.elements = elements != null ? elements : new ArrayList();
        return paged;
    } else {
        return elements;
    }
}
#method_after
public static Object paginate(String offset, List<? extends AbstractEntity> elements, UriInfo uriInfo) {
    if (offset != null) {
        Paged paged = new Paged();
        Link selfLink = new Link();
        selfLink.rel = "self";
        selfLink.href = uriInfo.getRequestUri().toString();
        paged.links.add(selfLink);
        if (elements != null) {
            if (elements.size() >= Paged.LIMIT) {
                Link nextLink = new Link();
                nextLink.rel = "next";
                // Create a new URL with the new offset.
                nextLink.href = uriInfo.getAbsolutePath().toString() + "?offset=" + elements.get(elements.size() - 1).getId();
                // Add the query parms back to the URL without the original offset.
                for (String parmKey : uriInfo.getQueryParameters().keySet()) {
                    if (!parmKey.equalsIgnoreCase("offset")) {
                        List<String> parmValList = uriInfo.getQueryParameters().get(parmKey);
                        for (String parmVal : parmValList) {
                            nextLink.href += "&" + parmKey + "=" + parmVal;
                        }
                    }
                }
                paged.links.add(nextLink);
            }
        }
        paged.elements = elements != null ? elements : new ArrayList();
        return paged;
    } else {
        return elements;
    }
}
#end_block

#method_before
public static Object paginateMeasurements(String offset, Measurements measurements, UriInfo uriInfo) {
    if (offset != null) {
        Paged paged = new Paged();
        Link selfLink = new Link();
        selfLink.rel = "self";
        selfLink.href = uriInfo.getRequestUri().toString();
        paged.links.add(selfLink);
        if (measurements.getMeasurements().size() >= Paged.limit) {
            Link nextLink = new Link();
            nextLink.rel = "next";
            // Create a new URL with the new offset.
            nextLink.href = uriInfo.getAbsolutePath().toString() + "?offset=" + measurements.getId();
            // Add the query parms back to the URL without the original offset.
            for (String parmKey : uriInfo.getQueryParameters().keySet()) {
                if (!parmKey.equalsIgnoreCase("offset")) {
                    List<String> parmValList = uriInfo.getQueryParameters().get(parmKey);
                    for (String parmVal : parmValList) {
                        nextLink.href += "&" + parmKey + "=" + parmVal;
                    }
                }
            }
            paged.links.add(nextLink);
        }
        List<Measurements> measurementsList = new ArrayList();
        measurementsList.add(measurements);
        paged.elements = measurementsList;
        return paged;
    } else {
        return measurements;
    }
}
#method_after
public static Object paginateMeasurements(String offset, Measurements measurements, UriInfo uriInfo) throws UnsupportedEncodingException {
    if (offset != null) {
        Paged paged = new Paged();
        Link selfLink = new Link();
        selfLink.rel = "self";
        selfLink.href = uriInfo.getRequestUri().toString();
        paged.links.add(selfLink);
        if (measurements.getMeasurements().size() >= Paged.LIMIT) {
            Link nextLink = new Link();
            nextLink.rel = "next";
            // Create a new URL with the new offset.
            nextLink.href = uriInfo.getAbsolutePath().toString() + "?offset=" + measurements.getId();
            // Add the query parms back to the URL without the original offset and dimensions.
            for (String parmKey : uriInfo.getQueryParameters().keySet()) {
                if (!parmKey.equalsIgnoreCase("offset") && !parmKey.equalsIgnoreCase("dimensions")) {
                    List<String> parmValList = uriInfo.getQueryParameters().get(parmKey);
                    for (String parmVal : parmValList) {
                        nextLink.href += "&" + parmKey + "=" + parmVal;
                    }
                }
            }
            // Add the dimensions for this particular measurement.
            Map<String, String> dimensionsMap = measurements.getDimensions();
            if (dimensionsMap != null && !dimensionsMap.isEmpty()) {
                nextLink.href += "&dimensions=";
                boolean firstDimension = true;
                for (String dimensionKey : dimensionsMap.keySet()) {
                    String dimensionVal = dimensionsMap.get(dimensionKey);
                    if (firstDimension) {
                        firstDimension = false;
                    } else {
                        nextLink.href += URLEncoder.encode(",", "UTF-8");
                    }
                    nextLink.href += dimensionKey + URLEncoder.encode(":", "UTF-8") + dimensionVal;
                }
            }
            paged.links.add(nextLink);
        }
        List<Measurements> measurementsList = new ArrayList();
        measurementsList.add(measurements);
        paged.elements = measurementsList;
        return paged;
    } else {
        return measurements;
    }
}
#end_block

#method_before
@Override
public List<NotificationMethod> find(String tenantId, String offset) {
    try (Handle h = db.open()) {
        if (offset != null) {
            return h.createQuery("select * from notification_method where tenant_id = :tenantId and id > :offset order by id asc limit :limit").bind("tenantId", tenantId).bind("offset", offset).bind("limit", Paged.limit).map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
        } else {
            return h.createQuery("select * from notification_method where tenant_id = :tenantId").bind("tenantId", tenantId).map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
        }
    }
}
#method_after
@Override
public List<NotificationMethod> find(String tenantId, String offset) {
    try (Handle h = db.open()) {
        if (offset != null) {
            return h.createQuery("select * from notification_method where tenant_id = :tenantId and id > :offset order by id asc limit :limit").bind("tenantId", tenantId).bind("offset", offset).bind("limit", Paged.LIMIT).map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
        } else {
            return h.createQuery("select * from notification_method where tenant_id = :tenantId").bind("tenantId", tenantId).map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
        }
    }
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object get(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("start_time") String startTimeStr, @QueryParam("end_time") String endTimeStr, @QueryParam("offset") String offset) throws Exception {
    // Validate query parameters
    DateTime startTime = Validation.parseAndValidateDate(startTimeStr, "start_time", true);
    DateTime endTime = Validation.parseAndValidateDate(endTimeStr, "end_time", false);
    Validation.validateTimes(startTime, endTime);
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    List<Measurements> measurementsList = repo.find(tenantId, name, dimensions, startTime, endTime, offset);
    List<Object> pagedList = new LinkedList();
    for (Measurements measurements : measurementsList) {
        pagedList.add(Links.paginateMeasurements(offset, measurements, uriInfo));
    }
    Paged paged = new Paged();
    paged.elements = pagedList;
    return paged;
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object get(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("start_time") String startTimeStr, @QueryParam("end_time") String endTimeStr, @QueryParam("offset") String offset) throws Exception {
    // Validate query parameters
    DateTime startTime = Validation.parseAndValidateDate(startTimeStr, "start_time", true);
    DateTime endTime = Validation.parseAndValidateDate(endTimeStr, "end_time", false);
    Validation.validateTimes(startTime, endTime);
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    List<Measurements> measurementsList = repo.find(tenantId, name, dimensions, startTime, endTime, offset);
    List<Object> pagedList = new LinkedList();
    for (Measurements measurements : measurementsList) {
        pagedList.add(Links.paginateMeasurements(offset, measurements, uriInfo));
    }
    if (offset != null) {
        Paged paged = new Paged();
        paged.elements = pagedList;
        return paged;
    } else {
        return pagedList;
    }
}
#end_block

#method_before
private List<MetricDefinition> buildMetricDefList(List<Serie> result, String offset) throws Exception {
    List<MetricDefinition> metricDefinitionList = new ArrayList<>();
    for (Serie serie : result) {
        for (Map<String, Object> point : serie.getRows()) {
            String encodedMetricName = (String) point.get("name");
            if (offset != null) {
                if (encodedMetricName.compareTo(offset) <= 0) {
                    continue;
                }
            }
            Utils.SerieNameDecoder serieNameDecoder;
            try {
                serieNameDecoder = new Utils.SerieNameDecoder(encodedMetricName);
            } catch (Utils.SerieNameDecodeException e) {
                logger.warn("Dropping series name that is not decodable: {}", point.get("name"), e);
                continue;
            }
            MetricDefinition metricDefinition = new MetricDefinition(serieNameDecoder.getMetricName(), serieNameDecoder.getDimensions());
            metricDefinition.setId(encodedMetricName);
            metricDefinitionList.add(metricDefinition);
            if (offset != null) {
                if (metricDefinitionList.size() >= Paged.limit) {
                    return metricDefinitionList;
                }
            }
        }
    }
    return metricDefinitionList;
}
#method_after
private List<MetricDefinition> buildMetricDefList(List<Serie> result, String offset) throws Exception {
    List<MetricDefinition> metricDefinitionList = new ArrayList<>();
    for (Serie serie : result) {
        for (Map<String, Object> point : serie.getRows()) {
            String encodedMetricName = (String) point.get("name");
            if (offset != null) {
                if (encodedMetricName.compareTo(offset) <= 0) {
                    continue;
                }
            }
            Utils.SerieNameDecoder serieNameDecoder;
            try {
                serieNameDecoder = new Utils.SerieNameDecoder(encodedMetricName);
            } catch (Utils.SerieNameDecodeException e) {
                logger.warn("Dropping series name that is not decodable: {}", point.get("name"), e);
                continue;
            }
            MetricDefinition metricDefinition = new MetricDefinition(serieNameDecoder.getMetricName(), serieNameDecoder.getDimensions());
            metricDefinition.setId(encodedMetricName);
            metricDefinitionList.add(metricDefinition);
            if (offset != null) {
                if (metricDefinitionList.size() >= Paged.LIMIT) {
                    return metricDefinitionList;
                }
            }
        }
    }
    return metricDefinitionList;
}
#end_block

#method_before
public static String buildOffsetPart(String offset) {
    if (offset != null) {
        if (!offset.isEmpty()) {
            return " and time < " + offset + "ms limit " + Paged.limit;
        } else {
            return " limit " + Paged.limit;
        }
    } else {
        return "";
    }
}
#method_after
public static String buildOffsetPart(String offset) {
    if (offset != null) {
        if (!offset.isEmpty()) {
            return " and time < " + offset + "ms limit " + Paged.LIMIT;
        } else {
            return " limit " + Paged.LIMIT;
        }
    } else {
        return "";
    }
}
#end_block

#method_before
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, String offset) {
    try (Handle h = db.open()) {
        StringBuilder sbWhere = new StringBuilder();
        if (alarmDefId != null) {
            sbWhere.append("and ad.id = :alarmDefId ");
        }
        if (metricName != null) {
            sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id " + "inner join (select distinct id from metric_definition " + "            where name = :metricName) as md " + "on md.id = mdd.metric_definition_id ");
            buildJoinClauseFor(metricDimensions, sbWhere);
            sbWhere.append(")");
        }
        if (state != null) {
            sbWhere.append(" and a.state = :state");
        }
        if (offset != null) {
            sbWhere.append(" and a.id > :offset");
        }
        String limit = offset != null ? " limit :limit" : "";
        String sql = String.format(ALARM_SQL, sbWhere, limit);
        final Query<Map<String, Object>> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        if (offset != null) {
            q.bind("offset", offset);
            q.bind("limit", Paged.limit);
        }
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        final long start = System.currentTimeMillis();
        final List<Map<String, Object>> rows = q.list();
        logger.debug("Query took {} milliseconds", System.currentTimeMillis() - start);
        final List<Alarm> alarms = createAlarms(tenantId, rows);
        return alarms;
    }
}
#method_after
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, String offset) {
    try (Handle h = db.open()) {
        StringBuilder sbWhere = new StringBuilder();
        if (alarmDefId != null) {
            sbWhere.append("and ad.id = :alarmDefId ");
        }
        if (metricName != null) {
            sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id " + "inner join (select distinct id from metric_definition " + "            where name = :metricName) as md " + "on md.id = mdd.metric_definition_id ");
            buildJoinClauseFor(metricDimensions, sbWhere);
            sbWhere.append(")");
        }
        if (state != null) {
            sbWhere.append(" and a.state = :state");
        }
        if (offset != null) {
            sbWhere.append(" and a.id > :offset");
        }
        String limit = offset != null ? " limit :limit" : "";
        String sql = String.format(ALARM_SQL, sbWhere, limit);
        final Query<Map<String, Object>> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        if (offset != null) {
            q.bind("offset", offset);
            q.bind("limit", Paged.LIMIT);
        }
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        final long start = System.currentTimeMillis();
        final List<Map<String, Object>> rows = q.list();
        logger.debug("Query took {} milliseconds", System.currentTimeMillis() - start);
        final List<Alarm> alarms = createAlarms(tenantId, rows);
        return alarms;
    }
}
#end_block

#method_before
private List<Measurements> buildMeasurementList(List<Serie> result) throws Exception {
    List<Measurements> measurementsList = new LinkedList<>();
    for (Serie serie : result) {
        Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new Utils.SerieNameDecoder(serie.getName());
        } catch (Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Measurements measurements = new Measurements();
        measurements.setName(serieNameDecoder.getMetricName());
        measurements.setDimensions(serieNameDecoder.getDimensions());
        List<Object[]> valObjArryList = new LinkedList<>();
        for (Map<String, Object> row : serie.getRows()) {
            Object[] objArry = new Object[3];
            // sequence_number
            objArry[0] = ((Double) row.get(serie.getColumns()[1])).longValue();
            // time
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            // last id wins. ids should be in descending order.
            measurements.setId(new Long(timeDouble.longValue()).toString());
            objArry[1] = DATETIME_FORMATTER.print(timeDouble.longValue());
            // value
            objArry[2] = (Double) row.get(serie.getColumns()[2]);
            valObjArryList.add(objArry);
        }
        measurements.setMeasurements(valObjArryList);
        measurementsList.add(measurements);
    }
    return measurementsList;
}
#method_after
private List<Measurements> buildMeasurementList(List<Serie> result) throws Exception {
    List<Measurements> measurementsList = new LinkedList<>();
    for (Serie serie : result) {
        Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new Utils.SerieNameDecoder(serie.getName());
        } catch (Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Measurements measurements = new Measurements();
        measurements.setName(serieNameDecoder.getMetricName());
        measurements.setDimensions(serieNameDecoder.getDimensions());
        List<Object[]> valObjArryList = new LinkedList<>();
        for (Map<String, Object> row : serie.getRows()) {
            Object[] objArry = new Object[3];
            // sequence_number
            objArry[0] = ((Double) row.get(serie.getColumns()[1])).longValue();
            // time
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            // last id wins. ids should be in descending order.
            measurements.setId(String.valueOf(timeDouble.longValue()));
            objArry[1] = DATETIME_FORMATTER.print(timeDouble.longValue());
            // value
            objArry[2] = (Double) row.get(serie.getColumns()[2]);
            valObjArryList.add(objArry);
        }
        measurements.setMeasurements(valObjArryList);
        measurementsList.add(measurements);
    }
    return measurementsList;
}
#end_block

#method_before
public void postProcessResult(File result) throws MojoExecutionException {
    // First transform the cover page
    transformCover();
    final File targetDirectory = result.getParentFile();
    try {
        final URL containerURL = getClass().getResource("/epub/container.xml");
        FileUtils.copyURLToFile(containerURL, new File(targetDirectory, "META-INF" + File.separator + "container.xml"));
    } catch (IOException e) {
        throw new MojoExecutionException("Unable to copy hardcoded container.xml file", e);
    }
    // copy the images directory so it gets zipped
    try {
        FileUtils.mkdir(targetDirectory.toString() + "/images");
        FileUtils.copyDirectory(getImageDirectory(), new File(targetDirectory.toString() + "/images"));
    } catch (IOException e) {
        throw new MojoExecutionException("Unable to copy images directory", e);
    }
    // make PNGs from SVG images
    rasterize(new File(targetDirectory.toString() + "/images"));
    // copy mimetype file
    try {
        final URL mimetypeURL = getClass().getResource("/epub/mimetype");
        FileUtils.copyURLToFile(mimetypeURL, new File(targetDirectory, "mimetype"));
    } catch (IOException e) {
        throw new MojoExecutionException("Unable to copy hardcoded mimetype file", e);
    }
    try {
        ZipArchiver zipArchiver = new ZipArchiver();
        zipArchiver.addDirectory(targetDirectory);
        // seems to not be a problem to have mimetype compressed
        zipArchiver.setCompress(true);
        // copy it to parent dir
        zipArchiver.setDestFile(new File(targetDirectory.getParentFile(), result.getName()));
        zipArchiver.createArchive();
        getLog().debug("epub file created at: " + zipArchiver.getDestFile().getAbsolutePath());
    } catch (Exception e) {
        throw new MojoExecutionException("Unable to zip epub file", e);
    }
}
#method_after
public void postProcessResult(File result) throws MojoExecutionException {
    // First transform the cover page
    transformCover();
    final File targetDirectory = result.getParentFile();
    try {
        final URL containerURL = getClass().getResource("/epub/container.xml");
        FileUtils.copyURLToFile(containerURL, new File(targetDirectory, "META-INF" + File.separator + "container.xml"));
    } catch (IOException e) {
        throw new MojoExecutionException("Unable to copy hardcoded container.xml file", e);
    }
    // copy the images and figures directories so they gets zipped
    try {
        copyImages(sourceDirectory.toString(), targetDirectory.toString());
    } catch (IOException e) {
        throw new MojoExecutionException("Unable to copy images/figures directory", e);
    }
    // make PNGs from SVG images
    rasterize(new File(targetDirectory.toString() + "/images"));
    // copy mimetype file
    try {
        final URL mimetypeURL = getClass().getResource("/epub/mimetype");
        FileUtils.copyURLToFile(mimetypeURL, new File(targetDirectory, "mimetype"));
    } catch (IOException e) {
        throw new MojoExecutionException("Unable to copy hardcoded mimetype file", e);
    }
    // do the CSS things
    copyTemplate(targetDirectory);
    try {
        ZipArchiver zipArchiver = new ZipArchiver();
        zipArchiver.addDirectory(targetDirectory);
        // seems to not be a problem to have mimetype compressed
        zipArchiver.setCompress(true);
        // copy it to parent dir
        zipArchiver.setDestFile(new File(targetDirectory.getParentFile(), result.getName()));
        zipArchiver.createArchive();
        getLog().debug("epub file created at: " + zipArchiver.getDestFile().getAbsolutePath());
    } catch (Exception e) {
        throw new MojoExecutionException("Unable to zip epub file", e);
    }
}
#end_block

#method_before
public void adjustTransformer(Transformer transformer, String sourceFilename, File targetFile) {
    GitHelper.addCommitProperties(transformer, projectBuildDirectory, 7, getLog());
    super.adjustTransformer(transformer, sourceFilename, targetFile);
    transformer.setParameter("branding", branding);
    transformer.setParameter("project.build.directory", projectBuildDirectory.toURI().toString());
    if (security != null) {
        transformer.setParameter("security", security);
    }
    if (trimWadlUriCount != null) {
        transformer.setParameter("trim.wadl.uri.count", trimWadlUriCount);
    }
    // 
    // Setup graphics paths
    // 
    sourceDocBook = new File(sourceFilename);
    sourceDirectory = sourceDocBook.getParentFile();
    File imageDirectory = getImageDirectory();
    File calloutDirectory = new File(imageDirectory, "callouts");
    transformer.setParameter("docbook.infile", sourceDocBook.toURI().toString());
    transformer.setParameter("source.directory", sourceDirectory.toURI().toString());
    transformer.setParameter("compute.wadl.path.from.docbook.path", computeWadlPathFromDocbookPath);
    transformer.setParameter("admon.graphics.path", imageDirectory.toURI().toString());
    transformer.setParameter("callout.graphics.path", calloutDirectory.toURI().toString());
    // 
    // Setup the background image file
    // 
    File cloudSub = new File(imageDirectory, "cloud");
    File ccSub = new File(imageDirectory, "cc");
    coverImage = new File(cloudSub, COVER_IMAGE_NAME);
    coverImageTemplate = new File(cloudSub, COVER_IMAGE_TEMPLATE_NAME);
    coverImageTemplate = new File(cloudSub, branding + "-cover.st");
    transformer.setParameter("cloud.api.background.image", coverImage.toURI().toString());
    transformer.setParameter("cloud.api.cc.image.dir", ccSub.toURI().toString());
}
#method_after
public void adjustTransformer(Transformer transformer, String sourceFilename, File targetFile) {
    GitHelper.addCommitProperties(transformer, projectBuildDirectory, 7, getLog());
    super.adjustTransformer(transformer, sourceFilename, targetFile);
    transformer.setParameter("branding", branding);
    transformer.setParameter("html.stylesheet", "common/css/positioning.css");
    transformer.setParameter("project.build.directory", projectBuildDirectory.toURI().toString());
    transformer.setParameter("imgSrcPath", ".");
    if (security != null) {
        transformer.setParameter("security", security);
    }
    if (trimWadlUriCount != null) {
        transformer.setParameter("trim.wadl.uri.count", trimWadlUriCount);
    }
    // 
    // Setup graphics paths
    // 
    sourceDocBook = new File(sourceFilename);
    sourceDirectory = sourceDocBook.getParentFile();
    File imageDirectory = getImageDirectory();
    File calloutDirectory = new File(imageDirectory, "callouts");
    transformer.setParameter("docbook.infile", sourceDocBook.toURI().toString());
    transformer.setParameter("source.directory", sourceDirectory.toURI().toString());
    transformer.setParameter("compute.wadl.path.from.docbook.path", computeWadlPathFromDocbookPath);
    // transformer.setParameter ("admon.graphics.path", imageDirectory.toURI().toString());
    transformer.setParameter("admon.graphics.path", "images/");
    transformer.setParameter("callout.graphics.path", calloutDirectory.toURI().toString());
    // 
    // Setup the background image file
    // 
    File cloudSub = new File(imageDirectory, "cloud");
    File ccSub = new File(imageDirectory, "cc");
    coverImage = new File(cloudSub, COVER_IMAGE_NAME);
    coverImageTemplate = new File(cloudSub, COVER_IMAGE_TEMPLATE_NAME);
    coverImageTemplate = new File(cloudSub, branding + "-cover.st");
    transformer.setParameter("cloud.api.background.image", coverImage.toURI().toString());
    transformer.setParameter("cloud.api.cc.image.dir", ccSub.toURI().toString());
}
#end_block

#method_before
public void rasterize(File imagedir) throws RuntimeException {
    try {
        // Create a JPEG transcoder
        PNGTranscoder t = new PNGTranscoder();
        // Set the transcoding hints.
        // t.addTranscodingHint(PNGTranscoder.KEY_QUALITY,
        // new Float(.8));
        File[] svgfiles = imagedir.listFiles(new FilenameFilter() {

            @Override
            public boolean accept(File dir, String name) {
                return name.endsWith(".svg");
            }
        });
        for (File svgimage : svgfiles) {
            // Create the transcoder input.
            // String svgURI = new File("/home/fifieldt/temp/clouddocs-maven-plugin/openstack-manuals/doc/install-guide/target/docbkx/images/note.svg").toURI().toURL().toString();
            String svgURI = svgimage.toURI().toURL().toString();
            TranscoderInput input = new TranscoderInput(svgURI);
            // Create the transcoder output.
            OutputStream ostream = new FileOutputStream(svgimage.toString().replace(".svg", ".png"));
            TranscoderOutput output = new TranscoderOutput(ostream);
            // Save the image.
            t.transcode(input, output);
            // Flush and close the stream.
            ostream.flush();
            ostream.close();
        }
    } catch (Exception e) {
        throw new RuntimeException("Could not able to rasterize the svg", e);
    }
}
#method_after
public void rasterize(File imagedir) throws MojoExecutionException {
    try {
        // Create a JPEG transcoder
        PNGTranscoder t = new PNGTranscoder();
        // Set the transcoding hints.
        // t.addTranscodingHint(PNGTranscoder.KEY_QUALITY,
        // new Float(.8));
        File[] svgfiles = imagedir.listFiles(new FilenameFilter() {

            @Override
            public boolean accept(File dir, String name) {
                return name.endsWith(".svg");
            }
        });
        for (File svgimage : svgfiles) {
            // Create the transcoder input.
            String svgURI = svgimage.toURI().toURL().toString();
            TranscoderInput input = new TranscoderInput(svgURI);
            // Create the transcoder output.
            OutputStream ostream = new FileOutputStream(svgimage.toString().replace(".svg", ".png"));
            TranscoderOutput output = new TranscoderOutput(ostream);
            // Save the image.
            t.transcode(input, output);
            // Flush and close the stream.
            ostream.flush();
            ostream.close();
        }
    } catch (Exception e) {
        throw new MojoExecutionException("Could not able to rasterize the svgs", e);
    }
}
#end_block

#method_before
private ByteBuffer makePrefix(MagnetoDBLocalSecondaryIndex index, DecoratedKey partition_key, List<IndexExpression> indexRestrictionList, ExtendedFilter filter, boolean isStart) {
    SliceQueryFilter columnFilter = (SliceQueryFilter) filter.columnFilter(partition_key.key);
    ByteBuffer base_column_name = isStart ? columnFilter.start() : columnFilter.finish();
    for (IndexExpression indexRestriction : indexRestrictionList) {
        if (isStart && !leftSideOperators.contains(indexRestriction.op)) {
            continue;
        }
        if (!isStart && !rightSideOperators.contains(indexRestriction.op)) {
            continue;
        }
        ColumnNameBuilder builder = ((CompositeType) index.indexCfs.getComparator()).builder().add(indexRestriction.value);
        if (strictOperators.contains(indexRestriction.op) || base_column_name.equals(ByteBufferUtil.EMPTY_BYTE_BUFFER)) {
            switch(indexRestriction.op) {
                case GT:
                    return builder.buildForRelation(Relation.Type.GT);
                case GTE:
                    return builder.buildForRelation(Relation.Type.GTE);
                case EQ:
                    return builder.buildForRelation(Relation.Type.EQ);
                case LT:
                    return builder.buildForRelation(Relation.Type.LT);
                case LTE:
                    return builder.buildForRelation(Relation.Type.LTE);
            }
        } else {
            ByteBuffer prefix = builder.build();
            DataOutputBuffer out = new DataOutputBuffer();
            try {
                ByteBufferUtil.write(prefix, out);
                ByteBufferUtil.write(base_column_name, out);
            } catch (IOException e) {
                throw new RuntimeException(e);
            }
            return ByteBuffer.wrap(out.getData(), 0, out.getLength());
        }
    }
    return ByteBufferUtil.EMPTY_BYTE_BUFFER;
}
#method_after
private ByteBuffer makePrefix(MagnetoDBLocalSecondaryIndex index, DecoratedKey partition_key, List<IndexExpression> indexRestrictionList, ExtendedFilter filter, boolean isStart) {
    SliceQueryFilter columnFilter = (SliceQueryFilter) filter.columnFilter(partition_key.key);
    ByteBuffer base_column_name = isStart ? columnFilter.start() : columnFilter.finish();
    for (IndexExpression indexRestriction : indexRestrictionList) {
        if (isStart && !leftSideOperators.contains(indexRestriction.op)) {
            continue;
        }
        if (!isStart && !rightSideOperators.contains(indexRestriction.op)) {
            continue;
        }
        ColumnNameBuilder builder = ((CompositeType) index.indexCfs.getComparator()).builder().add(indexRestriction.value);
        if (strictOperators.contains(indexRestriction.op) || base_column_name.equals(ByteBufferUtil.EMPTY_BYTE_BUFFER)) {
            switch(indexRestriction.op) {
                case GT:
                    return builder.buildForRelation(Relation.Type.GT);
                case GTE:
                    return builder.buildForRelation(Relation.Type.GTE);
                case EQ:
                    return builder.buildForRelation(isStart ? Relation.Type.GTE : Relation.Type.LTE);
                case LT:
                    return builder.buildForRelation(Relation.Type.LT);
                case LTE:
                    return builder.buildForRelation(Relation.Type.LTE);
            }
        } else {
            ByteBuffer prefix = builder.build();
            DataOutputBuffer out = new DataOutputBuffer();
            try {
                ByteBufferUtil.write(prefix, out);
                ByteBufferUtil.write(base_column_name, out);
            } catch (IOException e) {
                throw new RuntimeException(e);
            }
            return ByteBuffer.wrap(out.getData(), 0, out.getLength());
        }
    }
    return ByteBufferUtil.EMPTY_BYTE_BUFFER;
}
#end_block

#method_before
private ColumnFamilyStore.AbstractScanIterator getIndexedIterator(final ExtendedFilter filter) {
    Map<String, String> query_options = null;
    List<IndexExpression> indexValueRestrictionList = new ArrayList<IndexExpression>();
    MagnetoDBLocalSecondaryIndex indexToSearch = null;
    ByteBuffer indexedColumnName = null;
    Iterator<IndexExpression> iter = filter.getClause().iterator();
    while (iter.hasNext()) {
        IndexExpression expr = iter.next();
        MagnetoDBLocalSecondaryIndex index = (MagnetoDBLocalSecondaryIndex) indexManager.getIndexForColumn(expr.column_name);
        if (index.isQueryPropertiesField) {
            assert query_options == null;
            try {
                String sqo = ByteBufferUtil.string(expr.value);
                query_options = MagnetoDBLocalSecondaryIndex.QueryOptions.parse(sqo);
            } catch (CharacterCodingException e) {
                throw new RuntimeException(e);
            }
            iter.remove();
        } else {
            assert (indexedColumnName == null) || (indexedColumnName == expr.column_name);
            indexedColumnName = expr.column_name;
            indexToSearch = index;
            indexValueRestrictionList.add(expr);
        }
    }
    final MagnetoDBLocalSecondaryIndex index = indexToSearch;
    final boolean reversed = Boolean.parseBoolean(query_options.get(MagnetoDBLocalSecondaryIndex.QueryOptions.REVERSED));
    final CompositeType indexComparator = (CompositeType) index.getIndexCfs().getComparator();
    final DecoratedKey basicCFPartitionKey = (DecoratedKey) filter.dataRange.keyRange().left;
    final DecoratedKey indexCFPartitionKey = index.getIndexCfs().partitioner.decorateKey(basicCFPartitionKey.key);
    final ByteBuffer startPrefix = makePrefix(index, basicCFPartitionKey, indexValueRestrictionList, filter, !reversed);
    final ByteBuffer endPrefix = makePrefix(index, basicCFPartitionKey, indexValueRestrictionList, filter, reversed);
    final int limit = Math.min(filter.currentLimit(), SelectExpression.MAX_COLUMNS_DEFAULT);
    return new ColumnFamilyStore.AbstractScanIterator() {

        private int columnCount = 0;

        int columnToFetchCount = 0;

        int fetchedColumnCount = 0;

        private ByteBuffer lastPrefixSeen = startPrefix;

        public boolean needsFiltering() {
            return false;
        }

        private Row makeReturn(DecoratedKey key, ColumnFamily data) {
            if (data == null)
                return endOfData();
            assert key != null;
            return new Row(key, data);
        }

        protected Row computeNext() {
            /*
             * Our internal index code is wired toward internal rows. So we need to accumulate all results for a given
             * row before returning from this method. Which unfortunately means that this method has to do what
             * CFS.filter does for KeysIndex.
             */
            ColumnFamily data = null;
            MAIN_LOOP: while (fetchedColumnCount >= columnToFetchCount) {
                columnToFetchCount = (limit - columnCount);
                columnToFetchCount += Math.max(columnToFetchCount / 10, 2);
                QueryFilter indexFilter = QueryFilter.getSliceFilter(indexCFPartitionKey, index.getIndexCfs().name, lastPrefixSeen, endPrefix, reversed, columnToFetchCount, filter.timestamp);
                if (indexFilter == null)
                    break MAIN_LOOP;
                ColumnFamily indexRow = index.getIndexCfs().getColumnFamily(indexFilter);
                if (indexRow == null || indexRow.getColumnCount() == 0)
                    break MAIN_LOOP;
                fetchedColumnCount = indexRow.getColumnCount();
                Collection<Column> sortedColumns = reversed ? indexRow.getReverseSortedColumns() : indexRow.getSortedColumns();
                for (Column column : sortedColumns) {
                    lastPrefixSeen = column.name();
                    if (column.isMarkedForDelete(filter.timestamp)) {
                        logger.trace("skipping {}", column.name());
                        continue;
                    }
                    MagnetoDBLocalSecondaryIndex.IndexedEntry entry = index.decodeEntry(basicCFPartitionKey, column);
                    ByteBuffer start = entry.originalColumnNameStart();
                    logger.trace("Adding index hit to current row for {}", indexComparator.getString(column.name()));
                    // We always query the whole CQL3 row. In the case where the original filter was a name filter this might be
                    // slightly wasteful, but this probably doesn't matter in practice and it simplify things.
                    ColumnSlice dataSlice = new ColumnSlice(start, entry.originalColumnNameEnd());
                    ColumnSlice[] slices;
                    if (baseCfs.metadata.hasStaticColumns()) {
                        // If the table has static columns, we must fetch them too as they may need to be returned too.
                        // Note that this is potentially wasteful for 2 reasons:
                        // 1) we will retrieve the static parts for each indexed row, even if we have more than one row in
                        // the same partition. If we were to group data queries to rows on the same slice, which would
                        // speed up things in general, we would also optimize here since we would fetch static columns only
                        // once for each group.
                        // 2) at this point we don't know if the user asked for static columns or not, so we might be fetching
                        // them for nothing. We would however need to ship the list of "CQL3 columns selected" with getRangeSlice
                        // to be able to know that.
                        // TODO: we should improve both point above
                        ColumnSlice staticSlice = new ColumnSlice(ByteBufferUtil.EMPTY_BYTE_BUFFER, baseCfs.metadata.getStaticColumnNameBuilder().buildAsEndOfRange());
                        slices = new ColumnSlice[] { staticSlice, dataSlice };
                    } else {
                        slices = new ColumnSlice[] { dataSlice };
                    }
                    SliceQueryFilter dataFilter = new SliceQueryFilter(slices, false, Integer.MAX_VALUE, baseCfs.metadata.clusteringKeyColumns().size());
                    ColumnFamily newData = baseCfs.getColumnFamily(new QueryFilter(basicCFPartitionKey, baseCfs.name, dataFilter, filter.timestamp));
                    if (newData == null || index.isStale(entry, newData, filter.timestamp)) {
                        index.delete(indexCFPartitionKey.key, column);
                        continue;
                    }
                    assert newData != null : "An entry with not data should have been considered stale";
                    if (!filter.isSatisfiedBy(basicCFPartitionKey, newData, entry.originalColumnNameBuilder))
                        continue;
                    if (data == null) {
                        data = UnsortedColumns.factory.create(baseCfs.metadata);
                    }
                    data.resolve(newData);
                    columnCount++;
                    if (columnCount >= limit) {
                        break MAIN_LOOP;
                    }
                }
                lastPrefixSeen = ByteBufferUtil.clone(lastPrefixSeen);
                lastPrefixSeen.put(lastPrefixSeen.remaining() - 1, (byte) (reversed ? -1 : 1));
            }
            return makeReturn(basicCFPartitionKey, data);
        }

        public void close() throws IOException {
        }
    };
}
#method_after
private ColumnFamilyStore.AbstractScanIterator getIndexedIterator(final ExtendedFilter filter) {
    Map<String, Object> query_options = null;
    List<IndexExpression> indexValueRestrictionList = new ArrayList<IndexExpression>();
    MagnetoDBLocalSecondaryIndex indexToSearch = null;
    ByteBuffer indexedColumnName = null;
    Iterator<IndexExpression> iter = filter.getClause().iterator();
    while (iter.hasNext()) {
        IndexExpression expr = iter.next();
        MagnetoDBLocalSecondaryIndex index = (MagnetoDBLocalSecondaryIndex) indexManager.getIndexForColumn(expr.column_name);
        if (index.isQueryPropertiesField) {
            assert query_options == null;
            try {
                String sqo = ByteBufferUtil.string(expr.value);
                query_options = MagnetoDBLocalSecondaryIndex.QueryOptions.parse(sqo);
            } catch (CharacterCodingException e) {
                throw new RuntimeException(e);
            }
            iter.remove();
        } else {
            assert (indexedColumnName == null) || (indexedColumnName == expr.column_name);
            indexedColumnName = expr.column_name;
            indexToSearch = index;
            indexValueRestrictionList.add(expr);
        }
    }
    final MagnetoDBLocalSecondaryIndex index = indexToSearch;
    MagnetoDBLocalSecondaryIndex.QueryOptions.OrderType order = (MagnetoDBLocalSecondaryIndex.QueryOptions.OrderType) query_options.get(MagnetoDBLocalSecondaryIndex.QueryOptions.ORDER);
    final boolean reversed = (order == MagnetoDBLocalSecondaryIndex.QueryOptions.OrderType.DESC);
    final CompositeType indexComparator = (CompositeType) index.getIndexCfs().getComparator();
    final DecoratedKey basicCFPartitionKey = (DecoratedKey) filter.dataRange.keyRange().left;
    final DecoratedKey indexCFPartitionKey = index.getIndexCfs().partitioner.decorateKey(basicCFPartitionKey.key);
    final ByteBuffer startPrefix = makePrefix(index, basicCFPartitionKey, indexValueRestrictionList, filter, !reversed);
    final ByteBuffer endPrefix = makePrefix(index, basicCFPartitionKey, indexValueRestrictionList, filter, reversed);
    final int limit = Math.min(filter.currentLimit(), SelectExpression.MAX_COLUMNS_DEFAULT);
    return new ColumnFamilyStore.AbstractScanIterator() {

        private int columnCount = 0;

        int columnToFetchCount = 0;

        int fetchedColumnCount = 0;

        private ByteBuffer lastPrefixSeen = startPrefix;

        public boolean needsFiltering() {
            return false;
        }

        private Row makeReturn(DecoratedKey key, ColumnFamily data) {
            if (data == null)
                return endOfData();
            assert key != null;
            return new Row(key, data);
        }

        protected Row computeNext() {
            /*
             * Our internal index code is wired toward internal rows. So we need to accumulate all results for a given
             * row before returning from this method. Which unfortunately means that this method has to do what
             * CFS.filter does for KeysIndex.
             */
            ColumnFamily data = null;
            MAIN_LOOP: while (fetchedColumnCount >= columnToFetchCount) {
                columnToFetchCount = (limit - columnCount);
                columnToFetchCount += Math.max(columnToFetchCount / 10, 2);
                QueryFilter indexFilter = QueryFilter.getSliceFilter(indexCFPartitionKey, index.getIndexCfs().name, lastPrefixSeen, endPrefix, reversed, columnToFetchCount, filter.timestamp);
                if (indexFilter == null)
                    break MAIN_LOOP;
                ColumnFamily indexRow = index.getIndexCfs().getColumnFamily(indexFilter);
                if (indexRow == null || indexRow.getColumnCount() == 0)
                    break MAIN_LOOP;
                fetchedColumnCount = indexRow.getColumnCount();
                Collection<Column> sortedColumns = reversed ? indexRow.getReverseSortedColumns() : indexRow.getSortedColumns();
                for (Column column : sortedColumns) {
                    lastPrefixSeen = column.name();
                    if (column.isMarkedForDelete(filter.timestamp)) {
                        logger.trace("skipping {}", column.name());
                        continue;
                    }
                    MagnetoDBLocalSecondaryIndex.IndexedEntry entry = index.decodeEntry(basicCFPartitionKey, column);
                    ByteBuffer start = entry.originalColumnNameStart();
                    logger.trace("Adding index hit to current row for {}", indexComparator.getString(column.name()));
                    // We always query the whole CQL3 row. In the case where the original filter was a name filter this might be
                    // slightly wasteful, but this probably doesn't matter in practice and it simplify things.
                    ColumnSlice dataSlice = new ColumnSlice(start, entry.originalColumnNameEnd());
                    ColumnSlice[] slices;
                    if (baseCfs.metadata.hasStaticColumns()) {
                        // If the table has static columns, we must fetch them too as they may need to be returned too.
                        // Note that this is potentially wasteful for 2 reasons:
                        // 1) we will retrieve the static parts for each indexed row, even if we have more than one row in
                        // the same partition. If we were to group data queries to rows on the same slice, which would
                        // speed up things in general, we would also optimize here since we would fetch static columns only
                        // once for each group.
                        // 2) at this point we don't know if the user asked for static columns or not, so we might be fetching
                        // them for nothing. We would however need to ship the list of "CQL3 columns selected" with getRangeSlice
                        // to be able to know that.
                        // TODO: we should improve both point above
                        ColumnSlice staticSlice = new ColumnSlice(ByteBufferUtil.EMPTY_BYTE_BUFFER, baseCfs.metadata.getStaticColumnNameBuilder().buildAsEndOfRange());
                        slices = new ColumnSlice[] { staticSlice, dataSlice };
                    } else {
                        slices = new ColumnSlice[] { dataSlice };
                    }
                    SliceQueryFilter dataFilter = new SliceQueryFilter(slices, false, Integer.MAX_VALUE, baseCfs.metadata.clusteringKeyColumns().size());
                    ColumnFamily newData = baseCfs.getColumnFamily(new QueryFilter(basicCFPartitionKey, baseCfs.name, dataFilter, filter.timestamp));
                    if (newData == null || index.isStale(entry, newData, filter.timestamp)) {
                        index.delete(indexCFPartitionKey.key, column);
                        continue;
                    }
                    assert newData != null : "An entry with not data should have been considered stale";
                    if (!filter.isSatisfiedBy(basicCFPartitionKey, newData, entry.originalColumnNameBuilder))
                        continue;
                    if (data == null) {
                        data = UnsortedColumns.factory.create(baseCfs.metadata);
                    }
                    data.resolve(newData);
                    columnCount++;
                    if (columnCount >= limit) {
                        break MAIN_LOOP;
                    }
                }
                lastPrefixSeen = ByteBufferUtil.clone(lastPrefixSeen);
                lastPrefixSeen.put(lastPrefixSeen.remaining() - 1, (byte) (reversed ? -1 : 1));
            }
            return makeReturn(basicCFPartitionKey, data);
        }

        public void close() throws IOException {
        }
    };
}
#end_block

#method_before
private static CompositeType buildIndexComparator(CFMetaData baseMetadata, ColumnDefinition columnDef) {
    List<AbstractType<?>> types = new ArrayList<AbstractType<?>>(((CompositeType) baseMetadata.comparator).types);
    types.add(1, columnDef.getValidator());
    return CompositeType.getInstance(types);
}
#method_after
private static CompositeType buildIndexComparator(CFMetaData baseMetadata, ColumnDefinition columnDef) {
    List<AbstractType<?>> types = new ArrayList<AbstractType<?>>(((CompositeType) baseMetadata.comparator).types);
    types.add(0, columnDef.getValidator());
    return CompositeType.getInstance(types);
}
#end_block

#method_before
public static Map<String, String> parse(String query_parameters) {
    Map<String, String> res = new HashMap<String, String>();
    query_parameters = query_parameters.trim();
    if (query_parameters.isEmpty()) {
        return res;
    }
    String[] params = query_parameters.split(";");
    for (String param : params) {
        param = param.trim();
        String[] keyValue = param.split(":");
        assert keyValue.length == 2;
        res.put(keyValue[0].toLowerCase(), keyValue[1]);
    }
    return res;
}
#method_after
public static Map<String, Object> parse(String query_parameters) {
    Map<String, Object> res = new HashMap<String, Object>();
    query_parameters = query_parameters.trim();
    if (query_parameters.isEmpty()) {
        return res;
    }
    String[] params = query_parameters.split(";");
    for (String param : params) {
        param = param.trim();
        String[] keyValue = param.split(":");
        assert keyValue.length == 2;
        String key = keyValue[0];
        Object value;
        switch(key) {
            case ORDER:
                value = OrderType.valueOf(keyValue[1]);
                break;
            default:
                throw new RuntimeException("Unknown query property: " + key);
        }
        res.put(key, value);
    }
    return res;
}
#end_block

#method_before
public void delete(String tenantId, String alarmId) {
    Alarm alarm = repo.findById(tenantId, alarmId);
    Map<String, AlarmSubExpression> subAlarmMetricDefs = repo.findAlarmSubExpressions(alarmId);
    List<MetricDefinition> metrics = repo.findMetrics(tenantId, alarmId);
    repo.deleteById(alarmId);
    // Notify interested parties of alarm deletion
    String event = Serialization.toJson(new AlarmDeletedEvent(tenantId, alarmId, metrics, alarm.getAlarmDefinition().getId(), subAlarmMetricDefs));
    producer.send(new KeyedMessage<>(config.eventsTopic, tenantId, event));
}
#method_after
public void delete(String tenantId, String alarmId) {
    Alarm alarm = repo.findById(tenantId, alarmId);
    Map<String, AlarmSubExpression> subAlarmMetricDefs = repo.findAlarmSubExpressions(alarmId);
    List<MetricDefinition> metrics = repo.findMetrics(tenantId, alarmId);
    repo.deleteById(tenantId, alarmId);
    // Notify interested parties of alarm deletion
    String event = Serialization.toJson(new AlarmDeletedEvent(tenantId, alarmId, metrics, alarm.getAlarmDefinition().getId(), subAlarmMetricDefs));
    producer.send(new KeyedMessage<>(config.eventsTopic, tenantId, event));
}
#end_block

#method_before
@Test(groups = "database")
public void shouldDelete() {
    repo.deleteById("123111");
    try {
        assertNull(repo.findById("bob", "123111"));
        fail();
    } catch (EntityNotFoundException expected) {
    }
}
#method_after
@Test(groups = "database")
public void shouldDelete() {
    repo.deleteById("bob", "123111");
    try {
        assertNull(repo.findById("bob", "123111"));
        fail();
    } catch (EntityNotFoundException expected) {
    }
}
#end_block

#method_before
@Override
public void deleteById(String id) {
    try (Handle h = db.open()) {
        h.execute("delete from alarm where id = :id", id);
    }
}
#method_after
@Override
public void deleteById(String tenantId, String id) {
    String sql = "delete alarm.* from alarm " + "join (select distinct a.id " + "from alarm as a " + "inner join alarm_definition as ad " + "on ad.id = a.alarm_definition_id " + "where ad.tenant_id = :tenantId and a.id = :id) as b " + "on b.id = alarm.id";
    try (Handle h = db.open()) {
        h.execute(sql, tenantId, id);
    }
}
#end_block

#method_before
@Override
protected void setupResources() throws Exception {
    super.setupResources();
    notificationMethod = new NotificationMethod("123", "Joe's SMS", NotificationMethodType.SMS, "8675309");
    notificationMethodEndPoint = new NotificationMethod("1234", "MyEp", NotificationMethodType.ENDPOINT, "http://localhost");
    repo = mock(NotificationMethodRepository.class);
    when(repo.create(eq("abc"), eq("MySMS"), eq(NotificationMethodType.SMS), anyString())).thenReturn(notificationMethod);
    when(repo.create(eq("abc"), eq("MyEP"), eq(NotificationMethodType.ENDPOINT), anyString())).thenReturn(notificationMethodEndPoint);
    when(repo.findById(eq("abc"), eq("123"))).thenReturn(notificationMethod);
    when(repo.find(eq("abc"))).thenReturn(Arrays.asList(notificationMethod));
    addResources(new NotificationMethodResource(repo));
}
#method_after
@Override
protected void setupResources() throws Exception {
    super.setupResources();
    notificationMethod = new NotificationMethod("123", "Joe's SMS", NotificationMethodType.SMS, "8675309");
    notificationMethodWebhook = new NotificationMethod("1234", "MyWh", NotificationMethodType.WEBHOOK, "http://localhost");
    repo = mock(NotificationMethodRepository.class);
    when(repo.create(eq("abc"), eq("MySMS"), eq(NotificationMethodType.SMS), anyString())).thenReturn(notificationMethod);
    when(repo.create(eq("abc"), eq("MyWh"), eq(NotificationMethodType.WEBHOOK), anyString())).thenReturn(notificationMethodWebhook);
    when(repo.findById(eq("abc"), eq("123"))).thenReturn(notificationMethod);
    when(repo.find(eq("abc"))).thenReturn(Arrays.asList(notificationMethod));
    addResources(new NotificationMethodResource(repo));
}
#end_block

#method_before
public void shouldCreateEndpointNotification() {
    ClientResponse response = client().resource("/v2.0/notification-methods").header("X-Tenant-Id", "abc").header("Content-Type", MediaType.APPLICATION_JSON).post(ClientResponse.class, new CreateNotificationMethodCommand("MyEP", NotificationMethodType.ENDPOINT, "http://localhost"));
    NotificationMethod newNotificationMethod = response.getEntity(NotificationMethod.class);
    String location = response.getHeaders().get("Location").get(0);
    assertEquals(response.getStatus(), 201);
    assertEquals(location, "/v2.0/notification-methods/" + newNotificationMethod.getId());
    assertEquals(newNotificationMethod, notificationMethodEndPoint);
    verify(repo).create(eq("abc"), eq("MyEP"), eq(NotificationMethodType.ENDPOINT), anyString());
}
#method_after
public void shouldCreateEndpointNotification() {
    ClientResponse response = client().resource("/v2.0/notification-methods").header("X-Tenant-Id", "abc").header("Content-Type", MediaType.APPLICATION_JSON).post(ClientResponse.class, new CreateNotificationMethodCommand("MyWh", NotificationMethodType.WEBHOOK, "http://localhost"));
    NotificationMethod newNotificationMethod = response.getEntity(NotificationMethod.class);
    String location = response.getHeaders().get("Location").get(0);
    assertEquals(response.getStatus(), 201);
    assertEquals(location, "/v2.0/notification-methods/" + newNotificationMethod.getId());
    assertEquals(newNotificationMethod, notificationMethodWebhook);
    verify(repo).create(eq("abc"), eq("MyWh"), eq(NotificationMethodType.WEBHOOK), anyString());
}
#end_block

#method_before
@Override
public void onConfigurationChange() {
    // Bug: Configuration save occurs after this function is called
    // gets called on any configuration change
    // includes new slave and delete slave
    logger.info("---- " + ComputerListenerImpl.class.getName() + ":" + " onConfigurationChange");
    // update functions only when gearman-plugin is enabled
    if (!GearmanPluginConfig.get().enablePlugin()) {
        return;
    }
// TODO: adjust for an update to executors. Method does not provide the
// computer to know which thread to remove or add
}
#method_after
@Override
public void onConfigurationChange() {
    // only fired on nodes configuration changes like a label or
    // name change. Not fired on state changes, like offline or online.
    logger.info("---- " + ComputerListenerImpl.class.getName() + ":" + " onConfigurationChange");
    // update functions only when gearman-plugin is enabled
    if (!GearmanPluginConfig.get().enablePlugin()) {
        return;
    }
    // re-register gearman functions on node configuration changes,
    // specifically node label changes
    GearmanProxy.getInstance().registerJobs();
// TODO: adjust for an update to executors. Method does not provide the
// computer to know which thread to remove or add
}
#end_block

#method_before
@Override
public void onOnline(Computer c, TaskListener listener) throws IOException, InterruptedException {
    // gets called when master goes into online state
    // gets called when existing slave re-connects
    // gets called when new slave goes into online state
    logger.info("---- " + ComputerListenerImpl.class.getName() + ":" + " onOnline computer " + c);
    // update functions only when gearman-plugin is enabled
    if (!GearmanPluginConfig.get().enablePlugin()) {
        return;
    }
    /*
         * Need to treat the master differently than slaves because
         * the master is not the same as a slave
         */
    GearmanProxy gp = GearmanProxy.getInstance();
    /*
         * Spawn management executor worker if one doesn't exist yet.
         * This worker does not need any executors. It only needs
         * to work with gearman.
         */
    gp.createManagementWorker();
    // on creation of new slave
    gp.createExecutorWorkersOnNode(c);
}
#method_after
@Override
public void onOnline(Computer c, TaskListener listener) throws IOException, InterruptedException {
    // gets called when master goes into online state
    // gets called when existing slave re-connects
    // gets called when new slave goes into online state
    logger.info("---- " + ComputerListenerImpl.class.getName() + ":" + " onOnline computer " + c);
    // update functions only when gearman-plugin is enabled
    if (!GearmanPluginConfig.get().enablePlugin()) {
        return;
    }
    GearmanProxy gp = GearmanProxy.getInstance();
    /*
         * Spawn management executor worker if one doesn't exist yet.
         * This worker does not need any executors. It only needs
         * to work with gearman.
         */
    gp.createManagementWorker();
    // on creation of new slave
    gp.createExecutorWorkersOnNode(c);
}
#end_block

#method_before
@Override
public void onTemporarilyOffline(Computer c, OfflineCause cause) {
    // update functions only when gearman-plugin is enabled
    if (!GearmanPluginConfig.get().enablePlugin()) {
        return;
    }
    // stop worker when jenkins slave is set to offline
    GearmanProxy.getInstance().stop(c);
}
#method_after
@Override
public void onTemporarilyOffline(Computer c, OfflineCause cause) {
    // fired when master or slave goes into temporary offline state
    logger.info("---- " + ComputerListenerImpl.class.getName() + ":" + " onTemporarilyOffline computer " + c);
    // update functions only when gearman-plugin is enabled
    if (!GearmanPluginConfig.get().enablePlugin()) {
        return;
    }
    // stop worker when jenkins slave is set to offline
    GearmanProxy.getInstance().stop(c);
}
#end_block

#method_before
@Override
public void onTemporarilyOnline(Computer c) {
    // update functions only when gearman-plugin is enabled
    if (!GearmanPluginConfig.get().enablePlugin()) {
        return;
    }
    // on brining a slave back online
    GearmanProxy gp = GearmanProxy.getInstance();
    gp.createExecutorWorkersOnNode(c);
}
#method_after
@Override
public void onTemporarilyOnline(Computer c) {
    // fired when master or slave goes into temporary online state
    logger.info("---- " + ComputerListenerImpl.class.getName() + ":" + " onTemporarilyOnline computer " + c);
    // update functions only when gearman-plugin is enabled
    if (!GearmanPluginConfig.get().enablePlugin()) {
        return;
    }
    GearmanProxy gp = GearmanProxy.getInstance();
    /*
         * Spawn management executor worker if one doesn't exist yet.
         * This worker does not need any executors. It only needs
         * to work with gearman.
         */
    gp.createManagementWorker();
    // on brining a slave back online
    gp.createExecutorWorkersOnNode(c);
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    CreateAlarmDefinitionCommand other = (CreateAlarmDefinitionCommand) obj;
    if (name == null) {
        if (other.name != null)
            return false;
    } else if (!name.equals(other.name))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (!(obj instanceof CreateAlarmDefinitionCommand))
        return false;
    CreateAlarmDefinitionCommand other = (CreateAlarmDefinitionCommand) obj;
    if (alarmActions == null) {
        if (other.alarmActions != null)
            return false;
    } else if (!alarmActions.equals(other.alarmActions))
        return false;
    if (description == null) {
        if (other.description != null)
            return false;
    } else if (!description.equals(other.description))
        return false;
    if (expression == null) {
        if (other.expression != null)
            return false;
    } else if (!expression.equals(other.expression))
        return false;
    if (matchBy == null) {
        if (other.matchBy != null)
            return false;
    } else if (!matchBy.equals(other.matchBy))
        return false;
    if (name == null) {
        if (other.name != null)
            return false;
    } else if (!name.equals(other.name))
        return false;
    if (okActions == null) {
        if (other.okActions != null)
            return false;
    } else if (!okActions.equals(other.okActions))
        return false;
    if (severity == null) {
        if (other.severity != null)
            return false;
    } else if (!severity.equals(other.severity))
        return false;
    if (undeterminedActions == null) {
        if (other.undeterminedActions != null)
            return false;
    } else if (!undeterminedActions.equals(other.undeterminedActions))
        return false;
    return true;
}
#end_block

#method_before
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((name == null) ? 0 : name.hashCode());
    return result;
}
#method_after
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmActions == null) ? 0 : alarmActions.hashCode());
    result = prime * result + ((description == null) ? 0 : description.hashCode());
    result = prime * result + ((expression == null) ? 0 : expression.hashCode());
    result = prime * result + ((matchBy == null) ? 0 : matchBy.hashCode());
    result = prime * result + ((name == null) ? 0 : name.hashCode());
    result = prime * result + ((okActions == null) ? 0 : okActions.hashCode());
    result = prime * result + ((severity == null) ? 0 : severity.hashCode());
    result = prime * result + ((undeterminedActions == null) ? 0 : undeterminedActions.hashCode());
    return result;
}
#end_block

#method_before
public void delete(String tenantId, String alarmId) {
    Alarm alarm = repo.findById(alarmId);
    // Notify interested parties of alarm deletion
    String event = Serialization.toJson(new AlarmDeletedEvent(alarmId, alarm.getAlarmDefinitionId()));
    producer.send(new KeyedMessage<>(config.eventsTopic, tenantId, event));
}
#method_after
public void delete(String tenantId, String alarmId) {
    Alarm alarm = repo.findById(alarmId);
    Map<String, MetricDefinition> subAlarmMetricDefs = alarmDefRepo.findSubAlarmMetricDefinitions(alarm.getAlarmDefinitionId());
    List<MetricDefinition> metrics = repo.findMetrics(alarmId);
    repo.deleteById(alarmId);
    // Notify interested parties of alarm deletion
    String event = Serialization.toJson(new AlarmDeletedEvent(tenantId, alarmId, metrics, alarm.getAlarmDefinitionId(), subAlarmMetricDefs));
    producer.send(new KeyedMessage<>(config.eventsTopic, tenantId, event));
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (!super.equals(obj))
        return false;
    if (getClass() != obj.getClass())
        return false;
    UpdateAlarmDefinitionCommand other = (UpdateAlarmDefinitionCommand) obj;
    if (actionsEnabled == null) {
        if (other.actionsEnabled != null)
            return false;
    } else if (!actionsEnabled.equals(other.actionsEnabled))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (!super.equals(obj))
        return false;
    if (!(obj instanceof UpdateAlarmDefinitionCommand))
        return false;
    UpdateAlarmDefinitionCommand other = (UpdateAlarmDefinitionCommand) obj;
    if (actionsEnabled == null) {
        if (other.actionsEnabled != null)
            return false;
    } else if (!actionsEnabled.equals(other.actionsEnabled))
        return false;
    return true;
}
#end_block

#method_before
public static Map<String, String> dimensionsFor(String dimensionSet) {
    Map<String, String> dimensions = null;
    if (dimensionSet != null) {
        dimensions = new HashMap<String, String>();
        for (String kvStr : dimensionSet.split(",")) {
            String[] kv = kvStr.split("=");
            if (kv.length > 1)
                dimensions.put(kv[0], kv[1]);
        }
    }
    return dimensions;
}
#method_after
public static Map<String, String> dimensionsFor(String dimensionSet) {
    Map<String, String> dimensions = Collections.emptyMap();
    if (dimensionSet != null) {
        dimensions = new HashMap<String, String>();
        for (String kvStr : dimensionSet.split(",")) {
            String[] kv = kvStr.split("=");
            if (kv.length > 1)
                dimensions.put(kv[0], kv[1]);
        }
    }
    return dimensions;
}
#end_block

#method_before
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state) {
    try (Handle h = db.open()) {
        StringBuilder sbWhere = new StringBuilder();
        if (alarmDefId != null) {
            sbWhere.append(" and ad.id = :alarmDefId");
        }
        if (metricName != null) {
            sbWhere.append(" and md.name = :metricName");
        }
        if (state != null) {
            sbWhere.append(" and a.state = :state");
        }
        String sql = String.format(ALARM_SQL, buildJoinClauseFor(metricDimensions), sbWhere);
        Query<?> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        q = q.map(new BeanMapper<Alarm>(Alarm.class));
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        @SuppressWarnings("unchecked")
        List<Alarm> alarms = (List<Alarm>) q.list();
        for (Alarm alarm : alarms) {
            hdyrateMetricsFor(h, alarm);
        }
        return alarms;
    }
}
#method_after
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state) {
    try (Handle h = db.open()) {
        StringBuilder sbWhere = new StringBuilder();
        if (alarmDefId != null) {
            sbWhere.append(" and ad.id = :alarmDefId");
        }
        if (metricName != null) {
            sbWhere.append(" and md.name = :metricName");
        }
        if (state != null) {
            sbWhere.append(" and a.state = :state");
        }
        String sql = String.format(ALARM_SQL, buildJoinClauseFor(metricDimensions), sbWhere);
        Query<?> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        q = q.map(new BeanMapper<Alarm>(Alarm.class));
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        @SuppressWarnings("unchecked")
        List<Alarm> alarms = (List<Alarm>) q.list();
        for (Alarm alarm : alarms) {
            alarm.setMetrics(findMetrics(h, alarm.getId()));
        }
        return alarms;
    }
}
#end_block

#method_before
@Override
public Alarm findById(String alarmId) {
    try (Handle h = db.open()) {
        Alarm alarm = h.createQuery("select * from alarm where id = :id").bind("id", alarmId).map(new BeanMapper<Alarm>(Alarm.class)).first();
        if (alarm == null)
            throw new EntityNotFoundException("No alarm exists for %s", alarmId);
        // Hydrate metrics
        hdyrateMetricsFor(h, alarm);
        return alarm;
    }
}
#method_after
@Override
public Alarm findById(String alarmId) {
    try (Handle h = db.open()) {
        Alarm alarm = h.createQuery("select * from alarm where id = :id").bind("id", alarmId).map(new BeanMapper<Alarm>(Alarm.class)).first();
        if (alarm == null)
            throw new EntityNotFoundException("No alarm exists for %s", alarmId);
        // Hydrate metrics
        alarm.setMetrics(findMetrics(h, alarm.getId()));
        return alarm;
    }
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    AlarmStateHistory other = (AlarmStateHistory) obj;
    if (alarmId == null) {
        if (other.alarmId != null)
            return false;
    } else if (!alarmId.equals(other.alarmId))
        return false;
    if (metricDimensions == null) {
        if (other.metricDimensions != null)
            return false;
    } else if (!metricDimensions.equals(other.metricDimensions))
        return false;
    if (metricName == null) {
        if (other.metricName != null)
            return false;
    } else if (!metricName.equals(other.metricName))
        return false;
    if (newState != other.newState)
        return false;
    if (oldState != other.oldState)
        return false;
    if (reason == null) {
        if (other.reason != null)
            return false;
    } else if (!reason.equals(other.reason))
        return false;
    if (reasonData == null) {
        if (other.reasonData != null)
            return false;
    } else if (!reasonData.equals(other.reasonData))
        return false;
    if (timestamp == null) {
        if (other.timestamp != null)
            return false;
    } else if (!timestamp.equals(other.timestamp))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (!(obj instanceof AlarmStateHistory))
        return false;
    AlarmStateHistory other = (AlarmStateHistory) obj;
    if (alarmId == null) {
        if (other.alarmId != null)
            return false;
    } else if (!alarmId.equals(other.alarmId))
        return false;
    if (metrics == null) {
        if (other.metrics != null)
            return false;
    } else if (!metrics.equals(other.metrics))
        return false;
    if (newState != other.newState)
        return false;
    if (oldState != other.oldState)
        return false;
    if (reason == null) {
        if (other.reason != null)
            return false;
    } else if (!reason.equals(other.reason))
        return false;
    if (reasonData == null) {
        if (other.reasonData != null)
            return false;
    } else if (!reasonData.equals(other.reasonData))
        return false;
    if (timestamp == null) {
        if (other.timestamp != null)
            return false;
    } else if (!timestamp.equals(other.timestamp))
        return false;
    return true;
}
#end_block

#method_before
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmId == null) ? 0 : alarmId.hashCode());
    result = prime * result + ((metricDimensions == null) ? 0 : metricDimensions.hashCode());
    result = prime * result + ((metricName == null) ? 0 : metricName.hashCode());
    result = prime * result + ((newState == null) ? 0 : newState.hashCode());
    result = prime * result + ((oldState == null) ? 0 : oldState.hashCode());
    result = prime * result + ((reason == null) ? 0 : reason.hashCode());
    result = prime * result + ((reasonData == null) ? 0 : reasonData.hashCode());
    result = prime * result + ((timestamp == null) ? 0 : timestamp.hashCode());
    return result;
}
#method_after
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmId == null) ? 0 : alarmId.hashCode());
    result = prime * result + ((metrics == null) ? 0 : metrics.hashCode());
    result = prime * result + ((newState == null) ? 0 : newState.hashCode());
    result = prime * result + ((oldState == null) ? 0 : oldState.hashCode());
    result = prime * result + ((reason == null) ? 0 : reason.hashCode());
    result = prime * result + ((reasonData == null) ? 0 : reasonData.hashCode());
    result = prime * result + ((timestamp == null) ? 0 : timestamp.hashCode());
    return result;
}
#end_block

#method_before
@Override
public String toString() {
    return "AlarmStateHistory [alarmId=" + alarmId + ", metricName=" + metricName + ", metricDimensions=" + metricDimensions + ", oldState=" + oldState + ", newState=" + newState + ", reason=" + reason + ", reasonData=" + reasonData + ", timestamp=" + timestamp + "]";
}
#method_after
@Override
public String toString() {
    return "AlarmStateHistory [alarmId=" + alarmId + ", metrics=" + metrics + ", oldState=" + oldState + ", newState=" + newState + ", reason=" + reason + ", reasonData=" + reasonData + ", timestamp=" + timestamp + "]";
}
#end_block

#method_before
@Override
public AlarmDefinition create(String tenantId, String id, String name, String description, String severity, String expression, Map<String, AlarmSubExpression> subExpressions, List<String> matchBy, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions) {
    Handle h = db.open();
    try {
        h.begin();
        h.insert("insert into alarm_definition (id, tenant_id, name, description, severity, expression, match_by, actions_enabled, created_at, updated_at, deleted_at) values (?, ?, ?, ?, ?, ?, ?, ?, NOW(), NOW(), NULL)", id, tenantId, name, description, severity, expression, Iterables.isNullOrEmpty(matchBy) ? null : COMMA_JOINER.join(matchBy), true);
        // Persist sub-alarms
        createSubExpressions(h, id, subExpressions);
        // Persist actions
        persistActions(h, id, AlarmState.ALARM, alarmActions);
        persistActions(h, id, AlarmState.OK, okActions);
        persistActions(h, id, AlarmState.UNDETERMINED, undeterminedActions);
        h.commit();
        return new AlarmDefinition(id, name, description, severity, expression, matchBy, true, alarmActions, okActions == null ? Collections.<String>emptyList() : okActions, undeterminedActions == null ? Collections.<String>emptyList() : undeterminedActions);
    } catch (RuntimeException e) {
        h.rollback();
        throw e;
    } finally {
        h.close();
    }
}
#method_after
@Override
public AlarmDefinition create(String tenantId, String id, String name, String description, String severity, String expression, Map<String, AlarmSubExpression> subExpressions, List<String> matchBy, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions) {
    Handle h = db.open();
    try {
        h.begin();
        h.insert("insert into alarm_definition (id, tenant_id, name, description, severity, expression, match_by, actions_enabled, created_at, updated_at, deleted_at) values (?, ?, ?, ?, ?, ?, ?, ?, NOW(), NOW(), NULL)", id, tenantId, name, description, severity, expression, matchBy == null || Iterables.isEmpty(matchBy) ? null : COMMA_JOINER.join(matchBy), true);
        // Persist sub-alarms
        createSubExpressions(h, id, subExpressions);
        // Persist actions
        persistActions(h, id, AlarmState.ALARM, alarmActions);
        persistActions(h, id, AlarmState.OK, okActions);
        persistActions(h, id, AlarmState.UNDETERMINED, undeterminedActions);
        h.commit();
        return new AlarmDefinition(id, name, description, severity, expression, matchBy, true, alarmActions, okActions == null ? Collections.<String>emptyList() : okActions, undeterminedActions == null ? Collections.<String>emptyList() : undeterminedActions);
    } catch (RuntimeException e) {
        h.rollback();
        throw e;
    } finally {
        h.close();
    }
}
#end_block

#method_before
@Override
public void deleteById(String tenantId, String alarmDefId) {
    try (Handle h = db.open()) {
        if (h.update("update alarm_definition set deleted_at = NOW() where tenant_id = ? and id = ? and deleted_at is NULL", tenantId, alarmDefId) == 0)
            throw new EntityNotFoundException("No alarm definition exists for %s", alarmDefId);
    }
}
#method_after
@Override
public void deleteById(String tenantId, String alarmDefId) {
    try (Handle h = db.open()) {
        if (h.update("update alarm_definition set deleted_at = NOW() where tenant_id = ? and id = ? and deleted_at is NULL", tenantId, alarmDefId) == 0)
            throw new EntityNotFoundException("No alarm definition exists for %s", alarmDefId);
        // Cascade soft delete to alarms
        h.execute("delete from alarm where alarm_definition_id = :id", alarmDefId);
    }
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
public List<AlarmDefinition> find(String tenantId, String name, Map<String, String> dimensions) {
    try (Handle h = db.open()) {
        String query = "select distinct ad.id, ad.description, ad.tenant_id, ad.severity, ad.expression, ad.match_by, ad.name, ad.actions_enabled, ad.created_at, ad.updated_at, ad.deleted_at " + "from alarm_definition ad join sub_alarm sub on ad.id = sub.alarm_definition_id " + "left outer join sub_alarm_dimension dim on sub.id = dim.sub_alarm_id%s " + "where tenant_id = :tenantId and deleted_at is NULL %s order by ad.created_at";
        StringBuilder sbWhere = new StringBuilder();
        if (name != null) {
            sbWhere.append(" and ad.name = :name");
        }
        String sql = String.format(query, SubAlarmQueries.buildJoinClauseFor(dimensions), sbWhere);
        Query<?> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null) {
            q.bind("name", name);
        }
        q = q.map(new BeanMapper<AlarmDefinition>(AlarmDefinition.class));
        DimensionQueries.bindDimensionsToQuery(q, dimensions);
        List<AlarmDefinition> alarms = (List<AlarmDefinition>) q.list();
        for (AlarmDefinition alarm : alarms) hydrateRelationships(h, alarm);
        return alarms;
    }
}
#method_after
@Override
@SuppressWarnings("unchecked")
public List<AlarmDefinition> find(String tenantId, String name, Map<String, String> dimensions) {
    try (Handle h = db.open()) {
        String query = "select distinct ad.id, ad.description, ad.tenant_id, ad.severity, ad.expression, ad.match_by, ad.name, ad.actions_enabled, ad.created_at, ad.updated_at, ad.deleted_at " + "from alarm_definition ad join sub_alarm_definition sub on ad.id = sub.alarm_definition_id " + "left outer join sub_alarm_definition_dimension dim on sub.id = dim.sub_alarm_definition_id%s " + "where tenant_id = :tenantId and deleted_at is NULL %s order by ad.created_at";
        StringBuilder sbWhere = new StringBuilder();
        if (name != null) {
            sbWhere.append(" and ad.name = :name");
        }
        String sql = String.format(query, SubAlarmQueries.buildJoinClauseFor(dimensions), sbWhere);
        Query<?> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null) {
            q.bind("name", name);
        }
        q = q.map(new BeanMapper<AlarmDefinition>(AlarmDefinition.class));
        DimensionQueries.bindDimensionsToQuery(q, dimensions);
        List<AlarmDefinition> alarms = (List<AlarmDefinition>) q.list();
        for (AlarmDefinition alarm : alarms) hydrateRelationships(h, alarm);
        return alarms;
    }
}
#end_block

#method_before
@Override
public void update(String tenantId, String id, boolean patch, String name, String description, String expression, List<String> matchBy, String severity, boolean actionsEnabled, Collection<String> oldSubAlarmIds, Map<String, AlarmSubExpression> changedSubAlarms, Map<String, AlarmSubExpression> newSubAlarms, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions) {
    Handle h = db.open();
    try {
        h.begin();
        h.insert("update alarm_definition set name = ?, description = ?, expression = ?, match_by = ?, severity = ?, actions_enabled = ?, updated_at = NOW() where tenant_id = ? and id = ?", name, description, expression, COMMA_JOINER.join(matchBy), severity, actionsEnabled, tenantId, id);
        // Delete old sub-alarms
        if (oldSubAlarmIds != null)
            for (String oldSubAlarmId : oldSubAlarmIds) h.execute("delete from sub_alarm where id = ?", oldSubAlarmId);
        // Update changed sub-alarms
        if (changedSubAlarms != null)
            for (Map.Entry<String, AlarmSubExpression> entry : changedSubAlarms.entrySet()) {
                AlarmSubExpression sa = entry.getValue();
                h.execute("update sub_alarm set operator = ?, threshold = ?, updated_at = NOW() where id = ?", sa.getOperator().name(), sa.getThreshold(), entry.getKey());
            }
        // Insert new sub-alarms
        createSubExpressions(h, id, newSubAlarms);
        // Delete old actions
        if (patch) {
            deleteActions(h, id, AlarmState.ALARM, alarmActions);
            deleteActions(h, id, AlarmState.OK, okActions);
            deleteActions(h, id, AlarmState.UNDETERMINED, undeterminedActions);
        } else
            h.execute("delete from alarm_action where alarm_id = ?", id);
        // Insert new actions
        persistActions(h, id, AlarmState.ALARM, alarmActions);
        persistActions(h, id, AlarmState.OK, okActions);
        persistActions(h, id, AlarmState.UNDETERMINED, undeterminedActions);
        h.commit();
    } catch (RuntimeException e) {
        h.rollback();
        throw e;
    } finally {
        h.close();
    }
}
#method_after
@Override
public void update(String tenantId, String id, boolean patch, String name, String description, String expression, List<String> matchBy, String severity, boolean actionsEnabled, Collection<String> oldSubAlarmIds, Map<String, AlarmSubExpression> changedSubAlarms, Map<String, AlarmSubExpression> newSubAlarms, List<String> alarmActions, List<String> okActions, List<String> undeterminedActions) {
    Handle h = db.open();
    try {
        h.begin();
        h.insert("update alarm_definition set name = ?, description = ?, expression = ?, match_by = ?, severity = ?, actions_enabled = ?, updated_at = NOW() where tenant_id = ? and id = ?", name, description, expression, matchBy == null || Iterables.isEmpty(matchBy) ? null : COMMA_JOINER.join(matchBy), severity, actionsEnabled, tenantId, id);
        // Delete old sub-alarms
        if (oldSubAlarmIds != null)
            for (String oldSubAlarmId : oldSubAlarmIds) h.execute("delete from sub_alarm_definition where id = ?", oldSubAlarmId);
        // Update changed sub-alarms
        if (changedSubAlarms != null)
            for (Map.Entry<String, AlarmSubExpression> entry : changedSubAlarms.entrySet()) {
                AlarmSubExpression sa = entry.getValue();
                h.execute("update sub_alarm_definition set operator = ?, threshold = ?, updated_at = NOW() where id = ?", sa.getOperator().name(), sa.getThreshold(), entry.getKey());
            }
        // Insert new sub-alarms
        createSubExpressions(h, id, newSubAlarms);
        // Delete old actions
        if (patch) {
            deleteActions(h, id, AlarmState.ALARM, alarmActions);
            deleteActions(h, id, AlarmState.OK, okActions);
            deleteActions(h, id, AlarmState.UNDETERMINED, undeterminedActions);
        } else
            h.execute("delete from alarm_action where alarm_id = ?", id);
        // Insert new actions
        persistActions(h, id, AlarmState.ALARM, alarmActions);
        persistActions(h, id, AlarmState.OK, okActions);
        persistActions(h, id, AlarmState.UNDETERMINED, undeterminedActions);
        h.commit();
    } catch (RuntimeException e) {
        h.rollback();
        throw e;
    } finally {
        h.close();
    }
}
#end_block

#method_before
private void createSubExpressions(Handle handle, String id, Map<String, AlarmSubExpression> alarmSubExpressions) {
    if (alarmSubExpressions != null) {
        for (Map.Entry<String, AlarmSubExpression> subEntry : alarmSubExpressions.entrySet()) {
            String subAlarmId = subEntry.getKey();
            AlarmSubExpression subExpr = subEntry.getValue();
            MetricDefinition metricDef = subExpr.getMetricDefinition();
            // Persist sub-alarm
            handle.insert("insert into sub_alarm (id, alarm_definition_id, function, metric_name, operator, threshold, period, periods, created_at, updated_at) " + "values (?, ?, ?, ?, ?, ?, ?, ?, NOW(), NOW())", subAlarmId, id, subExpr.getFunction().name(), metricDef.name, subExpr.getOperator().name(), subExpr.getThreshold(), subExpr.getPeriod(), subExpr.getPeriods());
            // Persist sub-alarm dimensions
            if (metricDef.dimensions != null && !metricDef.dimensions.isEmpty())
                for (Map.Entry<String, String> dimEntry : metricDef.dimensions.entrySet()) handle.insert("insert into sub_alarm_dimension values (?, ?, ?)", subAlarmId, dimEntry.getKey(), dimEntry.getValue());
        }
    }
}
#method_after
private void createSubExpressions(Handle handle, String id, Map<String, AlarmSubExpression> alarmSubExpressions) {
    if (alarmSubExpressions != null) {
        for (Map.Entry<String, AlarmSubExpression> subEntry : alarmSubExpressions.entrySet()) {
            String subAlarmId = subEntry.getKey();
            AlarmSubExpression subExpr = subEntry.getValue();
            MetricDefinition metricDef = subExpr.getMetricDefinition();
            // Persist sub-alarm
            handle.insert("insert into sub_alarm_definition (id, alarm_definition_id, function, metric_name, operator, threshold, period, periods, created_at, updated_at) " + "values (?, ?, ?, ?, ?, ?, ?, ?, NOW(), NOW())", subAlarmId, id, subExpr.getFunction().name(), metricDef.name, subExpr.getOperator().name(), subExpr.getThreshold(), subExpr.getPeriod(), subExpr.getPeriods());
            // Persist sub-alarm dimensions
            if (metricDef.dimensions != null && !metricDef.dimensions.isEmpty())
                for (Map.Entry<String, String> dimEntry : metricDef.dimensions.entrySet()) handle.insert("insert into sub_alarm_definition_dimension values (?, ?, ?)", subAlarmId, dimEntry.getKey(), dimEntry.getValue());
        }
    }
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (!super.equals(obj))
        return false;
    if (getClass() != obj.getClass())
        return false;
    AlarmDefinition other = (AlarmDefinition) obj;
    if (actionsEnabled != other.actionsEnabled)
        return false;
    if (alarmActions == null) {
        if (other.alarmActions != null)
            return false;
    } else if (!alarmActions.equals(other.alarmActions))
        return false;
    if (description == null) {
        if (other.description != null)
            return false;
    } else if (!description.equals(other.description))
        return false;
    if (expression == null) {
        if (other.expression != null)
            return false;
    } else if (!expression.equals(other.expression))
        return false;
    if (expressionData == null) {
        if (other.expressionData != null)
            return false;
    } else if (!expressionData.equals(other.expressionData))
        return false;
    if (links == null) {
        if (other.links != null)
            return false;
    } else if (!links.equals(other.links))
        return false;
    if (matchBy == null) {
        if (other.matchBy != null)
            return false;
    } else if (!matchBy.equals(other.matchBy))
        return false;
    if (name == null) {
        if (other.name != null)
            return false;
    } else if (!name.equals(other.name))
        return false;
    if (okActions == null) {
        if (other.okActions != null)
            return false;
    } else if (!okActions.equals(other.okActions))
        return false;
    if (severity == null) {
        if (other.severity != null)
            return false;
    } else if (!severity.equals(other.severity))
        return false;
    if (undeterminedActions == null) {
        if (other.undeterminedActions != null)
            return false;
    } else if (!undeterminedActions.equals(other.undeterminedActions))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (!super.equals(obj))
        return false;
    if (!(obj instanceof AlarmDefinition))
        return false;
    AlarmDefinition other = (AlarmDefinition) obj;
    if (actionsEnabled != other.actionsEnabled)
        return false;
    if (alarmActions == null) {
        if (other.alarmActions != null)
            return false;
    } else if (!alarmActions.equals(other.alarmActions))
        return false;
    if (description == null) {
        if (other.description != null)
            return false;
    } else if (!description.equals(other.description))
        return false;
    if (expression == null) {
        if (other.expression != null)
            return false;
    } else if (!expression.equals(other.expression))
        return false;
    if (expressionData == null) {
        if (other.expressionData != null)
            return false;
    } else if (!expressionData.equals(other.expressionData))
        return false;
    if (links == null) {
        if (other.links != null)
            return false;
    } else if (!links.equals(other.links))
        return false;
    if (matchBy == null) {
        if (other.matchBy != null)
            return false;
    } else if (!matchBy.equals(other.matchBy))
        return false;
    if (name == null) {
        if (other.name != null)
            return false;
    } else if (!name.equals(other.name))
        return false;
    if (okActions == null) {
        if (other.okActions != null)
            return false;
    } else if (!okActions.equals(other.okActions))
        return false;
    if (severity == null) {
        if (other.severity != null)
            return false;
    } else if (!severity.equals(other.severity))
        return false;
    if (undeterminedActions == null) {
        if (other.undeterminedActions != null)
            return false;
    } else if (!undeterminedActions.equals(other.undeterminedActions))
        return false;
    return true;
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    AlarmDefinitionDeletedEvent other = (AlarmDefinitionDeletedEvent) obj;
    if (alarmDefinitionId == null) {
        if (other.alarmDefinitionId != null)
            return false;
    } else if (!alarmDefinitionId.equals(other.alarmDefinitionId))
        return false;
    if (subAlarmMetricDefinitions == null) {
        if (other.subAlarmMetricDefinitions != null)
            return false;
    } else if (!subAlarmMetricDefinitions.equals(other.subAlarmMetricDefinitions))
        return false;
    if (tenantId == null) {
        if (other.tenantId != null)
            return false;
    } else if (!tenantId.equals(other.tenantId))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (!(obj instanceof AlarmDefinitionDeletedEvent))
        return false;
    AlarmDefinitionDeletedEvent other = (AlarmDefinitionDeletedEvent) obj;
    if (alarmDefinitionId == null) {
        if (other.alarmDefinitionId != null)
            return false;
    } else if (!alarmDefinitionId.equals(other.alarmDefinitionId))
        return false;
    if (subAlarmMetricDefinitions == null) {
        if (other.subAlarmMetricDefinitions != null)
            return false;
    } else if (!subAlarmMetricDefinitions.equals(other.subAlarmMetricDefinitions))
        return false;
    return true;
}
#end_block

#method_before
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmDefinitionId == null) ? 0 : alarmDefinitionId.hashCode());
    result = prime * result + ((subAlarmMetricDefinitions == null) ? 0 : subAlarmMetricDefinitions.hashCode());
    result = prime * result + ((tenantId == null) ? 0 : tenantId.hashCode());
    return result;
}
#method_after
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmDefinitionId == null) ? 0 : alarmDefinitionId.hashCode());
    result = prime * result + ((subAlarmMetricDefinitions == null) ? 0 : subAlarmMetricDefinitions.hashCode());
    return result;
}
#end_block

#method_before
@Override
public String toString() {
    return String.format("AlarmDefinitionDeletedEvent [tenantId=%s, alarmDefinitionId=%s, subAlarmIds=%s]", tenantId, alarmDefinitionId, subAlarmMetricDefinitions);
}
#method_after
@Override
public String toString() {
    return String.format("AlarmDefinitionDeletedEvent [alarmDefinitionId=%s, subAlarmIds=%s]", alarmDefinitionId, subAlarmMetricDefinitions);
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    AlarmDefinitionUpdatedEvent other = (AlarmDefinitionUpdatedEvent) obj;
    if (alarmActionsEnabled != other.alarmActionsEnabled)
        return false;
    if (alarmDefinitionId == null) {
        if (other.alarmDefinitionId != null)
            return false;
    } else if (!alarmDefinitionId.equals(other.alarmDefinitionId))
        return false;
    if (alarmDescription == null) {
        if (other.alarmDescription != null)
            return false;
    } else if (!alarmDescription.equals(other.alarmDescription))
        return false;
    if (alarmExpression == null) {
        if (other.alarmExpression != null)
            return false;
    } else if (!alarmExpression.equals(other.alarmExpression))
        return false;
    if (alarmName == null) {
        if (other.alarmName != null)
            return false;
    } else if (!alarmName.equals(other.alarmName))
        return false;
    if (changedSubExpressions == null) {
        if (other.changedSubExpressions != null)
            return false;
    } else if (!changedSubExpressions.equals(other.changedSubExpressions))
        return false;
    if (matchBy == null) {
        if (other.matchBy != null)
            return false;
    } else if (!matchBy.equals(other.matchBy))
        return false;
    if (newAlarmSubExpressions == null) {
        if (other.newAlarmSubExpressions != null)
            return false;
    } else if (!newAlarmSubExpressions.equals(other.newAlarmSubExpressions))
        return false;
    if (oldAlarmSubExpressions == null) {
        if (other.oldAlarmSubExpressions != null)
            return false;
    } else if (!oldAlarmSubExpressions.equals(other.oldAlarmSubExpressions))
        return false;
    if (tenantId == null) {
        if (other.tenantId != null)
            return false;
    } else if (!tenantId.equals(other.tenantId))
        return false;
    if (unchangedSubExpressions == null) {
        if (other.unchangedSubExpressions != null)
            return false;
    } else if (!unchangedSubExpressions.equals(other.unchangedSubExpressions))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (!(obj instanceof AlarmDefinitionUpdatedEvent))
        return false;
    AlarmDefinitionUpdatedEvent other = (AlarmDefinitionUpdatedEvent) obj;
    if (alarmActionsEnabled != other.alarmActionsEnabled)
        return false;
    if (alarmDefinitionId == null) {
        if (other.alarmDefinitionId != null)
            return false;
    } else if (!alarmDefinitionId.equals(other.alarmDefinitionId))
        return false;
    if (alarmDescription == null) {
        if (other.alarmDescription != null)
            return false;
    } else if (!alarmDescription.equals(other.alarmDescription))
        return false;
    if (alarmExpression == null) {
        if (other.alarmExpression != null)
            return false;
    } else if (!alarmExpression.equals(other.alarmExpression))
        return false;
    if (alarmName == null) {
        if (other.alarmName != null)
            return false;
    } else if (!alarmName.equals(other.alarmName))
        return false;
    if (changedSubExpressions == null) {
        if (other.changedSubExpressions != null)
            return false;
    } else if (!changedSubExpressions.equals(other.changedSubExpressions))
        return false;
    if (matchBy == null) {
        if (other.matchBy != null)
            return false;
    } else if (!matchBy.equals(other.matchBy))
        return false;
    if (newAlarmSubExpressions == null) {
        if (other.newAlarmSubExpressions != null)
            return false;
    } else if (!newAlarmSubExpressions.equals(other.newAlarmSubExpressions))
        return false;
    if (oldAlarmSubExpressions == null) {
        if (other.oldAlarmSubExpressions != null)
            return false;
    } else if (!oldAlarmSubExpressions.equals(other.oldAlarmSubExpressions))
        return false;
    if (tenantId == null) {
        if (other.tenantId != null)
            return false;
    } else if (!tenantId.equals(other.tenantId))
        return false;
    if (unchangedSubExpressions == null) {
        if (other.unchangedSubExpressions != null)
            return false;
    } else if (!unchangedSubExpressions.equals(other.unchangedSubExpressions))
        return false;
    return true;
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    AlarmDeletedEvent other = (AlarmDeletedEvent) obj;
    if (alarmId == null) {
        if (other.alarmId != null)
            return false;
    } else if (!alarmId.equals(other.alarmId))
        return false;
    if (tenantId == null) {
        if (other.tenantId != null)
            return false;
    } else if (!tenantId.equals(other.tenantId))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (!(obj instanceof AlarmDeletedEvent))
        return false;
    AlarmDeletedEvent other = (AlarmDeletedEvent) obj;
    if (alarmDefinitionId == null) {
        if (other.alarmDefinitionId != null)
            return false;
    } else if (!alarmDefinitionId.equals(other.alarmDefinitionId))
        return false;
    if (alarmId == null) {
        if (other.alarmId != null)
            return false;
    } else if (!alarmId.equals(other.alarmId))
        return false;
    if (alarmMetrics == null) {
        if (other.alarmMetrics != null)
            return false;
    } else if (!alarmMetrics.equals(other.alarmMetrics))
        return false;
    if (subAlarmMetricDefinitions == null) {
        if (other.subAlarmMetricDefinitions != null)
            return false;
    } else if (!subAlarmMetricDefinitions.equals(other.subAlarmMetricDefinitions))
        return false;
    if (tenantId == null) {
        if (other.tenantId != null)
            return false;
    } else if (!tenantId.equals(other.tenantId))
        return false;
    return true;
}
#end_block

#method_before
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmId == null) ? 0 : alarmId.hashCode());
    result = prime * result + ((tenantId == null) ? 0 : tenantId.hashCode());
    return result;
}
#method_after
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmDefinitionId == null) ? 0 : alarmDefinitionId.hashCode());
    result = prime * result + ((alarmId == null) ? 0 : alarmId.hashCode());
    result = prime * result + ((alarmMetrics == null) ? 0 : alarmMetrics.hashCode());
    result = prime * result + ((subAlarmMetricDefinitions == null) ? 0 : subAlarmMetricDefinitions.hashCode());
    result = prime * result + ((tenantId == null) ? 0 : tenantId.hashCode());
    return result;
}
#end_block

#method_before
@Override
public String toString() {
    return "AlarmDeletedEvent [tenantId=" + tenantId + ", alarmId=" + alarmId + "]";
}
#method_after
@Override
public String toString() {
    return "AlarmDeletedEvent [tenantId=" + tenantId + ", alarmId=" + alarmId + ", alarmMetrics=" + alarmMetrics + ", alarmDefinitionId=" + alarmDefinitionId + ", subAlarmMetricDefinitions=" + subAlarmMetricDefinitions + "]";
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    AlarmUpdatedEvent other = (AlarmUpdatedEvent) obj;
    if (alarmState != other.alarmState)
        return false;
    if (alarmId == null) {
        if (other.alarmId != null)
            return false;
    } else if (!alarmId.equals(other.alarmId))
        return false;
    if (oldAlarmState != other.oldAlarmState)
        return false;
    if (tenantId == null) {
        if (other.tenantId != null)
            return false;
    } else if (!tenantId.equals(other.tenantId))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (!(obj instanceof AlarmUpdatedEvent))
        return false;
    AlarmUpdatedEvent other = (AlarmUpdatedEvent) obj;
    if (alarmDefinitionId == null) {
        if (other.alarmDefinitionId != null)
            return false;
    } else if (!alarmDefinitionId.equals(other.alarmDefinitionId))
        return false;
    if (alarmId == null) {
        if (other.alarmId != null)
            return false;
    } else if (!alarmId.equals(other.alarmId))
        return false;
    if (alarmState != other.alarmState)
        return false;
    if (oldAlarmState != other.oldAlarmState)
        return false;
    return true;
}
#end_block

#method_before
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmState == null) ? 0 : alarmState.hashCode());
    result = prime * result + ((alarmId == null) ? 0 : alarmId.hashCode());
    result = prime * result + ((oldAlarmState == null) ? 0 : oldAlarmState.hashCode());
    result = prime * result + ((tenantId == null) ? 0 : tenantId.hashCode());
    return result;
}
#method_after
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((alarmDefinitionId == null) ? 0 : alarmDefinitionId.hashCode());
    result = prime * result + ((alarmId == null) ? 0 : alarmId.hashCode());
    result = prime * result + ((alarmState == null) ? 0 : alarmState.hashCode());
    result = prime * result + ((oldAlarmState == null) ? 0 : oldAlarmState.hashCode());
    return result;
}
#end_block

#method_before
@Override
public String toString() {
    return "AlarmUpdatedEvent [tenantId=" + tenantId + ", alarmId=" + alarmId + ", alarmState=" + alarmState + ", oldAlarmState=" + oldAlarmState + "]";
}
#method_after
@Override
public String toString() {
    return "AlarmUpdatedEvent [alarmId=" + alarmId + ", alarmDefinitionId=" + alarmDefinitionId + ", alarmState=" + alarmState + ", oldAlarmState=" + oldAlarmState + "]";
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    AlarmDefinitionCreatedEvent other = (AlarmDefinitionCreatedEvent) obj;
    if (alarmDefinitionId == null) {
        if (other.alarmDefinitionId != null)
            return false;
    } else if (!alarmDefinitionId.equals(other.alarmDefinitionId))
        return false;
    if (alarmExpression == null) {
        if (other.alarmExpression != null)
            return false;
    } else if (!alarmExpression.equals(other.alarmExpression))
        return false;
    if (alarmName == null) {
        if (other.alarmName != null)
            return false;
    } else if (!alarmName.equals(other.alarmName))
        return false;
    if (alarmSubExpressions == null) {
        if (other.alarmSubExpressions != null)
            return false;
    } else if (!alarmSubExpressions.equals(other.alarmSubExpressions))
        return false;
    if (matchBy == null) {
        if (other.matchBy != null)
            return false;
    } else if (!matchBy.equals(other.matchBy))
        return false;
    if (tenantId == null) {
        if (other.tenantId != null)
            return false;
    } else if (!tenantId.equals(other.tenantId))
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (!(obj instanceof AlarmDefinitionCreatedEvent))
        return false;
    AlarmDefinitionCreatedEvent other = (AlarmDefinitionCreatedEvent) obj;
    if (alarmDefinitionId == null) {
        if (other.alarmDefinitionId != null)
            return false;
    } else if (!alarmDefinitionId.equals(other.alarmDefinitionId))
        return false;
    if (alarmExpression == null) {
        if (other.alarmExpression != null)
            return false;
    } else if (!alarmExpression.equals(other.alarmExpression))
        return false;
    if (alarmName == null) {
        if (other.alarmName != null)
            return false;
    } else if (!alarmName.equals(other.alarmName))
        return false;
    if (alarmSubExpressions == null) {
        if (other.alarmSubExpressions != null)
            return false;
    } else if (!alarmSubExpressions.equals(other.alarmSubExpressions))
        return false;
    if (matchBy == null) {
        if (other.matchBy != null)
            return false;
    } else if (!matchBy.equals(other.matchBy))
        return false;
    if (tenantId == null) {
        if (other.tenantId != null)
            return false;
    } else if (!tenantId.equals(other.tenantId))
        return false;
    return true;
}
#end_block

#method_before
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, List<String> statistics, int period) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String statsPart = buildStatsPart(statistics);
    String timePart = Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String periodPart = buildPeriodPart(period);
    String query = String.format("select time %1$s from /%2$s/ where 1=1 %3$s %4$s", statsPart, serieNameRegex, timePart, periodPart);
    logger.debug("Query string: {}", query);
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().equals(COULD_NOT_LOOK_UP_COLUMNS_EXC_MSG)) {
            return new LinkedList<>();
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    return buildStatisticsList(statistics, result);
}
#method_after
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, List<String> statistics, int period) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String statsPart = buildStatsPart(statistics);
    String timePart = Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String periodPart = buildPeriodPart(period);
    String query = String.format("select time %1$s from /%2$s/ where 1=1 %3$s %4$s", statsPart, serieNameRegex, timePart, periodPart);
    logger.debug("Query string: {}", query);
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().startsWith(Utils.COULD_NOT_LOOK_UP_COLUMNS_EXC_MSG)) {
            return new LinkedList<>();
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    return buildStatisticsList(statistics, result);
}
#end_block

#method_before
@Override
public Collection<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String timePart = Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String query = String.format("select value " + "from /%1$s/ where 1 = 1 " + " %2$s ", serieNameRegex, timePart);
    logger.debug("Query string: {}", query);
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().equals(COULD_NOT_LOOK_UP_COLUMNS_EXC_MSG)) {
            return new LinkedList<>();
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    return buildMeasurementList(result);
}
#method_after
@Override
public Collection<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String timePart = Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String query = String.format("select value " + "from /%1$s/ where 1 = 1 " + " %2$s ", serieNameRegex, timePart);
    logger.debug("Query string: {}", query);
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().startsWith(Utils.COULD_NOT_LOOK_UP_COLUMNS_EXC_MSG)) {
            return new LinkedList<>();
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    return buildMeasurementList(result);
}
#end_block

#method_before
@Override
public Collection<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String timePart = Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String query = String.format("select value " + "from /%1$s/ where 1 = 1 " + " %2$s ", serieNameRegex, timePart);
    logger.debug("Query string: {}", query);
    List<Measurements> measurementsList = new LinkedList<>();
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().equals("Couldn't look up columns")) {
            return measurementsList;
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    for (Serie serie : result) {
        Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new Utils.SerieNameDecoder(serie.getName());
        } catch (Exception e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Measurements measurements = new Measurements();
        measurements.setName(serieNameDecoder.getMetricName());
        measurements.setDimensions(serieNameDecoder.getDimensions());
        List<Object[]> valObjArryList = new LinkedList<>();
        for (Map<String, Object> row : serie.getRows()) {
            Object[] objArry = new Object[3];
            // sequence_number
            objArry[0] = ((Double) row.get(serie.getColumns()[1])).longValue();
            // time
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            objArry[1] = DATETIME_FORMATTER.print(timeDouble.longValue());
            // value
            objArry[2] = (Double) row.get(serie.getColumns()[2]);
            valObjArryList.add(objArry);
        }
        measurements.setMeasurements(valObjArryList);
        measurementsList.add(measurements);
    }
    return measurementsList;
}
#method_after
@Override
public Collection<Measurements> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String timePart = Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String query = String.format("select value " + "from /%1$s/ where 1 = 1 " + " %2$s ", serieNameRegex, timePart);
    logger.debug("Query string: {}", query);
    List<Measurements> measurementsList = new LinkedList<>();
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().equals("Couldn't look up columns")) {
            return measurementsList;
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    for (Serie serie : result) {
        Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new Utils.SerieNameDecoder(serie.getName());
        } catch (Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Measurements measurements = new Measurements();
        measurements.setName(serieNameDecoder.getMetricName());
        measurements.setDimensions(serieNameDecoder.getDimensions());
        List<Object[]> valObjArryList = new LinkedList<>();
        for (Map<String, Object> row : serie.getRows()) {
            Object[] objArry = new Object[3];
            // sequence_number
            objArry[0] = ((Double) row.get(serie.getColumns()[1])).longValue();
            // time
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            objArry[1] = DATETIME_FORMATTER.print(timeDouble.longValue());
            // value
            objArry[2] = (Double) row.get(serie.getColumns()[2]);
            valObjArryList.add(objArry);
        }
        measurements.setMeasurements(valObjArryList);
        measurementsList.add(measurements);
    }
    return measurementsList;
}
#end_block

#method_before
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String query = String.format("list series /%1$s/", serieNameRegex);
    logger.debug("Query string: {}", query);
    List<MetricDefinition> metricDefinitionList = new ArrayList<>();
    List<Serie> result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.SECONDS);
    for (Serie serie : result) {
        for (Map point : serie.getRows()) {
            Utils.SerieNameDecoder serieNameDecoder;
            try {
                serieNameDecoder = new Utils.SerieNameDecoder((String) point.get("name"));
            } catch (Exception e) {
                logger.warn("Dropping series name that is not decodable: {}", point.get("name"), e);
                continue;
            }
            MetricDefinition metricDefinition = new MetricDefinition(serieNameDecoder.getMetricName(), serieNameDecoder.getDimensions());
            metricDefinitionList.add(metricDefinition);
        }
    }
    return metricDefinitionList;
}
#method_after
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String query = String.format("list series /%1$s/", serieNameRegex);
    logger.debug("Query string: {}", query);
    List<MetricDefinition> metricDefinitionList = new ArrayList<>();
    List<Serie> result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.SECONDS);
    for (Serie serie : result) {
        for (Map point : serie.getRows()) {
            Utils.SerieNameDecoder serieNameDecoder;
            try {
                serieNameDecoder = new Utils.SerieNameDecoder((String) point.get("name"));
            } catch (Utils.SerieNameDecodeException e) {
                logger.warn("Dropping series name that is not decodable: {}", point.get("name"), e);
                continue;
            }
            MetricDefinition metricDefinition = new MetricDefinition(serieNameDecoder.getMetricName(), serieNameDecoder.getDimensions());
            metricDefinitionList.add(metricDefinition);
        }
    }
    return metricDefinitionList;
}
#end_block

#method_before
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, List<String> statistics, int period) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String statsPart = buildStatsPart(statistics);
    String timePart = Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String periodPart = buildPeriodPart(period);
    String query = String.format("select time %1$s from /%2$s/ where 1=1 %3$s %4$s", statsPart, serieNameRegex, timePart, periodPart);
    logger.debug("Query string: {}", query);
    List<Statistics> statisticsList = new LinkedList<Statistics>();
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().equals("Couldn't look up columns")) {
            return statisticsList;
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    for (Serie serie : result) {
        Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new Utils.SerieNameDecoder(serie.getName());
        } catch (Exception e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Statistics statistic = new Statistics();
        statistic.setName(serieNameDecoder.getMetricName());
        List<String> colNamesList = new LinkedList<>(statistics);
        colNamesList.add(0, "timestamp");
        statistic.setColumns(colNamesList);
        statistic.setDimensions(serieNameDecoder.getDimensions());
        List<List<Object>> valObjArryArry = new LinkedList<List<Object>>();
        statistic.setStatistics(valObjArryArry);
        for (Map<String, Object> row : serie.getRows()) {
            List<Object> valObjArry = new ArrayList<>();
            // First column is always time.
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            valObjArry.add(DATETIME_FORMATTER.print(timeDouble.longValue()));
            for (int j = 1; j < statistics.size() + 1; j++) {
                valObjArry.add(row.get(serie.getColumns()[j]));
            }
            valObjArryArry.add(valObjArry);
        }
        statisticsList.add(statistic);
    }
    return statisticsList;
}
#method_after
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, @Nullable DateTime endTime, List<String> statistics, int period) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, name, dimensions);
    String statsPart = buildStatsPart(statistics);
    String timePart = Utils.WhereClauseBuilder.buildTimePart(startTime, endTime);
    String periodPart = buildPeriodPart(period);
    String query = String.format("select time %1$s from /%2$s/ where 1=1 %3$s %4$s", statsPart, serieNameRegex, timePart, periodPart);
    logger.debug("Query string: {}", query);
    List<Statistics> statisticsList = new LinkedList<Statistics>();
    List<Serie> result = null;
    try {
        result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    } catch (RuntimeException e) {
        if (e.getMessage().equals("Couldn't look up columns")) {
            return statisticsList;
        } else {
            logger.error("Failed to get data from InfluxDB", e);
            throw e;
        }
    }
    for (Serie serie : result) {
        Utils.SerieNameDecoder serieNameDecoder;
        try {
            serieNameDecoder = new Utils.SerieNameDecoder(serie.getName());
        } catch (Utils.SerieNameDecodeException e) {
            logger.warn("Dropping series name that is not decodable: {}", serie.getName(), e);
            continue;
        }
        Statistics statistic = new Statistics();
        statistic.setName(serieNameDecoder.getMetricName());
        List<String> colNamesList = new LinkedList<>(statistics);
        colNamesList.add(0, "timestamp");
        statistic.setColumns(colNamesList);
        statistic.setDimensions(serieNameDecoder.getDimensions());
        List<List<Object>> valObjArryArry = new LinkedList<List<Object>>();
        statistic.setStatistics(valObjArryArry);
        for (Map<String, Object> row : serie.getRows()) {
            List<Object> valObjArry = new ArrayList<>();
            // First column is always time.
            Double timeDouble = (Double) row.get(serie.getColumns()[0]);
            valObjArry.add(DATETIME_FORMATTER.print(timeDouble.longValue()));
            for (int j = 1; j < statistics.size() + 1; j++) {
                valObjArry.add(row.get(serie.getColumns()[j]));
            }
            valObjArryArry.add(valObjArry);
        }
        statisticsList.add(statistic);
    }
    return statisticsList;
}
#end_block

#method_before
private MetricPipeline getMetricPipeline(MonPersisterConfiguration configuration, int threadNum, Injector injector) {
    logger.debug("Creating metric pipeline...");
    final int batchSize = configuration.getMetricConfiguration().getBatchSize();
    logger.debug("Batch size for metric pipeline [" + batchSize + "]");
    MetricHandlerFactory metricEventHandlerFactory = injector.getInstance(MetricHandlerFactory.class);
    MetricPipelineFactory metricPipelineFactory = injector.getInstance(MetricPipelineFactory.class);
    final MetricPipeline pipeline = metricPipelineFactory.create(metricEventHandlerFactory.create(threadNum, batchSize));
    logger.debug("Instance of metric pipeline fully created");
    return pipeline;
}
#method_after
private MetricPipeline getMetricPipeline(MonPersisterConfiguration configuration, int threadNum, Injector injector) {
    logger.debug("Creating metric pipeline...");
    final int batchSize = configuration.getMetricConfiguration().getBatchSize();
    logger.debug("Batch size for metric pipeline [" + batchSize + "]");
    MetricHandlerFactory metricEventHandlerFactory = injector.getInstance(MetricHandlerFactory.class);
    MetricPipelineFactory metricPipelineFactory = injector.getInstance(MetricPipelineFactory.class);
    final MetricPipeline pipeline = metricPipelineFactory.create(metricEventHandlerFactory.create(configuration.getMetricConfiguration(), threadNum, batchSize));
    logger.debug("Instance of metric pipeline fully created");
    return pipeline;
}
#end_block

#method_before
public AlarmStateTransitionPipeline getAlarmStateHistoryPipeline(MonPersisterConfiguration configuration, int threadNum, Injector injector) {
    logger.debug("Creating alarm state history pipeline...");
    int batchSize = configuration.getAlarmHistoryConfiguration().getBatchSize();
    logger.debug("Batch size for each AlarmStateHistoryPipeline [" + batchSize + "]");
    AlarmStateTransitionedEventHandlerFactory alarmHistoryEventHandlerFactory = injector.getInstance(AlarmStateTransitionedEventHandlerFactory.class);
    AlarmStateTransitionPipelineFactory alarmStateTransitionPipelineFactory = injector.getInstance(AlarmStateTransitionPipelineFactory.class);
    AlarmStateTransitionPipeline pipeline = alarmStateTransitionPipelineFactory.create(alarmHistoryEventHandlerFactory.create(threadNum, batchSize));
    logger.debug("Instance of alarm state history pipeline fully created");
    return pipeline;
}
#method_after
public AlarmStateTransitionPipeline getAlarmStateHistoryPipeline(MonPersisterConfiguration configuration, int threadNum, Injector injector) {
    logger.debug("Creating alarm state history pipeline...");
    int batchSize = configuration.getAlarmHistoryConfiguration().getBatchSize();
    logger.debug("Batch size for each AlarmStateHistoryPipeline [" + batchSize + "]");
    AlarmStateTransitionedEventHandlerFactory alarmHistoryEventHandlerFactory = injector.getInstance(AlarmStateTransitionedEventHandlerFactory.class);
    AlarmStateTransitionPipelineFactory alarmStateTransitionPipelineFactory = injector.getInstance(AlarmStateTransitionPipelineFactory.class);
    AlarmStateTransitionPipeline pipeline = alarmStateTransitionPipelineFactory.create(alarmHistoryEventHandlerFactory.create(configuration.getAlarmHistoryConfiguration(), threadNum, batchSize));
    logger.debug("Instance of alarm state history pipeline fully created");
    return pipeline;
}
#end_block

#method_before
@Override
protected void handleMessage(String message) {
    try {
        final MetricEnvelope[] envelopes = objectMapper.readValue(message, MetricEnvelope[].class);
        // other than last MetricEnvelope and persister crashed
        for (final MetricEnvelope envelope : envelopes) {
            logger.debug("{}", envelope);
            publishEvent(envelope);
        }
    } catch (Exception e) {
        logger.error("Failed to deserialize JSON message and place on pipeline queue: " + message, e);
    }
}
#method_after
@Override
protected void handleMessage(String message) {
    try {
        final MetricEnvelope[] envelopes = objectMapper.readValue(message, MetricEnvelope[].class);
        for (final MetricEnvelope envelope : envelopes) {
            logger.debug("{}", envelope);
        }
        publishEvent(envelopes);
    } catch (Exception e) {
        logger.error("Failed to deserialize JSON message and place on pipeline queue: " + message, e);
    }
}
#end_block

#method_before
@Override
protected void configure() {
    bind(MonPersisterConfiguration.class).toInstance(configuration);
    bind(Environment.class).toInstance(environment);
    install(new FactoryModuleBuilder().implement(MetricHandler.class, MetricHandler.class).build(MetricHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionedEventHandler.class, AlarmStateTransitionedEventHandler.class).build(AlarmStateTransitionedEventHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumerRunnableBasic.class, KafkaMetricsConsumerRunnableBasic.class).build(KafkaMetricsConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumerRunnableBasic.class, KafkaAlarmStateTransitionConsumerRunnableBasic.class).build(KafkaAlarmStateTransitionConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumer.class, KafkaMetricsConsumer.class).build(KafkaMetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricPipeline.class, MetricPipeline.class).build(MetricPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionPipeline.class, AlarmStateTransitionPipeline.class).build(AlarmStateTransitionPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionConsumer.class, AlarmStateTransitionConsumer.class).build(AlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumer.class, KafkaAlarmStateTransitionConsumer.class).build(KafkaAlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricsConsumer.class, MetricsConsumer.class).build(MetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaChannel.class, KafkaChannel.class).build(KafkaChannelFactory.class));
    if (configuration.getDatabaseConfiguration().getDatabaseType().equals("vertica")) {
        bind(DBI.class).toProvider(DBIProvider.class).in(Scopes.SINGLETON);
        bind(MetricRepository.class).to(VerticaMetricRepository.class);
        bind(AlarmRepository.class).to(VerticaAlarmRepository.class);
    } else if (configuration.getDatabaseConfiguration().getDatabaseType().equals("influxdb")) {
        bind(MetricRepository.class).to(InfluxDBMetricRepository.class);
        bind(AlarmRepository.class).to(InfluxDBAlarmRepository.class);
    } else {
        System.out.println("Unknown database type encountered: " + configuration.getDatabaseConfiguration().getDatabaseType());
        System.out.println("Supported databases are 'vertica' and 'influxdb'");
        System.out.println("Check your config file.");
        System.exit(1);
    }
// bind(MetricsConsumer.class);
// bind(AlarmStateTransitionsConsumer.class);
}
#method_after
@Override
protected void configure() {
    bind(MonPersisterConfiguration.class).toInstance(configuration);
    bind(Environment.class).toInstance(environment);
    install(new FactoryModuleBuilder().implement(MetricHandler.class, MetricHandler.class).build(MetricHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionedEventHandler.class, AlarmStateTransitionedEventHandler.class).build(AlarmStateTransitionedEventHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumerRunnableBasic.class, KafkaMetricsConsumerRunnableBasic.class).build(KafkaMetricsConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumerRunnableBasic.class, KafkaAlarmStateTransitionConsumerRunnableBasic.class).build(KafkaAlarmStateTransitionConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumer.class, KafkaMetricsConsumer.class).build(KafkaMetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricPipeline.class, MetricPipeline.class).build(MetricPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionPipeline.class, AlarmStateTransitionPipeline.class).build(AlarmStateTransitionPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionConsumer.class, AlarmStateTransitionConsumer.class).build(AlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumer.class, KafkaAlarmStateTransitionConsumer.class).build(KafkaAlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricsConsumer.class, MetricsConsumer.class).build(MetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaChannel.class, KafkaChannel.class).build(KafkaChannelFactory.class));
    if (configuration.getDatabaseConfiguration().getDatabaseType().equals("vertica")) {
        bind(DBI.class).toProvider(DBIProvider.class).in(Scopes.SINGLETON);
        bind(MetricRepository.class).to(VerticaMetricRepository.class);
        bind(AlarmRepository.class).to(VerticaAlarmRepository.class);
    } else if (configuration.getDatabaseConfiguration().getDatabaseType().equals("influxdb")) {
        bind(MetricRepository.class).to(InfluxDBMetricRepository.class);
        bind(AlarmRepository.class).to(InfluxDBAlarmRepository.class);
    } else {
        System.out.println("Unknown database type encountered: " + configuration.getDatabaseConfiguration().getDatabaseType());
        System.out.println("Supported databases are 'vertica' and 'influxdb'");
        System.out.println("Check your config file.");
        System.exit(1);
    }
}
#end_block

#method_before
@Override
protected KafkaConsumerRunnableBasic<MetricEnvelope> createRunnable(KafkaChannel kafkaChannel, int threadNumber) {
    return factory.create(pipeline, kafkaChannel, threadNumber);
}
#method_after
@Override
protected KafkaConsumerRunnableBasic<MetricEnvelope[]> createRunnable(KafkaChannel kafkaChannel, int threadNumber) {
    return factory.create(pipeline, kafkaChannel, threadNumber);
}
#end_block

#method_before
private Properties createKafkaProperties(KafkaConfiguration kafkaConfiguration, final String topic, int threadNum) {
    Properties properties = new Properties();
    properties.put("group.id", kafkaConfiguration.getGroupId() + "_" + topic);
    properties.put("zookeeper.connect", kafkaConfiguration.getZookeeperConnect());
    properties.put("consumer.id", String.format("%s_%d", kafkaConfiguration.getConsumerId(), threadNum));
    properties.put("socket.timeout.ms", kafkaConfiguration.getSocketTimeoutMs().toString());
    properties.put("socket.receive.buffer.bytes", kafkaConfiguration.getSocketReceiveBufferBytes().toString());
    properties.put("fetch.message.max.bytes", kafkaConfiguration.getFetchMessageMaxBytes().toString());
    // Set auto commit to false because the persister is going to explicitly commit
    properties.put("auto.commit.enable", "false");
    properties.put("queued.max.message.chunks", kafkaConfiguration.getQueuedMaxMessageChunks().toString());
    properties.put("rebalance.max.retries", kafkaConfiguration.getRebalanceMaxRetries().toString());
    properties.put("fetch.min.bytes", kafkaConfiguration.getFetchMinBytes().toString());
    properties.put("fetch.wait.max.ms", kafkaConfiguration.getFetchWaitMaxMs().toString());
    properties.put("rebalance.backoff.ms", kafkaConfiguration.getRebalanceBackoffMs().toString());
    properties.put("refresh.leader.backoff.ms", kafkaConfiguration.getRefreshLeaderBackoffMs().toString());
    properties.put("auto.offset.reset", kafkaConfiguration.getAutoOffsetReset());
    properties.put("consumer.timeout.ms", kafkaConfiguration.getConsumerTimeoutMs().toString());
    properties.put("client.id", String.format("%s_%d", kafkaConfiguration.getClientId(), threadNum));
    properties.put("zookeeper.session.timeout.ms", kafkaConfiguration.getZookeeperSessionTimeoutMs().toString());
    properties.put("zookeeper.connection.timeout.ms", kafkaConfiguration.getZookeeperConnectionTimeoutMs().toString());
    properties.put("zookeeper.sync.time.ms", kafkaConfiguration.getZookeeperSyncTimeMs().toString());
    for (String key : properties.stringPropertyNames()) {
        logger.info(KAFKA_CONFIGURATION + " " + key + " = " + properties.getProperty(key));
    }
    return properties;
}
#method_after
private Properties createKafkaProperties(KafkaConfiguration kafkaConfiguration, final PipelineConfiguration pipelineConfiguration) {
    Properties properties = new Properties();
    properties.put("group.id", pipelineConfiguration.getGroupId());
    properties.put("zookeeper.connect", kafkaConfiguration.getZookeeperConnect());
    properties.put("consumer.id", String.format("%s_%d", pipelineConfiguration.getConsumerId(), this.threadNum));
    properties.put("socket.timeout.ms", kafkaConfiguration.getSocketTimeoutMs().toString());
    properties.put("socket.receive.buffer.bytes", kafkaConfiguration.getSocketReceiveBufferBytes().toString());
    properties.put("fetch.message.max.bytes", kafkaConfiguration.getFetchMessageMaxBytes().toString());
    // Set auto commit to false because the persister is going to explicitly commit
    properties.put("auto.commit.enable", "false");
    properties.put("queued.max.message.chunks", kafkaConfiguration.getQueuedMaxMessageChunks().toString());
    properties.put("rebalance.max.retries", kafkaConfiguration.getRebalanceMaxRetries().toString());
    properties.put("fetch.min.bytes", kafkaConfiguration.getFetchMinBytes().toString());
    properties.put("fetch.wait.max.ms", kafkaConfiguration.getFetchWaitMaxMs().toString());
    properties.put("rebalance.backoff.ms", kafkaConfiguration.getRebalanceBackoffMs().toString());
    properties.put("refresh.leader.backoff.ms", kafkaConfiguration.getRefreshLeaderBackoffMs().toString());
    properties.put("auto.offset.reset", kafkaConfiguration.getAutoOffsetReset());
    properties.put("consumer.timeout.ms", kafkaConfiguration.getConsumerTimeoutMs().toString());
    properties.put("client.id", String.format("%s_%d", pipelineConfiguration.getClientId(), threadNum));
    properties.put("zookeeper.session.timeout.ms", kafkaConfiguration.getZookeeperSessionTimeoutMs().toString());
    properties.put("zookeeper.connection.timeout.ms", kafkaConfiguration.getZookeeperConnectionTimeoutMs().toString());
    properties.put("zookeeper.sync.time.ms", kafkaConfiguration.getZookeeperSyncTimeMs().toString());
    for (String key : properties.stringPropertyNames()) {
        logger.info(KAFKA_CONFIGURATION + " " + key + " = " + properties.getProperty(key));
    }
    return properties;
}
#end_block

