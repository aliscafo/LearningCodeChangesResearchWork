15
#method_before
public void stop() {
    logger.info("[{}]: stop", this.threadId);
    this.stop = true;
}
#method_after
public void stop() {
    logger.info("[{}]: stop", this.threadId);
    this.stop = true;
    int count = 0;
    while (active) {
        if (count++ >= 20) {
            break;
        }
        try {
            Thread.sleep(100);
        } catch (InterruptedException e) {
            logger.error("interrupted while waiting for the run loop to stop", e);
            break;
        }
    }
    if (!active) {
        this.kafkaChannel.markReadIfDirty();
    }
}
#end_block

#method_before
public void run() {
    logger.info("[{}]: run", this.threadId);
    final ConsumerIterator<byte[], byte[]> it = kafkaChannel.getKafkaStream().iterator();
    logger.debug("[{}]: KafkaChannel has stream iterator", this.threadId);
    while (!this.stop) {
        try {
            try {
                if (isInterrupted()) {
                    logger.debug("[{}]: is interrupted", this.threadId);
                    break;
                }
                if (it.hasNext()) {
                    if (isInterrupted()) {
                        logger.debug("[{}]: is interrupted", this.threadId);
                        break;
                    }
                    if (this.stop) {
                        logger.debug("[{}]: is stopped", this.threadId);
                        break;
                    }
                    final String msg = new String(it.next().message());
                    if (logger.isDebugEnabled()) {
                        logger.debug("[{}]: {}", this.threadId, msg);
                    }
                    publishEvent(msg);
                }
            } catch (kafka.consumer.ConsumerTimeoutException cte) {
                if (isInterrupted()) {
                    logger.debug("[{}]: is interrupted", this.threadId);
                    break;
                }
                if (this.stop) {
                    logger.debug("[{}]: is stopped", this.threadId);
                    break;
                }
                publishHeartbeat();
            }
        } catch (Throwable e) {
            logger.error("[{}]: caught fatal exception while publishing msg. Shutting entire persister down " + "now!", this.threadId, e);
            logger.error("[{}]: calling shutdown on executor service", this.threadId);
            this.executorService.shutdownNow();
            logger.error("[{}]: shutting down system. calling system.exit(1)", this.threadId);
            System.exit(1);
        }
    }
    logger.info("[{}]: calling stop on kafka channel", this.threadId);
    this.kafkaChannel.stop();
    logger.debug("[{}]: exiting main run loop", this.threadId);
}
#method_after
public void run() {
    logger.info("[{}]: run", this.threadId);
    active = true;
    final ConsumerIterator<byte[], byte[]> it = kafkaChannel.getKafkaStream().iterator();
    logger.debug("[{}]: KafkaChannel has stream iterator", this.threadId);
    while (!this.stop) {
        try {
            try {
                if (isInterrupted()) {
                    logger.debug("[{}]: is interrupted", this.threadId);
                    break;
                }
                if (it.hasNext()) {
                    if (isInterrupted()) {
                        logger.debug("[{}]: is interrupted", this.threadId);
                        break;
                    }
                    if (this.stop) {
                        logger.debug("[{}]: is stopped", this.threadId);
                        break;
                    }
                    final String msg = new String(it.next().message());
                    if (logger.isDebugEnabled()) {
                        logger.debug("[{}]: {}", this.threadId, msg);
                    }
                    publishEvent(msg);
                }
            } catch (kafka.consumer.ConsumerTimeoutException cte) {
                if (isInterrupted()) {
                    logger.debug("[{}]: is interrupted", this.threadId);
                    break;
                }
                if (this.stop) {
                    logger.debug("[{}]: is stopped", this.threadId);
                    break;
                }
                publishHeartbeat();
            }
        } catch (Throwable e) {
            logger.error("[{}]: caught fatal exception while publishing msg. Shutting entire persister down " + "now!", this.threadId, e);
            logger.error("[{}]: calling shutdown on executor service", this.threadId);
            this.executorService.shutdownNow();
            logger.error("[{}]: shutting down system. calling system.exit(1)", this.threadId);
            System.exit(1);
        }
    }
    logger.info("[{}]: calling stop on kafka channel", this.threadId);
    active = false;
    this.kafkaChannel.stop();
    logger.debug("[{}]: exiting main run loop", this.threadId);
}
#end_block

#method_before
@Override
public void addToBatch(MetricEnvelope metricEnvelope, String id) {
    Metric metric = metricEnvelope.metric;
    Map<String, Object> metaMap = metricEnvelope.meta;
    String tenantId = getMeta(TENANT_ID, metric, metaMap, id);
    String region = getMeta(REGION, metric, metaMap, id);
    String metricName = metric.getName();
    Map<String, String> dimensions = new TreeMap<>(metric.getDimensions());
    StringBuilder sb = new StringBuilder(region).append(tenantId).append(metricName);
    Iterator<String> it = dimensions.keySet().iterator();
    while (it.hasNext()) {
        String k = it.next();
        sb.append(k).append(dimensions.get(k));
    }
    byte[] defIdSha = DigestUtils.sha(sb.toString());
    Sha1HashId defIdShaHash = new Sha1HashId(defIdSha);
    if (metricIdCache.getIfPresent(defIdShaHash) == null) {
        BatchStatement batch = new BatchStatement(Type.UNLOGGED);
        addDefinitionToBatch(batch, defIdShaHash, metricName, dimensions, tenantId, region, id, metric.getTimestamp());
        batch.add(buildMeasurementInsertQuery(defIdShaHash, metric.getTimestamp(), metric.getValue(), metric.getValueMeta(), id));
        queue.offerLast(batch);
    } else {
        metricCacheHitMeter.mark();
        BatchStatement batch = new BatchStatement(Type.UNLOGGED);
        batch.add(metricUpdateStmt.bind(retention, new Timestamp(metric.getTimestamp()), region, tenantId, metricName, getDimensionList(dimensions)));
        batch.add(buildMeasurementUpdateQuery(defIdShaHash, metric.getTimestamp(), metric.getValue(), metric.getValueMeta(), id));
        queue.offerLast(batch);
    }
}
#method_after
@Override
public void addToBatch(MetricEnvelope metricEnvelope, String id) {
    Metric metric = metricEnvelope.metric;
    Map<String, Object> metaMap = metricEnvelope.meta;
    String tenantId = getMeta(TENANT_ID, metric, metaMap, id);
    String region = getMeta(REGION, metric, metaMap, id);
    String metricName = metric.getName();
    TreeMap<String, String> dimensions = metric.getDimensions() == null ? new TreeMap<String, String>() : new TreeMap<>(metric.getDimensions());
    StringBuilder sb = new StringBuilder(region).append(tenantId).append(metricName);
    Iterator<String> it = dimensions.keySet().iterator();
    while (it.hasNext()) {
        String k = it.next();
        sb.append(k).append(dimensions.get(k));
    }
    byte[] defIdSha = DigestUtils.sha(sb.toString());
    Sha1HashId defIdShaHash = new Sha1HashId(defIdSha);
    if (cluster.getMetricIdCache().getIfPresent(defIdShaHash.toHexString()) == null) {
        addDefinitionToBatch(defIdShaHash, metricName, dimensions, tenantId, region, id, metric.getTimestamp());
        batches.addMeasurementQuery(buildMeasurementInsertQuery(defIdShaHash, metric.getTimestamp(), metric.getValue(), metric.getValueMeta(), region, tenantId, metricName, dimensions, id));
    } else {
        metricCacheHitMeter.mark();
        batches.addMetricQuery(cluster.getMetricUpdateStmt().bind(retention, new Timestamp(metric.getTimestamp()), region, tenantId, metricName, getDimensionList(dimensions), new ArrayList<>(dimensions.keySet())));
        batches.addMeasurementQuery(buildMeasurementUpdateQuery(defIdShaHash, metric.getTimestamp(), metric.getValue(), metric.getValueMeta(), id));
    }
    metricCount++;
}
#end_block

#method_before
private BoundStatement buildMeasurementUpdateQuery(Sha1HashId defId, long timeStamp, double value, Map<String, String> valueMeta, String id) {
    String valueMetaString = getValueMetaString(valueMeta, id);
    if (logger.isDebugEnabled()) {
        logger.debug("[{}]: adding metric to batch: defDimsId: {}, time: {}, value: {}, value meta {}", id, defId.toHexString(), timeStamp, value, valueMetaString);
    }
    measurementMeter.mark();
    return measurementUpdateStmt.bind(retention, value, valueMetaString, defId.getSha1HashByteBuffer(), new Timestamp(timeStamp));
}
#method_after
private BoundStatement buildMeasurementUpdateQuery(Sha1HashId defId, long timeStamp, double value, Map<String, String> valueMeta, String id) {
    String valueMetaString = getValueMetaString(valueMeta, id);
    if (logger.isDebugEnabled()) {
        logger.debug("[{}]: adding metric to batch: metric id: {}, time: {}, value: {}, value meta {}", id, defId.toHexString(), timeStamp, value, valueMetaString);
    }
    return cluster.getMeasurementUpdateStmt().bind(retention, value, valueMetaString, defId.getSha1HashByteBuffer(), new Timestamp(timeStamp));
}
#end_block

#method_before
private BoundStatement buildMeasurementInsertQuery(Sha1HashId defId, long timeStamp, double value, Map<String, String> valueMeta, String id) {
    String valueMetaString = getValueMetaString(valueMeta, id);
    if (logger.isDebugEnabled()) {
        logger.debug("[{}]: adding metric to batch: defDimsId: {}, time: {}, value: {}, value meta {}", id, defId.toHexString(), timeStamp, value, valueMetaString);
    }
    measurementMeter.mark();
    return measurementInsertStmt.bind(retention, value, valueMetaString, defId.getSha1HashByteBuffer(), new Timestamp(timeStamp));
}
#method_after
private BoundStatement buildMeasurementInsertQuery(Sha1HashId defId, long timeStamp, double value, Map<String, String> valueMeta, String region, String tenantId, String metricName, Map<String, String> dimensions, String id) {
    String valueMetaString = getValueMetaString(valueMeta, id);
    if (logger.isDebugEnabled()) {
        logger.debug("[{}]: adding metric to batch: metric id: {}, time: {}, value: {}, value meta {}", id, defId.toHexString(), timeStamp, value, valueMetaString);
    }
    measurementMeter.mark();
    return cluster.getMeasurementInsertStmt().bind(retention, value, valueMetaString, region, tenantId, metricName, getDimensionList(dimensions), defId.getSha1HashByteBuffer(), new Timestamp(timeStamp));
}
#end_block

#method_before
private void addDefinitionToBatch(BatchStatement batch, Sha1HashId defId, String metricName, Map<String, String> dimensions, String tenantId, String region, String id, long timestamp) {
    metricCacheMissMeter.mark();
    if (logger.isDebugEnabled()) {
        logger.debug("[{}]: adding definition to batch: defId: {}, name: {}, tenantId: {}, region: {}", id, defId.toHexString(), metricName, tenantId, region);
    }
    Timestamp ts = new Timestamp(timestamp);
    batch.add(metricInsertStmt.bind(retention, defId.getSha1HashByteBuffer(), ts, ts, region, tenantId, metricName, getDimensionList(dimensions)));
    for (Map.Entry<String, String> entry : dimensions.entrySet()) {
        String name = entry.getKey();
        String value = entry.getValue();
        if (logger.isDebugEnabled()) {
            logger.debug("[{}]: adding dimension to batch: defId: {}, name: {}, value: {}", id, defId.toHexString(), name, value);
        }
        batch.add(dimensionStmt.bind(region, tenantId, name, value));
        batch.add(dimensionMetricStmt.bind(region, tenantId, name, value, metricName));
    }
    metricIdCache.put(defId, defId);
}
#method_after
private void addDefinitionToBatch(Sha1HashId defId, String metricName, Map<String, String> dimensions, String tenantId, String region, String id, long timestamp) {
    metricCacheMissMeter.mark();
    if (logger.isDebugEnabled()) {
        logger.debug("[{}]: adding definition to batch: defId: {}, name: {}, tenantId: {}, region: {}", id, defId.toHexString(), metricName, tenantId, region);
    }
    Timestamp ts = new Timestamp(timestamp);
    batches.addMetricQuery(cluster.getMetricInsertStmt().bind(retention, defId.getSha1HashByteBuffer(), ts, ts, region, tenantId, metricName, getDimensionList(dimensions), new ArrayList<>(dimensions.keySet())));
    for (Map.Entry<String, String> entry : dimensions.entrySet()) {
        String name = entry.getKey();
        String value = entry.getValue();
        String dimensionKey = cluster.getDimnesionEntryKey(region, tenantId, name, value);
        if (cluster.getDimensionCache().getIfPresent(dimensionKey) != null) {
            dimensionCacheHitMeter.mark();
        } else {
            dimensionCacheMissMeter.mark();
            if (logger.isDebugEnabled()) {
                logger.debug("[{}]: adding dimension to batch: defId: {}, name: {}, value: {}", id, defId.toHexString(), name, value);
            }
            batches.addDimensionQuery(cluster.getDimensionStmt().bind(region, tenantId, name, value));
            cluster.getDimensionCache().put(dimensionKey, Boolean.TRUE);
        }
        String metricDimensionKey = cluster.getMetricDimnesionEntryKey(region, tenantId, metricName, name, value);
        if (cluster.getMetricDimensionCache().getIfPresent(metricDimensionKey) != null) {
            metricDimensionCacheHitMeter.mark();
        } else {
            metricDimensionCacheMissMeter.mark();
            batches.addDimensionMetricQuery(cluster.getDimensionMetricStmt().bind(region, tenantId, name, value, metricName));
            batches.addMetricDimensionQuery(cluster.getMetricDimensionStmt().bind(region, tenantId, metricName, name, value));
            cluster.getMetricDimensionCache().put(metricDimensionKey, Boolean.TRUE);
        }
    }
    String metricId = defId.toHexString();
    cluster.getMetricIdCache().put(metricId, Boolean.TRUE);
}
#end_block

#method_before
private List<String> getDimensionList(Map<String, String> dimensions) {
    List<String> list = new ArrayList<>(dimensions.size());
    for (Entry<String, String> dim : dimensions.entrySet()) {
        list.add(new StringBuffer(urlEncode(dim.getKey())).append("=").append(urlEncode(dim.getValue())).toString());
    }
    return list;
}
#method_after
public List<String> getDimensionList(Map<String, String> dimensions) {
    List<String> list = new ArrayList<>(dimensions.size());
    for (Entry<String, String> dim : dimensions.entrySet()) {
        list.add(new StringBuffer(dim.getKey()).append('\t').append(dim.getValue()).toString());
    }
    return list;
}
#end_block

#method_before
@Override
public int flush(String id) throws RepoException {
    try {
        return handleFlush(id);
    } catch (Exception e) {
        throw new RepoException(e);
    }
}
#method_after
@Override
public int flush(String id) throws RepoException {
    long startTime = System.nanoTime();
    List<ResultSetFuture> results = new ArrayList<>();
    List<Deque<BatchStatement>> list = batches.getAllBatches();
    for (Deque<BatchStatement> q : list) {
        BatchStatement b;
        while ((b = q.poll()) != null) {
            results.add(session.executeAsync(b));
        }
    }
    List<ListenableFuture<ResultSet>> futures = Futures.inCompletionOrder(results);
    boolean cancel = false;
    Exception ex = null;
    for (ListenableFuture<ResultSet> future : futures) {
        if (cancel) {
            future.cancel(false);
            continue;
        }
        try {
            future.get();
        } catch (InterruptedException | ExecutionException e) {
            cancel = true;
            ex = e;
        }
    }
    this.commitTimer.update(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
    if (ex != null) {
        metricFailed.inc(metricCount);
        throw new RepoException(ex);
    }
    batches.clear();
    int flushCnt = metricCount;
    metricCount = 0;
    metricCompleted.inc(flushCnt);
    return flushCnt;
}
#end_block

#method_before
public void addToBatch(AlarmStateTransitionedEvent message, String id) {
    String metricsString = getSerializedString(message.metrics, id);
    // Validate metricsString does not exceed a sufficient maximum upper bound
    if (metricsString.length() * MAX_BYTES_PER_CHAR >= MAX_LENGTH_VARCHAR) {
        metricsString = "[]";
        logger.warn("length of metricsString for alarm ID {} exceeds max length of {}", message.alarmId, MAX_LENGTH_VARCHAR);
    }
    String subAlarmsString = getSerializedString(message.subAlarms, id);
    if (subAlarmsString.length() * MAX_BYTES_PER_CHAR >= MAX_LENGTH_VARCHAR) {
        subAlarmsString = "[]";
        logger.warn("length of subAlarmsString for alarm ID {} exceeds max length of {}", message.alarmId, MAX_LENGTH_VARCHAR);
    }
    queue.offerLast(preparedQuery.bind(metricsString, message.oldState.name(), message.newState.name(), subAlarmsString, message.stateChangeReason, EMPTY_REASON_DATA, message.tenantId, message.alarmId, message.timestamp));
}
#method_after
public void addToBatch(AlarmStateTransitionedEvent message, String id) {
    String metricsString = getSerializedString(message.metrics, id);
    // Validate metricsString does not exceed a sufficient maximum upper bound
    if (metricsString.length() * MAX_BYTES_PER_CHAR >= MAX_LENGTH_VARCHAR) {
        metricsString = "[]";
        logger.warn("length of metricsString for alarm ID {} exceeds max length of {}", message.alarmId, MAX_LENGTH_VARCHAR);
    }
    String subAlarmsString = getSerializedString(message.subAlarms, id);
    if (subAlarmsString.length() * MAX_BYTES_PER_CHAR >= MAX_LENGTH_VARCHAR) {
        subAlarmsString = "[]";
        logger.warn("length of subAlarmsString for alarm ID {} exceeds max length of {}", message.alarmId, MAX_LENGTH_VARCHAR);
    }
    queue.offerLast(cluster.getAlarmHistoryInsertStmt().bind(retention, metricsString, message.oldState.name(), message.newState.name(), subAlarmsString, message.stateChangeReason, EMPTY_REASON_DATA, message.tenantId, message.alarmId, new Timestamp(message.timestamp)));
}
#end_block

#method_before
protected void executeQuery(String id, Statement query) throws DriverException {
    _executeQuery(id, query, 0);
}
#method_after
protected void executeQuery(String id, Statement query, long startTime) throws DriverException {
    _executeQuery(id, query, startTime, 0);
}
#end_block

#method_before
private void _executeQuery(String id, Statement query, int retryCount) {
    try {
        Timer.Context context = commitTimer.time();
        session.execute(query);
        context.stop();
        completed.incrementAndGet();
    // retry only with the recoverable failures
    } catch (NoHostAvailableException e) {
        retryQuery(id, query, retryCount, e);
    } catch (BootstrappingException e) {
        retryQuery(id, query, retryCount, e);
    } catch (OverloadedException e) {
        retryQuery(id, query, retryCount, e);
    } catch (QueryConsistencyException e) {
        retryQuery(id, query, retryCount, e);
    } catch (UnavailableException e) {
        retryQuery(id, query, retryCount, e);
    } catch (DriverException e) {
        failed.incrementAndGet();
        throw e;
    }
}
#method_after
private void _executeQuery(final String id, final Statement query, final long startTime, final int retryCount) {
    try {
        session.execute(query);
        commitTimer.update(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
    // ResultSetFuture future = session.executeAsync(query);
    // Futures.addCallback(future, new FutureCallback<ResultSet>() {
    // @Override
    // public void onSuccess(ResultSet result) {
    // metricCompleted.inc();
    // commitTimer.update(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
    // }
    // 
    // @Override
    // public void onFailure(Throwable t) {
    // if (t instanceof NoHostAvailableException | t instanceof
    // BootstrappingException
    // | t instanceof OverloadedException | t instanceof QueryConsistencyException
    // | t instanceof UnavailableException) {
    // retryQuery(id, query, startTime, retryCount, (DriverException) t);
    // } else {
    // metricFailed.inc();
    // commitTimer.update(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
    // logger.error("Failed to execute query.", t);
    // }
    // }
    // }, MoreExecutors.sameThreadExecutor());
    } catch (NoHostAvailableException | BootstrappingException | OverloadedException | QueryConsistencyException | UnavailableException | OperationTimedOutException e) {
        retryQuery(id, query, startTime, retryCount, e);
    } catch (DriverException e) {
        metricFailed.inc(((BatchStatement) query).size());
        commitTimer.update(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
        throw e;
    }
}
#end_block

#method_before
private void retryQuery(String id, Statement query, int retryCount, DriverException e) throws DriverException {
    if (retryCount >= maxWriteRetries) {
        logger.error("[{}]: Query aborted after {} retry: ", id, retryCount, e.getMessage());
        failed.incrementAndGet();
        throw e;
    } else {
        logger.info("[{}]: Query failed, retrying {} of {}: {} ", id, retryCount, maxWriteRetries, e.getMessage());
        try {
            Thread.sleep(1000 * (1 << retryCount));
        } catch (InterruptedException ie) {
            logger.debug("[{}]: Interrupted: {}", id, ie);
        }
        _executeQuery(id, query, retryCount++);
    }
}
#method_after
private void retryQuery(String id, Statement query, final long startTime, int retryCount, DriverException e) throws DriverException {
    if (retryCount >= maxWriteRetries) {
        logger.error("[{}]: Query aborted after {} retry: ", id, retryCount, e.getMessage());
        metricFailed.inc(((BatchStatement) query).size());
        commitTimer.update(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
        throw e;
    } else {
        logger.warn("[{}]: Query failed, retrying {} of {}: {} ", id, retryCount, maxWriteRetries, e.getMessage());
        try {
            Thread.sleep(1000 * (1 << retryCount));
        } catch (InterruptedException ie) {
            logger.debug("[{}]: Interrupted: {}", id, ie);
        }
        _executeQuery(id, query, startTime, retryCount++);
    }
}
#end_block

#method_before
public int handleFlush(String id) {
    BatchStatement batch = new BatchStatement(Type.UNLOGGED);
    Statement query;
    for (int i = 0; i < batchSize; i++) {
        query = queue.poll();
        if (query == null) {
            break;
        }
        batch.add(query);
    }
    if (batch.size() == 0) {
        return 0;
    }
    Stopwatch sw = Stopwatch.createStarted();
    executeQuery(id, batch);
    sw.stop();
    if (logger.isDebugEnabled()) {
        logger.debug("[{}]: total time for writing batch size of {}: {}", id, batch.size(), sw);
    }
    if (timeToLogMetrics()) {
        logMetrics();
    }
    return batch.size();
}
#method_after
public int handleFlush(String id) throws RepoException {
    long startTime = System.nanoTime();
    int flushedCount = 0;
    List<ResultSetFuture> results = new ArrayList<>(queue.size());
    Statement query;
    while ((query = queue.poll()) != null) {
        flushedCount++;
        results.add(session.executeAsync(query));
    }
    List<ListenableFuture<ResultSet>> futures = Futures.inCompletionOrder(results);
    boolean cancel = false;
    Exception ex = null;
    for (ListenableFuture<ResultSet> future : futures) {
        if (cancel) {
            future.cancel(false);
            continue;
        }
        try {
            future.get();
        } catch (InterruptedException | ExecutionException e) {
            cancel = true;
            ex = e;
        }
    }
    commitTimer.update(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);
    if (ex != null) {
        throw new RepoException(ex);
    }
    return flushedCount;
}
#end_block

#method_before
@Override
public String toString() {
    return "Sha1HashId{" + "sha1Hash=" + Hex.encodeHexString(sha1Hash) + "}";
}
#method_after
@Override
public String toString() {
    return "Sha1HashId{" + "sha1Hash=" + hex + "}";
}
#end_block

#method_before
public String toHexString() {
    return Hex.encodeHexString(sha1Hash);
}
#method_after
public String toHexString() {
    return hex;
}
#end_block

