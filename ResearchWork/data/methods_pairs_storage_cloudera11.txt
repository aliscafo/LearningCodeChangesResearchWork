366
#method_before
public void migrateConjunctsToInlineView(Analyzer analyzer, InlineViewRef inlineViewRef) {
    List<Expr> unassignedConjuncts = analyzer.getUnassignedConjuncts(inlineViewRef.getId().asList(), true);
    if (!canMigrateConjuncts(inlineViewRef)) {
        // mark (fully resolve) slots referenced by unassigned conjuncts as
        // materialized
        List<Expr> substUnassigned = Expr.substituteList(unassignedConjuncts, inlineViewRef.getBaseTblSmap(), analyzer, false);
        analyzer.materializeSlots(substUnassigned);
        return;
    }
    List<Expr> preds = Lists.newArrayList();
    for (Expr e : unassignedConjuncts) {
        if (analyzer.canEvalPredicate(inlineViewRef.getId().asList(), e)) {
            preds.add(e);
        }
    }
    unassignedConjuncts.removeAll(preds);
    // Generate predicates to enforce equivalences among slots of the inline view
    // tuple. These predicates are also migrated into the inline view.
    analyzer.createEquivConjuncts(inlineViewRef.getId(), preds);
    // create new predicates against the inline view's unresolved result exprs, not
    // the resolved result exprs, in order to avoid skipping scopes (and ignoring
    // limit clauses on the way)
    List<Expr> viewPredicates = Expr.substituteList(preds, inlineViewRef.getSmap(), analyzer, false);
    // Remove unregistered predicates that reference the same slot on
    // both sides (e.g. a = a). Such predicates have been generated from slot
    // equivalences and may incorrectly reject rows with nulls (IMPALA-1412).
    Predicate<Expr> isIdentityPredicate = new Predicate<Expr>() {

        @Override
        public boolean apply(Expr expr) {
            if (!(expr instanceof BinaryPredicate) || ((BinaryPredicate) expr).getOp() != BinaryPredicate.Operator.EQ) {
                return false;
            }
            if (!expr.isRegisteredPredicate() && expr.getChild(0) instanceof SlotRef && expr.getChild(1) instanceof SlotRef && (((SlotRef) expr.getChild(0)).getSlotId() == ((SlotRef) expr.getChild(1)).getSlotId())) {
                return true;
            }
            return false;
        }
    };
    Iterables.removeIf(viewPredicates, isIdentityPredicate);
    // "migrate" conjuncts_ by marking them as assigned and re-registering them with
    // new ids.
    // Mark pre-substitution conjuncts as assigned, since the ids of the new exprs may
    // have changed.
    analyzer.markConjunctsAssigned(preds);
    try {
        inlineViewRef.getAnalyzer().registerConjuncts(viewPredicates);
    } catch (AnalysisException ex) {
        throw new IllegalStateException("Caught an AnalysisException in the planner", ex);
    }
    // mark (fully resolve) slots referenced by remaining unassigned conjuncts as
    // materialized
    List<Expr> substUnassigned = Expr.substituteList(unassignedConjuncts, inlineViewRef.getBaseTblSmap(), analyzer, false);
    analyzer.materializeSlots(substUnassigned);
}
#method_after
public void migrateConjunctsToInlineView(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    List<Expr> unassignedConjuncts = analyzer.getUnassignedConjuncts(inlineViewRef.getId().asList(), true);
    if (!canMigrateConjuncts(inlineViewRef)) {
        // mark (fully resolve) slots referenced by unassigned conjuncts as
        // materialized
        List<Expr> substUnassigned = Expr.substituteList(unassignedConjuncts, inlineViewRef.getBaseTblSmap(), analyzer, false);
        analyzer.materializeSlots(substUnassigned);
        return;
    }
    List<Expr> preds = Lists.newArrayList();
    for (Expr e : unassignedConjuncts) {
        if (analyzer.canEvalPredicate(inlineViewRef.getId().asList(), e)) {
            preds.add(e);
        }
    }
    unassignedConjuncts.removeAll(preds);
    // Generate predicates to enforce equivalences among slots of the inline view
    // tuple. These predicates are also migrated into the inline view.
    analyzer.createEquivConjuncts(inlineViewRef.getId(), preds);
    // create new predicates against the inline view's unresolved result exprs, not
    // the resolved result exprs, in order to avoid skipping scopes (and ignoring
    // limit clauses on the way)
    List<Expr> viewPredicates = Expr.substituteList(preds, inlineViewRef.getSmap(), analyzer, false);
    // Remove unregistered predicates that reference the same slot on
    // both sides (e.g. a = a). Such predicates have been generated from slot
    // equivalences and may incorrectly reject rows with nulls (IMPALA-1412).
    Predicate<Expr> isIdentityPredicate = new Predicate<Expr>() {

        @Override
        public boolean apply(Expr expr) {
            if (!(expr instanceof BinaryPredicate) || ((BinaryPredicate) expr).getOp() != BinaryPredicate.Operator.EQ) {
                return false;
            }
            if (!expr.isRegisteredPredicate() && expr.getChild(0) instanceof SlotRef && expr.getChild(1) instanceof SlotRef && (((SlotRef) expr.getChild(0)).getSlotId() == ((SlotRef) expr.getChild(1)).getSlotId())) {
                return true;
            }
            return false;
        }
    };
    Iterables.removeIf(viewPredicates, isIdentityPredicate);
    // "migrate" conjuncts_ by marking them as assigned and re-registering them with
    // new ids.
    // Mark pre-substitution conjuncts as assigned, since the ids of the new exprs may
    // have changed.
    analyzer.markConjunctsAssigned(preds);
    inlineViewRef.getAnalyzer().registerConjuncts(viewPredicates);
    // mark (fully resolve) slots referenced by remaining unassigned conjuncts as
    // materialized
    List<Expr> substUnassigned = Expr.substituteList(unassignedConjuncts, inlineViewRef.getBaseTblSmap(), analyzer, false);
    analyzer.materializeSlots(substUnassigned);
}
#end_block

#method_before
private PlanNode createUnionPlan(UnionStmt unionStmt, Analyzer analyzer) throws ImpalaException {
    List<Expr> conjuncts = analyzer.getUnassignedConjuncts(unionStmt.getTupleId().asList(), false);
    if (!unionStmt.hasAnalyticExprs()) {
        // pick up propagated predicates.
        for (UnionOperand op : unionStmt.getOperands()) {
            List<Expr> opConjuncts = Expr.substituteList(conjuncts, op.getSmap(), analyzer, false);
            try {
                op.getAnalyzer().registerConjuncts(opConjuncts);
            } catch (AnalysisException ex) {
                throw new IllegalStateException("Caught an AnalysisException in the planner", ex);
            }
        }
        analyzer.markConjunctsAssigned(conjuncts);
    } else {
        // mark slots referenced by the yet-unassigned conjuncts
        analyzer.materializeSlots(conjuncts);
    }
    // mark slots after predicate propagation but prior to plan tree generation
    unionStmt.materializeRequiredSlots(analyzer);
    PlanNode result = null;
    // create DISTINCT tree
    if (unionStmt.hasDistinctOps()) {
        result = createUnionPlan(analyzer, unionStmt, unionStmt.getDistinctOperands(), null);
        result = new AggregationNode(ctx_.getNextNodeId(), result, unionStmt.getDistinctAggInfo());
        result.init(analyzer);
    }
    // create ALL tree
    if (unionStmt.hasAllOps()) {
        result = createUnionPlan(analyzer, unionStmt, unionStmt.getAllOperands(), result);
    }
    if (unionStmt.hasAnalyticExprs()) {
        result = addUnassignedConjuncts(analyzer, unionStmt.getTupleId().asList(), result);
    }
    return result;
}
#method_after
private PlanNode createUnionPlan(UnionStmt unionStmt, Analyzer analyzer) throws ImpalaException {
    List<Expr> conjuncts = analyzer.getUnassignedConjuncts(unionStmt.getTupleId().asList(), false);
    if (!unionStmt.hasAnalyticExprs()) {
        // pick up propagated predicates.
        for (UnionOperand op : unionStmt.getOperands()) {
            List<Expr> opConjuncts = Expr.substituteList(conjuncts, op.getSmap(), analyzer, false);
            op.getAnalyzer().registerConjuncts(opConjuncts);
        }
        analyzer.markConjunctsAssigned(conjuncts);
    } else {
        // mark slots referenced by the yet-unassigned conjuncts
        analyzer.materializeSlots(conjuncts);
    }
    // mark slots after predicate propagation but prior to plan tree generation
    unionStmt.materializeRequiredSlots(analyzer);
    PlanNode result = null;
    // create DISTINCT tree
    if (unionStmt.hasDistinctOps()) {
        result = createUnionPlan(analyzer, unionStmt, unionStmt.getDistinctOperands(), null);
        result = new AggregationNode(ctx_.getNextNodeId(), result, unionStmt.getDistinctAggInfo());
        result.init(analyzer);
    }
    // create ALL tree
    if (unionStmt.hasAllOps()) {
        result = createUnionPlan(analyzer, unionStmt, unionStmt.getAllOperands(), result);
    }
    if (unionStmt.hasAnalyticExprs()) {
        result = addUnassignedConjuncts(analyzer, unionStmt.getTupleId().asList(), result);
    }
    return result;
}
#end_block

#method_before
public void setItemTupleDesc(TupleDescriptor t) {
    itemTupleDesc_ = t;
}
#method_after
public void setItemTupleDesc(TupleDescriptor t) {
    Preconditions.checkState(itemTupleDesc_ == null, "Item tuple descriptor already set.");
    itemTupleDesc_ = t;
}
#end_block

#method_before
public SlotDescriptor getSlotDescriptor(String qualifiedColumnName) {
    return slotRefMap_.get(qualifiedColumnName);
}
#method_after
public SlotDescriptor getSlotDescriptor(String qualifiedColumnName) {
    return slotPathMap_.get(qualifiedColumnName);
}
#end_block

#method_before
public SlotDescriptor registerSlotRef(Path slotPath, boolean ifNotExists) throws AnalysisException {
    // SlotRefs are registered against the slot's fully qualified lowercase path.
    TupleDescriptor tupleDesc = slotPath.getRootDesc();
    String key = slotPath.toString();
    SlotDescriptor existingSlotDesc = slotRefMap_.get(key);
    if (ifNotExists && existingSlotDesc != null)
        return existingSlotDesc;
    SlotDescriptor result = addSlotDescriptor(tupleDesc);
    result.setPath(slotPath);
    // Ensure slotRefMap_ always contains the first registered slot desc.
    if (existingSlotDesc == null)
        slotRefMap_.put(slotPath.toString(), result);
    return result;
}
#method_after
public SlotDescriptor registerSlotRef(Path slotPath) throws AnalysisException {
    Preconditions.checkNotNull(slotPath.getRootDesc());
    // Always register a new slot descriptor for collection types.
    if (slotPath.destType().isCollectionType()) {
        SlotDescriptor result = addSlotDescriptor(slotPath.getRootDesc());
        result.setPath(slotPath);
        return result;
    }
    // SlotRefs with a scalar type are registered against the slot's
    // fully-qualified lowercase path.
    String key = slotPath.toString();
    SlotDescriptor existingSlotDesc = slotPathMap_.get(key);
    if (existingSlotDesc != null)
        return existingSlotDesc;
    SlotDescriptor result = addSlotDescriptor(slotPath.getRootDesc());
    result.setPath(slotPath);
    slotPathMap_.put(key, result);
    return result;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    Preconditions.checkNotNull(getPrivilegeRequirement());
    desc_ = analyzer.registerTableRef(this);
    if (isRelativeRef()) {
        // Pass ifNotExists == false to always register a new slot desc in the root tuple
        // desc to avoid sharing them among multiple references to the same collection.
        SlotDescriptor parentSlotDesc = analyzer.registerSlotRef(resolvedPath_, false);
        parentSlotDesc.setItemTupleDesc(desc_);
        collectionExpr_ = new SlotRef(parentSlotDesc);
        // Must always be materialized to ensure the correct cardinality after unnesting.
        analyzer.materializeSlots(collectionExpr_);
        Analyzer parentAnalyzer = analyzer.findAnalyzer(resolvedPath_.getRootDesc().getId());
        Preconditions.checkNotNull(parentAnalyzer);
        isCorrelated_ = parentAnalyzer != analyzer;
    }
    isAnalyzed_ = true;
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    Preconditions.checkNotNull(getPrivilegeRequirement());
    desc_ = analyzer.registerTableRef(this);
    if (isRelativeRef()) {
        SlotDescriptor parentSlotDesc = analyzer.registerSlotRef(resolvedPath_);
        parentSlotDesc.setItemTupleDesc(desc_);
        collectionExpr_ = new SlotRef(parentSlotDesc);
        // Must always be materialized to ensure the correct cardinality after unnesting.
        analyzer.materializeSlots(collectionExpr_);
        Analyzer parentAnalyzer = analyzer.findAnalyzer(resolvedPath_.getRootDesc().getId());
        Preconditions.checkNotNull(parentAnalyzer);
        isCorrelated_ = parentAnalyzer != analyzer;
    }
    isAnalyzed_ = true;
    analyzeJoin(analyzer);
}
#end_block

#method_before
@Override
public TTableDescriptor toThriftDescriptor(Set<Long> referencedPartitions) {
    TTableDescriptor tableDescriptor = new TTableDescriptor(id_.asInt(), TTableType.HBASE_TABLE, getColumns().size(), numClusteringCols_, getTColumnTypes(), hbaseTableName_, db_.getName());
    tableDescriptor.setHbaseTable(getTHBaseTable());
    tableDescriptor.setColNames(getColumnNames());
    return tableDescriptor;
}
#method_after
@Override
public TTableDescriptor toThriftDescriptor(Set<Long> referencedPartitions) {
    TTableDescriptor tableDescriptor = new TTableDescriptor(id_.asInt(), TTableType.HBASE_TABLE, getTColumnDescriptors(), numClusteringCols_, hbaseTableName_, db_.getName());
    tableDescriptor.setHbaseTable(getTHBaseTable());
    return tableDescriptor;
}
#end_block

#method_before
@Override
public int getNumNodes() {
    // TODO: implement
    return 100;
}
#method_after
public int getNumNodes() {
    // TODO: implement
    return 100;
}
#end_block

#method_before
@Override
public int getNumNodes() {
    return 1;
}
#method_after
public int getNumNodes() {
    return 1;
}
#end_block

#method_before
@Override
public TTableDescriptor toThriftDescriptor(Set<Long> referencedPartitions) {
    TTableDescriptor tableDesc = new TTableDescriptor(id_.asInt(), TTableType.DATA_SOURCE_TABLE, getColumns().size(), numClusteringCols_, getTColumnTypes(), name_, db_.getName());
    tableDesc.setDataSourceTable(getDataSourceTable());
    tableDesc.setColNames(getColumnNames());
    return tableDesc;
}
#method_after
@Override
public TTableDescriptor toThriftDescriptor(Set<Long> referencedPartitions) {
    TTableDescriptor tableDesc = new TTableDescriptor(id_.asInt(), TTableType.DATA_SOURCE_TABLE, getTColumnDescriptors(), numClusteringCols_, name_, db_.getName());
    tableDesc.setDataSourceTable(getDataSourceTable());
    return tableDesc;
}
#end_block

#method_before
public static Table fromMetastoreTable(TableId id, Db db, org.apache.hadoop.hive.metastore.api.Table msTbl) {
    // Create a table of appropriate type
    Table table = null;
    if (TableType.valueOf(msTbl.getTableType()) == TableType.VIRTUAL_VIEW) {
        table = new View(id, msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    } else if (HBaseTable.isHBaseTable(msTbl)) {
        table = new HBaseTable(id, msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    } else if (DataSourceTable.isDataSourceTable(msTbl)) {
        // It's important to check if this is a DataSourceTable before HdfsTable because
        // DataSourceTables are still represented by HDFS tables in the metastore but
        // have a special table property to indicate that Impala should use an external
        // data source.
        table = new DataSourceTable(id, msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    } else if (HdfsFileFormat.isHdfsFormatClass(msTbl.getSd().getInputFormat())) {
        table = new HdfsTable(id, msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    }
    return table;
}
#method_after
public static Table fromMetastoreTable(TableId id, Db db, org.apache.hadoop.hive.metastore.api.Table msTbl) {
    // Create a table of appropriate type
    Table table = null;
    if (TableType.valueOf(msTbl.getTableType()) == TableType.VIRTUAL_VIEW) {
        table = new View(id, msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    } else if (HBaseTable.isHBaseTable(msTbl)) {
        table = new HBaseTable(id, msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    } else if (DataSourceTable.isDataSourceTable(msTbl)) {
        // It's important to check if this is a DataSourceTable before HdfsTable because
        // DataSourceTables are still represented by HDFS tables in the metastore but
        // have a special table property to indicate that Impala should use an external
        // data source.
        table = new DataSourceTable(id, msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    } else if (HdfsFileFormat.isHdfsInputFormatClass(msTbl.getSd().getInputFormat())) {
        table = new HdfsTable(id, msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    }
    return table;
}
#end_block

#method_before
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    if (!SUPPORTS_VOLUME_ID)
        return;
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        // part of the FileSystem interface, so we'll need to downcast.
        if (!(fs instanceof DistributedFileSystem))
            continue;
        LOG.trace("Loading disk ids for: " + getFullName() + ". nodes: " + getNumNodes() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        BlockStorageLocation[] storageLocs = null;
        try {
            // Get the BlockStorageLocations for all the blocks
            storageLocs = dfs.getFileBlockStorageLocations(blockLists.locations);
        } catch (IOException e) {
            LOG.error("Couldn't determine block storage locations for filesystem " + fs + ":\n" + e.getMessage());
            continue;
        }
        if (storageLocs == null || storageLocs.length == 0) {
            LOG.warn("Attempted to get block locations for filesystem " + fs + " but the call returned no results");
            continue;
        }
        if (storageLocs.length != blockLists.locations.size()) {
            // Block locations and storage locations didn't match up.
            LOG.error("Number of block storage locations not equal to number of blocks: " + "#storage locations=" + Long.toString(storageLocs.length) + " #blocks=" + Long.toString(blockLists.locations.size()));
            continue;
        }
        long unknownDiskIdCount = 0;
        // THdfsFileBlocks.
        for (int locIdx = 0; locIdx < storageLocs.length; ++locIdx) {
            VolumeId[] volumeIds = storageLocs[locIdx].getVolumeIds();
            THdfsFileBlock block = blockLists.blocks.get(locIdx);
            // Convert opaque VolumeId to 0 based ids.
            // TODO: the diskId should be eventually retrievable from Hdfs when the
            // community agrees this API is useful.
            int[] diskIds = new int[volumeIds.length];
            for (int i = 0; i < volumeIds.length; ++i) {
                diskIds[i] = getDiskId(volumeIds[i]);
                if (diskIds[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIds, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#method_after
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    if (!SUPPORTS_VOLUME_ID)
        return;
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        // part of the FileSystem interface, so we'll need to downcast.
        if (!(fs instanceof DistributedFileSystem))
            continue;
        LOG.trace("Loading disk ids for: " + getFullName() + ". nodes: " + hostIndex_.size() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        BlockStorageLocation[] storageLocs = null;
        try {
            // Get the BlockStorageLocations for all the blocks
            storageLocs = dfs.getFileBlockStorageLocations(blockLists.locations);
        } catch (IOException e) {
            LOG.error("Couldn't determine block storage locations for filesystem " + fs + ":\n" + e.getMessage());
            continue;
        }
        if (storageLocs == null || storageLocs.length == 0) {
            LOG.warn("Attempted to get block locations for filesystem " + fs + " but the call returned no results");
            continue;
        }
        if (storageLocs.length != blockLists.locations.size()) {
            // Block locations and storage locations didn't match up.
            LOG.error("Number of block storage locations not equal to number of blocks: " + "#storage locations=" + Long.toString(storageLocs.length) + " #blocks=" + Long.toString(blockLists.locations.size()));
            continue;
        }
        long unknownDiskIdCount = 0;
        // THdfsFileBlocks.
        for (int locIdx = 0; locIdx < storageLocs.length; ++locIdx) {
            VolumeId[] volumeIds = storageLocs[locIdx].getVolumeIds();
            THdfsFileBlock block = blockLists.blocks.get(locIdx);
            // Convert opaque VolumeId to 0 based ids.
            // TODO: the diskId should be eventually retrievable from Hdfs when the
            // community agrees this API is useful.
            int[] diskIds = new int[volumeIds.length];
            for (int i = 0; i < volumeIds.length; ++i) {
                diskIds[i] = getDiskId(volumeIds[i]);
                if (diskIds[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIds, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#end_block

#method_before
@Override
public TTableDescriptor toThriftDescriptor(Set<Long> referencedPartitions) {
    TTableDescriptor tableDesc = new TTableDescriptor(id_.asInt(), TTableType.HDFS_TABLE, getColumns().size(), numClusteringCols_, getTColumnTypes(), name_, db_.getName());
    tableDesc.setHdfsTable(getTHdfsTable(false, referencedPartitions));
    tableDesc.setColNames(getColumnNames());
    return tableDesc;
}
#method_after
@Override
public TTableDescriptor toThriftDescriptor(Set<Long> referencedPartitions) {
    TTableDescriptor tableDesc = new TTableDescriptor(id_.asInt(), TTableType.HDFS_TABLE, getTColumnDescriptors(), numClusteringCols_, name_, db_.getName());
    tableDesc.setHdfsTable(getTHdfsTable(false, referencedPartitions));
    return tableDesc;
}
#end_block

#method_before
@Test
public void TestStarPathAmbiguity() {
    addTestDb("a");
    addTestTable("create table a.a (a struct<a:struct<a:int>>)");
    // Star path is not ambiguous.
    AnalyzesOk("select a.a.a.a.* from a.a");
    AnalyzesOk("select t.a.a.* from a.a t");
    // Not ambiguous, but illegal.
    AnalysisError("select a.a.a.a.a.* from a.a", "Cannot expand star in 'a.a.a.a.a.*' because path 'a.a.a.a.a' " + "resolved to type 'INT'.");
    AnalysisError("select t.a.a.a.* from a.a t", "Cannot expand star in 't.a.a.a.*' because path 't.a.a.a' " + "resolved to type 'INT'.");
    // Not ambiguous and legal.
    AnalyzesOk("select t.* from a.a t");
    // Star paths are ambiguous.
    AnalysisError("select a.* from a.a", "Star expression is ambiguous: 'a.*'");
    AnalysisError("select a.a.* from a.a", "Star expression is ambiguous: 'a.a.*'");
    AnalysisError("select a.a.a.* from a.a", "Star expression is ambiguous: 'a.a.a.*'");
    // Cannot resolve star paths.
    AnalysisError("select a.a.a.a.a.a.* from a.a", "Could not resolve star expression: 'a.a.a.a.a.a.*'");
    AnalysisError("select t.a.a.a.a.* from a.a t", "Could not resolve star expression: 't.a.a.a.a.*'");
    // Paths resolve to an existing implicit table alias
    // (the unqualified path resolution would be illegal).
    addTestTable("create table a.array_test (a array<int>)");
    addTestTable("create table a.map_test (a map<int, int>)");
    AnalyzesOk("select a.* from a.array_test t, t.a");
    AnalyzesOk("select a.* from a.map_test t, t.a");
}
#method_after
@Test
public void TestStarPathAmbiguity() {
    addTestDb("a");
    addTestTable("create table a.a (a struct<a:struct<a:int>>)");
    // Star path is not ambiguous.
    AnalyzesOk("select a.a.a.a.* from a.a");
    AnalyzesOk("select t.a.a.* from a.a t");
    // Not ambiguous, but illegal.
    AnalysisError("select a.a.a.a.a.* from a.a", "Cannot expand star in 'a.a.a.a.a.*' because path 'a.a.a.a.a' " + "resolved to type 'INT'.");
    AnalysisError("select t.a.a.a.* from a.a t", "Cannot expand star in 't.a.a.a.*' because path 't.a.a.a' " + "resolved to type 'INT'.");
    // Not ambiguous, but expands to an empty select list.
    AnalysisError("select t.* from a.a t", "The star exprs expanded to an empty select list because the referenced " + "tables only have complex-typed columns.");
    // Star paths are ambiguous.
    AnalysisError("select a.* from a.a", "Star expression is ambiguous: 'a.*'");
    AnalysisError("select a.a.* from a.a", "Star expression is ambiguous: 'a.a.*'");
    AnalysisError("select a.a.a.* from a.a", "Star expression is ambiguous: 'a.a.a.*'");
    // Cannot resolve star paths.
    AnalysisError("select a.a.a.a.a.a.* from a.a", "Could not resolve star expression: 'a.a.a.a.a.a.*'");
    AnalysisError("select t.a.a.a.a.* from a.a t", "Could not resolve star expression: 't.a.a.a.a.*'");
    // Paths resolve to an existing implicit table alias
    // (the unqualified path resolution would be illegal).
    addTestTable("create table a.array_test (a array<int>)");
    addTestTable("create table a.map_test (a map<int, int>)");
    AnalyzesOk("select a.* from a.array_test t, t.a");
    AnalyzesOk("select a.* from a.map_test t, t.a");
}
#end_block

#method_before
public static Expr createExpr(FunctionName fnName, FunctionParams params) {
    FunctionCallExpr functionCallExpr = new FunctionCallExpr(fnName, params);
    if (fnName.getFunction().equals("decode") && (fnName.getDb() == null || Catalog.BUILTINS_DB.equals(fnName.getDb()))) {
        // continue to use it by the fully qualified name.
        return new CaseExpr(functionCallExpr);
    }
    return functionCallExpr;
}
#method_after
public static Expr createExpr(FunctionName fnName, FunctionParams params) {
    FunctionCallExpr functionCallExpr = new FunctionCallExpr(fnName, params);
    if (fnName.getFnNamePath().size() == 1 && fnName.getFnNamePath().get(0).equalsIgnoreCase("decode") || fnName.getFnNamePath().size() == 2 && fnName.getFnNamePath().get(0).equalsIgnoreCase(Catalog.BUILTINS_DB) && fnName.getFnNamePath().get(1).equalsIgnoreCase("decode")) {
        return new CaseExpr(functionCallExpr);
    }
    return functionCallExpr;
}
#end_block

#method_before
// Provide better error message for some aggregate builtins. These can be
// a bit more user friendly than a generic function not found.
// TODO: should we bother to do this? We could also improve the general
protected String getFunctionNotFoundError(Type[] argTypes) {
    if (fnName_.isBuiltin_) {
        // Some custom error message for builtins
        if (params_.isStar()) {
            return "'*' can only be used in conjunction with COUNT";
        }
        if (fnName_.getFunction().equalsIgnoreCase("count")) {
            if (!params_.isDistinct() && argTypes.length > 1) {
                return "COUNT must have DISTINCT for multiple arguments: " + toSql();
            }
        }
        if (fnName_.getFunction().equalsIgnoreCase("sum")) {
            return "SUM requires a numeric parameter: " + toSql();
        }
        if (fnName_.getFunction().equalsIgnoreCase("avg")) {
            return "AVG requires a numeric or timestamp parameter: " + toSql();
        }
    }
    String[] argTypesSql = new String[argTypes.length];
    for (int i = 0; i < argTypes.length; ++i) {
        argTypesSql[i] = argTypes[i].toSql();
    }
    return String.format("No matching function with signature: %s(%s).", fnName_, params_.isStar() ? "*" : Joiner.on(", ").join(argTypesSql));
}
#method_after
// Provide better error message for some aggregate builtins. These can be
// a bit more user friendly than a generic function not found.
// TODO: should we bother to do this? We could also improve the general
protected String getFunctionNotFoundError(Type[] argTypes) {
    if (fnName_.isBuiltin()) {
        // Some custom error message for builtins
        if (params_.isStar()) {
            return "'*' can only be used in conjunction with COUNT";
        }
        if (fnName_.getFunction().equalsIgnoreCase("count")) {
            if (!params_.isDistinct() && argTypes.length > 1) {
                return "COUNT must have DISTINCT for multiple arguments: " + toSql();
            }
        }
        if (fnName_.getFunction().equalsIgnoreCase("sum")) {
            return "SUM requires a numeric parameter: " + toSql();
        }
        if (fnName_.getFunction().equalsIgnoreCase("avg")) {
            return "AVG requires a numeric or timestamp parameter: " + toSql();
        }
    }
    String[] argTypesSql = new String[argTypes.length];
    for (int i = 0; i < argTypes.length; ++i) {
        argTypesSql[i] = argTypes[i].toSql();
    }
    return String.format("No matching function with signature: %s(%s).", fnName_, params_.isStar() ? "*" : Joiner.on(", ").join(argTypesSql));
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    fnName_.analyze(analyzer);
    if (isMergeAggFn_) {
        // This is the function call expr after splitting up to a merge aggregation.
        // The function has already been analyzed so just do the minimal sanity
        // check here.
        AggregateFunction aggFn = (AggregateFunction) fn_;
        Preconditions.checkNotNull(aggFn);
        Type intermediateType = aggFn.getIntermediateType();
        if (intermediateType == null)
            intermediateType = type_;
        Preconditions.checkState(!type_.isWildcardDecimal());
        return;
    }
    Type[] argTypes = collectChildReturnTypes();
    // User needs DB access.
    Db db = analyzer.getDb(fnName_.getDb(), Privilege.VIEW_METADATA);
    if (!db.containsFunction(fnName_.getFunction())) {
        throw new AnalysisException(fnName_ + "() unknown");
    }
    if (fnName_.getFunction().equals("count") && params_.isDistinct()) {
        // Treat COUNT(DISTINCT ...) special because of how we do the rewrite.
        // There is no version of COUNT() that takes more than 1 argument but after
        // the rewrite, we only need count(*).
        // TODO: fix how we rewrite count distinct.
        argTypes = new Type[0];
        Function searchDesc = new Function(fnName_, argTypes, Type.INVALID, false);
        fn_ = db.getFunction(searchDesc, Function.CompareMode.IS_SUPERTYPE_OF);
        type_ = fn_.getReturnType();
        // Make sure BE doesn't see any TYPE_NULL exprs
        for (int i = 0; i < children_.size(); ++i) {
            if (getChild(i).getType().isNull()) {
                uncheckedCastChild(ScalarType.BOOLEAN, i);
            }
        }
        return;
    }
    // to timestamp
    if (fnName_.getFunction().equalsIgnoreCase("avg") && children_.size() == 1 && children_.get(0).getType().isStringType()) {
        throw new AnalysisException("AVG requires a numeric or timestamp parameter: " + toSql());
    }
    Function searchDesc = new Function(fnName_, argTypes, Type.INVALID, false);
    fn_ = db.getFunction(searchDesc, Function.CompareMode.IS_SUPERTYPE_OF);
    if (fn_ == null || (!isInternalFnCall_ && !fn_.userVisible())) {
        throw new AnalysisException(getFunctionNotFoundError(argTypes));
    }
    if (isAggregateFunction()) {
        // subexprs must not contain aggregates
        if (TreeNode.contains(children_, Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function must not contain aggregate parameters: " + this.toSql());
        }
        // .. or analytic exprs
        if (Expr.contains(children_, AnalyticExpr.class)) {
            throw new AnalysisException("aggregate function must not contain analytic parameters: " + this.toSql());
        }
        // no other aggregate functions (currently) can accept '*'.
        if (fnName_.getFunction().equalsIgnoreCase("count") && !params_.isStar() && children_.size() == 0) {
            throw new AnalysisException("count() is not allowed.");
        }
        // TODO: the distinct rewrite does not handle this but why?
        if (params_.isDistinct()) {
            if (fnName_.getFunction().equalsIgnoreCase("group_concat")) {
                throw new AnalysisException("GROUP_CONCAT() does not support DISTINCT.");
            }
            if (fn_.getBinaryType() != TFunctionBinaryType.BUILTIN) {
                throw new AnalysisException("User defined aggregates do not support DISTINCT.");
            }
        }
        AggregateFunction aggFn = (AggregateFunction) fn_;
        if (aggFn.ignoresDistinct())
            params_.setIsDistinct(false);
    }
    if (isScalarFunction())
        validateScalarFnParams(params_);
    if (fn_ instanceof AggregateFunction && ((AggregateFunction) fn_).isAnalyticFn() && !((AggregateFunction) fn_).isAggregateFn() && !isAnalyticFnCall_) {
        throw new AnalysisException("Analytic function requires an OVER clause: " + toSql());
    }
    castForFunctionCall(false);
    type_ = fn_.getReturnType();
    if (type_.isDecimal() && type_.isWildcardDecimal()) {
        type_ = resolveDecimalReturnType(analyzer);
    }
    // support for this was not added to the backend in 2.0
    if (type_.isWildcardChar() || type_.isWildcardVarchar()) {
        type_ = ScalarType.STRING;
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    fnName_.analyze(analyzer);
    if (isMergeAggFn_) {
        // This is the function call expr after splitting up to a merge aggregation.
        // The function has already been analyzed so just do the minimal sanity
        // check here.
        AggregateFunction aggFn = (AggregateFunction) fn_;
        Preconditions.checkNotNull(aggFn);
        Type intermediateType = aggFn.getIntermediateType();
        if (intermediateType == null)
            intermediateType = type_;
        Preconditions.checkState(!type_.isWildcardDecimal());
        return;
    }
    Type[] argTypes = collectChildReturnTypes();
    // User needs DB access.
    Db db = analyzer.getDb(fnName_.getDb(), Privilege.VIEW_METADATA, true);
    if (!db.containsFunction(fnName_.getFunction())) {
        throw new AnalysisException(fnName_ + "() unknown");
    }
    if (fnName_.getFunction().equals("count") && params_.isDistinct()) {
        // Treat COUNT(DISTINCT ...) special because of how we do the rewrite.
        // There is no version of COUNT() that takes more than 1 argument but after
        // the rewrite, we only need count(*).
        // TODO: fix how we rewrite count distinct.
        argTypes = new Type[0];
        Function searchDesc = new Function(fnName_, argTypes, Type.INVALID, false);
        fn_ = db.getFunction(searchDesc, Function.CompareMode.IS_SUPERTYPE_OF);
        type_ = fn_.getReturnType();
        // Make sure BE doesn't see any TYPE_NULL exprs
        for (int i = 0; i < children_.size(); ++i) {
            if (getChild(i).getType().isNull()) {
                uncheckedCastChild(ScalarType.BOOLEAN, i);
            }
        }
        return;
    }
    // to timestamp
    if (fnName_.getFunction().equalsIgnoreCase("avg") && children_.size() == 1 && children_.get(0).getType().isStringType()) {
        throw new AnalysisException("AVG requires a numeric or timestamp parameter: " + toSql());
    }
    Function searchDesc = new Function(fnName_, argTypes, Type.INVALID, false);
    fn_ = db.getFunction(searchDesc, Function.CompareMode.IS_SUPERTYPE_OF);
    if (fn_ == null || (!isInternalFnCall_ && !fn_.userVisible())) {
        throw new AnalysisException(getFunctionNotFoundError(argTypes));
    }
    if (isAggregateFunction()) {
        // subexprs must not contain aggregates
        if (TreeNode.contains(children_, Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function must not contain aggregate parameters: " + this.toSql());
        }
        // .. or analytic exprs
        if (Expr.contains(children_, AnalyticExpr.class)) {
            throw new AnalysisException("aggregate function must not contain analytic parameters: " + this.toSql());
        }
        // no other aggregate functions (currently) can accept '*'.
        if (fnName_.getFunction().equalsIgnoreCase("count") && !params_.isStar() && children_.size() == 0) {
            throw new AnalysisException("count() is not allowed.");
        }
        // TODO: the distinct rewrite does not handle this but why?
        if (params_.isDistinct()) {
            if (fnName_.getFunction().equalsIgnoreCase("group_concat")) {
                throw new AnalysisException("GROUP_CONCAT() does not support DISTINCT.");
            }
            if (fn_.getBinaryType() != TFunctionBinaryType.BUILTIN) {
                throw new AnalysisException("User defined aggregates do not support DISTINCT.");
            }
        }
        AggregateFunction aggFn = (AggregateFunction) fn_;
        if (aggFn.ignoresDistinct())
            params_.setIsDistinct(false);
    }
    if (isScalarFunction())
        validateScalarFnParams(params_);
    if (fn_ instanceof AggregateFunction && ((AggregateFunction) fn_).isAnalyticFn() && !((AggregateFunction) fn_).isAggregateFn() && !isAnalyticFnCall_) {
        throw new AnalysisException("Analytic function requires an OVER clause: " + toSql());
    }
    castForFunctionCall(false);
    type_ = fn_.getReturnType();
    if (type_.isDecimal() && type_.isWildcardDecimal()) {
        type_ = resolveDecimalReturnType(analyzer);
    }
    // support for this was not added to the backend in 2.0
    if (type_.isWildcardChar() || type_.isWildcardVarchar()) {
        type_ = ScalarType.STRING;
    }
}
#end_block

#method_before
@Test
public void TestInPredicates() throws AnalysisException {
    AnalyzesOk("select * from functional.alltypes where int_col in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where int_col not in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where " + "string_col in ('a', 'b', 'c', 'd')");
    AnalyzesOk("select * from functional.alltypes where " + "string_col not in ('a', 'b', 'c', 'd')");
    // Test booleans.
    AnalyzesOk("select * from functional.alltypes where " + "true in (bool_col, true and false)");
    AnalyzesOk("select * from functional.alltypes where " + "true not in (bool_col, true and false)");
    // In list requires implicit casts.
    AnalyzesOk("select * from functional.alltypes where " + "double_col in (int_col, bigint_col)");
    // Comparison expr requires implicit cast.
    AnalyzesOk("select * from functional.alltypes where " + "int_col in (double_col, bigint_col)");
    // Test predicates.
    AnalyzesOk("select * from functional.alltypes where " + "!true in (false or true, true and false)");
    // Test NULLs.
    AnalyzesOk("select * from functional.alltypes where " + "NULL in (NULL, NULL)");
    // Incompatible types.
    AnalysisError("select * from functional.alltypes where " + "string_col in (bool_col, double_col)", "Incompatible return types 'STRING' and 'BOOLEAN' " + "of exprs 'string_col' and 'bool_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (int_col, double_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (NULL, int_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select 1 from functional.allcomplextypes where " + "int_array_col in (id, NULL)", "Incompatible return types 'ARRAY<INT>' and 'INT' " + "of exprs 'int_array_col' and 'id'.");
}
#method_after
@Test
public void TestInPredicates() throws AnalysisException {
    AnalyzesOk("select * from functional.alltypes where int_col in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where int_col not in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where " + "string_col in ('a', 'b', 'c', 'd')");
    AnalyzesOk("select * from functional.alltypes where " + "string_col not in ('a', 'b', 'c', 'd')");
    // Test booleans.
    AnalyzesOk("select * from functional.alltypes where " + "true in (bool_col, true and false)");
    AnalyzesOk("select * from functional.alltypes where " + "true not in (bool_col, true and false)");
    // In list requires implicit casts.
    AnalyzesOk("select * from functional.alltypes where " + "double_col in (int_col, bigint_col)");
    // Comparison expr requires implicit cast.
    AnalyzesOk("select * from functional.alltypes where " + "int_col in (double_col, bigint_col)");
    // Test predicates.
    AnalyzesOk("select * from functional.alltypes where " + "!true in (false or true, true and false)");
    // Test NULLs.
    AnalyzesOk("select * from functional.alltypes where " + "NULL in (NULL, NULL)");
    // Test IN in binary predicates
    AnalyzesOk("select bool_col = (int_col in (1,2)), " + "case when tinyint_col in (10, NULL) then tinyint_col else NULL end " + "from functional.alltypestiny where int_col > (bool_col in (false)) " + "and (int_col in (1,2)) = (select min(bool_col) from functional.alltypes) " + "and (int_col in (3,4)) = (tinyint_col in (4,5))");
    // Incompatible types.
    AnalysisError("select * from functional.alltypes where " + "string_col in (bool_col, double_col)", "Incompatible return types 'STRING' and 'BOOLEAN' " + "of exprs 'string_col' and 'bool_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (int_col, double_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (NULL, int_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select 1 from functional.allcomplextypes where " + "int_array_col in (id, NULL)", "Incompatible return types 'ARRAY<INT>' and 'INT' " + "of exprs 'int_array_col' and 'id'.");
}
#end_block

#method_before
@Test
public void TestFunctionCallExpr() throws AnalysisException {
    AnalyzesOk("select pi()");
    // Regression test for fully qualified function name (IMPALA-1951)
    AnalyzesOk("select _impala_builtins.pi()");
    AnalyzesOk("select sin(pi())");
    AnalyzesOk("select sin(cos(pi()))");
    AnalyzesOk("select sin(cos(tan(e())))");
    AnalysisError("select pi(*)", "Cannot pass '*' to scalar function.");
    AnalysisError("select sin(DISTINCT 1)", "Cannot pass 'DISTINCT' to scalar function.");
    AnalysisError("select * from functional.alltypes where pi(*) = 5", "Cannot pass '*' to scalar function.");
    // Call function that only accepts decimal
    AnalyzesOk("select precision(1)");
    AnalyzesOk("select precision(cast('1.1' as decimal))");
    AnalyzesOk("select scale(1.1)");
    AnalysisError("select scale('1.1')", "No matching function with signature: scale(STRING).");
    AnalyzesOk("select round(cast('1.1' as decimal), cast(1 as int))");
    // 1 is a tinyint, so the function is not a perfect match
    AnalyzesOk("select round(cast('1.1' as decimal), 1)");
    // No matching signature for complex type.
    AnalysisError("select lower(int_struct_col) from functional.allcomplextypes", "No matching function with signature: lower(STRUCT<f1:INT,f2:INT>).");
    // Special cases for FROM in function call
    AnalyzesOk("select extract(year from now())");
    AnalysisError("select extract(foo from now())", "Time unit 'foo' in expression 'EXTRACT(foo FROM now())' is invalid. Expected " + "one of YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, MILLISECOND, EPOCH.");
    AnalysisError("select extract(year from 0)", "Expression '0' in 'EXTRACT(year FROM 0)' has a return type of TINYINT but a " + "TIMESTAMP is required.");
    AnalysisError("select functional.extract(year from now())", "Function functional.extract conflicts with the EXTRACT builtin");
    AnalysisError("select date_part(year from now())", "Function DATE_PART does not accept the keyword FROM");
}
#method_after
@Test
public void TestFunctionCallExpr() throws AnalysisException {
    AnalyzesOk("select pi()");
    AnalyzesOk("select _impala_builtins.pi()");
    AnalyzesOk("select _impala_builtins.decode(1, 2, 3)");
    AnalyzesOk("select _impala_builtins.DECODE(1, 2, 3)");
    AnalyzesOk("select sin(pi())");
    AnalyzesOk("select sin(cos(pi()))");
    AnalyzesOk("select sin(cos(tan(e())))");
    AnalysisError("select pi(*)", "Cannot pass '*' to scalar function.");
    AnalysisError("select sin(DISTINCT 1)", "Cannot pass 'DISTINCT' to scalar function.");
    AnalysisError("select * from functional.alltypes where pi(*) = 5", "Cannot pass '*' to scalar function.");
    // Invalid function name.
    AnalysisError("select a.b.sin()", "Invalid function name: 'a.b.sin'. Expected [dbname].funcname");
    // Call function that only accepts decimal
    AnalyzesOk("select precision(1)");
    AnalyzesOk("select precision(cast('1.1' as decimal))");
    AnalyzesOk("select scale(1.1)");
    AnalysisError("select scale('1.1')", "No matching function with signature: scale(STRING).");
    AnalyzesOk("select round(cast('1.1' as decimal), cast(1 as int))");
    // 1 is a tinyint, so the function is not a perfect match
    AnalyzesOk("select round(cast('1.1' as decimal), 1)");
    // No matching signature for complex type.
    AnalysisError("select lower(int_struct_col) from functional.allcomplextypes", "No matching function with signature: lower(STRUCT<f1:INT,f2:INT>).");
    // Special cases for FROM in function call
    AnalyzesOk("select extract(year from now())");
    AnalysisError("select extract(foo from now())", "Time unit 'foo' in expression 'EXTRACT(foo FROM now())' is invalid. Expected " + "one of YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, MILLISECOND, EPOCH.");
    AnalysisError("select extract(year from 0)", "Expression '0' in 'EXTRACT(year FROM 0)' has a return type of TINYINT but a " + "TIMESTAMP is required.");
    AnalysisError("select functional.extract(year from now())", "Function functional.extract conflicts with the EXTRACT builtin");
    AnalysisError("select date_part(year from now())", "Function DATE_PART does not accept the keyword FROM");
}
#end_block

#method_before
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        String jsonLineageGraph = analysisResult.getJsonLineageGraph();
        if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
            result.catalog_op_request.setLineage_graph(jsonLineageGraph);
        }
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int fragmentId = 0; fragmentId < fragments.size(); ++fragmentId) {
        PlanFragment fragment = fragments.get(fragmentId);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, fragmentId);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use VERBOSE by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.VERBOSE;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    String jsonLineageGraph = analysisResult.getJsonLineageGraph();
    if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
        queryExecRequest.setLineage_graph(jsonLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else {
        Preconditions.checkState(analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt());
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    }
    // IMPALA-1702: check that we don't have any duplicate table IDs
    Map<TableId, Table> tableIds = new HashMap<TableId, Table>();
    Collection<TupleDescriptor> tupleDescs = analysisResult.getAnalyzer().getDescTbl().getTupleDescs();
    for (TupleDescriptor desc : tupleDescs) {
        Table table = desc.getTable();
        // Non-materialized table
        if (table == null)
            continue;
        if (tableIds.containsKey(table.getId())) {
            // Duplicate table ID!
            Table otherTable = tableIds.get(table.getId());
            LOG.error("Found duplicate table ID! id=" + table.getId() + "\ntable1=\n" + table.toTCatalogObject() + "\ntable2=\n" + otherTable.toTCatalogObject() + "\nexec_request=\n" + result);
            throw new AnalysisException("Query encountered invalid metadata, likely due to " + "IMPALA-1702. Try rerunning the query.");
        }
        tableIds.put(table.getId(), table);
    }
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#method_after
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        String jsonLineageGraph = analysisResult.getJsonLineageGraph();
        if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
            result.catalog_op_request.setLineage_graph(jsonLineageGraph);
        }
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int fragmentId = 0; fragmentId < fragments.size(); ++fragmentId) {
        PlanFragment fragment = fragments.get(fragmentId);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, fragmentId);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use VERBOSE by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.VERBOSE;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    String jsonLineageGraph = analysisResult.getJsonLineageGraph();
    if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
        queryExecRequest.setLineage_graph(jsonLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else {
        Preconditions.checkState(analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt());
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    }
    validateTableIds(analysisResult.getAnalyzer(), result);
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#end_block

#method_before
public void setPath(List<Integer> path) {
    path_ = path;
}
#method_after
public void setPath(Path path) {
    Preconditions.checkNotNull(path);
    Preconditions.checkNotNull(path.getRootDesc());
    Preconditions.checkState(path.getRootDesc() == parent_);
    path_ = path;
    type_ = path_.destType();
    label_ = Joiner.on(".").join(path.getRawPath());
}
#end_block

#method_before
public Column getColumn() {
    return column_;
}
#method_after
public Column getColumn() {
    if (path_ == null)
        return null;
    return path_.destColumn();
}
#end_block

#method_before
public ColumnStats getStats() {
    if (stats_ == null) {
        if (column_ != null) {
            stats_ = column_.getStats();
        } else {
            stats_ = new ColumnStats(type_);
        }
    }
    return stats_;
}
#method_after
public ColumnStats getStats() {
    if (stats_ == null) {
        Column c = getColumn();
        if (c != null) {
            stats_ = c.getStats();
        } else {
            stats_ = new ColumnStats(type_);
        }
    }
    return stats_;
}
#end_block

#method_before
public TSlotDescriptor toThrift() {
    List<Integer> slotPath = Lists.newArrayList();
    ;
    if (path_ != null) {
        slotPath.addAll(path_);
    } else if (column_ != null) {
        slotPath.add(column_.getPosition());
    }
    TSlotDescriptor result = new TSlotDescriptor(id_.asInt(), parent_.getId().asInt(), type_.toThrift(), slotPath, byteOffset_, nullIndicatorByte_, nullIndicatorBit_, slotIdx_, isMaterialized_);
    return result;
}
#method_after
public TSlotDescriptor toThrift() {
    List<Integer> slotPath = getAbsolutePath();
    TSlotDescriptor result = new TSlotDescriptor(id_.asInt(), parent_.getId().asInt(), type_.toThrift(), slotPath, byteOffset_, nullIndicatorByte_, nullIndicatorBit_, slotIdx_, isMaterialized_);
    return result;
}
#end_block

#method_before
public String debugString() {
    String colStr = (column_ == null ? "null" : column_.getName());
    String typeStr = (type_ == null ? "null" : type_.toString());
    String pathStr = (path_ == null) ? "null" : Joiner.on(".").join(path_);
    return Objects.toStringHelper(this).add("id", id_.asInt()).add("col", colStr).add("path", pathStr).add("type", typeStr).add("materialized", isMaterialized_).add("byteSize", byteSize_).add("byteOffset", byteOffset_).add("nullIndicatorByte", nullIndicatorByte_).add("nullIndicatorBit", nullIndicatorBit_).add("slotIdx", slotIdx_).add("stats", stats_).toString();
}
#method_after
public String debugString() {
    String pathStr = (path_ == null) ? "null" : path_.toString();
    String typeStr = (type_ == null ? "null" : type_.toString());
    return Objects.toStringHelper(this).add("id", id_.asInt()).add("path", pathStr).add("type", typeStr).add("materialized", isMaterialized_).add("byteSize", byteSize_).add("byteOffset", byteOffset_).add("nullIndicatorByte", nullIndicatorByte_).add("nullIndicatorBit", nullIndicatorBit_).add("slotIdx", slotIdx_).add("stats", stats_).toString();
}
#end_block

#method_before
private void testCollectionTableRefs(String collectionTable, String collectionField, boolean testInlineView) {
    TableName tbl = new TableName("functional", "allcomplextypes");
    // Collection table uses unqualified implicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL, allcomplextypes.%s", collectionField, collectionTable), tbl);
    // Collection table uses fully qualified implicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL, functional.allcomplextypes.%s", collectionField, collectionTable), tbl);
    // Collection table uses explicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL a, a.%s", collectionField, collectionTable), tbl);
    if (testInlineView) {
        // Parent table is an inline view.
        TblsAnalyzeOk(String.format("select %s from (select %s from $TBL) a, a.%s", collectionField, collectionTable, collectionTable), tbl);
    }
    // Parent/collection/collection join.
    TblsAnalyzeOk(String.format("select b.%s from $TBL a, a.%s b, a.int_map_col c", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select c.%s from $TBL a, a.int_array_col b, a.%s c", collectionField, collectionTable), tbl);
    // Test join types. Parent/collection joins do not require an ON or USING clause.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin())
            continue;
        TblsAnalyzeOk(String.format("select 1 from $TBL %s allcomplextypes.%s", joinOp, collectionTable), tbl);
        TblsAnalyzeOk(String.format("select 1 from $TBL a %s a.%s", joinOp, collectionTable), tbl);
    }
    // Legal, but not a parent/collection join.
    TblsAnalyzeOk(String.format("select %s from $TBL a, functional.allcomplextypes.%s", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select %s from $TBL.%s, functional.allcomplextypes", collectionField, collectionTable), tbl);
    // Non parent/collection outer or semi  joins require an ON or USING clause.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin() || joinOp.isCrossJoin() || joinOp.isInnerJoin()) {
            continue;
        }
        AnalysisError(String.format("select 1 from functional.allcomplextypes.%s %s functional.allcomplextypes", collectionTable, joinOp), String.format("%s requires an ON or USING clause", joinOp));
    }
    // Duplicate explicit alias.
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s a", collectionField, collectionTable), tbl, "Duplicate table alias: 'a'");
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s b, a.%s b", collectionField, collectionTable, collectionTable), tbl, "Duplicate table alias: 'b'");
    // Duplicate implicit alias.
    String[] childTblPath = collectionTable.split("\\.");
    String childTblAlias = childTblPath[childTblPath.length - 1];
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s, a.%s", collectionField, collectionTable, collectionTable), tbl, String.format("Duplicate table alias: '%s'", childTblAlias));
    TblsAnalysisError(String.format("select 1 from $TBL, allcomplextypes.%s, functional.allcomplextypes.%s", collectionTable, collectionTable), tbl, String.format("Duplicate table alias: '%s'", childTblAlias));
    // Duplicate implicit/explicit alias.
    TblsAnalysisError(String.format("select %s from $TBL, functional.allcomplextypes.%s allcomplextypes", collectionField, collectionTable), tbl, "Duplicate table alias: 'allcomplextypes'");
    // Parent/collection join requires the child to use an alias of the parent.
    AnalysisError(String.format("select %s from allcomplextypes, %s", collectionField, collectionTable), createAnalyzer("functional"), String.format("Could not resolve table reference: '%s'", collectionTable));
    AnalysisError(String.format("select %s from functional.allcomplextypes, %s", collectionField, collectionTable), String.format("Could not resolve table reference: '%s'", collectionTable));
    // Collection table must use explicit alias of the parent.
    AnalysisError(String.format("select item from allcomplextypes a, allcomplextypes.%s", collectionField), createAnalyzer("functional"), String.format("Could not resolve table reference: 'allcomplextypes.%s'", collectionField));
    AnalysisError(String.format("select item from functional.allcomplextypes a, allcomplextypes.%s", collectionField), String.format("Could not resolve table reference: 'allcomplextypes.%s'", collectionField));
    // Ambiguous collection table ref.
    AnalysisError(String.format("select %s from functional.allcomplextypes, " + "functional_parquet.allcomplextypes, allcomplextypes.%s", collectionField, collectionTable), "Unqualified table alias is ambiguous: 'allcomplextypes'");
}
#method_after
private void testCollectionTableRefs(String collectionTable, String collectionField) {
    TableName tbl = new TableName("functional", "allcomplextypes");
    // Collection table uses unqualified implicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL, allcomplextypes.%s", collectionField, collectionTable), tbl);
    // Collection table uses fully qualified implicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL, functional.allcomplextypes.%s", collectionField, collectionTable), tbl);
    // Collection table uses explicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL a, a.%s", collectionField, collectionTable), tbl);
    // Parent/collection/collection join.
    TblsAnalyzeOk(String.format("select b.%s from $TBL a, a.%s b, a.int_map_col c", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select c.%s from $TBL a, a.int_array_col b, a.%s c", collectionField, collectionTable), tbl);
    // Test join types. Parent/collection joins do not require an ON or USING clause.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin())
            continue;
        TblsAnalyzeOk(String.format("select 1 from $TBL %s allcomplextypes.%s", joinOp, collectionTable), tbl);
        TblsAnalyzeOk(String.format("select 1 from $TBL a %s a.%s", joinOp, collectionTable), tbl);
    }
    // Legal, but not a parent/collection join.
    TblsAnalyzeOk(String.format("select %s from $TBL a, functional.allcomplextypes.%s", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select %s from $TBL.%s, functional.allcomplextypes", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select %s from functional.allcomplextypes a, $TBL.%s", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select %s from functional.allcomplextypes.%s, $TBL", collectionField, collectionTable), tbl);
    // Non parent/collection outer or semi  joins require an ON or USING clause.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin() || joinOp.isCrossJoin() || joinOp.isInnerJoin()) {
            continue;
        }
        AnalysisError(String.format("select 1 from functional.allcomplextypes.%s %s functional.allcomplextypes", collectionTable, joinOp), String.format("%s requires an ON or USING clause", joinOp));
    }
    // Duplicate explicit alias.
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s a", collectionField, collectionTable), tbl, "Duplicate table alias: 'a'");
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s b, a.%s b", collectionField, collectionTable, collectionTable), tbl, "Duplicate table alias: 'b'");
    // Duplicate implicit alias.
    String[] childTblPath = collectionTable.split("\\.");
    String childTblAlias = childTblPath[childTblPath.length - 1];
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s, a.%s", collectionField, collectionTable, collectionTable), tbl, String.format("Duplicate table alias: '%s'", childTblAlias));
    TblsAnalysisError(String.format("select 1 from $TBL, allcomplextypes.%s, functional.allcomplextypes.%s", collectionTable, collectionTable), tbl, String.format("Duplicate table alias: '%s'", childTblAlias));
    // Duplicate implicit/explicit alias.
    TblsAnalysisError(String.format("select %s from $TBL, functional.allcomplextypes.%s allcomplextypes", collectionField, collectionTable), tbl, "Duplicate table alias: 'allcomplextypes'");
    // Parent/collection join requires the child to use an alias of the parent.
    AnalysisError(String.format("select %s from allcomplextypes, %s", collectionField, collectionTable), createAnalyzer("functional"), String.format("Could not resolve table reference: '%s'", collectionTable));
    AnalysisError(String.format("select %s from functional.allcomplextypes, %s", collectionField, collectionTable), String.format("Could not resolve table reference: '%s'", collectionTable));
    // Ambiguous collection table ref.
    AnalysisError(String.format("select %s from functional.allcomplextypes, " + "functional_parquet.allcomplextypes, allcomplextypes.%s", collectionField, collectionTable), "Unqualified table alias is ambiguous: 'allcomplextypes'");
}
#end_block

#method_before
private void testAllTableAliases(String[] tables, String[] columns) throws AnalysisException {
    for (String tbl : tables) {
        TableName tblName = new TableName("functional", tbl);
        String uqAlias = tbl.substring(tbl.lastIndexOf(".") + 1);
        String fqAlias = "functional." + tbl;
        // True if 'tbl' refers to a collection, false otherwise. A value of false implies
        // the table must be a base table or view.
        boolean isCollectionTblRef = isCollectionTableRef(tbl);
        for (String col : columns) {
            // Test implicit table aliases with unqualified and fully-qualified table names.
            TblsAnalyzeOk(String.format("select %s from $TBL", col), tblName);
            TblsAnalyzeOk(String.format("select %s.%s from $TBL", uqAlias, col), tblName);
            // Only references to base tables/views have a fully-qualified implicit alias.
            if (!isCollectionTblRef) {
                TblsAnalyzeOk(String.format("select %s.%s from $TBL", fqAlias, col), tblName);
            }
            // Explicit table alias.
            TblsAnalyzeOk(String.format("select %s from $TBL a", col), tblName);
            TblsAnalyzeOk(String.format("select a.%s from $TBL a", col), tblName);
            String errRefStr = "column/field reference";
            if (col.endsWith("*"))
                errRefStr = "star expression";
            // Explicit table alias must be used.
            TblsAnalysisError(String.format("select %s.%s from $TBL a", uqAlias, col, tbl), tblName, String.format("Could not resolve %s: '%s.%s'", errRefStr, uqAlias, col));
            TblsAnalysisError(String.format("select %s.%s from $TBL a", uqAlias, col, tbl), tblName, String.format("Could not resolve %s: '%s.%s'", errRefStr, uqAlias, col));
        }
    }
    // Test that multiple implicit fully-qualified aliases work.
    for (String t1 : tables) {
        for (String t2 : tables) {
            if (t1.equals(t2))
                continue;
            // Collection tables do not have a fully-qualified implicit alias.
            if (isCollectionTableRef(t1) && isCollectionTableRef(t2))
                continue;
            for (String col : columns) {
                AnalyzesOk(String.format("select functional.%s.%s, functional.%s.%s " + "from functional.%s, functional.%s", t1, col, t2, col, t1, t2));
            }
        }
    }
    String col = columns[0];
    for (String tbl : tables) {
        TableName tblName = new TableName("functional", tbl);
        // Make sure a column reference requires an existing table alias.
        TblsAnalysisError("select alltypessmall.int_col from $TBL", tblName, "Could not resolve column/field reference: 'alltypessmall.int_col'");
        // Duplicate explicit alias.
        TblsAnalysisError(String.format("select a.%s from $TBL a, functional.testtbl a", col), tblName, "Duplicate table alias");
        // Duplicate implicit alias.
        TblsAnalysisError(String.format("select %s from $TBL, $TBL", col), tblName, "Duplicate table alias");
        // Duplicate implicit/explicit alias.
        String uqAlias = tbl.substring(tbl.lastIndexOf(".") + 1);
        TblsAnalysisError(String.format("select %s.%s from $TBL, functional.testtbl %s", tbl, col, uqAlias), tblName, "Duplicate table alias");
    }
}
#method_after
private void testAllTableAliases(String[] tables, String[] columns) throws AnalysisException {
    for (String tbl : tables) {
        TableName tblName = new TableName("functional", tbl);
        String uqAlias = tbl.substring(tbl.lastIndexOf(".") + 1);
        String fqAlias = "functional." + tbl;
        // True if 'tbl' refers to a collection, false otherwise. A value of false implies
        // the table must be a base table or view.
        boolean isCollectionTblRef = isCollectionTableRef(tbl);
        for (String col : columns) {
            // Test implicit table aliases with unqualified and fully-qualified table names.
            TblsAnalyzeOk(String.format("select %s from $TBL", col), tblName);
            TblsAnalyzeOk(String.format("select %s.%s from $TBL", uqAlias, col), tblName);
            // Only references to base tables/views have a fully-qualified implicit alias.
            if (!isCollectionTblRef) {
                TblsAnalyzeOk(String.format("select %s.%s from $TBL", fqAlias, col), tblName);
            }
            // Explicit table alias.
            TblsAnalyzeOk(String.format("select %s from $TBL a", col), tblName);
            TblsAnalyzeOk(String.format("select a.%s from $TBL a", col), tblName);
            String errRefStr = "column/field reference";
            if (col.endsWith("*"))
                errRefStr = "star expression";
            // Explicit table alias must be used.
            TblsAnalysisError(String.format("select %s.%s from $TBL a", uqAlias, col, tbl), tblName, String.format("Could not resolve %s: '%s.%s'", errRefStr, uqAlias, col));
            TblsAnalysisError(String.format("select %s.%s from $TBL a", fqAlias, col, tbl), tblName, String.format("Could not resolve %s: '%s.%s'", errRefStr, fqAlias, col));
        }
    }
    // Test that multiple implicit fully-qualified aliases work.
    for (String t1 : tables) {
        for (String t2 : tables) {
            if (t1.equals(t2))
                continue;
            // Collection tables do not have a fully-qualified implicit alias.
            if (isCollectionTableRef(t1) && isCollectionTableRef(t2))
                continue;
            for (String col : columns) {
                AnalyzesOk(String.format("select functional.%s.%s, functional.%s.%s " + "from functional.%s, functional.%s", t1, col, t2, col, t1, t2));
            }
        }
    }
    String col = columns[0];
    for (String tbl : tables) {
        TableName tblName = new TableName("functional", tbl);
        // Make sure a column reference requires an existing table alias.
        TblsAnalysisError("select alltypessmall.int_col from $TBL", tblName, "Could not resolve column/field reference: 'alltypessmall.int_col'");
        // Duplicate explicit alias.
        TblsAnalysisError(String.format("select a.%s from $TBL a, functional.testtbl a", col), tblName, "Duplicate table alias");
        // Duplicate implicit alias.
        TblsAnalysisError(String.format("select %s from $TBL, $TBL", col), tblName, "Duplicate table alias");
        // Duplicate implicit/explicit alias.
        String uqAlias = tbl.substring(tbl.lastIndexOf(".") + 1);
        TblsAnalysisError(String.format("select %s.%s from $TBL, functional.testtbl %s", tbl, col, uqAlias), tblName, "Duplicate table alias");
    }
}
#end_block

#method_before
@Test
public void TestCollectionTableRefs() throws AnalysisException {
    // Test ARRAY type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_array_col", "allcomplextypes_view.int_array_col" }, new String[] { "item", "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_array_col", "allcomplextypes_view.struct_array_col" }, new String[] { "f1", "f2", "*" });
    // Test MAP type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_map_col", "allcomplextypes_view.int_map_col" }, new String[] { "key", "value", "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_map_col", "allcomplextypes_view.struct_map_col" }, new String[] { "key", "f1", "f2", "*" });
    // Test complex table ref path with structs and multiple collections.
    testAllTableAliases(new String[] { "allcomplextypes.complex_nested_struct_col.f2.f12", "allcomplextypes_view.complex_nested_struct_col.f2.f12" }, new String[] { "key", "f21", "*" });
    // Test resolution of collection table refs.
    testCollectionTableRefs("int_array_col", "item", true);
    testCollectionTableRefs("int_map_col", "key", true);
    testCollectionTableRefs("complex_nested_struct_col.f2.f12", "f21", false);
    // Invalid reference to existing tuple descriptor.
    AnalysisError("select 1 from functional.allcomplextypes a, a", "Invalid table reference to existing table alias: 'a'");
    // Invalid reference to non-collection type.
    AnalysisError("select 1 from functional.allcomplextypes.int_struct_col", "Invalid table reference to non-collection type: " + "'functional.allcomplextypes.int_struct_col'\n" + "Path resolved to type: STRUCT<f1:INT,f2:INT>");
    AnalysisError("select 1 from functional.allcomplextypes a, a.int_struct_col", "Invalid table reference to non-collection type: 'a.int_struct_col'\n" + "Path resolved to type: STRUCT<f1:INT,f2:INT>");
    AnalysisError("select 1 from functional.allcomplextypes.int_array_col.item", "Invalid table reference to non-collection type: " + "'functional.allcomplextypes.int_array_col.item'\n" + "Path resolved to type: INT");
    AnalysisError("select 1 from functional.allcomplextypes.int_array_col a, a.item", "Invalid table reference to non-collection type: 'a.item'\n" + "Path resolved to type: INT");
    AnalysisError("select 1 from functional.allcomplextypes.int_map_col.key", "Invalid table reference to non-collection type: " + "'functional.allcomplextypes.int_map_col.key'\n" + "Path resolved to type: STRING");
    AnalysisError("select 1 from functional.allcomplextypes.int_map_col a, a.key", "Invalid table reference to non-collection type: 'a.key'\n" + "Path resolved to type: STRING");
    // Test that parent/collection joins without an ON clause analyze ok.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin())
            continue;
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.int_array_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.struct_array_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.int_map_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.struct_map_col", joinOp));
    }
}
#method_after
@Test
public void TestCollectionTableRefs() throws AnalysisException {
    // Test ARRAY type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_array_col" }, new String[] { Path.ARRAY_POS_FIELD_NAME, Path.ARRAY_ITEM_FIELD_NAME, "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_array_col" }, new String[] { "f1", "f2", "*" });
    // Test MAP type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_map_col" }, new String[] { Path.MAP_KEY_FIELD_NAME, Path.MAP_VALUE_FIELD_NAME, "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_map_col" }, new String[] { Path.MAP_KEY_FIELD_NAME, "f1", "f2", "*" });
    // Test complex table ref path with structs and multiple collections.
    testAllTableAliases(new String[] { "allcomplextypes.complex_nested_struct_col.f2.f12" }, new String[] { Path.MAP_KEY_FIELD_NAME, "f21", "*" });
    // Test resolution of collection table refs.
    testCollectionTableRefs("int_array_col", Path.ARRAY_POS_FIELD_NAME);
    testCollectionTableRefs("int_array_col", Path.ARRAY_ITEM_FIELD_NAME);
    testCollectionTableRefs("int_map_col", Path.MAP_KEY_FIELD_NAME);
    testCollectionTableRefs("complex_nested_struct_col.f2.f12", "f21");
    // Path resolution error is reported before duplicate alias.
    AnalysisError("select 1 from functional.allcomplextypes a, a", "Illegal table reference to non-collection type: 'a'");
    // Invalid reference to non-collection type.
    AnalysisError("select 1 from functional.allcomplextypes.int_struct_col", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_struct_col'\n" + "Path resolved to type: STRUCT<f1:INT,f2:INT>");
    AnalysisError("select 1 from functional.allcomplextypes a, a.int_struct_col", "Illegal table reference to non-collection type: 'a.int_struct_col'\n" + "Path resolved to type: STRUCT<f1:INT,f2:INT>");
    AnalysisError("select 1 from functional.allcomplextypes.int_array_col.item", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_array_col.item'\n" + "Path resolved to type: INT");
    AnalysisError("select 1 from functional.allcomplextypes.int_array_col a, a.pos", "Illegal table reference to non-collection type: 'a.pos'\n" + "Path resolved to type: BIGINT");
    AnalysisError("select 1 from functional.allcomplextypes.int_map_col.key", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_map_col.key'\n" + "Path resolved to type: STRING");
    AnalysisError("select 1 from functional.allcomplextypes.int_map_col a, a.key", "Illegal table reference to non-collection type: 'a.key'\n" + "Path resolved to type: STRING");
    // Test that parent/collection joins without an ON clause analyze ok.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin())
            continue;
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.int_array_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.struct_array_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.int_map_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.struct_map_col", joinOp));
    }
}
#end_block

#method_before
@Test
public void TestCatalogTableRefs() throws AnalysisException {
    String[] tables = new String[] { "alltypes", "alltypes_view" };
    String[] columns = new String[] { "int_col", "*" };
    testAllTableAliases(tables, columns);
    // Unqualified '*' is not ambiguous.
    AnalyzesOk("select * from functional.alltypes " + "cross join functional_parquet.alltypes");
    // Ambiguous unqualified column reference.
    AnalysisError("select int_col from functional.alltypes " + "cross join functional_parquet.alltypes", "Column/field reference is ambiguous: 'int_col'");
    // Ambiguous implicit unqualified table alias.
    AnalysisError("select alltypes.int_col from functional.alltypes " + "cross join functional_parquet.alltypes", "Unqualified table alias is ambiguous: 'alltypes'");
    AnalysisError("select alltypes.* from functional.alltypes " + "cross join functional_parquet.alltypes", "Unqualified table alias is ambiguous: 'alltypes'");
    // Mixing unqualified and fully-qualified table refs without explicit aliases is an
    // error because we'd expect a consistent result if we created a view of this stmt
    // (table names are fully qualified during view creation -> duplicate table alias).
    AnalysisError("select alltypes.smallint_col, functional.alltypes.int_col " + "from alltypes inner join functional.alltypes " + "on (alltypes.id = functional.alltypes.id)", createAnalyzer("functional"), "Duplicate table alias: 'functional.alltypes'");
}
#method_after
@Test
public void TestCatalogTableRefs() throws AnalysisException {
    String[] tables = new String[] { "alltypes", "alltypes_view" };
    String[] columns = new String[] { "int_col", "*" };
    testAllTableAliases(tables, columns);
    // Unqualified '*' is not ambiguous.
    AnalyzesOk("select * from functional.alltypes " + "cross join functional_parquet.alltypes");
    // Ambiguous unqualified column reference.
    AnalysisError("select int_col from functional.alltypes " + "cross join functional_parquet.alltypes", "Column/field reference is ambiguous: 'int_col'");
    // Ambiguous implicit unqualified table alias.
    AnalysisError("select alltypes.int_col from functional.alltypes " + "cross join functional_parquet.alltypes", "Unqualified table alias is ambiguous: 'alltypes'");
    AnalysisError("select alltypes.* from functional.alltypes " + "cross join functional_parquet.alltypes", "Unqualified table alias is ambiguous: 'alltypes'");
    // Mixing unqualified and fully-qualified table refs without explicit aliases is an
    // error because we'd expect a consistent result if we created a view of this stmt
    // (table names are fully qualified during view creation).
    AnalysisError("select alltypes.smallint_col, functional.alltypes.int_col " + "from alltypes inner join functional.alltypes " + "on (alltypes.id = functional.alltypes.id)", createAnalyzer("functional"), "Duplicate table alias: 'functional.alltypes'");
}
#end_block

#method_before
@Test
public void TestStructFields() throws AnalysisException {
    String[] tables = new String[] { "allcomplextypes", "allcomplextypes_view" };
    String[] columns = new String[] { "id", "int_struct_col.f1", "nested_struct_col.f2.f12.f21" };
    testAllTableAliases(tables, columns);
    // Unknown struct fields.
    AnalysisError("select nested_struct_col.badfield from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.badfield'");
    AnalysisError("select nested_struct_col.f2.badfield from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.f2.badfield'");
    AnalysisError("select nested_struct_col.badfield.f2 from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.badfield.f2'");
    // Illegal intermediate reference to collection type.
    AnalysisError("select int_array_col.item from functional.allcomplextypes", "Could not resolve column/field reference: 'int_array_col.item'\n" + "Illegal reference to intermediate collection 'int_array_col' of type " + "'ARRAY<INT>'");
    AnalysisError("select struct_array_col.f1 from functional.allcomplextypes", "Could not resolve column/field reference: 'struct_array_col.f1'\n" + "Illegal reference to intermediate collection 'struct_array_col' of type " + "'ARRAY<STRUCT<f1:BIGINT,f2:STRING>>'");
    AnalysisError("select int_map_col.key from functional.allcomplextypes", "Could not resolve column/field reference: 'int_map_col.key'\n" + "Illegal reference to intermediate collection 'int_map_col' of type " + "'MAP<STRING,INT>'");
    AnalysisError("select struct_map_col.f1 from functional.allcomplextypes", "Could not resolve column/field reference: 'struct_map_col.f1'\n" + "Illegal reference to intermediate collection 'struct_map_col' of type " + "'MAP<STRING,STRUCT<f1:BIGINT,f2:STRING>>'");
    AnalysisError("select complex_nested_struct_col.f2.f11 from functional.allcomplextypes", "Could not resolve column/field reference: " + "'complex_nested_struct_col.f2.f11'\n" + "Illegal reference to intermediate collection 'f2' of type " + "'ARRAY<STRUCT<f11:BIGINT,f12:MAP<STRING,STRUCT<f21:BIGINT>>>>'");
    AnalysisError("select complex_nested_struct_col.f2.f11 from functional.allcomplextypes", "Could not resolve column/field reference: " + "'complex_nested_struct_col.f2.f11'\n" + "Illegal reference to intermediate collection 'f2' of type " + "'ARRAY<STRUCT<f11:BIGINT,f12:MAP<STRING,STRUCT<f21:BIGINT>>>>'");
}
#method_after
@Test
public void TestStructFields() throws AnalysisException {
    String[] tables = new String[] { "allcomplextypes" };
    String[] columns = new String[] { "id", "int_struct_col.f1", "nested_struct_col.f2.f12.f21" };
    testAllTableAliases(tables, columns);
    // Unknown struct fields.
    AnalysisError("select nested_struct_col.badfield from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.badfield'");
    AnalysisError("select nested_struct_col.f2.badfield from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.f2.badfield'");
    AnalysisError("select nested_struct_col.badfield.f2 from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.badfield.f2'");
    // Illegal intermediate reference to collection type.
    AnalysisError("select int_array_col.item from functional.allcomplextypes", "Illegal column/field reference 'int_array_col.item' with intermediate " + "collection 'int_array_col' of type 'ARRAY<INT>'");
    AnalysisError("select struct_array_col.f1 from functional.allcomplextypes", "Illegal column/field reference 'struct_array_col.f1' with intermediate " + "collection 'struct_array_col' of type 'ARRAY<STRUCT<f1:BIGINT,f2:STRING>>'");
    AnalysisError("select int_map_col.key from functional.allcomplextypes", "Illegal column/field reference 'int_map_col.key' with intermediate " + "collection 'int_map_col' of type 'MAP<STRING,INT>'");
    AnalysisError("select struct_map_col.f1 from functional.allcomplextypes", "Illegal column/field reference 'struct_map_col.f1' with intermediate " + "collection 'struct_map_col' of type 'MAP<STRING,STRUCT<f1:BIGINT,f2:STRING>>'");
    AnalysisError("select complex_nested_struct_col.f2.f11 from functional.allcomplextypes", "Illegal column/field reference 'complex_nested_struct_col.f2.f11' with " + "intermediate collection 'f2' of type " + "'ARRAY<STRUCT<f11:BIGINT,f12:MAP<STRING,STRUCT<f21:BIGINT>>>>'");
    AnalysisError("select complex_nested_struct_col.f2.f11 from functional.allcomplextypes", "Illegal column/field reference 'complex_nested_struct_col.f2.f11' with " + "intermediate collection 'f2' of type " + "'ARRAY<STRUCT<f11:BIGINT,f12:MAP<STRING,STRUCT<f21:BIGINT>>>>'");
}
#end_block

#method_before
@Test
public void TestStar() throws AnalysisException {
    AnalyzesOk("select * from functional.AllTypes");
    AnalyzesOk("select functional.alltypes.* from functional.AllTypes");
    // different db
    AnalyzesOk("select functional_seq.alltypes.* from functional_seq.alltypes");
    // two tables w/ identical names from different dbs
    AnalyzesOk("select functional.alltypes.*, functional_seq.alltypes.* " + "from functional.alltypes, functional_seq.alltypes");
    AnalyzesOk("select * from functional.alltypes, functional_seq.alltypes");
    // result includes complex types (star does not auto-expand structs)
    AnalysisError("select * from functional.allcomplextypes", "Expr 'functional.allcomplextypes.int_array_col' in select list of " + "root statement returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
    // '*' without from clause has no meaning.
    AnalysisError("select *", "'*' expression in select list requires FROM clause.");
    AnalysisError("select 1, *, 2+4", "'*' expression in select list requires FROM clause.");
    AnalysisError("select a.*", "Could not resolve star expression: 'a.*'");
    // invalid star expansions
    AnalysisError("select functional.* from functional.alltypes", "Could not resolve star expression: 'functional.*'");
    AnalysisError("select int_col.* from functional.alltypes", "Could not resolve star expression: 'int_col.*'");
    AnalysisError("select complex_struct_col.f2.* from functional.allcomplextypes", "Could not resolve star expression: 'complex_struct_col.f2.*'");
    for (String joinType : new String[] { "left semi join", "left anti join" }) {
        // ignore semi-/anti-joined tables in unqualified '*' expansion
        SelectStmt stmt = (SelectStmt) AnalyzesOk(String.format("select * from functional.alltypes a " + "%s functional.testtbl b on (a.id = b.id)", joinType));
        // expect to have as many result exprs as alltypes has columns
        assertEquals(13, stmt.getResultExprs().size());
        // cannot expand '*" for a semi-/anti-joined table
        AnalysisError(String.format("select a.*, b.* from functional.alltypes a " + "%s functional.alltypes b on (a.id = b.id)", joinType), "'*' expression cannot reference semi-/anti-joined table 'b'");
    }
    for (String joinType : new String[] { "right semi join", "right anti join" }) {
        // ignore semi-/anti-joined tables in unqualified '*' expansion
        SelectStmt stmt = (SelectStmt) AnalyzesOk(String.format("select * from functional.alltypes a " + "%s functional.testtbl b on (a.id = b.id)", joinType));
        // expect to have as many result exprs as testtbl has columns
        assertEquals(3, stmt.getResultExprs().size());
        // cannot expand '*" for a semi-/anti-joined table
        AnalysisError(String.format("select a.*, b.* from functional.alltypes a " + "%s functional.alltypes b on (a.id = b.id)", joinType), "'*' expression cannot reference semi-/anti-joined table 'a'");
    }
}
#method_after
@Test
public void TestStar() throws AnalysisException {
    AnalyzesOk("select * from functional.AllTypes");
    AnalyzesOk("select functional.alltypes.* from functional.AllTypes");
    // different db
    AnalyzesOk("select functional_seq.alltypes.* from functional_seq.alltypes");
    // two tables w/ identical names from different dbs
    AnalyzesOk("select functional.alltypes.*, functional_seq.alltypes.* " + "from functional.alltypes, functional_seq.alltypes");
    AnalyzesOk("select * from functional.alltypes, functional_seq.alltypes");
    // expand '*' on a struct-typed column
    AnalyzesOk("select int_struct_col.* from functional.allcomplextypes");
    AnalyzesOk("select a.int_struct_col.* from functional.allcomplextypes a");
    AnalyzesOk("select allcomplextypes.int_struct_col.* from functional.allcomplextypes");
    AnalyzesOk("select functional.allcomplextypes.int_struct_col.* " + "from functional.allcomplextypes");
    // '*' without from clause has no meaning.
    AnalysisError("select *", "'*' expression in select list requires FROM clause.");
    AnalysisError("select 1, *, 2+4", "'*' expression in select list requires FROM clause.");
    AnalysisError("select a.*", "Could not resolve star expression: 'a.*'");
    // invalid star expansions
    AnalysisError("select functional.* from functional.alltypes", "Could not resolve star expression: 'functional.*'");
    AnalysisError("select int_col.* from functional.alltypes", "Cannot expand star in 'int_col.*' because " + "path 'int_col' resolved to type 'INT'.\n" + "Star expansion is only valid for paths to a struct type.");
    AnalysisError("select complex_struct_col.f2.* from functional.allcomplextypes", "Cannot expand star in 'complex_struct_col.f2.*' because " + "path 'complex_struct_col.f2' resolved to type 'ARRAY<INT>'.\n" + "Star expansion is only valid for paths to a struct type.");
    for (String joinType : new String[] { "left semi join", "left anti join" }) {
        // ignore semi-/anti-joined tables in unqualified '*' expansion
        SelectStmt stmt = (SelectStmt) AnalyzesOk(String.format("select * from functional.alltypes a " + "%s functional.testtbl b on (a.id = b.id)", joinType));
        // expect to have as many result exprs as alltypes has columns
        assertEquals(13, stmt.getResultExprs().size());
        // cannot expand '*" for a semi-/anti-joined table
        AnalysisError(String.format("select a.*, b.* from functional.alltypes a " + "%s functional.alltypes b on (a.id = b.id)", joinType), "Illegal star expression 'b.*' of semi-/anti-joined table 'b'");
    }
    for (String joinType : new String[] { "right semi join", "right anti join" }) {
        // ignore semi-/anti-joined tables in unqualified '*' expansion
        SelectStmt stmt = (SelectStmt) AnalyzesOk(String.format("select * from functional.alltypes a " + "%s functional.testtbl b on (a.id = b.id)", joinType));
        // expect to have as many result exprs as testtbl has columns
        assertEquals(3, stmt.getResultExprs().size());
        // cannot expand '*" for a semi-/anti-joined table
        AnalysisError(String.format("select a.*, b.* from functional.alltypes a " + "%s functional.alltypes b on (a.id = b.id)", joinType), "Illegal star expression 'a.*' of semi-/anti-joined table 'a'");
    }
}
#end_block

#method_before
@Test
public void TestComplexTypesInSelectList() {
    // Legal complex-types result exprs in views.
    AnalyzesOk("with t as (select * from functional.allcomplextypes) " + "select t.id, t.int_struct_col.f1 from t");
    AnalyzesOk("with t as (select * from functional.allcomplextypes.struct_map_col) " + "select t.f1, t.f2 from t");
    AnalyzesOk("select t.id, t.int_struct_col.f1 " + "from (select * from functional.allcomplextypes) t");
    AnalyzesOk("select t2.id, t2.int_struct_col.f1 " + "from (select * from (select * from functional.allcomplextypes) t1) t2");
    AnalyzesOk("select t.f1, t.f2 " + "from (select * from functional.allcomplextypes.struct_array_col) t");
    AnalyzesOk("select id, int_struct_col.f1 from functional.allcomplextypes_view");
    AnalyzesOk("select t.id, t.int_struct_col.f1 " + "from functional.allcomplextypes_view t");
    // Illegal complex-typed result expr in root stmt.
    AnalysisError("select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list of root statement returns " + "a complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
    AnalysisError("select int_array_col from functional.allcomplextypes_view", "Expr 'int_array_col' in select list of root statement returns a " + "complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
    // Legal star expansion adds illegal complex-typed result expr in root stmt.
    AnalysisError("select * from functional.allcomplextypes " + "cross join functional_parquet.alltypes", "Expr 'functional.allcomplextypes.int_array_col' in select list of " + "root statement returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
    AnalysisError("select * from functional.allcomplextypes_view ", "Expr 'functional.allcomplextypes_view.int_array_col' in select list " + "of root statement returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
}
#method_after
@Test
public void TestComplexTypesInSelectList() {
    // Illegal complex-typed expr in select list.
    AnalysisError("select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Illegal complex-typed expr in a union.
    AnalysisError("select int_struct_col from functional.allcomplextypes " + "union all select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Illegal complex-typed expr inside inline view.
    AnalysisError("select 1 from " + "(select int_struct_col from functional.allcomplextypes) v", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Illegal complex-typed expr in an insert.
    AnalysisError("insert into functional.allcomplextypes " + "select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Illegal complex-typed expr in a CTAS.
    AnalysisError("create table new_tbl as " + "select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Legal star expansion adds illegal complex-typed expr.
    AnalysisError("select * from functional.allcomplextypes " + "cross join functional_parquet.alltypes", "Expr 'functional.allcomplextypes.int_array_col' in select list returns a " + "complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("select complex_struct_col.* from functional.allcomplextypes", "Expr 'functional.allcomplextypes.complex_struct_col.f2' in select list " + "returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
}
#end_block

#method_before
@Test
public void TestCorrelatedInlineViews() {
    // Basic inline view with a single correlated table ref.
    AnalyzesOk("select cnt from functional.allcomplextypes t, " + "(select count(*) cnt from t.int_array_col) v");
    AnalyzesOk("select cnt from functional.allcomplextypes t, " + "(select cnt from (select count(*) cnt from t.int_array_col) v1) v2");
    AnalyzesOk("select item from functional.allcomplextypes t, " + "(select item from t.array_map_col.value) v");
    // Multiple nesting levels.
    AnalyzesOk("select item from functional.allcomplextypes t, " + "(select * from (select * from (select item from t.int_array_col) v1) v2) v3");
    // Mixing correlated and uncorrelated inline views in the same select block works.
    AnalyzesOk("select v.cnt from functional.allcomplextypes t1 " + "join (select * from functional.alltypes) t2 on (t1.id = t2.id) " + "cross join (select count(*) cnt from t1.int_map_col) v");
    // Multiple correlated table refs.
    AnalyzesOk("select avg from functional.allcomplextypes t, " + "(select avg(a1.item) avg from t.int_array_col a1, t.int_array_col a2) v");
    AnalyzesOk("select item, key, value from functional.allcomplextypes t, " + "(select * from t.int_array_col, t.int_map_col) v");
    // Correlated inline view inside uncorrelated inline view.
    AnalyzesOk("select cnt from functional.alltypes t1 inner join " + "(select id, cnt from functional.allcomplextypes t2, " + "(select count(1) cnt from t2.int_array_col) v1) v2");
    // Correlated table ref has child ref itself.
    AnalyzesOk("select key, item from functional.allcomplextypes t, " + "(select a1.key, a2.item from t.array_map_col a1, a1.value a2) v");
    AnalyzesOk("select key, av from functional.allcomplextypes t, " + "(select a1.key, av from t.array_map_col a1, " + "(select avg(item) av from a1.value a2) v1) v2");
    AnalyzesOk("select key, av from functional.allcomplextypes t, " + "(select a1.key, a1.value from t.array_map_col a1) v1, " + "(select avg(item) av from v1.value) v2");
    // Multiple correlated table refs with different parents.
    AnalyzesOk("select t1.id, t2.id, cnt, av from functional.allcomplextypes t1 " + "left outer join functional.allcomplextypes t2 on (t1.id = t2.id), " + "(select count(*) cnt from t1.array_map_col) v1, " + "(select avg(item) av from t2.int_array_col) v2");
    // Correlated table refs in a union.
    AnalyzesOk("select item from functional.allcomplextypes t, " + "(select * from t.int_array_col union all select * from t.int_array_col) v");
    AnalyzesOk("select item from functional.allcomplextypes t, " + "(select item from t.int_array_col union distinct " + "select value from t.int_map_col) v");
    // Correlated inline view in WITH-clause.
    AnalyzesOk("with w as (select item from functional.allcomplextypes t, " + "(select item from t.int_array_col) v) " + "select * from w");
    AnalyzesOk("with w as (select key, av from functional.allcomplextypes t, " + "(select a1.key, av from t.array_map_col a1, " + "(select avg(item) av from a1.value a2) v1) v2) " + "select * from w");
    AnalyzesOk("with w as (select key, av from functional.allcomplextypes t, " + "(select a1.key, a1.value from t.array_map_col a1) v1, " + "(select avg(item) av from v1.value) v2) " + "select * from w");
    // Test behavior of aliases in correlated inline views.
    // Inner reference resolves to the base table, not the implicit parent alias.
    AnalyzesOk("select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from functional.allcomplextypes) v");
    AnalyzesOk("select cnt from functional.allcomplextypes, " + "(select count(1) cnt from functional.allcomplextypes) v");
    AnalyzesOk("select cnt from functional.allcomplextypes, " + "(select count(1) cnt from allcomplextypes) v", createAnalyzer("functional"));
    // Illegal correlated reference.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from t) v", "Invalid table reference to existing table alias: 't'");
    AnalysisError("select cnt from functional.allcomplextypes, " + "(select count(1) cnt from allcomplextypes) v", "Invalid table reference to existing table alias: 'allcomplextypes'");
    // Un/correlated refs in a single nested query block.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from functional.alltypes, t.int_array_col) v", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT count(1) cnt FROM functional.alltypes, t.int_array_col");
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from t.int_array_col, functional.alltypes) v", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT count(1) cnt FROM t.int_array_col, functional.alltypes");
    // Un/correlated refs across multiple nested query blocks.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select * from functional.alltypes, " + "(select count(1) cnt from t.int_array_col) v1) v2", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT * FROM functional.alltypes, (SELECT count(1) cnt " + "FROM t.int_array_col) v1");
    // Correlated table ref has correlated inline view as parent.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select value arr from t.array_map_col) v1, " + "(select item from v1.arr, functional.alltypestiny) v2", "Nested query is illegal because it contains a table reference " + "'v1.arr' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypestiny':\n" + "SELECT item FROM v1.arr, functional.alltypestiny");
    // Un/correlated refs in union operands.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select bigint_col from functional.alltypes " + "union select count(1) cnt from t.int_array_col) v1", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT bigint_col FROM functional.alltypes " + "UNION SELECT count(1) cnt FROM t.int_array_col");
    // Un/correlated refs in WITH-clause view.
    AnalysisError("with w as (select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from t.int_array_col, functional.alltypes) v) " + "select * from w", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT count(1) cnt FROM t.int_array_col, functional.alltypes");
}
#method_after
@Test
public void TestCorrelatedInlineViews() {
    // Basic inline view with a single correlated table ref.
    AnalyzesOk("select cnt from functional.allcomplextypes t, " + "(select count(*) cnt from t.int_array_col) v");
    AnalyzesOk("select cnt from functional.allcomplextypes t, " + "(select cnt from (select count(*) cnt from t.int_array_col) v1) v2");
    AnalyzesOk("select item from functional.allcomplextypes t, " + "(select item from t.array_map_col.value) v");
    // Multiple nesting levels.
    AnalyzesOk("select item from functional.allcomplextypes t, " + "(select * from (select * from (select item from t.int_array_col) v1) v2) v3");
    // Mixing correlated and uncorrelated inline views in the same select block works.
    AnalyzesOk("select v.cnt from functional.allcomplextypes t1 " + "join (select * from functional.alltypes) t2 on (t1.id = t2.id) " + "cross join (select count(*) cnt from t1.int_map_col) v");
    // Multiple correlated table refs.
    AnalyzesOk("select avg from functional.allcomplextypes t, " + "(select avg(a1.item) avg from t.int_array_col a1, t.int_array_col a2) v");
    AnalyzesOk("select item, key, value from functional.allcomplextypes t, " + "(select * from t.int_array_col, t.int_map_col) v");
    // Correlated inline view inside uncorrelated inline view.
    AnalyzesOk("select cnt from functional.alltypes t1 inner join " + "(select id, cnt from functional.allcomplextypes t2, " + "(select count(1) cnt from t2.int_array_col) v1) v2");
    // Correlated table ref has child ref itself.
    AnalyzesOk("select key, item from functional.allcomplextypes t, " + "(select a1.key, a2.item from t.array_map_col a1, a1.value a2) v");
    AnalyzesOk("select key, av from functional.allcomplextypes t, " + "(select a1.key, av from t.array_map_col a1, " + "(select avg(item) av from a1.value a2) v1) v2");
    // TOOD: Enable once we support complex-typed exprs in the select list.
    // AnalyzesOk("select key, av from functional.allcomplextypes t, " +
    // "(select a1.key, a1.value from t.array_map_col a1) v1, " +
    // "(select avg(item) av from v1.value) v2");
    // Multiple correlated table refs with different parents.
    AnalyzesOk("select t1.id, t2.id, cnt, av from functional.allcomplextypes t1 " + "left outer join functional.allcomplextypes t2 on (t1.id = t2.id), " + "(select count(*) cnt from t1.array_map_col) v1, " + "(select avg(item) av from t2.int_array_col) v2");
    // Correlated table refs in a union.
    AnalyzesOk("select item from functional.allcomplextypes t, " + "(select * from t.int_array_col union all select * from t.int_array_col) v");
    AnalyzesOk("select item from functional.allcomplextypes t, " + "(select item from t.int_array_col union distinct " + "select value from t.int_map_col) v");
    // Correlated inline view in WITH-clause.
    AnalyzesOk("with w as (select item from functional.allcomplextypes t, " + "(select item from t.int_array_col) v) " + "select * from w");
    AnalyzesOk("with w as (select key, av from functional.allcomplextypes t, " + "(select a1.key, av from t.array_map_col a1, " + "(select avg(item) av from a1.value a2) v1) v2) " + "select * from w");
    // TOOD: Enable once we support complex-typed exprs in the select list.
    // AnalyzesOk("with w as (select key, av from functional.allcomplextypes t, " +
    // "(select a1.key, a1.value from t.array_map_col a1) v1, " +
    // "(select avg(item) av from v1.value) v2) " +
    // "select * from w");
    // Test behavior of aliases in correlated inline views.
    // Inner reference resolves to the base table, not the implicit parent alias.
    AnalyzesOk("select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from functional.allcomplextypes) v");
    AnalyzesOk("select cnt from functional.allcomplextypes, " + "(select count(1) cnt from functional.allcomplextypes) v");
    AnalyzesOk("select cnt from functional.allcomplextypes, " + "(select count(1) cnt from allcomplextypes) v", createAnalyzer("functional"));
    // Illegal correlated reference.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from t) v", "Illegal table reference to non-collection type: 't'");
    AnalysisError("select cnt from functional.allcomplextypes, " + "(select count(1) cnt from allcomplextypes) v", "Illegal table reference to non-collection type: 'allcomplextypes'");
    // Un/correlated refs in a single nested query block.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from functional.alltypes, t.int_array_col) v", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT count(1) cnt FROM functional.alltypes, t.int_array_col");
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from t.int_array_col, functional.alltypes) v", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT count(1) cnt FROM t.int_array_col, functional.alltypes");
    // Un/correlated refs across multiple nested query blocks.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select * from functional.alltypes, " + "(select count(1) cnt from t.int_array_col) v1) v2", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT * FROM functional.alltypes, (SELECT count(1) cnt " + "FROM t.int_array_col) v1");
    // TOOD: Enable once we support complex-typed exprs in the select list.
    // Correlated table ref has correlated inline view as parent.
    // AnalysisError("select cnt from functional.allcomplextypes t, " +
    // "(select value arr from t.array_map_col) v1, " +
    // "(select item from v1.arr, functional.alltypestiny) v2",
    // "Nested query is illegal because it contains a table reference " +
    // "'v1.arr' correlated with an outer block as well as an " +
    // "uncorrelated one 'functional.alltypestiny':\n" +
    // "SELECT item FROM v1.arr, functional.alltypestiny");
    // Un/correlated refs in union operands.
    AnalysisError("select cnt from functional.allcomplextypes t, " + "(select bigint_col from functional.alltypes " + "union select count(1) cnt from t.int_array_col) v1", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT bigint_col FROM functional.alltypes " + "UNION SELECT count(1) cnt FROM t.int_array_col");
    // Un/correlated refs in WITH-clause view.
    AnalysisError("with w as (select cnt from functional.allcomplextypes t, " + "(select count(1) cnt from t.int_array_col, functional.alltypes) v) " + "select * from w", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT count(1) cnt FROM t.int_array_col, functional.alltypes");
}
#end_block

#method_before
@Test
public void TestWithClause() throws AnalysisException {
    // Single view in WITH clause.
    AnalyzesOk("with t as (select int_col x, bigint_col y from functional.alltypes) " + "select x, y from t");
    // Multiple views in WITH clause. Only one view is used.
    AnalyzesOk("with t1 as (select int_col x, bigint_col y from functional.alltypes), " + "t2 as (select 1 x , 10 y), t3 as (values(2 x , 20 y), (3, 30)), " + "t4 as (select 4 x, 40 y union all select 5, 50), " + "t5 as (select * from (values(6 x, 60 y)) as a) " + "select x, y from t3");
    // Multiple views in WITH clause. All views used in a union.
    AnalyzesOk("with t1 as (select int_col x, bigint_col y from functional.alltypes), " + "t2 as (select 1 x , 10 y), t3 as (values(2 x , 20 y), (3, 30)), " + "t4 as (select 4 x, 40 y union all select 5, 50), " + "t5 as (select * from (values(6 x, 60 y)) as a) " + "select * from t1 union all select * from t2 union all select * from t3 " + "union all select * from t4 union all select * from t5");
    // Multiple views in WITH clause. All views used in a join.
    AnalyzesOk("with t1 as (select int_col x, bigint_col y from functional.alltypes), " + "t2 as (select 1 x , 10 y), t3 as (values(2 x , 20 y), (3, 30)), " + "t4 as (select 4 x, 40 y union all select 5, 50), " + "t5 as (select * from (values(6 x, 60 y)) as a) " + "select t1.y, t2.y, t3.y, t4.y, t5.y from t1, t2, t3, t4, t5 " + "where t1.y = t2.y and t2.y = t3.y and t3.y = t4.y and t4.y = t5.y");
    // WITH clause in insert statement.
    AnalyzesOk("with t1 as (select * from functional.alltypestiny)" + "insert into functional.alltypes partition(year, month) select * from t1");
    // WITH clause in insert statement with a select statement that has a WITH
    // clause and an inline view (IMPALA-1100)
    AnalyzesOk("with test_ctas_1 as (select * from functional.alltypestiny) insert " + "into functional.alltypes partition (year, month) with with_1 as " + "(select t1.* from test_ctas_1 as t1 right join (select 1 as int_col " + "from functional.alltypestiny as t1) as t2 ON t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
    // Insert with a select statement containing a WITH clause and an inline
    // view
    AnalyzesOk("insert into functional.alltypes partition (year, month) with " + "with_1 as (select t1.* from functional.alltypes as t1 right " + "join (select * from functional.alltypestiny as t1) t2 on t1.int_col = " + "t2.int_col) select * from with_1 limit 10");
    // WITH-clause views belong to different scopes.
    AnalyzesOk("with t1 as (select id from functional.alltypestiny) " + "insert into functional.alltypes partition(year, month) " + "with t1 as (select * from functional.alltypessmall) select * from t1");
    // WITH-clause view used in inline view.
    AnalyzesOk("with t1 as (select 'a') select * from (select * from t1) as t2");
    AnalyzesOk("with t1 as (select 'a') " + "select * from (select * from (select * from t1) as t2) as t3");
    // WITH-clause inside inline view.
    AnalyzesOk("select * from (with t1 as (values(1 x, 10 y)) select * from t1) as t2");
    // Test case-insensitive matching of WITH-clause views to base table refs.
    AnalyzesOk("with T1 as (select int_col x, bigint_col y from functional.alltypes)," + "t2 as (select 1 x , 10 y), T3 as (values(2 x , 20 y), (3, 30)), " + "t4 as (select 4 x, 40 y union all select 5, 50), " + "T5 as (select * from (values(6 x, 60 y)) as a) " + "select * from t1 union all select * from T2 union all select * from t3 " + "union all select * from T4 union all select * from t5");
    // Multiple WITH clauses. One for the UnionStmt and one for each union operand.
    AnalyzesOk("with t1 as (values('a', 'b')) " + "(with t2 as (values('c', 'd')) select * from t2) union all" + "(with t3 as (values('e', 'f')) select * from t3) order by 1 limit 1");
    // Multiple WITH clauses. One before the insert and one inside the query statement.
    AnalyzesOk("with t1 as (select * from functional.alltypestiny) " + "insert into functional.alltypes partition(year, month) " + "with t2 as (select * from functional.alltypessmall) select * from t1");
    // Table aliases do not conflict because they are in different scopes.
    // Aliases are resolved from inner-most to the outer-most scope.
    AnalyzesOk("with t1 as (select 'a') " + "select t2.* from (with t1 as (select 'b') select * from t1) as t2");
    // Table aliases do not conflict because t1 from the inline view is never used.
    AnalyzesOk("with t1 as (select 1), t2 as (select 2)" + "select * from functional.alltypes as t1");
    AnalyzesOk("with t1 as (select 1), t2 as (select 2) select * from t2 as t1");
    AnalyzesOk("with t1 as (select 1) select * from (select 2) as t1");
    // Fully-qualified table does not conflict with WITH-clause table.
    AnalyzesOk("with alltypes as (select * from functional.alltypes) " + "select * from functional.alltypes union all select * from alltypes");
    // Use a custom analyzer to change the default db to functional.
    // Recursion is prevented because 'alltypes' in t1 refers to the table
    // functional.alltypes, and 'alltypes' in the final query refers to the
    // view 'alltypes'.
    AnalyzesOk("with t1 as (select int_col x, bigint_col y from alltypes), " + "alltypes as (select x a, y b from t1)" + "select a, b from alltypes", createAnalyzer("functional"));
    // Recursion is prevented because of scoping rules. The inner 'complex_view'
    // refers to a view in the catalog.
    AnalyzesOk("with t1 as (select abc x, xyz y from complex_view), " + "complex_view as (select x a, y b from t1)" + "select a, b from complex_view", createAnalyzer("functional"));
    // Nested WITH clauses. Scoping prevents recursion.
    AnalyzesOk("with t1 as (with t1 as (select int_col x, bigint_col y from alltypes) " + "select x, y from t1), " + "alltypes as (select x a, y b from t1) " + "select a, b from alltypes", createAnalyzer("functional"));
    // Nested WITH clause inside a subquery.
    AnalyzesOk("with t1 as " + "(select * from (with t2 as (select * from functional.alltypes) " + "select * from t2) t3) " + "select * from t1");
    // Nested WITH clause inside a union stmt.
    AnalyzesOk("with t1 as " + "(with t2 as (values('a', 'b')) select * from t2 union all select * from t2) " + "select * from t1");
    // Nested WITH clause inside a union stmt's operand.
    AnalyzesOk("with t1 as " + "(select 'x', 'y' union all (with t2 as (values('a', 'b')) select * from t2)) " + "select * from t1");
    // Single WITH clause. Multiple references to same view.
    AnalyzesOk("with t as (select 1 x)" + "select x from t union all select x from t");
    // Multiple references in same select statement require aliases.
    AnalyzesOk("with t as (select 'a' x)" + "select t1.x, t2.x, t.x from t as t1, t as t2, t " + "where t1.x = t2.x and t2.x = t.x");
    // Test column labels in WITH-clause view for non-SlotRef exprs.
    AnalyzesOk("with t as (select int_col + 2, !bool_col from functional.alltypes) " + "select `int_col + 2`, `NOT bool_col` from t");
    // Test analysis of WITH clause after subquery rewrite does not pollute
    // global state (IMPALA-1357).
    AnalyzesOk("select 1 from (with w as (select 1 from functional.alltypes " + "where exists (select 1 from functional.alltypes)) select 1 from w) tt");
    AnalyzesOk("create table test_with as select 1 from (with w as " + "(select 1 from functional.alltypes where exists " + "(select 1 from functional.alltypes)) select 1 from w) tt");
    AnalyzesOk("insert into functional.alltypesnopart (id) select 1 from " + "(with w as (select 1 from functional.alltypes where exists " + "(select 1 from functional.alltypes)) select 1 from w) tt");
    // Conflicting table aliases in WITH clause.
    AnalysisError("with t1 as (select 1), t1 as (select 2) select * from t1", "Duplicate table alias: 't1'");
    // Check that aliases from WITH-clause views conflict with other table aliases.
    AnalysisError("with t1 as (select 1 x), t2 as (select 2 y)" + "select * from functional.alltypes as t1 inner join t1", "Duplicate table alias: 't1'");
    AnalysisError("with t1 as (select 1), t2 as (select 2) " + "select * from t2 as t1 inner join t1", "Duplicate table alias: 't1'");
    AnalysisError("with t1 as (select 1) select * from (select 2) as t1 inner join t1", "Duplicate table alias: 't1'");
    // Multiple references in same select statement require aliases.
    AnalysisError("with t1 as (select 'a' x) select * from t1 inner join t1", "Duplicate table alias: 't1'");
    // If one was given, we must use the explicit alias for column references.
    AnalysisError("with t1 as (select 'a' x) select t1.x from t1 as t2", "Could not resolve column/field reference: 't1.x'");
    // WITH-clause tables cannot be inserted into.
    AnalysisError("with t1 as (select 'a' x) insert into t1 values('b' x)", "Table does not exist: default.t1");
    // The inner alltypes_view gets resolved to the catalog view.
    AnalyzesOk("with alltypes_view as (select int_col x from alltypes_view) " + "select x from alltypes_view", createAnalyzer("functional"));
    // The inner 't' is resolved to a non-existent base table.
    AnalysisError("with t as (select int_col x, bigint_col y from t1) " + "select x, y from t", "Could not resolve table reference: 't1'");
    AnalysisError("with t as (select 1 as x, 2 as y union all select * from t) " + "select x, y from t", "Could not resolve table reference: 't'");
    AnalysisError("with t as (select a.* from (select * from t) as a) " + "select x, y from t", "Could not resolve table reference: 't'");
    // The inner 't1' in a nested WITH clause gets resolved to a non-existent base table.
    AnalysisError("with t1 as (with t2 as (select * from t1) select * from t2) " + "select * from t1 ", "Could not resolve table reference: 't1'");
    AnalysisError("with t1 as " + "(select * from (with t2 as (select * from t1) select * from t2) t3) " + "select * from t1", "Could not resolve table reference: 't1'");
    // The inner 't1' in the gets resolved to a non-existent base table.
    AnalysisError("with t1 as " + "(with t2 as (select * from t1) select * from t2 union all select * from t2)" + "select * from t1", "Could not resolve table reference: 't1'");
    AnalysisError("with t1 as " + "(select 'x', 'y' union all (with t2 as (select * from t1) select * from t2))" + "select * from t1", "Could not resolve table reference: 't1'");
    // The 't2' inside 't1's definition gets resolved to a non-existent base table.
    AnalysisError("with t1 as (select int_col x, bigint_col y from t2), " + "t2 as (select int_col x, bigint_col y from t1) select x, y from t1", "Could not resolve table reference: 't2'");
    // WITH clause with subqueries
    AnalyzesOk("with t as (select * from functional.alltypesagg where id in " + "(select id from functional.alltypes)) select int_col from t");
    AnalyzesOk("with t as (select * from functional.alltypes) select * from " + "functional.alltypesagg a where exists (select id from t where t.id = a.id)");
    AnalyzesOk("with t as (select * from functional.alltypes) select * from " + "functional.alltypesagg where 10 > (select count(*) from t) and " + "100 < (select max(int_col) from t)");
    AnalyzesOk("with t as (select * from functional.alltypes a where exists " + "(select * from functional.alltypesagg t where t.id = 1 and a.id = t.id) " + "and not exists (select * from functional.alltypesagg b where b.id = 1 " + "and b.int_col = a.int_col)) select * from t");
    // Deeply nested WITH clauses (see IMPALA-1106)
    AnalyzesOk("with with_1 as (select 1 as int_col_1), with_2 as " + "(select 1 as int_col_1 from (with with_3 as (select 1 as int_col_1 from " + "with_1) select 1 as int_col_1 from with_3) as t1) select 1 as int_col_1 " + "from with_2");
    AnalyzesOk("with with_1 as (select 1 as int_col_1), with_2 as (select 1 as " + "int_col_1 from (with with_3 as (select 1 as int_col_1 from with_1) " + "select 1 as int_col_1 from with_3) as t1), with_4 as (select 1 as " + "int_col_1 from with_2) select 1 as int_col_1 from with_4");
    AnalyzesOk("with with_1 as (select 1 as int_col_1), with_2 as (with with_3 " + "as (select 1 as int_col_1 from (with with_4 as (select 1 as int_col_1 " + "from with_1) select 1 as int_col_1 from with_4) as t1) select 1 as " + "int_col_1 from with_3) select 1 as int_col_1 from with_2");
    // WITH clasue with a between predicate
    AnalyzesOk("with with_1 as (select int_col from functional.alltypestiny " + "where int_col between 0 and 10) select * from with_1");
    // WITH clause with a between predicate in the select list
    AnalyzesOk("with with_1 as (select int_col between 0 and 10 " + "from functional.alltypestiny) select * from with_1");
    // WITH clause with a between predicate in the select list that
    // uses casting
    AnalyzesOk("with with_1 as (select timestamp_col between " + "cast('2001-01-01' as timestamp) and " + "(cast('2001-01-01' as timestamp) + interval 10 days) " + "from functional.alltypestiny) select * from with_1");
    // WITH clause with a between predicate that uses explicit casting
    AnalyzesOk("with with_1 as (select * from functional.alltypestiny " + "where timestamp_col between cast('2001-01-01' as timestamp) and " + "(cast('2001-01-01' as timestamp) + interval 10 days)) " + "select * from with_1");
}
#method_after
@Test
public void TestWithClause() throws AnalysisException {
    // Single view in WITH clause.
    AnalyzesOk("with t as (select int_col x, bigint_col y from functional.alltypes) " + "select x, y from t");
    // Multiple views in WITH clause. Only one view is used.
    AnalyzesOk("with t1 as (select int_col x, bigint_col y from functional.alltypes), " + "t2 as (select 1 x , 10 y), t3 as (values(2 x , 20 y), (3, 30)), " + "t4 as (select 4 x, 40 y union all select 5, 50), " + "t5 as (select * from (values(6 x, 60 y)) as a) " + "select x, y from t3");
    // Multiple views in WITH clause. All views used in a union.
    AnalyzesOk("with t1 as (select int_col x, bigint_col y from functional.alltypes), " + "t2 as (select 1 x , 10 y), t3 as (values(2 x , 20 y), (3, 30)), " + "t4 as (select 4 x, 40 y union all select 5, 50), " + "t5 as (select * from (values(6 x, 60 y)) as a) " + "select * from t1 union all select * from t2 union all select * from t3 " + "union all select * from t4 union all select * from t5");
    // Multiple views in WITH clause. All views used in a join.
    AnalyzesOk("with t1 as (select int_col x, bigint_col y from functional.alltypes), " + "t2 as (select 1 x , 10 y), t3 as (values(2 x , 20 y), (3, 30)), " + "t4 as (select 4 x, 40 y union all select 5, 50), " + "t5 as (select * from (values(6 x, 60 y)) as a) " + "select t1.y, t2.y, t3.y, t4.y, t5.y from t1, t2, t3, t4, t5 " + "where t1.y = t2.y and t2.y = t3.y and t3.y = t4.y and t4.y = t5.y");
    // WITH clause in insert statement.
    AnalyzesOk("with t1 as (select * from functional.alltypestiny)" + "insert into functional.alltypes partition(year, month) select * from t1");
    // WITH clause in insert statement with a select statement that has a WITH
    // clause and an inline view (IMPALA-1100)
    AnalyzesOk("with test_ctas_1 as (select * from functional.alltypestiny) insert " + "into functional.alltypes partition (year, month) with with_1 as " + "(select t1.* from test_ctas_1 as t1 right join (select 1 as int_col " + "from functional.alltypestiny as t1) as t2 ON t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
    // Insert with a select statement containing a WITH clause and an inline
    // view
    AnalyzesOk("insert into functional.alltypes partition (year, month) with " + "with_1 as (select t1.* from functional.alltypes as t1 right " + "join (select * from functional.alltypestiny as t1) t2 on t1.int_col = " + "t2.int_col) select * from with_1 limit 10");
    // WITH-clause views belong to different scopes.
    AnalyzesOk("with t1 as (select id from functional.alltypestiny) " + "insert into functional.alltypes partition(year, month) " + "with t1 as (select * from functional.alltypessmall) select * from t1");
    // WITH-clause view used in inline view.
    AnalyzesOk("with t1 as (select 'a') select * from (select * from t1) as t2");
    AnalyzesOk("with t1 as (select 'a') " + "select * from (select * from (select * from t1) as t2) as t3");
    // WITH-clause inside inline view.
    AnalyzesOk("select * from (with t1 as (values(1 x, 10 y)) select * from t1) as t2");
    // Test case-insensitive matching of WITH-clause views to base table refs.
    AnalyzesOk("with T1 as (select int_col x, bigint_col y from functional.alltypes)," + "t2 as (select 1 x , 10 y), T3 as (values(2 x , 20 y), (3, 30)), " + "t4 as (select 4 x, 40 y union all select 5, 50), " + "T5 as (select * from (values(6 x, 60 y)) as a) " + "select * from t1 union all select * from T2 union all select * from t3 " + "union all select * from T4 union all select * from t5");
    // Multiple WITH clauses. One for the UnionStmt and one for each union operand.
    AnalyzesOk("with t1 as (values('a', 'b')) " + "(with t2 as (values('c', 'd')) select * from t2) union all" + "(with t3 as (values('e', 'f')) select * from t3) order by 1 limit 1");
    // Multiple WITH clauses. One before the insert and one inside the query statement.
    AnalyzesOk("with t1 as (select * from functional.alltypestiny) " + "insert into functional.alltypes partition(year, month) " + "with t2 as (select * from functional.alltypessmall) select * from t1");
    // Table aliases do not conflict because they are in different scopes.
    // Aliases are resolved from inner-most to the outer-most scope.
    AnalyzesOk("with t1 as (select 'a') " + "select t2.* from (with t1 as (select 'b') select * from t1) as t2");
    // Table aliases do not conflict because t1 from the inline view is never used.
    AnalyzesOk("with t1 as (select 1), t2 as (select 2)" + "select * from functional.alltypes as t1");
    AnalyzesOk("with t1 as (select 1), t2 as (select 2) select * from t2 as t1");
    AnalyzesOk("with t1 as (select 1) select * from (select 2) as t1");
    // Fully-qualified table does not conflict with WITH-clause table.
    AnalyzesOk("with alltypes as (select * from functional.alltypes) " + "select * from functional.alltypes union all select * from alltypes");
    // Use a custom analyzer to change the default db to functional.
    // Recursion is prevented because 'alltypes' in t1 refers to the table
    // functional.alltypes, and 'alltypes' in the final query refers to the
    // view 'alltypes'.
    AnalyzesOk("with t1 as (select int_col x, bigint_col y from alltypes), " + "alltypes as (select x a, y b from t1)" + "select a, b from alltypes", createAnalyzer("functional"));
    // Recursion is prevented because of scoping rules. The inner 'complex_view'
    // refers to a view in the catalog.
    AnalyzesOk("with t1 as (select abc x, xyz y from complex_view), " + "complex_view as (select x a, y b from t1)" + "select a, b from complex_view", createAnalyzer("functional"));
    // Nested WITH clauses. Scoping prevents recursion.
    AnalyzesOk("with t1 as (with t1 as (select int_col x, bigint_col y from alltypes) " + "select x, y from t1), " + "alltypes as (select x a, y b from t1) " + "select a, b from alltypes", createAnalyzer("functional"));
    // Nested WITH clause inside a subquery.
    AnalyzesOk("with t1 as " + "(select * from (with t2 as (select * from functional.alltypes) " + "select * from t2) t3) " + "select * from t1");
    // Nested WITH clause inside a union stmt.
    AnalyzesOk("with t1 as " + "(with t2 as (values('a', 'b')) select * from t2 union all select * from t2) " + "select * from t1");
    // Nested WITH clause inside a union stmt's operand.
    AnalyzesOk("with t1 as " + "(select 'x', 'y' union all (with t2 as (values('a', 'b')) select * from t2)) " + "select * from t1");
    // Single WITH clause. Multiple references to same view.
    AnalyzesOk("with t as (select 1 x)" + "select x from t union all select x from t");
    // Multiple references in same select statement require aliases.
    AnalyzesOk("with t as (select 'a' x)" + "select t1.x, t2.x, t.x from t as t1, t as t2, t " + "where t1.x = t2.x and t2.x = t.x");
    // Test column labels in WITH-clause view for non-SlotRef exprs.
    AnalyzesOk("with t as (select int_col + 2, !bool_col from functional.alltypes) " + "select `int_col + 2`, `NOT bool_col` from t");
    // Test analysis of WITH clause after subquery rewrite does not pollute
    // global state (IMPALA-1357).
    AnalyzesOk("select 1 from (with w as (select 1 from functional.alltypes " + "where exists (select 1 from functional.alltypes)) select 1 from w) tt");
    AnalyzesOk("create table test_with as select 1 from (with w as " + "(select 1 from functional.alltypes where exists " + "(select 1 from functional.alltypes)) select 1 from w) tt");
    AnalyzesOk("insert into functional.alltypesnopart (id) select 1 from " + "(with w as (select 1 from functional.alltypes where exists " + "(select 1 from functional.alltypes)) select 1 from w) tt");
    // Conflicting table aliases in WITH clause.
    AnalysisError("with t1 as (select 1), t1 as (select 2) select * from t1", "Duplicate table alias: 't1'");
    // Check that aliases from WITH-clause views conflict with other table aliases.
    AnalysisError("with t1 as (select 1 x), t2 as (select 2 y)" + "select * from functional.alltypes as t1 inner join t1", "Duplicate table alias: 't1'");
    AnalysisError("with t1 as (select 1), t2 as (select 2) " + "select * from t2 as t1 inner join t1", "Duplicate table alias: 't1'");
    AnalysisError("with t1 as (select 1) select * from (select 2) as t1 inner join t1", "Duplicate table alias: 't1'");
    // Multiple references in same select statement require aliases.
    AnalysisError("with t1 as (select 'a' x) select * from t1 inner join t1", "Duplicate table alias: 't1'");
    // If one was given, we must use the explicit alias for column references.
    AnalysisError("with t1 as (select 'a' x) select t1.x from t1 as t2", "Could not resolve column/field reference: 't1.x'");
    // WITH-clause tables cannot be inserted into.
    AnalysisError("with t1 as (select 'a' x) insert into t1 values('b' x)", "Table does not exist: default.t1");
    // The inner alltypes_view gets resolved to the catalog view.
    AnalyzesOk("with alltypes_view as (select int_col x from alltypes_view) " + "select x from alltypes_view", createAnalyzer("functional"));
    // The inner 't' is resolved to a non-existent base table.
    AnalysisError("with t as (select int_col x, bigint_col y from t1) " + "select x, y from t", "Could not resolve table reference: 't1'");
    AnalysisError("with t as (select 1 as x, 2 as y union all select * from t) " + "select x, y from t", "Could not resolve table reference: 't'");
    AnalysisError("with t as (select a.* from (select * from t) as a) " + "select x, y from t", "Could not resolve table reference: 't'");
    // The inner 't1' in a nested WITH clause gets resolved to a non-existent base table.
    AnalysisError("with t1 as (with t2 as (select * from t1) select * from t2) " + "select * from t1 ", "Could not resolve table reference: 't1'");
    AnalysisError("with t1 as " + "(select * from (with t2 as (select * from t1) select * from t2) t3) " + "select * from t1", "Could not resolve table reference: 't1'");
    // The inner 't1' in the gets resolved to a non-existent base table.
    AnalysisError("with t1 as " + "(with t2 as (select * from t1) select * from t2 union all select * from t2)" + "select * from t1", "Could not resolve table reference: 't1'");
    AnalysisError("with t1 as " + "(select 'x', 'y' union all (with t2 as (select * from t1) select * from t2))" + "select * from t1", "Could not resolve table reference: 't1'");
    // The 't2' inside 't1's definition gets resolved to a non-existent base table.
    AnalysisError("with t1 as (select int_col x, bigint_col y from t2), " + "t2 as (select int_col x, bigint_col y from t1) select x, y from t1", "Could not resolve table reference: 't2'");
    // WITH clause with subqueries
    AnalyzesOk("with t as (select * from functional.alltypesagg where id in " + "(select id from functional.alltypes)) select int_col from t");
    AnalyzesOk("with t as (select * from functional.alltypes) select * from " + "functional.alltypesagg a where exists (select id from t where t.id = a.id)");
    AnalyzesOk("with t as (select * from functional.alltypes) select * from " + "functional.alltypesagg where 10 > (select count(*) from t) and " + "100 < (select max(int_col) from t)");
    AnalyzesOk("with t as (select * from functional.alltypes a where exists " + "(select * from functional.alltypesagg t where t.id = 1 and a.id = t.id) " + "and not exists (select * from functional.alltypesagg b where b.id = 1 " + "and b.int_col = a.int_col)) select * from t");
    // WITH clause with a collection table ref.
    AnalyzesOk("with w as (select t.id, a.item from functional.allcomplextypes t, " + "t.int_array_col a) select * from w");
    // Deeply nested WITH clauses (see IMPALA-1106)
    AnalyzesOk("with with_1 as (select 1 as int_col_1), with_2 as " + "(select 1 as int_col_1 from (with with_3 as (select 1 as int_col_1 from " + "with_1) select 1 as int_col_1 from with_3) as t1) select 1 as int_col_1 " + "from with_2");
    AnalyzesOk("with with_1 as (select 1 as int_col_1), with_2 as (select 1 as " + "int_col_1 from (with with_3 as (select 1 as int_col_1 from with_1) " + "select 1 as int_col_1 from with_3) as t1), with_4 as (select 1 as " + "int_col_1 from with_2) select 1 as int_col_1 from with_4");
    AnalyzesOk("with with_1 as (select 1 as int_col_1), with_2 as (with with_3 " + "as (select 1 as int_col_1 from (with with_4 as (select 1 as int_col_1 " + "from with_1) select 1 as int_col_1 from with_4) as t1) select 1 as " + "int_col_1 from with_3) select 1 as int_col_1 from with_2");
    // WITH clasue with a between predicate
    AnalyzesOk("with with_1 as (select int_col from functional.alltypestiny " + "where int_col between 0 and 10) select * from with_1");
    // WITH clause with a between predicate in the select list
    AnalyzesOk("with with_1 as (select int_col between 0 and 10 " + "from functional.alltypestiny) select * from with_1");
    // WITH clause with a between predicate in the select list that
    // uses casting
    AnalyzesOk("with with_1 as (select timestamp_col between " + "cast('2001-01-01' as timestamp) and " + "(cast('2001-01-01' as timestamp) + interval 10 days) " + "from functional.alltypestiny) select * from with_1");
    // WITH clause with a between predicate that uses explicit casting
    AnalyzesOk("with with_1 as (select * from functional.alltypestiny " + "where timestamp_col between cast('2001-01-01' as timestamp) and " + "(cast('2001-01-01' as timestamp) + interval 10 days)) " + "select * from with_1");
}
#end_block

#method_before
@Test
public void TestClone() {
    testNumberOfMembers(QueryStmt.class, 10);
    testNumberOfMembers(UnionStmt.class, 8);
    testNumberOfMembers(ValuesStmt.class, 0);
    // Also check TableRefs.
    testNumberOfMembers(TableRef.class, 14);
    testNumberOfMembers(BaseTableRef.class, 0);
    testNumberOfMembers(InlineViewRef.class, 8);
}
#method_after
@Test
public void TestClone() {
    testNumberOfMembers(QueryStmt.class, 9);
    testNumberOfMembers(UnionStmt.class, 8);
    testNumberOfMembers(ValuesStmt.class, 0);
    // Also check TableRefs.
    testNumberOfMembers(TableRef.class, 14);
    testNumberOfMembers(BaseTableRef.class, 0);
    testNumberOfMembers(InlineViewRef.class, 8);
}
#end_block

#method_before
private void testChildTableRefs(String childTable, String childColumn, boolean testInlineView) {
    TableName tbl = new TableName("functional", "allcomplextypes");
    // Child table uses unqualified implicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL, allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    // Child table uses fully qualified implicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL, functional.allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    // Child table uses explicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL a, a.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s a, a.%s", childColumn, tbl.toSql(), childTable));
    if (testInlineView) {
        // Parent table is an inline view.
        TblsTestToSql(String.format("select %s from (select %s from $TBL) a, a.%s", childColumn, childTable, childTable), tbl, String.format("SELECT %s FROM (SELECT %s FROM %s) a, a.%s", childColumn, childTable, tbl.toSql(), childTable));
    }
    // Parent/child/child join.
    TblsTestToSql(String.format("select b.%s from $TBL a, a.%s b, a.int_map_col c", childColumn, childTable), tbl, String.format("SELECT b.%s FROM %s a, a.%s b, a.int_map_col c", childColumn, tbl.toSql(), childTable));
    TblsTestToSql(String.format("select c.%s from $TBL a, a.int_array_col b, a.%s c", childColumn, childTable), tbl, String.format("SELECT c.%s FROM %s a, a.int_array_col b, a.%s c", childColumn, tbl.toSql(), childTable));
    // Test join types. Parent/child joins do not require an ON or USING clause.
    for (String joinType : joinTypes_) {
        TblsTestToSql(String.format("select 1 from $TBL %s allcomplextypes.%s", joinType, childTable), tbl, String.format("SELECT 1 FROM %s %s functional.allcomplextypes.%s", tbl.toSql(), joinType, childTable));
        TblsTestToSql(String.format("select 1 from $TBL a %s a.%s", joinType, childTable), tbl, String.format("SELECT 1 FROM %s a %s a.%s", tbl.toSql(), joinType, childTable));
    }
    // Legal, but not a parent/child join.
    TblsTestToSql(String.format("select %s from $TBL a, functional.allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s a, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    TblsTestToSql(String.format("select %s from $TBL.%s, functional.allcomplextypes", childColumn, childTable), tbl, String.format("SELECT %s FROM %s.%s, functional.allcomplextypes", childColumn, tbl.toSql(), childTable));
}
#method_after
private void testChildTableRefs(String childTable, String childColumn) {
    TableName tbl = new TableName("functional", "allcomplextypes");
    // Child table uses unqualified implicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL, allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    // Child table uses fully qualified implicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL, functional.allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    // Child table uses explicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL a, a.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s a, a.%s", childColumn, tbl.toSql(), childTable));
    // Parent/child/child join.
    TblsTestToSql(String.format("select b.%s from $TBL a, a.%s b, a.int_map_col c", childColumn, childTable), tbl, String.format("SELECT b.%s FROM %s a, a.%s b, a.int_map_col c", childColumn, tbl.toSql(), childTable));
    TblsTestToSql(String.format("select c.%s from $TBL a, a.int_array_col b, a.%s c", childColumn, childTable), tbl, String.format("SELECT c.%s FROM %s a, a.int_array_col b, a.%s c", childColumn, tbl.toSql(), childTable));
    // Test join types. Parent/child joins do not require an ON or USING clause.
    for (String joinType : joinTypes_) {
        TblsTestToSql(String.format("select 1 from $TBL %s allcomplextypes.%s", joinType, childTable), tbl, String.format("SELECT 1 FROM %s %s functional.allcomplextypes.%s", tbl.toSql(), joinType, childTable));
        TblsTestToSql(String.format("select 1 from $TBL a %s a.%s", joinType, childTable), tbl, String.format("SELECT 1 FROM %s a %s a.%s", tbl.toSql(), joinType, childTable));
    }
    // Legal, but not a parent/child join.
    TblsTestToSql(String.format("select %s from $TBL a, functional.allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s a, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    TblsTestToSql(String.format("select %s from $TBL.%s, functional.allcomplextypes", childColumn, childTable), tbl, String.format("SELECT %s FROM %s.%s, functional.allcomplextypes", childColumn, tbl.toSql(), childTable));
}
#end_block

#method_before
@Test
public void TestStructFields() throws AnalysisException {
    String[] tables = new String[] { "allcomplextypes", "allcomplextypes_view" };
    String[] columns = new String[] { "id", "int_struct_col.f1", "nested_struct_col.f2.f12.f21" };
    testAllTableAliases(tables, columns);
}
#method_after
@Test
public void TestStructFields() throws AnalysisException {
    String[] tables = new String[] { "allcomplextypes" };
    String[] columns = new String[] { "id", "int_struct_col.f1", "nested_struct_col.f2.f12.f21" };
    testAllTableAliases(tables, columns);
}
#end_block

#method_before
@Test
public void TestCollectionTableRefs() throws AnalysisException {
    // Test ARRAY type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_array_col", "allcomplextypes_view.int_array_col" }, new String[] { "item", "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_array_col", "allcomplextypes_view.struct_array_col" }, new String[] { "f1", "f2", "*" });
    // Test MAP type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_map_col", "allcomplextypes_view.int_map_col" }, new String[] { "key", "value", "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_map_col", "allcomplextypes_view.struct_map_col" }, new String[] { "key", "f1", "f2", "*" });
    // Test complex table ref path with structs and multiple collections.
    testAllTableAliases(new String[] { "allcomplextypes.complex_nested_struct_col.f2.f12", "allcomplextypes_view.complex_nested_struct_col.f2.f12" }, new String[] { "key", "f21", "*" });
    // Test toSql() of child table refs.
    testChildTableRefs("int_array_col", "item", true);
    testChildTableRefs("int_map_col", "key", true);
    testChildTableRefs("complex_nested_struct_col.f2.f12", "f21", false);
}
#method_after
@Test
public void TestCollectionTableRefs() throws AnalysisException {
    // Test ARRAY type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_array_col" }, new String[] { Path.ARRAY_ITEM_FIELD_NAME, "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_array_col" }, new String[] { "f1", "f2", "*" });
    // Test MAP type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_map_col" }, new String[] { Path.MAP_KEY_FIELD_NAME, Path.MAP_VALUE_FIELD_NAME, "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_map_col" }, new String[] { Path.MAP_KEY_FIELD_NAME, "f1", "f2", "*" });
    // Test complex table ref path with structs and multiple collections.
    testAllTableAliases(new String[] { "allcomplextypes.complex_nested_struct_col.f2.f12" }, new String[] { Path.MAP_KEY_FIELD_NAME, "f21", "*" });
    // Test toSql() of child table refs.
    testChildTableRefs("int_array_col", Path.ARRAY_ITEM_FIELD_NAME);
    testChildTableRefs("int_map_col", Path.MAP_KEY_FIELD_NAME);
    testChildTableRefs("complex_nested_struct_col.f2.f12", "f21");
}
#end_block

#method_before
@Test
public void inlineViewTest() {
    // Test joins in an inline view.
    testToSql("select t.* from " + "(select a.* from functional.alltypes a, functional.alltypes b " + "where a.id = b.id) t", "SELECT t.* FROM " + "(SELECT a.* FROM functional.alltypes a, functional.alltypes b " + "WHERE a.id = b.id) t");
    testToSql("select t.* from (select a.* from functional.alltypes a " + "cross join functional.alltypes b) t", "SELECT t.* FROM (SELECT a.* FROM functional.alltypes a " + "CROSS JOIN functional.alltypes b) t");
    runTestTemplate("select t.* from (select a.* from functional.alltypes a %s " + "functional.alltypes b %s) t", "SELECT t.* FROM (SELECT a.* FROM functional.alltypes a %s " + "functional.alltypes b %s) t", nonSemiJoinTypes_, joinConditions_);
    runTestTemplate("select t.* from (select a.* from functional.alltypes a %s " + "functional.alltypes b %s) t", "SELECT t.* FROM (SELECT a.* FROM functional.alltypes a %s " + "functional.alltypes b %s) t", leftSemiJoinTypes_, joinConditions_);
    runTestTemplate("select t.* from (select b.* from functional.alltypes a %s " + "functional.alltypes b %s) t", "SELECT t.* FROM (SELECT b.* FROM functional.alltypes a %s " + "functional.alltypes b %s) t", rightSemiJoinTypes_, joinConditions_);
    // Test undoing expr substitution in select-list exprs and on clause.
    testToSql("select t1.int_col, t2.int_col from " + "(select int_col, rank() over (order by int_col) from functional.alltypes) " + "t1 inner join " + "(select int_col from functional.alltypes) t2 on (t1.int_col = t2.int_col)", "SELECT t1.int_col, t2.int_col FROM " + "(SELECT int_col, rank() OVER (ORDER BY int_col ASC) " + "FROM functional.alltypes) t1 INNER JOIN " + "(SELECT int_col FROM functional.alltypes) t2 ON (t1.int_col = t2.int_col)");
    // Test undoing expr substitution in aggregates and group by and having clause.
    testToSql("select count(t1.string_col), sum(t2.float_col) from " + "(select id, string_col from functional.alltypes) t1 inner join " + "(select id, float_col from functional.alltypes) t2 on (t1.id = t2.id) " + "group by t1.id, t2.id having count(t2.float_col) > 2", "SELECT count(t1.string_col), sum(t2.float_col) FROM " + "(SELECT id, string_col FROM functional.alltypes) t1 INNER JOIN " + "(SELECT id, float_col FROM functional.alltypes) t2 ON (t1.id = t2.id) " + "GROUP BY t1.id, t2.id HAVING count(t2.float_col) > 2");
    // Test undoing expr substitution in order by clause.
    testToSql("select t1.id, t2.id from " + "(select id, string_col from functional.alltypes) t1 inner join " + "(select id, float_col from functional.alltypes) t2 on (t1.id = t2.id) " + "order by t1.id, t2.id nulls first", "SELECT t1.id, t2.id FROM " + "(SELECT id, string_col FROM functional.alltypes) t1 INNER JOIN " + "(SELECT id, float_col FROM functional.alltypes) t2 ON (t1.id = t2.id) " + "ORDER BY t1.id ASC, t2.id ASC NULLS FIRST");
    // Test undoing expr substitution in where-clause conjuncts.
    testToSql("select t1.id, t2.id from " + "(select id, string_col from functional.alltypes) t1, " + "(select id, float_col from functional.alltypes) t2 " + "where t1.id = t2.id and t1.string_col = 'abc' and t2.float_col < 10", "SELECT t1.id, t2.id FROM " + "(SELECT id, string_col FROM functional.alltypes) t1, " + "(SELECT id, float_col FROM functional.alltypes) t2 " + "WHERE t1.id = t2.id AND t1.string_col = 'abc' AND t2.float_col < 10");
    // Test inline views with correlated table refs. Implicit alias only.
    testToSql("select cnt from functional.allcomplextypes t, " + "(select count(*) cnt from t.int_array_col) v", "SELECT cnt FROM functional.allcomplextypes t, " + "(SELECT count(*) cnt FROM t.int_array_col) v");
    // Multiple correlated table refs. Explicit aliases.
    testToSql("select avg from functional.allcomplextypes t, " + "(select avg(a1.item) avg from t.int_array_col a1, t.int_array_col a2) v", "SELECT avg FROM functional.allcomplextypes t, " + "(SELECT avg(a1.item) avg FROM t.int_array_col a1, t.int_array_col a2) v");
    // Correlated table ref has child ref itself. Mix of explicit and implicit aliases.
    testToSql("select key, item from functional.allcomplextypes t, " + "(select a1.key, value.item from t.array_map_col a1, a1.value) v", "SELECT key, item FROM functional.allcomplextypes t, " + "(SELECT a1.key, value.item FROM t.array_map_col a1, a1.value) v");
    // Correlated table refs in a union.
    testToSql("select item from functional.allcomplextypes t, " + "(select * from t.int_array_col union all select * from t.int_array_col) v", "SELECT item FROM functional.allcomplextypes t, " + "(SELECT * FROM t.int_array_col UNION ALL SELECT * FROM t.int_array_col) v");
    // Correlated inline view in WITH-clause.
    testToSql("with w as (select key, av from functional.allcomplextypes t, " + "(select a1.key, a1.value from t.array_map_col a1) v1, " + "(select avg(item) av from v1.value) v2) " + "select * from w", "WITH w AS (SELECT key, av FROM functional.allcomplextypes t, " + "(SELECT a1.key, a1.value FROM t.array_map_col a1) v1, " + "(SELECT avg(item) av FROM v1.value) v2) " + "SELECT * FROM w");
}
#method_after
@Test
public void inlineViewTest() {
    // Test joins in an inline view.
    testToSql("select t.* from " + "(select a.* from functional.alltypes a, functional.alltypes b " + "where a.id = b.id) t", "SELECT t.* FROM " + "(SELECT a.* FROM functional.alltypes a, functional.alltypes b " + "WHERE a.id = b.id) t");
    testToSql("select t.* from (select a.* from functional.alltypes a " + "cross join functional.alltypes b) t", "SELECT t.* FROM (SELECT a.* FROM functional.alltypes a " + "CROSS JOIN functional.alltypes b) t");
    runTestTemplate("select t.* from (select a.* from functional.alltypes a %s " + "functional.alltypes b %s) t", "SELECT t.* FROM (SELECT a.* FROM functional.alltypes a %s " + "functional.alltypes b %s) t", nonSemiJoinTypes_, joinConditions_);
    runTestTemplate("select t.* from (select a.* from functional.alltypes a %s " + "functional.alltypes b %s) t", "SELECT t.* FROM (SELECT a.* FROM functional.alltypes a %s " + "functional.alltypes b %s) t", leftSemiJoinTypes_, joinConditions_);
    runTestTemplate("select t.* from (select b.* from functional.alltypes a %s " + "functional.alltypes b %s) t", "SELECT t.* FROM (SELECT b.* FROM functional.alltypes a %s " + "functional.alltypes b %s) t", rightSemiJoinTypes_, joinConditions_);
    // Test undoing expr substitution in select-list exprs and on clause.
    testToSql("select t1.int_col, t2.int_col from " + "(select int_col, rank() over (order by int_col) from functional.alltypes) " + "t1 inner join " + "(select int_col from functional.alltypes) t2 on (t1.int_col = t2.int_col)", "SELECT t1.int_col, t2.int_col FROM " + "(SELECT int_col, rank() OVER (ORDER BY int_col ASC) " + "FROM functional.alltypes) t1 INNER JOIN " + "(SELECT int_col FROM functional.alltypes) t2 ON (t1.int_col = t2.int_col)");
    // Test undoing expr substitution in aggregates and group by and having clause.
    testToSql("select count(t1.string_col), sum(t2.float_col) from " + "(select id, string_col from functional.alltypes) t1 inner join " + "(select id, float_col from functional.alltypes) t2 on (t1.id = t2.id) " + "group by t1.id, t2.id having count(t2.float_col) > 2", "SELECT count(t1.string_col), sum(t2.float_col) FROM " + "(SELECT id, string_col FROM functional.alltypes) t1 INNER JOIN " + "(SELECT id, float_col FROM functional.alltypes) t2 ON (t1.id = t2.id) " + "GROUP BY t1.id, t2.id HAVING count(t2.float_col) > 2");
    // Test undoing expr substitution in order by clause.
    testToSql("select t1.id, t2.id from " + "(select id, string_col from functional.alltypes) t1 inner join " + "(select id, float_col from functional.alltypes) t2 on (t1.id = t2.id) " + "order by t1.id, t2.id nulls first", "SELECT t1.id, t2.id FROM " + "(SELECT id, string_col FROM functional.alltypes) t1 INNER JOIN " + "(SELECT id, float_col FROM functional.alltypes) t2 ON (t1.id = t2.id) " + "ORDER BY t1.id ASC, t2.id ASC NULLS FIRST");
    // Test undoing expr substitution in where-clause conjuncts.
    testToSql("select t1.id, t2.id from " + "(select id, string_col from functional.alltypes) t1, " + "(select id, float_col from functional.alltypes) t2 " + "where t1.id = t2.id and t1.string_col = 'abc' and t2.float_col < 10", "SELECT t1.id, t2.id FROM " + "(SELECT id, string_col FROM functional.alltypes) t1, " + "(SELECT id, float_col FROM functional.alltypes) t2 " + "WHERE t1.id = t2.id AND t1.string_col = 'abc' AND t2.float_col < 10");
    // Test inline views with correlated table refs. Implicit alias only.
    testToSql("select cnt from functional.allcomplextypes t, " + "(select count(*) cnt from t.int_array_col) v", "SELECT cnt FROM functional.allcomplextypes t, " + "(SELECT count(*) cnt FROM t.int_array_col) v");
    // Multiple correlated table refs. Explicit aliases.
    testToSql("select avg from functional.allcomplextypes t, " + "(select avg(a1.item) avg from t.int_array_col a1, t.int_array_col a2) v", "SELECT avg FROM functional.allcomplextypes t, " + "(SELECT avg(a1.item) avg FROM t.int_array_col a1, t.int_array_col a2) v");
    // Correlated table ref has child ref itself. Mix of explicit and implicit aliases.
    testToSql("select key, item from functional.allcomplextypes t, " + "(select a1.key, value.item from t.array_map_col a1, a1.value) v", "SELECT key, item FROM functional.allcomplextypes t, " + "(SELECT a1.key, value.item FROM t.array_map_col a1, a1.value) v");
    // Correlated table refs in a union.
    testToSql("select item from functional.allcomplextypes t, " + "(select * from t.int_array_col union all select * from t.int_array_col) v", "SELECT item FROM functional.allcomplextypes t, " + "(SELECT * FROM t.int_array_col UNION ALL SELECT * FROM t.int_array_col) v");
    // Correlated inline view in WITH-clause.
    testToSql("with w as (select c from functional.allcomplextypes t, " + "(select count(a1.key) c from t.array_map_col a1) v1) " + "select * from w", "WITH w AS (SELECT c FROM functional.allcomplextypes t, " + "(SELECT count(a1.key) c FROM t.array_map_col a1) v1) " + "SELECT * FROM w");
}
#end_block

#method_before
public Analyzer findAnalyzer(TupleDescriptor tupleDesc) {
    TupleDescriptor foundTupleDesc = aliasMap_.get(tupleDesc.getAlias());
    if (foundTupleDesc != null && foundTupleDesc.getId().equals(tupleDesc.getId())) {
        return this;
    }
    if (hasAncestors())
        return getParentAnalyzer().findAnalyzer(tupleDesc);
    return null;
}
#method_after
public Analyzer findAnalyzer(TupleId tid) {
    if (tableRefMap_.containsKey(tid))
        return this;
    if (hasAncestors())
        return getParentAnalyzer().findAnalyzer(tid);
    return null;
}
#end_block

#method_before
public TupleDescriptor registerTableRef(TableRef ref) throws AnalysisException {
    String uniqueAlias = ref.getUniqueAlias();
    if (aliasMap_.containsKey(uniqueAlias)) {
        throw new AnalysisException("Duplicate table alias: '" + uniqueAlias + "'");
    }
    // If ref has no explicit alias, then the unqualified and the fully-qualified table
    // names are legal implicit aliases. Column references against unqualified implicit
    // aliases can be ambiguous, therefore, we register such ambiguous aliases here.
    String unqualifiedAlias = null;
    String[] aliases = ref.getAliases();
    if (aliases.length > 1) {
        unqualifiedAlias = aliases[1];
        TupleDescriptor tupleDesc = aliasMap_.get(unqualifiedAlias);
        if (tupleDesc != null) {
            if (tupleDesc.hasExplicitAlias()) {
                throw new AnalysisException("Duplicate table alias: '" + unqualifiedAlias + "'");
            } else {
                ambiguousAliases_.add(unqualifiedAlias);
            }
        }
    }
    // Delegate creation of the tuple descriptor to the concrete table ref.
    TupleDescriptor result = ref.createTupleDescriptor(this);
    result.setAliases(aliases, ref.hasExplicitAlias());
    // Register all legal aliases.
    for (String alias : aliases) {
        aliasMap_.put(alias, result);
    }
    tableRefMap_.put(result.getId(), ref);
    // Register non-inline view refs with the list of un/correlated table refs.
    if (!(ref instanceof InlineViewRef)) {
        if (ref.isCorrelated()) {
            globalState_.correlatedTableRefs_.add(ref);
        } else {
            globalState_.uncorrelatedTableRefs_.add(ref);
        }
    }
    return result;
}
#method_after
public TupleDescriptor registerTableRef(TableRef ref) throws AnalysisException {
    String uniqueAlias = ref.getUniqueAlias();
    if (aliasMap_.containsKey(uniqueAlias)) {
        throw new AnalysisException("Duplicate table alias: '" + uniqueAlias + "'");
    }
    // If ref has no explicit alias, then the unqualified and the fully-qualified table
    // names are legal implicit aliases. Column references against unqualified implicit
    // aliases can be ambiguous, therefore, we register such ambiguous aliases here.
    String unqualifiedAlias = null;
    String[] aliases = ref.getAliases();
    if (aliases.length > 1) {
        unqualifiedAlias = aliases[1];
        TupleDescriptor tupleDesc = aliasMap_.get(unqualifiedAlias);
        if (tupleDesc != null) {
            if (tupleDesc.hasExplicitAlias()) {
                throw new AnalysisException("Duplicate table alias: '" + unqualifiedAlias + "'");
            } else {
                ambiguousAliases_.add(unqualifiedAlias);
            }
        }
    }
    // Delegate creation of the tuple descriptor to the concrete table ref.
    TupleDescriptor result = ref.createTupleDescriptor(this);
    result.setAliases(aliases, ref.hasExplicitAlias());
    // Register all legal aliases.
    for (String alias : aliases) {
        aliasMap_.put(alias, result);
    }
    tableRefMap_.put(result.getId(), ref);
    return result;
}
#end_block

#method_before
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    tableRef.analyze(this);
    Path resolvedPath = tableRef.getResolvedPath();
    if (resolvedPath.matchesTable()) {
        Table table = resolvedPath.getTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef);
    } else {
        Type type = resolvedPath.getMatch();
        Preconditions.checkState(type.isCollectionType());
        return new CollectionTableRef(type, tableRef);
    }
}
#method_after
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    tableRef.analyze(this);
    Path resolvedPath = tableRef.getResolvedPath();
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef);
    } else {
        return new CollectionTableRef(tableRef);
    }
}
#end_block

#method_before
public SlotDescriptor registerSlotRef(Path slotPath) throws AnalysisException {
    // SlotRefs are registered against the tuple's explicit or fully-qualified
    // implicit alias.
    TupleDescriptor tupleDesc = slotPath.getTupleDesc();
    String slotLabel = Joiner.on(".").join(slotPath.getMatchedPath());
    String key = tupleDesc.getAlias() + "." + slotLabel;
    SlotDescriptor result = slotRefMap_.get(key);
    if (result != null)
        return result;
    result = addSlotDescriptor(tupleDesc);
    Column col = slotPath.getMatchedColumn();
    if (col != null)
        result.setColumn(col);
    Preconditions.checkNotNull(slotPath.getMatch());
    result.setType(slotPath.getMatch());
    result.setPath(slotPath.getMatchedPositions());
    result.setLabel(slotLabel);
    slotRefMap_.put(key, result);
    return result;
}
#method_after
public SlotDescriptor registerSlotRef(Path slotPath) throws AnalysisException {
    // SlotRefs are registered against the tuple's explicit or fully-qualified
    // implicit alias.
    TupleDescriptor tupleDesc = slotPath.getRootDesc();
    String key = slotPath.toString();
    SlotDescriptor result = slotRefMap_.get(key);
    if (result != null)
        return result;
    result = addSlotDescriptor(tupleDesc);
    result.setPath(slotPath);
    slotRefMap_.put(slotPath.toString(), result);
    return result;
}
#end_block

#method_before
public SlotDescriptor copySlotDescriptor(SlotDescriptor srcSlotDesc, TupleDescriptor tupleDesc) {
    SlotDescriptor result = globalState_.descTbl.addSlotDescriptor(tupleDesc);
    globalState_.blockBySlot.put(result.getId(), this);
    result.setSourceExprs(srcSlotDesc.getSourceExprs());
    result.setLabel(srcSlotDesc.getLabel());
    result.setStats(srcSlotDesc.getStats());
    if (srcSlotDesc.getColumn() != null)
        result.setColumn(srcSlotDesc.getColumn());
    result.setType(srcSlotDesc.getType());
    return result;
}
#method_after
public SlotDescriptor copySlotDescriptor(SlotDescriptor srcSlotDesc, TupleDescriptor tupleDesc) {
    SlotDescriptor result = globalState_.descTbl.addSlotDescriptor(tupleDesc);
    globalState_.blockBySlot.put(result.getId(), this);
    result.setSourceExprs(srcSlotDesc.getSourceExprs());
    result.setLabel(srcSlotDesc.getLabel());
    result.setStats(srcSlotDesc.getStats());
    result.setType(srcSlotDesc.getType());
    result.setItemTupleDesc(srcSlotDesc.getItemTupleDesc());
    return result;
}
#end_block

#method_before
private Table getTable(String dbName, String tableName) throws AnalysisException, TableLoadingException {
    Table table = null;
    try {
        table = getCatalog().getTable(dbName, tableName);
    } catch (DatabaseNotFoundException e) {
        throw new AnalysisException(DB_DOES_NOT_EXIST_ERROR_MSG + dbName);
    } catch (CatalogException e) {
        String errMsg = String.format("Failed to load metadata for table: %s", tableName);
        // We don't want to log all AnalysisExceptions as ERROR, only failures due to
        // TableLoadingExceptions.
        LOG.error(String.format("%s\n%s", errMsg, e.getMessage()));
        if (e instanceof TableLoadingException)
            throw (TableLoadingException) e;
        throw new TableLoadingException(errMsg, e);
    }
    if (table == null) {
        throw new AnalysisException(TBL_DOES_NOT_EXIST_ERROR_MSG + dbName + "." + tableName);
    }
    if (!table.isLoaded()) {
        missingTbls_.add(new TableName(table.getDb().getName(), table.getName()));
        throw new AnalysisException("Table/view is missing metadata: " + table.getFullName());
    }
    return table;
}
#method_after
public Table getTable(String dbName, String tableName) throws AnalysisException, TableLoadingException {
    Table table = null;
    try {
        table = getCatalog().getTable(dbName, tableName);
    } catch (DatabaseNotFoundException e) {
        throw new AnalysisException(DB_DOES_NOT_EXIST_ERROR_MSG + dbName);
    } catch (CatalogException e) {
        String errMsg = String.format("Failed to load metadata for table: %s", tableName);
        // We don't want to log all AnalysisExceptions as ERROR, only failures due to
        // TableLoadingExceptions.
        LOG.error(String.format("%s\n%s", errMsg, e.getMessage()));
        if (e instanceof TableLoadingException)
            throw (TableLoadingException) e;
        throw new TableLoadingException(errMsg, e);
    }
    if (table == null) {
        throw new AnalysisException(TBL_DOES_NOT_EXIST_ERROR_MSG + dbName + "." + tableName);
    }
    if (!table.isLoaded()) {
        missingTbls_.add(new TableName(table.getDb().getName(), table.getName()));
        throw new AnalysisException("Table/view is missing metadata: " + table.getFullName());
    }
    return table;
}
#end_block

#method_before
@Test
public void TestInSubqueries() throws AnalysisException {
    String[] colNames = { "bool_col", "tinyint_col", "smallint_col", "int_col", "bigint_col", "float_col", "double_col", "string_col", "date_string_col", "timestamp_col" };
    String[] joinOperators = { "inner join", "left outer join", "right outer join", "left semi join", "left anti join" };
    // [NOT] IN subquery predicates
    String[] operators = { "in", "not in" };
    for (String op : operators) {
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select id from functional.alltypestiny)", op));
        // Using column and table aliases similar to the ones produced by the
        // column/table alias generators during a rewrite.
        AnalyzesOk(String.format("select id `$c$1` from functional.alltypestiny `$a$1` " + "where id %s (select id from functional.alltypessmall)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "t.id %s (select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select t.id, max(t.int_col) from " + "functional.alltypes t where t.int_col %s (select int_col from " + "functional.alltypesagg) group by t.id having count(*) < 10", op));
        AnalyzesOk(String.format("select t.bigint_col, t.string_col from " + "functional.alltypes t where t.id %s (select id from " + "functional.alltypesagg where int_col < 10) order by bigint_col", op));
        AnalyzesOk(String.format("select * from functional.alltypes a where a.id %s " + "(select id from functional.alltypes b where a.id = b.id)", op));
        // Complex expressions
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id + int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "t.int_col + 1 %s (select int_col - 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "abs(t.double_col) %s (select int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select NULL from functional.alltypes t where " + "cast(t.double_col as int) %s (select int_col from " + "functional.alltypestiny)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes where id %s " + "(select 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select 1 + 1 from functional.alltypestiny group by int_col)", op));
        AnalyzesOk(String.format("select max(id) from functional.alltypes where id %s " + "(select max(id) from functional.alltypesagg a where a.int_col < 10) " + "and bool_col = false", op));
        // Subquery returns multiple columns
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select id, int_col from functional.alltypessmall)", op), "Subquery must return a single column: (SELECT id, int_col " + "FROM functional.alltypessmall)");
        // Subquery returns an incompatible column type
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select timestamp_col from functional.alltypessmall)", op), "Incompatible return types 'INT' and 'TIMESTAMP' of exprs 'id' and " + "'timestamp_col'.");
        // Different column types in the subquery predicate
        for (String col : colNames) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.%s %s " + "(select a.%s from functional.alltypestiny a)", col, op, col));
        }
        // Decimal in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.double_col %s (select d3 from functional.decimal_tbl a)", op));
        // Varchar in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.string_col %s (select cast(a.string_col as varchar(1)) from " + "functional.alltypestiny a)", op));
        // Subqueries with multiple predicates in the WHERE clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col > 10 and " + "a.tinyint_col < 5)", op));
        // Subqueries with a GROUP BY clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.double_col < 10.1 " + "group by a.id)", op));
        // Subqueries with GROUP BY and HAVING clauses
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.bool_col = true and " + "int_col < 10 group by id having count(*) < 10)", op));
        // Subqueries with a LIMIT clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where id < 100 limit 10)", op));
        // Subqueries with multiple tables in the FROM clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, functional.alltypessmall s " + "where a.int_col = s.int_col and s.bigint_col < 100 and a.tinyint_col < 10)", op));
        // Different join operators between the tables in subquery's FROM clause
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a %s functional.alltypessmall " + "s on a.int_col = s.int_col where a.bool_col = false)", op, joinOp));
        }
        // Correlated predicates in the subquery's ON clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.bigint_col = a.bigint_col and " + "s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on a.bool_col = s.bool_col and t.int_col = 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on ifnull(s.int_col, s.int_col + 20) = " + "t.int_col + t.bigint_col)", op));
        // Subqueries with inline views
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, " + "(select * from functional.alltypessmall) s where s.int_col = a.int_col " + "and s.bool_col = false)", op));
        // Subqueries with inline views that contain subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select id from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select g.* from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a where " + "a.bigint_col = 100)", op));
        // Multiple tables in the FROM clause of the outer query block
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t %s " + "functional.alltypessmall s on t.int_col = s.int_col where " + "t.tinyint_col %s (select tinyint_col from functional.alltypesagg) " + "and t.bool_col = false and t.bigint_col = 10", joinOp, op));
        }
        // Subqueries in WITH clause
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a where " + "id %s (select id from functional.alltypestiny)) select * from t where " + "t.bool_col = false and t.int_col = 10", op));
        // Subqueries in WITH and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny s)) select * from t " + "where t.int_col in (select int_col from functional.alltypessmall) and " + "t.bool_col = false", op));
        // Subqueries in WITH, FROM and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny)) select t.* from t, " + "(select * from functional.alltypesagg g where g.id in " + "(select id from functional.alltypes)) s where s.string_col = t.string_col " + "and t.int_col in (select int_col from functional.alltypessmall) and " + "s.bool_col = false", op));
        // Correlated subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col) " + "and t.bool_col = false", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col + 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + a.int_col  = " + "a.bigint_col and a.bool_col = true)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col = false and " + "a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col)", op));
        // Multiple nesting levels (uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg where int_col %s " + "(select int_col from functional.alltypestiny) and bool_col = false) " + "and bigint_col < 1000", op, op));
        // Multiple nesting levels (correlated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where a.int_col = t.int_col " + "and a.tinyint_col %s (select tinyint_col from functional.alltypestiny s " + "where s.bigint_col = a.bigint_col))", op, op));
        // Multiple nesting levels (correlated and uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col " + "and a.int_col %s (select int_col from functional.alltypestiny s))", op, op));
        // NOT ([NOT] IN predicate)
        AnalyzesOk(String.format("select * from functional.alltypes t where not (id %s " + "(select id from functional.alltypesagg))", op));
        // Different cmp operators in the correlation predicate
        for (String cmpOp : cmpOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t " + "where t.id %s (select a.id from functional.alltypesagg a where " + "t.int_col %s a.int_col)", op, cmpOp));
        }
        // Uncorrelated IN subquery with analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col %s (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", op));
    }
    // Constant on the left hand side
    AnalyzesOk("select * from functional.alltypes a where 1 in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)");
    AnalysisError("select * from functional.alltypes a where 1 not in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)", "Unsupported NOT IN predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypesagg s WHERE s.int_col = a.int_col)");
    // IN subquery that is equivalent to an uncorrelated EXISTS subquery
    AnalysisError("select * from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg)", "Unsupported " + "predicate with subquery: 1 IN (SELECT int_col FROM functional.alltypesagg)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        AnalysisError(String.format("select 1 from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg g where g.id %s t.id)", cmpOp), String.format("Unsupported predicate with subquery: 1 " + "IN (SELECT int_col FROM functional.alltypesagg g WHERE g.id %s t.id)", cmpOp));
    }
    // NOT IN subquery with a correlated predicate that can't be used in an equi
    // join
    AnalysisError("select 1 from functional.alltypes t where 1 not in " + "(select id from functional.alltypestiny g where g.id < t.id)", "Unsupported predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypestiny g WHERE g.id < t.id)");
    // Statement with a GROUP BY and a correlated IN subquery that has
    // correlated predicate that cannot be transformed into an equi-join.
    AnalysisError("select id, count(*) from functional.alltypes t " + "where 1 IN (select id from functional.alltypesagg g where t.int_col < " + "g.int_col) group by id", "Unsupported predicate with subquery: 1 IN " + "(SELECT id FROM functional.alltypesagg g WHERE t.int_col < g.int_col)");
    // Reference a non-existing table in the subquery
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s left outer join p on " + "(s.int_col = p.int_col))", "Could not resolve table reference: 'p'");
    // Reference a non-existing column from a table in the outer scope
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s where s.int_col = t.bad_col)", "Could not resolve column/field reference: 't.bad_col'");
    // Referencing the same table in the inner and the outer query block
    // No explicit alias
    AnalyzesOk("select id from functional.alltypestiny where int_col in " + "(select int_col from functional.alltypestiny)");
    // Different alias between inner and outer block referencing the same table
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny p)");
    // Alias only in the outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny)");
    // Same alias in both inner and outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny t)");
    // Binary predicate with non-comparable operands
    AnalysisError("select * from functional.alltypes t where " + "(id in (select id from functional.alltypestiny)) = 'string_val'", "operands of type BOOLEAN and STRING are not comparable: " + "(id IN (SELECT id FROM functional.alltypestiny)) = 'string_val'");
    // OR with subquery predicates
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select id from functional.alltypesagg) or t.bool_col = false", "Subqueries in OR predicates are not supported: t.id IN " + "(SELECT id FROM functional.alltypesagg) OR t.bool_col = FALSE");
    AnalysisError("select * from functional.alltypes t where not (t.id in " + "(select id from functional.alltypesagg) and t.int_col = 10)", "Subqueries in OR predicates are not supported: t.id NOT IN " + "(SELECT id FROM functional.alltypesagg) OR t.int_col != 10");
    AnalysisError("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg g where g.bool_col = false) " + "or t.bool_col = true", "Subqueries in OR predicates are not " + "supported: EXISTS (SELECT * FROM functional.alltypesagg g WHERE " + "g.bool_col = FALSE) OR t.bool_col = TRUE");
    AnalysisError("select * from functional.alltypes t where t.id = " + "(select min(id) from functional.alltypesagg g) or t.id = 10", "Subqueries in OR predicates are not supported: t.id = " + "(SELECT min(id) FROM functional.alltypesagg g) OR t.id = 10");
    // Correlated subquery with OR predicate
    AnalysisError("select * from functional.alltypes t where id in " + "(select id from functional.alltypesagg a where " + "a.int_col = t.int_col or a.bool_col = false)", "Disjunctions " + "with correlated predicates are not supported: a.int_col = " + "t.int_col OR a.bool_col = FALSE");
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny) and (bool_col = false or " + "int_col = 10)");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select max(a.id) from functional.alltypesagg a where " + "t.int_col = a.int_col)", "Unsupported correlated subquery with grouping " + "and/or aggregation: SELECT max(a.id) FROM functional.alltypesagg a " + "WHERE t.int_col = a.int_col");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select a.id from functional.alltypesagg a where " + "t.int_col = a.int_col group by a.id)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT a.id FROM " + "functional.alltypesagg a WHERE t.int_col = a.int_col GROUP BY a.id");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select distinct a.id from functional.alltypesagg a where " + "a.bigint_col = t.bigint_col)", "Unsupported correlated subquery with " + "grouping and/or aggregation: SELECT DISTINCT a.id FROM " + "functional.alltypesagg a WHERE a.bigint_col = t.bigint_col");
    // NOT compound predicates with OR
    AnalyzesOk("select * from functional.alltypes t where not (" + "id in (select id from functional.alltypesagg) or int_col < 10)");
    AnalyzesOk("select * from functional.alltypes t where not (" + "t.id < 10 or not (t.int_col in (select int_col from " + "functional.alltypesagg) and t.bool_col = false))");
    // Multiple subquery predicates
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny where int_col = 10) and int_col in " + "(select int_col from functional.alltypessmall where bigint_col = 1000) and " + "string_col not in (select string_col from functional.alltypesagg where " + "tinyint_col > 10) and bool_col = false");
    // Correlated subquery with a LIMIT clause
    AnalysisError("select * from functional.alltypes t where id in " + "(select s.id from functional.alltypesagg s where s.int_col = t.int_col " + "limit 1)", "Unsupported correlated subquery with a LIMIT clause: " + "SELECT s.id FROM functional.alltypesagg s WHERE s.int_col = t.int_col " + "LIMIT 1");
    // Correlated IN with an analytic function
    AnalysisError("select id, int_col, bool_col from functional.alltypestiny t1 " + "where int_col in (select min(bigint_col) over (partition by bool_col) " + "from functional.alltypessmall t2 where t1.id < t2.id)", "Unsupported " + "correlated subquery with grouping and/or aggregation: SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE t1.id < t2.id");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "int_col in (select 1 as int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "int_col not in (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#method_after
@Test
public void TestInSubqueries() throws AnalysisException {
    String[] colNames = { "bool_col", "tinyint_col", "smallint_col", "int_col", "bigint_col", "float_col", "double_col", "string_col", "date_string_col", "timestamp_col" };
    String[] joinOperators = { "inner join", "left outer join", "right outer join", "left semi join", "left anti join" };
    // [NOT] IN subquery predicates
    String[] operators = { "in", "not in" };
    for (String op : operators) {
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select id from functional.alltypestiny)", op));
        // Using column and table aliases similar to the ones produced by the
        // column/table alias generators during a rewrite.
        AnalyzesOk(String.format("select id `$c$1` from functional.alltypestiny `$a$1` " + "where id %s (select id from functional.alltypessmall)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "t.id %s (select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select t.id, max(t.int_col) from " + "functional.alltypes t where t.int_col %s (select int_col from " + "functional.alltypesagg) group by t.id having count(*) < 10", op));
        AnalyzesOk(String.format("select t.bigint_col, t.string_col from " + "functional.alltypes t where t.id %s (select id from " + "functional.alltypesagg where int_col < 10) order by bigint_col", op));
        AnalyzesOk(String.format("select * from functional.alltypes a where a.id %s " + "(select id from functional.alltypes b where a.id = b.id)", op));
        // Complex expressions
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id + int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "t.int_col + 1 %s (select int_col - 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "abs(t.double_col) %s (select int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select NULL from functional.alltypes t where " + "cast(t.double_col as int) %s (select int_col from " + "functional.alltypestiny)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes where id %s " + "(select 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select 1 + 1 from functional.alltypestiny group by int_col)", op));
        AnalyzesOk(String.format("select max(id) from functional.alltypes where id %s " + "(select max(id) from functional.alltypesagg a where a.int_col < 10) " + "and bool_col = false", op));
        // Subquery returns multiple columns
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select id, int_col from functional.alltypessmall)", op), "Subquery must return a single column: (SELECT id, int_col " + "FROM functional.alltypessmall)");
        // Subquery returns an incompatible column type
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select timestamp_col from functional.alltypessmall)", op), "Incompatible return types 'INT' and 'TIMESTAMP' of exprs 'id' and " + "'timestamp_col'.");
        // Different column types in the subquery predicate
        for (String col : colNames) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.%s %s " + "(select a.%s from functional.alltypestiny a)", col, op, col));
        }
        // Decimal in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.double_col %s (select d3 from functional.decimal_tbl a)", op));
        // Varchar in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.string_col %s (select cast(a.string_col as varchar(1)) from " + "functional.alltypestiny a)", op));
        // Subqueries with multiple predicates in the WHERE clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col > 10 and " + "a.tinyint_col < 5)", op));
        // Subqueries with a GROUP BY clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.double_col < 10.1 " + "group by a.id)", op));
        // Subqueries with GROUP BY and HAVING clauses
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.bool_col = true and " + "int_col < 10 group by id having count(*) < 10)", op));
        // Subqueries with a LIMIT clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where id < 100 limit 10)", op));
        // Subqueries with multiple tables in the FROM clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, functional.alltypessmall s " + "where a.int_col = s.int_col and s.bigint_col < 100 and a.tinyint_col < 10)", op));
        // Different join operators between the tables in subquery's FROM clause
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a %s functional.alltypessmall " + "s on a.int_col = s.int_col where a.bool_col = false)", op, joinOp));
        }
        // Correlated predicates in the subquery's ON clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.bigint_col = a.bigint_col and " + "s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on a.bool_col = s.bool_col and t.int_col = 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on ifnull(s.int_col, s.int_col + 20) = " + "t.int_col + t.bigint_col)", op));
        // Subqueries with inline views
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, " + "(select * from functional.alltypessmall) s where s.int_col = a.int_col " + "and s.bool_col = false)", op));
        // Subqueries with inline views that contain subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select id from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select g.* from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a where " + "a.bigint_col = 100)", op));
        // Multiple tables in the FROM clause of the outer query block
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t %s " + "functional.alltypessmall s on t.int_col = s.int_col where " + "t.tinyint_col %s (select tinyint_col from functional.alltypesagg) " + "and t.bool_col = false and t.bigint_col = 10", joinOp, op));
        }
        // Subqueries in WITH clause
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a where " + "id %s (select id from functional.alltypestiny)) select * from t where " + "t.bool_col = false and t.int_col = 10", op));
        // Subqueries in WITH and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny s)) select * from t " + "where t.int_col in (select int_col from functional.alltypessmall) and " + "t.bool_col = false", op));
        // Subqueries in WITH, FROM and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny)) select t.* from t, " + "(select * from functional.alltypesagg g where g.id in " + "(select id from functional.alltypes)) s where s.string_col = t.string_col " + "and t.int_col in (select int_col from functional.alltypessmall) and " + "s.bool_col = false", op));
        // Correlated subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col) " + "and t.bool_col = false", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col + 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + a.int_col  = " + "a.bigint_col and a.bool_col = true)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col = false and " + "a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col)", op));
        // Multiple nesting levels (uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg where int_col %s " + "(select int_col from functional.alltypestiny) and bool_col = false) " + "and bigint_col < 1000", op, op));
        // Multiple nesting levels (correlated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where a.int_col = t.int_col " + "and a.tinyint_col %s (select tinyint_col from functional.alltypestiny s " + "where s.bigint_col = a.bigint_col))", op, op));
        // Multiple nesting levels (correlated and uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col " + "and a.int_col %s (select int_col from functional.alltypestiny s))", op, op));
        // NOT ([NOT] IN predicate)
        AnalyzesOk(String.format("select * from functional.alltypes t where not (id %s " + "(select id from functional.alltypesagg))", op));
        // Different cmp operators in the correlation predicate
        for (String cmpOp : cmpOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t " + "where t.id %s (select a.id from functional.alltypesagg a where " + "t.int_col %s a.int_col)", op, cmpOp));
        }
        // Uncorrelated IN subquery with analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col %s (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", op));
    }
    // Constant on the left hand side
    AnalyzesOk("select * from functional.alltypes a where 1 in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)");
    AnalysisError("select * from functional.alltypes a where 1 not in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)", "Unsupported NOT IN predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypesagg s WHERE s.int_col = a.int_col)");
    // IN subquery that is equivalent to an uncorrelated EXISTS subquery
    AnalysisError("select * from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg)", "Unsupported " + "predicate with subquery: 1 IN (SELECT int_col FROM functional.alltypesagg)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        AnalysisError(String.format("select 1 from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg g where g.id %s t.id)", cmpOp), String.format("Unsupported predicate with subquery: 1 " + "IN (SELECT int_col FROM functional.alltypesagg g WHERE g.id %s t.id)", cmpOp));
    }
    // NOT IN subquery with a correlated predicate that can't be used in an equi
    // join
    AnalysisError("select 1 from functional.alltypes t where 1 not in " + "(select id from functional.alltypestiny g where g.id < t.id)", "Unsupported predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypestiny g WHERE g.id < t.id)");
    // Statement with a GROUP BY and a correlated IN subquery that has
    // correlated predicate that cannot be transformed into an equi-join.
    AnalysisError("select id, count(*) from functional.alltypes t " + "where 1 IN (select id from functional.alltypesagg g where t.int_col < " + "g.int_col) group by id", "Unsupported predicate with subquery: 1 IN " + "(SELECT id FROM functional.alltypesagg g WHERE t.int_col < g.int_col)");
    // Reference a non-existing table in the subquery
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s left outer join p on " + "(s.int_col = p.int_col))", "Could not resolve table reference: 'p'");
    // Reference a non-existing column from a table in the outer scope
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s where s.int_col = t.bad_col)", "Could not resolve column/field reference: 't.bad_col'");
    // Referencing the same table in the inner and the outer query block
    // No explicit alias
    AnalyzesOk("select id from functional.alltypestiny where int_col in " + "(select int_col from functional.alltypestiny)");
    // Different alias between inner and outer block referencing the same table
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny p)");
    // Alias only in the outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny)");
    // Same alias in both inner and outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny t)");
    // Binary predicate with non-comparable operands
    AnalysisError("select * from functional.alltypes t where " + "(id in (select id from functional.alltypestiny)) = 'string_val'", "operands of type BOOLEAN and STRING are not comparable: " + "(id IN (SELECT id FROM functional.alltypestiny)) = 'string_val'");
    // OR with subquery predicates
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select id from functional.alltypesagg) or t.bool_col = false", "Subqueries in OR predicates are not supported: t.id IN " + "(SELECT id FROM functional.alltypesagg) OR t.bool_col = FALSE");
    AnalysisError("select * from functional.alltypes t where not (t.id in " + "(select id from functional.alltypesagg) and t.int_col = 10)", "Subqueries in OR predicates are not supported: t.id NOT IN " + "(SELECT id FROM functional.alltypesagg) OR t.int_col != 10");
    AnalysisError("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg g where g.bool_col = false) " + "or t.bool_col = true", "Subqueries in OR predicates are not " + "supported: EXISTS (SELECT * FROM functional.alltypesagg g WHERE " + "g.bool_col = FALSE) OR t.bool_col = TRUE");
    AnalysisError("select * from functional.alltypes t where t.id = " + "(select min(id) from functional.alltypesagg g) or t.id = 10", "Subqueries in OR predicates are not supported: t.id = " + "(SELECT min(id) FROM functional.alltypesagg g) OR t.id = 10");
    // Correlated subquery with OR predicate
    AnalysisError("select * from functional.alltypes t where id in " + "(select id from functional.alltypesagg a where " + "a.int_col = t.int_col or a.bool_col = false)", "Disjunctions " + "with correlated predicates are not supported: a.int_col = " + "t.int_col OR a.bool_col = FALSE");
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny) and (bool_col = false or " + "int_col = 10)");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select max(a.id) from functional.alltypesagg a where " + "t.int_col = a.int_col)", "Unsupported correlated subquery with grouping " + "and/or aggregation: SELECT max(a.id) FROM functional.alltypesagg a " + "WHERE t.int_col = a.int_col");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select a.id from functional.alltypesagg a where " + "t.int_col = a.int_col group by a.id)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT a.id FROM " + "functional.alltypesagg a WHERE t.int_col = a.int_col GROUP BY a.id");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select distinct a.id from functional.alltypesagg a where " + "a.bigint_col = t.bigint_col)", "Unsupported correlated subquery with " + "grouping and/or aggregation: SELECT DISTINCT a.id FROM " + "functional.alltypesagg a WHERE a.bigint_col = t.bigint_col");
    // NOT compound predicates with OR
    AnalyzesOk("select * from functional.alltypes t where not (" + "id in (select id from functional.alltypesagg) or int_col < 10)");
    AnalyzesOk("select * from functional.alltypes t where not (" + "t.id < 10 or not (t.int_col in (select int_col from " + "functional.alltypesagg) and t.bool_col = false))");
    // Multiple subquery predicates
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny where int_col = 10) and int_col in " + "(select int_col from functional.alltypessmall where bigint_col = 1000) and " + "string_col not in (select string_col from functional.alltypesagg where " + "tinyint_col > 10) and bool_col = false");
    // Correlated subquery with a LIMIT clause
    AnalysisError("select * from functional.alltypes t where id in " + "(select s.id from functional.alltypesagg s where s.int_col = t.int_col " + "limit 1)", "Unsupported correlated subquery with a LIMIT clause: " + "SELECT s.id FROM functional.alltypesagg s WHERE s.int_col = t.int_col " + "LIMIT 1");
    // Correlated IN with an analytic function
    AnalysisError("select id, int_col, bool_col from functional.alltypestiny t1 " + "where int_col in (select min(bigint_col) over (partition by bool_col) " + "from functional.alltypessmall t2 where t1.id < t2.id)", "Unsupported " + "correlated subquery with grouping and/or aggregation: SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE t1.id < t2.id");
    // IN subquery in binary predicate
    AnalysisError("select * from functional.alltypestiny where " + "(tinyint_col in (1,2)) = (bool_col in (select bool_col from " + "functional.alltypes))", "IN subquery predicates are not supported " + "in binary predicates: (tinyint_col IN (1, 2)) = (bool_col IN (SELECT " + "bool_col FROM functional.alltypes))");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "int_col in (select 1 as int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "int_col not in (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#end_block

#method_before
@Test
public void TestExistsSubqueries() throws AnalysisException {
    String[] existsOperators = { "exists", "not exists" };
    for (String op : existsOperators) {
        // [NOT] EXISTS predicate (correlated)
        AnalyzesOk(String.format("select * from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.id = t.id)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.int_col = t.int_col and p.bool_col = false)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col and a.bool_col = " + "t.bool_col)", op));
        // Multiple [NOT] EXISTS predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypessmall s where s.id = t.id) and " + "%s (select NULL from functional.alltypesagg g where t.int_col = g.int_col)", op, op));
        // OR between two subqueries
        AnalysisError(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id) or %s " + "(select * from functional.alltypessmall s where s.int_col = t.int_col)", op, op), String.format("Subqueries in OR predicates are not supported: %s " + "(SELECT * FROM functional.alltypesagg a WHERE a.id = t.id) OR %s (SELECT " + "* FROM functional.alltypessmall s WHERE s.int_col = t.int_col)", op.toUpperCase(), op.toUpperCase()));
        // Complex correlation predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id + 1) and " + "%s (select 1 from functional.alltypes s where s.int_col + s.bigint_col = " + "t.bigint_col + 1)", op, op));
        // Correlated predicates
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg g where t.int_col = g.int_col " + "and t.bool_col = false)", op));
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select id from functional.alltypessmall s where t.tinyint_col = " + "s.tinyint_col and t.bool_col)", op));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.int_col = s.int_col))", op, op));
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.bool_col = " + "s.bool_col))", op, op));
        // Correlated EXISTS subquery with a group by and aggregation
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t " + "where %s (select id, count(*) from functional.alltypesagg g where " + "t.id = g.id group by id)", op));
        // Correlated EXISTS subquery with an analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where %s (select min(bigint_col) over " + "(partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id)", op));
        // Correlated EXISTS subquery with an analytic function and a group by
        // clause
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where exists (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 " + "where t1.id = t2.id group by bigint_col, bool_col)", op));
        String[] nullOps = { "is null", "is not null" };
        for (String nullOp : nullOps) {
            // Uncorrelated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes where %s " + "(select * from functional.alltypestiny) %s and id < 5", op, nullOp));
            // Correlated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes t where " + "%s (select 1 from functional.alltypestiny s where t.id = s.id) " + "%s and t.bool_col = false", op, nullOp));
        }
    }
    // Uncorrelated EXISTS subquery with an analytic function
    AnalyzesOk("select * from functional.alltypestiny t " + "where EXISTS (select id, min(int_col) over (partition by bool_col) " + "from functional.alltypesagg a where bigint_col < 10)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        AnalysisError(String.format("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where t.id %s a.id)", cmpOp), String.format("Unsupported predicate with subquery: EXISTS (SELECT * FROM " + "functional.alltypesagg a WHERE t.id %s a.id)", cmpOp));
    }
    // Uncorrelated EXISTS in a query with GROUP BY
    AnalyzesOk("select id, count(*) from functional.alltypes t " + "where exists (select 1 from functional.alltypestiny where id < 5) group by id");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select int_col + 1 from functional.alltypessmall s where " + "t.int_col = 10)", "Unsupported predicate with subquery: EXISTS " + "(SELECT int_col + 1 FROM functional.alltypessmall s WHERE t.int_col = 10)");
    // Uncorrelated EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where exists " + "(select * from functional.alltypesagg where id < 10)");
    AnalyzesOk("select id from functional.alltypestiny where exists " + "(select id from functional.alltypessmall where bool_col = false)");
    AnalyzesOk("select 1 from functional.alltypestiny t where exists " + "(select 1 from functional.alltypessmall where id < 5)");
    AnalyzesOk("select 1 + 1 from functional.alltypestiny where exists " + "(select null from functional.alltypessmall where id != 5)");
    // Multiple nesting levels with uncorrelated EXISTS
    AnalyzesOk("select id from functional.alltypes where exists " + "(select id from functional.alltypestiny where int_col < 10 and exists (" + "select id from functional.alltypessmall where bool_col = true))");
    // Uncorrelated NOT EXISTS subquery
    AnalysisError("select * from functional.alltypestiny where not exists " + "(select 1 from functional.alltypessmall where bool_col = false)", "Unsupported uncorrelated NOT EXISTS subquery: SELECT 1 FROM " + "functional.alltypessmall WHERE bool_col = FALSE");
    // Subquery references an explicit alias from the outer block in the FROM
    // clause
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select * from t)", "Could not resolve table reference: 't'");
    // Uncorrelated subquery with no FROM clause
    AnalyzesOk("select * from functional.alltypes where exists (select 1,2)");
    // EXISTS subquery in a binary predicate
    AnalysisError("select * from functional.alltypes where " + "if(exists(select * from functional.alltypesagg), 1, 0) = 1", "IN and/or EXISTS subquery predicates are not supported in binary predicates: " + "if(EXISTS (SELECT * FROM functional.alltypesagg), 1, 0) = 1");
    // Correlated subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select 1 from functional.alltypesagg g where t.id = g.id limit 1)");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "exists (select int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "not exists (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#method_after
@Test
public void TestExistsSubqueries() throws AnalysisException {
    String[] existsOperators = { "exists", "not exists" };
    for (String op : existsOperators) {
        // [NOT] EXISTS predicate (correlated)
        AnalyzesOk(String.format("select * from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.id = t.id)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.int_col = t.int_col and p.bool_col = false)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col and a.bool_col = " + "t.bool_col)", op));
        // Multiple [NOT] EXISTS predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypessmall s where s.id = t.id) and " + "%s (select NULL from functional.alltypesagg g where t.int_col = g.int_col)", op, op));
        // OR between two subqueries
        AnalysisError(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id) or %s " + "(select * from functional.alltypessmall s where s.int_col = t.int_col)", op, op), String.format("Subqueries in OR predicates are not supported: %s " + "(SELECT * FROM functional.alltypesagg a WHERE a.id = t.id) OR %s (SELECT " + "* FROM functional.alltypessmall s WHERE s.int_col = t.int_col)", op.toUpperCase(), op.toUpperCase()));
        // Complex correlation predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id + 1) and " + "%s (select 1 from functional.alltypes s where s.int_col + s.bigint_col = " + "t.bigint_col + 1)", op, op));
        // Correlated predicates
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg g where t.int_col = g.int_col " + "and t.bool_col = false)", op));
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select id from functional.alltypessmall s where t.tinyint_col = " + "s.tinyint_col and t.bool_col)", op));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.int_col = s.int_col))", op, op));
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.bool_col = " + "s.bool_col))", op, op));
        // Correlated EXISTS subquery with a group by and aggregation
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t " + "where %s (select id, count(*) from functional.alltypesagg g where " + "t.id = g.id group by id)", op));
        // Correlated EXISTS subquery with an analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where %s (select min(bigint_col) over " + "(partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id)", op));
        // Correlated EXISTS subquery with an analytic function and a group by
        // clause
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where exists (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 " + "where t1.id = t2.id group by bigint_col, bool_col)", op));
        String[] nullOps = { "is null", "is not null" };
        for (String nullOp : nullOps) {
            // Uncorrelated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes where %s " + "(select * from functional.alltypestiny) %s and id < 5", op, nullOp));
            // Correlated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes t where " + "%s (select 1 from functional.alltypestiny s where t.id = s.id) " + "%s and t.bool_col = false", op, nullOp));
        }
    }
    // Uncorrelated EXISTS subquery with an analytic function
    AnalyzesOk("select * from functional.alltypestiny t " + "where EXISTS (select id, min(int_col) over (partition by bool_col) " + "from functional.alltypesagg a where bigint_col < 10)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        AnalysisError(String.format("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where t.id %s a.id)", cmpOp), String.format("Unsupported predicate with subquery: EXISTS (SELECT * FROM " + "functional.alltypesagg a WHERE t.id %s a.id)", cmpOp));
    }
    // Uncorrelated EXISTS in a query with GROUP BY
    AnalyzesOk("select id, count(*) from functional.alltypes t " + "where exists (select 1 from functional.alltypestiny where id < 5) group by id");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select int_col + 1 from functional.alltypessmall s where " + "t.int_col = 10)", "Unsupported predicate with subquery: EXISTS " + "(SELECT int_col + 1 FROM functional.alltypessmall s WHERE t.int_col = 10)");
    // Uncorrelated EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where exists " + "(select * from functional.alltypesagg where id < 10)");
    AnalyzesOk("select id from functional.alltypestiny where exists " + "(select id from functional.alltypessmall where bool_col = false)");
    AnalyzesOk("select 1 from functional.alltypestiny t where exists " + "(select 1 from functional.alltypessmall where id < 5)");
    AnalyzesOk("select 1 + 1 from functional.alltypestiny where exists " + "(select null from functional.alltypessmall where id != 5)");
    // Multiple nesting levels with uncorrelated EXISTS
    AnalyzesOk("select id from functional.alltypes where exists " + "(select id from functional.alltypestiny where int_col < 10 and exists (" + "select id from functional.alltypessmall where bool_col = true))");
    // Uncorrelated NOT EXISTS subquery
    AnalysisError("select * from functional.alltypestiny where not exists " + "(select 1 from functional.alltypessmall where bool_col = false)", "Unsupported uncorrelated NOT EXISTS subquery: SELECT 1 FROM " + "functional.alltypessmall WHERE bool_col = FALSE");
    // Subquery references an explicit alias from the outer block in the FROM
    // clause
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select * from t)", "Illegal table reference to non-collection type: 't'");
    // Uncorrelated subquery with no FROM clause
    AnalyzesOk("select * from functional.alltypes where exists (select 1,2)");
    // EXISTS subquery in a binary predicate
    AnalysisError("select * from functional.alltypes where " + "if(exists(select * from functional.alltypesagg), 1, 0) = 1", "EXISTS subquery predicates are not supported in binary predicates: " + "if(EXISTS (SELECT * FROM functional.alltypesagg), 1, 0) = 1");
    // Correlated subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select 1 from functional.alltypesagg g where t.id = g.id limit 1)");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "exists (select int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "not exists (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#end_block

#method_before
@Test
public void TestAggregateSubqueries() throws AnalysisException {
    String[] aggFns = { "count(id)", "max(id)", "min(id)", "avg(id)", "sum(id)" };
    for (String aggFn : aggFns) {
        for (String cmpOp : cmpOperators) {
            // Uncorrelated
            AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select %s from functional.alltypestiny)", cmpOp, aggFn));
            AnalyzesOk(String.format("select * from functional.alltypes where " + "(select %s from functional.alltypestiny) %s id", aggFn, cmpOp));
            // Uncorrelated with constant expr
            AnalyzesOk(String.format("select * from functional.alltypes where 10 %s " + "(select %s from functional.alltypestiny)", cmpOp, aggFn));
            // Uncorrelated with complex cmp expr
            AnalyzesOk(String.format("select * from functional.alltypes where id + 10 %s " + "(select %s from functional.alltypestiny)", cmpOp, aggFn));
            AnalyzesOk(String.format("select * from functional.alltypes where id + 10 %s " + "(select %s + 1 from functional.alltypestiny)", cmpOp, aggFn));
            AnalyzesOk(String.format("select * from functional.alltypes where " + "(select %s + 1 from functional.alltypestiny) %s id + 10", aggFn, cmpOp));
            AnalyzesOk(String.format("select 1 from functional.alltypes where " + "1 + (select %s - 1 from functional.alltypestiny where bool_col = false) " + "%s id - 10", aggFn, cmpOp));
            AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 where " + "(select %s from functional.alltypes) - t1.id %s " + "t1.tinyint_col", aggFn, cmpOp));
            AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 where " + "(select %s from functional.alltypes) + t1.id %s " + "t1.tinyint_col + t1.bigint_col + 1", aggFn, cmpOp));
            AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 inner " + "join functional.alltypessmall t2 on t1.id = t2.id where " + "(select %s from functional.alltypes) + 1 %s t1.int_col + t2.int_col", aggFn, cmpOp));
            // Correlated
            AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "id %s (select %s from functional.alltypestiny t where t.bool_col = false " + "and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn));
            // Correlated with constant expr
            AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "10 %s (select %s from functional.alltypestiny t where t.bool_col = false " + "and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn));
            // Correlated with complex expr
            AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "id - 10 %s (select %s from functional.alltypestiny t where t.bool_col = " + "false and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn));
            // count is not supported in select list expressions of a correlated subquery
            if (aggFn.equals("count(id)")) {
                AnalysisError(String.format("select count(*) from functional.alltypes a where " + "id - 10 %s (select 1 + %s from functional.alltypestiny t where " + "t.bool_col = false and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn), String.format("Aggregate function that returns non-null " + "on an empty input cannot be used in an expression in a " + "correlated subquery's select list: (SELECT 1 + %s FROM " + "functional.alltypestiny t WHERE t.bool_col = FALSE AND a.int_col = " + "t.int_col)", aggFn));
            } else {
                AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "id - 10 %s (select 1 + %s from functional.alltypestiny t where " + "t.bool_col = false and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn));
                AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "(select 1 + %s from functional.alltypestiny t where t.bool_col = false " + "and a.int_col = t.int_col) %s id - 10 and a.bigint_col < 10", aggFn, cmpOp));
                AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "1 + (select 1 + %s from functional.alltypestiny t where t.id = a.id " + "and t.int_col < 10) %s a.id + 10", aggFn, cmpOp));
            }
        }
    }
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select 1 from functional.alltypestiny where " + "int_col = (select count(int_col) as int_col from functional.alltypesagg)");
    AnalyzesOk("select 1 from functional.alltypestiny where " + "int_col in (select sum(int_col) as int_col from functional.alltypesagg)");
    for (String cmpOp : cmpOperators) {
        // Multiple tables in parent and subquery query blocks
        AnalyzesOk(String.format("select * from functional.alltypes t, " + "functional.alltypesagg a where a.id = t.id and t.int_col %s (" + "select max(g.int_col) from functional.alltypestiny g left outer join " + "functional.alltypessmall s on s.bigint_col = g.bigint_col where " + "g.bool_col = false) and t.bool_col = true", cmpOp));
        // Group by in the parent query block
        AnalyzesOk(String.format("select t.int_col, count(*) from " + "functional.alltypes t left outer join functional.alltypesagg g " + "on t.id = g.id where t.bigint_col %s (select count(*) from " + "functional.alltypestiny a where a.int_col < 10) and g.bool_col = false " + "group by t.int_col having count(*) < 100", cmpOp));
        // Multiple binary predicates
        AnalyzesOk(String.format("select * from functional.alltypes a where " + "int_col %s (select min(int_col) from functional.alltypesagg g where " + "g.bool_col = false) and int_col %s (select max(int_col) from " + "functional.alltypesagg g where g.bool_col = true) and a.tinyint_col = 10", cmpOp, cmpOp));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes a where " + "tinyint_col %s (select count(*) from functional.alltypesagg g where " + "g.int_col %s (select max(int_col) from functional.alltypestiny t where " + "t.id = g.id) and g.id = a.id and g.bool_col = false) and a.int_col < 10", cmpOp, cmpOp));
        // NOT with a binary subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes a where " + "not (int_col %s (select max(int_col) from functional.alltypesagg g where " + "a.id = g.id and g.bool_col = false))", cmpOp));
        // Subquery returns a scalar (no FORM clause)
        AnalyzesOk(String.format("select id from functional.alltypestiny where id %s " + "(select 1)", cmpOp));
        // Incompatible comparison types
        AnalysisError(String.format("select id from functional.alltypestiny where " + "int_col %s (select max(timestamp_col) from functional.alltypessmall)", cmpOp), String.format("operands of type INT and TIMESTAMP are not comparable: " + "int_col %s (SELECT max(timestamp_col) FROM functional.alltypessmall)", cmpOp));
        // Distinct in the outer select block
        AnalyzesOk(String.format("select distinct id from functional.alltypes a " + "where 100 %s (select count(*) from functional.alltypesagg g where " + "a.int_col %s g.int_col) and a.bool_col = false", cmpOp, cmpOp));
    }
    // Subquery returns multiple rows
    AnalysisError("select * from functional.alltypestiny where " + "(select max(id) from functional.alltypes) = " + "(select id from functional.alltypestiny)", "Subquery must return a single row: " + "(SELECT id FROM functional.alltypestiny)");
    AnalysisError("select id from functional.alltypestiny t where int_col = " + "(select int_col from functional.alltypessmall limit 2)", "Subquery must return a single row: " + "(SELECT int_col FROM functional.alltypessmall LIMIT 2)");
    AnalysisError("select id from functional.alltypestiny where int_col = " + "(select id from functional.alltypessmall)", "Subquery must return a single row: " + "(SELECT id FROM functional.alltypessmall)");
    // Subquery returns multiple columns
    AnalysisError("select id from functional.alltypestiny where int_col = " + "(select id, int_col from functional.alltypessmall)", "Subquery must return a single row: " + "(SELECT id, int_col FROM functional.alltypessmall)");
    AnalysisError("select * from functional.alltypestiny where id in " + "(select * from (values(1,2)) as t)", "Subquery must return a single column: (SELECT * FROM (VALUES(1, 2)) t)");
    // Subquery returns multiple columns due to a group by clause
    AnalysisError("select id from functional.alltypestiny where int_col = " + "(select int_col, count(*) from functional.alltypessmall group by int_col)", "Subquery must return a single row: " + "(SELECT int_col, count(*) FROM functional.alltypessmall " + "GROUP BY int_col)");
    // Outer join with a table from the outer block using an explicit alias
    AnalysisError("select id from functional.alltypestiny t where int_col = " + "(select count(*) from functional.alltypessmall s left outer join t " + "on (t.id = s.id))", "Could not resolve table reference: 't'");
    AnalysisError("select id from functional.alltypestiny t where int_col = " + "(select count(*) from functional.alltypessmall s right outer join t " + "on (t.id = s.id))", "Could not resolve table reference: 't'");
    AnalysisError("select id from functional.alltypestiny t where int_col = " + "(select count(*) from functional.alltypessmall s full outer join t " + "on (t.id = s.id))", "Could not resolve table reference: 't'");
    // Multiple subqueries in a binary predicate
    AnalysisError("select * from functional.alltypestiny t where " + "(select count(*) from functional.alltypessmall) = " + "(select count(*) from functional.alltypesagg)", "Multiple subqueries are not " + "supported in binary predicates: (SELECT count(*) FROM " + "functional.alltypessmall) = (SELECT count(*) FROM functional.alltypesagg)");
    AnalysisError("select * from functional.alltypestiny t where " + "(select max(id) from functional.alltypessmall) + " + "(select min(id) from functional.alltypessmall) - " + "(select count(id) from functional.alltypessmall) < 1000", "Multiple subqueries are not supported in binary predicates: (SELECT max(id) " + "FROM functional.alltypessmall) + (SELECT min(id) FROM " + "functional.alltypessmall) - (SELECT count(id) FROM functional.alltypessmall) " + "< 1000");
    // Comparison between invalid types
    AnalysisError("select * from functional.alltypes where " + "(select max(string_col) from functional.alltypesagg) = 1", "operands of type STRING and TINYINT are not comparable: (SELECT " + "max(string_col) FROM functional.alltypesagg) = 1");
    // Aggregate subquery with a LIMIT 1 clause
    AnalyzesOk("select id from functional.alltypestiny t where int_col = " + "(select int_col from functional.alltypessmall limit 1)");
    // Correlated aggregate suquery with correlated predicate that can't be
    // transformed into an equi-join
    AnalyzesOk("select id from functional.alltypestiny t where " + "1 < (select sum(int_col) from functional.alltypessmall s where " + "t.id < 10)");
    // Aggregate subqueries in an IS [NOT] NULL predicate
    String[] nullOps = { "is null", "is not null" };
    for (String aggFn : aggFns) {
        for (String nullOp : nullOps) {
            // Uncorrelated aggregate subquery
            AnalyzesOk(String.format("select * from functional.alltypestiny where " + "(select %s from functional.alltypessmall where bool_col = false) " + "%s and int_col < 10", aggFn, nullOp));
            // Correlated aggregate subquery
            AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "(select %s from functional.alltypessmall s where s.id = t.id " + "and s.bool_col = false) %s and bool_col = true", aggFn, nullOp));
        }
    }
    // Aggregate subquery with a correlated predicate that can't be transformed
    // into an equi-join in an IS NULL predicate
    AnalyzesOk("select 1 from functional.alltypestiny t where " + "(select max(id) from functional.alltypessmall s where t.id < 10) " + "is null");
    // Mathematical functions with scalar subqueries
    String[] mathFns = { "abs", "cos", "ceil", "floor" };
    for (String mathFn : mathFns) {
        for (String aggFn : aggFns) {
            String expr = aggFn.equals("count(id)") ? "" : "1 + ";
            for (String cmpOp : cmpOperators) {
                // Uncorrelated scalar subquery
                AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "%s((select %s %s from functional.alltypessmall where bool_col = " + "false)) %s 100 - t.int_col and t.bigint_col < 100", mathFn, expr, aggFn, cmpOp));
                // Correlated scalar subquery
                AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "%s((select %s %s from functional.alltypessmall s where bool_col = false " + "and t.id = s.id)) %s 100 - t.int_col and t.bigint_col < 100", mathFn, expr, aggFn, cmpOp));
            }
        }
    }
    // Conditional functions with scalar subqueries
    for (String aggFn : aggFns) {
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "nullifzero((select %s from functional.alltypessmall s where " + "s.bool_col = false)) is null", aggFn));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "zeroifnull((select %s from functional.alltypessmall s where t.id = s.id)) " + "= 0 and t.int_col < 10", aggFn));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "isnull((select %s from functional.alltypestiny s where s.bool_col = false " + "), 10) < 5", aggFn));
    }
    // Correlated aggregate subquery with a GROUP BY
    AnalysisError("select min(t.id) as min_id from functional.alltypestiny t " + "where t.int_col < (select max(s.int_col) from functional.alltypessmall s " + "where s.id = t.id group by s.bigint_col order by 1 limit 1)", "Unsupported correlated subquery with grouping and/or aggregation: " + "SELECT max(s.int_col) FROM functional.alltypessmall s WHERE " + "s.id = t.id GROUP BY s.bigint_col ORDER BY 1 ASC LIMIT 1");
    // Correlated aggregate subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where " + "t.id = (select count(*) from functional.alltypesagg g where " + "g.int_col = t.int_col limit 1)");
    // Aggregate subquery with analytic function
    AnalysisError("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col = (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", "Subquery must return a single row: (SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE int_col < 10)");
    // Aggregate subquery with analytic function and limit 1 clause
    AnalysisError("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col = (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id and int_col < 10 limit 1)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT min(bigint_col) " + "OVER (PARTITION BY bool_col) FROM functional.alltypessmall t2 WHERE " + "t1.id = t2.id AND int_col < 10 LIMIT 1");
    // Uncorrelated aggregate subquery with analytic function and limit 1 clause
    AnalyzesOk("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col = (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10 limit 1)");
    // Subquery with distinct in binary predicate
    AnalysisError("select * from functional.alltypes where int_col = " + "(select distinct int_col from functional.alltypesagg)", "Subquery " + "must return a single row: (SELECT DISTINCT int_col FROM " + "functional.alltypesagg)");
    AnalyzesOk("select * from functional.alltypes where int_col = " + "(select count(distinct int_col) from functional.alltypesagg)");
    // Multiple count aggregate functions in a correlated subquery's select list
    AnalysisError("select * from functional.alltypes t where " + "int_col = (select count(id) + count(int_col) - 1 from " + "functional.alltypesagg g where g.int_col = t.int_col)", "Aggregate function that returns non-null on an empty input " + "cannot be used in an expression in a correlated " + "subquery's select list: (SELECT count(id) + count(int_col) - 1 " + "FROM functional.alltypesagg g WHERE g.int_col = t.int_col)");
    // UDAs in aggregate subqqueries
    addTestUda("AggFn", Type.BIGINT, Type.BIGINT);
    AnalysisError("select * from functional.alltypesagg g where " + "(select aggfn(int_col) from functional.alltypes s where " + "s.id = g.id) = 10", "UDAs are not supported in the select list of " + "correlated subqueries: (SELECT default.aggfn(int_col) FROM " + "functional.alltypes s WHERE s.id = g.id)");
    AnalyzesOk("select * from functional.alltypesagg g where " + "(select aggfn(int_col) from functional.alltypes s where " + "s.bool_col = false) < 10");
    // sample, histogram in scalar subqueries
    String[] aggFnsReturningStringOnEmpty = { "sample(int_col)", "histogram(int_col)" };
    for (String aggFn : aggFnsReturningStringOnEmpty) {
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "t.string_col = (select %s from functional.alltypesagg g where t.id = " + "g.id)", aggFn));
    }
    // Complex correlated predicate in which columns from the subquery appear in
    // both sides of a correlated binary predicate
    AnalysisError("select 1 from functional.alltypestiny t where " + "(select sum(t1.id) from functional.alltypesagg t1 inner join " + "functional.alltypes t2 on t1.id = t2.id where " + "t1.id + t2.id = t.int_col + t1.int_col) = t.int_col", "All subquery columns that participate in a predicate " + "must be on the same side of that predicate: t1.id + t2.id = t.int_col " + "+ t1.int_col");
}
#method_after
@Test
public void TestAggregateSubqueries() throws AnalysisException {
    String[] aggFns = { "count(id)", "max(id)", "min(id)", "avg(id)", "sum(id)" };
    for (String aggFn : aggFns) {
        for (String cmpOp : cmpOperators) {
            // Uncorrelated
            AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select %s from functional.alltypestiny)", cmpOp, aggFn));
            AnalyzesOk(String.format("select * from functional.alltypes where " + "(select %s from functional.alltypestiny) %s id", aggFn, cmpOp));
            // Uncorrelated with constant expr
            AnalyzesOk(String.format("select * from functional.alltypes where 10 %s " + "(select %s from functional.alltypestiny)", cmpOp, aggFn));
            // Uncorrelated with complex cmp expr
            AnalyzesOk(String.format("select * from functional.alltypes where id + 10 %s " + "(select %s from functional.alltypestiny)", cmpOp, aggFn));
            AnalyzesOk(String.format("select * from functional.alltypes where id + 10 %s " + "(select %s + 1 from functional.alltypestiny)", cmpOp, aggFn));
            AnalyzesOk(String.format("select * from functional.alltypes where " + "(select %s + 1 from functional.alltypestiny) %s id + 10", aggFn, cmpOp));
            AnalyzesOk(String.format("select 1 from functional.alltypes where " + "1 + (select %s - 1 from functional.alltypestiny where bool_col = false) " + "%s id - 10", aggFn, cmpOp));
            AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 where " + "(select %s from functional.alltypes) - t1.id %s " + "t1.tinyint_col", aggFn, cmpOp));
            AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 where " + "(select %s from functional.alltypes) + t1.id %s " + "t1.tinyint_col + t1.bigint_col + 1", aggFn, cmpOp));
            AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 inner " + "join functional.alltypessmall t2 on t1.id = t2.id where " + "(select %s from functional.alltypes) + 1 %s t1.int_col + t2.int_col", aggFn, cmpOp));
            // Correlated
            AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "id %s (select %s from functional.alltypestiny t where t.bool_col = false " + "and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn));
            // Correlated with constant expr
            AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "10 %s (select %s from functional.alltypestiny t where t.bool_col = false " + "and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn));
            // Correlated with complex expr
            AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "id - 10 %s (select %s from functional.alltypestiny t where t.bool_col = " + "false and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn));
            // count is not supported in select list expressions of a correlated subquery
            if (aggFn.equals("count(id)")) {
                AnalysisError(String.format("select count(*) from functional.alltypes a where " + "id - 10 %s (select 1 + %s from functional.alltypestiny t where " + "t.bool_col = false and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn), String.format("Aggregate function that returns non-null " + "on an empty input cannot be used in an expression in a " + "correlated subquery's select list: (SELECT 1 + %s FROM " + "functional.alltypestiny t WHERE t.bool_col = FALSE AND a.int_col = " + "t.int_col)", aggFn));
            } else {
                AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "id - 10 %s (select 1 + %s from functional.alltypestiny t where " + "t.bool_col = false and a.int_col = t.int_col) and a.bigint_col < 10", cmpOp, aggFn));
                AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "(select 1 + %s from functional.alltypestiny t where t.bool_col = false " + "and a.int_col = t.int_col) %s id - 10 and a.bigint_col < 10", aggFn, cmpOp));
                AnalyzesOk(String.format("select count(*) from functional.alltypes a where " + "1 + (select 1 + %s from functional.alltypestiny t where t.id = a.id " + "and t.int_col < 10) %s a.id + 10", aggFn, cmpOp));
            }
        }
    }
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select 1 from functional.alltypestiny where " + "int_col = (select count(int_col) as int_col from functional.alltypesagg)");
    AnalyzesOk("select 1 from functional.alltypestiny where " + "int_col in (select sum(int_col) as int_col from functional.alltypesagg)");
    for (String cmpOp : cmpOperators) {
        // Multiple tables in parent and subquery query blocks
        AnalyzesOk(String.format("select * from functional.alltypes t, " + "functional.alltypesagg a where a.id = t.id and t.int_col %s (" + "select max(g.int_col) from functional.alltypestiny g left outer join " + "functional.alltypessmall s on s.bigint_col = g.bigint_col where " + "g.bool_col = false) and t.bool_col = true", cmpOp));
        // Group by in the parent query block
        AnalyzesOk(String.format("select t.int_col, count(*) from " + "functional.alltypes t left outer join functional.alltypesagg g " + "on t.id = g.id where t.bigint_col %s (select count(*) from " + "functional.alltypestiny a where a.int_col < 10) and g.bool_col = false " + "group by t.int_col having count(*) < 100", cmpOp));
        // Multiple binary predicates
        AnalyzesOk(String.format("select * from functional.alltypes a where " + "int_col %s (select min(int_col) from functional.alltypesagg g where " + "g.bool_col = false) and int_col %s (select max(int_col) from " + "functional.alltypesagg g where g.bool_col = true) and a.tinyint_col = 10", cmpOp, cmpOp));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes a where " + "tinyint_col %s (select count(*) from functional.alltypesagg g where " + "g.int_col %s (select max(int_col) from functional.alltypestiny t where " + "t.id = g.id) and g.id = a.id and g.bool_col = false) and a.int_col < 10", cmpOp, cmpOp));
        // NOT with a binary subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes a where " + "not (int_col %s (select max(int_col) from functional.alltypesagg g where " + "a.id = g.id and g.bool_col = false))", cmpOp));
        // Subquery returns a scalar (no FORM clause)
        AnalyzesOk(String.format("select id from functional.alltypestiny where id %s " + "(select 1)", cmpOp));
        // Incompatible comparison types
        AnalysisError(String.format("select id from functional.alltypestiny where " + "int_col %s (select max(timestamp_col) from functional.alltypessmall)", cmpOp), String.format("operands of type INT and TIMESTAMP are not comparable: " + "int_col %s (SELECT max(timestamp_col) FROM functional.alltypessmall)", cmpOp));
        // Distinct in the outer select block
        AnalyzesOk(String.format("select distinct id from functional.alltypes a " + "where 100 %s (select count(*) from functional.alltypesagg g where " + "a.int_col %s g.int_col) and a.bool_col = false", cmpOp, cmpOp));
    }
    // Subquery returns multiple rows
    AnalysisError("select * from functional.alltypestiny where " + "(select max(id) from functional.alltypes) = " + "(select id from functional.alltypestiny)", "Subquery must return a single row: " + "(SELECT id FROM functional.alltypestiny)");
    AnalysisError("select id from functional.alltypestiny t where int_col = " + "(select int_col from functional.alltypessmall limit 2)", "Subquery must return a single row: " + "(SELECT int_col FROM functional.alltypessmall LIMIT 2)");
    AnalysisError("select id from functional.alltypestiny where int_col = " + "(select id from functional.alltypessmall)", "Subquery must return a single row: " + "(SELECT id FROM functional.alltypessmall)");
    // Subquery returns multiple columns
    AnalysisError("select id from functional.alltypestiny where int_col = " + "(select id, int_col from functional.alltypessmall)", "Subquery must return a single row: " + "(SELECT id, int_col FROM functional.alltypessmall)");
    AnalysisError("select * from functional.alltypestiny where id in " + "(select * from (values(1,2)) as t)", "Subquery must return a single column: (SELECT * FROM (VALUES(1, 2)) t)");
    // Subquery returns multiple columns due to a group by clause
    AnalysisError("select id from functional.alltypestiny where int_col = " + "(select int_col, count(*) from functional.alltypessmall group by int_col)", "Subquery must return a single row: " + "(SELECT int_col, count(*) FROM functional.alltypessmall " + "GROUP BY int_col)");
    // Outer join with a table from the outer block using an explicit alias
    AnalysisError("select id from functional.alltypestiny t where int_col = " + "(select count(*) from functional.alltypessmall s left outer join t " + "on (t.id = s.id))", "Illegal table reference to non-collection type: 't'");
    AnalysisError("select id from functional.alltypestiny t where int_col = " + "(select count(*) from functional.alltypessmall s right outer join t " + "on (t.id = s.id))", "Illegal table reference to non-collection type: 't'");
    AnalysisError("select id from functional.alltypestiny t where int_col = " + "(select count(*) from functional.alltypessmall s full outer join t " + "on (t.id = s.id))", "Illegal table reference to non-collection type: 't'");
    // Multiple subqueries in a binary predicate
    AnalysisError("select * from functional.alltypestiny t where " + "(select count(*) from functional.alltypessmall) = " + "(select count(*) from functional.alltypesagg)", "Multiple subqueries are not " + "supported in binary predicates: (SELECT count(*) FROM " + "functional.alltypessmall) = (SELECT count(*) FROM functional.alltypesagg)");
    AnalysisError("select * from functional.alltypestiny t where " + "(select max(id) from functional.alltypessmall) + " + "(select min(id) from functional.alltypessmall) - " + "(select count(id) from functional.alltypessmall) < 1000", "Multiple subqueries are not supported in binary predicates: (SELECT max(id) " + "FROM functional.alltypessmall) + (SELECT min(id) FROM " + "functional.alltypessmall) - (SELECT count(id) FROM functional.alltypessmall) " + "< 1000");
    // Comparison between invalid types
    AnalysisError("select * from functional.alltypes where " + "(select max(string_col) from functional.alltypesagg) = 1", "operands of type STRING and TINYINT are not comparable: (SELECT " + "max(string_col) FROM functional.alltypesagg) = 1");
    // Aggregate subquery with a LIMIT 1 clause
    AnalyzesOk("select id from functional.alltypestiny t where int_col = " + "(select int_col from functional.alltypessmall limit 1)");
    // Correlated aggregate suquery with correlated predicate that can't be
    // transformed into an equi-join
    AnalyzesOk("select id from functional.alltypestiny t where " + "1 < (select sum(int_col) from functional.alltypessmall s where " + "t.id < 10)");
    // Aggregate subqueries in an IS [NOT] NULL predicate
    String[] nullOps = { "is null", "is not null" };
    for (String aggFn : aggFns) {
        for (String nullOp : nullOps) {
            // Uncorrelated aggregate subquery
            AnalyzesOk(String.format("select * from functional.alltypestiny where " + "(select %s from functional.alltypessmall where bool_col = false) " + "%s and int_col < 10", aggFn, nullOp));
            // Correlated aggregate subquery
            AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "(select %s from functional.alltypessmall s where s.id = t.id " + "and s.bool_col = false) %s and bool_col = true", aggFn, nullOp));
        }
    }
    // Aggregate subquery with a correlated predicate that can't be transformed
    // into an equi-join in an IS NULL predicate
    AnalyzesOk("select 1 from functional.alltypestiny t where " + "(select max(id) from functional.alltypessmall s where t.id < 10) " + "is null");
    // Mathematical functions with scalar subqueries
    String[] mathFns = { "abs", "cos", "ceil", "floor" };
    for (String mathFn : mathFns) {
        for (String aggFn : aggFns) {
            String expr = aggFn.equals("count(id)") ? "" : "1 + ";
            for (String cmpOp : cmpOperators) {
                // Uncorrelated scalar subquery
                AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "%s((select %s %s from functional.alltypessmall where bool_col = " + "false)) %s 100 - t.int_col and t.bigint_col < 100", mathFn, expr, aggFn, cmpOp));
                // Correlated scalar subquery
                AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "%s((select %s %s from functional.alltypessmall s where bool_col = false " + "and t.id = s.id)) %s 100 - t.int_col and t.bigint_col < 100", mathFn, expr, aggFn, cmpOp));
            }
        }
    }
    // Conditional functions with scalar subqueries
    for (String aggFn : aggFns) {
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "nullifzero((select %s from functional.alltypessmall s where " + "s.bool_col = false)) is null", aggFn));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "zeroifnull((select %s from functional.alltypessmall s where t.id = s.id)) " + "= 0 and t.int_col < 10", aggFn));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "isnull((select %s from functional.alltypestiny s where s.bool_col = false " + "), 10) < 5", aggFn));
    }
    // Correlated aggregate subquery with a GROUP BY
    AnalysisError("select min(t.id) as min_id from functional.alltypestiny t " + "where t.int_col < (select max(s.int_col) from functional.alltypessmall s " + "where s.id = t.id group by s.bigint_col order by 1 limit 1)", "Unsupported correlated subquery with grouping and/or aggregation: " + "SELECT max(s.int_col) FROM functional.alltypessmall s WHERE " + "s.id = t.id GROUP BY s.bigint_col ORDER BY 1 ASC LIMIT 1");
    // Correlated aggregate subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where " + "t.id = (select count(*) from functional.alltypesagg g where " + "g.int_col = t.int_col limit 1)");
    // Aggregate subquery with analytic function
    AnalysisError("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col = (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", "Subquery must return a single row: (SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE int_col < 10)");
    // Aggregate subquery with analytic function and limit 1 clause
    AnalysisError("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col = (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id and int_col < 10 limit 1)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT min(bigint_col) " + "OVER (PARTITION BY bool_col) FROM functional.alltypessmall t2 WHERE " + "t1.id = t2.id AND int_col < 10 LIMIT 1");
    // Uncorrelated aggregate subquery with analytic function and limit 1 clause
    AnalyzesOk("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col = (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10 limit 1)");
    // Subquery with distinct in binary predicate
    AnalysisError("select * from functional.alltypes where int_col = " + "(select distinct int_col from functional.alltypesagg)", "Subquery " + "must return a single row: (SELECT DISTINCT int_col FROM " + "functional.alltypesagg)");
    AnalyzesOk("select * from functional.alltypes where int_col = " + "(select count(distinct int_col) from functional.alltypesagg)");
    // Multiple count aggregate functions in a correlated subquery's select list
    AnalysisError("select * from functional.alltypes t where " + "int_col = (select count(id) + count(int_col) - 1 from " + "functional.alltypesagg g where g.int_col = t.int_col)", "Aggregate function that returns non-null on an empty input " + "cannot be used in an expression in a correlated " + "subquery's select list: (SELECT count(id) + count(int_col) - 1 " + "FROM functional.alltypesagg g WHERE g.int_col = t.int_col)");
    // UDAs in aggregate subqqueries
    addTestUda("AggFn", Type.BIGINT, Type.BIGINT);
    AnalysisError("select * from functional.alltypesagg g where " + "(select aggfn(int_col) from functional.alltypes s where " + "s.id = g.id) = 10", "UDAs are not supported in the select list of " + "correlated subqueries: (SELECT default.aggfn(int_col) FROM " + "functional.alltypes s WHERE s.id = g.id)");
    AnalyzesOk("select * from functional.alltypesagg g where " + "(select aggfn(int_col) from functional.alltypes s where " + "s.bool_col = false) < 10");
    // sample, histogram in scalar subqueries
    String[] aggFnsReturningStringOnEmpty = { "sample(int_col)", "histogram(int_col)" };
    for (String aggFn : aggFnsReturningStringOnEmpty) {
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "t.string_col = (select %s from functional.alltypesagg g where t.id = " + "g.id)", aggFn));
    }
    // Complex correlated predicate in which columns from the subquery appear in
    // both sides of a correlated binary predicate
    AnalysisError("select 1 from functional.alltypestiny t where " + "(select sum(t1.id) from functional.alltypesagg t1 inner join " + "functional.alltypes t2 on t1.id = t2.id where " + "t1.id + t2.id = t.int_col + t1.int_col) = t.int_col", "All subquery columns that participate in a predicate " + "must be on the same side of that predicate: t1.id + t2.id = t.int_col " + "+ t1.int_col");
}
#end_block

#method_before
@Test
public void TestSubqueries() throws AnalysisException {
    // Test resolution of column references inside subqueries.
    // Correlated column references can be qualified or unqualified.
    AnalyzesOk("select * from functional.jointbl t where exists " + "(select id from functional.alltypes where id = test_id and id = t.test_id)");
    // Correlated column references are invalid outside of WHERE and ON clauses.
    AnalysisError("select * from functional.jointbl t where exists " + "(select t.test_id = id from functional.alltypes)", "Could not resolve column/field reference: 't.test_id'");
    AnalysisError("select * from functional.jointbl t where exists " + "(select count(*) from functional.alltypes group by t.test_id)", "Could not resolve column/field reference: 't.test_id'");
    AnalysisError("select * from functional.jointbl t where exists " + "(select 1 from functional.alltypes order by t.test_id limit 1)", "Could not resolve column/field reference: 't.test_id'");
    // Test resolution of correlated table references inside subqueries. The testing
    // here is rather basic, because the analysis goes through the same codepath
    // as the analysis of correlated inline views, which are more thoroughly tested.
    AnalyzesOk("select id from functional.allcomplextypes t " + "where exists (select count(*) cnt from t.int_array_col where item > 0)");
    AnalyzesOk("select id from functional.allcomplextypes t " + "where id in (select item cnt from t.int_array_col)");
    AnalyzesOk("select id from functional.allcomplextypes t " + "where id < (select count(*) from t.int_array_col)");
    // Test behavior of aliases in subqueries with correlated table references.
    // Inner reference resolves to the base table, not the implicit parent alias.
    AnalyzesOk("select id from functional.allcomplextypes t " + "where exists (select id from functional.allcomplextypes)");
    AnalyzesOk("select id from functional.allcomplextypes " + "where id in (select id from functional.allcomplextypes)");
    AnalyzesOk("select id from functional.allcomplextypes " + "where id < (select count(1) cnt from allcomplextypes)", createAnalyzer("functional"));
    // Illegal correlated table references.
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where t.int_col = (select count(*) from t)", "Invalid table reference to existing table alias: 't'");
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where t.int_col = (select count(*) from t) and " + "t.string_col in (select string_col from t)", "Invalid table reference to existing table alias: 't'");
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where exists (select * from t, functional.alltypesagg p where " + "t.id = p.id)", "Invalid table reference to existing table alias: 't'");
    AnalysisError("select id from functional.allcomplextypes " + "where exists (select id from allcomplextypes)", "Invalid table reference to existing table alias: 'allcomplextypes'");
    // Un/correlated refs in a single nested query block.
    AnalysisError("select id from functional.allcomplextypes t " + "where exists (select item from functional.alltypes, t.int_array_col)", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM functional.alltypes, t.int_array_col");
    // Correlated table ref has correlated inline view as parent.
    AnalysisError("select id from functional.allcomplextypes t " + "where id in (select item from (select value arr from t.array_map_col) v1, " + "(select item from v1.arr, functional.alltypestiny) v2)", "Nested query is illegal because it contains a table reference " + "'v1.arr' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypestiny':\n" + "SELECT item FROM v1.arr, functional.alltypestiny");
    // EXISTS, IN and aggregate subqueries
    AnalyzesOk("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where a.int_col = " + "t.int_col) and t.bigint_col in (select bigint_col from " + "functional.alltypestiny s) and t.bool_col = false and " + "t.int_col = (select min(int_col) from functional.alltypesagg)");
    // Nested IN with an EXISTS subquery that contains an aggregate subquery
    AnalyzesOk("select count(*) from functional.alltypes t where t.id " + "in (select id from functional.alltypesagg a where a.int_col = " + "t.int_col and exists (select * from functional.alltypestiny s " + "where s.bool_col = a.bool_col and s.int_col = (select min(int_col) " + "from functional.alltypessmall where bigint_col = 10)))");
    // Nested EXISTS with an IN subquery that has a nested aggregate subquery
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where a.id in (select id " + "from functional.alltypestiny s where bool_col = false and " + "s.int_col < (select max(int_col) from functional.alltypessmall where " + "bigint_col < 100)) and a.int_col = t.int_col)");
    // Nested aggregate subqueries with EXISTS and IN subqueries
    AnalyzesOk("select count(*) from functional.alltypes t where t.int_col = " + "(select avg(g.int_col) * 2 from functional.alltypesagg g where g.id in " + "(select id from functional.alltypessmall s where exists (select " + "* from functional.alltypestiny a where a.int_col = s.int_col and " + "a.bigint_col < 10)))");
    // INSERT SELECT
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypes where id in (select id from " + "functional.alltypesagg a where a.bool_col = false)");
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypes t where int_col in (select int_col " + "from functional.alltypesagg a where a.id = t.id) and exists " + "(select * from functional.alltypestiny s where s.bigint_col = " + "t.bigint_col) and int_col < (select min(int_col) from functional.alltypes)");
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypestiny where id = (select 1) " + "union select * from functional.alltypestiny where id = (select 2)");
    // CTAS with correlated subqueries
    AnalyzesOk("create table functional.test_tbl as select * from " + "functional.alltypes t where t.id in (select id from functional.alltypesagg " + "a where a.int_col = t.int_col and a.bool_col = false) and not exists " + "(select * from functional.alltypestiny s where s.int_col = t.int_col) " + "and t.bigint_col = (select count(*) from functional.alltypessmall)");
    AnalyzesOk("create table functional.test_tbl as " + "select * from functional.alltypestiny where id = (select 1) " + "union select * from functional.alltypestiny where id = (select 2)");
    // Predicate with a child subquery in the HAVING clause
    AnalysisError("select id, count(*) from functional.alltypestiny t group by " + "id having count(*) > (select count(*) from functional.alltypesagg)", "Subqueries are not supported in the HAVING clause.");
    AnalysisError("select id, count(*) from functional.alltypestiny t group by " + "id having (select count(*) from functional.alltypesagg) > 10", "Subqueries are not supported in the HAVING clause.");
    // Subquery in the select list
    AnalysisError("select id, (select int_col from functional.alltypestiny) " + "from functional.alltypestiny", "Subqueries are not supported in the select list.");
    // Subquery in the GROUP BY clause
    AnalysisError("select id, count(*) from functional.alltypestiny " + "group by (select int_col from functional.alltypestiny)", "Subqueries are not supported in the GROUP BY clause.");
    // Subquery in the ORDER BY clause
    AnalysisError("select id from functional.alltypestiny " + "order by (select int_col from functional.alltypestiny)", "Subqueries are not supported in the ORDER BY clause.");
    // Subquery with an inline view
    AnalyzesOk("select id from functional.alltypestiny t where exists " + "(select * from (select id, int_col from functional.alltypesagg) a where " + "a.id < 10 and a.int_col = t.int_col)");
    // Subquery referencing a view
    AnalyzesOk("select * from functional.alltypes a where exists " + "(select * from functional.alltypes_view b where b.id = a.id)");
    // Same view referenced in both the inner and outer block
    AnalyzesOk("select * from functional.alltypes_view a where exists " + "(select * from functional.alltypes_view b where a.id = b.id)");
    // Union query with subqueries
    AnalyzesOk("select * from functional.alltypes where id = " + "(select max(id) from functional.alltypestiny) union " + "select * from functional.alltypes where id = " + "(select min(id) from functional.alltypessmall)");
    AnalyzesOk("select * from functional.alltypes where id = (select 1) " + "union all select * from functional.alltypes where id in " + "(select int_col from functional.alltypestiny)");
    AnalyzesOk("select * from functional.alltypes where id = (select 1) " + "union select * from (select * from functional.alltypes where id in " + "(select int_col from functional.alltypestiny)) t");
    // Union in the subquery
    AnalysisError("select * from functional.alltypes where exists " + "(select id from functional.alltypestiny union " + "select id from functional.alltypesagg)", "A subquery must contain a single select block: " + "(SELECT id FROM functional.alltypestiny UNION " + "SELECT id FROM functional.alltypesagg)");
    AnalysisError("select * from functional.alltypes where exists (values(1))", "A subquery must contain a single select block: (VALUES(1))");
    // Subquery in LIMIT
    AnalysisError("select * from functional.alltypes limit " + "(select count(*) from functional.alltypesagg)", "LIMIT expression must be a constant expression: " + "(SELECT count(*) FROM functional.alltypesagg)");
    // NOT predicates in conjunction with subqueries
    AnalyzesOk("select * from functional.alltypes t where t.id not in " + "(select id from functional.alltypesagg g where g.bool_col = false) " + "and t.string_col not like '%1%' and not (t.int_col < 5) " + "and not (t.int_col is null) and not (t.int_col between 5 and 10)");
    // IS NULL with an InPredicate that contains a subquery
    AnalysisError("select * from functional.alltypestiny t where (id in " + "(select id from functional.alltypes)) is null", "Unsupported IS NULL " + "predicate that contains a subquery: (id IN (SELECT id FROM " + "functional.alltypes)) IS NULL");
    // IS NULL with a BinaryPredicate that contains a subquery
    AnalyzesOk("select * from functional.alltypestiny where (id = " + "(select max(id) from functional.alltypessmall)) is null");
    // between predicates with subqueries
    AnalyzesOk("select * from functional.alltypestiny where " + "(select avg(id) from functional.alltypesagg where bool_col = true) " + "between 1 and 100 and int_col < 10");
    AnalyzesOk("select count(*) from functional.alltypestiny t where " + "(select count(id) from functional.alltypesagg g where t.id = g.id " + "and g.bigint_col < 10) between 1 and 1000");
    AnalyzesOk("select id from functional.alltypestiny where " + "int_col between (select min(int_col) from functional.alltypesagg where " + "id < 10) and 100 and bool_col = false");
    AnalyzesOk("select * from functional.alltypessmall s where " + "int_col between (select count(t.id) from functional.alltypestiny t where " + "t.int_col = s.int_col) and (select max(int_col) from " + "functional.alltypes a where a.id = s.id and a.bool_col = false)");
    AnalyzesOk("select * from functional.alltypessmall where " + "int_col between (select min(int_col) from functional.alltypestiny) and " + "(select max(int_col) from functional.alltypestiny) and bigint_col between " + "(select min(bigint_col) from functional.alltypesagg) and (select " + "max(bigint_col) from functional.alltypesagg)");
    AnalysisError("select * from functional.alltypestiny where (select min(id) " + "from functional.alltypes) between 1 and (select max(id) from " + "functional.alltypes)", "Comparison between subqueries is not supported " + "in a between predicate: (SELECT min(id) FROM functional.alltypes) BETWEEN " + "1 AND (SELECT max(id) FROM functional.alltypes)");
    AnalyzesOk("select * from functional.alltypestiny where " + "int_col between 0 and 10 and exists (select 1)");
    AnalyzesOk("select * from functional.alltypestiny a where " + "double_col between cast(1 as double) and cast(10 as double) and " + "exists (select 1 from functional.alltypessmall b where a.id = b.id)");
}
#method_after
@Test
public void TestSubqueries() throws AnalysisException {
    // Test resolution of column references inside subqueries.
    // Correlated column references can be qualified or unqualified.
    AnalyzesOk("select * from functional.jointbl t where exists " + "(select id from functional.alltypes where id = test_id and id = t.test_id)");
    // Correlated column references are invalid outside of WHERE and ON clauses.
    AnalysisError("select * from functional.jointbl t where exists " + "(select t.test_id = id from functional.alltypes)", "Could not resolve column/field reference: 't.test_id'");
    AnalysisError("select * from functional.jointbl t where test_zip in " + "(select count(*) from functional.alltypes group by t.test_id)", "Could not resolve column/field reference: 't.test_id'");
    AnalysisError("select * from functional.jointbl t where exists " + "(select 1 from functional.alltypes order by t.test_id limit 1)", "Could not resolve column/field reference: 't.test_id'");
    // Star exprs cannot reference an alias from a parent block.
    AnalysisError("select * from functional.jointbl t where exists " + "(select t.* from functional.alltypes)", "Could not resolve star expression: 't.*'");
    // Test resolution of correlated table references inside subqueries. The testing
    // here is rather basic, because the analysis goes through the same codepath
    // as the analysis of correlated inline views, which are more thoroughly tested.
    AnalyzesOk("select id from functional.allcomplextypes t " + "where exists (select count(*) cnt from t.int_array_col where item > 0)");
    AnalyzesOk("select id from functional.allcomplextypes t " + "where id in (select item cnt from t.int_array_col)");
    AnalyzesOk("select id from functional.allcomplextypes t " + "where id < (select count(*) from t.int_array_col)");
    // Test behavior of aliases in subqueries with correlated table references.
    // Inner reference resolves to the base table, not the implicit parent alias.
    AnalyzesOk("select id from functional.allcomplextypes t " + "where exists (select id from functional.allcomplextypes)");
    AnalyzesOk("select id from functional.allcomplextypes " + "where id in (select id from functional.allcomplextypes)");
    AnalyzesOk("select id from functional.allcomplextypes " + "where id < (select count(1) cnt from allcomplextypes)", createAnalyzer("functional"));
    // Illegal correlated table references.
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where t.int_col = (select count(*) from t)", "Illegal table reference to non-collection type: 't'");
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where t.int_col = (select count(*) from t) and " + "t.string_col in (select string_col from t)", "Illegal table reference to non-collection type: 't'");
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where exists (select * from t, functional.alltypesagg p where " + "t.id = p.id)", "Illegal table reference to non-collection type: 't'");
    AnalysisError("select id from functional.allcomplextypes " + "where exists (select id from allcomplextypes)", "Illegal table reference to non-collection type: 'allcomplextypes'");
    // Un/correlated refs in a single nested query block.
    AnalysisError("select id from functional.allcomplextypes t " + "where exists (select item from functional.alltypes, t.int_array_col)", "Nested query is illegal because it contains a table reference " + "'t.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM functional.alltypes, t.int_array_col");
    // Correlated table ref has correlated inline view as parent.
    // TOOD: Enable once we support complex-typed exprs in the select list.
    // AnalysisError("select id from functional.allcomplextypes t " +
    // "where id in (select item from (select value arr from t.array_map_col) v1, " +
    // "(select item from v1.arr, functional.alltypestiny) v2)",
    // "Nested query is illegal because it contains a table reference " +
    // "'v1.arr' correlated with an outer block as well as an " +
    // "uncorrelated one 'functional.alltypestiny':\n" +
    // "SELECT item FROM v1.arr, functional.alltypestiny");
    // EXISTS, IN and aggregate subqueries
    AnalyzesOk("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where a.int_col = " + "t.int_col) and t.bigint_col in (select bigint_col from " + "functional.alltypestiny s) and t.bool_col = false and " + "t.int_col = (select min(int_col) from functional.alltypesagg)");
    // Nested IN with an EXISTS subquery that contains an aggregate subquery
    AnalyzesOk("select count(*) from functional.alltypes t where t.id " + "in (select id from functional.alltypesagg a where a.int_col = " + "t.int_col and exists (select * from functional.alltypestiny s " + "where s.bool_col = a.bool_col and s.int_col = (select min(int_col) " + "from functional.alltypessmall where bigint_col = 10)))");
    // Nested EXISTS with an IN subquery that has a nested aggregate subquery
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where a.id in (select id " + "from functional.alltypestiny s where bool_col = false and " + "s.int_col < (select max(int_col) from functional.alltypessmall where " + "bigint_col < 100)) and a.int_col = t.int_col)");
    // Nested aggregate subqueries with EXISTS and IN subqueries
    AnalyzesOk("select count(*) from functional.alltypes t where t.int_col = " + "(select avg(g.int_col) * 2 from functional.alltypesagg g where g.id in " + "(select id from functional.alltypessmall s where exists (select " + "* from functional.alltypestiny a where a.int_col = s.int_col and " + "a.bigint_col < 10)))");
    // INSERT SELECT
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypes where id in (select id from " + "functional.alltypesagg a where a.bool_col = false)");
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypes t where int_col in (select int_col " + "from functional.alltypesagg a where a.id = t.id) and exists " + "(select * from functional.alltypestiny s where s.bigint_col = " + "t.bigint_col) and int_col < (select min(int_col) from functional.alltypes)");
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypestiny where id = (select 1) " + "union select * from functional.alltypestiny where id = (select 2)");
    // CTAS with correlated subqueries
    AnalyzesOk("create table functional.test_tbl as select * from " + "functional.alltypes t where t.id in (select id from functional.alltypesagg " + "a where a.int_col = t.int_col and a.bool_col = false) and not exists " + "(select * from functional.alltypestiny s where s.int_col = t.int_col) " + "and t.bigint_col = (select count(*) from functional.alltypessmall)");
    AnalyzesOk("create table functional.test_tbl as " + "select * from functional.alltypestiny where id = (select 1) " + "union select * from functional.alltypestiny where id = (select 2)");
    // Predicate with a child subquery in the HAVING clause
    AnalysisError("select id, count(*) from functional.alltypestiny t group by " + "id having count(*) > (select count(*) from functional.alltypesagg)", "Subqueries are not supported in the HAVING clause.");
    AnalysisError("select id, count(*) from functional.alltypestiny t group by " + "id having (select count(*) from functional.alltypesagg) > 10", "Subqueries are not supported in the HAVING clause.");
    // Subquery in the select list
    AnalysisError("select id, (select int_col from functional.alltypestiny) " + "from functional.alltypestiny", "Subqueries are not supported in the select list.");
    // Subquery in the GROUP BY clause
    AnalysisError("select id, count(*) from functional.alltypestiny " + "group by (select int_col from functional.alltypestiny)", "Subqueries are not supported in the GROUP BY clause.");
    // Subquery in the ORDER BY clause
    AnalysisError("select id from functional.alltypestiny " + "order by (select int_col from functional.alltypestiny)", "Subqueries are not supported in the ORDER BY clause.");
    // Subquery with an inline view
    AnalyzesOk("select id from functional.alltypestiny t where exists " + "(select * from (select id, int_col from functional.alltypesagg) a where " + "a.id < 10 and a.int_col = t.int_col)");
    // Subquery referencing a view
    AnalyzesOk("select * from functional.alltypes a where exists " + "(select * from functional.alltypes_view b where b.id = a.id)");
    // Same view referenced in both the inner and outer block
    AnalyzesOk("select * from functional.alltypes_view a where exists " + "(select * from functional.alltypes_view b where a.id = b.id)");
    // Subquery with collection table ref.
    AnalyzesOk("select int_col from functional.alltypes where int_col < " + "(select count(a.item) from functional.allcomplextypes t, t.int_array_col a)");
    // Union query with subqueries
    AnalyzesOk("select * from functional.alltypes where id = " + "(select max(id) from functional.alltypestiny) union " + "select * from functional.alltypes where id = " + "(select min(id) from functional.alltypessmall)");
    AnalyzesOk("select * from functional.alltypes where id = (select 1) " + "union all select * from functional.alltypes where id in " + "(select int_col from functional.alltypestiny)");
    AnalyzesOk("select * from functional.alltypes where id = (select 1) " + "union select * from (select * from functional.alltypes where id in " + "(select int_col from functional.alltypestiny)) t");
    // Union in the subquery
    AnalysisError("select * from functional.alltypes where exists " + "(select id from functional.alltypestiny union " + "select id from functional.alltypesagg)", "A subquery must contain a single select block: " + "(SELECT id FROM functional.alltypestiny UNION " + "SELECT id FROM functional.alltypesagg)");
    AnalysisError("select * from functional.alltypes where exists (values(1))", "A subquery must contain a single select block: (VALUES(1))");
    // Subquery in LIMIT
    AnalysisError("select * from functional.alltypes limit " + "(select count(*) from functional.alltypesagg)", "LIMIT expression must be a constant expression: " + "(SELECT count(*) FROM functional.alltypesagg)");
    // NOT predicates in conjunction with subqueries
    AnalyzesOk("select * from functional.alltypes t where t.id not in " + "(select id from functional.alltypesagg g where g.bool_col = false) " + "and t.string_col not like '%1%' and not (t.int_col < 5) " + "and not (t.int_col is null) and not (t.int_col between 5 and 10)");
    // IS NULL with an InPredicate that contains a subquery
    AnalysisError("select * from functional.alltypestiny t where (id in " + "(select id from functional.alltypes)) is null", "Unsupported IS NULL " + "predicate that contains a subquery: (id IN (SELECT id FROM " + "functional.alltypes)) IS NULL");
    // IS NULL with a BinaryPredicate that contains a subquery
    AnalyzesOk("select * from functional.alltypestiny where (id = " + "(select max(id) from functional.alltypessmall)) is null");
    // between predicates with subqueries
    AnalyzesOk("select * from functional.alltypestiny where " + "(select avg(id) from functional.alltypesagg where bool_col = true) " + "between 1 and 100 and int_col < 10");
    AnalyzesOk("select count(*) from functional.alltypestiny t where " + "(select count(id) from functional.alltypesagg g where t.id = g.id " + "and g.bigint_col < 10) between 1 and 1000");
    AnalyzesOk("select id from functional.alltypestiny where " + "int_col between (select min(int_col) from functional.alltypesagg where " + "id < 10) and 100 and bool_col = false");
    AnalyzesOk("select * from functional.alltypessmall s where " + "int_col between (select count(t.id) from functional.alltypestiny t where " + "t.int_col = s.int_col) and (select max(int_col) from " + "functional.alltypes a where a.id = s.id and a.bool_col = false)");
    AnalyzesOk("select * from functional.alltypessmall where " + "int_col between (select min(int_col) from functional.alltypestiny) and " + "(select max(int_col) from functional.alltypestiny) and bigint_col between " + "(select min(bigint_col) from functional.alltypesagg) and (select " + "max(bigint_col) from functional.alltypesagg)");
    AnalysisError("select * from functional.alltypestiny where (select min(id) " + "from functional.alltypes) between 1 and (select max(id) from " + "functional.alltypes)", "Comparison between subqueries is not supported " + "in a between predicate: (SELECT min(id) FROM functional.alltypes) BETWEEN " + "1 AND (SELECT max(id) FROM functional.alltypes)");
    AnalyzesOk("select * from functional.alltypestiny where " + "int_col between 0 and 10 and exists (select 1)");
    AnalyzesOk("select * from functional.alltypestiny a where " + "double_col between cast(1 as double) and cast(10 as double) and " + "exists (select 1 from functional.alltypessmall b where a.id = b.id)");
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    analyzer.analyzeNestedStmt(queryStmt_, inlineViewAnalyzer_);
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // create smap_ and baseTblSmap_ and register auxiliary eq predicates between our
    // tuple descriptor's slots and our *unresolved* select list exprs;
    // we create these auxiliary predicates so that the analyzer can compute the value
    // transfer graph through this inline view correctly (ie, predicates can get
    // propagated through the view);
    // if the view stmt contains analytic functions, we cannot propagate predicates
    // into the view, because those extra filters would alter the results of the
    // analytic functions (see IMPALA-1243)
    // TODO: relax this a bit by allowing propagation out of the inline view (but
    // not into it)
    boolean createAuxPredicates = !(queryStmt_ instanceof SelectStmt) || !(((SelectStmt) queryStmt_).hasAnalyticInfo());
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i);
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        Path p = Path.resolve(desc_, Lists.newArrayList(colName), 0);
        Preconditions.checkNotNull(p);
        SlotDescriptor slotDesc = analyzer.registerSlotRef(p);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicates) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getUniqueAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getUniqueAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    queryStmt_.checkCorrelatedTableRefs(inlineViewAnalyzer_);
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // not into it)
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i).toLowerCase();
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        Path p = new Path(desc_, Lists.newArrayList(colName));
        Preconditions.checkState(p.resolve());
        SlotDescriptor slotDesc = analyzer.registerSlotRef(p);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicate(colExpr)) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getUniqueAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getUniqueAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#end_block

#method_before
@Override
public TupleDescriptor createTupleDescriptor(Analyzer analyzer) throws AnalysisException {
    int numColLabels = getColLabels().size();
    Preconditions.checkState(numColLabels > 0);
    HashSet<String> uniqueColAliases = Sets.newHashSetWithExpectedSize(numColLabels);
    ArrayList<StructField> fields = Lists.newArrayListWithCapacity(numColLabels);
    for (int i = 0; i < numColLabels; ++i) {
        // inline view select statement has been analyzed. Col label should be filled.
        Expr selectItemExpr = queryStmt_.getResultExprs().get(i);
        String colAlias = getColLabels().get(i).toLowerCase();
        // inline view col cannot have duplicate name
        if (!uniqueColAliases.add(colAlias)) {
            throw new AnalysisException("duplicated inline view column alias: '" + colAlias + "'" + " in inline view " + "'" + getUniqueAlias() + "'");
        }
        StructField field = new StructField(colAlias, selectItemExpr.getType(), null);
        field.setPosition(i);
        fields.add(field);
    }
    // Create the non-materialized tuple and set the fake table in it.
    TupleDescriptor result = analyzer.getDescTbl().createTupleDescriptor("inl-view-" + getUniqueAlias());
    result.setIsMaterialized(false);
    result.setType(new ArrayType(new StructType(fields)));
    return result;
}
#method_after
@Override
public TupleDescriptor createTupleDescriptor(Analyzer analyzer) throws AnalysisException {
    int numColLabels = getColLabels().size();
    Preconditions.checkState(numColLabels > 0);
    HashSet<String> uniqueColAliases = Sets.newHashSetWithExpectedSize(numColLabels);
    ArrayList<StructField> fields = Lists.newArrayListWithCapacity(numColLabels);
    for (int i = 0; i < numColLabels; ++i) {
        // inline view select statement has been analyzed. Col label should be filled.
        Expr selectItemExpr = queryStmt_.getResultExprs().get(i);
        String colAlias = getColLabels().get(i).toLowerCase();
        // inline view col cannot have duplicate name
        if (!uniqueColAliases.add(colAlias)) {
            throw new AnalysisException("duplicated inline view column alias: '" + colAlias + "'" + " in inline view " + "'" + getUniqueAlias() + "'");
        }
        fields.add(new StructField(colAlias, selectItemExpr.getType(), null));
    }
    // Create the non-materialized tuple and set its type.
    TupleDescriptor result = analyzer.getDescTbl().createTupleDescriptor(getClass().getSimpleName() + " " + getUniqueAlias());
    result.setIsMaterialized(false);
    result.setType(new StructType(fields));
    return result;
}
#end_block

#method_before
@Override
public TableRef clone() {
    return new InlineViewRef(this);
}
#method_after
@Override
protected TableRef clone() {
    return new InlineViewRef(this);
}
#end_block

#method_before
@Test
public void TestAlterView() {
    // View-definition references a table.
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypesagg");
    // View-definition references a view.
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypes_view");
    // View-definition resulting in Hive-style auto-generated column names.
    AnalyzesOk("alter view functional.alltypes_view as " + "select trim('abc'), 17 * 7");
    // Cannot ALTER VIEW a table.
    AnalysisError("alter view functional.alltypes as " + "select * from functional.alltypesagg", "ALTER VIEW not allowed on a table: functional.alltypes");
    AnalysisError("alter view functional_hbase.alltypesagg as " + "select * from functional.alltypesagg", "ALTER VIEW not allowed on a table: functional_hbase.alltypesagg");
    // Target database does not exist.
    AnalysisError("alter view baddb.alltypes_view as " + "select * from functional.alltypesagg", "Database does not exist: baddb");
    // Target view does not exist.
    AnalysisError("alter view functional.badview as " + "select * from functional.alltypesagg", "Table does not exist: functional.badview");
    // View-definition statement fails to analyze. Database does not exist.
    AnalysisError("alter view functional.alltypes_view as " + "select * from baddb.alltypesagg", "Could not resolve table reference: 'baddb.alltypesagg'");
    // View-definition statement fails to analyze. Table does not exist.
    AnalysisError("alter view functional.alltypes_view as " + "select * from functional.badtable", "Could not resolve table reference: 'functional.badtable'");
    // Duplicate column name.
    AnalysisError("alter view functional.alltypes_view as " + "select * from functional.alltypessmall a inner join " + "functional.alltypessmall b on a.id = b.id", "Duplicate column name: id");
    // Invalid column name.
    AnalysisError("alter view functional.alltypes_view as select 'abc' as `???`", "Invalid column/field name: ???");
}
#method_after
@Test
public void TestAlterView() {
    // View-definition references a table.
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypesagg");
    // View-definition references a view.
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypes_view");
    // View-definition resulting in Hive-style auto-generated column names.
    AnalyzesOk("alter view functional.alltypes_view as " + "select trim('abc'), 17 * 7");
    // Cannot ALTER VIEW a table.
    AnalysisError("alter view functional.alltypes as " + "select * from functional.alltypesagg", "ALTER VIEW not allowed on a table: functional.alltypes");
    AnalysisError("alter view functional_hbase.alltypesagg as " + "select * from functional.alltypesagg", "ALTER VIEW not allowed on a table: functional_hbase.alltypesagg");
    // Target database does not exist.
    AnalysisError("alter view baddb.alltypes_view as " + "select * from functional.alltypesagg", "Database does not exist: baddb");
    // Target view does not exist.
    AnalysisError("alter view functional.badview as " + "select * from functional.alltypesagg", "Table does not exist: functional.badview");
    // View-definition statement fails to analyze. Database does not exist.
    AnalysisError("alter view functional.alltypes_view as " + "select * from baddb.alltypesagg", "Could not resolve table reference: 'baddb.alltypesagg'");
    // View-definition statement fails to analyze. Table does not exist.
    AnalysisError("alter view functional.alltypes_view as " + "select * from functional.badtable", "Could not resolve table reference: 'functional.badtable'");
    // Duplicate column name.
    AnalysisError("alter view functional.alltypes_view as " + "select * from functional.alltypessmall a inner join " + "functional.alltypessmall b on a.id = b.id", "Duplicate column name: id");
    // Invalid column name.
    AnalysisError("alter view functional.alltypes_view as select 'abc' as `???`", "Invalid column/field name: ???");
    // Change the view definition to contain a subquery (IMPALA-1797)
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypestiny where id in " + "(select id from functional.alltypessmall where int_col = 1)");
}
#end_block

#method_before
@Test
public void TestComputeStats() throws AnalysisException {
    // Analyze the stmt itself as well as the generated child queries.
    checkComputeStatsStmt("compute stats functional.alltypes");
    checkComputeStatsStmt("compute stats functional_hbase.alltypes");
    // Test that complex-typed columns are ignored.
    checkComputeStatsStmt("compute stats functional.allcomplextypes");
    // Cannot compute stats on a database.
    AnalysisError("compute stats tbl_does_not_exist", "Table does not exist: default.tbl_does_not_exist");
    // Cannot compute stats on a view.
    AnalysisError("compute stats functional.alltypes_view", "COMPUTE STATS not supported for view functional.alltypes_view");
    AnalyzesOk("compute stats functional_avro_snap.alltypes");
    // Test mismatched column definitions and Avro schema (HIVE-6308, IMPALA-867).
    // See testdata/avro_schema_resolution/create_table.sql for the CREATE TABLE stmts.
    // Mismatched column type is ok because the conflict is resolved in favor of
    // the type in the column definition list in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_type_mismatch");
    // Missing column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_missing_coldef");
    // Extra column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_extra_coldef");
    // Mismatched column name (tables were created by Hive).
    AnalysisError("compute stats functional_avro_snap.schema_resolution_test", "Cannot COMPUTE STATS on Avro table 'schema_resolution_test' because its " + "column definitions do not match those in the Avro schema.\nDefinition of " + "column 'col1' of type 'string' does not match the Avro-schema column " + "'boolean1' of type 'BOOLEAN' at position '0'.\nPlease re-create the table " + "with column definitions, e.g., using the result of 'SHOW CREATE TABLE'");
}
#method_after
@Test
public void TestComputeStats() throws AnalysisException {
    // Analyze the stmt itself as well as the generated child queries.
    checkComputeStatsStmt("compute stats functional.alltypes");
    checkComputeStatsStmt("compute stats functional_hbase.alltypes");
    // Test that complex-typed columns are ignored.
    checkComputeStatsStmt("compute stats functional.allcomplextypes");
    // Cannot compute stats on a database.
    AnalysisError("compute stats tbl_does_not_exist", "Table does not exist: default.tbl_does_not_exist");
    // Cannot compute stats on a view.
    AnalysisError("compute stats functional.alltypes_view", "COMPUTE STATS not supported for view functional.alltypes_view");
    AnalyzesOk("compute stats functional_avro_snap.alltypes");
    // Test mismatched column definitions and Avro schema (HIVE-6308, IMPALA-867).
    // See testdata/avro_schema_resolution/create_table.sql for the CREATE TABLE stmts.
    // Mismatched column type is ok because the conflict is resolved in favor of
    // the type in the column definition list in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_type_mismatch");
    // Missing column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_missing_coldef");
    // Extra column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_extra_coldef");
    // No column definitions are ok.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_no_coldef");
    // Mismatched column name (table was created by Hive).
    AnalysisError("compute stats functional_avro_snap.schema_resolution_test", "Cannot COMPUTE STATS on Avro table 'schema_resolution_test' because its " + "column definitions do not match those in the Avro schema.\nDefinition of " + "column 'col1' of type 'string' does not match the Avro-schema column " + "'boolean1' of type 'BOOLEAN' at position '0'.\nPlease re-create the table " + "with column definitions, e.g., using the result of 'SHOW CREATE TABLE'");
}
#end_block

#method_before
@Test
public void TestCreateTableAsSelect() throws AnalysisException {
    // Constant select.
    AnalyzesOk("create table newtbl as select 1+2, 'abc'");
    // Select from partitioned and unpartitioned tables using different
    // queries.
    AnalyzesOk("create table newtbl stored as textfile " + "as select * from functional.jointbl");
    AnalyzesOk("create table newtbl stored as parquetfile " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl stored as parquet " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl as select int_col from functional.alltypes");
    AnalyzesOk("create table functional.newtbl " + "as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.tbl as select a.* from functional.alltypes a " + "join functional.alltypes b on (a.int_col=b.int_col) limit 1000");
    // Caching operations
    AnalyzesOk("create table functional.newtbl cached in 'testPool'" + " as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.newtbl uncached" + " as select count(*) as CNT from functional.alltypes");
    // Table already exists with and without IF NOT EXISTS
    AnalysisError("create table functional.alltypes as select 1", "Table already exists: functional.alltypes");
    AnalyzesOk("create table if not exists functional.alltypes as select 1");
    // Database does not exist
    AnalysisError("create table db_does_not_exist.new_table as select 1", "Database does not exist: db_does_not_exist");
    // Analysis errors in the SELECT statement
    AnalysisError("create table newtbl as select * from tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    AnalysisError("create table newtbl as select 1 as c1, 2 as c1", "Duplicate column name: c1");
    // TODO: Enable this once we can execute it.
    AnalysisError("create table newtbl as select * from functional.allcomplextypes", "Target table 'default.newtbl' is incompatible with " + "SELECT / PARTITION expressions.\n" + "Expression 'functional.allcomplextypes.int_array_col' (type: ARRAY<INT>) is " + "not compatible with column 'int_array_col' (type: ARRAY<INT>)");
    // Unsupported file formats
    AnalysisError("create table foo stored as sequencefile as select 1", "CREATE TABLE AS SELECT does not support (SEQUENCEFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE)");
    AnalysisError("create table foo stored as RCFILE as select 1", "CREATE TABLE AS SELECT does not support (RCFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE)");
    // CTAS with a WITH clause and inline view (IMPALA-1100)
    AnalyzesOk("create table test_with as with with_1 as (select 1 as int_col from " + "functional.alltypes as t1 right join (select 1 as int_col from " + "functional.alltypestiny as t1) as t2 on t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
    // CTAS with a correlated inline view.
    AnalyzesOk("create table test as select id, item " + "from functional.allcomplextypes b, (select item from b.int_array_col) v1");
    // Correlated inline view in WITH clause.
    AnalyzesOk("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col) v1) select * from w");
    // CTAS with illegal correlated inline views.
    AnalysisError("create table test as select id, item " + "from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    AnalysisError("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1) select * from w", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
}
#method_after
@Test
public void TestCreateTableAsSelect() throws AnalysisException {
    // Constant select.
    AnalyzesOk("create table newtbl as select 1+2, 'abc'");
    // Select from partitioned and unpartitioned tables using different
    // queries.
    AnalyzesOk("create table newtbl stored as textfile " + "as select * from functional.jointbl");
    AnalyzesOk("create table newtbl stored as parquetfile " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl stored as parquet " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl as select int_col from functional.alltypes");
    AnalyzesOk("create table functional.newtbl " + "as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.tbl as select a.* from functional.alltypes a " + "join functional.alltypes b on (a.int_col=b.int_col) limit 1000");
    // Caching operations
    AnalyzesOk("create table functional.newtbl cached in 'testPool'" + " as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.newtbl uncached" + " as select count(*) as CNT from functional.alltypes");
    // Table already exists with and without IF NOT EXISTS
    AnalysisError("create table functional.alltypes as select 1", "Table already exists: functional.alltypes");
    AnalyzesOk("create table if not exists functional.alltypes as select 1");
    // Database does not exist
    AnalysisError("create table db_does_not_exist.new_table as select 1", "Database does not exist: db_does_not_exist");
    // Analysis errors in the SELECT statement
    AnalysisError("create table newtbl as select * from tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    AnalysisError("create table newtbl as select 1 as c1, 2 as c1", "Duplicate column name: c1");
    // Unsupported file formats
    AnalysisError("create table foo stored as sequencefile as select 1", "CREATE TABLE AS SELECT does not support (SEQUENCEFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE)");
    AnalysisError("create table foo stored as RCFILE as select 1", "CREATE TABLE AS SELECT does not support (RCFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE)");
    // CTAS with a WITH clause and inline view (IMPALA-1100)
    AnalyzesOk("create table test_with as with with_1 as (select 1 as int_col from " + "functional.alltypes as t1 right join (select 1 as int_col from " + "functional.alltypestiny as t1) as t2 on t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
    // CTAS with a correlated inline view.
    AnalyzesOk("create table test as select id, item " + "from functional.allcomplextypes b, (select item from b.int_array_col) v1");
    // Correlated inline view in WITH clause.
    AnalyzesOk("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col) v1) select * from w");
    // CTAS with illegal correlated inline views.
    AnalysisError("create table test as select id, item " + "from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    AnalysisError("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1) select * from w", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
}
#end_block

#method_before
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38.");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0. Size is too small: 0.");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355. Size is too large: 65356.");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0. Size is too small: 0.");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255. Size is too large: 256.");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    for (String format : fileFormats) {
        AnalyzesOk(String.format("create table new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", format));
        // No column definitions.
        AnalysisError(String.format("create table new_table " + "partitioned by (d decimal) comment 'c' stored as %s", format), "Table requires at least 1 column");
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: I");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d date)", "Type 'DATE' is not supported as partition-column type in column: d");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d datetime)", "Type 'DATETIME' is not supported as partition-column type in column: d");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#method_after
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    for (String format : fileFormats) {
        AnalyzesOk(String.format("create table new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", format));
        // No column definitions.
        AnalysisError(String.format("create table new_table " + "partitioned by (d decimal) comment 'c' stored as %s", format), "Table requires at least 1 column");
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: I");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d date)", "Type 'DATE' is not supported as partition-column type in column: d");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d datetime)", "Type 'DATETIME' is not supported as partition-column type in column: d");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#end_block

#method_before
@Test
public void TestCreateView() throws AnalysisException {
    AnalyzesOk("create view foo_new as select int_col, string_col from functional.alltypes");
    AnalyzesOk("create view functional.foo as select * from functional.alltypes");
    AnalyzesOk("create view if not exists foo as select * from functional.alltypes");
    AnalyzesOk("create view foo (a, b) as select int_col, string_col " + "from functional.alltypes");
    AnalyzesOk("create view functional.foo (a, b) as select int_col x, double_col y " + "from functional.alltypes");
    // View can have complex-typed columns.
    AnalyzesOk("create view functional.foo (a, b, c) as " + "select int_array_col, int_map_col, int_struct_col " + "from functional.allcomplextypes");
    // Creating a view on a view is ok (alltypes_view is a view on alltypes).
    AnalyzesOk("create view foo as select * from functional.alltypes_view");
    AnalyzesOk("create view foo (aaa, bbb) as select * from functional.complex_view");
    // Create a view resulting in Hive-style auto-generated column names.
    AnalyzesOk("create view foo as select trim('abc'), 17 * 7");
    // Creating a view on an HBase table is ok.
    AnalyzesOk("create view foo as select * from functional_hbase.alltypesagg");
    // Complex view definition with joins and aggregates.
    AnalyzesOk("create view foo (cnt) as " + "select count(distinct x.int_col) from functional.alltypessmall x " + "inner join functional.alltypessmall y on (x.id = y.id) group by x.bigint_col");
    // Test different query-statement types as view definition.
    AnalyzesOk("create view foo (a, b) as values(1, 'a'), (2, 'b')");
    AnalyzesOk("create view foo (a, b) as select 1, 'a' union all select 2, 'b'");
    // Mismatching number of columns in column definition and view-definition statement.
    AnalysisError("create view foo (a) as select int_col, string_col " + "from functional.alltypes", "Column-definition list has fewer columns (1) than the " + "view-definition query statement returns (2).");
    AnalysisError("create view foo (a, b, c) as select int_col " + "from functional.alltypes", "Column-definition list has more columns (3) than the " + "view-definition query statement returns (1).");
    // Duplicate columns in the view-definition statement.
    AnalysisError("create view foo as select * from functional.alltypessmall a " + "inner join functional.alltypessmall b on a.id = b.id", "Duplicate column name: id");
    // Duplicate columns in the column definition.
    AnalysisError("create view foo (a, b, a) as select int_col, int_col, int_col " + "from functional.alltypes", "Duplicate column name: a");
    // Invalid database/view/column names.
    AnalysisError("create view `???`.new_view as select 1, 2, 3", "Invalid database name: ???");
    AnalysisError("create view `^%&` as select 1, 2, 3", "Invalid table/view name: ^%&");
    AnalysisError("create view foo as select 1 as `???`", "Invalid column/field name: ???");
    AnalysisError("create view foo(`%^&`) as select 1", "Invalid column/field name: %^&");
    // Table/view already exists.
    AnalysisError("create view functional.alltypes as " + "select * from functional.alltypessmall ", "Table already exists: functional.alltypes");
    // Target database does not exist.
    AnalysisError("create view wrongdb.test as " + "select * from functional.alltypessmall ", "Database does not exist: wrongdb");
    // Source database does not exist,
    AnalysisError("create view foo as " + "select * from wrongdb.alltypessmall ", "Could not resolve table reference: 'wrongdb.alltypessmall'");
    // Source table does not exist,
    AnalysisError("create view foo as " + "select * from wrongdb.alltypessmall ", "Could not resolve table reference: 'wrongdb.alltypessmall'");
    // Analysis error in view-definition statement.
    AnalysisError("create view foo as " + "select int_col from functional.alltypessmall union all " + "select string_col from functional.alltypes", "Incompatible return types 'INT' and 'STRING' of exprs " + "'int_col' and 'string_col'.");
    // View with a subquery
    AnalyzesOk("create view test_view_with_subquery as " + "select * from functional.alltypestiny t where exists " + "(select * from functional.alltypessmall s where s.id = t.id)");
}
#method_after
@Test
public void TestCreateView() throws AnalysisException {
    AnalyzesOk("create view foo_new as select int_col, string_col from functional.alltypes");
    AnalyzesOk("create view functional.foo as select * from functional.alltypes");
    AnalyzesOk("create view if not exists foo as select * from functional.alltypes");
    AnalyzesOk("create view foo (a, b) as select int_col, string_col " + "from functional.alltypes");
    AnalyzesOk("create view functional.foo (a, b) as select int_col x, double_col y " + "from functional.alltypes");
    // Creating a view on a view is ok (alltypes_view is a view on alltypes).
    AnalyzesOk("create view foo as select * from functional.alltypes_view");
    AnalyzesOk("create view foo (aaa, bbb) as select * from functional.complex_view");
    // Create a view resulting in Hive-style auto-generated column names.
    AnalyzesOk("create view foo as select trim('abc'), 17 * 7");
    // Creating a view on an HBase table is ok.
    AnalyzesOk("create view foo as select * from functional_hbase.alltypesagg");
    // Complex view definition with joins and aggregates.
    AnalyzesOk("create view foo (cnt) as " + "select count(distinct x.int_col) from functional.alltypessmall x " + "inner join functional.alltypessmall y on (x.id = y.id) group by x.bigint_col");
    // Test different query-statement types as view definition.
    AnalyzesOk("create view foo (a, b) as values(1, 'a'), (2, 'b')");
    AnalyzesOk("create view foo (a, b) as select 1, 'a' union all select 2, 'b'");
    // View with a subquery
    AnalyzesOk("create view test_view_with_subquery as " + "select * from functional.alltypestiny t where exists " + "(select * from functional.alltypessmall s where s.id = t.id)");
    // Mismatching number of columns in column definition and view-definition statement.
    AnalysisError("create view foo (a) as select int_col, string_col " + "from functional.alltypes", "Column-definition list has fewer columns (1) than the " + "view-definition query statement returns (2).");
    AnalysisError("create view foo (a, b, c) as select int_col " + "from functional.alltypes", "Column-definition list has more columns (3) than the " + "view-definition query statement returns (1).");
    // Duplicate columns in the view-definition statement.
    AnalysisError("create view foo as select * from functional.alltypessmall a " + "inner join functional.alltypessmall b on a.id = b.id", "Duplicate column name: id");
    // Duplicate columns in the column definition.
    AnalysisError("create view foo (a, b, a) as select int_col, int_col, int_col " + "from functional.alltypes", "Duplicate column name: a");
    // Invalid database/view/column names.
    AnalysisError("create view `???`.new_view as select 1, 2, 3", "Invalid database name: ???");
    AnalysisError("create view `^%&` as select 1, 2, 3", "Invalid table/view name: ^%&");
    AnalysisError("create view foo as select 1 as `???`", "Invalid column/field name: ???");
    AnalysisError("create view foo(`%^&`) as select 1", "Invalid column/field name: %^&");
    // Table/view already exists.
    AnalysisError("create view functional.alltypes as " + "select * from functional.alltypessmall ", "Table already exists: functional.alltypes");
    // Target database does not exist.
    AnalysisError("create view wrongdb.test as " + "select * from functional.alltypessmall ", "Database does not exist: wrongdb");
    // Source database does not exist,
    AnalysisError("create view foo as " + "select * from wrongdb.alltypessmall ", "Could not resolve table reference: 'wrongdb.alltypessmall'");
    // Source table does not exist,
    AnalysisError("create view foo as " + "select * from wrongdb.alltypessmall ", "Could not resolve table reference: 'wrongdb.alltypessmall'");
    // Analysis error in view-definition statement.
    AnalysisError("create view foo as " + "select int_col from functional.alltypessmall union all " + "select string_col from functional.alltypes", "Incompatible return types 'INT' and 'STRING' of exprs " + "'int_col' and 'string_col'.");
    // View cannot have complex-typed columns because complex-typed exprs are
    // not supported in the select list.
    AnalysisError("create view functional.foo (a, b, c) as " + "select int_array_col, int_map_col, int_struct_col " + "from functional.allcomplextypes", "Expr 'int_array_col' in select list returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
}
#end_block

#method_before
@Test
public void TestUdf() throws AnalysisException {
    final String symbol = "'_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'";
    final String udfSuffix = " LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=" + symbol;
    final String udfSuffixIr = " LOCATION '/test-warehouse/test-udfs.ll' " + "SYMBOL=" + symbol;
    final String hdfsPath = "hdfs://localhost:20500/test-warehouse/libTestUdfs.so";
    AnalyzesOk("create function foo() RETURNS int" + udfSuffix);
    AnalyzesOk("create function foo(int, int, string) RETURNS int" + udfSuffix);
    // Try some fully qualified function names
    AnalyzesOk("create function functional.B() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.B1() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.`B1C`() RETURNS int" + udfSuffix);
    // Name with underscore
    AnalyzesOk("create function A_B() RETURNS int" + udfSuffix);
    // Locations for all the udfs types.
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.so' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'", "Could not load binary: /test-warehouse/libTestUdfs.ll");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo(int) RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' SYMBOL='Identity'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.SO' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/hive-exec.jar' SYMBOL='a'");
    // Test hive UDFs for unsupported types
    AnalysisError("create function foo() RETURNS timestamp LOCATION '/a.jar'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo(timestamp) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo() RETURNS decimal LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(Decimal) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(char(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(varchar(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5) LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5) LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5)" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5)" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo(CHAR(5)) RETURNS int" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(VARCHAR(5)) RETURNS int" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalyzesOk("create function foo() RETURNS decimal" + udfSuffix);
    AnalyzesOk("create function foo() RETURNS decimal(38,10)" + udfSuffix);
    AnalyzesOk("create function foo(Decimal, decimal(10, 2)) RETURNS int" + udfSuffix);
    AnalysisError("create function foo() RETURNS decimal(100)" + udfSuffix, "Decimal precision must be <= 38.");
    AnalysisError("create function foo(Decimal(2, 3)) RETURNS int" + udfSuffix, "Decimal scale (3) must be <= precision (2).");
    // Varargs
    AnalyzesOk("create function foo(INT...) RETURNS int" + udfSuffix);
    // Prepare/Close functions
    AnalyzesOk("create function foo() returns int" + udfSuffix + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='_Z19ValidateOpenPreparePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'" + " close_fn='_Z17ValidateOpenClosePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " close_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn='FakePrepare'", "Could not find function FakePrepare(impala_udf::FunctionContext*, " + "impala_udf::FunctionContext::FunctionStateScope) in: ");
    // Try to create a function with the same name as a builtin
    AnalysisError("create function sin(double) RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    AnalysisError("create function sin() RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    // Try to create with a bad location
    AnalysisError("create function foo() RETURNS int LOCATION 'bad-location' SYMBOL='c'", "URI path must be absolute: bad-location");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'blah://localhost:50200/bad-location' SYMBOL='c'", "No FileSystem for scheme: blah");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'file:///foo.jar' SYMBOL='c'", "Could not load binary: file:///foo.jar");
    // Try creating udfs with unknown extensions
    AnalysisError("create function foo() RETURNS int LOCATION '/binary' SYMBOL='a'", "Unknown binary type: '/binary'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.a' SYMBOL='a'", "Unknown binary type: '/binary.a'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so.' SYMBOL='a'", "Unknown binary type: '/binary.so.'. Binary must end in .jar, .so or .ll");
    // Try with missing symbol
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so'", "Argument 'SYMBOL' must be set.");
    // Try with symbols missing in binary and symbols
    AnalysisError("create function foo() RETURNS int LOCATION '/blah.so' " + "SYMBOL='ab'", "Could not load binary: /blah.so");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.JAR' SYMBOL='a'", "Could not load binary: /binary.JAR");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='b'", "Could not find function b() in: " + hdfsPath);
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=''", "Could not find symbol ''");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='_ZAB'", "Could not find symbol '_ZAB' in: " + hdfsPath);
    // Infer the fully mangled symbol from the signature
    AnalyzesOk("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // We can't get the return type so any of those will match
    AnalyzesOk("create function foo() RETURNS double " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // The part the user specifies is case sensitive
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='noArgs'", "Could not find function noArgs() in: " + hdfsPath);
    // Types no longer match
    AnalysisError("create function foo(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'", "Could not find function NoArgs(INT) in: " + hdfsPath);
    // Check we can match identity for all types
    AnalyzesOk("create function identity(boolean) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(tinyint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(smallint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(bigint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(float) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(double) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(string) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function all_types_fn(string, boolean, tinyint, " + "smallint, int, bigint, float, double, decimal) returns int " + "location '/test-warehouse/libTestUdfs.so' symbol='AllTypes'");
    // Try creating functions with illegal function names.
    AnalysisError("create function 123A() RETURNS int" + udfSuffix, "Function cannot start with a digit: 123a");
    AnalysisError("create function A.`1A`() RETURNS int" + udfSuffix, "Function cannot start with a digit: 1a");
    AnalysisError("create function A.`ABC-D`() RETURNS int" + udfSuffix, "Function names must be all alphanumeric or underscore. Invalid name: abc-d");
    AnalysisError("create function baddb.f() RETURNS int" + udfSuffix, "Database does not exist: baddb");
    AnalysisError("create function a.b.c() RETURNS int" + udfSuffix, "Invalid function name: 'a.b.c'. Expected [dbname].funcname.");
    AnalysisError("create function a.b.c.d(smallint) RETURNS int" + udfSuffix, "Invalid function name: 'a.b.c.d'. Expected [dbname].funcname.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try dropping functions.
    AnalyzesOk("drop function if exists foo()");
    AnalysisError("drop function foo()", "Function does not exist: foo()");
    AnalyzesOk("drop function if exists a.foo()");
    AnalysisError("drop function a.foo()", "Database does not exist: a");
    AnalyzesOk("drop function if exists foo()");
    AnalyzesOk("drop function if exists foo(int...)");
    AnalyzesOk("drop function if exists foo(double, int...)");
    // Add functions default.TestFn(), default.TestFn(double), default.TestFn(String...),
    addTestFunction("TestFn", new ArrayList<ScalarType>(), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.DOUBLE), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.STRING), true);
    AnalysisError("create function TestFn() RETURNS INT " + udfSuffix, "Function already exists: testfn()");
    AnalysisError("create function TestFn(double) RETURNS INT " + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Fn(Double) and Fn(Double...) should be a conflict.
    AnalysisError("create function TestFn(double...) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    AnalysisError("create function TestFn(double) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Add default.TestFn(int, int)
    addTestFunction("TestFn", Lists.newArrayList(Type.INT, Type.INT), false);
    AnalyzesOk("drop function TestFn(int, int)");
    AnalysisError("drop function TestFn(int, int, int)", "Function does not exist: testfn(INT, INT, INT)");
    // Fn(String...) was already added.
    AnalysisError("create function TestFn(String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String...) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String, String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalyzesOk("create function TestFn(String, String, Int) RETURNS INT" + udfSuffix);
    // Check function overloading.
    AnalyzesOk("create function TestFn(int) RETURNS INT " + udfSuffix);
    // Create a function with the same signature in a different db
    AnalyzesOk("create function functional.TestFn() RETURNS INT " + udfSuffix);
    AnalyzesOk("drop function TestFn()");
    AnalyzesOk("drop function TestFn(double)");
    AnalyzesOk("drop function TestFn(string...)");
    AnalysisError("drop function TestFn(double...)", "Function does not exist: testfn(DOUBLE...)");
    AnalysisError("drop function TestFn(int)", "Function does not exist: testfn(INT)");
    AnalysisError("drop function functional.TestFn()", "Function does not exist: testfn()");
    AnalysisError("create function f() returns int " + udfSuffix + "init_fn='a'", "Optional argument 'INIT_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "serialize_fn='a'", "Optional argument 'SERIALIZE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "merge_fn='a'", "Optional argument 'MERGE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "finalize_fn='a'", "Optional argument 'FINALIZE_FN' should not be set");
}
#method_after
@Test
public void TestUdf() throws AnalysisException {
    final String symbol = "'_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'";
    final String udfSuffix = " LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=" + symbol;
    final String udfSuffixIr = " LOCATION '/test-warehouse/test-udfs.ll' " + "SYMBOL=" + symbol;
    final String hdfsPath = "hdfs://localhost:20500/test-warehouse/libTestUdfs.so";
    AnalyzesOk("create function foo() RETURNS int" + udfSuffix);
    AnalyzesOk("create function foo(int, int, string) RETURNS int" + udfSuffix);
    // Try some fully qualified function names
    AnalyzesOk("create function functional.B() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.B1() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.`B1C`() RETURNS int" + udfSuffix);
    // Name with underscore
    AnalyzesOk("create function A_B() RETURNS int" + udfSuffix);
    // Locations for all the udfs types.
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.so' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'", "Could not load binary: /test-warehouse/libTestUdfs.ll");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo(int) RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' SYMBOL='Identity'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.SO' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/hive-exec.jar' SYMBOL='a'");
    // Test hive UDFs for unsupported types
    AnalysisError("create function foo() RETURNS timestamp LOCATION '/test-warehouse/hive-exec.jar' SYMBOL='a'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo(timestamp) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo() RETURNS decimal LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(Decimal) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(char(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(varchar(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5) LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5) LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5)" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5)" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo(CHAR(5)) RETURNS int" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(VARCHAR(5)) RETURNS int" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalyzesOk("create function foo() RETURNS decimal" + udfSuffix);
    AnalyzesOk("create function foo() RETURNS decimal(38,10)" + udfSuffix);
    AnalyzesOk("create function foo(Decimal, decimal(10, 2)) RETURNS int" + udfSuffix);
    AnalysisError("create function foo() RETURNS decimal(100)" + udfSuffix, "Decimal precision must be <= 38: 100");
    AnalysisError("create function foo(Decimal(2, 3)) RETURNS int" + udfSuffix, "Decimal scale (3) must be <= precision (2)");
    // Varargs
    AnalyzesOk("create function foo(INT...) RETURNS int" + udfSuffix);
    // Prepare/Close functions
    AnalyzesOk("create function foo() returns int" + udfSuffix + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='_Z19ValidateOpenPreparePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'" + " close_fn='_Z17ValidateOpenClosePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " close_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn='FakePrepare'", "Could not find function FakePrepare(impala_udf::FunctionContext*, " + "impala_udf::FunctionContext::FunctionStateScope) in: ");
    // Try to create a function with the same name as a builtin
    AnalysisError("create function sin(double) RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    AnalysisError("create function sin() RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    // Try to create with a bad location
    AnalysisError("create function foo() RETURNS int LOCATION 'bad-location' SYMBOL='c'", "URI path must be absolute: bad-location");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'blah://localhost:50200/bad-location' SYMBOL='c'", "No FileSystem for scheme: blah");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'file:///foo.jar' SYMBOL='c'", "Could not load binary: file:///foo.jar");
    // Try creating udfs with unknown extensions
    AnalysisError("create function foo() RETURNS int LOCATION '/binary' SYMBOL='a'", "Unknown binary type: '/binary'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.a' SYMBOL='a'", "Unknown binary type: '/binary.a'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so.' SYMBOL='a'", "Unknown binary type: '/binary.so.'. Binary must end in .jar, .so or .ll");
    // Try with missing symbol
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so'", "Argument 'SYMBOL' must be set.");
    // Try with symbols missing in binary and symbols
    AnalysisError("create function foo() RETURNS int LOCATION '/blah.so' " + "SYMBOL='ab'", "Could not load binary: /blah.so");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.JAR' SYMBOL='a'", "Could not load binary: /binary.JAR");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='b'", "Could not find function b() in: " + hdfsPath);
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=''", "Could not find symbol ''");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='_ZAB'", "Could not find symbol '_ZAB' in: " + hdfsPath);
    // Infer the fully mangled symbol from the signature
    AnalyzesOk("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // We can't get the return type so any of those will match
    AnalyzesOk("create function foo() RETURNS double " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // The part the user specifies is case sensitive
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='noArgs'", "Could not find function noArgs() in: " + hdfsPath);
    // Types no longer match
    AnalysisError("create function foo(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'", "Could not find function NoArgs(INT) in: " + hdfsPath);
    // Check we can match identity for all types
    AnalyzesOk("create function identity(boolean) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(tinyint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(smallint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(bigint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(float) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(double) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(string) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function all_types_fn(string, boolean, tinyint, " + "smallint, int, bigint, float, double, decimal) returns int " + "location '/test-warehouse/libTestUdfs.so' symbol='AllTypes'");
    // Try creating functions with illegal function names.
    AnalysisError("create function 123A() RETURNS int" + udfSuffix, "Function cannot start with a digit: 123a");
    AnalysisError("create function A.`1A`() RETURNS int" + udfSuffix, "Function cannot start with a digit: 1a");
    AnalysisError("create function A.`ABC-D`() RETURNS int" + udfSuffix, "Function names must be all alphanumeric or underscore. Invalid name: abc-d");
    AnalysisError("create function baddb.f() RETURNS int" + udfSuffix, "Database does not exist: baddb");
    AnalysisError("create function a.b.c() RETURNS int" + udfSuffix, "Invalid function name: 'a.b.c'. Expected [dbname].funcname.");
    AnalysisError("create function a.b.c.d(smallint) RETURNS int" + udfSuffix, "Invalid function name: 'a.b.c.d'. Expected [dbname].funcname.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try dropping functions.
    AnalyzesOk("drop function if exists foo()");
    AnalysisError("drop function foo()", "Function does not exist: foo()");
    AnalyzesOk("drop function if exists a.foo()");
    AnalysisError("drop function a.foo()", "Database does not exist: a");
    AnalyzesOk("drop function if exists foo()");
    AnalyzesOk("drop function if exists foo(int...)");
    AnalyzesOk("drop function if exists foo(double, int...)");
    // Add functions default.TestFn(), default.TestFn(double), default.TestFn(String...),
    addTestFunction("TestFn", new ArrayList<ScalarType>(), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.DOUBLE), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.STRING), true);
    AnalysisError("create function TestFn() RETURNS INT " + udfSuffix, "Function already exists: testfn()");
    AnalysisError("create function TestFn(double) RETURNS INT " + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Fn(Double) and Fn(Double...) should be a conflict.
    AnalysisError("create function TestFn(double...) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    AnalysisError("create function TestFn(double) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Add default.TestFn(int, int)
    addTestFunction("TestFn", Lists.newArrayList(Type.INT, Type.INT), false);
    AnalyzesOk("drop function TestFn(int, int)");
    AnalysisError("drop function TestFn(int, int, int)", "Function does not exist: testfn(INT, INT, INT)");
    // Fn(String...) was already added.
    AnalysisError("create function TestFn(String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String...) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String, String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalyzesOk("create function TestFn(String, String, Int) RETURNS INT" + udfSuffix);
    // Check function overloading.
    AnalyzesOk("create function TestFn(int) RETURNS INT " + udfSuffix);
    // Create a function with the same signature in a different db
    AnalyzesOk("create function functional.TestFn() RETURNS INT " + udfSuffix);
    AnalyzesOk("drop function TestFn()");
    AnalyzesOk("drop function TestFn(double)");
    AnalyzesOk("drop function TestFn(string...)");
    AnalysisError("drop function TestFn(double...)", "Function does not exist: testfn(DOUBLE...)");
    AnalysisError("drop function TestFn(int)", "Function does not exist: testfn(INT)");
    AnalysisError("drop function functional.TestFn()", "Function does not exist: testfn()");
    AnalysisError("create function f() returns int " + udfSuffix + "init_fn='a'", "Optional argument 'INIT_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "serialize_fn='a'", "Optional argument 'SERIALIZE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "merge_fn='a'", "Optional argument 'MERGE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "finalize_fn='a'", "Optional argument 'FINALIZE_FN' should not be set");
}
#end_block

#method_before
@Test
public void TestUda() throws AnalysisException {
    final String loc = " LOCATION '/test-warehouse/libTestUdas.so' ";
    final String hdfsLoc = "hdfs://localhost:20500/test-warehouse/libTestUdas.so";
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AGgInit'", "Could not find function AGgInit() returns INT in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(int, int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate'");
    // TODO: remove these when the BE can execute them
    AnalysisError("create aggregate function foo(int...) RETURNS int" + loc, "UDAs with varargs are not yet supported.");
    AnalysisError("create aggregate function " + "foo(int, int, int, int, int, int, int , int, int) " + "RETURNS int" + loc, "UDAs with more than 8 arguments are not yet supported.");
    // Check that CHAR and VARCHAR are not valid UDA argument or return types
    String symbols = " UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'";
    AnalysisError("create aggregate function foo(CHAR(5)) RETURNS int" + loc + symbols, "UDAs with CHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(VARCHAR(5)) RETURNS int" + loc + symbols, "UDAs with VARCHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS CHAR(5)" + loc + symbols, "UDAs with CHAR return type are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS VARCHAR(5)" + loc + symbols, "UDAs with VARCHAR return type are not yet supported.");
    // Specify the complete symbol. If the user does this, we can't guess the
    // other function names.
    // TODO: think about these error messages more. Perhaps they can be made
    // more actionable.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'");
    // Try with intermediate type
    // TODO: this is currently not supported. Remove these tests and re-enable
    // the commented out ones when we do.
    AnalyzesOk("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE int" + loc + "UPDATE_FN='AggUpdate'");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE double" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DOUBLE, that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE char(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, CHAR(10), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DECIMAL(10,0), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(40)" + loc + "UPDATE_FN='AggUpdate'", "Decimal precision must be <= 38.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='AggUpdate'");
    // AnalysisError("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge'" ,
    // "Finalize() is required for this UDA.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge' FINALIZE_FN='AggFinalize'");
    // Udf only arguments must not be set.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "SYMBOL='Bad'", "Optional argument 'SYMBOL' should not be set.");
    // Invalid char(0) type.
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE CHAR(0) LOCATION '/foo.so' UPDATE_FN='b'", "Char size must be > 0. Size is too small: 0.");
    AnalysisError("create aggregate function foo() RETURNS int" + loc, "UDAs must take at least one argument.");
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.jar' UPDATE_FN='b'", "Java UDAs are not supported.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create aggregate function foo(string, double) RETURNS array<int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(map<string,int>) RETURNS int " + loc + "UPDATE_FN='AggUpdate'", "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(int) RETURNS struct<f:int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Test missing .ll file. TODO: reenable when we can run IR UDAs
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.ll' UPDATE_FN='Fn'", "IR UDAs are not yet supported.");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='Fn'", "Could not load binary: /foo.ll");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='_ZABCD'", "Could not load binary: /foo.ll");
    // Test cases where the UPDATE_FN doesn't contain "Update" in which case the user has
    // to explicitly specify the other functions.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggFinalize'");
    // Serialize and Finalize have the same signature, make sure that's possible.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggFinalize' FINALIZE_FN='AggFinalize'");
    // If you don't specify the full symbol, we look for it in the binary. This should
    // prevent mismatched names by accident.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggSerialize'", "Could not find function AggSerialize() returns STRING in: " + hdfsLoc);
    // If you specify a mangled name, we just check it exists.
    // TODO: we should be able to validate better. This is almost certainly going
    // to crash everything.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' " + "INIT_FN='_Z12AggSerializePN10impala_udf15FunctionContextERKNS_6IntValE'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='_ZAggSerialize'", "Could not find symbol '_ZAggSerialize' in: " + hdfsLoc);
    // Tests for checking the symbol exists
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update'", "Could not find function Agg2Init() returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit'", "Could not find function Agg2Merge(STRING) returns STRING in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='BadFn'", "Could not find function BadFn(STRING) returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "FINALIZE_FN='not there'", "Could not find function not there(STRING) in: " + hdfsLoc);
}
#method_after
@Test
public void TestUda() throws AnalysisException {
    final String loc = " LOCATION '/test-warehouse/libTestUdas.so' ";
    final String hdfsLoc = "hdfs://localhost:20500/test-warehouse/libTestUdas.so";
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AGgInit'", "Could not find function AGgInit() returns INT in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(int, int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate'");
    // TODO: remove these when the BE can execute them
    AnalysisError("create aggregate function foo(int...) RETURNS int" + loc, "UDAs with varargs are not yet supported.");
    AnalysisError("create aggregate function " + "foo(int, int, int, int, int, int, int , int, int) " + "RETURNS int" + loc, "UDAs with more than 8 arguments are not yet supported.");
    // Check that CHAR and VARCHAR are not valid UDA argument or return types
    String symbols = " UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'";
    AnalysisError("create aggregate function foo(CHAR(5)) RETURNS int" + loc + symbols, "UDAs with CHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(VARCHAR(5)) RETURNS int" + loc + symbols, "UDAs with VARCHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS CHAR(5)" + loc + symbols, "UDAs with CHAR return type are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS VARCHAR(5)" + loc + symbols, "UDAs with VARCHAR return type are not yet supported.");
    // Specify the complete symbol. If the user does this, we can't guess the
    // other function names.
    // TODO: think about these error messages more. Perhaps they can be made
    // more actionable.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'");
    // Try with intermediate type
    // TODO: this is currently not supported. Remove these tests and re-enable
    // the commented out ones when we do.
    AnalyzesOk("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE int" + loc + "UPDATE_FN='AggUpdate'");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE double" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DOUBLE, that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE char(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, CHAR(10), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DECIMAL(10,0), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(40)" + loc + "UPDATE_FN='AggUpdate'", "Decimal precision must be <= 38: 40");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='AggUpdate'");
    // AnalysisError("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge'" ,
    // "Finalize() is required for this UDA.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge' FINALIZE_FN='AggFinalize'");
    // Udf only arguments must not be set.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "SYMBOL='Bad'", "Optional argument 'SYMBOL' should not be set.");
    // Invalid char(0) type.
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE CHAR(0) LOCATION '/foo.so' UPDATE_FN='b'", "Char size must be > 0: 0");
    AnalysisError("create aggregate function foo() RETURNS int" + loc, "UDAs must take at least one argument.");
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.jar' UPDATE_FN='b'", "Java UDAs are not supported.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create aggregate function foo(string, double) RETURNS array<int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(map<string,int>) RETURNS int " + loc + "UPDATE_FN='AggUpdate'", "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(int) RETURNS struct<f:int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Test missing .ll file. TODO: reenable when we can run IR UDAs
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.ll' UPDATE_FN='Fn'", "IR UDAs are not yet supported.");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='Fn'", "Could not load binary: /foo.ll");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='_ZABCD'", "Could not load binary: /foo.ll");
    // Test cases where the UPDATE_FN doesn't contain "Update" in which case the user has
    // to explicitly specify the other functions.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggFinalize'");
    // Serialize and Finalize have the same signature, make sure that's possible.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggFinalize' FINALIZE_FN='AggFinalize'");
    // If you don't specify the full symbol, we look for it in the binary. This should
    // prevent mismatched names by accident.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggSerialize'", "Could not find function AggSerialize() returns STRING in: " + hdfsLoc);
    // If you specify a mangled name, we just check it exists.
    // TODO: we should be able to validate better. This is almost certainly going
    // to crash everything.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' " + "INIT_FN='_Z12AggSerializePN10impala_udf15FunctionContextERKNS_6IntValE'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='_ZAggSerialize'", "Could not find symbol '_ZAggSerialize' in: " + hdfsLoc);
    // Tests for checking the symbol exists
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update'", "Could not find function Agg2Init() returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit'", "Could not find function Agg2Merge(STRING) returns STRING in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='BadFn'", "Could not find function BadFn(STRING) returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "FINALIZE_FN='not there'", "Could not find function not there(STRING) in: " + hdfsLoc);
}
#end_block

#method_before
@Test
public void TestTypes() {
    // Test primitive types.
    TypeDefsAnalyzeOk("BOOLEAN");
    TypeDefsAnalyzeOk("TINYINT");
    TypeDefsAnalyzeOk("SMALLINT");
    TypeDefsAnalyzeOk("INT", "INTEGER");
    TypeDefsAnalyzeOk("BIGINT");
    TypeDefsAnalyzeOk("FLOAT");
    TypeDefsAnalyzeOk("DOUBLE", "REAL");
    TypeDefsAnalyzeOk("STRING");
    TypeDefsAnalyzeOk("CHAR(1)", "CHAR(20)");
    TypeDefsAnalyzeOk("BINARY");
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("TIMESTAMP");
    // Test decimal.
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("DECIMAL(1)");
    TypeDefsAnalyzeOk("DECIMAL(12, 7)");
    TypeDefsAnalyzeOk("DECIMAL(38)");
    TypeDefsAnalyzeOk("DECIMAL(38, 1)");
    TypeDefsAnalyzeOk("DECIMAL(38, 38)");
    TypeDefAnalysisError("DECIMAL(1, 10)", "Decimal scale (10) must be <= precision (1).");
    TypeDefAnalysisError("DECIMAL(0, 0)", "Decimal precision must be greater than 0.");
    TypeDefAnalysisError("DECIMAL(39, 0)", "Decimal precision must be <= 38.");
    // Test complex types.
    TypeDefsAnalyzeOk("ARRAY<BIGINT>");
    TypeDefsAnalyzeOk("MAP<TINYINT, DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<f:TINYINT>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT, b:BIGINT, c:DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT COMMENT 'x', b:BIGINT, c:DOUBLE COMMENT 'y'>");
    // Map keys can't be complex types.
    TypeDefAnalysisError("map<array<int>, int>", "Map type cannot have a complex-typed key: MAP<ARRAY<INT>,INT>");
    // Duplicate struct-field name.
    TypeDefAnalysisError("STRUCT<f1: int, f2: string, f1: float>", "Duplicate field name 'f1' in struct 'STRUCT<f1:INT,f2:STRING,f1:FLOAT>'");
    // Invalid struct-field name.
    TypeDefAnalysisError("STRUCT<`???`: int>", "Invalid struct field name: ???");
}
#method_after
@Test
public void TestTypes() {
    // Test primitive types.
    TypeDefsAnalyzeOk("BOOLEAN");
    TypeDefsAnalyzeOk("TINYINT");
    TypeDefsAnalyzeOk("SMALLINT");
    TypeDefsAnalyzeOk("INT", "INTEGER");
    TypeDefsAnalyzeOk("BIGINT");
    TypeDefsAnalyzeOk("FLOAT");
    TypeDefsAnalyzeOk("DOUBLE", "REAL");
    TypeDefsAnalyzeOk("STRING");
    TypeDefsAnalyzeOk("CHAR(1)", "CHAR(20)");
    TypeDefsAnalyzeOk("BINARY");
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("TIMESTAMP");
    // Test decimal.
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("DECIMAL(1)");
    TypeDefsAnalyzeOk("DECIMAL(12, 7)");
    TypeDefsAnalyzeOk("DECIMAL(38)");
    TypeDefsAnalyzeOk("DECIMAL(38, 1)");
    TypeDefsAnalyzeOk("DECIMAL(38, 38)");
    TypeDefAnalysisError("DECIMAL(1, 10)", "Decimal scale (10) must be <= precision (1)");
    TypeDefAnalysisError("DECIMAL(0, 0)", "Decimal precision must be > 0: 0");
    TypeDefAnalysisError("DECIMAL(39, 0)", "Decimal precision must be <= 38");
    // Test complex types.
    TypeDefsAnalyzeOk("ARRAY<BIGINT>");
    TypeDefsAnalyzeOk("MAP<TINYINT, DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<f:TINYINT>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT, b:BIGINT, c:DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT COMMENT 'x', b:BIGINT, c:DOUBLE COMMENT 'y'>");
    // Map keys can't be complex types.
    TypeDefAnalysisError("map<array<int>, int>", "Map type cannot have a complex-typed key: MAP<ARRAY<INT>,INT>");
    // Duplicate struct-field name.
    TypeDefAnalysisError("STRUCT<f1: int, f2: string, f1: float>", "Duplicate field name 'f1' in struct 'STRUCT<f1:INT,f2:STRING,f1:FLOAT>'");
    // Invalid struct-field name.
    TypeDefAnalysisError("STRUCT<`???`: int>", "Invalid struct field name: ???");
}
#end_block

#method_before
@Test
public void TestPermissionValidation() throws AnalysisException {
    String location = "/test-warehouse/.tmp_" + UUID.randomUUID().toString();
    Path parentPath = FileSystemUtil.createFullyQualifiedPath(new Path(location));
    FileSystem fs = null;
    try {
        fs = parentPath.getFileSystem(FileSystemUtil.getConfiguration());
        // Test location doesn't exist
        AnalyzesOk(String.format("create table new_table (col INT) location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        // Test localtion path with trailing slash.
        AnalyzesOk(String.format("create table new_table (col INT) location " + "'%s/new_table/'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table location '%s/new_table' " + "as select 1, 1", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table like functional.alltypes " + "location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create database new_db location '%s/new_db'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        fs.mkdirs(parentPath);
        // Create a test data file for load data test
        FSDataOutputStream out = fs.create(new Path(parentPath, "test_loaddata/testdata.txt"));
        out.close();
        fs.setPermission(parentPath, new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));
        // Test location exists but Impala doesn't have sufficient permission
        AnalyzesOk(String.format("create data Source serverlog location " + "'%s/foo.jar' class 'foo.Bar' API_VERSION 'V1'", location), String.format("Impala does not have READ access to path '%s'", parentPath));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "add partition (s2='hello') location '%s/new_partition'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "partition(s2=NULL) set location '%s/new_part_loc'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        // Test location exists and Impala does have sufficient permission
        fs.setPermission(parentPath, new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location));
    } catch (IOException e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        // Clean up
        try {
            if (fs != null && fs.exists(parentPath)) {
                fs.delete(parentPath, true);
            }
        } catch (IOException e) {
        // Ignore
        }
    }
}
#method_after
@Test
public void TestPermissionValidation() throws AnalysisException {
    String location = "/test-warehouse/.tmp_" + UUID.randomUUID().toString();
    Path parentPath = FileSystemUtil.createFullyQualifiedPath(new Path(location));
    FileSystem fs = null;
    try {
        fs = parentPath.getFileSystem(FileSystemUtil.getConfiguration());
        // Test location doesn't exist
        AnalyzesOk(String.format("create table new_table (col INT) location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        // Test localtion path with trailing slash.
        AnalyzesOk(String.format("create table new_table (col INT) location " + "'%s/new_table/'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table location '%s/new_table' " + "as select 1, 1", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table like functional.alltypes " + "location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create database new_db location '%s/new_db'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        fs.mkdirs(parentPath);
        // Create a test data file for load data test
        FSDataOutputStream out = fs.create(new Path(parentPath, "test_loaddata/testdata.txt"));
        out.close();
        fs.setPermission(parentPath, new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));
        // Test location exists but Impala doesn't have sufficient permission
        AnalyzesOk(String.format("create data Source serverlog location " + "'%s/foo.jar' class 'foo.Bar' API_VERSION 'V1'", location), String.format("Impala does not have READ access to path '%s'", parentPath));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "add partition (s2='hello') location '%s/new_partition'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.stringpartitionkey " + "partition(string_col = 'partition1') set location '%s/new_part_loc'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        // Test location exists and Impala does have sufficient permission
        fs.setPermission(parentPath, new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location));
    } catch (IOException e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        // Clean up
        try {
            if (fs != null && fs.exists(parentPath)) {
                fs.delete(parentPath, true);
            }
        } catch (IOException e) {
        // Ignore
        }
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    try {
        resolvedPath_ = analyzer.resolveTableRefPath(path_, false);
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(path_)), e);
    }
    if (resolvedPath_ == null) {
        // reveal the non-existence of a table/database if the user is not authorized.
        if (path_.size() > 1) {
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(path_.get(0), path_.get(1)).allOf(getPrivilegeRequirement()).toRequest());
        }
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(analyzer.getDefaultDb(), path_.get(0)).allOf(getPrivilegeRequirement()).toRequest());
        throw new AnalysisException(String.format("Could not resolve table reference: '%s'", Joiner.on(".").join(path_)));
    }
    Preconditions.checkNotNull(resolvedPath_);
    if (!resolvedPath_.getMatch().isCollectionType()) {
        throw new AnalysisException(String.format("Invalid table reference to non-collection type: '%s'\n" + "Path resolved to type: %s", Joiner.on(".").join(path_), resolvedPath_.getMatch().toSql()));
    }
    if (resolvedPath_.getTupleDesc() != null) {
        if (resolvedPath_.matchesTableAlias()) {
            throw new AnalysisException(String.format("Invalid table reference to existing table alias: '%s'", Joiner.on(".").join(path_)));
        }
        if (!analyzer.isVisible(resolvedPath_.getTupleDesc().getId())) {
            throw new AnalysisException(String.format("Illegal table reference '%s' of semi-/anti-joined table '%s'", Joiner.on(".").join(path_), resolvedPath_.getTupleDesc().getAlias()));
        }
    }
    if (resolvedPath_.getTable() != null) {
        Table table = resolvedPath_.getTable();
        // Add access event for auditing.
        if (table instanceof View) {
            View view = (View) table;
            if (!view.isLocalView()) {
                analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, getPrivilegeRequirement().toString()));
            }
        } else {
            analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, getPrivilegeRequirement().toString()));
        }
        // Add privilege requests for authorization.
        TableName tableName = table.getTableName();
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(getPrivilegeRequirement()).toRequest());
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    try {
        resolvedPath_ = analyzer.resolvePath(rawPath_, PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!analyzer.hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath_.size() > 1) {
                analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath_.get(0), rawPath_.get(1)).allOf(getPrivilegeRequirement()).toRequest());
            }
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(analyzer.getDefaultDb(), rawPath_.get(0)).allOf(getPrivilegeRequirement()).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath_)), e);
    }
    if (resolvedPath_.getRootTable() != null) {
        // Add access event for auditing.
        Table table = resolvedPath_.getRootTable();
        if (table instanceof View) {
            View view = (View) table;
            if (!view.isLocalView()) {
                analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, getPrivilegeRequirement().toString()));
            }
        } else {
            analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, getPrivilegeRequirement().toString()));
        }
        // Add privilege requests for authorization.
        TableName tableName = table.getTableName();
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(getPrivilegeRequirement()).toRequest());
    }
}
#end_block

#method_before
public TupleDescriptor createTupleDescriptor(Analyzer analyzer) throws AnalysisException {
    throw new AnalysisException("Unresolved table reference: " + tableRefToSql());
}
#method_after
public TupleDescriptor createTupleDescriptor(Analyzer analyzer) throws AnalysisException {
    TupleDescriptor result = analyzer.getDescTbl().createTupleDescriptor(getClass().getSimpleName() + " " + getUniqueAlias());
    result.setPath(resolvedPath_);
    return result;
}
#end_block

#method_before
public List<String> getPath() {
    return path_;
}
#method_after
public List<String> getPath() {
    return rawPath_;
}
#end_block

#method_before
public Table getTable() {
    Preconditions.checkNotNull(resolvedPath_);
    return resolvedPath_.getTable();
}
#method_after
public Table getTable() {
    Preconditions.checkNotNull(resolvedPath_);
    return resolvedPath_.getRootTable();
}
#end_block

#method_before
public void analyzeJoin(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(desc_ != null);
    analyzeJoinHints(analyzer);
    if (joinOp_ == JoinOperator.CROSS_JOIN) {
        // A CROSS JOIN is always a broadcast join, regardless of the join hints
        isBroadcastJoin_ = true;
    }
    if (usingColNames_ != null) {
        Preconditions.checkState(joinOp_ != JoinOperator.CROSS_JOIN);
        // Turn USING clause into equivalent ON clause.
        onClause_ = null;
        for (String colName : usingColNames_) {
            // check whether colName exists both for our table and the one
            // to the left of us
            List<String> colPath = Lists.newArrayList(colName);
            if (Path.resolve(leftTblRef_.getDesc(), colPath, 0) == null) {
                throw new AnalysisException("unknown column " + colName + " for alias " + leftTblRef_.getUniqueAlias() + " (in \"" + this.toSql() + "\")");
            }
            if (Path.resolve(desc_, colPath, 0) == null) {
                throw new AnalysisException("unknown column " + colName + " for alias " + getUniqueAlias() + " (in \"" + this.toSql() + "\")");
            }
            // create predicate "<left>.colName = <right>.colName"
            BinaryPredicate eqPred = new BinaryPredicate(BinaryPredicate.Operator.EQ, new SlotRef(Path.createPath(leftTblRef_.desc_, colName)), new SlotRef(Path.createPath(desc_, colName)));
            onClause_ = CompoundPredicate.createConjunction(eqPred, onClause_);
        }
    }
    // at this point, both 'this' and leftTblRef have been analyzed and registered;
    // register the tuple ids of the TableRefs on the nullable side of an outer join
    boolean lhsIsNullable = false;
    boolean rhsIsNullable = false;
    if (joinOp_ == JoinOperator.LEFT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerOuterJoinedTids(getId().asList(), this);
        rhsIsNullable = true;
    }
    if (joinOp_ == JoinOperator.RIGHT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerOuterJoinedTids(leftTblRef_.getAllTupleIds(), this);
        lhsIsNullable = true;
    }
    // register the tuple ids of a full outer join
    if (joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerFullOuterJoinedTids(leftTblRef_.getAllTupleIds(), this);
        analyzer.registerFullOuterJoinedTids(getId().asList(), this);
    }
    // register the tuple id of the rhs of a left semi join
    TupleId semiJoinedTupleId = null;
    if (joinOp_ == JoinOperator.LEFT_SEMI_JOIN || joinOp_ == JoinOperator.LEFT_ANTI_JOIN || joinOp_ == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
        analyzer.registerSemiJoinedTid(getId(), this);
        semiJoinedTupleId = getId();
    }
    // register the tuple id of the lhs of a right semi join
    if (joinOp_ == JoinOperator.RIGHT_SEMI_JOIN || joinOp_ == JoinOperator.RIGHT_ANTI_JOIN) {
        analyzer.registerSemiJoinedTid(leftTblRef_.getId(), this);
        semiJoinedTupleId = leftTblRef_.getId();
    }
    if (onClause_ != null) {
        Preconditions.checkState(joinOp_ != JoinOperator.CROSS_JOIN);
        analyzer.setVisibleSemiJoinedTuple(semiJoinedTupleId);
        onClause_.analyze(analyzer);
        analyzer.setVisibleSemiJoinedTuple(null);
        onClause_.checkReturnsBool("ON clause", true);
        if (onClause_.contains(Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function not allowed in ON clause: " + toSql());
        }
        if (onClause_.contains(AnalyticExpr.class)) {
            throw new AnalysisException("analytic expression not allowed in ON clause: " + toSql());
        }
        Set<TupleId> onClauseTupleIds = Sets.newHashSet();
        for (Expr e : onClause_.getConjuncts()) {
            // Outer join clause conjuncts are registered for this particular table ref
            // (ie, can only be evaluated by the plan node that implements this join).
            // The exception are conjuncts that only pertain to the nullable side
            // of the outer join; those can be evaluated directly when materializing tuples
            // without violating outer join semantics.
            analyzer.registerOnClauseConjuncts(e, this);
            List<TupleId> tupleIds = Lists.newArrayList();
            e.getIds(tupleIds, null);
            onClauseTupleIds.addAll(tupleIds);
        }
        onClauseTupleIds_.addAll(onClauseTupleIds);
    } else if (!isParentChildJoin() && (getJoinOp().isOuterJoin() || getJoinOp().isSemiJoin())) {
        throw new AnalysisException(joinOp_.toString() + " requires an ON or USING clause.");
    }
    // Make constant expressions from inline view refs nullable in its substitution map.
    if (lhsIsNullable && leftTblRef_ instanceof InlineViewRef) {
        ((InlineViewRef) leftTblRef_).makeOutputNullable(analyzer);
    }
    if (rhsIsNullable && this instanceof InlineViewRef) {
        ((InlineViewRef) this).makeOutputNullable(analyzer);
    }
}
#method_after
public void analyzeJoin(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(desc_ != null);
    analyzeJoinHints(analyzer);
    if (joinOp_ == JoinOperator.CROSS_JOIN) {
        // A CROSS JOIN is always a broadcast join, regardless of the join hints
        isBroadcastJoin_ = true;
    }
    if (usingColNames_ != null) {
        Preconditions.checkState(joinOp_ != JoinOperator.CROSS_JOIN);
        // Turn USING clause into equivalent ON clause.
        onClause_ = null;
        for (String colName : usingColNames_) {
            // check whether colName exists both for our table and the one
            // to the left of us
            Path leftColPath = new Path(leftTblRef_.getDesc(), Lists.newArrayList(colName.toLowerCase()));
            if (!leftColPath.resolve()) {
                throw new AnalysisException("unknown column " + colName + " for alias " + leftTblRef_.getUniqueAlias() + " (in \"" + this.toSql() + "\")");
            }
            Path rightColPath = new Path(desc_, Lists.newArrayList(colName.toLowerCase()));
            if (!rightColPath.resolve()) {
                throw new AnalysisException("unknown column " + colName + " for alias " + getUniqueAlias() + " (in \"" + this.toSql() + "\")");
            }
            // create predicate "<left>.colName = <right>.colName"
            BinaryPredicate eqPred = new BinaryPredicate(BinaryPredicate.Operator.EQ, new SlotRef(Path.createRawPath(leftTblRef_.getUniqueAlias(), colName)), new SlotRef(Path.createRawPath(getUniqueAlias(), colName)));
            onClause_ = CompoundPredicate.createConjunction(eqPred, onClause_);
        }
    }
    // register the tuple ids of the TableRefs on the nullable side of an outer join
    if (joinOp_ == JoinOperator.LEFT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerOuterJoinedTids(getId().asList(), this);
    }
    if (joinOp_ == JoinOperator.RIGHT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerOuterJoinedTids(leftTblRef_.getAllTupleIds(), this);
    }
    // register the tuple ids of a full outer join
    if (joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerFullOuterJoinedTids(leftTblRef_.getAllTupleIds(), this);
        analyzer.registerFullOuterJoinedTids(getId().asList(), this);
    }
    // register the tuple id of the rhs of a left semi join
    TupleId semiJoinedTupleId = null;
    if (joinOp_ == JoinOperator.LEFT_SEMI_JOIN || joinOp_ == JoinOperator.LEFT_ANTI_JOIN || joinOp_ == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
        analyzer.registerSemiJoinedTid(getId(), this);
        semiJoinedTupleId = getId();
    }
    // register the tuple id of the lhs of a right semi join
    if (joinOp_ == JoinOperator.RIGHT_SEMI_JOIN || joinOp_ == JoinOperator.RIGHT_ANTI_JOIN) {
        analyzer.registerSemiJoinedTid(leftTblRef_.getId(), this);
        semiJoinedTupleId = leftTblRef_.getId();
    }
    if (onClause_ != null) {
        Preconditions.checkState(joinOp_ != JoinOperator.CROSS_JOIN);
        analyzer.setVisibleSemiJoinedTuple(semiJoinedTupleId);
        onClause_.analyze(analyzer);
        analyzer.setVisibleSemiJoinedTuple(null);
        onClause_.checkReturnsBool("ON clause", true);
        if (onClause_.contains(Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function not allowed in ON clause: " + toSql());
        }
        if (onClause_.contains(AnalyticExpr.class)) {
            throw new AnalysisException("analytic expression not allowed in ON clause: " + toSql());
        }
        Set<TupleId> onClauseTupleIds = Sets.newHashSet();
        for (Expr e : onClause_.getConjuncts()) {
            // Outer join clause conjuncts are registered for this particular table ref
            // (ie, can only be evaluated by the plan node that implements this join).
            // The exception are conjuncts that only pertain to the nullable side
            // of the outer join; those can be evaluated directly when materializing tuples
            // without violating outer join semantics.
            analyzer.registerOnClauseConjuncts(e, this);
            List<TupleId> tupleIds = Lists.newArrayList();
            e.getIds(tupleIds, null);
            onClauseTupleIds.addAll(tupleIds);
        }
        onClauseTupleIds_.addAll(onClauseTupleIds);
    } else if (!isRelativeRef() && (getJoinOp().isOuterJoin() || getJoinOp().isSemiJoin())) {
        throw new AnalysisException(joinOp_.toString() + " requires an ON or USING clause.");
    }
}
#end_block

#method_before
protected String tableRefToSql() {
    String aliasSql = null;
    String alias = getExplicitAlias();
    if (alias != null)
        aliasSql = ToSqlUtils.getIdentSql(alias);
    List<String> path = path_;
    if (resolvedPath_ != null)
        path = resolvedPath_.getFullyQualifiedPath();
    return ToSqlUtils.getPathSql(path) + ((aliasSql != null) ? " " + aliasSql : "");
}
#method_after
protected String tableRefToSql() {
    String aliasSql = null;
    String alias = getExplicitAlias();
    if (alias != null)
        aliasSql = ToSqlUtils.getIdentSql(alias);
    List<String> path = rawPath_;
    if (resolvedPath_ != null)
        path = resolvedPath_.getFullyQualifiedRawPath();
    return ToSqlUtils.getPathSql(path) + ((aliasSql != null) ? " " + aliasSql : "");
}
#end_block

#method_before
@Override
public TableRef clone() {
    return new TableRef(this);
}
#method_after
@Override
protected TableRef clone() {
    return new TableRef(this);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(getPrivilegeRequirement());
    // Determine if this table ref is correlated before registering it, because
    // registration relies on knowing whether a rable re is correlated or not.
    Analyzer parentAnalyzer = analyzer;
    TableRef parentTableRef = null;
    if (resolvedPath_.getTupleDesc() != null) {
        parentAnalyzer = analyzer.findAnalyzer(resolvedPath_.getTupleDesc());
        Preconditions.checkNotNull(parentAnalyzer);
        parentTableRef = parentAnalyzer.getTableRef(resolvedPath_.getTupleDesc().getId());
        isCorrelated_ = parentAnalyzer != analyzer || parentTableRef.isCorrelated();
    }
    desc_ = analyzer.registerTableRef(this);
    if (resolvedPath_.getTupleDesc() != null) {
        Preconditions.checkNotNull(parentTableRef);
        SlotDescriptor parentSlotDesc = parentAnalyzer.registerSlotRef(resolvedPath_);
        parentSlotDesc.setItemTupleDesc(desc_);
        SlotRef parentSlotRef = new SlotRef(parentSlotDesc);
        if (parentTableRef instanceof InlineViewRef) {
            InlineViewRef parentInlineViewRef = (InlineViewRef) parentTableRef;
            collectionExpr_ = parentSlotRef.substitute(parentInlineViewRef.getBaseTblSmap(), parentAnalyzer, false);
        } else {
            collectionExpr_ = parentSlotRef;
        }
        // Must always be materialized to ensure the correct cardinality after unnesting.
        analyzer.materializeSlots(collectionExpr_);
    }
    isAnalyzed_ = true;
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    Preconditions.checkNotNull(getPrivilegeRequirement());
    desc_ = analyzer.registerTableRef(this);
    if (isRelativeRef()) {
        SlotDescriptor parentSlotDesc = analyzer.registerSlotRef(resolvedPath_);
        parentSlotDesc.setItemTupleDesc(desc_);
        collectionExpr_ = new SlotRef(parentSlotDesc);
        // Must always be materialized to ensure the correct cardinality after unnesting.
        analyzer.materializeSlots(collectionExpr_);
        Analyzer parentAnalyzer = analyzer.findAnalyzer(resolvedPath_.getRootDesc().getId());
        Preconditions.checkNotNull(parentAnalyzer);
        isCorrelated_ = parentAnalyzer != analyzer;
    }
    isAnalyzed_ = true;
    analyzeJoin(analyzer);
}
#end_block

#method_before
@Override
public TableRef clone() {
    return new CollectionTableRef(this);
}
#method_after
@Override
protected CollectionTableRef clone() {
    return new CollectionTableRef(this);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    if (!(stmt_ instanceof SelectStmt)) {
        throw new AnalysisException("A subquery must contain a single select block: " + toSql());
    }
    // The subquery is analyzed with its own analyzer.
    analyzer_ = new Analyzer(analyzer);
    analyzer_.setIsSubquery();
    analyzer.analyzeNestedStmt(stmt_, analyzer_);
    // Set the subquery type based on the types of the exprs in the
    // result list of the associated SelectStmt.
    ArrayList<Expr> stmtResultExprs = stmt_.getResultExprs();
    if (stmtResultExprs.size() == 1) {
        type_ = stmtResultExprs.get(0).getType();
        Preconditions.checkState(!type_.isComplexType());
    } else {
        type_ = createStructTypeFromExprList();
    }
    // If the subquery returns many rows, set its type to ArrayType.
    if (!((SelectStmt) stmt_).returnsSingleRow())
        type_ = new ArrayType(type_);
    Preconditions.checkNotNull(type_);
    type_.analyze();
    isAnalyzed_ = true;
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    if (!(stmt_ instanceof SelectStmt)) {
        throw new AnalysisException("A subquery must contain a single select block: " + toSql());
    }
    // The subquery is analyzed with its own analyzer.
    analyzer_ = new Analyzer(analyzer);
    analyzer_.setIsSubquery();
    stmt_.analyze(analyzer_);
    stmt_.checkCorrelatedTableRefs(analyzer_);
    // Set the subquery type based on the types of the exprs in the
    // result list of the associated SelectStmt.
    ArrayList<Expr> stmtResultExprs = stmt_.getResultExprs();
    if (stmtResultExprs.size() == 1) {
        type_ = stmtResultExprs.get(0).getType();
        Preconditions.checkState(!type_.isComplexType());
    } else {
        type_ = createStructTypeFromExprList();
    }
    // If the subquery returns many rows, set its type to ArrayType.
    if (!((SelectStmt) stmt_).returnsSingleRow())
        type_ = new ArrayType(type_);
    Preconditions.checkNotNull(type_);
    isAnalyzed_ = true;
}
#end_block

#method_before
@Override
public CollectionTableRef clone() {
    return new CollectionTableRef(this);
}
#method_after
@Override
protected CollectionTableRef clone() {
    return new CollectionTableRef(this);
}
#end_block

#method_before
private void addStarResultExpr(Path resolvedPath, Analyzer analyzer, String... relRawPath) throws AnalysisException {
    Path p = Path.createRelPath(resolvedPath, relRawPath);
    Preconditions.checkState(p.resolve());
    SlotRef slotRef = new SlotRef(p);
    slotRef.analyze(analyzer);
    resultExprs_.add(slotRef);
    colLabels_.add(relRawPath[relRawPath.length - 1]);
}
#method_after
private void addStarResultExpr(Path resolvedPath, Analyzer analyzer, String... relRawPath) throws AnalysisException {
    Path p = Path.createRelPath(resolvedPath, relRawPath);
    Preconditions.checkState(p.resolve());
    SlotDescriptor slotDesc = analyzer.registerSlotRef(p);
    SlotRef slotRef = new SlotRef(slotDesc);
    slotRef.analyze(analyzer);
    resultExprs_.add(slotRef);
    colLabels_.add(relRawPath[relRawPath.length - 1]);
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    selectList_.reset();
    colLabels_.clear();
    for (int i = 0; i < tableRefs_.size(); ++i) {
        TableRef origTblRef = tableRefs_.get(i);
        if (origTblRef.isResolved() && !(origTblRef instanceof InlineViewRef)) {
            // Replace resolved table refs with unresolved ones.
            TableRef newTblRef = new TableRef(origTblRef);
            // Use the fully qualified raw path to preserve the original resolution.
            // Otherwise, non-fully qualified paths might incorrectly match a local view.
            newTblRef.rawPath_ = origTblRef.getResolvedPath().getFullyQualifiedRawPath();
            tableRefs_.set(i, newTblRef);
        }
        tableRefs_.get(i).reset();
    }
    baseTblSmap_.clear();
    if (whereClause_ != null)
        whereClause_.reset();
    if (groupingExprs_ != null)
        Expr.resetList(groupingExprs_);
    if (havingClause_ != null)
        havingClause_.reset();
}
#method_after
@Override
public void reset() {
    super.reset();
    selectList_.reset();
    colLabels_.clear();
    for (int i = 0; i < tableRefs_.size(); ++i) {
        TableRef origTblRef = tableRefs_.get(i);
        if (origTblRef.isResolved() && !(origTblRef instanceof InlineViewRef)) {
            // Replace resolved table refs with unresolved ones.
            TableRef newTblRef = new TableRef(origTblRef);
            // Use the fully qualified raw path to preserve the original resolution.
            // Otherwise, non-fully qualified paths might incorrectly match a local view.
            // TODO for 2.3: This full qualification preserves analysis state which is
            // contraty to the intended semantics of reset(). We could address this issue by
            // changing the WITH-clause analysis to register local views that have
            // fully-qualified table refs, and then remove the full qualification here.
            newTblRef.rawPath_ = origTblRef.getResolvedPath().getFullyQualifiedRawPath();
            tableRefs_.set(i, newTblRef);
        }
        tableRefs_.get(i).reset();
    }
    baseTblSmap_.clear();
    if (whereClause_ != null)
        whereClause_.reset();
    if (groupingExprs_ != null)
        Expr.resetList(groupingExprs_);
    if (havingClause_ != null)
        havingClause_.reset();
}
#end_block

#method_before
private void computeCommonPartitionExprs() {
    for (Expr analyticExpr : analyticExprs_) {
        Preconditions.checkState(analyticExpr.isAnalyzed_);
        List<Expr> partitionExprs = ((AnalyticExpr) analyticExpr).getPartitionExprs();
        if (partitionExprs == null)
            continue;
        if (commonPartitionExprs_.isEmpty()) {
            commonPartitionExprs_.addAll(partitionExprs);
        } else {
            commonPartitionExprs_.retainAll(partitionExprs);
            if (commonPartitionExprs_.isEmpty())
                break;
        }
    }
}
#method_after
private List<Expr> computeCommonPartitionExprs() {
    List<Expr> result = Lists.newArrayList();
    for (Expr analyticExpr : analyticExprs_) {
        Preconditions.checkState(analyticExpr.isAnalyzed_);
        List<Expr> partitionExprs = ((AnalyticExpr) analyticExpr).getPartitionExprs();
        if (partitionExprs == null)
            continue;
        if (result.isEmpty()) {
            result.addAll(partitionExprs);
        } else {
            result.retainAll(partitionExprs);
            if (result.isEmpty())
                break;
        }
    }
    return result;
}
#end_block

#method_before
public void setQualifier(Qualifier qualifier) {
    this.qualifier_ = qualifier;
}
#method_after
public void setQualifier(Qualifier qualifier) {
    qualifier_ = qualifier;
}
#end_block

#method_before
public void reset() {
    queryStmt_.reset();
    analyzer_ = null;
    smap_.clear();
    isDropped_ = false;
}
#method_after
public void reset() {
    queryStmt_.reset();
    qualifier_ = originalQualifier_;
    analyzer_ = null;
    smap_.clear();
    isDropped_ = false;
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    for (UnionOperand op : operands_) {
        op.reset();
    }
    distinctOperands_.clear();
    allOperands_.clear();
    distinctAggInfo_ = null;
    tupleId_ = null;
    toSqlString_ = null;
    hasAnalyticExprs_ = false;
}
#method_after
@Override
public void reset() {
    super.reset();
    for (UnionOperand op : operands_) op.reset();
    distinctOperands_.clear();
    allOperands_.clear();
    distinctAggInfo_ = null;
    tupleId_ = null;
    toSqlString_ = null;
    hasAnalyticExprs_ = false;
}
#end_block

#method_before
private void createMergeAggInfo(Analyzer analyzer) {
    Preconditions.checkState(mergeAggInfo_ == null);
    TupleDescriptor inputDesc = intermediateTupleDesc_;
    // construct grouping exprs
    ArrayList<Expr> groupingExprs = Lists.newArrayList();
    for (int i = 0; i < getGroupingExprs().size(); ++i) {
        SlotRef slotRef = new SlotRef(inputDesc.getSlots().get(i));
        groupingExprs.add(slotRef);
    }
    // construct agg exprs
    ArrayList<FunctionCallExpr> aggExprs = Lists.newArrayList();
    for (int i = 0; i < getAggregateExprs().size(); ++i) {
        FunctionCallExpr inputExpr = getAggregateExprs().get(i);
        Preconditions.checkState(inputExpr.isAggregateFunction());
        Expr aggExprParam = new SlotRef(inputDesc.getSlots().get(i + getGroupingExprs().size()));
        FunctionCallExpr aggExpr = FunctionCallExpr.createMergeAggCall(inputExpr, Lists.newArrayList(aggExprParam));
        aggExpr.analyzeNoThrow(analyzer);
        aggExprs.add(aggExpr);
    }
    AggPhase aggPhase = (aggPhase_ == AggPhase.FIRST) ? AggPhase.FIRST_MERGE : AggPhase.SECOND_MERGE;
    mergeAggInfo_ = new AggregateInfo(groupingExprs, aggExprs, aggPhase);
    mergeAggInfo_.intermediateTupleDesc_ = intermediateTupleDesc_;
    mergeAggInfo_.outputTupleDesc_ = outputTupleDesc_;
    mergeAggInfo_.intermediateTupleSmap_ = intermediateTupleSmap_;
    mergeAggInfo_.outputTupleSmap_ = outputTupleSmap_;
    // mergeAggInfo_.mergeAggInfo_ = mergeAggInfo_;
    mergeAggInfo_.materializedSlots_ = materializedSlots_;
}
#method_after
private void createMergeAggInfo(Analyzer analyzer) {
    Preconditions.checkState(mergeAggInfo_ == null);
    TupleDescriptor inputDesc = intermediateTupleDesc_;
    // construct grouping exprs
    ArrayList<Expr> groupingExprs = Lists.newArrayList();
    for (int i = 0; i < getGroupingExprs().size(); ++i) {
        SlotRef slotRef = new SlotRef(inputDesc.getSlots().get(i));
        groupingExprs.add(slotRef);
    }
    // construct agg exprs
    ArrayList<FunctionCallExpr> aggExprs = Lists.newArrayList();
    for (int i = 0; i < getAggregateExprs().size(); ++i) {
        FunctionCallExpr inputExpr = getAggregateExprs().get(i);
        Preconditions.checkState(inputExpr.isAggregateFunction());
        Expr aggExprParam = new SlotRef(inputDesc.getSlots().get(i + getGroupingExprs().size()));
        FunctionCallExpr aggExpr = FunctionCallExpr.createMergeAggCall(inputExpr, Lists.newArrayList(aggExprParam));
        aggExpr.analyzeNoThrow(analyzer);
        aggExprs.add(aggExpr);
    }
    AggPhase aggPhase = (aggPhase_ == AggPhase.FIRST) ? AggPhase.FIRST_MERGE : AggPhase.SECOND_MERGE;
    mergeAggInfo_ = new AggregateInfo(groupingExprs, aggExprs, aggPhase);
    mergeAggInfo_.intermediateTupleDesc_ = intermediateTupleDesc_;
    mergeAggInfo_.outputTupleDesc_ = outputTupleDesc_;
    mergeAggInfo_.intermediateTupleSmap_ = intermediateTupleSmap_;
    mergeAggInfo_.outputTupleSmap_ = outputTupleSmap_;
    mergeAggInfo_.materializedSlots_ = materializedSlots_;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        // Subqueries need to be rewritten.
        if (analyzer.containsSubquery())
            return;
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    createStmt_.getColumnDefs().clear();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, null);
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE"));
    }
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient();
    try {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        // target location for the INSERT statement, it is important the two match.
        if (createStmt_.getLocation() == null) {
            createStmt_.setLocation(new HdfsUri(msTbl.getSd().getLocation()));
        }
        // Create a "temp" table based off the given metastore.api.Table object. Normally,
        // the CatalogService assigns all table IDs, but in this case we need to assign the
        // "temp" table an ID locally. This table ID cannot conflict with any table in the
        // SelectStmt (or the BE will be very confused). To ensure the ID is unique within
        // this query, just assign it the invalid table ID. The CatalogServer will assign
        // this table a proper ID once it is created there as part of the CTAS execution.
        Table table = Table.fromMetastoreTable(TableId.createInvalidId(), db, msTbl);
        Preconditions.checkState(table != null && table instanceof HdfsTable);
        HdfsTable hdfsTable = (HdfsTable) table;
        hdfsTable.load(hdfsTable, client.getHiveClient(), msTbl);
        insertStmt_.setTargetTable(table);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        client.release();
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        // Subqueries need to be rewritten by the StmtRewriter first.
        if (analyzer.containsSubquery())
            return;
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    createStmt_.getColumnDefs().clear();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, null);
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE"));
    }
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient();
    try {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        // target location for the INSERT statement, it is important the two match.
        if (createStmt_.getLocation() == null) {
            createStmt_.setLocation(new HdfsUri(msTbl.getSd().getLocation()));
        }
        // Create a "temp" table based off the given metastore.api.Table object. Normally,
        // the CatalogService assigns all table IDs, but in this case we need to assign the
        // "temp" table an ID locally. This table ID cannot conflict with any table in the
        // SelectStmt (or the BE will be very confused). To ensure the ID is unique within
        // this query, just assign it the invalid table ID. The CatalogServer will assign
        // this table a proper ID once it is created there as part of the CTAS execution.
        Table table = Table.fromMetastoreTable(TableId.createInvalidId(), db, msTbl);
        Preconditions.checkState(table != null && table instanceof HdfsTable);
        HdfsTable hdfsTable = (HdfsTable) table;
        hdfsTable.load(hdfsTable, client.getHiveClient(), msTbl);
        insertStmt_.setTargetTable(table);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        client.release();
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#end_block

#method_before
@Override
public TableRef clone() {
    return new TableRef(this);
}
#method_after
@Override
protected TableRef clone() {
    return new TableRef(this);
}
#end_block

#method_before
public void reset() {
    isAnalyzed_ = false;
    resolvedPath_ = null;
    if (usingColNames_ != null) {
        onClause_ = null;
    } else if (onClause_ != null) {
        onClause_.reset();
    }
    isBroadcastJoin_ = false;
    isPartitionedJoin_ = false;
    leftTblRef_ = null;
    onClauseTupleIds_.clear();
    desc_ = null;
}
#method_after
public void reset() {
    isAnalyzed_ = false;
    resolvedPath_ = null;
    if (usingColNames_ != null) {
        // The using col names are converted into an on-clause predicate during analysis,
        // so unset the on-clause here.
        onClause_ = null;
    } else if (onClause_ != null) {
        onClause_.reset();
    }
    isBroadcastJoin_ = false;
    isPartitionedJoin_ = false;
    leftTblRef_ = null;
    onClauseTupleIds_.clear();
    desc_ = null;
}
#end_block

#method_before
@Override
public TableRef clone() {
    return new InlineViewRef(this);
}
#method_after
@Override
protected TableRef clone() {
    return new InlineViewRef(this);
}
#end_block

#method_before
protected void createSortTupleInfo(Analyzer analyzer) {
    Preconditions.checkState(evaluateOrderBy_);
    // sourceSlots contains the slots from the input row to materialize.
    Set<SlotRef> sourceSlots = Sets.newHashSet();
    TreeNode.collect(resultExprs_, Predicates.instanceOf(SlotRef.class), sourceSlots);
    TreeNode.collect(sortInfo_.getOrderingExprs(), Predicates.instanceOf(SlotRef.class), sourceSlots);
    TupleDescriptor sortTupleDesc = analyzer.getDescTbl().createTupleDescriptor("sort");
    List<Expr> sortTupleExprs = Lists.newArrayList();
    sortTupleDesc.setIsMaterialized(true);
    // substOrderBy is the mapping from slot refs in the input row to slot refs in the
    // materialized sort tuple.
    ExprSubstitutionMap substOrderBy = new ExprSubstitutionMap();
    for (SlotRef origSlotRef : sourceSlots) {
        SlotDescriptor origSlotDesc = origSlotRef.getDesc();
        SlotDescriptor materializedDesc = analyzer.addSlotDescriptor(sortTupleDesc);
        Column origColumn = origSlotDesc.getColumn();
        if (origColumn != null)
            materializedDesc.setColumn(origColumn);
        materializedDesc.setType(origSlotDesc.getType());
        materializedDesc.setLabel(origSlotDesc.getLabel());
        materializedDesc.setSourceExprs(origSlotDesc.getSourceExprs());
        materializedDesc.setStats(ColumnStats.fromExpr(origSlotRef));
        SlotRef cloneRef = new SlotRef(materializedDesc);
        substOrderBy.put(origSlotRef, cloneRef);
        analyzer.createAuxEquivPredicate(cloneRef, origSlotRef);
        sortTupleExprs.add(origSlotRef);
    }
    resultExprs_ = Expr.substituteList(resultExprs_, substOrderBy, analyzer, false);
    sortInfo_.substituteOrderingExprs(substOrderBy, analyzer);
    sortInfo_.setMaterializedTupleInfo(sortTupleDesc, sortTupleExprs);
}
#method_after
protected void createSortTupleInfo(Analyzer analyzer) {
    Preconditions.checkState(evaluateOrderBy_);
    // sourceSlots contains the slots from the input row to materialize.
    Set<SlotRef> sourceSlots = Sets.newHashSet();
    TreeNode.collect(resultExprs_, Predicates.instanceOf(SlotRef.class), sourceSlots);
    TreeNode.collect(sortInfo_.getOrderingExprs(), Predicates.instanceOf(SlotRef.class), sourceSlots);
    TupleDescriptor sortTupleDesc = analyzer.getDescTbl().createTupleDescriptor("sort");
    List<Expr> sortTupleExprs = Lists.newArrayList();
    sortTupleDesc.setIsMaterialized(true);
    // substOrderBy is the mapping from slot refs in the input row to slot refs in the
    // materialized sort tuple.
    ExprSubstitutionMap substOrderBy = new ExprSubstitutionMap();
    for (SlotRef origSlotRef : sourceSlots) {
        SlotDescriptor origSlotDesc = origSlotRef.getDesc();
        SlotDescriptor materializedDesc = analyzer.copySlotDescriptor(origSlotDesc, sortTupleDesc);
        SlotRef cloneRef = new SlotRef(materializedDesc);
        substOrderBy.put(origSlotRef, cloneRef);
        analyzer.createAuxEquivPredicate(cloneRef, origSlotRef);
        sortTupleExprs.add(origSlotRef);
    }
    resultExprs_ = Expr.substituteList(resultExprs_, substOrderBy, analyzer, false);
    sortInfo_.substituteOrderingExprs(substOrderBy, analyzer);
    sortInfo_.setMaterializedTupleInfo(sortTupleDesc, sortTupleExprs);
}
#end_block

#method_before
public ArrayList<OrderByElement> cloneOrderByElements() {
    if (orderByElements_ == null)
        return null;
    ArrayList<OrderByElement> result = Lists.newArrayListWithCapacity(orderByElements_.size());
    for (OrderByElement o : orderByElements_) {
        result.add(o.clone());
    }
    return result;
}
#method_after
public ArrayList<OrderByElement> cloneOrderByElements() {
    if (orderByElements_ == null)
        return null;
    ArrayList<OrderByElement> result = Lists.newArrayListWithCapacity(orderByElements_.size());
    for (OrderByElement o : orderByElements_) result.add(o.clone());
    return result;
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    if (orderByElements_ != null) {
        for (OrderByElement o : orderByElements_) {
            o.getExpr().reset();
        }
    }
    limitElement_.reset();
    resultExprs_.clear();
    baseTblResultExprs_.clear();
    aliasSmap_.clear();
    ambiguousAliasList_.clear();
    sortInfo_ = null;
    evaluateOrderBy_ = false;
}
#method_after
@Override
public void reset() {
    super.reset();
    if (orderByElements_ != null) {
        for (OrderByElement o : orderByElements_) o.getExpr().reset();
    }
    limitElement_.reset();
    resultExprs_.clear();
    baseTblResultExprs_.clear();
    aliasSmap_.clear();
    ambiguousAliasList_.clear();
    sortInfo_ = null;
    evaluateOrderBy_ = false;
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    isRepartition_ = null;
    resultExprs_.clear();
}
#method_after
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    isRepartition_ = null;
    resultExprs_.clear();
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    // if (isExplain_) analyzer.setIsExplain();
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Subqueries need to be rewritten.
            if (analyzer.containsSubquery())
                return;
            selectListExprs = Expr.cloneList(queryStmt_.getBaseTblResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    setTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Subqueries need to be rewritten by the StmtRewriter first.
            if (analyzer.containsSubquery())
                return;
            selectListExprs = Expr.cloneList(queryStmt_.getBaseTblResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    setTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#end_block

#method_before
public String toColumnLabel(int selectListPos, boolean useHiveColLabels) {
    if (alias_ != null)
        return alias_.toLowerCase();
    if (expr_ instanceof SlotRef) {
        SlotRef slotRef = (SlotRef) expr_;
        return slotRef.getMatchedPath().toLowerCase();
    }
    // Optionally return auto-generated column label.
    if (useHiveColLabels)
        return "_c" + selectListPos;
    // Abbreviate the toSql() for analytic exprs.
    if (expr_ instanceof AnalyticExpr) {
        AnalyticExpr expr = (AnalyticExpr) expr_;
        return expr.getFnCall().toSql() + " OVER(...)";
    }
    return expr_.toSql().toLowerCase();
}
#method_after
public String toColumnLabel(int selectListPos, boolean useHiveColLabels) {
    if (alias_ != null)
        return alias_.toLowerCase();
    if (expr_ instanceof SlotRef) {
        SlotRef slotRef = (SlotRef) expr_;
        return Joiner.on(".").join(slotRef.getResolvedPath().getRawPath());
    }
    // Optionally return auto-generated column label.
    if (useHiveColLabels)
        return "_c" + selectListPos;
    // Abbreviate the toSql() for analytic exprs.
    if (expr_ instanceof AnalyticExpr) {
        AnalyticExpr expr = (AnalyticExpr) expr_;
        return expr.getFnCall().toSql() + " OVER(...)";
    }
    return expr_.toSql().toLowerCase();
}
#end_block

#method_before
public String getPartitionName() {
    List<String> partitionCols = Lists.newArrayList();
    List<String> partitionValues = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(getTable().getColumns().get(i).getName());
    }
    return org.apache.hadoop.hive.common.FileUtils.makePartName(partitionCols, getPartitionValuesAsStrings(true));
}
#method_after
public String getPartitionName() {
    List<String> partitionCols = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(getTable().getColumns().get(i).getName());
    }
    return org.apache.hadoop.hive.common.FileUtils.makePartName(partitionCols, getPartitionValuesAsStrings(true));
}
#end_block

#method_before
public String getConjunctSql() {
    List<String> partitionCols = Lists.newArrayList();
    List<String> partitionValues = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(ToSqlUtils.getIdentSql(getTable().getColumns().get(i).getName()));
    }
    List<String> conjuncts = Lists.newArrayList();
    for (int i = 0; i < partitionCols.size(); ++i) {
        LiteralExpr expr = getPartitionValues().get(i);
        String sql = expr.toSql();
        if (expr instanceof NullLiteral || sql.isEmpty()) {
            conjuncts.add(ToSqlUtils.getIdentSql(partitionCols.get(i)) + " IS NULL");
        } else {
            conjuncts.add(ToSqlUtils.getIdentSql(partitionCols.get(i)) + "=" + sql);
        }
    }
    return "(" + Joiner.on(" AND ").join(conjuncts) + ")";
}
#method_after
public String getConjunctSql() {
    List<String> partitionCols = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(ToSqlUtils.getIdentSql(getTable().getColumns().get(i).getName()));
    }
    List<String> conjuncts = Lists.newArrayList();
    for (int i = 0; i < partitionCols.size(); ++i) {
        LiteralExpr expr = getPartitionValues().get(i);
        String sql = expr.toSql();
        if (expr instanceof NullLiteral || sql.isEmpty()) {
            conjuncts.add(ToSqlUtils.getIdentSql(partitionCols.get(i)) + " IS NULL");
        } else {
            conjuncts.add(ToSqlUtils.getIdentSql(partitionCols.get(i)) + "=" + sql);
        }
    }
    return "(" + Joiner.on(" AND ").join(conjuncts) + ")";
}
#end_block

#method_before
public void setFileFormat(HdfsFileFormat fileFormat) {
    fileFormatDescriptor_.setFileFormat(fileFormat);
}
#method_after
public void setFileFormat(HdfsFileFormat fileFormat) {
    fileFormatDescriptor_.setFileFormat(fileFormat);
    cachedMsPartitionDescriptor_.sdInputFormat = fileFormat.inputFormat();
    cachedMsPartitionDescriptor_.sdOutputFormat = fileFormat.outputFormat();
    cachedMsPartitionDescriptor_.sdSerdeInfo.setSerializationLib(fileFormatDescriptor_.getFileFormat().serializationLib());
}
#end_block

#method_before
public org.apache.hadoop.hive.metastore.api.Partition toHmsPartition() {
    if (cachedMsPartitionDescriptor_ == null)
        return null;
    Preconditions.checkNotNull(table_.getNonPartitionFieldSchemas());
    // Update the serde library class based on the currently used file format.
    cachedMsPartitionDescriptor_.sdSerdeInfo.setSerializationLib(fileFormatDescriptor_.getFileFormat().serializationLib());
    org.apache.hadoop.hive.metastore.api.StorageDescriptor storageDescriptor = new org.apache.hadoop.hive.metastore.api.StorageDescriptor(table_.getNonPartitionFieldSchemas(), location_, fileFormatDescriptor_.getFileFormat().inputFormat(), fileFormatDescriptor_.getFileFormat().outputFormat(), cachedMsPartitionDescriptor_.sdCompressed, cachedMsPartitionDescriptor_.sdNumBuckets, cachedMsPartitionDescriptor_.sdSerdeInfo, cachedMsPartitionDescriptor_.sdBucketCols, cachedMsPartitionDescriptor_.sdSortCols, cachedMsPartitionDescriptor_.sdParameters);
    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition(getPartitionValuesAsStrings(true), getTable().getDb().getName(), getTable().getName(), cachedMsPartitionDescriptor_.msCreateTime, cachedMsPartitionDescriptor_.msLastAccessTime, storageDescriptor, getParameters());
    return partition;
}
#method_after
public org.apache.hadoop.hive.metastore.api.Partition toHmsPartition() {
    if (cachedMsPartitionDescriptor_ == null)
        return null;
    Preconditions.checkNotNull(table_.getNonPartitionFieldSchemas());
    // Update the serde library class based on the currently used file format.
    org.apache.hadoop.hive.metastore.api.StorageDescriptor storageDescriptor = new org.apache.hadoop.hive.metastore.api.StorageDescriptor(table_.getNonPartitionFieldSchemas(), location_, cachedMsPartitionDescriptor_.sdInputFormat, cachedMsPartitionDescriptor_.sdOutputFormat, cachedMsPartitionDescriptor_.sdCompressed, cachedMsPartitionDescriptor_.sdNumBuckets, cachedMsPartitionDescriptor_.sdSerdeInfo, cachedMsPartitionDescriptor_.sdBucketCols, cachedMsPartitionDescriptor_.sdSortCols, cachedMsPartitionDescriptor_.sdParameters);
    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition(getPartitionValuesAsStrings(true), getTable().getDb().getName(), getTable().getName(), cachedMsPartitionDescriptor_.msCreateTime, cachedMsPartitionDescriptor_.msLastAccessTime, storageDescriptor, getParameters());
    return partition;
}
#end_block

#method_before
protected void computeNumNodes(Analyzer analyzer, long cardinality) {
    Preconditions.checkNotNull(scanRanges_);
    MembershipSnapshot cluster = MembershipSnapshot.getCluster();
    HashSet<TNetworkAddress> localHostSet = Sets.newHashSet();
    int numLocalRanges = 0;
    for (TScanRangeLocations range : scanRanges_) {
        boolean anyLocal = false;
        for (TScanRangeLocation loc : range.locations) {
            TNetworkAddress dataNode = analyzer.getHostIndex().getEntry(loc.getHost_idx());
            if (cluster.contains(dataNode)) {
                anyLocal = true;
                // Use the full datanode address (including port) to account for the test
                // minicluster where there are multiple datanodes and impalads on a single
                // host.  This assumes that when an impalad is colocated with a datanode,
                // there are the same number of impalads as datanodes on this host in this
                // cluster.
                localHostSet.add(dataNode);
            }
        }
        // will be sheduled on one of those nodes.
        if (anyLocal)
            ++numLocalRanges;
    }
    // Approximate the number of nodes that will execute locally assigned ranges to be
    // the smaller of the number of locally assigned ranges and the number of hosts
    // that hold block replica for those ranges.
    int numLocalNodes = Math.min(numLocalRanges, localHostSet.size());
    // The remote ranges are round-robined across all the impalads.
    int numRemoteRanges = scanRanges_.size() - numLocalRanges;
    int numRemoteNodes = Math.min(numRemoteRanges, cluster.numNodes());
    // The local and remote assignments may overlap, but we don't know by how much so
    // conservatively assume no overlap.
    int totalNodes = Math.min(numLocalNodes + numRemoteNodes, cluster.numNodes());
    // Tables can reside on 0 nodes (empty table), but a plan node must always be
    // executed on at least one node.
    numNodes_ = (cardinality == 0 || totalNodes == 0) ? 1 : totalNodes;
    // TODO: delete this line to use the new logic for local tables.
    if (numLocalRanges == scanRanges_.size())
        numNodes_ = tbl_.getNumNodes();
}
#method_after
protected void computeNumNodes(Analyzer analyzer, long cardinality) {
    Preconditions.checkNotNull(scanRanges_);
    MembershipSnapshot cluster = MembershipSnapshot.getCluster();
    HashSet<TNetworkAddress> localHostSet = Sets.newHashSet();
    int numLocalRanges = 0;
    for (TScanRangeLocations range : scanRanges_) {
        boolean anyLocal = false;
        for (TScanRangeLocation loc : range.locations) {
            TNetworkAddress dataNode = analyzer.getHostIndex().getEntry(loc.getHost_idx());
            if (cluster.contains(dataNode)) {
                anyLocal = true;
                // Use the full datanode address (including port) to account for the test
                // minicluster where there are multiple datanodes and impalads on a single
                // host.  This assumes that when an impalad is colocated with a datanode,
                // there are the same number of impalads as datanodes on this host in this
                // cluster.
                localHostSet.add(dataNode);
            }
        }
        // will be scheduled on one of those nodes.
        if (anyLocal)
            ++numLocalRanges;
    }
    // Approximate the number of nodes that will execute locally assigned ranges to be
    // the smaller of the number of locally assigned ranges and the number of hosts
    // that hold block replica for those ranges.
    int numLocalNodes = Math.min(numLocalRanges, localHostSet.size());
    // The remote ranges are round-robined across all the impalads.
    int numRemoteRanges = scanRanges_.size() - numLocalRanges;
    int numRemoteNodes = Math.min(numRemoteRanges, cluster.numNodes());
    // The local and remote assignments may overlap, but we don't know by how much so
    // conservatively assume no overlap.
    int totalNodes = Math.min(numLocalNodes + numRemoteNodes, cluster.numNodes());
    // Tables can reside on 0 nodes (empty table), but a plan node must always be
    // executed on at least one node.
    numNodes_ = (cardinality == 0 || totalNodes == 0) ? 1 : totalNodes;
    // TODO: delete this line to use the new logic for local tables.
    if (numLocalRanges == scanRanges_.size())
        numNodes_ = tbl_.getNumNodes();
    LOG.debug("computeNumNodes localRanges=" + numLocalRanges + " remoteRanges=" + numRemoteRanges + " localHostSet.size=" + localHostSet.size() + " clusterNodes=" + cluster.numNodes());
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    // Make sure the target table is HdfsTable.
    if (!(table_ instanceof HdfsTable)) {
        throw new AnalysisException("ALTER TABLE RECOVER PARTITIONS " + "must target an HDFS table: " + tableName_);
    }
    // Make sure the target table is partitioned.
    if (table_.getMetaStoreTable().getPartitionKeysSize() == 0) {
        throw new AnalysisException("Table is not partitioned: " + tableName_);
    }
}
#end_block

#method_before
public TDdlExecResponse execDdlRequest(TDdlExecRequest ddlRequest) throws ImpalaException {
    TDdlExecResponse response = new TDdlExecResponse();
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    User requestingUser = null;
    if (ddlRequest.isSetHeader()) {
        requestingUser = new User(ddlRequest.getHeader().getRequesting_user());
    }
    switch(ddlRequest.ddl_type) {
        case ALTER_TABLE:
            alterTable(ddlRequest.getAlter_table_params(), response);
            break;
        case ALTER_VIEW:
            alterView(ddlRequest.getAlter_view_params(), response);
            break;
        case CREATE_DATABASE:
            createDatabase(ddlRequest.getCreate_db_params(), response);
            break;
        case CREATE_TABLE_AS_SELECT:
            response.setNew_table_created(createTable(ddlRequest.getCreate_table_params(), response));
            break;
        case CREATE_TABLE:
            createTable(ddlRequest.getCreate_table_params(), response);
            break;
        case CREATE_TABLE_LIKE:
            createTableLike(ddlRequest.getCreate_table_like_params(), response);
            break;
        case CREATE_VIEW:
            createView(ddlRequest.getCreate_view_params(), response);
            break;
        case CREATE_FUNCTION:
            createFunction(ddlRequest.getCreate_fn_params(), response);
            break;
        case CREATE_DATA_SOURCE:
            createDataSource(ddlRequest.getCreate_data_source_params(), response);
            break;
        case COMPUTE_STATS:
            Preconditions.checkState(false, "Compute stats should trigger an ALTER TABLE.");
            break;
        case DROP_STATS:
            dropStats(ddlRequest.getDrop_stats_params(), response);
            break;
        case DROP_DATABASE:
            dropDatabase(ddlRequest.getDrop_db_params(), response);
            break;
        case DROP_TABLE:
        case DROP_VIEW:
            dropTableOrView(ddlRequest.getDrop_table_or_view_params(), response);
            break;
        case DROP_FUNCTION:
            dropFunction(ddlRequest.getDrop_fn_params(), response);
            break;
        case DROP_DATA_SOURCE:
            dropDataSource(ddlRequest.getDrop_data_source_params(), response);
            break;
        case CREATE_ROLE:
        case DROP_ROLE:
            createDropRole(requestingUser, ddlRequest.getCreate_drop_role_params(), response);
            break;
        case GRANT_ROLE:
        case REVOKE_ROLE:
            grantRevokeRoleGroup(requestingUser, ddlRequest.getGrant_revoke_role_params(), response);
            break;
        case GRANT_PRIVILEGE:
        case REVOKE_PRIVILEGE:
            grantRevokeRolePrivilege(requestingUser, ddlRequest.getGrant_revoke_priv_params(), response);
            break;
        default:
            throw new IllegalStateException("Unexpected DDL exec request type: " + ddlRequest.ddl_type);
    }
    // At this point, the operation is considered successful. If any errors occurred
    // during execution, this function will throw an exception and the CatalogServer
    // will handle setting a bad status code.
    response.getResult().setStatus(new TStatus(TStatusCode.OK, new ArrayList<String>()));
    return response;
}
#method_after
public TDdlExecResponse execDdlRequest(TDdlExecRequest ddlRequest) throws ImpalaException {
    TDdlExecResponse response = new TDdlExecResponse();
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    User requestingUser = null;
    if (ddlRequest.isSetHeader()) {
        requestingUser = new User(ddlRequest.getHeader().getRequesting_user());
    }
    switch(ddlRequest.ddl_type) {
        case ALTER_TABLE:
            alterTable(ddlRequest.getAlter_table_params(), response);
            break;
        case ALTER_VIEW:
            alterView(ddlRequest.getAlter_view_params(), response);
            break;
        case CREATE_DATABASE:
            createDatabase(ddlRequest.getCreate_db_params(), response);
            break;
        case CREATE_TABLE_AS_SELECT:
            response.setNew_table_created(createTable(ddlRequest.getCreate_table_params(), response));
            break;
        case CREATE_TABLE:
            createTable(ddlRequest.getCreate_table_params(), response);
            break;
        case CREATE_TABLE_LIKE:
            createTableLike(ddlRequest.getCreate_table_like_params(), response);
            break;
        case CREATE_VIEW:
            createView(ddlRequest.getCreate_view_params(), response);
            break;
        case CREATE_FUNCTION:
            createFunction(ddlRequest.getCreate_fn_params(), response);
            break;
        case CREATE_DATA_SOURCE:
            createDataSource(ddlRequest.getCreate_data_source_params(), response);
            break;
        case COMPUTE_STATS:
            Preconditions.checkState(false, "Compute stats should trigger an ALTER TABLE.");
            break;
        case DROP_STATS:
            dropStats(ddlRequest.getDrop_stats_params(), response);
            break;
        case DROP_DATABASE:
            dropDatabase(ddlRequest.getDrop_db_params(), response);
            break;
        case DROP_TABLE:
        case DROP_VIEW:
            dropTableOrView(ddlRequest.getDrop_table_or_view_params(), response);
            break;
        case TRUNCATE_TABLE:
            truncateTable(ddlRequest.getTruncate_params(), response);
            break;
        case DROP_FUNCTION:
            dropFunction(ddlRequest.getDrop_fn_params(), response);
            break;
        case DROP_DATA_SOURCE:
            dropDataSource(ddlRequest.getDrop_data_source_params(), response);
            break;
        case CREATE_ROLE:
        case DROP_ROLE:
            createDropRole(requestingUser, ddlRequest.getCreate_drop_role_params(), response);
            break;
        case GRANT_ROLE:
        case REVOKE_ROLE:
            grantRevokeRoleGroup(requestingUser, ddlRequest.getGrant_revoke_role_params(), response);
            break;
        case GRANT_PRIVILEGE:
        case REVOKE_PRIVILEGE:
            grantRevokeRolePrivilege(requestingUser, ddlRequest.getGrant_revoke_priv_params(), response);
            break;
        default:
            throw new IllegalStateException("Unexpected DDL exec request type: " + ddlRequest.ddl_type);
    }
    // At this point, the operation is considered successful. If any errors occurred
    // during execution, this function will throw an exception and the CatalogServer
    // will handle setting a bad status code.
    response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    return response;
}
#end_block

#method_before
private void alterTableUpdateStats(TAlterTableUpdateStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkState(params.isSetPartition_stats() && params.isSetTable_stats());
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    LOG.info(String.format("Updating table stats for: %s", tableName));
    Table table = getExistingTable(tableName.getDb(), tableName.getTbl());
    // Deep copy the msTbl to avoid updating our cache before successfully persisting
    // the results to the metastore.
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
    List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
    if (table instanceof HdfsTable) {
        // Fill the msPartitions from the the cached metadata.
        HdfsTable hdfsTable = (HdfsTable) table;
        for (HdfsPartition p : hdfsTable.getPartitions()) {
            if (p.getMetaStorePartition() != null) {
                msPartitions.add(p.getMetaStorePartition());
            }
        }
    }
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    int numTargetedPartitions;
    int numUpdatedColumns = 0;
    try {
        // Update the table and partition row counts based on the query results.
        List<org.apache.hadoop.hive.metastore.api.Partition> modifiedParts = Lists.newArrayList();
        numTargetedPartitions = updateTableStats(table, params, msTbl, msPartitions, modifiedParts);
        ColumnStatistics colStats = null;
        if (params.isSetColumn_stats()) {
            // Create Hive column stats from the query results.
            colStats = createHiveColStats(params.getColumn_stats(), table);
            numUpdatedColumns = colStats.getStatsObjSize();
        }
        // Update all partitions.
        bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
        synchronized (metastoreDdlLock_) {
            if (numUpdatedColumns > 0) {
                Preconditions.checkNotNull(colStats);
                // Update column stats.
                try {
                    msClient.getHiveClient().updateTableColumnStatistics(colStats);
                } catch (Exception e) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "updateTableColumnStatistics"), e);
                }
            }
            // Update the table stats. Apply the table alteration last to ensure the
            // lastDdlTime is as accurate as possible.
            applyAlterTable(msTbl);
        }
    } finally {
        msClient.release();
    }
    // Set the results to be reported to the client.
    TResultSet resultSet = new TResultSet();
    resultSet.setSchema(new TResultSetMetadata(Lists.newArrayList(new TColumn("summary", Type.STRING.toThrift()))));
    TColumnValue resultColVal = new TColumnValue();
    resultColVal.setString_val("Updated " + numTargetedPartitions + " partition(s) and " + numUpdatedColumns + " column(s).");
    TResultRow resultRow = new TResultRow();
    resultRow.setColVals(Lists.newArrayList(resultColVal));
    resultSet.setRows(Lists.newArrayList(resultRow));
    resp.setResult_set(resultSet);
}
#method_after
private void alterTableUpdateStats(TAlterTableUpdateStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkState(params.isSetPartition_stats() && params.isSetTable_stats());
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    LOG.info(String.format("Updating table stats for: %s", tableName));
    Table table = getExistingTable(tableName.getDb(), tableName.getTbl());
    // Deep copy the msTbl to avoid updating our cache before successfully persisting
    // the results to the metastore.
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
    List<HdfsPartition> partitions = Lists.newArrayList();
    if (table instanceof HdfsTable) {
        // Build a list of non-default partitions to update.
        HdfsTable hdfsTable = (HdfsTable) table;
        for (HdfsPartition p : hdfsTable.getPartitions()) {
            if (!p.isDefaultPartition())
                partitions.add(p);
        }
    }
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    int numTargetedPartitions;
    int numUpdatedColumns = 0;
    try {
        // Update the table and partition row counts based on the query results.
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        numTargetedPartitions = updateTableStats(table, params, msTbl, partitions, modifiedParts);
        ColumnStatistics colStats = null;
        if (params.isSetColumn_stats()) {
            // Create Hive column stats from the query results.
            colStats = createHiveColStats(params.getColumn_stats(), table);
            numUpdatedColumns = colStats.getStatsObjSize();
        }
        // Update all partitions.
        bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
        synchronized (metastoreDdlLock_) {
            if (numUpdatedColumns > 0) {
                Preconditions.checkNotNull(colStats);
                // Update column stats.
                try {
                    msClient.getHiveClient().updateTableColumnStatistics(colStats);
                } catch (Exception e) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "updateTableColumnStatistics"), e);
                }
            }
            // Update the table stats. Apply the table alteration last to ensure the
            // lastDdlTime is as accurate as possible.
            applyAlterTable(msTbl);
        }
    } finally {
        msClient.release();
    }
    // Set the results to be reported to the client.
    TResultSet resultSet = new TResultSet();
    resultSet.setSchema(new TResultSetMetadata(Lists.newArrayList(new TColumn("summary", Type.STRING.toThrift()))));
    TColumnValue resultColVal = new TColumnValue();
    resultColVal.setString_val("Updated " + numTargetedPartitions + " partition(s) and " + numUpdatedColumns + " column(s).");
    TResultRow resultRow = new TResultRow();
    resultRow.setColVals(Lists.newArrayList(resultColVal));
    resultSet.setRows(Lists.newArrayList(resultRow));
    resp.setResult_set(resultSet);
}
#end_block

#method_before
private int updateTableStats(Table table, TAlterTableUpdateStatsParams params, org.apache.hadoop.hive.metastore.api.Table msTbl, List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, List<org.apache.hadoop.hive.metastore.api.Partition> modifiedParts) throws ImpalaException {
    Preconditions.checkState(params.isSetPartition_stats());
    Preconditions.checkState(params.isSetTable_stats());
    // Update the partitions' ROW_COUNT parameter.
    int numTargetedPartitions = 0;
    for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
        List<String> partitionValues = Lists.newArrayList(msPartition.getValues());
        // need to rewrite the list of partition values.
        if (table instanceof HdfsTable) {
            String nullValue = ((HdfsTable) table).getNullPartitionKeyValue();
            partitionValues.clear();
            for (String s : msPartition.getValues()) {
                if (s.equals(nullValue)) {
                    partitionValues.add("NULL");
                } else {
                    partitionValues.add(s);
                }
            }
        }
        TPartitionStats partitionStats = params.partition_stats.get(partitionValues);
        TPartitionStats existingPartStats = PartitionStatsUtil.partStatsFromParameters(msPartition.getParameters());
        if (partitionStats == null) {
            // TPartitionStats object.
            if (params.expect_all_partitions == false)
                continue;
            // If all partitions are expected, fill in any missing stats with an empty entry.
            partitionStats = new TPartitionStats();
            if (params.is_incremental) {
                partitionStats.intermediate_col_stats = Maps.newHashMap();
            }
            partitionStats.stats = new TTableStats();
            partitionStats.stats.setNum_rows(0L);
        }
        long numRows = partitionStats.stats.num_rows;
        LOG.debug(String.format("Updating stats for partition %s: numRows=%s", Joiner.on(",").join(msPartition.getValues()), numRows));
        boolean updatedPartition = false;
        // partition, or they're different.
        if (existingPartStats == null || !existingPartStats.equals(partitionStats)) {
            PartitionStatsUtil.partStatsToParameters(partitionStats, msPartition);
            updatedPartition = true;
        }
        String existingRowCount = msPartition.getParameters().get(StatsSetupConst.ROW_COUNT);
        String newRowCount = String.valueOf(numRows);
        // Update table stats
        if (existingRowCount == null || !existingRowCount.equals(newRowCount)) {
            // The existing row count value wasn't set or has changed.
            msPartition.putToParameters(StatsSetupConst.ROW_COUNT, newRowCount);
            msPartition.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
            updatedPartition = true;
        }
        if (updatedPartition) {
            ++numTargetedPartitions;
            modifiedParts.add(msPartition);
        }
    }
    // For unpartitioned tables and HBase tables report a single updated partition.
    if (table.getNumClusteringCols() == 0 || table instanceof HBaseTable) {
        numTargetedPartitions = 1;
    }
    // Update the table's ROW_COUNT parameter.
    msTbl.putToParameters(StatsSetupConst.ROW_COUNT, String.valueOf(params.getTable_stats().num_rows));
    msTbl.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
    return numTargetedPartitions;
}
#method_after
private int updateTableStats(Table table, TAlterTableUpdateStatsParams params, org.apache.hadoop.hive.metastore.api.Table msTbl, List<HdfsPartition> partitions, List<HdfsPartition> modifiedParts) throws ImpalaException {
    Preconditions.checkState(params.isSetPartition_stats());
    Preconditions.checkState(params.isSetTable_stats());
    // Update the partitions' ROW_COUNT parameter.
    int numTargetedPartitions = 0;
    for (HdfsPartition partition : partitions) {
        // NULL keys are returned as 'NULL' in the partition_stats map, so don't substitute
        // this partition's keys with Hive's replacement value.
        List<String> partitionValues = partition.getPartitionValuesAsStrings(false);
        TPartitionStats partitionStats = params.partition_stats.get(partitionValues);
        TPartitionStats existingPartStats = PartitionStatsUtil.partStatsFromParameters(partition.getParameters());
        if (partitionStats == null) {
            // TPartitionStats object.
            if (params.expect_all_partitions == false)
                continue;
            // If all partitions are expected, fill in any missing stats with an empty entry.
            partitionStats = new TPartitionStats();
            if (params.is_incremental) {
                partitionStats.intermediate_col_stats = Maps.newHashMap();
            }
            partitionStats.stats = new TTableStats();
            partitionStats.stats.setNum_rows(0L);
        }
        long numRows = partitionStats.stats.num_rows;
        LOG.debug(String.format("Updating stats for partition %s: numRows=%s", partition.getValuesAsString(), numRows));
        boolean updatedPartition = false;
        // partition, or they're different.
        if (existingPartStats == null || !existingPartStats.equals(partitionStats)) {
            PartitionStatsUtil.partStatsToParameters(partitionStats, partition);
            updatedPartition = true;
        }
        String existingRowCount = partition.getParameters().get(StatsSetupConst.ROW_COUNT);
        String newRowCount = String.valueOf(numRows);
        // Update table stats
        if (existingRowCount == null || !existingRowCount.equals(newRowCount)) {
            // The existing row count value wasn't set or has changed.
            partition.putToParameters(StatsSetupConst.ROW_COUNT, newRowCount);
            partition.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
            updatedPartition = true;
        }
        if (updatedPartition) {
            ++numTargetedPartitions;
            modifiedParts.add(partition);
        }
    }
    // For unpartitioned tables and HBase tables report a single updated partition.
    if (table.getNumClusteringCols() == 0 || table instanceof HBaseTable) {
        numTargetedPartitions = 1;
    }
    // Update the table's ROW_COUNT parameter.
    msTbl.putToParameters(StatsSetupConst.ROW_COUNT, String.valueOf(params.getTable_stats().num_rows));
    msTbl.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
    return numTargetedPartitions;
}
#end_block

#method_before
private static ColumnStatisticsData createHiveColStatsData(TColumnStats colStats, Type colType) {
    ColumnStatisticsData colStatsData = new ColumnStatisticsData();
    long ndvs = colStats.getNum_distinct_values();
    long numNulls = colStats.getNum_nulls();
    switch(colType.getPrimitiveType()) {
        case BOOLEAN:
            // TODO: Gather and set the numTrues and numFalse stats as well. The planner
            // currently does not rely on them.
            colStatsData.setBooleanStats(new BooleanColumnStatsData(1, -1, numNulls));
            break;
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case // Hive and Impala use LongColumnStatsData for timestamps.
        TIMESTAMP:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setLongStats(new LongColumnStatsData(numNulls, ndvs));
            break;
        case FLOAT:
        case DOUBLE:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDoubleStats(new DoubleColumnStatsData(numNulls, ndvs));
            break;
        case STRING:
            long maxStrLen = colStats.getMax_size();
            double avgStrLen = colStats.getAvg_size();
            colStatsData.setStringStats(new StringColumnStatsData(maxStrLen, avgStrLen, numNulls, ndvs));
            break;
        case DECIMAL:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDecimalStats(new DecimalColumnStatsData(numNulls, ndvs));
            break;
        default:
            return null;
    }
    return colStatsData;
}
#method_after
private static ColumnStatisticsData createHiveColStatsData(TColumnStats colStats, Type colType) {
    ColumnStatisticsData colStatsData = new ColumnStatisticsData();
    long ndvs = colStats.getNum_distinct_values();
    long numNulls = colStats.getNum_nulls();
    switch(colType.getPrimitiveType()) {
        case BOOLEAN:
            // TODO: Gather and set the numTrues and numFalse stats as well. The planner
            // currently does not rely on them.
            colStatsData.setBooleanStats(new BooleanColumnStatsData(1, -1, numNulls));
            break;
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case // Hive and Impala use LongColumnStatsData for timestamps.
        TIMESTAMP:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setLongStats(new LongColumnStatsData(numNulls, ndvs));
            break;
        case FLOAT:
        case DOUBLE:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDoubleStats(new DoubleColumnStatsData(numNulls, ndvs));
            break;
        case CHAR:
        case VARCHAR:
        case STRING:
            long maxStrLen = colStats.getMax_size();
            double avgStrLen = colStats.getAvg_size();
            colStatsData.setStringStats(new StringColumnStatsData(maxStrLen, avgStrLen, numNulls, ndvs));
            break;
        case DECIMAL:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDecimalStats(new DecimalColumnStatsData(numNulls, ndvs));
            break;
        default:
            return null;
    }
    return colStatsData;
}
#end_block

#method_before
private void createDatabase(TCreateDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    String dbName = params.getDb();
    Preconditions.checkState(dbName != null && !dbName.isEmpty(), "Null or empty database name passed as argument to Catalog.createDatabase");
    if (params.if_not_exists && catalog_.getDb(dbName) != null) {
        LOG.debug("Skipping database creation because " + dbName + " already exists and " + "IF NOT EXISTS was specified.");
        resp.getResult().setVersion(catalog_.getCatalogVersion());
        return;
    }
    org.apache.hadoop.hive.metastore.api.Database db = new org.apache.hadoop.hive.metastore.api.Database();
    db.setName(dbName);
    if (params.getComment() != null) {
        db.setDescription(params.getComment());
    }
    if (params.getLocation() != null) {
        db.setLocationUri(params.getLocation());
    }
    LOG.debug("Creating database " + dbName);
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().createDatabase(db);
        } catch (AlreadyExistsException e) {
            if (!params.if_not_exists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating database %s because " + "IF NOT EXISTS was specified.", e, dbName));
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
        } finally {
            msClient.release();
        }
        Db newDb = catalog_.addDb(dbName);
        TCatalogObject thriftDb = new TCatalogObject(TCatalogObjectType.DATABASE, Catalog.INITIAL_CATALOG_VERSION);
        thriftDb.setDb(newDb.toThrift());
        thriftDb.setCatalog_version(newDb.getCatalogVersion());
        resp.result.setUpdated_catalog_object(thriftDb);
    }
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#method_after
private void createDatabase(TCreateDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    String dbName = params.getDb();
    Preconditions.checkState(dbName != null && !dbName.isEmpty(), "Null or empty database name passed as argument to Catalog.createDatabase");
    if (params.if_not_exists && catalog_.getDb(dbName) != null) {
        LOG.debug("Skipping database creation because " + dbName + " already exists and " + "IF NOT EXISTS was specified.");
        resp.getResult().setVersion(catalog_.getCatalogVersion());
        return;
    }
    org.apache.hadoop.hive.metastore.api.Database db = new org.apache.hadoop.hive.metastore.api.Database();
    db.setName(dbName);
    if (params.getComment() != null) {
        db.setDescription(params.getComment());
    }
    if (params.getLocation() != null) {
        db.setLocationUri(params.getLocation());
    }
    LOG.debug("Creating database " + dbName);
    Db newDb = null;
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().createDatabase(db);
            newDb = catalog_.addDb(dbName);
        } catch (AlreadyExistsException e) {
            if (!params.if_not_exists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating database %s because " + "IF NOT EXISTS was specified.", e, dbName));
            newDb = catalog_.getDb(dbName);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
        } finally {
            msClient.release();
        }
        Preconditions.checkNotNull(newDb);
        TCatalogObject thriftDb = new TCatalogObject(TCatalogObjectType.DATABASE, Catalog.INITIAL_CATALOG_VERSION);
        thriftDb.setDb(newDb.toThrift());
        thriftDb.setCatalog_version(newDb.getCatalogVersion());
        resp.result.setUpdated_catalog_object(thriftDb);
    }
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#end_block

#method_before
private void dropStats(TDropStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Table table = getExistingTable(params.getTable_name().getDb_name(), params.getTable_name().getTable_name());
    Preconditions.checkNotNull(table);
    if (params.getPartition_spec() == null) {
        // TODO: Report the number of updated partitions/columns to the user?
        dropColumnStats(table);
        dropTableStats(table);
    } else {
        HdfsPartition partition = ((HdfsTable) table).getPartitionFromThriftPartitionSpec(params.getPartition_spec());
        if (partition == null) {
            List<String> partitionDescription = Lists.newArrayList();
            for (TPartitionKeyValue v : params.getPartition_spec()) {
                partitionDescription.add(v.name + " = " + v.value);
            }
            throw new ImpalaRuntimeException("Could not find partition: " + Joiner.on("/").join(partitionDescription));
        }
        if (partition.getPartitionStats() != null) {
            synchronized (metastoreDdlLock_) {
                org.apache.hadoop.hive.metastore.api.Partition msPartition = partition.getMetaStorePartition();
                Preconditions.checkNotNull(msPartition);
                PartitionStatsUtil.deletePartStats(msPartition);
                // Remove the ROW_COUNT parameter if it has been set.
                msPartition.getParameters().remove(StatsSetupConst.ROW_COUNT);
                try {
                    applyAlterPartition(table.getTableName(), msPartition);
                } finally {
                    partition.markDirty();
                }
            }
        }
    }
    Table refreshedTable = catalog_.reloadTable(params.getTable_name());
    resp.result.setUpdated_catalog_object(TableToTCatalogObject(refreshedTable));
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#method_after
private void dropStats(TDropStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Table table = getExistingTable(params.getTable_name().getDb_name(), params.getTable_name().getTable_name());
    Preconditions.checkNotNull(table);
    if (params.getPartition_spec() == null) {
        // TODO: Report the number of updated partitions/columns to the user?
        dropColumnStats(table);
        dropTableStats(table);
    } else {
        HdfsPartition partition = ((HdfsTable) table).getPartitionFromThriftPartitionSpec(params.getPartition_spec());
        if (partition == null) {
            List<String> partitionDescription = Lists.newArrayList();
            for (TPartitionKeyValue v : params.getPartition_spec()) {
                partitionDescription.add(v.name + " = " + v.value);
            }
            throw new ImpalaRuntimeException("Could not find partition: " + Joiner.on("/").join(partitionDescription));
        }
        if (partition.getPartitionStats() != null) {
            synchronized (metastoreDdlLock_) {
                PartitionStatsUtil.deletePartStats(partition);
                try {
                    applyAlterPartition(table.getTableName(), partition);
                } finally {
                    partition.markDirty();
                }
            }
        }
    }
    Table refreshedTable = catalog_.reloadTable(params.getTable_name());
    resp.result.setUpdated_catalog_object(TableToTCatalogObject(refreshedTable));
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#end_block

#method_before
private int dropTableStats(Table table) throws ImpalaRuntimeException {
    // Delete the ROW_COUNT from the table (if it was set).
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable();
    int numTargetedPartitions = 0;
    if (msTbl.getParameters().remove(StatsSetupConst.ROW_COUNT) != null) {
        applyAlterTable(msTbl);
        ++numTargetedPartitions;
    }
    if (!(table instanceof HdfsTable) || table.getNumClusteringCols() == 0) {
        // is no more work to be done so just return.
        return numTargetedPartitions;
    }
    // Now clear the stats for all partitions in the table.
    HdfsTable hdfsTable = (HdfsTable) table;
    Preconditions.checkNotNull(hdfsTable);
    // List of partitions that were modified as part of this operation.
    List<org.apache.hadoop.hive.metastore.api.Partition> modifiedParts = Lists.newArrayList();
    for (HdfsPartition part : hdfsTable.getPartitions()) {
        boolean isModified = false;
        // represented in the Hive Metastore.
        if (part.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
            continue;
        }
        org.apache.hadoop.hive.metastore.api.Partition msPart = part.getMetaStorePartition();
        Preconditions.checkNotNull(msPart);
        if (part.getPartitionStats() != null) {
            PartitionStatsUtil.deletePartStats(msPart);
            isModified = true;
        }
        // Remove the ROW_COUNT parameter if it has been set.
        if (msPart.getParameters().remove(StatsSetupConst.ROW_COUNT) != null) {
            isModified = true;
        }
        if (isModified)
            modifiedParts.add(msPart);
    }
    bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
    return modifiedParts.size();
}
#method_after
private int dropTableStats(Table table) throws ImpalaRuntimeException {
    // Delete the ROW_COUNT from the table (if it was set).
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable();
    int numTargetedPartitions = 0;
    if (msTbl.getParameters().remove(StatsSetupConst.ROW_COUNT) != null) {
        applyAlterTable(msTbl);
        ++numTargetedPartitions;
    }
    if (!(table instanceof HdfsTable) || table.getNumClusteringCols() == 0) {
        // is no more work to be done so just return.
        return numTargetedPartitions;
    }
    // Now clear the stats for all partitions in the table.
    HdfsTable hdfsTable = (HdfsTable) table;
    Preconditions.checkNotNull(hdfsTable);
    // List of partitions that were modified as part of this operation.
    List<HdfsPartition> modifiedParts = Lists.newArrayList();
    for (HdfsPartition part : hdfsTable.getPartitions()) {
        boolean isModified = false;
        // represented in the Hive Metastore.
        if (part.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
            continue;
        }
        if (part.getPartitionStats() != null) {
            PartitionStatsUtil.deletePartStats(part);
            isModified = true;
        }
        // Remove the ROW_COUNT parameter if it has been set.
        if (part.getParameters().remove(StatsSetupConst.ROW_COUNT) != null) {
            isModified = true;
        }
        if (isModified)
            modifiedParts.add(part);
    }
    bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
    return modifiedParts.size();
}
#end_block

#method_before
private void dropTableOrView(TDropTableOrViewParams params, TDdlExecResponse resp) throws ImpalaException {
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    LOG.debug(String.format("Dropping table/view %s", tableName));
    TCatalogObject removedObject = new TCatalogObject();
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().dropTable(tableName.getDb(), tableName.getTbl(), true, params.if_exists);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropTable"), e);
        } finally {
            msClient.release();
        }
        Table table = catalog_.removeTable(params.getTable_name().db_name, params.getTable_name().table_name);
        if (table != null) {
            resp.result.setVersion(table.getCatalogVersion());
            if (table instanceof HdfsTable) {
                HdfsTable hdfsTable = (HdfsTable) table;
                if (hdfsTable.isMarkedCached()) {
                    try {
                        HdfsCachingUtil.uncacheTbl(table.getMetaStoreTable());
                    } catch (Exception e) {
                        LOG.error("Unable to uncache table: " + table.getFullName(), e);
                    }
                }
                if (table.getNumClusteringCols() > 0) {
                    for (HdfsPartition partition : hdfsTable.getPartitions()) {
                        if (partition.isMarkedCached()) {
                            try {
                                HdfsCachingUtil.uncachePartition(partition.getMetaStorePartition());
                            } catch (Exception e) {
                                LOG.error("Unable to uncache partition: " + partition.getPartitionName(), e);
                            }
                        }
                    }
                }
            }
        } else {
            resp.result.setVersion(catalog_.getCatalogVersion());
        }
    }
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(resp.result.getVersion());
    resp.result.setRemoved_catalog_object(removedObject);
}
#method_after
private void dropTableOrView(TDropTableOrViewParams params, TDdlExecResponse resp) throws ImpalaException {
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    LOG.debug(String.format("Dropping table/view %s", tableName));
    TCatalogObject removedObject = new TCatalogObject();
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().dropTable(tableName.getDb(), tableName.getTbl(), true, params.if_exists);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropTable"), e);
        } finally {
            msClient.release();
        }
        Table table = catalog_.removeTable(params.getTable_name().db_name, params.getTable_name().table_name);
        if (table != null) {
            resp.result.setVersion(table.getCatalogVersion());
            if (table instanceof HdfsTable) {
                HdfsTable hdfsTable = (HdfsTable) table;
                if (hdfsTable.isMarkedCached()) {
                    try {
                        HdfsCachingUtil.uncacheTbl(table.getMetaStoreTable());
                    } catch (Exception e) {
                        LOG.error("Unable to uncache table: " + table.getFullName(), e);
                    }
                }
                if (table.getNumClusteringCols() > 0) {
                    for (HdfsPartition partition : hdfsTable.getPartitions()) {
                        if (partition.isMarkedCached()) {
                            try {
                                HdfsCachingUtil.uncachePartition(partition);
                            } catch (Exception e) {
                                LOG.error("Unable to uncache partition: " + partition.getPartitionName(), e);
                            }
                        }
                    }
                }
            }
        } else {
            resp.result.setVersion(catalog_.getCatalogVersion());
        }
    }
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(resp.result.getVersion());
    resp.result.setRemoved_catalog_object(removedObject);
}
#end_block

#method_before
private boolean createTable(org.apache.hadoop.hive.metastore.api.Table newTable, boolean ifNotExists, THdfsCachingOp cacheOp, TDdlExecResponse response) throws ImpalaException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    synchronized (metastoreDdlLock_) {
        try {
            msClient.getHiveClient().createTable(newTable);
            // the user, an extra step is needed to read the table to find the location.
            if (cacheOp != null && cacheOp.isSet_cached() && newTable.getSd().getLocation() == null) {
                newTable = msClient.getHiveClient().getTable(newTable.getDbName(), newTable.getTableName());
            }
        } catch (AlreadyExistsException e) {
            if (!ifNotExists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createTable"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating table %s.%s because " + "IF NOT EXISTS was specified.", e, newTable.getDbName(), newTable.getTableName()));
            return false;
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createTable"), e);
        } finally {
            msClient.release();
        }
    }
    // Submit the cache request and update the table metadata.
    if (cacheOp != null && cacheOp.isSet_cached()) {
        long id = HdfsCachingUtil.submitCacheTblDirective(newTable, cacheOp.getCache_pool_name());
        catalog_.watchCacheDirs(Lists.<Long>newArrayList(id), new TTableName(newTable.getDbName(), newTable.getTableName()));
        applyAlterTable(newTable);
    }
    Table newTbl = catalog_.addTable(newTable.getDbName(), newTable.getTableName());
    response.result.setUpdated_catalog_object(TableToTCatalogObject(newTbl));
    response.result.setVersion(response.result.getUpdated_catalog_object().getCatalog_version());
    return true;
}
#method_after
private boolean createTable(org.apache.hadoop.hive.metastore.api.Table newTable, boolean ifNotExists, THdfsCachingOp cacheOp, TDdlExecResponse response) throws ImpalaException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    synchronized (metastoreDdlLock_) {
        try {
            msClient.getHiveClient().createTable(newTable);
            // the user, an extra step is needed to read the table to find the location.
            if (cacheOp != null && cacheOp.isSet_cached() && newTable.getSd().getLocation() == null) {
                newTable = msClient.getHiveClient().getTable(newTable.getDbName(), newTable.getTableName());
            }
        } catch (AlreadyExistsException e) {
            if (!ifNotExists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createTable"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating table %s.%s because " + "IF NOT EXISTS was specified.", e, newTable.getDbName(), newTable.getTableName()));
            return false;
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createTable"), e);
        } finally {
            msClient.release();
        }
    }
    // Submit the cache request and update the table metadata.
    if (cacheOp != null && cacheOp.isSet_cached()) {
        short replication = cacheOp.isSetReplication() ? cacheOp.getReplication() : JniCatalogConstants.HDFS_DEFAULT_CACHE_REPLICATION_FACTOR;
        long id = HdfsCachingUtil.submitCacheTblDirective(newTable, cacheOp.getCache_pool_name(), replication);
        catalog_.watchCacheDirs(Lists.<Long>newArrayList(id), new TTableName(newTable.getDbName(), newTable.getTableName()));
        applyAlterTable(newTable);
    }
    Table newTbl = catalog_.addTable(newTable.getDbName(), newTable.getTableName());
    response.result.setUpdated_catalog_object(TableToTCatalogObject(newTbl));
    response.result.setVersion(response.result.getUpdated_catalog_object().getCatalog_version());
    return true;
}
#end_block

#method_before
private Table alterTableAddPartition(TableName tableName, List<TPartitionKeyValue> partitionSpec, boolean ifNotExists, String location, THdfsCachingOp cacheOp) throws ImpalaException {
    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
    if (ifNotExists && catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.debug(String.format("Skipping partition creation because (%s) already exists" + " and ifNotExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        partition.setDbName(tableName.getDb());
        partition.setTableName(tableName.getTbl());
        Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirIdFromParams(msTbl.getParameters());
        List<String> values = Lists.newArrayList();
        // Need to add in the values in the same order they are defined in the table.
        for (FieldSchema fs : msTbl.getPartitionKeys()) {
            for (TPartitionKeyValue kv : partitionSpec) {
                if (fs.getName().toLowerCase().equals(kv.getName().toLowerCase())) {
                    values.add(kv.getValue());
                }
            }
        }
        partition.setValues(values);
        StorageDescriptor sd = msTbl.getSd().deepCopy();
        sd.setLocation(location);
        partition.setSd(sd);
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // Add the new partition.
            partition = msClient.getHiveClient().add_partition(partition);
            String cachePoolName = null;
            if (cacheOp == null && parentTblCacheDirId != null) {
                // The user didn't specify an explicit caching operation, inherit the value
                // from the parent table.
                cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
            } else if (cacheOp != null && cacheOp.isSet_cached()) {
                // The explicitly stated that this partition should be cached.
                cachePoolName = cacheOp.getCache_pool_name();
            }
            // If cache pool name is not null, it indicates this partition should be cached.
            if (cachePoolName != null) {
                long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName);
                catalog_.watchCacheDirs(Lists.<Long>newArrayList(id), tableName.toThrift());
                // Update the partition metadata to include the cache directive id.
                msClient.getHiveClient().alter_partition(partition.getDbName(), partition.getTableName(), partition);
            }
            updateLastDdlTime(msTbl, msClient);
        } catch (AlreadyExistsException e) {
            if (!ifNotExists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when adding partition to %s because" + " ifNotExists is true.", e, tableName));
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
        } finally {
            msClient.release();
        }
    }
    // version.
    return addHdfsPartition(tableName, partition);
}
#method_after
private Table alterTableAddPartition(TableName tableName, List<TPartitionKeyValue> partitionSpec, boolean ifNotExists, String location, THdfsCachingOp cacheOp) throws ImpalaException {
    if (ifNotExists && catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.debug(String.format("Skipping partition creation because (%s) already exists" + " and ifNotExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    org.apache.hadoop.hive.metastore.api.Partition partition = null;
    Table result = null;
    List<Long> cacheIds = null;
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        partition = createHmsPartition(partitionSpec, msTbl, tableName, location);
        try {
            // Add the new partition.
            partition = msClient.getHiveClient().add_partition(partition);
            String cachePoolName = null;
            Short replication = null;
            if (cacheOp == null && parentTblCacheDirId != null) {
                // The user didn't specify an explicit caching operation, inherit the value
                // from the parent table.
                cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
                Preconditions.checkNotNull(cachePoolName);
                replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
                Preconditions.checkNotNull(replication);
            } else if (cacheOp != null && cacheOp.isSet_cached()) {
                // The user explicitly stated that this partition should be cached.
                cachePoolName = cacheOp.getCache_pool_name();
                // explicitly set, use the default value.
                if (!cacheOp.isSetReplication() && parentTblCacheDirId != null) {
                    replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
                } else {
                    replication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
                }
            }
            // If cache pool name is not null, it indicates this partition should be cached.
            if (cachePoolName != null) {
                long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
                cacheIds = Lists.<Long>newArrayList(id);
                // Update the partition metadata to include the cache directive id.
                msClient.getHiveClient().alter_partition(partition.getDbName(), partition.getTableName(), partition);
            }
            updateLastDdlTime(msTbl, msClient);
        } catch (AlreadyExistsException e) {
            if (!ifNotExists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when adding partition to %s because" + " ifNotExists is true.", e, tableName));
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
        } finally {
            msClient.release();
        }
    }
    if (cacheIds != null)
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    // Return the table object with an updated catalog version after creating the
    // partition.
    result = addHdfsPartition(tableName, partition);
    return result;
}
#end_block

#method_before
private Table alterTableDropPartition(TableName tableName, List<TPartitionKeyValue> partitionSpec, boolean ifExists) throws ImpalaException {
    if (ifExists && !catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.debug(String.format("Skipping partition drop because (%s) does not exist " + "and ifExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    HdfsPartition part = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        List<String> values = Lists.newArrayList();
        // Need to add in the values in the same order they are defined in the table.
        for (FieldSchema fs : msTbl.getPartitionKeys()) {
            for (TPartitionKeyValue kv : partitionSpec) {
                if (fs.getName().toLowerCase().equals(kv.getName().toLowerCase())) {
                    values.add(kv.getValue());
                }
            }
        }
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().dropPartition(tableName.getDb(), tableName.getTbl(), values);
            updateLastDdlTime(msTbl, msClient);
            if (part.isMarkedCached()) {
                HdfsCachingUtil.uncachePartition(part.getMetaStorePartition());
            }
        } catch (NoSuchObjectException e) {
            if (!ifExists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when dropping partition from %s because" + " ifExists is true.", e, tableName));
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
        } finally {
            msClient.release();
        }
    }
    return catalog_.dropPartition(tableName, partitionSpec);
}
#method_after
private Table alterTableDropPartition(TableName tableName, List<TPartitionKeyValue> partitionSpec, boolean ifExists) throws ImpalaException {
    if (ifExists && !catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.debug(String.format("Skipping partition drop because (%s) does not exist " + "and ifExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    HdfsPartition part = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        List<String> values = Lists.newArrayList();
        // Need to add in the values in the same order they are defined in the table.
        for (FieldSchema fs : msTbl.getPartitionKeys()) {
            for (TPartitionKeyValue kv : partitionSpec) {
                if (fs.getName().toLowerCase().equals(kv.getName().toLowerCase())) {
                    values.add(kv.getValue());
                }
            }
        }
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().dropPartition(tableName.getDb(), tableName.getTbl(), values);
            updateLastDdlTime(msTbl, msClient);
            if (part.isMarkedCached()) {
                HdfsCachingUtil.uncachePartition(part);
            }
        } catch (NoSuchObjectException e) {
            if (!ifExists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when dropping partition from %s because" + " ifExists is true.", e, tableName));
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
        } finally {
            msClient.release();
        }
    }
    return catalog_.dropPartition(tableName, partitionSpec);
}
#end_block

#method_before
private void alterTableOrViewRename(TableName tableName, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        msTbl.setDbName(newTableName.getDb());
        msTbl.setTableName(newTableName.getTbl());
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
        } finally {
            msClient.release();
        }
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object(removedObject);
    response.result.setUpdated_catalog_object(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#method_after
private void alterTableOrViewRename(TableName tableName, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        msTbl.setDbName(newTableName.getDb());
        msTbl.setTableName(newTableName.getTbl());
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column stats
            // across databases, we save, drop and restore the column stats because the HMS
            // does not properly move them to the new table via alteration. The following
            // block needs to be protected by the metastoreDdlLock_ to avoid conflicts with
            // concurrent DDL on this same table (e.g., drop+add table with same db/name).
            ColumnStatistics hmsColStats = null;
            if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
                Table oldTbl = getExistingTable(tableName.getDb(), tableName.getTbl());
                Map<String, TColumnStats> colStats = Maps.newHashMap();
                for (Column c : oldTbl.getColumns()) {
                    colStats.put(c.getName(), c.getStats().toThrift());
                }
                hmsColStats = createHiveColStats(colStats, oldTbl);
                // Set the new db/table.
                hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
                LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
                // Delete all column stats of the original table from the HMS.
                msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
            }
            // Perform the table rename in any case.
            msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
            if (hmsColStats != null) {
                LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
                msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
            }
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
        } finally {
            msClient.release();
        }
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object(removedObject);
    response.result.setUpdated_catalog_object(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#end_block

#method_before
private void alterTableSetFileFormat(TableName tableName, List<TPartitionKeyValue> partitionSpec, THdfsFileFormat fileFormat) throws ImpalaException {
    Preconditions.checkState(partitionSpec == null || !partitionSpec.isEmpty());
    if (partitionSpec == null) {
        synchronized (metastoreDdlLock_) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
            setStorageDescriptorFileFormat(msTbl.getSd(), fileFormat);
            applyAlterTable(msTbl);
        }
    } else {
        synchronized (metastoreDdlLock_) {
            HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
            org.apache.hadoop.hive.metastore.api.Partition msPartition = partition.getMetaStorePartition();
            Preconditions.checkNotNull(msPartition);
            setStorageDescriptorFileFormat(msPartition.getSd(), fileFormat);
            try {
                applyAlterPartition(tableName, msPartition);
            } finally {
                partition.markDirty();
            }
        }
    }
}
#method_after
private void alterTableSetFileFormat(TableName tableName, List<TPartitionKeyValue> partitionSpec, THdfsFileFormat fileFormat) throws ImpalaException {
    Preconditions.checkState(partitionSpec == null || !partitionSpec.isEmpty());
    if (partitionSpec == null) {
        synchronized (metastoreDdlLock_) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
            setStorageDescriptorFileFormat(msTbl.getSd(), fileFormat);
            applyAlterTable(msTbl);
        }
    } else {
        synchronized (metastoreDdlLock_) {
            HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
            Preconditions.checkNotNull(partition);
            partition.setFileFormat(HdfsFileFormat.fromThrift(fileFormat));
            try {
                applyAlterPartition(tableName, partition);
            } finally {
                partition.markDirty();
            }
        }
    }
}
#end_block

#method_before
private void alterTableSetLocation(TableName tableName, List<TPartitionKeyValue> partitionSpec, String location) throws ImpalaException {
    Preconditions.checkState(partitionSpec == null || !partitionSpec.isEmpty());
    if (partitionSpec == null) {
        synchronized (metastoreDdlLock_) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
            msTbl.getSd().setLocation(location);
            applyAlterTable(msTbl);
        }
    } else {
        synchronized (metastoreDdlLock_) {
            HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
            org.apache.hadoop.hive.metastore.api.Partition msPartition = partition.getMetaStorePartition();
            Preconditions.checkNotNull(msPartition);
            msPartition.getSd().setLocation(location);
            try {
                applyAlterPartition(tableName, msPartition);
            } finally {
                partition.markDirty();
            }
        }
    }
}
#method_after
private void alterTableSetLocation(TableName tableName, List<TPartitionKeyValue> partitionSpec, String location) throws ImpalaException {
    Preconditions.checkState(partitionSpec == null || !partitionSpec.isEmpty());
    if (partitionSpec == null) {
        synchronized (metastoreDdlLock_) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
            msTbl.getSd().setLocation(location);
            applyAlterTable(msTbl);
        }
    } else {
        synchronized (metastoreDdlLock_) {
            HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
            partition.setLocation(location);
            try {
                applyAlterPartition(tableName, partition);
            } finally {
                partition.markDirty();
            }
        }
    }
}
#end_block

#method_before
private void alterTableSetTblProperties(TableName tableName, TAlterTableSetTblPropertiesParams params) throws ImpalaException {
    Map<String, String> properties = params.getProperties();
    Preconditions.checkNotNull(properties);
    synchronized (metastoreDdlLock_) {
        if (params.isSetPartition_spec()) {
            // Alter partition params.
            HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), params.getPartition_spec());
            org.apache.hadoop.hive.metastore.api.Partition msPartition = partition.getMetaStorePartition();
            Preconditions.checkNotNull(msPartition);
            switch(params.getTarget()) {
                case TBL_PROPERTY:
                    msPartition.getParameters().putAll(properties);
                    break;
                case SERDE_PROPERTY:
                    msPartition.getSd().getSerdeInfo().getParameters().putAll(properties);
                    break;
                default:
                    throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
            }
            try {
                applyAlterPartition(tableName, msPartition);
            } finally {
                partition.markDirty();
            }
        } else {
            // Alter table params.
            org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
            switch(params.getTarget()) {
                case TBL_PROPERTY:
                    msTbl.getParameters().putAll(properties);
                    break;
                case SERDE_PROPERTY:
                    msTbl.getSd().getSerdeInfo().getParameters().putAll(properties);
                    break;
                default:
                    throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
            }
            applyAlterTable(msTbl);
        }
    }
}
#method_after
private void alterTableSetTblProperties(TableName tableName, TAlterTableSetTblPropertiesParams params) throws ImpalaException {
    Map<String, String> properties = params.getProperties();
    Preconditions.checkNotNull(properties);
    synchronized (metastoreDdlLock_) {
        if (params.isSetPartition_spec()) {
            // Alter partition params.
            HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), params.getPartition_spec());
            switch(params.getTarget()) {
                case TBL_PROPERTY:
                    partition.getParameters().putAll(properties);
                    break;
                case SERDE_PROPERTY:
                    partition.getSerdeInfo().getParameters().putAll(properties);
                    break;
                default:
                    throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
            }
            try {
                applyAlterPartition(tableName, partition);
            } finally {
                partition.markDirty();
            }
        } else {
            // Alter table params.
            org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
            switch(params.getTarget()) {
                case TBL_PROPERTY:
                    msTbl.getParameters().putAll(properties);
                    break;
                case SERDE_PROPERTY:
                    msTbl.getSd().getSerdeInfo().getParameters().putAll(properties);
                    break;
                default:
                    throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
            }
            applyAlterTable(msTbl);
        }
    }
}
#end_block

#method_before
private void alterTableSetCached(TableName tableName, TAlterTableSetCachedParams params) throws ImpalaException {
    THdfsCachingOp cacheOp = params.getCache_op();
    Preconditions.checkNotNull(cacheOp);
    // Alter table params.
    Table table = getExistingTable(tableName.getDb(), tableName.getTbl());
    if (!(table instanceof HdfsTable)) {
        throw new ImpalaRuntimeException("ALTER TABLE SET CACHED/UNCACHED must target " + "an HDFS table.");
    }
    HdfsTable hdfsTable = (HdfsTable) table;
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable();
    Long cacheDirId = HdfsCachingUtil.getCacheDirIdFromParams(msTbl.getParameters());
    if (cacheOp.isSet_cached()) {
        // List of cache directive IDs that were submitted as part of this
        // ALTER TABLE operation.
        List<Long> cacheDirIds = Lists.newArrayList();
        if (cacheDirId == null) {
            // Table was not already cached.
            cacheDirIds.add(HdfsCachingUtil.submitCacheTblDirective(msTbl, cacheOp.getCache_pool_name()));
        } else {
            // Table is already cached, verify the pool name doesn't conflict.
            String pool = HdfsCachingUtil.getCachePool(cacheDirId);
            if (!cacheOp.getCache_pool_name().equals(pool)) {
                throw new ImpalaRuntimeException(String.format("Cannot cache table in " + "pool '%s' because it is already cached in pool '%s'. To change the " + "pool for this table, first uncache using: ALTER TABLE %s.%s SET UNCACHED", cacheOp.getCache_pool_name(), pool, msTbl.getDbName(), msTbl.getTableName()));
            }
        }
        if (table.getNumClusteringCols() > 0) {
            // partitions.
            for (HdfsPartition partition : hdfsTable.getPartitions()) {
                // not referred to by scan nodes.
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition msPart = partition.getMetaStorePartition();
                Preconditions.checkNotNull(msPart);
                if (!partition.isMarkedCached()) {
                    try {
                        cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(msPart, cacheOp.getCache_pool_name()));
                    } catch (ImpalaRuntimeException e) {
                        LOG.error("Unable to cache partition: " + partition.getPartitionName(), e);
                    }
                    // Update the partition metadata.
                    try {
                        applyAlterPartition(tableName, msPart);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        }
        // Nothing to do.
        if (cacheDirIds.isEmpty())
            return;
        // Submit a request to watch these cache directives. The TableLoadingMgr will
        // asynchronously refresh the table metadata once the directives complete.
        catalog_.watchCacheDirs(cacheDirIds, tableName.toThrift());
    } else {
        // Uncache the table.
        if (cacheDirId != null)
            HdfsCachingUtil.uncacheTbl(msTbl);
        // Uncache all table partitions.
        if (table.getNumClusteringCols() > 0) {
            for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition msPart = partition.getMetaStorePartition();
                Preconditions.checkNotNull(msPart);
                if (partition.isMarkedCached()) {
                    HdfsCachingUtil.uncachePartition(msPart);
                    try {
                        applyAlterPartition(tableName, msPart);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        }
    }
    // Update the table metadata.
    applyAlterTable(msTbl);
}
#method_after
private void alterTableSetCached(TableName tableName, TAlterTableSetCachedParams params) throws ImpalaException {
    THdfsCachingOp cacheOp = params.getCache_op();
    Preconditions.checkNotNull(cacheOp);
    // Alter table params.
    Table table = getExistingTable(tableName.getDb(), tableName.getTbl());
    if (!(table instanceof HdfsTable)) {
        throw new ImpalaRuntimeException("ALTER TABLE SET CACHED/UNCACHED must target " + "an HDFS table.");
    }
    HdfsTable hdfsTable = (HdfsTable) table;
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable();
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (cacheOp.isSet_cached()) {
        // List of cache directive IDs that were submitted as part of this
        // ALTER TABLE operation.
        List<Long> cacheDirIds = Lists.newArrayList();
        short cacheReplication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
        // the pool name and update the cache replication factor if necessary
        if (cacheDirId == null) {
            cacheDirIds.add(HdfsCachingUtil.submitCacheTblDirective(msTbl, cacheOp.getCache_pool_name(), cacheReplication));
        } else {
            // Check if the cache directive needs to be changed
            if (HdfsCachingUtil.isUpdateOp(cacheOp, msTbl.getParameters())) {
                HdfsCachingUtil.validateCachePool(cacheOp, cacheDirId, tableName);
                cacheDirIds.add(HdfsCachingUtil.modifyCacheDirective(cacheDirId, msTbl, cacheOp.getCache_pool_name(), cacheReplication));
            }
        }
        if (table.getNumClusteringCols() > 0) {
            // partitions.
            for (HdfsPartition partition : hdfsTable.getPartitions()) {
                // not referred to by scan nodes.
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                // needs to be updated
                if (!partition.isMarkedCached() || HdfsCachingUtil.isUpdateOp(cacheOp, partition.getParameters())) {
                    try {
                        // issue new cache directive
                        if (!partition.isMarkedCached()) {
                            cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(partition, cacheOp.getCache_pool_name(), cacheReplication));
                        } else {
                            Long directiveId = HdfsCachingUtil.getCacheDirectiveId(partition.getParameters());
                            cacheDirIds.add(HdfsCachingUtil.modifyCacheDirective(directiveId, partition, cacheOp.getCache_pool_name(), cacheReplication));
                        }
                    } catch (ImpalaRuntimeException e) {
                        if (partition.isMarkedCached()) {
                            LOG.error("Unable to modify cache partition: " + partition.getPartitionName(), e);
                        } else {
                            LOG.error("Unable to cache partition: " + partition.getPartitionName(), e);
                        }
                    }
                    // Update the partition metadata.
                    try {
                        applyAlterPartition(tableName, partition);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        }
        // Nothing to do.
        if (cacheDirIds.isEmpty())
            return;
        // Submit a request to watch these cache directives. The TableLoadingMgr will
        // asynchronously refresh the table metadata once the directives complete.
        catalog_.watchCacheDirs(cacheDirIds, tableName.toThrift());
    } else {
        // Uncache the table.
        if (cacheDirId != null)
            HdfsCachingUtil.uncacheTbl(msTbl);
        // Uncache all table partitions.
        if (table.getNumClusteringCols() > 0) {
            for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                if (partition.isMarkedCached()) {
                    HdfsCachingUtil.uncachePartition(partition);
                    try {
                        applyAlterPartition(tableName, partition);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        }
    }
    // Update the table metadata.
    applyAlterTable(msTbl);
}
#end_block

#method_before
private void alterPartitionSetCached(TableName tableName, TAlterTableSetCachedParams params) throws ImpalaException {
    THdfsCachingOp cacheOp = params.getCache_op();
    Preconditions.checkNotNull(cacheOp);
    Preconditions.checkNotNull(params.getPartition_spec());
    synchronized (metastoreDdlLock_) {
        // Alter partition params.
        HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), params.getPartition_spec());
        org.apache.hadoop.hive.metastore.api.Partition msPartition = partition.getMetaStorePartition();
        Preconditions.checkNotNull(msPartition);
        if (cacheOp.isSet_cached()) {
            if (partition.isMarkedCached()) {
                Long cacheReq = HdfsCachingUtil.getCacheDirIdFromParams(partition.getMetaStorePartition().getParameters());
                String pool = HdfsCachingUtil.getCachePool(cacheReq);
                if (!cacheOp.getCache_pool_name().equals(pool)) {
                    throw new ImpalaRuntimeException(String.format("Cannot cache partition in " + "pool '%s' because it is already cached in '%s'. To change the cache " + "pool for this partition, first uncache using: ALTER TABLE %s.%s " + "PARTITION(%s) SET UNCACHED", cacheOp.getCache_pool_name(), pool, tableName.getDb(), tableName, partition.getPartitionName().replaceAll("/", ", ")));
                }
                // Partition is already cached. Nothing to do.
                return;
            }
            long id = HdfsCachingUtil.submitCachePartitionDirective(msPartition, cacheOp.getCache_pool_name());
            catalog_.watchCacheDirs(Lists.<Long>newArrayList(id), tableName.toThrift());
        } else {
            // Partition is not cached, just return.
            if (!partition.isMarkedCached())
                return;
            HdfsCachingUtil.uncachePartition(msPartition);
        }
        try {
            applyAlterPartition(tableName, msPartition);
        } finally {
            partition.markDirty();
        }
    }
}
#method_after
private void alterPartitionSetCached(TableName tableName, TAlterTableSetCachedParams params) throws ImpalaException {
    THdfsCachingOp cacheOp = params.getCache_op();
    Preconditions.checkNotNull(cacheOp);
    Preconditions.checkNotNull(params.getPartition_spec());
    synchronized (metastoreDdlLock_) {
        // Alter partition params.
        HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), params.getPartition_spec());
        if (cacheOp.isSet_cached()) {
            // The directive is null if the partition is not cached
            Long directiveId = HdfsCachingUtil.getCacheDirectiveId(partition.getParameters());
            short replication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
            List<Long> cacheDirs = Lists.newArrayList();
            if (directiveId == null) {
                cacheDirs.add(HdfsCachingUtil.submitCachePartitionDirective(partition, cacheOp.getCache_pool_name(), replication));
            } else {
                if (HdfsCachingUtil.isUpdateOp(cacheOp, partition.getParameters())) {
                    HdfsCachingUtil.validateCachePool(cacheOp, directiveId, tableName, partition);
                    cacheDirs.add(HdfsCachingUtil.modifyCacheDirective(directiveId, partition, cacheOp.getCache_pool_name(), replication));
                }
            }
            // until no more progress is made -- either fully cached or out of cache memory
            if (!cacheDirs.isEmpty()) {
                catalog_.watchCacheDirs(cacheDirs, tableName.toThrift());
            }
        } else {
            // Partition is not cached, just return.
            if (!partition.isMarkedCached())
                return;
            HdfsCachingUtil.uncachePartition(partition);
        }
        try {
            applyAlterPartition(tableName, partition);
        } finally {
            partition.markDirty();
        }
    }
}
#end_block

#method_before
private void alterTableRecoverPartitions(TableName tableName) throws ImpalaException {
    HdfsTable hdfsTable;
    Set<Path> partitionsNotInMs;
    Table table = catalog_.getTable(tableName.getDb(), tableName.getTbl());
    if (!(table instanceof HdfsTable)) {
        throw new CatalogException("Table " + table.getFullName() + " is not an HDFS table");
    }
    hdfsTable = (HdfsTable) table;
    partitionsNotInMs = hdfsTable.getMissingPartitions();
    synchronized (metastoreDdlLock_) {
        // Add partitions to metastore.
        for (Path path : partitionsNotInMs) {
            List<TPartitionKeyValue> list = getTPartitionKeyValueList(hdfsTable, new Path(hdfsTable.getHdfsBaseDir()), path);
            alterTableAddPartition(tableName, list, true, null, null);
        }
    }
}
#method_after
private void alterTableRecoverPartitions(TableName tableName) throws ImpalaException {
    Table table = getExistingTable(tableName.getDb(), tableName.getTbl());
    if (!(table instanceof HdfsTable)) {
        throw new CatalogException("Table " + tableName + " is not an HDFS table");
    }
    HdfsTable hdfsTable = (HdfsTable) table;
    List<List<String>> partitionsNotInHms = hdfsTable.getPathsWithoutPartitions();
    if (partitionsNotInHms.isEmpty())
        return;
    List<Partition> hmsPartitions = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
    for (List<String> partitionSpecValues : partitionsNotInHms) {
        hmsPartitions.add(createHmsPartitionFromValues(partitionSpecValues, msTbl, tableName, null));
    }
    synchronized (metastoreDdlLock_) {
        String cachePoolName = null;
        Short replication = null;
        List<Long> cacheIds = Lists.newArrayList();
        Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
        if (parentTblCacheDirId != null) {
            // Inherit the HDFS cache value from the parent table.
            cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
            Preconditions.checkNotNull(cachePoolName);
            replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            Preconditions.checkNotNull(replication);
        }
        // Add partitions to metastore.
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // ifNotExists and needResults are true.
            hmsPartitions = msClient.getHiveClient().add_partitions(hmsPartitions, true, true);
            for (Partition partition : hmsPartitions) {
                // Create and add the HdfsPartition. Return the table object with an updated
                // catalog version.
                addHdfsPartition(tableName, partition);
            }
            // Handle HDFS cache.
            if (cachePoolName != null) {
                for (Partition partition : hmsPartitions) {
                    long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
                    cacheIds.add(id);
                }
                // Update the partition metadata to include the cache directive id.
                msClient.getHiveClient().alter_partitions(tableName.getDb(), tableName.getTbl(), hmsPartitions);
            }
            updateLastDdlTime(msTbl, msClient);
        } catch (AlreadyExistsException e) {
            // This may happen when another client of HMS has added the partitions.
            LOG.debug(String.format("Ignoring '%s' when adding partition to %s.", e, tableName));
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
        } finally {
            msClient.release();
        }
        if (!cacheIds.isEmpty()) {
            catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
        }
    }
}
#end_block

#method_before
private void applyAlterPartition(TableName tableName, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws ImpalaException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        msClient.getHiveClient().alter_partition(tableName.getDb(), tableName.getTbl(), msPartition);
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        updateLastDdlTime(msTbl, msClient);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partition"), e);
    } finally {
        msClient.release();
    }
}
#method_after
private void applyAlterPartition(TableName tableName, HdfsPartition partition) throws ImpalaException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        msClient.getHiveClient().alter_partition(tableName.getDb(), tableName.getTbl(), partition.toHmsPartition());
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        updateLastDdlTime(msTbl, msClient);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partition"), e);
    } finally {
        msClient.release();
    }
}
#end_block

#method_before
private void bulkAlterPartitions(String dbName, String tableName, List<org.apache.hadoop.hive.metastore.api.Partition> modifiedParts) throws ImpalaRuntimeException {
    MetaStoreClient msClient = null;
    try {
        msClient = catalog_.getMetaStoreClient();
        // Apply the updates in batches of 'MAX_PARTITION_UPDATES_PER_RPC'.
        for (int i = 0; i < modifiedParts.size(); i += MAX_PARTITION_UPDATES_PER_RPC) {
            int numPartitionsToUpdate = Math.min(i + MAX_PARTITION_UPDATES_PER_RPC, modifiedParts.size());
            List<org.apache.hadoop.hive.metastore.api.Partition> partsToUpdate = modifiedParts.subList(i, numPartitionsToUpdate);
            synchronized (metastoreDdlLock_) {
                try {
                    // Alter partitions in bulk.
                    msClient.getHiveClient().alter_partitions(dbName, tableName, partsToUpdate);
                } catch (TException e) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partitions"), e);
                }
            }
        }
    } finally {
        if (msClient != null)
            msClient.release();
    }
}
#method_after
private void bulkAlterPartitions(String dbName, String tableName, List<HdfsPartition> modifiedParts) throws ImpalaRuntimeException {
    MetaStoreClient msClient = null;
    List<org.apache.hadoop.hive.metastore.api.Partition> hmsPartitions = Lists.newArrayList();
    for (HdfsPartition p : modifiedParts) {
        org.apache.hadoop.hive.metastore.api.Partition msPart = p.toHmsPartition();
        if (msPart != null)
            hmsPartitions.add(msPart);
    }
    if (hmsPartitions.size() == 0)
        return;
    try {
        msClient = catalog_.getMetaStoreClient();
        // Apply the updates in batches of 'MAX_PARTITION_UPDATES_PER_RPC'.
        for (int i = 0; i < hmsPartitions.size(); i += MAX_PARTITION_UPDATES_PER_RPC) {
            int numPartitionsToUpdate = Math.min(i + MAX_PARTITION_UPDATES_PER_RPC, hmsPartitions.size());
            synchronized (metastoreDdlLock_) {
                try {
                    // Alter partitions in bulk.
                    msClient.getHiveClient().alter_partitions(dbName, tableName, hmsPartitions.subList(i, numPartitionsToUpdate));
                } catch (TException e) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partitions"), e);
                }
            }
        }
    } finally {
        if (msClient != null)
            msClient.release();
    }
}
#end_block

#method_before
public TResetMetadataResponse execResetMetadata(TResetMetadataRequest req) throws CatalogException {
    TResetMetadataResponse resp = new TResetMetadataResponse();
    resp.setResult(new TCatalogUpdateResult());
    resp.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (req.isSetTable_name()) {
        // Tracks any CatalogObjects updated/added/removed as a result of
        // the invalidate metadata or refresh call. For refresh() it is only expected
        // that a table be modified, but for invalidateTable() the table's parent database
        // may have also been added if it did not previously exist in the catalog.
        Pair<Db, Table> modifiedObjects = new Pair<Db, Table>(null, null);
        boolean wasRemoved = false;
        if (req.isIs_refresh()) {
            modifiedObjects.second = catalog_.reloadTable(req.getTable_name());
        } else {
            wasRemoved = catalog_.invalidateTable(req.getTable_name(), modifiedObjects);
        }
        if (modifiedObjects.first == null) {
            TCatalogObject thriftTable = TableToTCatalogObject(modifiedObjects.second);
            if (modifiedObjects.second != null) {
                // processed as a direct DDL operation.
                if (wasRemoved) {
                    resp.getResult().setRemoved_catalog_object(thriftTable);
                } else {
                    resp.getResult().setUpdated_catalog_object(thriftTable);
                }
            } else {
                // Table does not exist in the meta store and Impala catalog, throw error.
                throw new TableNotFoundException("Table not found: " + req.getTable_name().getDb_name() + "." + req.getTable_name().getTable_name());
            }
            resp.getResult().setVersion(thriftTable.getCatalog_version());
        } else {
            // If there were two catalog objects modified it indicates there was an
            // "invalidateTable()" call that added a new table AND database to the catalog.
            Preconditions.checkState(!req.isIs_refresh());
            Preconditions.checkNotNull(modifiedObjects.first);
            Preconditions.checkNotNull(modifiedObjects.second);
            // The database should always have a lower catalog version than the table because
            // it needs to be created before the table can be added.
            Preconditions.checkState(modifiedObjects.first.getCatalogVersion() < modifiedObjects.second.getCatalogVersion());
            // Since multiple catalog objects were modified, don't treat this as a direct DDL
            // operation. Just set the overall catalog version and the impalad will wait for
            // a statestore heartbeat that contains the update.
            resp.getResult().setVersion(modifiedObjects.second.getCatalogVersion());
        }
    } else {
        // Invalidate the entire catalog if no table name is provided.
        Preconditions.checkArgument(!req.isIs_refresh());
        catalog_.reset();
        resp.result.setVersion(catalog_.getCatalogVersion());
    }
    resp.getResult().setStatus(new TStatus(TStatusCode.OK, new ArrayList<String>()));
    return resp;
}
#method_after
public TResetMetadataResponse execResetMetadata(TResetMetadataRequest req) throws CatalogException {
    TResetMetadataResponse resp = new TResetMetadataResponse();
    resp.setResult(new TCatalogUpdateResult());
    resp.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (req.isSetTable_name()) {
        // Tracks any CatalogObjects updated/added/removed as a result of
        // the invalidate metadata or refresh call. For refresh() it is only expected
        // that a table be modified, but for invalidateTable() the table's parent database
        // may have also been added if it did not previously exist in the catalog.
        Pair<Db, Table> modifiedObjects = new Pair<Db, Table>(null, null);
        boolean wasRemoved = false;
        if (req.isIs_refresh()) {
            modifiedObjects.second = catalog_.reloadTable(req.getTable_name());
        } else {
            wasRemoved = catalog_.invalidateTable(req.getTable_name(), modifiedObjects);
        }
        if (modifiedObjects.first == null) {
            TCatalogObject thriftTable = TableToTCatalogObject(modifiedObjects.second);
            if (modifiedObjects.second != null) {
                // processed as a direct DDL operation.
                if (wasRemoved) {
                    resp.getResult().setRemoved_catalog_object(thriftTable);
                } else {
                    resp.getResult().setUpdated_catalog_object(thriftTable);
                }
            } else {
                // Table does not exist in the meta store and Impala catalog, throw error.
                throw new TableNotFoundException("Table not found: " + req.getTable_name().getDb_name() + "." + req.getTable_name().getTable_name());
            }
            resp.getResult().setVersion(thriftTable.getCatalog_version());
        } else {
            // If there were two catalog objects modified it indicates there was an
            // "invalidateTable()" call that added a new table AND database to the catalog.
            Preconditions.checkState(!req.isIs_refresh());
            Preconditions.checkNotNull(modifiedObjects.first);
            Preconditions.checkNotNull(modifiedObjects.second);
            // The database should always have a lower catalog version than the table because
            // it needs to be created before the table can be added.
            Preconditions.checkState(modifiedObjects.first.getCatalogVersion() < modifiedObjects.second.getCatalogVersion());
            // Since multiple catalog objects were modified, don't treat this as a direct DDL
            // operation. Just set the overall catalog version and the impalad will wait for
            // a statestore heartbeat that contains the update.
            resp.getResult().setVersion(modifiedObjects.second.getCatalogVersion());
        }
    } else {
        // Invalidate the entire catalog if no table name is provided.
        Preconditions.checkArgument(!req.isIs_refresh());
        catalog_.reset();
        resp.result.setVersion(catalog_.getCatalogVersion());
    }
    resp.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    return resp;
}
#end_block

#method_before
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    // Collects the cache directive IDs of any cached table/partitions that were
    // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
    // and the table will be refreshed asynchronously after all cache directives
    // complete.
    List<Long> cacheDirIds = Lists.<Long>newArrayList();
    // If the table is cached, get its cache pool name. New partitions will inherit
    // this property.
    String cachePoolName = null;
    Long cacheDirId = HdfsCachingUtil.getCacheDirIdFromParams(table.getMetaStoreTable().getParameters());
    if (cacheDirId != null) {
        cachePoolName = HdfsCachingUtil.getCachePool(cacheDirId);
        if (table.getNumClusteringCols() == 0)
            cacheDirIds.add(cacheDirId);
    }
    TableName tblName = new TableName(table.getDb().getName(), table.getName());
    AtomicBoolean addedNewPartition = new AtomicBoolean(false);
    if (table.getNumClusteringCols() > 0) {
        // Set of all partition names targeted by the insert that that need to be created
        // in the Metastore (partitions that do not currently exist in the catalog).
        // In the BE, we don't currently distinguish between which targeted partitions are
        // new and which already exist, so initialize the set with all targeted partition
        // names and remove the ones that are found to exist.
        Set<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
        for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
            // Skip dummy default partition.
            if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                continue;
            }
            // TODO: In the BE we build partition names without a trailing char. In FE we
            // build partition name with a trailing char. We should make this consistent.
            String partName = partition.getPartitionName() + "/";
            // returns true, it indicates the partition already exists.
            if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                // The partition was targeted by the insert and is also a cached. Since data
                // was written to the partition, a watch needs to be placed on the cache
                // cache directive so the TableLoadingMgr can perform an async refresh once
                // all data becomes cached.
                cacheDirIds.add(HdfsCachingUtil.getCacheDirIdFromParams(partition.getMetaStorePartition().getParameters()));
            }
            if (partsToCreate.size() == 0)
                break;
        }
        if (!partsToCreate.isEmpty()) {
            SettableFuture<Void> allFinished = SettableFuture.create();
            AtomicInteger numPartitions = new AtomicInteger(partsToCreate.size());
            // Add all partitions to metastore.
            for (String partName : partsToCreate) {
                Preconditions.checkState(partName != null && !partName.isEmpty());
                CreatePartitionRunnable rbl = new CreatePartitionRunnable(tblName, partName, cachePoolName, addedNewPartition, allFinished, numPartitions, cacheDirIds);
                executor_.execute(rbl);
            }
            try {
                // Will throw if any operation calls setException
                allFinished.get();
            } catch (Exception e) {
                throw new InternalException("Error updating metastore", e);
            }
        }
    }
    if (addedNewPartition.get()) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // Operate on a copy of msTbl to prevent our cached msTbl becoming inconsistent
            // if the alteration fails in the metastore.
            org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
            updateLastDdlTime(msTbl, msClient);
        } catch (Exception e) {
            throw new InternalException("Error updating lastDdlTime", e);
        } finally {
            msClient.release();
        }
    }
    // Submit the watch request for the given cache directives.
    if (!cacheDirIds.isEmpty())
        catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    response.getResult().setStatus(new TStatus(TStatusCode.OK, new ArrayList<String>()));
    // Perform an incremental refresh to load new/modified partitions and files.
    Table refreshedTbl = catalog_.reloadTable(tblName.toThrift());
    response.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    response.getResult().setVersion(response.getResult().getUpdated_catalog_object().getCatalog_version());
    return response;
}
#method_after
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    // Collects the cache directive IDs of any cached table/partitions that were
    // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
    // and the table will be refreshed asynchronously after all cache directives
    // complete.
    List<Long> cacheDirIds = Lists.<Long>newArrayList();
    // If the table is cached, get its cache pool name and replication factor. New
    // partitions will inherit this property.
    String cachePoolName = null;
    Short cacheReplication = 0;
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(table.getMetaStoreTable().getParameters());
    if (cacheDirId != null) {
        try {
            cachePoolName = HdfsCachingUtil.getCachePool(cacheDirId);
            cacheReplication = HdfsCachingUtil.getCacheReplication(cacheDirId);
            Preconditions.checkNotNull(cacheReplication);
            if (table.getNumClusteringCols() == 0)
                cacheDirIds.add(cacheDirId);
        } catch (ImpalaRuntimeException e) {
            // Catch the error so that the actual update to the catalog can progress,
            // this resets caching for the table though
            LOG.error(String.format("Cache directive %d was not found, uncache the table %s.%s" + "to remove this message.", cacheDirId, update.getDb_name(), update.getTarget_table()));
            cacheDirId = null;
        }
    }
    TableName tblName = new TableName(table.getDb().getName(), table.getName());
    List<String> errorMessages = Lists.newArrayList();
    if (table.getNumClusteringCols() > 0) {
        // Set of all partition names targeted by the insert that that need to be created
        // in the Metastore (partitions that do not currently exist in the catalog).
        // In the BE, we don't currently distinguish between which targeted partitions are
        // new and which already exist, so initialize the set with all targeted partition
        // names and remove the ones that are found to exist.
        Set<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
        for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
            // Skip dummy default partition.
            if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                continue;
            }
            // TODO: In the BE we build partition names without a trailing char. In FE we
            // build partition name with a trailing char. We should make this consistent.
            String partName = partition.getPartitionName() + "/";
            // returns true, it indicates the partition already exists.
            if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                // The partition was targeted by the insert and is also a cached. Since data
                // was written to the partition, a watch needs to be placed on the cache
                // cache directive so the TableLoadingMgr can perform an async refresh once
                // all data becomes cached.
                cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
            }
            if (partsToCreate.size() == 0)
                break;
        }
        if (!partsToCreate.isEmpty()) {
            MetaStoreClient msClient = catalog_.getMetaStoreClient();
            try {
                org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tblName);
                List<org.apache.hadoop.hive.metastore.api.Partition> hmsParts = Lists.newArrayList();
                HiveConf hiveConf = new HiveConf(this.getClass());
                Warehouse warehouse = new Warehouse(hiveConf);
                for (String partName : partsToCreate) {
                    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
                    hmsParts.add(partition);
                    partition.setDbName(tblName.getDb());
                    partition.setTableName(tblName.getTbl());
                    partition.setValues(getPartValsFromName(msTbl, partName));
                    partition.setParameters(new HashMap<String, String>());
                    partition.setSd(msTbl.getSd().deepCopy());
                    partition.getSd().setSerdeInfo(msTbl.getSd().getSerdeInfo().deepCopy());
                    partition.getSd().setLocation(msTbl.getSd().getLocation() + "/" + partName.substring(0, partName.length() - 1));
                    MetaStoreUtils.updatePartitionStatsFast(partition, warehouse);
                }
                // First add_partitions and then alter_partitions the successful ones with
                // caching directives. The reason is that some partitions could have been
                // added concurrently, and we want to avoid caching a partition twice and
                // leaking a caching directive.
                List<org.apache.hadoop.hive.metastore.api.Partition> addedHmsParts = msClient.getHiveClient().add_partitions(hmsParts, true, true);
                if (addedHmsParts.size() > 0) {
                    if (cachePoolName != null) {
                        List<org.apache.hadoop.hive.metastore.api.Partition> cachedHmsParts = Lists.newArrayList();
                        // the directive id.
                        for (org.apache.hadoop.hive.metastore.api.Partition part : addedHmsParts) {
                            try {
                                cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(part, cachePoolName, cacheReplication));
                                cachedHmsParts.add(part);
                            } catch (ImpalaRuntimeException e) {
                                String msg = String.format("Partition %s.%s(%s): State: Not cached." + " Action: Cache manully via 'ALTER TABLE'.", part.getDbName(), part.getTableName(), part.getValues());
                                LOG.error(msg, e);
                                errorMessages.add(msg);
                            }
                        }
                        try {
                            msClient.getHiveClient().alter_partitions(tblName.getDb(), tblName.getTbl(), cachedHmsParts);
                        } catch (Exception e) {
                            LOG.error("Failed in alter_partitions: ", e);
                            // Try to uncache the partitions when the alteration in the HMS failed.
                            for (org.apache.hadoop.hive.metastore.api.Partition part : cachedHmsParts) {
                                try {
                                    HdfsCachingUtil.uncachePartition(part);
                                } catch (ImpalaException e1) {
                                    String msg = String.format("Partition %s.%s(%s): State: Leaked caching directive. " + "Action: Manually uncache directory %s via hdfs cacheAdmin.", part.getDbName(), part.getTableName(), part.getValues(), part.getSd().getLocation());
                                    LOG.error(msg, e);
                                    errorMessages.add(msg);
                                }
                            }
                        }
                    }
                    updateLastDdlTime(msTbl, msClient);
                }
            } catch (AlreadyExistsException e) {
                throw new InternalException("AlreadyExistsException thrown although ifNotExists given", e);
            } catch (Exception e) {
                throw new InternalException("Error adding partitions", e);
            } finally {
                msClient.release();
            }
        }
    }
    // Submit the watch request for the given cache directives.
    if (!cacheDirIds.isEmpty())
        catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (errorMessages.size() > 0) {
        errorMessages.add("Please refer to the catalogd error log for details " + "regarding the failed un/caching operations.");
        response.getResult().setStatus(new TStatus(TErrorCode.INTERNAL_ERROR, errorMessages));
    } else {
        response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    }
    // Perform an incremental refresh to load new/modified partitions and files.
    Table refreshedTbl = catalog_.reloadTable(tblName.toThrift());
    response.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    response.getResult().setVersion(response.getResult().getUpdated_catalog_object().getCatalog_version());
    return response;
}
#end_block

#method_before
private void loadDiskIds(DistributedFileSystem dfs, List<BlockLocation> blockLocations, Map<String, List<FileDescriptor>> fileDescriptors) {
    // BlockStorageLocations for all the blocks
    // block described by blockMetadataList[i] is located at locations[i]
    BlockStorageLocation[] locations = null;
    try {
        // Get the BlockStorageLocations for all the blocks
        locations = dfs.getFileBlockStorageLocations(blockLocations);
    } catch (IOException e) {
        LOG.error("Couldn't determine block storage locations:\n" + e.getMessage());
        return;
    }
    if (locations == null || locations.length == 0) {
        LOG.warn("Attempted to get block locations but the call returned nulls");
        return;
    }
    if (locations.length != blockLocations.size()) {
        // blocks and locations don't match up
        LOG.error("Number of block locations not equal to number of blocks: " + "#locations=" + Long.toString(locations.length) + " #blocks=" + Long.toString(blockLocations.size()));
        return;
    }
    int locationsIdx = 0;
    int unknownDiskIdCount = 0;
    for (String parentPath : fileDescriptors.keySet()) {
        for (FileDescriptor fileDescriptor : fileDescriptors.get(parentPath)) {
            for (THdfsFileBlock blockMd : fileDescriptor.getFileBlocks()) {
                VolumeId[] volumeIds = locations[locationsIdx++].getVolumeIds();
                // Convert opaque VolumeId to 0 based ids.
                // TODO: the diskId should be eventually retrievable from Hdfs when
                // the community agrees this API is useful.
                int[] diskIds = new int[volumeIds.length];
                for (int i = 0; i < volumeIds.length; ++i) {
                    diskIds[i] = getDiskId(volumeIds[i]);
                    if (diskIds[i] < 0)
                        ++unknownDiskIdCount;
                }
                FileBlock.setDiskIds(diskIds, blockMd);
            }
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("unknown disk id count " + unknownDiskIdCount);
        }
    }
}
#method_after
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    if (!SUPPORTS_VOLUME_ID)
        return;
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        // part of the FileSystem interface, so we'll need to downcast.
        if (!(fs instanceof DistributedFileSystem))
            continue;
        LOG.trace("Loading disk ids for: " + getFullName() + ". nodes: " + getNumNodes() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        BlockStorageLocation[] storageLocs = null;
        try {
            // Get the BlockStorageLocations for all the blocks
            storageLocs = dfs.getFileBlockStorageLocations(blockLists.locations);
        } catch (IOException e) {
            LOG.error("Couldn't determine block storage locations for filesystem " + fs + ":\n" + e.getMessage());
            continue;
        }
        if (storageLocs == null || storageLocs.length == 0) {
            LOG.warn("Attempted to get block locations for filesystem " + fs + " but the call returned no results");
            continue;
        }
        if (storageLocs.length != blockLists.locations.size()) {
            // Block locations and storage locations didn't match up.
            LOG.error("Number of block storage locations not equal to number of blocks: " + "#storage locations=" + Long.toString(storageLocs.length) + " #blocks=" + Long.toString(blockLists.locations.size()));
            continue;
        }
        long unknownDiskIdCount = 0;
        // THdfsFileBlocks.
        for (int locIdx = 0; locIdx < storageLocs.length; ++locIdx) {
            VolumeId[] volumeIds = storageLocs[locIdx].getVolumeIds();
            THdfsFileBlock block = blockLists.blocks.get(locIdx);
            // Convert opaque VolumeId to 0 based ids.
            // TODO: the diskId should be eventually retrievable from Hdfs when the
            // community agrees this API is useful.
            int[] diskIds = new int[volumeIds.length];
            for (int i = 0; i < volumeIds.length; ++i) {
                diskIds[i] = getDiskId(volumeIds[i]);
                if (diskIds[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIds, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#end_block

#method_before
private void loadPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl, Map<String, List<FileDescriptor>> oldFileDescMap) throws IOException, CatalogException {
    resetPartitionMd();
    partitions_.clear();
    hdfsBaseDir_ = msTbl.getSd().getLocation();
    // Map of filesystem to parent path to a list of new/modified
    // FileDescriptors. FileDescriptors in this Map will have their block location
    // information (re)loaded. This is used to speed up the incremental refresh of a
    // table's metadata by skipping unmodified, previously loaded FileDescriptors.
    Map<FsKey, Map<String, List<FileDescriptor>>> fileDescsToLoad = Maps.newHashMap();
    // INSERT statements need to refer to this if they try to write to new partitions
    // Scans don't refer to this because by definition all partitions they refer to
    // exist.
    addDefaultPartition(msTbl.getSd());
    Long cacheDirectiveId = HdfsCachingUtil.getCacheDirIdFromParams(msTbl.getParameters());
    isMarkedCached_ = cacheDirectiveId != null;
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null, oldFileDescMap, fileDescsToLoad);
        addPartition(part);
        if (isMarkedCached_)
            part.markCached();
        Path location = new Path(hdfsBaseDir_);
        FileSystem fs = location.getFileSystem(CONF);
        if (fs.exists(location)) {
            accessLevel_ = getAvailableAccessLevel(fs, location);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, oldFileDescMap, fileDescsToLoad);
            addPartition(partition);
            // this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null)
                ;
            {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
        }
    }
    loadBlockMd(fileDescsToLoad);
}
#method_after
private void loadPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl, Map<String, List<FileDescriptor>> oldFileDescMap) throws IOException, CatalogException {
    resetPartitionMd();
    partitions_.clear();
    hdfsBaseDir_ = msTbl.getSd().getLocation();
    // Map of filesystem to the file blocks for new/modified FileDescriptors. Blocks in
    // this map will have their disk volume IDs information (re)loaded. This is used to
    // speed up the incremental refresh of a table's metadata by skipping unmodified,
    // previously loaded blocks.
    Map<FsKey, FileBlocksInfo> blocksToLoad = Maps.newHashMap();
    // INSERT statements need to refer to this if they try to write to new partitions
    // Scans don't refer to this because by definition all partitions they refer to
    // exist.
    addDefaultPartition(msTbl.getSd());
    // We silently ignore cache directives that no longer exist in HDFS, and remove
    // non-existing cache directives from the parameters.
    isMarkedCached_ = HdfsCachingUtil.validateCacheParams(msTbl.getParameters());
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null, oldFileDescMap, blocksToLoad);
        addPartition(part);
        if (isMarkedCached_)
            part.markCached();
        Path location = new Path(hdfsBaseDir_);
        FileSystem fs = location.getFileSystem(CONF);
        if (fs.exists(location)) {
            accessLevel_ = getAvailableAccessLevel(fs, location);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, oldFileDescMap, blocksToLoad);
            addPartition(partition);
            // this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null)
                ;
            {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
        }
    }
    loadDiskIds(blocksToLoad);
}
#end_block

#method_before
public HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws CatalogException {
    Map<FsKey, Map<String, List<FileDescriptor>>> fileDescsToLoad = Maps.newHashMap();
    HdfsPartition hdfsPartition = createPartition(storageDescriptor, msPartition, fileDescMap_, fileDescsToLoad);
    loadBlockMd(fileDescsToLoad);
    return hdfsPartition;
}
#method_after
public HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws CatalogException {
    Map<FsKey, FileBlocksInfo> blocksToLoad = Maps.newHashMap();
    HdfsPartition hdfsPartition = createPartition(storageDescriptor, msPartition, fileDescMap_, blocksToLoad);
    loadDiskIds(blocksToLoad);
    return hdfsPartition;
}
#end_block

#method_before
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition, Map<String, List<FileDescriptor>> oldFileDescMap, Map<FsKey, Map<String, List<FileDescriptor>>> perFsFileDescMap) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    Path partDirPath = new Path(storageDescriptor.getLocation());
    List<FileDescriptor> fileDescriptors = Lists.newArrayList();
    // If the partition is marked as cached, the block location metadata must be
    // reloaded, even if the file times have not changed.
    boolean isMarkedCached = isMarkedCached_;
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        isMarkedCached = HdfsCachingUtil.getCacheDirIdFromParams(msPartition.getParameters()) != null;
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        try {
            Expr.analyze(keyValues, null);
        } catch (AnalysisException e) {
            // should never happen
            throw new IllegalStateException(e);
        }
    }
    try {
        // Each partition could reside on a different filesystem.
        FileSystem fs = partDirPath.getFileSystem(CONF);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (fs.exists(partDirPath)) {
            // fs.listStatus() to list all the files.
            for (FileStatus fileStatus : fs.listStatus(partDirPath)) {
                String fileName = fileStatus.getPath().getName().toString();
                if (fileStatus.isDirectory() || FileSystemUtil.isHiddenFile(fileName) || HdfsCompression.fromFileName(fileName) == HdfsCompression.LZO_INDEX) {
                    // Skip index files, these are read by the LZO scanner directly.
                    continue;
                }
                String partitionDir = fileStatus.getPath().getParent().toString();
                FileDescriptor fd = null;
                // is found, it will be chosen as a candidate to reuse.
                if (oldFileDescMap != null && oldFileDescMap.get(partitionDir) != null) {
                    for (FileDescriptor oldFileDesc : oldFileDescMap.get(partitionDir)) {
                        if (oldFileDesc.getFileName().equals(fileName)) {
                            fd = oldFileDesc;
                            break;
                        }
                    }
                }
                // value can be reused.
                if (fd == null || isMarkedCached || fd.getFileLength() != fileStatus.getLen() || fd.getModificationTime() != fileStatus.getModificationTime()) {
                    // Create a new file descriptor, the block metadata will be populated by
                    // loadBlockMd.
                    fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
                    addPerFsFileDesc(perFsFileDescMap, fs, partitionDir, fd);
                }
                List<FileDescriptor> fds = fileDescMap_.get(partitionDir);
                if (fds == null) {
                    fds = Lists.newArrayList();
                    fileDescMap_.put(partitionDir, fds);
                }
                fds.add(fd);
                // Add to the list of FileDescriptors for this partition.
                fileDescriptors.add(fd);
            }
            numHdfsFiles_ += fileDescriptors.size();
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, fileDescriptors, getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (Exception e) {
        throw new CatalogException("Failed to create partition: ", e);
    }
}
#method_after
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition, Map<String, List<FileDescriptor>> oldFileDescMap, Map<FsKey, FileBlocksInfo> perFsFileBlocks) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    Path partDirPath = new Path(storageDescriptor.getLocation());
    List<FileDescriptor> fileDescriptors = Lists.newArrayList();
    // If the partition is marked as cached, the block location metadata must be
    // reloaded, even if the file times have not changed.
    boolean isMarkedCached = isMarkedCached_;
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        isMarkedCached = HdfsCachingUtil.validateCacheParams(msPartition.getParameters());
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        try {
            Expr.analyze(keyValues, null);
        } catch (AnalysisException e) {
            // should never happen
            throw new IllegalStateException(e);
        }
    }
    try {
        // Each partition could reside on a different filesystem.
        FileSystem fs = partDirPath.getFileSystem(CONF);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (fs.exists(partDirPath)) {
            // fs.listStatus() to list all the files.
            for (FileStatus fileStatus : fs.listStatus(partDirPath)) {
                String fileName = fileStatus.getPath().getName().toString();
                if (fileStatus.isDirectory() || FileSystemUtil.isHiddenFile(fileName) || HdfsCompression.fromFileName(fileName) == HdfsCompression.LZO_INDEX) {
                    // Skip index files, these are read by the LZO scanner directly.
                    continue;
                }
                String partitionDir = fileStatus.getPath().getParent().toString();
                FileDescriptor fd = null;
                // is found, it will be chosen as a candidate to reuse.
                if (oldFileDescMap != null && oldFileDescMap.get(partitionDir) != null) {
                    for (FileDescriptor oldFileDesc : oldFileDescMap.get(partitionDir)) {
                        if (oldFileDesc.getFileName().equals(fileName)) {
                            fd = oldFileDesc;
                            break;
                        }
                    }
                }
                // value can be reused.
                if (fd == null || isMarkedCached || fd.getFileLength() != fileStatus.getLen() || fd.getModificationTime() != fileStatus.getModificationTime()) {
                    // Create a new file descriptor and load the file block metadata,
                    // collecting the block metadata into perFsFileBlocks.  The disk IDs for
                    // all the blocks of each filesystem will be loaded by loadDiskIds().
                    fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
                    loadBlockMetadata(fs, fileStatus, fd, fileFormatDescriptor.getFileFormat(), perFsFileBlocks);
                }
                List<FileDescriptor> fds = fileDescMap_.get(partitionDir);
                if (fds == null) {
                    fds = Lists.newArrayList();
                    fileDescMap_.put(partitionDir, fds);
                }
                fds.add(fd);
                // Add to the list of FileDescriptors for this partition.
                fileDescriptors.add(fd);
            }
            numHdfsFiles_ += fileDescriptors.size();
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, fileDescriptors, getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (Exception e) {
        throw new CatalogException("Failed to create partition: ", e);
    }
}
#end_block

#method_before
@Override
public /**
 * Load the table metadata and reuse metadata to speed up metadata loading.
 * If the lastDdlTime has not been changed, that means the Hive metastore metadata has
 * not been changed. Reuses the old Hive partition metadata from cachedEntry.
 * To speed up Hdfs metadata loading, if a file's mtime has not been changed, reuses
 * the old file block metadata from old value.
 *
 * There are several cases where the cachedEntry might be reused incorrectly:
 * 1. an ALTER TABLE ADD PARTITION or dynamic partition insert is executed through
 *    Hive. This does not update the lastDdlTime.
 * 2. Hdfs rebalancer is executed. This changes the block locations but won't update
 *    the mtime (file modification time).
 * If any of these occurs, user has to execute "invalidate metadata" to invalidate the
 * metadata cache of the table to trigger a fresh load.
 */
void load(Table cachedEntry, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    LOG.debug("load table: " + db_.getName() + "." + name_);
    // turn all exceptions into TableLoadingException
    try {
        // set nullPartitionKeyValue from the hive conf.
        nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
        // set NULL indicator string from table properties
        nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
        if (nullColumnValue_ == null)
            nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
        // populate with both partition keys and regular columns
        List<FieldSchema> partKeys = msTbl.getPartitionKeys();
        List<FieldSchema> tblFields = Lists.newArrayList();
        String inputFormat = msTbl.getSd().getInputFormat();
        if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO) {
            // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
            // taking precedence.
            List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
            schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
            schemaSearchLocations.add(getMetaStoreTable().getParameters());
            avroSchema_ = HdfsTable.getAvroSchema(schemaSearchLocations, getFullName(), true);
            String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
            if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
                // If the SerDe library is null or set to LazySimpleSerDe or is null, it
                // indicates there is an issue with the table metadata since Avro table need a
                // non-native serde. Instead of failing to load the table, fall back to
                // using the fields from the storage descriptor (same as Hive).
                tblFields.addAll(msTbl.getSd().getCols());
            } else {
                // Load the fields from the Avro schema.
                // Since Avro does not include meta-data for CHAR or VARCHAR, an Avro type of
                // "string" is used for CHAR, VARCHAR and STRING. Default back to the storage
                // descriptor to determine the the type for "string"
                List<FieldSchema> sdTypes = msTbl.getSd().getCols();
                int i = 0;
                List<Column> avroTypeList = AvroSchemaParser.parse(avroSchema_);
                boolean canFallBack = sdTypes.size() == avroTypeList.size();
                for (Column parsedCol : avroTypeList) {
                    FieldSchema fs = new FieldSchema();
                    fs.setName(parsedCol.getName());
                    String avroType = parsedCol.getType().toSql();
                    if (avroType.toLowerCase().equals("string") && canFallBack) {
                        fs.setType(sdTypes.get(i).getType());
                    } else {
                        fs.setType(avroType);
                    }
                    fs.setComment("from deserializer");
                    tblFields.add(fs);
                    i++;
                }
            }
        } else {
            tblFields.addAll(msTbl.getSd().getCols());
        }
        List<FieldSchema> fieldSchemas = new ArrayList<FieldSchema>(partKeys.size() + tblFields.size());
        fieldSchemas.addAll(partKeys);
        fieldSchemas.addAll(tblFields);
        // The number of clustering columns is the number of partition keys.
        numClusteringCols_ = partKeys.size();
        loadColumns(fieldSchemas, client);
        // Collect the list of partitions to use for the table. Partitions may be reused
        // from the existing cached table entry (if one exists), read from the metastore,
        // or a mix of both. Whether or not a partition is reused depends on whether
        // the table or partition has been modified.
        List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
        if (cachedEntry == null || !(cachedEntry instanceof HdfsTable) || cachedEntry.lastDdlTime_ != lastDdlTime_) {
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
        } else {
            // The table was already in the metadata cache and it has not been modified.
            Preconditions.checkArgument(cachedEntry instanceof HdfsTable);
            HdfsTable cachedHdfsTableEntry = (HdfsTable) cachedEntry;
            // Set of partition names that have been modified. Partitions in this Set need to
            // be reloaded from the metastore.
            Set<String> modifiedPartitionNames = Sets.newHashSet();
            // "temp" table that doesn't actually exist in the metastore.
            if (cachedEntry != this) {
                // Since the table has not been modified, we might be able to reuse some of the
                // old partition metadata if the individual partitions have not been modified.
                // First get a list of all the partition names for this table from the
                // metastore, this is much faster than listing all the Partition objects.
                modifiedPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
            }
            int totalPartitions = modifiedPartitionNames.size();
            // Get all the partitions from the cached entry that have not been modified.
            for (HdfsPartition cachedPart : cachedHdfsTableEntry.getPartitions()) {
                // Skip the default partition and any partitions that have been modified.
                if (cachedPart.isDirty() || cachedPart.getMetaStorePartition() == null || cachedPart.getId() == DEFAULT_PARTITION_ID) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition cachedMsPart = cachedPart.getMetaStorePartition();
                Preconditions.checkNotNull(cachedMsPart);
                // This is a partition we already know about and it hasn't been modified.
                // No need to reload the metadata.
                String cachedPartName = cachedPart.getPartitionName();
                if (modifiedPartitionNames.contains(cachedPartName)) {
                    msPartitions.add(cachedMsPart);
                    modifiedPartitionNames.remove(cachedPartName);
                }
            }
            LOG.info(String.format("Incrementally refreshing %d/%d partitions.", modifiedPartitionNames.size(), totalPartitions));
            // No need to make the metastore call if no partitions are to be updated.
            if (modifiedPartitionNames.size() > 0) {
                // Now reload the the remaining partitions.
                msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(modifiedPartitionNames), db_.getName(), name_));
            }
        }
        Map<String, List<FileDescriptor>> oldFileDescMap = null;
        if (cachedEntry != null && cachedEntry instanceof HdfsTable) {
            HdfsTable cachedHdfsTable = (HdfsTable) cachedEntry;
            oldFileDescMap = cachedHdfsTable.fileDescMap_;
            hostIndex_.populate(cachedHdfsTable.hostIndex_.getList());
        }
        loadPartitions(msPartitions, msTbl, oldFileDescMap);
        // load table stats
        numRows_ = getRowCount(msTbl.getParameters());
        LOG.debug("table #rows=" + Long.toString(numRows_));
        // to the table's numRows.
        if (numClusteringCols_ == 0 && !partitions_.isEmpty()) {
            // Unpartitioned tables have a 'dummy' partition and a default partition.
            // Temp tables used in CTAS statements have one partition.
            Preconditions.checkState(partitions_.size() == 2 || partitions_.size() == 1);
            for (HdfsPartition p : partitions_) {
                p.setNumRows(numRows_);
            }
        }
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#method_after
@Override
public /**
 * Load the table metadata and reuse metadata to speed up metadata loading.
 * If the lastDdlTime has not been changed, that means the Hive metastore metadata has
 * not been changed. Reuses the old Hive partition metadata from cachedEntry.
 * To speed up Hdfs metadata loading, if a file's mtime has not been changed, reuses
 * the old file block metadata from old value.
 *
 * There are several cases where the cachedEntry might be reused incorrectly:
 * 1. an ALTER TABLE ADD PARTITION or dynamic partition insert is executed through
 *    Hive. This does not update the lastDdlTime.
 * 2. Hdfs rebalancer is executed. This changes the block locations but won't update
 *    the mtime (file modification time).
 * If any of these occurs, user has to execute "invalidate metadata" to invalidate the
 * metadata cache of the table to trigger a fresh load.
 */
void load(Table cachedEntry, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    LOG.debug("load table: " + db_.getName() + "." + name_);
    // turn all exceptions into TableLoadingException
    try {
        // set nullPartitionKeyValue from the hive conf.
        nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
        // set NULL indicator string from table properties
        nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
        if (nullColumnValue_ == null)
            nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
        String inputFormat = msTbl.getSd().getInputFormat();
        if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO) {
            // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
            // taking precedence.
            List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
            schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
            schemaSearchLocations.add(getMetaStoreTable().getParameters());
            avroSchema_ = HdfsTable.getAvroSchema(schemaSearchLocations, getFullName());
            String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
            if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
                // If the SerDe library is null or set to LazySimpleSerDe or is null, it
                // indicates there is an issue with the table metadata since Avro table need a
                // non-native serde. Instead of failing to load the table, fall back to
                // using the fields from the storage descriptor (same as Hive).
                nonPartFieldSchemas_.addAll(msTbl.getSd().getCols());
            } else {
                // Load the fields from the Avro schema.
                // Since Avro does not include meta-data for CHAR or VARCHAR, an Avro type of
                // "string" is used for CHAR, VARCHAR and STRING. Default back to the storage
                // descriptor to determine the the type for "string"
                List<FieldSchema> sdTypes = msTbl.getSd().getCols();
                int i = 0;
                List<Column> avroTypeList = AvroSchemaParser.parse(avroSchema_);
                boolean canFallBack = sdTypes.size() == avroTypeList.size();
                for (Column parsedCol : avroTypeList) {
                    FieldSchema fs = new FieldSchema();
                    fs.setName(parsedCol.getName());
                    String avroType = parsedCol.getType().toSql();
                    if (avroType.toLowerCase().equals("string") && canFallBack) {
                        fs.setType(sdTypes.get(i).getType());
                    } else {
                        fs.setType(avroType);
                    }
                    fs.setComment("from deserializer");
                    nonPartFieldSchemas_.add(fs);
                    i++;
                }
            }
        } else {
            nonPartFieldSchemas_.addAll(msTbl.getSd().getCols());
        }
        // The number of clustering columns is the number of partition keys.
        numClusteringCols_ = msTbl.getPartitionKeys().size();
        // Add all columns to the table. Ordering is important: partition columns first,
        // then all other columns.
        addColumnsFromFieldSchemas(msTbl.getPartitionKeys());
        addColumnsFromFieldSchemas(nonPartFieldSchemas_);
        loadAllColumnStats(client);
        // Collect the list of partitions to use for the table. Partitions may be reused
        // from the existing cached table entry (if one exists), read from the metastore,
        // or a mix of both. Whether or not a partition is reused depends on whether
        // the table or partition has been modified.
        List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
        if (cachedEntry == null || !(cachedEntry instanceof HdfsTable) || cachedEntry.lastDdlTime_ != lastDdlTime_) {
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
        } else {
            // The table was already in the metadata cache and it has not been modified.
            Preconditions.checkArgument(cachedEntry instanceof HdfsTable);
            HdfsTable cachedHdfsTableEntry = (HdfsTable) cachedEntry;
            // Set of partition names that have been modified. Partitions in this Set need to
            // be reloaded from the metastore.
            Set<String> modifiedPartitionNames = Sets.newHashSet();
            // "temp" table that doesn't actually exist in the metastore.
            if (cachedEntry != this) {
                // Since the table has not been modified, we might be able to reuse some of the
                // old partition metadata if the individual partitions have not been modified.
                // First get a list of all the partition names for this table from the
                // metastore, this is much faster than listing all the Partition objects.
                modifiedPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
            }
            int totalPartitions = modifiedPartitionNames.size();
            // Get all the partitions from the cached entry that have not been modified.
            for (HdfsPartition cachedPart : cachedHdfsTableEntry.getPartitions()) {
                // Skip the default partition and any partitions that have been modified.
                if (cachedPart.isDirty() || cachedPart.isDefaultPartition()) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition cachedMsPart = cachedPart.toHmsPartition();
                if (cachedMsPart == null)
                    continue;
                // This is a partition we already know about and it hasn't been modified.
                // No need to reload the metadata.
                String cachedPartName = cachedPart.getPartitionName();
                if (modifiedPartitionNames.contains(cachedPartName)) {
                    msPartitions.add(cachedMsPart);
                    modifiedPartitionNames.remove(cachedPartName);
                }
            }
            LOG.info(String.format("Incrementally refreshing %d/%d partitions.", modifiedPartitionNames.size(), totalPartitions));
            // No need to make the metastore call if no partitions are to be updated.
            if (modifiedPartitionNames.size() > 0) {
                // Now reload the the remaining partitions.
                msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(modifiedPartitionNames), db_.getName(), name_));
            }
        }
        Map<String, List<FileDescriptor>> oldFileDescMap = null;
        if (cachedEntry != null && cachedEntry instanceof HdfsTable) {
            HdfsTable cachedHdfsTable = (HdfsTable) cachedEntry;
            oldFileDescMap = cachedHdfsTable.fileDescMap_;
            hostIndex_.populate(cachedHdfsTable.hostIndex_.getList());
        }
        loadPartitions(msPartitions, msTbl, oldFileDescMap);
        // load table stats
        numRows_ = getRowCount(msTbl.getParameters());
        LOG.debug("table #rows=" + Long.toString(numRows_));
        // to the table's numRows.
        if (numClusteringCols_ == 0 && !partitions_.isEmpty()) {
            // Unpartitioned tables have a 'dummy' partition and a default partition.
            // Temp tables used in CTAS statements have one partition.
            Preconditions.checkState(partitions_.size() == 2 || partitions_.size() == 1);
            for (HdfsPartition p : partitions_) {
                p.setNumRows(numRows_);
            }
        }
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + getFullName(), e);
    }
}
#end_block

#method_before
public static String getAvroSchema(List<Map<String, String>> schemaSearchLocations, String tableName, boolean downloadSchema) throws TableLoadingException {
    String url = null;
    // Search all locations and break out on the first valid schema found.
    for (Map<String, String> schemaLocation : schemaSearchLocations) {
        if (schemaLocation == null)
            continue;
        String literal = schemaLocation.get(AvroSerdeUtils.SCHEMA_LITERAL);
        if (literal != null && !literal.equals(AvroSerdeUtils.SCHEMA_NONE))
            return literal;
        url = schemaLocation.get(AvroSerdeUtils.SCHEMA_URL);
        if (url != null) {
            url = url.trim();
            break;
        }
    }
    if (url == null || url.equals(AvroSerdeUtils.SCHEMA_NONE)) {
        throw new TableLoadingException(String.format("No Avro schema provided in " + "SERDEPROPERTIES or TBLPROPERTIES for table: %s ", tableName));
    }
    if (!url.toLowerCase().startsWith("hdfs://") && !url.toLowerCase().startsWith("http://")) {
        throw new TableLoadingException("avro.schema.url must be of form " + "\"http://path/to/schema/file\" or " + "\"hdfs://namenode:port/path/to/schema/file\", got " + url);
    }
    return downloadSchema ? loadAvroSchemaFromUrl(url) : url;
}
#method_after
public static String getAvroSchema(List<Map<String, String>> schemaSearchLocations, String tableName) throws TableLoadingException {
    String url = null;
    // Search all locations and break out on the first valid schema found.
    for (Map<String, String> schemaLocation : schemaSearchLocations) {
        if (schemaLocation == null)
            continue;
        String literal = schemaLocation.get(AvroSerdeUtils.SCHEMA_LITERAL);
        if (literal != null && !literal.equals(AvroSerdeUtils.SCHEMA_NONE))
            return literal;
        url = schemaLocation.get(AvroSerdeUtils.SCHEMA_URL);
        if (url != null) {
            url = url.trim();
            break;
        }
    }
    if (url == null || url.equals(AvroSerdeUtils.SCHEMA_NONE)) {
        throw new TableLoadingException(String.format("No Avro schema provided in " + "SERDEPROPERTIES or TBLPROPERTIES for table: %s ", tableName));
    }
    String schema = null;
    if (url.toLowerCase().startsWith("http://")) {
        InputStream urlStream = null;
        try {
            urlStream = new URL(url).openStream();
            schema = IOUtils.toString(urlStream);
        } catch (IOException e) {
            throw new TableLoadingException("Problem reading Avro schema from: " + url, e);
        } finally {
            IOUtils.closeQuietly(urlStream);
        }
    } else {
        Path path = new Path(url);
        FileSystem fs = null;
        try {
            fs = path.getFileSystem(FileSystemUtil.getConfiguration());
        } catch (Exception e) {
            throw new TableLoadingException(String.format("Invalid avro.schema.url: %s. %s", path, e.getMessage()));
        }
        StringBuilder errorMsg = new StringBuilder();
        if (!FileSystemUtil.isPathReachable(path, fs, errorMsg)) {
            throw new TableLoadingException(String.format("Invalid avro.schema.url: %s. %s", path, errorMsg));
        }
        try {
            schema = FileSystemUtil.readFile(path);
        } catch (IOException e) {
            throw new TableLoadingException("Problem reading Avro schema at: " + url, e);
        }
    }
    return schema;
}
#end_block

#method_before
@Override
protected void loadFromThrift(TTable thriftTable) throws TableLoadingException {
    super.loadFromThrift(thriftTable);
    THdfsTable hdfsTable = thriftTable.getHdfs_table();
    hdfsBaseDir_ = hdfsTable.getHdfsBaseDir();
    nullColumnValue_ = hdfsTable.nullColumnValue;
    nullPartitionKeyValue_ = hdfsTable.nullPartitionKeyValue;
    multipleFileSystems_ = hdfsTable.multiple_filesystems;
    hostIndex_.populate(hdfsTable.getNetwork_addresses());
    resetPartitionMd();
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    for (Map.Entry<Long, THdfsPartition> part : hdfsTable.getPartitions().entrySet()) {
        HdfsPartition hdfsPart = HdfsPartition.fromThrift(this, part.getKey(), part.getValue());
        numHdfsFiles_ += hdfsPart.getFileDescriptors().size();
        totalHdfsBytes_ += hdfsPart.getSize();
        partitions_.add(hdfsPart);
    }
    avroSchema_ = hdfsTable.isSetAvroSchema() ? hdfsTable.getAvroSchema() : null;
    isMarkedCached_ = HdfsCachingUtil.getCacheDirIdFromParams(getMetaStoreTable().getParameters()) != null;
    populatePartitionMd();
}
#method_after
@Override
protected void loadFromThrift(TTable thriftTable) throws TableLoadingException {
    super.loadFromThrift(thriftTable);
    THdfsTable hdfsTable = thriftTable.getHdfs_table();
    hdfsBaseDir_ = hdfsTable.getHdfsBaseDir();
    nullColumnValue_ = hdfsTable.nullColumnValue;
    nullPartitionKeyValue_ = hdfsTable.nullPartitionKeyValue;
    multipleFileSystems_ = hdfsTable.multiple_filesystems;
    hostIndex_.populate(hdfsTable.getNetwork_addresses());
    resetPartitionMd();
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    for (Map.Entry<Long, THdfsPartition> part : hdfsTable.getPartitions().entrySet()) {
        HdfsPartition hdfsPart = HdfsPartition.fromThrift(this, part.getKey(), part.getValue());
        numHdfsFiles_ += hdfsPart.getFileDescriptors().size();
        totalHdfsBytes_ += hdfsPart.getSize();
        partitions_.add(hdfsPart);
    }
    avroSchema_ = hdfsTable.isSetAvroSchema() ? hdfsTable.getAvroSchema() : null;
    isMarkedCached_ = HdfsCachingUtil.getCacheDirectiveId(getMetaStoreTable().getParameters()) != null;
    populatePartitionMd();
}
#end_block

#method_before
public TResultSet getTableStats() {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    for (int i = 0; i < numClusteringCols_; ++i) {
        // Add the partition-key values as strings for simplicity.
        Column partCol = getColumns().get(i);
        TColumn colDesc = new TColumn(partCol.getName(), Type.STRING.toThrift());
        resultSchema.addToColumns(colDesc);
    }
    resultSchema.addToColumns(new TColumn("#Rows", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("#Files", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("Size", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Bytes Cached", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Format", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Incremental stats", Type.STRING.toThrift()));
    // Pretty print partitions and their stats.
    ArrayList<HdfsPartition> orderedPartitions = Lists.newArrayList(partitions_);
    Collections.sort(orderedPartitions);
    long totalCachedBytes = 0L;
    for (HdfsPartition p : orderedPartitions) {
        // Ignore dummy default partition.
        if (p.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID)
            continue;
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        // Add the partition-key values (as strings for simplicity).
        for (LiteralExpr expr : p.getPartitionValues()) {
            rowBuilder.add(expr.getStringValue());
        }
        // Add number of rows, files, bytes, cache stats, and file format.
        rowBuilder.add(p.getNumRows()).add(p.getFileDescriptors().size()).addBytes(p.getSize());
        if (!p.isMarkedCached()) {
            // Helps to differentiate partitions that have 0B cached versus partitions
            // that are not marked as cached.
            rowBuilder.add("NOT CACHED");
        } else {
            // Calculate the number the number of bytes that are cached.
            long cachedBytes = 0L;
            for (FileDescriptor fd : p.getFileDescriptors()) {
                for (THdfsFileBlock fb : fd.getFileBlocks()) {
                    if (fb.getIs_replica_cached().contains(true)) {
                        cachedBytes += fb.getLength();
                    }
                }
            }
            totalCachedBytes += cachedBytes;
            rowBuilder.addBytes(cachedBytes);
        }
        rowBuilder.add(p.getInputFormatDescriptor().getFileFormat().toString());
        rowBuilder.add(String.valueOf(p.hasIncrementalStats()));
        result.addToRows(rowBuilder.get());
    }
    // For partitioned tables add a summary row at the bottom.
    if (numClusteringCols_ > 0) {
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        int numEmptyCells = numClusteringCols_ - 1;
        rowBuilder.add("Total");
        for (int i = 0; i < numEmptyCells; ++i) {
            rowBuilder.add("");
        }
        // Total num rows, files, and bytes (leave format empty).
        rowBuilder.add(numRows_).add(numHdfsFiles_).addBytes(totalHdfsBytes_).addBytes(totalCachedBytes).add("").add("");
        result.addToRows(rowBuilder.get());
    }
    return result;
}
#method_after
public TResultSet getTableStats() {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    for (int i = 0; i < numClusteringCols_; ++i) {
        // Add the partition-key values as strings for simplicity.
        Column partCol = getColumns().get(i);
        TColumn colDesc = new TColumn(partCol.getName(), Type.STRING.toThrift());
        resultSchema.addToColumns(colDesc);
    }
    resultSchema.addToColumns(new TColumn("#Rows", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("#Files", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("Size", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Bytes Cached", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Cache Replication", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Format", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Incremental stats", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Location", Type.STRING.toThrift()));
    // Pretty print partitions and their stats.
    ArrayList<HdfsPartition> orderedPartitions = Lists.newArrayList(partitions_);
    Collections.sort(orderedPartitions);
    long totalCachedBytes = 0L;
    for (HdfsPartition p : orderedPartitions) {
        // Ignore dummy default partition.
        if (p.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID)
            continue;
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        // Add the partition-key values (as strings for simplicity).
        for (LiteralExpr expr : p.getPartitionValues()) {
            rowBuilder.add(expr.getStringValue());
        }
        // Add number of rows, files, bytes, cache stats, and file format.
        rowBuilder.add(p.getNumRows()).add(p.getFileDescriptors().size()).addBytes(p.getSize());
        if (!p.isMarkedCached()) {
            // Helps to differentiate partitions that have 0B cached versus partitions
            // that are not marked as cached.
            rowBuilder.add("NOT CACHED");
            rowBuilder.add("NOT CACHED");
        } else {
            // Calculate the number the number of bytes that are cached.
            long cachedBytes = 0L;
            for (FileDescriptor fd : p.getFileDescriptors()) {
                for (THdfsFileBlock fb : fd.getFileBlocks()) {
                    if (fb.getIs_replica_cached().contains(true)) {
                        cachedBytes += fb.getLength();
                    }
                }
            }
            totalCachedBytes += cachedBytes;
            rowBuilder.addBytes(cachedBytes);
            // Extract cache replication factor from the parameters of the table
            // if the table is not partitioned or directly from the partition.
            Short rep = HdfsCachingUtil.getCachedCacheReplication(numClusteringCols_ == 0 ? p.getTable().getMetaStoreTable().getParameters() : p.getParameters());
            rowBuilder.add(rep.toString());
        }
        rowBuilder.add(p.getInputFormatDescriptor().getFileFormat().toString());
        rowBuilder.add(String.valueOf(p.hasIncrementalStats()));
        rowBuilder.add(p.getLocation());
        result.addToRows(rowBuilder.get());
    }
    // For partitioned tables add a summary row at the bottom.
    if (numClusteringCols_ > 0) {
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        int numEmptyCells = numClusteringCols_ - 1;
        rowBuilder.add("Total");
        for (int i = 0; i < numEmptyCells; ++i) {
            rowBuilder.add("");
        }
        // Total num rows, files, and bytes (leave format empty).
        rowBuilder.add(numRows_).add(numHdfsFiles_).addBytes(totalHdfsBytes_).addBytes(totalCachedBytes).add("").add("").add("").add("");
        result.addToRows(rowBuilder.get());
    }
    return result;
}
#end_block

#method_before
private PlanNode createJoinNode(Analyzer analyzer, PlanNode outer, PlanNode inner, TableRef outerRef, TableRef innerRef) throws ImpalaException {
    Preconditions.checkState(innerRef != null ^ outerRef != null);
    TableRef tblRef = (innerRef != null) ? innerRef : outerRef;
    List<BinaryPredicate> eqJoinConjuncts = Lists.newArrayList();
    List<Expr> eqJoinPredicates = Lists.newArrayList();
    // are materialized)
    if (innerRef != null) {
        getHashLookupJoinConjuncts(analyzer, outer.getTblRefIds(), innerRef, eqJoinConjuncts, eqJoinPredicates);
        // Outer joins should only use On-clause predicates as eqJoinConjuncts.
        if (!innerRef.getJoinOp().isOuterJoin()) {
            analyzer.createEquivConjuncts(outer.getTblRefIds(), innerRef.getId(), eqJoinConjuncts);
        }
    } else {
        getHashLookupJoinConjuncts(analyzer, inner.getTblRefIds(), outerRef, eqJoinConjuncts, eqJoinPredicates);
        // Outer joins should only use On-clause predicates as eqJoinConjuncts.
        if (!outerRef.getJoinOp().isOuterJoin()) {
            analyzer.createEquivConjuncts(inner.getTblRefIds(), outerRef.getId(), eqJoinConjuncts);
        }
        // Reverse the lhs/rhs of the join conjuncts.
        for (BinaryPredicate eqJoinConjunct : eqJoinConjuncts) {
            Expr swapTmp = eqJoinConjunct.getChild(0);
            eqJoinConjunct.setChild(0, eqJoinConjunct.getChild(1));
            eqJoinConjunct.setChild(1, swapTmp);
        }
    }
    // Handle implicit cross joins
    if (eqJoinConjuncts.isEmpty()) {
        // the otherJoinConjuncts.
        if (tblRef.getJoinOp().isOuterJoin() || tblRef.getJoinOp().isSemiJoin()) {
            throw new NotImplementedException(String.format("%s join with '%s' without equi-join " + "conjuncts is not supported.", tblRef.getJoinOp().isOuterJoin() ? "Outer" : "Semi", innerRef.getUniqueAlias()));
        }
        CrossJoinNode result = new CrossJoinNode(outer, inner, tblRef, Collections.EMPTY_LIST);
        result.init(analyzer);
        return result;
    }
    // Handle explicit cross joins with equi join conditions
    if (tblRef.getJoinOp() == JoinOperator.CROSS_JOIN) {
        tblRef.setJoinOp(JoinOperator.INNER_JOIN);
    }
    analyzer.markConjunctsAssigned(eqJoinPredicates);
    List<Expr> otherJoinConjuncts = Lists.newArrayList();
    if (tblRef.getJoinOp().isOuterJoin()) {
        // Also assign conjuncts from On clause. All remaining unassigned conjuncts
        // that can be evaluated by this join are assigned in createSelectPlan().
        otherJoinConjuncts = analyzer.getUnassignedOjConjuncts(tblRef);
    } else if (tblRef.getJoinOp().isSemiJoin()) {
        // Unassigned conjuncts bound by the invisible tuple id of a semi join must have
        // come from the join's On-clause, and therefore, must be added to the other join
        // conjuncts to produce correct results.
        otherJoinConjuncts = analyzer.getUnassignedConjuncts(tblRef.getAllTupleIds(), false);
        if (tblRef.getJoinOp().isNullAwareLeftAntiJoin()) {
            boolean hasNullMatchingEqOperator = false;
            // Keep only the null-matching eq conjunct in the eqJoinConjuncts and move
            // all the others in otherJoinConjuncts. The BE relies on this
            // separation for correct execution of the null-aware left anti join.
            Iterator<BinaryPredicate> it = eqJoinConjuncts.iterator();
            while (it.hasNext()) {
                BinaryPredicate conjunct = it.next();
                if (!conjunct.isNullMatchingEq()) {
                    otherJoinConjuncts.add(conjunct);
                    it.remove();
                } else {
                    // Only one null-matching eq conjunct is allowed
                    Preconditions.checkState(!hasNullMatchingEqOperator);
                    hasNullMatchingEqOperator = true;
                }
            }
            Preconditions.checkState(hasNullMatchingEqOperator);
        }
    }
    analyzer.markConjunctsAssigned(otherJoinConjuncts);
    HashJoinNode result = new HashJoinNode(outer, inner, tblRef, eqJoinConjuncts, otherJoinConjuncts);
    result.init(analyzer);
    return result;
}
#method_after
private PlanNode createJoinNode(Analyzer analyzer, PlanNode outer, PlanNode inner, TableRef outerRef, TableRef innerRef) throws ImpalaException {
    Preconditions.checkState(innerRef != null ^ outerRef != null);
    TableRef tblRef = (innerRef != null) ? innerRef : outerRef;
    List<BinaryPredicate> eqJoinConjuncts = Lists.newArrayList();
    List<Expr> eqJoinPredicates = Lists.newArrayList();
    // are materialized)
    if (innerRef != null) {
        getHashLookupJoinConjuncts(analyzer, outer.getTblRefIds(), innerRef, eqJoinConjuncts, eqJoinPredicates);
        // Outer joins should only use On-clause predicates as eqJoinConjuncts.
        if (!innerRef.getJoinOp().isOuterJoin()) {
            analyzer.createEquivConjuncts(outer.getTblRefIds(), innerRef.getId(), eqJoinConjuncts);
        }
    } else {
        getHashLookupJoinConjuncts(analyzer, inner.getTblRefIds(), outerRef, eqJoinConjuncts, eqJoinPredicates);
        // Outer joins should only use On-clause predicates as eqJoinConjuncts.
        if (!outerRef.getJoinOp().isOuterJoin()) {
            analyzer.createEquivConjuncts(inner.getTblRefIds(), outerRef.getId(), eqJoinConjuncts);
        }
        // Reverse the lhs/rhs of the join conjuncts.
        for (BinaryPredicate eqJoinConjunct : eqJoinConjuncts) {
            Expr swapTmp = eqJoinConjunct.getChild(0);
            eqJoinConjunct.setChild(0, eqJoinConjunct.getChild(1));
            eqJoinConjunct.setChild(1, swapTmp);
        }
    }
    // Handle implicit cross joins
    if (eqJoinConjuncts.isEmpty()) {
        // the otherJoinConjuncts.
        if (tblRef.getJoinOp().isOuterJoin() || tblRef.getJoinOp().isSemiJoin()) {
            throw new NotImplementedException(String.format("%s join with '%s' without equi-join " + "conjuncts is not supported.", tblRef.getJoinOp().isOuterJoin() ? "Outer" : "Semi", innerRef.getUniqueAlias()));
        }
        CrossJoinNode result = new CrossJoinNode(outer, inner, tblRef, Collections.<Expr>emptyList());
        result.init(analyzer);
        return result;
    }
    // Handle explicit cross joins with equi join conditions
    if (tblRef.getJoinOp() == JoinOperator.CROSS_JOIN) {
        tblRef.setJoinOp(JoinOperator.INNER_JOIN);
    }
    analyzer.markConjunctsAssigned(eqJoinPredicates);
    List<Expr> otherJoinConjuncts = Lists.newArrayList();
    if (tblRef.getJoinOp().isOuterJoin()) {
        // Also assign conjuncts from On clause. All remaining unassigned conjuncts
        // that can be evaluated by this join are assigned in createSelectPlan().
        otherJoinConjuncts = analyzer.getUnassignedOjConjuncts(tblRef);
    } else if (tblRef.getJoinOp().isSemiJoin()) {
        // Unassigned conjuncts bound by the invisible tuple id of a semi join must have
        // come from the join's On-clause, and therefore, must be added to the other join
        // conjuncts to produce correct results.
        otherJoinConjuncts = analyzer.getUnassignedConjuncts(tblRef.getAllTupleIds(), false);
        if (tblRef.getJoinOp().isNullAwareLeftAntiJoin()) {
            boolean hasNullMatchingEqOperator = false;
            // Keep only the null-matching eq conjunct in the eqJoinConjuncts and move
            // all the others in otherJoinConjuncts. The BE relies on this
            // separation for correct execution of the null-aware left anti join.
            Iterator<BinaryPredicate> it = eqJoinConjuncts.iterator();
            while (it.hasNext()) {
                BinaryPredicate conjunct = it.next();
                if (!conjunct.isNullMatchingEq()) {
                    otherJoinConjuncts.add(conjunct);
                    it.remove();
                } else {
                    // Only one null-matching eq conjunct is allowed
                    Preconditions.checkState(!hasNullMatchingEqOperator);
                    hasNullMatchingEqOperator = true;
                }
            }
            Preconditions.checkState(hasNullMatchingEqOperator);
        }
    }
    analyzer.markConjunctsAssigned(otherJoinConjuncts);
    HashJoinNode result = new HashJoinNode(outer, inner, tblRef, eqJoinConjuncts, otherJoinConjuncts);
    result.init(analyzer);
    return result;
}
#end_block

#method_before
private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment rightChildFragment, PlanFragment leftChildFragment, long perNodeMemLimit, ArrayList<PlanFragment> fragments) throws InternalException {
    // broadcast: send the rightChildFragment's output to each node executing
    // the leftChildFragment; the cost across all nodes is proportional to the
    // total amount of data sent
    Analyzer analyzer = ctx_.getRootAnalyzer();
    PlanNode rhsTree = rightChildFragment.getPlanRoot();
    long rhsDataSize = 0;
    long broadcastCost = Long.MAX_VALUE;
    if (rhsTree.getCardinality() != -1 && leftChildFragment.getNumNodes() != -1) {
        rhsDataSize = Math.round((double) rhsTree.getCardinality() * rhsTree.getAvgRowSize());
        broadcastCost = rhsDataSize * leftChildFragment.getNumNodes();
    }
    LOG.debug("broadcast: cost=" + Long.toString(broadcastCost));
    LOG.debug("card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()) + " #nodes=" + Integer.toString(leftChildFragment.getNumNodes()));
    // repartition: both left- and rightChildFragment are partitioned on the
    // join exprs
    PlanNode lhsTree = leftChildFragment.getPlanRoot();
    long partitionCost = Long.MAX_VALUE;
    List<Expr> lhsJoinExprs = Lists.newArrayList();
    List<Expr> rhsJoinExprs = Lists.newArrayList();
    for (Expr joinConjunct : node.getEqJoinConjuncts()) {
        // no remapping necessary
        lhsJoinExprs.add(joinConjunct.getChild(0).clone());
        rhsJoinExprs.add(joinConjunct.getChild(1).clone());
    }
    boolean lhsHasCompatPartition = false;
    boolean rhsHasCompatPartition = false;
    if (lhsTree.getCardinality() != -1 && rhsTree.getCardinality() != -1) {
        lhsHasCompatPartition = analyzer.equivSets(lhsJoinExprs, leftChildFragment.getDataPartition().getPartitionExprs());
        rhsHasCompatPartition = analyzer.equivSets(rhsJoinExprs, rightChildFragment.getDataPartition().getPartitionExprs());
        double lhsCost = (lhsHasCompatPartition) ? 0.0 : Math.round((double) lhsTree.getCardinality() * lhsTree.getAvgRowSize());
        double rhsCost = (rhsHasCompatPartition) ? 0.0 : Math.round((double) rhsTree.getCardinality() * rhsTree.getAvgRowSize());
        partitionCost = Math.round(lhsCost + rhsCost);
    }
    LOG.debug("partition: cost=" + Long.toString(partitionCost));
    LOG.debug("lhs card=" + Long.toString(lhsTree.getCardinality()) + " row_size=" + Float.toString(lhsTree.getAvgRowSize()));
    LOG.debug("rhs card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()));
    LOG.debug(rhsTree.getExplainString());
    boolean doBroadcast;
    // we're unable to estimate the cost
    if ((node.getJoinOp() != JoinOperator.RIGHT_OUTER_JOIN && node.getJoinOp() != JoinOperator.FULL_OUTER_JOIN && node.getJoinOp() != JoinOperator.RIGHT_SEMI_JOIN && node.getJoinOp() != JoinOperator.RIGHT_ANTI_JOIN && (perNodeMemLimit == 0 || Math.round((double) rhsDataSize * PlannerContext.HASH_TBL_SPACE_OVERHEAD) <= perNodeMemLimit) && (node.getTableRef().isBroadcastJoin() || (!node.getTableRef().isPartitionedJoin() && broadcastCost <= partitionCost))) || node.getJoinOp().isNullAwareLeftAntiJoin()) {
        doBroadcast = true;
    } else {
        doBroadcast = false;
    }
    if (doBroadcast) {
        node.setDistributionMode(HashJoinNode.DistributionMode.BROADCAST);
        // Doesn't create a new fragment, but modifies leftChildFragment to execute
        // the join; the build input is provided by an ExchangeNode, which is the
        // destination of the rightChildFragment's output
        node.setChild(0, leftChildFragment.getPlanRoot());
        connectChildFragment(node, 1, rightChildFragment);
        leftChildFragment.setPlanRoot(node);
        return leftChildFragment;
    } else {
        node.setDistributionMode(HashJoinNode.DistributionMode.PARTITIONED);
        // on partition exprs (both using the canonical equivalence class representatives).
        if (lhsHasCompatPartition && rhsHasCompatPartition && isCompatPartition(leftChildFragment.getDataPartition(), rightChildFragment.getDataPartition(), lhsJoinExprs, rhsJoinExprs, analyzer)) {
            node.setChild(0, leftChildFragment.getPlanRoot());
            node.setChild(1, rightChildFragment.getPlanRoot());
            // Redirect fragments sending to rightFragment to leftFragment.
            for (PlanFragment fragment : fragments) {
                if (fragment.getDestFragment() == rightChildFragment) {
                    fragment.setDestination(fragment.getDestNode());
                }
            }
            // Remove right fragment because its plan tree has been merged into leftFragment.
            fragments.remove(rightChildFragment);
            leftChildFragment.setPlanRoot(node);
            return leftChildFragment;
        }
        // The lhs input fragment is already partitioned on the join exprs.
        // Make the HashJoin the new root of leftChildFragment and set the join's
        // first child to the lhs plan root. The second child of the join is an
        // ExchangeNode that is fed by the rhsInputFragment whose sink repartitions
        // its data by the rhs join exprs.
        DataPartition rhsJoinPartition = null;
        if (lhsHasCompatPartition) {
            rhsJoinPartition = getCompatPartition(lhsJoinExprs, leftChildFragment.getDataPartition(), rhsJoinExprs, analyzer);
            if (rhsJoinPartition != null) {
                node.setChild(0, leftChildFragment.getPlanRoot());
                connectChildFragment(node, 1, rightChildFragment);
                rightChildFragment.setOutputPartition(rhsJoinPartition);
                leftChildFragment.setPlanRoot(node);
                return leftChildFragment;
            }
        }
        // Same as above but with rhs and lhs reversed.
        DataPartition lhsJoinPartition = null;
        if (rhsHasCompatPartition) {
            lhsJoinPartition = getCompatPartition(rhsJoinExprs, rightChildFragment.getDataPartition(), lhsJoinExprs, analyzer);
            if (lhsJoinPartition != null) {
                node.setChild(1, rightChildFragment.getPlanRoot());
                connectChildFragment(node, 0, leftChildFragment);
                leftChildFragment.setOutputPartition(lhsJoinPartition);
                rightChildFragment.setPlanRoot(node);
                return rightChildFragment;
            }
        }
        Preconditions.checkState(lhsJoinPartition == null);
        Preconditions.checkState(rhsJoinPartition == null);
        lhsJoinPartition = new DataPartition(TPartitionType.HASH_PARTITIONED, Expr.cloneList(lhsJoinExprs));
        rhsJoinPartition = new DataPartition(TPartitionType.HASH_PARTITIONED, Expr.cloneList(rhsJoinExprs));
        // Neither lhs nor rhs are already partitioned on the join exprs.
        // Create a new parent fragment containing a HashJoin node with two
        // ExchangeNodes as inputs; the latter are the destinations of the
        // left- and rightChildFragments, which now partition their output
        // on their respective join exprs.
        // The new fragment is hash-partitioned on the lhs input join exprs.
        ExchangeNode lhsExchange = new ExchangeNode(ctx_.getNextNodeId());
        lhsExchange.addChild(leftChildFragment.getPlanRoot(), false);
        lhsExchange.computeStats(null);
        node.setChild(0, lhsExchange);
        ExchangeNode rhsExchange = new ExchangeNode(ctx_.getNextNodeId());
        rhsExchange.addChild(rightChildFragment.getPlanRoot(), false);
        rhsExchange.computeStats(null);
        node.setChild(1, rhsExchange);
        // Connect the child fragments in a new fragment, and set the data partition
        // of the new fragment and its child fragments.
        PlanFragment joinFragment = new PlanFragment(ctx_.getNextFragmentId(), node, lhsJoinPartition);
        leftChildFragment.setDestination(lhsExchange);
        leftChildFragment.setOutputPartition(lhsJoinPartition);
        rightChildFragment.setDestination(rhsExchange);
        rightChildFragment.setOutputPartition(rhsJoinPartition);
        return joinFragment;
    }
}
#method_after
private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment rightChildFragment, PlanFragment leftChildFragment, long perNodeMemLimit, ArrayList<PlanFragment> fragments) throws InternalException {
    // broadcast: send the rightChildFragment's output to each node executing
    // the leftChildFragment; the cost across all nodes is proportional to the
    // total amount of data sent
    Analyzer analyzer = ctx_.getRootAnalyzer();
    PlanNode rhsTree = rightChildFragment.getPlanRoot();
    long rhsDataSize = 0;
    long broadcastCost = Long.MAX_VALUE;
    if (rhsTree.getCardinality() != -1 && leftChildFragment.getNumNodes() != -1) {
        rhsDataSize = Math.round((double) rhsTree.getCardinality() * rhsTree.getAvgRowSize());
        broadcastCost = rhsDataSize * leftChildFragment.getNumNodes();
    }
    LOG.debug("broadcast: cost=" + Long.toString(broadcastCost));
    LOG.debug("card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()) + " #nodes=" + Integer.toString(leftChildFragment.getNumNodes()));
    // repartition: both left- and rightChildFragment are partitioned on the
    // join exprs
    PlanNode lhsTree = leftChildFragment.getPlanRoot();
    long partitionCost = Long.MAX_VALUE;
    List<Expr> lhsJoinExprs = Lists.newArrayList();
    List<Expr> rhsJoinExprs = Lists.newArrayList();
    for (Expr joinConjunct : node.getEqJoinConjuncts()) {
        // no remapping necessary
        lhsJoinExprs.add(joinConjunct.getChild(0).clone());
        rhsJoinExprs.add(joinConjunct.getChild(1).clone());
    }
    boolean lhsHasCompatPartition = false;
    boolean rhsHasCompatPartition = false;
    if (lhsTree.getCardinality() != -1 && rhsTree.getCardinality() != -1) {
        lhsHasCompatPartition = analyzer.equivSets(lhsJoinExprs, leftChildFragment.getDataPartition().getPartitionExprs());
        rhsHasCompatPartition = analyzer.equivSets(rhsJoinExprs, rightChildFragment.getDataPartition().getPartitionExprs());
        double lhsCost = (lhsHasCompatPartition) ? 0.0 : Math.round((double) lhsTree.getCardinality() * lhsTree.getAvgRowSize());
        double rhsCost = (rhsHasCompatPartition) ? 0.0 : Math.round((double) rhsTree.getCardinality() * rhsTree.getAvgRowSize());
        partitionCost = Math.round(lhsCost + rhsCost);
    }
    LOG.debug("partition: cost=" + Long.toString(partitionCost));
    LOG.debug("lhs card=" + Long.toString(lhsTree.getCardinality()) + " row_size=" + Float.toString(lhsTree.getAvgRowSize()));
    LOG.debug("rhs card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()));
    LOG.debug(rhsTree.getExplainString());
    boolean doBroadcast;
    // we're unable to estimate the cost
    if ((node.getJoinOp() != JoinOperator.RIGHT_OUTER_JOIN && node.getJoinOp() != JoinOperator.FULL_OUTER_JOIN && node.getJoinOp() != JoinOperator.RIGHT_SEMI_JOIN && node.getJoinOp() != JoinOperator.RIGHT_ANTI_JOIN && (perNodeMemLimit == 0 || Math.round((double) rhsDataSize * PlannerContext.HASH_TBL_SPACE_OVERHEAD) <= perNodeMemLimit) && (node.getDistributionModeHint() == DistributionMode.BROADCAST || (node.getDistributionModeHint() != DistributionMode.PARTITIONED && broadcastCost <= partitionCost))) || node.getJoinOp().isNullAwareLeftAntiJoin()) {
        doBroadcast = true;
    } else {
        doBroadcast = false;
    }
    if (doBroadcast) {
        node.setDistributionMode(HashJoinNode.DistributionMode.BROADCAST);
        // Doesn't create a new fragment, but modifies leftChildFragment to execute
        // the join; the build input is provided by an ExchangeNode, which is the
        // destination of the rightChildFragment's output
        node.setChild(0, leftChildFragment.getPlanRoot());
        connectChildFragment(node, 1, rightChildFragment);
        leftChildFragment.setPlanRoot(node);
        return leftChildFragment;
    } else {
        node.setDistributionMode(HashJoinNode.DistributionMode.PARTITIONED);
        // on partition exprs (both using the canonical equivalence class representatives).
        if (lhsHasCompatPartition && rhsHasCompatPartition && isCompatPartition(leftChildFragment.getDataPartition(), rightChildFragment.getDataPartition(), lhsJoinExprs, rhsJoinExprs, analyzer)) {
            node.setChild(0, leftChildFragment.getPlanRoot());
            node.setChild(1, rightChildFragment.getPlanRoot());
            // Redirect fragments sending to rightFragment to leftFragment.
            for (PlanFragment fragment : fragments) {
                if (fragment.getDestFragment() == rightChildFragment) {
                    fragment.setDestination(fragment.getDestNode());
                }
            }
            // Remove right fragment because its plan tree has been merged into leftFragment.
            fragments.remove(rightChildFragment);
            leftChildFragment.setPlanRoot(node);
            return leftChildFragment;
        }
        // The lhs input fragment is already partitioned on the join exprs.
        // Make the HashJoin the new root of leftChildFragment and set the join's
        // first child to the lhs plan root. The second child of the join is an
        // ExchangeNode that is fed by the rhsInputFragment whose sink repartitions
        // its data by the rhs join exprs.
        DataPartition rhsJoinPartition = null;
        if (lhsHasCompatPartition) {
            rhsJoinPartition = getCompatPartition(lhsJoinExprs, leftChildFragment.getDataPartition(), rhsJoinExprs, analyzer);
            if (rhsJoinPartition != null) {
                node.setChild(0, leftChildFragment.getPlanRoot());
                connectChildFragment(node, 1, rightChildFragment);
                rightChildFragment.setOutputPartition(rhsJoinPartition);
                leftChildFragment.setPlanRoot(node);
                return leftChildFragment;
            }
        }
        // Same as above but with rhs and lhs reversed.
        DataPartition lhsJoinPartition = null;
        if (rhsHasCompatPartition) {
            lhsJoinPartition = getCompatPartition(rhsJoinExprs, rightChildFragment.getDataPartition(), lhsJoinExprs, analyzer);
            if (lhsJoinPartition != null) {
                node.setChild(1, rightChildFragment.getPlanRoot());
                connectChildFragment(node, 0, leftChildFragment);
                leftChildFragment.setOutputPartition(lhsJoinPartition);
                rightChildFragment.setPlanRoot(node);
                return rightChildFragment;
            }
        }
        Preconditions.checkState(lhsJoinPartition == null);
        Preconditions.checkState(rhsJoinPartition == null);
        lhsJoinPartition = new DataPartition(TPartitionType.HASH_PARTITIONED, Expr.cloneList(lhsJoinExprs));
        rhsJoinPartition = new DataPartition(TPartitionType.HASH_PARTITIONED, Expr.cloneList(rhsJoinExprs));
        // Neither lhs nor rhs are already partitioned on the join exprs.
        // Create a new parent fragment containing a HashJoin node with two
        // ExchangeNodes as inputs; the latter are the destinations of the
        // left- and rightChildFragments, which now partition their output
        // on their respective join exprs.
        // The new fragment is hash-partitioned on the lhs input join exprs.
        ExchangeNode lhsExchange = new ExchangeNode(ctx_.getNextNodeId());
        lhsExchange.addChild(leftChildFragment.getPlanRoot(), false);
        lhsExchange.computeStats(null);
        node.setChild(0, lhsExchange);
        ExchangeNode rhsExchange = new ExchangeNode(ctx_.getNextNodeId());
        rhsExchange.addChild(rightChildFragment.getPlanRoot(), false);
        rhsExchange.computeStats(null);
        node.setChild(1, rhsExchange);
        // Connect the child fragments in a new fragment, and set the data partition
        // of the new fragment and its child fragments.
        PlanFragment joinFragment = new PlanFragment(ctx_.getNextFragmentId(), node, lhsJoinPartition);
        leftChildFragment.setDestination(lhsExchange);
        leftChildFragment.setOutputPartition(lhsJoinPartition);
        rightChildFragment.setDestination(rhsExchange);
        rightChildFragment.setOutputPartition(rhsJoinPartition);
        return joinFragment;
    }
}
#end_block

#method_before
private void createCatalogOpRequest(AnalysisContext.AnalysisResult analysis, TExecRequest result) {
    TCatalogOpRequest ddl = new TCatalogOpRequest();
    TResultSetMetadata metadata = new TResultSetMetadata();
    if (analysis.isUseStmt()) {
        ddl.op_type = TCatalogOpType.USE;
        ddl.setUse_db_params(analysis.getUseStmt().toThrift());
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isShowTablesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_TABLES;
        ddl.setShow_tables_params(analysis.getShowTablesStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowDbsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_DBS;
        ddl.setShow_dbs_params(analysis.getShowDbsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowDataSrcsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_DATA_SRCS;
        ddl.setShow_data_srcs_params(analysis.getShowDataSrcsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("location", Type.STRING.toThrift()), new TColumn("class name", Type.STRING.toThrift()), new TColumn("api version", Type.STRING.toThrift())));
    } else if (analysis.isShowStatsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_STATS;
        ddl.setShow_stats_params(analysis.getShowStatsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowFunctionsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_FUNCTIONS;
        ShowFunctionsStmt stmt = (ShowFunctionsStmt) analysis.getStmt();
        ddl.setShow_fns_params(stmt.toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("return type", Type.STRING.toThrift()), new TColumn("signature", Type.STRING.toThrift())));
    } else if (analysis.isShowCreateTableStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_CREATE_TABLE;
        ddl.setShow_create_table_params(analysis.getShowCreateTableStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("result", Type.STRING.toThrift())));
    } else if (analysis.isShowFilesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_FILES;
        ddl.setShow_files_params(analysis.getShowFilesStmt().toThrift());
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDescribeStmt()) {
        ddl.op_type = TCatalogOpType.DESCRIBE;
        ddl.setDescribe_table_params(analysis.getDescribeStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("type", Type.STRING.toThrift()), new TColumn("comment", Type.STRING.toThrift())));
    } else if (analysis.isAlterTableStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.ALTER_TABLE);
        req.setAlter_table_params(analysis.getAlterTableStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isAlterViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.ALTER_VIEW);
        req.setAlter_view_params(analysis.getAlterViewStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateTableStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE);
        req.setCreate_table_params(analysis.getCreateTableStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateTableAsSelectStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE_AS_SELECT);
        req.setCreate_table_params(analysis.getCreateTableAsSelectStmt().getCreateStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Arrays.asList(new TColumn("summary", Type.STRING.toThrift())));
    } else if (analysis.isCreateTableLikeStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE_LIKE);
        req.setCreate_table_like_params(analysis.getCreateTableLikeStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_VIEW);
        req.setCreate_view_params(analysis.getCreateViewStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateDbStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_DATABASE);
        req.setCreate_db_params(analysis.getCreateDbStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateUdfStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        CreateUdfStmt stmt = (CreateUdfStmt) analysis.getStmt();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_FUNCTION);
        req.setCreate_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateUdaStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_FUNCTION);
        CreateUdaStmt stmt = (CreateUdaStmt) analysis.getStmt();
        req.setCreate_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateDataSrcStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_DATA_SOURCE);
        CreateDataSrcStmt stmt = (CreateDataSrcStmt) analysis.getStmt();
        req.setCreate_data_source_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isComputeStatsStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.COMPUTE_STATS);
        req.setCompute_stats_params(analysis.getComputeStatsStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropDbStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_DATABASE);
        req.setDrop_db_params(analysis.getDropDbStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropTableOrViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        DropTableOrViewStmt stmt = analysis.getDropTableOrViewStmt();
        req.setDdl_type(stmt.isDropTable() ? TDdlType.DROP_TABLE : TDdlType.DROP_VIEW);
        req.setDrop_table_or_view_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropTableOrViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        TruncateStmt stmt = analysis.getTruncateStmt();
        req.setDdl_type(TDdlType.TRUNCATE_TABLE);
        req.setTruncate_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropFunctionStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_FUNCTION);
        DropFunctionStmt stmt = (DropFunctionStmt) analysis.getStmt();
        req.setDrop_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropDataSrcStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_DATA_SOURCE);
        DropDataSrcStmt stmt = (DropDataSrcStmt) analysis.getStmt();
        req.setDrop_data_source_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropStatsStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_STATS);
        DropStatsStmt stmt = (DropStatsStmt) analysis.getStmt();
        req.setDrop_stats_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isResetMetadataStmt()) {
        ddl.op_type = TCatalogOpType.RESET_METADATA;
        ResetMetadataStmt resetMetadataStmt = (ResetMetadataStmt) analysis.getStmt();
        TResetMetadataRequest req = resetMetadataStmt.toThrift();
        ddl.setReset_metadata_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isShowRolesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_ROLES;
        ShowRolesStmt showRolesStmt = (ShowRolesStmt) analysis.getStmt();
        ddl.setShow_roles_params(showRolesStmt.toThrift());
        Set<String> groupNames = getAuthzChecker().getUserGroups(analysis.getAnalyzer().getUser());
        // Check if the user is part of the group (case-sensitive) this SHOW ROLE
        // statement is targeting. If they are already a member of the group,
        // the admin requirement can be removed.
        Preconditions.checkState(ddl.getShow_roles_params().isSetIs_admin_op());
        if (ddl.getShow_roles_params().isSetGrant_group() && groupNames.contains(ddl.getShow_roles_params().getGrant_group())) {
            ddl.getShow_roles_params().setIs_admin_op(false);
        }
        metadata.setColumns(Arrays.asList(new TColumn("role_name", Type.STRING.toThrift())));
    } else if (analysis.isShowGrantRoleStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_GRANT_ROLE;
        ShowGrantRoleStmt showGrantRoleStmt = (ShowGrantRoleStmt) analysis.getStmt();
        ddl.setShow_grant_role_params(showGrantRoleStmt.toThrift());
        Set<String> groupNames = getAuthzChecker().getUserGroups(analysis.getAnalyzer().getUser());
        // User must be an admin to execute this operation if they have not been granted
        // this role.
        ddl.getShow_grant_role_params().setIs_admin_op(Sets.intersection(groupNames, showGrantRoleStmt.getRole().getGrantGroups()).isEmpty());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isCreateDropRoleStmt()) {
        CreateDropRoleStmt createDropRoleStmt = (CreateDropRoleStmt) analysis.getStmt();
        TCreateDropRoleParams params = createDropRoleStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_drop() ? TDdlType.DROP_ROLE : TDdlType.CREATE_ROLE);
        req.setCreate_drop_role_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isGrantRevokeRoleStmt()) {
        GrantRevokeRoleStmt grantRoleStmt = (GrantRevokeRoleStmt) analysis.getStmt();
        TGrantRevokeRoleParams params = grantRoleStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_grant() ? TDdlType.GRANT_ROLE : TDdlType.REVOKE_ROLE);
        req.setGrant_revoke_role_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isGrantRevokePrivStmt()) {
        GrantRevokePrivStmt grantRevokePrivStmt = (GrantRevokePrivStmt) analysis.getStmt();
        TGrantRevokePrivParams params = grantRevokePrivStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_grant() ? TDdlType.GRANT_PRIVILEGE : TDdlType.REVOKE_PRIVILEGE);
        req.setGrant_revoke_priv_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else {
        throw new IllegalStateException("Unexpected CatalogOp statement type.");
    }
    result.setResult_set_metadata(metadata);
    result.setCatalog_op_request(ddl);
    if (ddl.getOp_type() == TCatalogOpType.DDL) {
        TCatalogServiceRequestHeader header = new TCatalogServiceRequestHeader();
        header.setRequesting_user(analysis.getAnalyzer().getUser().getName());
        ddl.getDdl_params().setHeader(header);
    }
}
#method_after
private void createCatalogOpRequest(AnalysisContext.AnalysisResult analysis, TExecRequest result) {
    TCatalogOpRequest ddl = new TCatalogOpRequest();
    TResultSetMetadata metadata = new TResultSetMetadata();
    if (analysis.isUseStmt()) {
        ddl.op_type = TCatalogOpType.USE;
        ddl.setUse_db_params(analysis.getUseStmt().toThrift());
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isShowTablesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_TABLES;
        ddl.setShow_tables_params(analysis.getShowTablesStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowDbsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_DBS;
        ddl.setShow_dbs_params(analysis.getShowDbsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowDataSrcsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_DATA_SRCS;
        ddl.setShow_data_srcs_params(analysis.getShowDataSrcsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("location", Type.STRING.toThrift()), new TColumn("class name", Type.STRING.toThrift()), new TColumn("api version", Type.STRING.toThrift())));
    } else if (analysis.isShowStatsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_STATS;
        ddl.setShow_stats_params(analysis.getShowStatsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowFunctionsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_FUNCTIONS;
        ShowFunctionsStmt stmt = (ShowFunctionsStmt) analysis.getStmt();
        ddl.setShow_fns_params(stmt.toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("return type", Type.STRING.toThrift()), new TColumn("signature", Type.STRING.toThrift())));
    } else if (analysis.isShowCreateTableStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_CREATE_TABLE;
        ddl.setShow_create_table_params(analysis.getShowCreateTableStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("result", Type.STRING.toThrift())));
    } else if (analysis.isShowFilesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_FILES;
        ddl.setShow_files_params(analysis.getShowFilesStmt().toThrift());
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDescribeStmt()) {
        ddl.op_type = TCatalogOpType.DESCRIBE;
        ddl.setDescribe_table_params(analysis.getDescribeStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("type", Type.STRING.toThrift()), new TColumn("comment", Type.STRING.toThrift())));
    } else if (analysis.isAlterTableStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.ALTER_TABLE);
        req.setAlter_table_params(analysis.getAlterTableStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isAlterViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.ALTER_VIEW);
        req.setAlter_view_params(analysis.getAlterViewStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateTableStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE);
        req.setCreate_table_params(analysis.getCreateTableStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateTableAsSelectStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE_AS_SELECT);
        req.setCreate_table_params(analysis.getCreateTableAsSelectStmt().getCreateStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Arrays.asList(new TColumn("summary", Type.STRING.toThrift())));
    } else if (analysis.isCreateTableLikeStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE_LIKE);
        req.setCreate_table_like_params(analysis.getCreateTableLikeStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_VIEW);
        req.setCreate_view_params(analysis.getCreateViewStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateDbStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_DATABASE);
        req.setCreate_db_params(analysis.getCreateDbStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateUdfStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        CreateUdfStmt stmt = (CreateUdfStmt) analysis.getStmt();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_FUNCTION);
        req.setCreate_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateUdaStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_FUNCTION);
        CreateUdaStmt stmt = (CreateUdaStmt) analysis.getStmt();
        req.setCreate_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateDataSrcStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_DATA_SOURCE);
        CreateDataSrcStmt stmt = (CreateDataSrcStmt) analysis.getStmt();
        req.setCreate_data_source_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isComputeStatsStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.COMPUTE_STATS);
        req.setCompute_stats_params(analysis.getComputeStatsStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropDbStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_DATABASE);
        req.setDrop_db_params(analysis.getDropDbStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropTableOrViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        DropTableOrViewStmt stmt = analysis.getDropTableOrViewStmt();
        req.setDdl_type(stmt.isDropTable() ? TDdlType.DROP_TABLE : TDdlType.DROP_VIEW);
        req.setDrop_table_or_view_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isTruncateStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        TruncateStmt stmt = analysis.getTruncateStmt();
        req.setDdl_type(TDdlType.TRUNCATE_TABLE);
        req.setTruncate_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropFunctionStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_FUNCTION);
        DropFunctionStmt stmt = (DropFunctionStmt) analysis.getStmt();
        req.setDrop_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropDataSrcStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_DATA_SOURCE);
        DropDataSrcStmt stmt = (DropDataSrcStmt) analysis.getStmt();
        req.setDrop_data_source_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropStatsStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_STATS);
        DropStatsStmt stmt = (DropStatsStmt) analysis.getStmt();
        req.setDrop_stats_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isResetMetadataStmt()) {
        ddl.op_type = TCatalogOpType.RESET_METADATA;
        ResetMetadataStmt resetMetadataStmt = (ResetMetadataStmt) analysis.getStmt();
        TResetMetadataRequest req = resetMetadataStmt.toThrift();
        ddl.setReset_metadata_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isShowRolesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_ROLES;
        ShowRolesStmt showRolesStmt = (ShowRolesStmt) analysis.getStmt();
        ddl.setShow_roles_params(showRolesStmt.toThrift());
        Set<String> groupNames = getAuthzChecker().getUserGroups(analysis.getAnalyzer().getUser());
        // Check if the user is part of the group (case-sensitive) this SHOW ROLE
        // statement is targeting. If they are already a member of the group,
        // the admin requirement can be removed.
        Preconditions.checkState(ddl.getShow_roles_params().isSetIs_admin_op());
        if (ddl.getShow_roles_params().isSetGrant_group() && groupNames.contains(ddl.getShow_roles_params().getGrant_group())) {
            ddl.getShow_roles_params().setIs_admin_op(false);
        }
        metadata.setColumns(Arrays.asList(new TColumn("role_name", Type.STRING.toThrift())));
    } else if (analysis.isShowGrantRoleStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_GRANT_ROLE;
        ShowGrantRoleStmt showGrantRoleStmt = (ShowGrantRoleStmt) analysis.getStmt();
        ddl.setShow_grant_role_params(showGrantRoleStmt.toThrift());
        Set<String> groupNames = getAuthzChecker().getUserGroups(analysis.getAnalyzer().getUser());
        // User must be an admin to execute this operation if they have not been granted
        // this role.
        ddl.getShow_grant_role_params().setIs_admin_op(Sets.intersection(groupNames, showGrantRoleStmt.getRole().getGrantGroups()).isEmpty());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isCreateDropRoleStmt()) {
        CreateDropRoleStmt createDropRoleStmt = (CreateDropRoleStmt) analysis.getStmt();
        TCreateDropRoleParams params = createDropRoleStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_drop() ? TDdlType.DROP_ROLE : TDdlType.CREATE_ROLE);
        req.setCreate_drop_role_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isGrantRevokeRoleStmt()) {
        GrantRevokeRoleStmt grantRoleStmt = (GrantRevokeRoleStmt) analysis.getStmt();
        TGrantRevokeRoleParams params = grantRoleStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_grant() ? TDdlType.GRANT_ROLE : TDdlType.REVOKE_ROLE);
        req.setGrant_revoke_role_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isGrantRevokePrivStmt()) {
        GrantRevokePrivStmt grantRevokePrivStmt = (GrantRevokePrivStmt) analysis.getStmt();
        TGrantRevokePrivParams params = grantRevokePrivStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_grant() ? TDdlType.GRANT_PRIVILEGE : TDdlType.REVOKE_PRIVILEGE);
        req.setGrant_revoke_priv_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else {
        throw new IllegalStateException("Unexpected CatalogOp statement type.");
    }
    result.setResult_set_metadata(metadata);
    result.setCatalog_op_request(ddl);
    if (ddl.getOp_type() == TCatalogOpType.DDL) {
        TCatalogServiceRequestHeader header = new TCatalogServiceRequestHeader();
        header.setRequesting_user(analysis.getAnalyzer().getUser().getName());
        ddl.getDdl_params().setHeader(header);
    }
}
#end_block

#method_before
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int fragmentId = 0; fragmentId < fragments.size(); ++fragmentId) {
        PlanFragment fragment = fragments.get(fragmentId);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, fragmentId);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use VERBOSE by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.VERBOSE;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    String jsonLineageGraph = analysisResult.getJsonLineageGraph();
    if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
        queryExecRequest.setLineage_graph(jsonLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else {
        Preconditions.checkState(analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt());
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    }
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#method_after
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        String jsonLineageGraph = analysisResult.getJsonLineageGraph();
        if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
            result.catalog_op_request.setLineage_graph(jsonLineageGraph);
        }
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int fragmentId = 0; fragmentId < fragments.size(); ++fragmentId) {
        PlanFragment fragment = fragments.get(fragmentId);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, fragmentId);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use VERBOSE by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.VERBOSE;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    String jsonLineageGraph = analysisResult.getJsonLineageGraph();
    if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
        queryExecRequest.setLineage_graph(jsonLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else {
        Preconditions.checkState(analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt());
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    }
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Also checks if the target table is missing.
    for (TableName tblName : tableNames_) {
        if (!tblName.isFullyQualified()) {
            tblName = new TableName(analyzer.getDefaultDb(), tblName.getTbl());
        }
        Table table = analyzer.getTable(tblName, Privilege.INSERT);
        // We do not support truncating views or HBase tables.
        if (table instanceof View || table instanceof HBaseTable) {
            throw new AnalysisException(String.format("Impala does not support truncating views or HBase tables: %s", table.getFullName()));
        }
        tables_.add(table);
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    tableName_ = analyzer.getFqTableName(tableName_);
    table_ = analyzer.getTable(tableName_, Privilege.INSERT);
    // We only support truncating hdfs tables now.
    if (!(table_ instanceof HdfsTable)) {
        throw new AnalysisException(String.format("TRUNCATE TABLE not supported on non-HDFS table: %s", table_.getFullName()));
    }
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    strBuilder.append("TRUNCATE TABLE ");
    for (TableName tblName : tableNames_) {
        strBuilder.append(tblName + ", ");
    }
    return strBuilder.substring(0, strBuilder.length() - 2);
}
#method_after
@Override
public String toSql() {
    return "TRUNCATE TABLE " + tableName_;
}
#end_block

#method_before
public TTruncateParams toThrift() {
    TTruncateParams params = new TTruncateParams();
    List<TTableName> list = Lists.newArrayList();
    for (TableName tblName : tableNames_) {
        list.add(tblName.toThrift());
    }
    params.setTable_names(list);
    return params;
}
#method_after
public TTruncateParams toThrift() {
    TTruncateParams params = new TTruncateParams();
    params.setTable_name(tableName_.toThrift());
    return params;
}
#end_block

#method_before
private void createDatabase(TCreateDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    String dbName = params.getDb();
    Preconditions.checkState(dbName != null && !dbName.isEmpty(), "Null or empty database name passed as argument to Catalog.createDatabase");
    if (params.if_not_exists && catalog_.getDb(dbName) != null) {
        LOG.debug("Skipping database creation because " + dbName + " already exists and " + "IF NOT EXISTS was specified.");
        resp.getResult().setVersion(catalog_.getCatalogVersion());
        return;
    }
    org.apache.hadoop.hive.metastore.api.Database db = new org.apache.hadoop.hive.metastore.api.Database();
    db.setName(dbName);
    if (params.getComment() != null) {
        db.setDescription(params.getComment());
    }
    if (params.getLocation() != null) {
        db.setLocationUri(params.getLocation());
    }
    LOG.debug("Creating database " + dbName);
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().createDatabase(db);
        } catch (AlreadyExistsException e) {
            if (!params.if_not_exists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating database %s because " + "IF NOT EXISTS was specified.", e, dbName));
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
        } finally {
            msClient.release();
        }
        Db newDb = catalog_.addDb(dbName);
        TCatalogObject thriftDb = new TCatalogObject(TCatalogObjectType.DATABASE, Catalog.INITIAL_CATALOG_VERSION);
        thriftDb.setDb(newDb.toThrift());
        thriftDb.setCatalog_version(newDb.getCatalogVersion());
        resp.result.setUpdated_catalog_object(thriftDb);
    }
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#method_after
private void createDatabase(TCreateDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    String dbName = params.getDb();
    Preconditions.checkState(dbName != null && !dbName.isEmpty(), "Null or empty database name passed as argument to Catalog.createDatabase");
    if (params.if_not_exists && catalog_.getDb(dbName) != null) {
        LOG.debug("Skipping database creation because " + dbName + " already exists and " + "IF NOT EXISTS was specified.");
        resp.getResult().setVersion(catalog_.getCatalogVersion());
        return;
    }
    org.apache.hadoop.hive.metastore.api.Database db = new org.apache.hadoop.hive.metastore.api.Database();
    db.setName(dbName);
    if (params.getComment() != null) {
        db.setDescription(params.getComment());
    }
    if (params.getLocation() != null) {
        db.setLocationUri(params.getLocation());
    }
    LOG.debug("Creating database " + dbName);
    Db newDb = null;
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().createDatabase(db);
            newDb = catalog_.addDb(dbName);
        } catch (AlreadyExistsException e) {
            if (!params.if_not_exists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating database %s because " + "IF NOT EXISTS was specified.", e, dbName));
            newDb = catalog_.getDb(dbName);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
        } finally {
            msClient.release();
        }
        Preconditions.checkNotNull(newDb);
        TCatalogObject thriftDb = new TCatalogObject(TCatalogObjectType.DATABASE, Catalog.INITIAL_CATALOG_VERSION);
        thriftDb.setDb(newDb.toThrift());
        thriftDb.setCatalog_version(newDb.getCatalogVersion());
        resp.result.setUpdated_catalog_object(thriftDb);
    }
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#end_block

#method_before
private void truncateTable(TTruncateParams params, TDdlExecResponse resp) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        try {
            for (TTableName tblName : params.getTable_names()) {
                TableName tableName = TableName.fromThrift(tblName);
                HdfsTable hdfsTable = (HdfsTable) catalog_.getOrLoadTable(tableName.getDb(), tableName.getTbl());
                for (HdfsPartition part : hdfsTable.getPartitions()) {
                    org.apache.hadoop.hive.metastore.api.Partition msPart = part.toHmsPartition();
                    FileSystemUtil.emptyDirectory(new Path(msPart.getSd().getLocation()));
                    if (needToUpdateStats(msPart)) {
                        try {
                            applyAlterPartition(tableName, part);
                        } finally {
                            part.markDirty();
                        }
                    }
                }
            }
        } catch (IOException e) {
            throw new CatalogException("Failed to truncate table: ", e);
        }
    }
    resp.result.setVersion(catalog_.getCatalogVersion());
}
#method_after
private void truncateTable(TTruncateParams params, TDdlExecResponse resp) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        TTableName tblName = params.getTable_name();
        try {
            Table table = getExistingTable(tblName.getDb_name(), tblName.getTable_name());
            Preconditions.checkNotNull(table);
            if (!(table instanceof HdfsTable)) {
                throw new CatalogException(String.format("TRUNCATE TABLE not supported on non-HDFS table: %s", table.getFullName()));
            }
            HdfsTable hdfsTable = (HdfsTable) table;
            for (HdfsPartition part : hdfsTable.getPartitions()) {
                if (part.isDefaultPartition())
                    continue;
                FileSystemUtil.deleteAllVisibleFiles(new Path(part.getLocation()));
            }
            dropColumnStats(table);
            dropTableStats(table);
        } catch (Exception e) {
            String fqName = tblName.db_name + "." + tblName.table_name;
            throw new CatalogException(String.format("Failed to truncate table: %s.\n" + "Table may be in a partially truncated state.", fqName), e);
        }
    }
    // Reload the table to pick up on the now empty directories.
    Table refreshedTbl = catalog_.reloadTable(params.getTable_name());
    resp.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    resp.getResult().setVersion(resp.getResult().getUpdated_catalog_object().getCatalog_version());
}
#end_block

#method_before
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    // Collects the cache directive IDs of any cached table/partitions that were
    // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
    // and the table will be refreshed asynchronously after all cache directives
    // complete.
    List<Long> cacheDirIds = Lists.<Long>newArrayList();
    // If the table is cached, get its cache pool name and replication factor. New
    // partitions will inherit this property.
    String cachePoolName = null;
    Short cacheReplication = 0;
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(table.getMetaStoreTable().getParameters());
    if (cacheDirId != null) {
        try {
            cachePoolName = HdfsCachingUtil.getCachePool(cacheDirId);
            cacheReplication = HdfsCachingUtil.getCacheReplication(cacheDirId);
            Preconditions.checkNotNull(cacheReplication);
            if (table.getNumClusteringCols() == 0)
                cacheDirIds.add(cacheDirId);
        } catch (ImpalaRuntimeException e) {
            // Catch the error so that the actual update to the catalog can progress,
            // this resets caching for the table though
            LOG.error(String.format("Cache directive %d was not found, uncache the table %s.%s" + "to remove this message.", cacheDirId, update.getDb_name(), update.getTarget_table()));
            cacheDirId = null;
        }
    }
    TableName tblName = new TableName(table.getDb().getName(), table.getName());
    AtomicBoolean addedNewPartition = new AtomicBoolean(false);
    if (table.getNumClusteringCols() > 0) {
        // Set of all partition names targeted by the insert that that need to be created
        // in the Metastore (partitions that do not currently exist in the catalog).
        // In the BE, we don't currently distinguish between which targeted partitions are
        // new and which already exist, so initialize the set with all targeted partition
        // names and remove the ones that are found to exist.
        Set<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
        for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
            // Skip dummy default partition.
            if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                continue;
            }
            // TODO: In the BE we build partition names without a trailing char. In FE we
            // build partition name with a trailing char. We should make this consistent.
            String partName = partition.getPartitionName() + "/";
            // returns true, it indicates the partition already exists.
            if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                // The partition was targeted by the insert and is also a cached. Since data
                // was written to the partition, a watch needs to be placed on the cache
                // cache directive so the TableLoadingMgr can perform an async refresh once
                // all data becomes cached.
                cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
            }
            if (partsToCreate.size() == 0)
                break;
        }
        if (!partsToCreate.isEmpty()) {
            SettableFuture<Void> allFinished = SettableFuture.create();
            AtomicInteger numPartitions = new AtomicInteger(partsToCreate.size());
            // Add all partitions to metastore.
            for (String partName : partsToCreate) {
                Preconditions.checkState(partName != null && !partName.isEmpty());
                CreatePartitionRunnable rbl = new CreatePartitionRunnable(tblName, partName, cachePoolName, addedNewPartition, allFinished, numPartitions, cacheDirIds, cacheReplication);
                executor_.execute(rbl);
            }
            try {
                // Will throw if any operation calls setException
                allFinished.get();
            } catch (Exception e) {
                throw new InternalException("Error updating metastore", e);
            }
        }
    }
    if (addedNewPartition.get()) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // Operate on a copy of msTbl to prevent our cached msTbl becoming inconsistent
            // if the alteration fails in the metastore.
            org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
            updateLastDdlTime(msTbl, msClient);
        } catch (Exception e) {
            throw new InternalException("Error updating lastDdlTime", e);
        } finally {
            msClient.release();
        }
    }
    // Submit the watch request for the given cache directives.
    if (!cacheDirIds.isEmpty())
        catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    // Perform an incremental refresh to load new/modified partitions and files.
    Table refreshedTbl = catalog_.reloadTable(tblName.toThrift());
    response.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    response.getResult().setVersion(response.getResult().getUpdated_catalog_object().getCatalog_version());
    return response;
}
#method_after
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    // Collects the cache directive IDs of any cached table/partitions that were
    // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
    // and the table will be refreshed asynchronously after all cache directives
    // complete.
    List<Long> cacheDirIds = Lists.<Long>newArrayList();
    // If the table is cached, get its cache pool name and replication factor. New
    // partitions will inherit this property.
    String cachePoolName = null;
    Short cacheReplication = 0;
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(table.getMetaStoreTable().getParameters());
    if (cacheDirId != null) {
        try {
            cachePoolName = HdfsCachingUtil.getCachePool(cacheDirId);
            cacheReplication = HdfsCachingUtil.getCacheReplication(cacheDirId);
            Preconditions.checkNotNull(cacheReplication);
            if (table.getNumClusteringCols() == 0)
                cacheDirIds.add(cacheDirId);
        } catch (ImpalaRuntimeException e) {
            // Catch the error so that the actual update to the catalog can progress,
            // this resets caching for the table though
            LOG.error(String.format("Cache directive %d was not found, uncache the table %s.%s" + "to remove this message.", cacheDirId, update.getDb_name(), update.getTarget_table()));
            cacheDirId = null;
        }
    }
    TableName tblName = new TableName(table.getDb().getName(), table.getName());
    List<String> errorMessages = Lists.newArrayList();
    if (table.getNumClusteringCols() > 0) {
        // Set of all partition names targeted by the insert that that need to be created
        // in the Metastore (partitions that do not currently exist in the catalog).
        // In the BE, we don't currently distinguish between which targeted partitions are
        // new and which already exist, so initialize the set with all targeted partition
        // names and remove the ones that are found to exist.
        Set<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
        for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
            // Skip dummy default partition.
            if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                continue;
            }
            // TODO: In the BE we build partition names without a trailing char. In FE we
            // build partition name with a trailing char. We should make this consistent.
            String partName = partition.getPartitionName() + "/";
            // returns true, it indicates the partition already exists.
            if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                // The partition was targeted by the insert and is also a cached. Since data
                // was written to the partition, a watch needs to be placed on the cache
                // cache directive so the TableLoadingMgr can perform an async refresh once
                // all data becomes cached.
                cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
            }
            if (partsToCreate.size() == 0)
                break;
        }
        if (!partsToCreate.isEmpty()) {
            MetaStoreClient msClient = catalog_.getMetaStoreClient();
            try {
                org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tblName);
                List<org.apache.hadoop.hive.metastore.api.Partition> hmsParts = Lists.newArrayList();
                HiveConf hiveConf = new HiveConf(this.getClass());
                Warehouse warehouse = new Warehouse(hiveConf);
                for (String partName : partsToCreate) {
                    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
                    hmsParts.add(partition);
                    partition.setDbName(tblName.getDb());
                    partition.setTableName(tblName.getTbl());
                    partition.setValues(getPartValsFromName(msTbl, partName));
                    partition.setParameters(new HashMap<String, String>());
                    partition.setSd(msTbl.getSd().deepCopy());
                    partition.getSd().setSerdeInfo(msTbl.getSd().getSerdeInfo().deepCopy());
                    partition.getSd().setLocation(msTbl.getSd().getLocation() + "/" + partName.substring(0, partName.length() - 1));
                    MetaStoreUtils.updatePartitionStatsFast(partition, warehouse);
                }
                // First add_partitions and then alter_partitions the successful ones with
                // caching directives. The reason is that some partitions could have been
                // added concurrently, and we want to avoid caching a partition twice and
                // leaking a caching directive.
                List<org.apache.hadoop.hive.metastore.api.Partition> addedHmsParts = msClient.getHiveClient().add_partitions(hmsParts, true, true);
                if (addedHmsParts.size() > 0) {
                    if (cachePoolName != null) {
                        List<org.apache.hadoop.hive.metastore.api.Partition> cachedHmsParts = Lists.newArrayList();
                        // the directive id.
                        for (org.apache.hadoop.hive.metastore.api.Partition part : addedHmsParts) {
                            try {
                                cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(part, cachePoolName, cacheReplication));
                                cachedHmsParts.add(part);
                            } catch (ImpalaRuntimeException e) {
                                String msg = String.format("Partition %s.%s(%s): State: Not cached." + " Action: Cache manully via 'ALTER TABLE'.", part.getDbName(), part.getTableName(), part.getValues());
                                LOG.error(msg, e);
                                errorMessages.add(msg);
                            }
                        }
                        try {
                            msClient.getHiveClient().alter_partitions(tblName.getDb(), tblName.getTbl(), cachedHmsParts);
                        } catch (Exception e) {
                            LOG.error("Failed in alter_partitions: ", e);
                            // Try to uncache the partitions when the alteration in the HMS failed.
                            for (org.apache.hadoop.hive.metastore.api.Partition part : cachedHmsParts) {
                                try {
                                    HdfsCachingUtil.uncachePartition(part);
                                } catch (ImpalaException e1) {
                                    String msg = String.format("Partition %s.%s(%s): State: Leaked caching directive. " + "Action: Manually uncache directory %s via hdfs cacheAdmin.", part.getDbName(), part.getTableName(), part.getValues(), part.getSd().getLocation());
                                    LOG.error(msg, e);
                                    errorMessages.add(msg);
                                }
                            }
                        }
                    }
                    updateLastDdlTime(msTbl, msClient);
                }
            } catch (AlreadyExistsException e) {
                throw new InternalException("AlreadyExistsException thrown although ifNotExists given", e);
            } catch (Exception e) {
                throw new InternalException("Error adding partitions", e);
            } finally {
                msClient.release();
            }
        }
    }
    // Submit the watch request for the given cache directives.
    if (!cacheDirIds.isEmpty())
        catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (errorMessages.size() > 0) {
        errorMessages.add("Please refer to the catalogd error log for details " + "regarding the failed un/caching operations.");
        response.getResult().setStatus(new TStatus(TErrorCode.INTERNAL_ERROR, errorMessages));
    } else {
        response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    }
    // Perform an incremental refresh to load new/modified partitions and files.
    Table refreshedTbl = catalog_.reloadTable(tblName.toThrift());
    response.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    response.getResult().setVersion(response.getResult().getUpdated_catalog_object().getCatalog_version());
    return response;
}
#end_block

#method_before
public void addColumn(Column col) {
    colsByPos_.add(col);
    colsByName_.put(col.getName().toLowerCase(), col);
}
#method_after
public void addColumn(Column col) {
    colsByPos_.add(col);
    colsByName_.put(col.getName().toLowerCase(), col);
    ((StructType) type_.getItemType()).addField(new StructField(col.getName(), col.getType(), col.getComment()));
}
#end_block

#method_before
public void clearColumns() {
    colsByPos_.clear();
    colsByName_.clear();
}
#method_after
public void clearColumns() {
    colsByPos_.clear();
    colsByName_.clear();
    ((StructType) type_.getItemType()).clearFields();
}
#end_block

#method_before
protected void loadFromThrift(TTable thriftTable) throws TableLoadingException {
    List<TColumn> columns = new ArrayList<TColumn>();
    columns.addAll(thriftTable.getClustering_columns());
    columns.addAll(thriftTable.getColumns());
    colsByPos_.clear();
    colsByPos_.ensureCapacity(columns.size());
    for (int i = 0; i < columns.size(); ++i) {
        Column col = Column.fromThrift(columns.get(i));
        colsByPos_.add(col.getPosition(), col);
        colsByName_.put(col.getName().toLowerCase(), col);
    }
    numClusteringCols_ = thriftTable.getClustering_columns().size();
    // Estimated number of rows
    numRows_ = thriftTable.isSetTable_stats() ? thriftTable.getTable_stats().getNum_rows() : -1;
    // Default to READ_WRITE access if the field is not set.
    accessLevel_ = thriftTable.isSetAccess_level() ? thriftTable.getAccess_level() : TAccessLevel.READ_WRITE;
}
#method_after
protected void loadFromThrift(TTable thriftTable) throws TableLoadingException {
    List<TColumn> columns = new ArrayList<TColumn>();
    columns.addAll(thriftTable.getClustering_columns());
    columns.addAll(thriftTable.getColumns());
    colsByPos_.clear();
    colsByPos_.ensureCapacity(columns.size());
    for (int i = 0; i < columns.size(); ++i) {
        Column col = Column.fromThrift(columns.get(i));
        colsByPos_.add(col.getPosition(), col);
        colsByName_.put(col.getName().toLowerCase(), col);
        ((StructType) type_.getItemType()).addField(new StructField(col.getName(), col.getType(), col.getComment()));
    }
    numClusteringCols_ = thriftTable.getClustering_columns().size();
    // Estimated number of rows
    numRows_ = thriftTable.isSetTable_stats() ? thriftTable.getTable_stats().getNum_rows() : -1;
    // Default to READ_WRITE access if the field is not set.
    accessLevel_ = thriftTable.isSetAccess_level() ? thriftTable.getAccess_level() : TAccessLevel.READ_WRITE;
}
#end_block

#method_before
private void addColumnsFromFieldSchemas(List<FieldSchema> fieldSchemas) throws TableLoadingException {
    int pos = 0;
    for (FieldSchema s : fieldSchemas) {
        Type type = parseColumnType(s);
        // Check if we support partitioning on columns of such a type.
        if (pos < numClusteringCols_ && !type.supportsTablePartitioning()) {
            throw new TableLoadingException(String.format("Failed to load metadata for table '%s' because of " + "unsupported partition-column type '%s' in partition column '%s'", getName(), type.toString(), s.getName()));
        }
        Column col = new Column(s.getName(), type, s.getComment(), pos);
        addColumn(col);
        ++pos;
    }
}
#method_after
private void addColumnsFromFieldSchemas(List<FieldSchema> fieldSchemas) throws TableLoadingException {
    int pos = colsByPos_.size();
    for (FieldSchema s : fieldSchemas) {
        Type type = parseColumnType(s);
        // Check if we support partitioning on columns of such a type.
        if (pos < numClusteringCols_ && !type.supportsTablePartitioning()) {
            throw new TableLoadingException(String.format("Failed to load metadata for table '%s' because of " + "unsupported partition-column type '%s' in partition column '%s'", getFullName(), type.toString(), s.getName()));
        }
        Column col = new Column(s.getName(), type, s.getComment(), pos);
        addColumn(col);
        ++pos;
    }
}
#end_block

#method_before
@Override
public /**
 * Load the table metadata and reuse metadata to speed up metadata loading.
 * If the lastDdlTime has not been changed, that means the Hive metastore metadata has
 * not been changed. Reuses the old Hive partition metadata from cachedEntry.
 * To speed up Hdfs metadata loading, if a file's mtime has not been changed, reuses
 * the old file block metadata from old value.
 *
 * There are several cases where the cachedEntry might be reused incorrectly:
 * 1. an ALTER TABLE ADD PARTITION or dynamic partition insert is executed through
 *    Hive. This does not update the lastDdlTime.
 * 2. Hdfs rebalancer is executed. This changes the block locations but won't update
 *    the mtime (file modification time).
 * If any of these occurs, user has to execute "invalidate metadata" to invalidate the
 * metadata cache of the table to trigger a fresh load.
 */
void load(Table cachedEntry, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    LOG.debug("load table: " + db_.getName() + "." + name_);
    // turn all exceptions into TableLoadingException
    try {
        // set nullPartitionKeyValue from the hive conf.
        nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
        // set NULL indicator string from table properties
        nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
        if (nullColumnValue_ == null)
            nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
        String inputFormat = msTbl.getSd().getInputFormat();
        if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO) {
            // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
            // taking precedence.
            List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
            schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
            schemaSearchLocations.add(getMetaStoreTable().getParameters());
            avroSchema_ = HdfsTable.getAvroSchema(schemaSearchLocations, getFullName());
            String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
            if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
                // If the SerDe library is null or set to LazySimpleSerDe or is null, it
                // indicates there is an issue with the table metadata since Avro table need a
                // non-native serde. Instead of failing to load the table, fall back to
                // using the fields from the storage descriptor (same as Hive).
                nonPartFieldSchemas_.addAll(msTbl.getSd().getCols());
            } else {
                // Load the fields from the Avro schema.
                // Since Avro does not include meta-data for CHAR or VARCHAR, an Avro type of
                // "string" is used for CHAR, VARCHAR and STRING. Default back to the storage
                // descriptor to determine the the type for "string"
                List<FieldSchema> sdTypes = msTbl.getSd().getCols();
                int i = 0;
                List<Column> avroTypeList = AvroSchemaParser.parse(avroSchema_);
                boolean canFallBack = sdTypes.size() == avroTypeList.size();
                for (Column parsedCol : avroTypeList) {
                    FieldSchema fs = new FieldSchema();
                    fs.setName(parsedCol.getName());
                    String avroType = parsedCol.getType().toSql();
                    if (avroType.toLowerCase().equals("string") && canFallBack) {
                        fs.setType(sdTypes.get(i).getType());
                    } else {
                        fs.setType(avroType);
                    }
                    fs.setComment("from deserializer");
                    nonPartFieldSchemas_.add(fs);
                    i++;
                }
            }
        } else {
            nonPartFieldSchemas_.addAll(msTbl.getSd().getCols());
        }
        // The number of clustering columns is the number of partition keys.
        numClusteringCols_ = msTbl.getPartitionKeys().size();
        // Add all columns to the table. Ordering is important: partition columns first,
        // then all other columns.
        addColumnsFromFieldSchemas(msTbl.getPartitionKeys());
        addColumnsFromFieldSchemas(nonPartFieldSchemas_);
        loadAllColumnStats(client);
        // Collect the list of partitions to use for the table. Partitions may be reused
        // from the existing cached table entry (if one exists), read from the metastore,
        // or a mix of both. Whether or not a partition is reused depends on whether
        // the table or partition has been modified.
        List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
        if (cachedEntry == null || !(cachedEntry instanceof HdfsTable) || cachedEntry.lastDdlTime_ != lastDdlTime_) {
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
        } else {
            // The table was already in the metadata cache and it has not been modified.
            Preconditions.checkArgument(cachedEntry instanceof HdfsTable);
            HdfsTable cachedHdfsTableEntry = (HdfsTable) cachedEntry;
            // Set of partition names that have been modified. Partitions in this Set need to
            // be reloaded from the metastore.
            Set<String> modifiedPartitionNames = Sets.newHashSet();
            // "temp" table that doesn't actually exist in the metastore.
            if (cachedEntry != this) {
                // Since the table has not been modified, we might be able to reuse some of the
                // old partition metadata if the individual partitions have not been modified.
                // First get a list of all the partition names for this table from the
                // metastore, this is much faster than listing all the Partition objects.
                modifiedPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
            }
            int totalPartitions = modifiedPartitionNames.size();
            // Get all the partitions from the cached entry that have not been modified.
            for (HdfsPartition cachedPart : cachedHdfsTableEntry.getPartitions()) {
                // Skip the default partition and any partitions that have been modified.
                if (cachedPart.isDirty() || cachedPart.isDefaultPartition()) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition cachedMsPart = cachedPart.toHmsPartition();
                if (cachedMsPart == null)
                    continue;
                // This is a partition we already know about and it hasn't been modified.
                // No need to reload the metadata.
                String cachedPartName = cachedPart.getPartitionName();
                if (modifiedPartitionNames.contains(cachedPartName)) {
                    msPartitions.add(cachedMsPart);
                    modifiedPartitionNames.remove(cachedPartName);
                }
            }
            LOG.info(String.format("Incrementally refreshing %d/%d partitions.", modifiedPartitionNames.size(), totalPartitions));
            // No need to make the metastore call if no partitions are to be updated.
            if (modifiedPartitionNames.size() > 0) {
                // Now reload the the remaining partitions.
                msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(modifiedPartitionNames), db_.getName(), name_));
            }
        }
        Map<String, List<FileDescriptor>> oldFileDescMap = null;
        if (cachedEntry != null && cachedEntry instanceof HdfsTable) {
            HdfsTable cachedHdfsTable = (HdfsTable) cachedEntry;
            oldFileDescMap = cachedHdfsTable.fileDescMap_;
            hostIndex_.populate(cachedHdfsTable.hostIndex_.getList());
        }
        loadPartitions(msPartitions, msTbl, oldFileDescMap);
        // load table stats
        numRows_ = getRowCount(msTbl.getParameters());
        LOG.debug("table #rows=" + Long.toString(numRows_));
        // to the table's numRows.
        if (numClusteringCols_ == 0 && !partitions_.isEmpty()) {
            // Unpartitioned tables have a 'dummy' partition and a default partition.
            // Temp tables used in CTAS statements have one partition.
            Preconditions.checkState(partitions_.size() == 2 || partitions_.size() == 1);
            for (HdfsPartition p : partitions_) {
                p.setNumRows(numRows_);
            }
        }
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#method_after
@Override
public /**
 * Load the table metadata and reuse metadata to speed up metadata loading.
 * If the lastDdlTime has not been changed, that means the Hive metastore metadata has
 * not been changed. Reuses the old Hive partition metadata from cachedEntry.
 * To speed up Hdfs metadata loading, if a file's mtime has not been changed, reuses
 * the old file block metadata from old value.
 *
 * There are several cases where the cachedEntry might be reused incorrectly:
 * 1. an ALTER TABLE ADD PARTITION or dynamic partition insert is executed through
 *    Hive. This does not update the lastDdlTime.
 * 2. Hdfs rebalancer is executed. This changes the block locations but won't update
 *    the mtime (file modification time).
 * If any of these occurs, user has to execute "invalidate metadata" to invalidate the
 * metadata cache of the table to trigger a fresh load.
 */
void load(Table cachedEntry, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    LOG.debug("load table: " + db_.getName() + "." + name_);
    // turn all exceptions into TableLoadingException
    try {
        // set nullPartitionKeyValue from the hive conf.
        nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
        // set NULL indicator string from table properties
        nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
        if (nullColumnValue_ == null)
            nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
        String inputFormat = msTbl.getSd().getInputFormat();
        if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO) {
            // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
            // taking precedence.
            List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
            schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
            schemaSearchLocations.add(getMetaStoreTable().getParameters());
            avroSchema_ = HdfsTable.getAvroSchema(schemaSearchLocations, getFullName());
            String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
            if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
                // If the SerDe library is null or set to LazySimpleSerDe or is null, it
                // indicates there is an issue with the table metadata since Avro table need a
                // non-native serde. Instead of failing to load the table, fall back to
                // using the fields from the storage descriptor (same as Hive).
                nonPartFieldSchemas_.addAll(msTbl.getSd().getCols());
            } else {
                // Load the fields from the Avro schema.
                // Since Avro does not include meta-data for CHAR or VARCHAR, an Avro type of
                // "string" is used for CHAR, VARCHAR and STRING. Default back to the storage
                // descriptor to determine the the type for "string"
                List<FieldSchema> sdTypes = msTbl.getSd().getCols();
                int i = 0;
                List<Column> avroTypeList = AvroSchemaParser.parse(avroSchema_);
                boolean canFallBack = sdTypes.size() == avroTypeList.size();
                for (Column parsedCol : avroTypeList) {
                    FieldSchema fs = new FieldSchema();
                    fs.setName(parsedCol.getName());
                    String avroType = parsedCol.getType().toSql();
                    if (avroType.toLowerCase().equals("string") && canFallBack) {
                        fs.setType(sdTypes.get(i).getType());
                    } else {
                        fs.setType(avroType);
                    }
                    fs.setComment("from deserializer");
                    nonPartFieldSchemas_.add(fs);
                    i++;
                }
            }
        } else {
            nonPartFieldSchemas_.addAll(msTbl.getSd().getCols());
        }
        // The number of clustering columns is the number of partition keys.
        numClusteringCols_ = msTbl.getPartitionKeys().size();
        // Add all columns to the table. Ordering is important: partition columns first,
        // then all other columns.
        addColumnsFromFieldSchemas(msTbl.getPartitionKeys());
        addColumnsFromFieldSchemas(nonPartFieldSchemas_);
        loadAllColumnStats(client);
        // Collect the list of partitions to use for the table. Partitions may be reused
        // from the existing cached table entry (if one exists), read from the metastore,
        // or a mix of both. Whether or not a partition is reused depends on whether
        // the table or partition has been modified.
        List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
        if (cachedEntry == null || !(cachedEntry instanceof HdfsTable) || cachedEntry.lastDdlTime_ != lastDdlTime_) {
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
        } else {
            // The table was already in the metadata cache and it has not been modified.
            Preconditions.checkArgument(cachedEntry instanceof HdfsTable);
            HdfsTable cachedHdfsTableEntry = (HdfsTable) cachedEntry;
            // Set of partition names that have been modified. Partitions in this Set need to
            // be reloaded from the metastore.
            Set<String> modifiedPartitionNames = Sets.newHashSet();
            // "temp" table that doesn't actually exist in the metastore.
            if (cachedEntry != this) {
                // Since the table has not been modified, we might be able to reuse some of the
                // old partition metadata if the individual partitions have not been modified.
                // First get a list of all the partition names for this table from the
                // metastore, this is much faster than listing all the Partition objects.
                modifiedPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
            }
            int totalPartitions = modifiedPartitionNames.size();
            // Get all the partitions from the cached entry that have not been modified.
            for (HdfsPartition cachedPart : cachedHdfsTableEntry.getPartitions()) {
                // Skip the default partition and any partitions that have been modified.
                if (cachedPart.isDirty() || cachedPart.isDefaultPartition()) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition cachedMsPart = cachedPart.toHmsPartition();
                if (cachedMsPart == null)
                    continue;
                // This is a partition we already know about and it hasn't been modified.
                // No need to reload the metadata.
                String cachedPartName = cachedPart.getPartitionName();
                if (modifiedPartitionNames.contains(cachedPartName)) {
                    msPartitions.add(cachedMsPart);
                    modifiedPartitionNames.remove(cachedPartName);
                }
            }
            LOG.info(String.format("Incrementally refreshing %d/%d partitions.", modifiedPartitionNames.size(), totalPartitions));
            // No need to make the metastore call if no partitions are to be updated.
            if (modifiedPartitionNames.size() > 0) {
                // Now reload the the remaining partitions.
                msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(modifiedPartitionNames), db_.getName(), name_));
            }
        }
        Map<String, List<FileDescriptor>> oldFileDescMap = null;
        if (cachedEntry != null && cachedEntry instanceof HdfsTable) {
            HdfsTable cachedHdfsTable = (HdfsTable) cachedEntry;
            oldFileDescMap = cachedHdfsTable.fileDescMap_;
            hostIndex_.populate(cachedHdfsTable.hostIndex_.getList());
        }
        loadPartitions(msPartitions, msTbl, oldFileDescMap);
        // load table stats
        numRows_ = getRowCount(msTbl.getParameters());
        LOG.debug("table #rows=" + Long.toString(numRows_));
        // to the table's numRows.
        if (numClusteringCols_ == 0 && !partitions_.isEmpty()) {
            // Unpartitioned tables have a 'dummy' partition and a default partition.
            // Temp tables used in CTAS statements have one partition.
            Preconditions.checkState(partitions_.size() == 2 || partitions_.size() == 1);
            for (HdfsPartition p : partitions_) {
                p.setNumRows(numRows_);
            }
        }
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + getFullName(), e);
    }
}
#end_block

#method_before
public TResultSet getTableStats() {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    for (int i = 0; i < numClusteringCols_; ++i) {
        // Add the partition-key values as strings for simplicity.
        Column partCol = getColumns().get(i);
        TColumn colDesc = new TColumn(partCol.getName(), Type.STRING.toThrift());
        resultSchema.addToColumns(colDesc);
    }
    resultSchema.addToColumns(new TColumn("#Rows", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("#Files", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("Size", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Bytes Cached", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Cache Replication", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Format", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Incremental stats", Type.STRING.toThrift()));
    // Pretty print partitions and their stats.
    ArrayList<HdfsPartition> orderedPartitions = Lists.newArrayList(partitions_);
    Collections.sort(orderedPartitions);
    long totalCachedBytes = 0L;
    for (HdfsPartition p : orderedPartitions) {
        // Ignore dummy default partition.
        if (p.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID)
            continue;
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        // Add the partition-key values (as strings for simplicity).
        for (LiteralExpr expr : p.getPartitionValues()) {
            rowBuilder.add(expr.getStringValue());
        }
        // Add number of rows, files, bytes, cache stats, and file format.
        rowBuilder.add(p.getNumRows()).add(p.getFileDescriptors().size()).addBytes(p.getSize());
        if (!p.isMarkedCached()) {
            // Helps to differentiate partitions that have 0B cached versus partitions
            // that are not marked as cached.
            rowBuilder.add("NOT CACHED");
            rowBuilder.add("NOT CACHED");
        } else {
            // Calculate the number the number of bytes that are cached.
            long cachedBytes = 0L;
            for (FileDescriptor fd : p.getFileDescriptors()) {
                for (THdfsFileBlock fb : fd.getFileBlocks()) {
                    if (fb.getIs_replica_cached().contains(true)) {
                        cachedBytes += fb.getLength();
                    }
                }
            }
            totalCachedBytes += cachedBytes;
            rowBuilder.addBytes(cachedBytes);
            // Extract cache replication factor from the parameters of the table
            // if the table is not partitioned or directly from the partition.
            Short rep = HdfsCachingUtil.getCachedCacheReplication(numClusteringCols_ == 0 ? p.getTable().getMetaStoreTable().getParameters() : p.getParameters());
            rowBuilder.add(rep.toString());
        }
        rowBuilder.add(p.getInputFormatDescriptor().getFileFormat().toString());
        rowBuilder.add(String.valueOf(p.hasIncrementalStats()));
        result.addToRows(rowBuilder.get());
    }
    // For partitioned tables add a summary row at the bottom.
    if (numClusteringCols_ > 0) {
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        int numEmptyCells = numClusteringCols_ - 1;
        rowBuilder.add("Total");
        for (int i = 0; i < numEmptyCells; ++i) {
            rowBuilder.add("");
        }
        // Total num rows, files, and bytes (leave format empty).
        rowBuilder.add(numRows_).add(numHdfsFiles_).addBytes(totalHdfsBytes_).addBytes(totalCachedBytes).add("").add("").add("");
        result.addToRows(rowBuilder.get());
    }
    return result;
}
#method_after
public TResultSet getTableStats() {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    for (int i = 0; i < numClusteringCols_; ++i) {
        // Add the partition-key values as strings for simplicity.
        Column partCol = getColumns().get(i);
        TColumn colDesc = new TColumn(partCol.getName(), Type.STRING.toThrift());
        resultSchema.addToColumns(colDesc);
    }
    resultSchema.addToColumns(new TColumn("#Rows", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("#Files", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("Size", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Bytes Cached", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Cache Replication", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Format", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Incremental stats", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Location", Type.STRING.toThrift()));
    // Pretty print partitions and their stats.
    ArrayList<HdfsPartition> orderedPartitions = Lists.newArrayList(partitions_);
    Collections.sort(orderedPartitions);
    long totalCachedBytes = 0L;
    for (HdfsPartition p : orderedPartitions) {
        // Ignore dummy default partition.
        if (p.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID)
            continue;
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        // Add the partition-key values (as strings for simplicity).
        for (LiteralExpr expr : p.getPartitionValues()) {
            rowBuilder.add(expr.getStringValue());
        }
        // Add number of rows, files, bytes, cache stats, and file format.
        rowBuilder.add(p.getNumRows()).add(p.getFileDescriptors().size()).addBytes(p.getSize());
        if (!p.isMarkedCached()) {
            // Helps to differentiate partitions that have 0B cached versus partitions
            // that are not marked as cached.
            rowBuilder.add("NOT CACHED");
            rowBuilder.add("NOT CACHED");
        } else {
            // Calculate the number the number of bytes that are cached.
            long cachedBytes = 0L;
            for (FileDescriptor fd : p.getFileDescriptors()) {
                for (THdfsFileBlock fb : fd.getFileBlocks()) {
                    if (fb.getIs_replica_cached().contains(true)) {
                        cachedBytes += fb.getLength();
                    }
                }
            }
            totalCachedBytes += cachedBytes;
            rowBuilder.addBytes(cachedBytes);
            // Extract cache replication factor from the parameters of the table
            // if the table is not partitioned or directly from the partition.
            Short rep = HdfsCachingUtil.getCachedCacheReplication(numClusteringCols_ == 0 ? p.getTable().getMetaStoreTable().getParameters() : p.getParameters());
            rowBuilder.add(rep.toString());
        }
        rowBuilder.add(p.getInputFormatDescriptor().getFileFormat().toString());
        rowBuilder.add(String.valueOf(p.hasIncrementalStats()));
        rowBuilder.add(p.getLocation());
        result.addToRows(rowBuilder.get());
    }
    // For partitioned tables add a summary row at the bottom.
    if (numClusteringCols_ > 0) {
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        int numEmptyCells = numClusteringCols_ - 1;
        rowBuilder.add("Total");
        for (int i = 0; i < numEmptyCells; ++i) {
            rowBuilder.add("");
        }
        // Total num rows, files, and bytes (leave format empty).
        rowBuilder.add(numRows_).add(numHdfsFiles_).addBytes(totalHdfsBytes_).addBytes(totalCachedBytes).add("").add("").add("").add("");
        result.addToRows(rowBuilder.get());
    }
    return result;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    if (resolvedPath_ == null) {
        try {
            resolvedPath_ = analyzer.resolvePath(rawPath_, PathType.SLOT_REF);
        } catch (TableLoadingException e) {
            // Should never happen because we only check registered table aliases.
            Preconditions.checkState(false);
        }
    }
    Preconditions.checkNotNull(resolvedPath_);
    desc_ = analyzer.registerSlotRef(resolvedPath_);
    type_ = desc_.getType();
    if (!type_.isSupported()) {
        throw new AnalysisException("Unsupported type '" + type_.toSql() + "' in '" + toSql() + "'.");
    }
    if (type_.isInvalid()) {
        // HMS string.
        throw new AnalysisException("Unsupported type in '" + toSql() + "'.");
    }
    numDistinctValues_ = desc_.getStats().getNumDistinctValues();
    if (type_.isBoolean())
        selectivity_ = DEFAULT_SELECTIVITY;
    isAnalyzed_ = true;
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    Path resolvedPath = null;
    try {
        resolvedPath = analyzer.resolvePath(rawPath_, PathType.SLOT_REF);
    } catch (TableLoadingException e) {
        // Should never happen because we only check registered table aliases.
        Preconditions.checkState(false);
    }
    Preconditions.checkNotNull(resolvedPath);
    desc_ = analyzer.registerSlotRef(resolvedPath);
    type_ = desc_.getType();
    if (!type_.isSupported()) {
        throw new AnalysisException("Unsupported type '" + type_.toSql() + "' in '" + toSql() + "'.");
    }
    if (type_.isInvalid()) {
        // HMS string.
        throw new AnalysisException("Unsupported type in '" + toSql() + "'.");
    }
    numDistinctValues_ = desc_.getStats().getNumDistinctValues();
    if (type_.isBoolean())
        selectivity_ = DEFAULT_SELECTIVITY;
    isAnalyzed_ = true;
}
#end_block

#method_before
public Path getResolvedPath() {
    Preconditions.checkState(isAnalyzed_);
    return resolvedPath_;
}
#method_after
public Path getResolvedPath() {
    Preconditions.checkState(isAnalyzed_);
    return desc_.getPath();
}
#end_block

#method_before
public List<Integer> getAbsolutePath() {
    Preconditions.checkNotNull(parent_);
    if (path_ == null)
        return Collections.emptyList();
    if (parent_.getPath() == null)
        return Collections.emptyList();
    return Lists.newArrayList(path_.getAbsolutePath());
}
#method_after
public List<Integer> getAbsolutePath() {
    Preconditions.checkNotNull(parent_);
    // path pointing into the inline-view tuple (which has no path).
    if (path_ == null || parent_.getPath() == null)
        return Collections.emptyList();
    return Lists.newArrayList(path_.getAbsolutePath());
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    // Start out with table refs to establish aliases.
    // the one to the left of tblRef
    TableRef leftTblRef = null;
    for (int i = 0; i < tableRefs_.size(); ++i) {
        // Resolve and replace non-InlineViewRef table refs with a BaseTableRef or ViewRef.
        TableRef tblRef = tableRefs_.get(i);
        tblRef = analyzer.resolveTableRef(tblRef);
        Preconditions.checkNotNull(tblRef);
        tableRefs_.set(i, tblRef);
        tblRef.setLeftTblRef(leftTblRef);
        try {
            tblRef.analyze(analyzer);
        } catch (AnalysisException e) {
            // Only re-throw the exception if no tables are missing.
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
        leftTblRef = tblRef;
    }
    // There is no reason to proceed with analysis past this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    // analyze plan hints from select list
    selectList_.analyzePlanHints(analyzer);
    // populate resultExprs_, aliasSmap_, and colLabels_
    for (int i = 0; i < selectList_.getItems().size(); ++i) {
        SelectListItem item = selectList_.getItems().get(i);
        if (item.isStar()) {
            if (item.getPath() != null) {
                expandStar(analyzer, item.getPath());
            } else {
                expandStar(analyzer);
            }
        } else {
            // Analyze the resultExpr before generating a label to ensure enforcement
            // of expr child and depth limits (toColumn() label may call toSql()).
            item.getExpr().analyze(analyzer);
            if (item.getExpr().contains(Predicates.instanceOf(Subquery.class))) {
                throw new AnalysisException("Subqueries are not supported in the select list.");
            }
            resultExprs_.add(item.getExpr());
            String label = item.toColumnLabel(i, analyzer.useHiveColLabels());
            SlotRef aliasRef = new SlotRef(label);
            Expr existingAliasExpr = aliasSmap_.get(aliasRef);
            if (existingAliasExpr != null && !existingAliasExpr.equals(item.getExpr())) {
                // If we have already seen this alias, it refers to more than one column and
                // therefore is ambiguous.
                ambiguousAliasList_.add(aliasRef);
            }
            aliasSmap_.put(aliasRef, item.getExpr().clone());
            colLabels_.add(label);
        }
    }
    // non-root stmts to support views.
    for (Expr expr : resultExprs_) {
        if (expr.getType().isComplexType() && analyzer.isRootAnalyzer()) {
            throw new AnalysisException(String.format("Expr '%s' in select list of root statement returns a complex type '%s'.\n" + "Only scalar types are allowed in the select list of the root statement.", expr.toSql(), expr.getType().toSql()));
        }
    }
    if (TreeNode.contains(resultExprs_, AnalyticExpr.class)) {
        if (tableRefs_.isEmpty()) {
            throw new AnalysisException("Analytic expressions require FROM clause.");
        }
        // will get substituted away
        if (selectList_.isDistinct()) {
            throw new AnalysisException("cannot combine SELECT DISTINCT with analytic functions");
        }
    }
    if (whereClause_ != null) {
        whereClause_.analyze(analyzer);
        if (whereClause_.contains(Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function not allowed in WHERE clause");
        }
        whereClause_.checkReturnsBool("WHERE clause", false);
        Expr e = whereClause_.findFirstOf(AnalyticExpr.class);
        if (e != null) {
            throw new AnalysisException("WHERE clause must not contain analytic expressions: " + e.toSql());
        }
        analyzer.registerConjuncts(whereClause_, false);
    }
    createSortInfo(analyzer);
    analyzeAggregation(analyzer);
    analyzeAnalytics(analyzer);
    if (evaluateOrderBy_)
        createSortTupleInfo(analyzer);
    // Remember the SQL string before inline-view expression substitution.
    sqlString_ = toSql();
    resolveInlineViewRefs(analyzer);
    // block has no aggregation, then mark this block as returning an empty result set.
    if (analyzer.hasEmptySpjResultSet() && aggInfo_ == null) {
        analyzer.setHasEmptyResultSet();
    }
    ColumnLineageGraph graph = analyzer.getColumnLineageGraph();
    if (aggInfo_ != null && !aggInfo_.getAggregateExprs().isEmpty()) {
        graph.addDependencyPredicates(aggInfo_.getGroupingExprs());
    }
    if (sortInfo_ != null && hasLimit()) {
        // When there is a LIMIT clause in conjunction with an ORDER BY, the ordering exprs
        // must be added in the column lineage graph.
        graph.addDependencyPredicates(sortInfo_.getOrderingExprs());
    }
    if (aggInfo_ != null)
        LOG.debug("post-analysis " + aggInfo_.debugString());
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    // Start out with table refs to establish aliases.
    // the one to the left of tblRef
    TableRef leftTblRef = null;
    for (int i = 0; i < tableRefs_.size(); ++i) {
        // Resolve and replace non-InlineViewRef table refs with a BaseTableRef or ViewRef.
        TableRef tblRef = tableRefs_.get(i);
        tblRef = analyzer.resolveTableRef(tblRef);
        Preconditions.checkNotNull(tblRef);
        tableRefs_.set(i, tblRef);
        tblRef.setLeftTblRef(leftTblRef);
        try {
            tblRef.analyze(analyzer);
        } catch (AnalysisException e) {
            // Only re-throw the exception if no tables are missing.
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
        leftTblRef = tblRef;
    }
    // There is no reason to proceed with analysis past this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    // analyze plan hints from select list
    selectList_.analyzePlanHints(analyzer);
    // populate resultExprs_, aliasSmap_, and colLabels_
    for (int i = 0; i < selectList_.getItems().size(); ++i) {
        SelectListItem item = selectList_.getItems().get(i);
        if (item.isStar()) {
            if (item.getRawPath() != null) {
                Path resolvedPath = analyzeStarPath(item.getRawPath(), analyzer);
                expandStar(resolvedPath, analyzer);
            } else {
                expandStar(analyzer);
            }
        } else {
            // Analyze the resultExpr before generating a label to ensure enforcement
            // of expr child and depth limits (toColumn() label may call toSql()).
            item.getExpr().analyze(analyzer);
            if (item.getExpr().contains(Predicates.instanceOf(Subquery.class))) {
                throw new AnalysisException("Subqueries are not supported in the select list.");
            }
            resultExprs_.add(item.getExpr());
            String label = item.toColumnLabel(i, analyzer.useHiveColLabels());
            SlotRef aliasRef = new SlotRef(label);
            Expr existingAliasExpr = aliasSmap_.get(aliasRef);
            if (existingAliasExpr != null && !existingAliasExpr.equals(item.getExpr())) {
                // If we have already seen this alias, it refers to more than one column and
                // therefore is ambiguous.
                ambiguousAliasList_.add(aliasRef);
            }
            aliasSmap_.put(aliasRef, item.getExpr().clone());
            colLabels_.add(label);
        }
    }
    // to serialize them in a meaningful way.
    for (Expr expr : resultExprs_) {
        if (expr.getType().isComplexType()) {
            throw new AnalysisException(String.format("Expr '%s' in select list returns a complex type '%s'.\n" + "Only scalar types are allowed in the select list.", expr.toSql(), expr.getType().toSql()));
        }
    }
    if (TreeNode.contains(resultExprs_, AnalyticExpr.class)) {
        if (tableRefs_.isEmpty()) {
            throw new AnalysisException("Analytic expressions require FROM clause.");
        }
        // will get substituted away
        if (selectList_.isDistinct()) {
            throw new AnalysisException("cannot combine SELECT DISTINCT with analytic functions");
        }
    }
    if (whereClause_ != null) {
        whereClause_.analyze(analyzer);
        if (whereClause_.contains(Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function not allowed in WHERE clause");
        }
        whereClause_.checkReturnsBool("WHERE clause", false);
        Expr e = whereClause_.findFirstOf(AnalyticExpr.class);
        if (e != null) {
            throw new AnalysisException("WHERE clause must not contain analytic expressions: " + e.toSql());
        }
        analyzer.registerConjuncts(whereClause_, false);
    }
    createSortInfo(analyzer);
    analyzeAggregation(analyzer);
    analyzeAnalytics(analyzer);
    if (evaluateOrderBy_)
        createSortTupleInfo(analyzer);
    // Remember the SQL string before inline-view expression substitution.
    sqlString_ = toSql();
    resolveInlineViewRefs(analyzer);
    // block has no aggregation, then mark this block as returning an empty result set.
    if (analyzer.hasEmptySpjResultSet() && aggInfo_ == null) {
        analyzer.setHasEmptyResultSet();
    }
    ColumnLineageGraph graph = analyzer.getColumnLineageGraph();
    if (aggInfo_ != null && !aggInfo_.getAggregateExprs().isEmpty()) {
        graph.addDependencyPredicates(aggInfo_.getGroupingExprs());
    }
    if (sortInfo_ != null && hasLimit()) {
        // When there is a LIMIT clause in conjunction with an ORDER BY, the ordering exprs
        // must be added in the column lineage graph.
        graph.addDependencyPredicates(sortInfo_.getOrderingExprs());
    }
    if (aggInfo_ != null)
        LOG.debug("post-analysis " + aggInfo_.debugString());
}
#end_block

#method_before
private void expandStar(Analyzer analyzer) throws AnalysisException {
    if (tableRefs_.isEmpty()) {
        throw new AnalysisException("'*' expression in select list requires FROM clause.");
    }
    // expand in From clause order
    for (TableRef tableRef : tableRefs_) {
        if (analyzer.isSemiJoined(tableRef.getId()))
            continue;
        expandStar(analyzer, tableRef.getDesc());
    }
}
#method_after
private void expandStar(Analyzer analyzer) throws AnalysisException {
    if (tableRefs_.isEmpty()) {
        throw new AnalysisException("'*' expression in select list requires FROM clause.");
    }
    // expand in From clause order
    for (TableRef tableRef : tableRefs_) {
        if (analyzer.isSemiJoined(tableRef.getId()))
            continue;
        Path resolvedPath = new Path(tableRef.getDesc(), Collections.<String>emptyList());
        Preconditions.checkState(resolvedPath.resolve());
        expandStar(resolvedPath, analyzer);
    }
}
#end_block

#method_before
private void expandStar(Analyzer analyzer, TupleDescriptor desc) throws AnalysisException {
    Preconditions.checkState(!analyzer.isSemiJoined(desc.getId()));
    if (desc.getPath().isEmpty() && desc.getParentTable() != null) {
        // The given tuple descriptor materializes the top level of a base table.
        for (Column c : desc.getParentTable().getColumnsInHiveOrder()) {
            SlotRef slotRef = new SlotRef(Path.createRawPath(desc.getAlias(), c.getName()));
            slotRef.analyze(analyzer);
            resultExprs_.add(slotRef);
            colLabels_.add(c.getName().toLowerCase());
        }
    } else {
        // The given tuple descriptor materializes a nested collection.
        StructType structType = desc.getType();
        Preconditions.checkNotNull(structType);
        for (StructField f : structType.getFields()) {
            SlotRef slotRef = new SlotRef(Path.createRawPath(desc.getAlias(), f.getName()));
            slotRef.analyze(analyzer);
            resultExprs_.add(slotRef);
            colLabels_.add(f.getName().toLowerCase());
        }
    }
}
#method_after
private void expandStar(Path resolvedPath, Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(resolvedPath.isResolved());
    if (resolvedPath.destTupleDesc() != null && resolvedPath.destTupleDesc().getTable() != null && resolvedPath.destTupleDesc().getPath().getMatchedTypes().isEmpty()) {
        // The resolved path targets a registered tuple descriptor of a catalog
        // table. Expand the '*' based on the Hive-column order.
        TupleDescriptor tupleDesc = resolvedPath.destTupleDesc();
        Table table = tupleDesc.getTable();
        for (Column c : table.getColumnsInHiveOrder()) {
            addStarResultExpr(resolvedPath, analyzer, c.getName());
        }
    } else {
        // The resolved path does not target the descriptor of a catalog table.
        // Expand '*' based on the destination type of the resolved path.
        Preconditions.checkState(resolvedPath.destType().isStructType());
        StructType structType = (StructType) resolvedPath.destType();
        Preconditions.checkNotNull(structType);
        // map<int,struct<f1,f2,...,fn>>  --> key, f1, f2, ..., fn
        if (structType instanceof CollectionStructType) {
            CollectionStructType cst = (CollectionStructType) structType;
            if (cst.isMapStruct()) {
                addStarResultExpr(resolvedPath, analyzer, Path.MAP_KEY_FIELD_NAME);
            }
            if (cst.getOptionalField().getType().isStructType()) {
                structType = (StructType) cst.getOptionalField().getType();
                for (StructField f : structType.getFields()) {
                    addStarResultExpr(resolvedPath, analyzer, cst.getOptionalField().getName(), f.getName());
                }
            } else if (cst.isMapStruct()) {
                addStarResultExpr(resolvedPath, analyzer, Path.MAP_VALUE_FIELD_NAME);
            } else {
                addStarResultExpr(resolvedPath, analyzer, Path.ARRAY_ITEM_FIELD_NAME);
            }
        } else {
            // Default star expansion.
            for (StructField f : structType.getFields()) {
                addStarResultExpr(resolvedPath, analyzer, f.getName());
            }
        }
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // create smap_ and baseTblSmap_ and register auxiliary eq predicates between our
    // tuple descriptor's slots and our *unresolved* select list exprs;
    // we create these auxiliary predicates so that the analyzer can compute the value
    // transfer graph through this inline view correctly (ie, predicates can get
    // propagated through the view);
    // if the view stmt contains analytic functions, we cannot propagate predicates
    // into the view, because those extra filters would alter the results of the
    // analytic functions (see IMPALA-1243)
    // TODO: relax this a bit by allowing propagation out of the inline view (but
    // not into it)
    boolean createAuxPredicates = !(queryStmt_ instanceof SelectStmt) || !(((SelectStmt) queryStmt_).hasAnalyticInfo());
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i);
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        Path p = new Path(desc_, Lists.newArrayList(colName));
        Preconditions.checkState(p.resolve());
        SlotDescriptor slotDesc = analyzer.registerSlotRef(p);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicates) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getUniqueAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getUniqueAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // not into it)
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i).toLowerCase();
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        Path p = new Path(desc_, Lists.newArrayList(colName));
        Preconditions.checkState(p.resolve());
        SlotDescriptor slotDesc = analyzer.registerSlotRef(p);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicate(colExpr)) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getUniqueAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getUniqueAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#end_block

#method_before
@Override
public TupleDescriptor createTupleDescriptor(Analyzer analyzer) throws AnalysisException {
    int numColLabels = getColLabels().size();
    Preconditions.checkState(numColLabels > 0);
    HashSet<String> uniqueColAliases = Sets.newHashSetWithExpectedSize(numColLabels);
    ArrayList<StructField> fields = Lists.newArrayListWithCapacity(numColLabels);
    for (int i = 0; i < numColLabels; ++i) {
        // inline view select statement has been analyzed. Col label should be filled.
        Expr selectItemExpr = queryStmt_.getResultExprs().get(i);
        String colAlias = getColLabels().get(i).toLowerCase();
        // inline view col cannot have duplicate name
        if (!uniqueColAliases.add(colAlias)) {
            throw new AnalysisException("duplicated inline view column alias: '" + colAlias + "'" + " in inline view " + "'" + getUniqueAlias() + "'");
        }
        StructField field = new StructField(colAlias, selectItemExpr.getType(), null);
        field.setPosition(i);
        fields.add(field);
    }
    // Create the non-materialized tuple and set the fake table in it.
    TupleDescriptor result = analyzer.getDescTbl().createTupleDescriptor("inl-view-" + getUniqueAlias());
    result.setIsMaterialized(false);
    result.setType(new StructType(fields));
    return result;
}
#method_after
@Override
public TupleDescriptor createTupleDescriptor(Analyzer analyzer) throws AnalysisException {
    int numColLabels = getColLabels().size();
    Preconditions.checkState(numColLabels > 0);
    HashSet<String> uniqueColAliases = Sets.newHashSetWithExpectedSize(numColLabels);
    ArrayList<StructField> fields = Lists.newArrayListWithCapacity(numColLabels);
    for (int i = 0; i < numColLabels; ++i) {
        // inline view select statement has been analyzed. Col label should be filled.
        Expr selectItemExpr = queryStmt_.getResultExprs().get(i);
        String colAlias = getColLabels().get(i).toLowerCase();
        // inline view col cannot have duplicate name
        if (!uniqueColAliases.add(colAlias)) {
            throw new AnalysisException("duplicated inline view column alias: '" + colAlias + "'" + " in inline view " + "'" + getUniqueAlias() + "'");
        }
        fields.add(new StructField(colAlias, selectItemExpr.getType(), null));
    }
    // Create the non-materialized tuple and set its type.
    TupleDescriptor result = analyzer.getDescTbl().createTupleDescriptor(getClass().getSimpleName() + " " + getUniqueAlias());
    result.setIsMaterialized(false);
    result.setType(new StructType(fields));
    return result;
}
#end_block

#method_before
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    inputCardinality_ = numRowsEstimate_;
    cardinality_ = numRowsEstimate_;
    cardinality_ *= computeSelectivity();
    cardinality_ = Math.max(0, cardinality_);
    cardinality_ = capAtLimit(cardinality_);
    LOG.debug("computeStats DataSourceScan: cardinality=" + Long.toString(cardinality_));
    numNodes_ = desc_.getParentTable().getNumNodes();
    LOG.debug("computeStats DataSourceScan: #nodes=" + Integer.toString(numNodes_));
}
#method_after
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    inputCardinality_ = numRowsEstimate_;
    cardinality_ = numRowsEstimate_;
    cardinality_ *= computeSelectivity();
    cardinality_ = Math.max(0, cardinality_);
    cardinality_ = capAtLimit(cardinality_);
    LOG.debug("computeStats DataSourceScan: cardinality=" + Long.toString(cardinality_));
    numNodes_ = desc_.getTable().getNumNodes();
    LOG.debug("computeStats DataSourceScan: #nodes=" + Integer.toString(numNodes_));
}
#end_block

#method_before
private void computeScanRangeLocations(Analyzer analyzer) {
    long maxScanRangeLength = analyzer.getQueryCtx().getRequest().getQuery_options().getMax_scan_range_length();
    scanRanges_ = Lists.newArrayList();
    for (HdfsPartition partition : partitions_) {
        Preconditions.checkState(partition.getId() >= 0);
        for (HdfsPartition.FileDescriptor fileDesc : partition.getFileDescriptors()) {
            for (THdfsFileBlock thriftBlock : fileDesc.getFileBlocks()) {
                HdfsPartition.FileBlock block = FileBlock.fromThrift(thriftBlock);
                List<Integer> replicaHostIdxs = block.getReplicaHostIdxs();
                if (replicaHostIdxs.size() == 0) {
                    // TODO: do something meaningful with that
                    continue;
                }
                // Collect the network address and volume ID of all replicas of this block.
                List<TScanRangeLocation> locations = Lists.newArrayList();
                for (int i = 0; i < replicaHostIdxs.size(); ++i) {
                    TScanRangeLocation location = new TScanRangeLocation();
                    // Translate from the host index (local to the HdfsTable) to network address.
                    Integer tableHostIdx = replicaHostIdxs.get(i);
                    TNetworkAddress networkAddress = partition.getTable().getHostIndex().getEntry(tableHostIdx);
                    Preconditions.checkNotNull(networkAddress);
                    // Translate from network address to the global (to this request) host index.
                    Integer globalHostIdx = analyzer.getHostIndex().getIndex(networkAddress);
                    location.setHost_idx(globalHostIdx);
                    location.setVolume_id(block.getDiskId(i));
                    location.setIs_cached(block.isCached(i));
                    locations.add(location);
                }
                // create scan ranges, taking into account maxScanRangeLength
                long currentOffset = block.getOffset();
                long remainingLength = block.getLength();
                while (remainingLength > 0) {
                    long currentLength = remainingLength;
                    if (maxScanRangeLength > 0 && remainingLength > maxScanRangeLength) {
                        currentLength = maxScanRangeLength;
                    }
                    TScanRange scanRange = new TScanRange();
                    scanRange.setHdfs_file_split(new THdfsFileSplit(fileDesc.getFileName(), currentOffset, currentLength, partition.getId(), fileDesc.getFileLength(), fileDesc.getFileCompression()));
                    TScanRangeLocations scanRangeLocations = new TScanRangeLocations();
                    scanRangeLocations.scan_range = scanRange;
                    scanRangeLocations.locations = locations;
                    scanRanges_.add(scanRangeLocations);
                    remainingLength -= currentLength;
                    currentOffset += currentLength;
                }
            }
        }
    }
}
#method_after
private void computeScanRangeLocations(Analyzer analyzer) {
    long maxScanRangeLength = analyzer.getQueryCtx().getRequest().getQuery_options().getMax_scan_range_length();
    scanRanges_ = Lists.newArrayList();
    for (HdfsPartition partition : partitions_) {
        Preconditions.checkState(partition.getId() >= 0);
        for (HdfsPartition.FileDescriptor fileDesc : partition.getFileDescriptors()) {
            for (THdfsFileBlock thriftBlock : fileDesc.getFileBlocks()) {
                HdfsPartition.FileBlock block = FileBlock.fromThrift(thriftBlock);
                List<Integer> replicaHostIdxs = block.getReplicaHostIdxs();
                if (replicaHostIdxs.size() == 0) {
                    // TODO: do something meaningful with that
                    continue;
                }
                // Collect the network address and volume ID of all replicas of this block.
                List<TScanRangeLocation> locations = Lists.newArrayList();
                for (int i = 0; i < replicaHostIdxs.size(); ++i) {
                    TScanRangeLocation location = new TScanRangeLocation();
                    // Translate from the host index (local to the HdfsTable) to network address.
                    Integer tableHostIdx = replicaHostIdxs.get(i);
                    TNetworkAddress networkAddress = partition.getTable().getHostIndex().getEntry(tableHostIdx);
                    Preconditions.checkNotNull(networkAddress);
                    // Translate from network address to the global (to this request) host index.
                    Integer globalHostIdx = analyzer.getHostIndex().getIndex(networkAddress);
                    location.setHost_idx(globalHostIdx);
                    location.setVolume_id(block.getDiskId(i));
                    location.setIs_cached(block.isCached(i));
                    locations.add(location);
                }
                // create scan ranges, taking into account maxScanRangeLength
                long currentOffset = block.getOffset();
                long remainingLength = block.getLength();
                while (remainingLength > 0) {
                    long currentLength = remainingLength;
                    if (maxScanRangeLength > 0 && remainingLength > maxScanRangeLength) {
                        currentLength = maxScanRangeLength;
                    }
                    TScanRange scanRange = new TScanRange();
                    scanRange.setHdfs_file_split(new THdfsFileSplit(fileDesc.getFileName(), currentOffset, currentLength, partition.getId(), fileDesc.getFileLength(), fileDesc.getFileCompression(), fileDesc.getModificationTime()));
                    TScanRangeLocations scanRangeLocations = new TScanRangeLocations();
                    scanRangeLocations.scan_range = scanRange;
                    scanRangeLocations.locations = locations;
                    scanRanges_.add(scanRangeLocations);
                    remainingLength -= currentLength;
                    currentOffset += currentLength;
                }
            }
        }
    }
}
#end_block

#method_before
@Override
protected String getDisplayLabelDetail() {
    HdfsTable table = (HdfsTable) desc_.getParentTable();
    List<String> path = Lists.newArrayList();
    path.add(table.getDb().getName());
    path.add(table.getName());
    Type currentType = table.getType();
    for (Integer relPathIdx : desc_.getPath()) {
        StructType fields = Path.getFields(currentType);
        StructField f = fields.getFields().get(relPathIdx);
        path.add(f.getName());
        Preconditions.checkState(f.getType().isComplexType());
        currentType = f.getType();
    }
    if (desc_.hasExplicitAlias()) {
        return Joiner.on(".").join(path) + " " + desc_.getAlias();
    } else {
        return Joiner.on(".").join(path);
    }
}
#method_after
@Override
protected String getDisplayLabelDetail() {
    HdfsTable table = (HdfsTable) desc_.getTable();
    List<String> path = Lists.newArrayList();
    path.add(table.getDb().getName());
    path.add(table.getName());
    Preconditions.checkNotNull(desc_.getPath());
    if (desc_.hasExplicitAlias()) {
        return desc_.getPath().toString() + " " + desc_.getAlias();
    } else {
        return desc_.getPath().toString();
    }
}
#end_block

#method_before
@Override
protected String getNodeExplainString(String prefix, String detailPrefix, TExplainLevel detailLevel) {
    StringBuilder output = new StringBuilder();
    HdfsTable table = (HdfsTable) desc_.getParentTable();
    output.append(String.format("%s%s [%s", prefix, getDisplayLabel(), getDisplayLabelDetail()));
    if (detailLevel.ordinal() >= TExplainLevel.EXTENDED.ordinal() && fragment_.isPartitioned()) {
        output.append(", " + fragment_.getDataPartition().getExplainString());
    }
    output.append("]\n");
    if (detailLevel.ordinal() >= TExplainLevel.STANDARD.ordinal()) {
        int numPartitions = partitions_.size();
        if (tbl_.getNumClusteringCols() == 0)
            numPartitions = 1;
        output.append(String.format("%spartitions=%s/%s files=%s size=%s", detailPrefix, numPartitions, table.getPartitions().size() - 1, totalFiles_, PrintUtils.printBytes(totalBytes_)));
        output.append("\n");
        if (!conjuncts_.isEmpty()) {
            output.append(detailPrefix + "predicates: " + getExplainString(conjuncts_) + "\n");
        }
    }
    if (detailLevel.ordinal() >= TExplainLevel.EXTENDED.ordinal()) {
        output.append(getStatsExplainString(detailPrefix, detailLevel));
        output.append("\n");
    }
    return output.toString();
}
#method_after
@Override
protected String getNodeExplainString(String prefix, String detailPrefix, TExplainLevel detailLevel) {
    StringBuilder output = new StringBuilder();
    HdfsTable table = (HdfsTable) desc_.getTable();
    output.append(String.format("%s%s [%s", prefix, getDisplayLabel(), getDisplayLabelDetail()));
    if (detailLevel.ordinal() >= TExplainLevel.EXTENDED.ordinal() && fragment_.isPartitioned()) {
        output.append(", " + fragment_.getDataPartition().getExplainString());
    }
    output.append("]\n");
    if (detailLevel.ordinal() >= TExplainLevel.STANDARD.ordinal()) {
        int numPartitions = partitions_.size();
        if (tbl_.getNumClusteringCols() == 0)
            numPartitions = 1;
        output.append(String.format("%spartitions=%s/%s files=%s size=%s", detailPrefix, numPartitions, table.getPartitions().size() - 1, totalFiles_, PrintUtils.printBytes(totalBytes_)));
        output.append("\n");
        if (!conjuncts_.isEmpty()) {
            output.append(detailPrefix + "predicates: " + getExplainString(conjuncts_) + "\n");
        }
    }
    if (detailLevel.ordinal() >= TExplainLevel.EXTENDED.ordinal()) {
        output.append(getStatsExplainString(detailPrefix, detailLevel));
        output.append("\n");
    }
    return output.toString();
}
#end_block

#method_before
@Override
public void computeCosts(TQueryOptions queryOptions) {
    Preconditions.checkNotNull(scanRanges_, "Cost estimation requires scan ranges.");
    if (scanRanges_.isEmpty()) {
        perHostMemCost_ = 0;
        return;
    }
    // Number of nodes for the purpose of resource estimation adjusted
    // for the special cases listed below.
    long adjNumNodes = numNodes_;
    if (numNodes_ <= 0) {
        adjNumNodes = 1;
    } else if (scanRanges_.size() < numNodes_) {
        // TODO: Empirically evaluate whether there is more Hdfs block skew for relatively
        // small files, i.e., whether this estimate is too optimistic.
        adjNumNodes = scanRanges_.size();
    }
    Preconditions.checkNotNull(desc_);
    Preconditions.checkNotNull(desc_.getParentTable() instanceof HdfsTable);
    HdfsTable table = (HdfsTable) desc_.getParentTable();
    int perHostScanRanges;
    if (table.getMajorityFormat() == HdfsFileFormat.PARQUET) {
        // For the purpose of this estimation, the number of per-host scan ranges for
        // Parquet files are equal to the number of non-partition columns scanned.
        perHostScanRanges = 0;
        for (SlotDescriptor slot : desc_.getSlots()) {
            if (slot.getColumn() == null || slot.getColumn().getPosition() >= table.getNumClusteringCols()) {
                ++perHostScanRanges;
            }
        }
    } else {
        perHostScanRanges = (int) Math.ceil(((double) scanRanges_.size() / (double) adjNumNodes) * SCAN_RANGE_SKEW_FACTOR);
    }
    // TODO: The total memory consumption for a particular query depends on the number
    // of *available* cores, i.e., it depends the resource consumption of other
    // concurrent queries. Figure out how to account for that.
    int maxScannerThreads = Math.min(perHostScanRanges, RuntimeEnv.INSTANCE.getNumCores() * THREADS_PER_CORE);
    // Account for the max scanner threads query option.
    if (queryOptions.isSetNum_scanner_threads() && queryOptions.getNum_scanner_threads() > 0) {
        maxScannerThreads = Math.min(maxScannerThreads, queryOptions.getNum_scanner_threads());
    }
    long avgScanRangeBytes = (long) Math.ceil(totalBytes_ / (double) scanRanges_.size());
    // The +1 accounts for an extra I/O buffer to read past the scan range due to a
    // trailing record spanning Hdfs blocks.
    long perThreadIoBuffers = Math.min((long) Math.ceil(avgScanRangeBytes / (double) IO_MGR_BUFFER_SIZE), MAX_IO_BUFFERS_PER_THREAD) + 1;
    perHostMemCost_ = maxScannerThreads * perThreadIoBuffers * IO_MGR_BUFFER_SIZE;
    // Sanity check: the tighter estimation should not exceed the per-host maximum.
    long perHostUpperBound = getPerHostMemUpperBound();
    if (perHostMemCost_ > perHostUpperBound) {
        LOG.warn(String.format("Per-host mem cost %s exceeded per-host upper bound %s.", PrintUtils.printBytes(perHostMemCost_), PrintUtils.printBytes(perHostUpperBound)));
        perHostMemCost_ = perHostUpperBound;
    }
}
#method_after
@Override
public void computeCosts(TQueryOptions queryOptions) {
    Preconditions.checkNotNull(scanRanges_, "Cost estimation requires scan ranges.");
    if (scanRanges_.isEmpty()) {
        perHostMemCost_ = 0;
        return;
    }
    // Number of nodes for the purpose of resource estimation adjusted
    // for the special cases listed below.
    long adjNumNodes = numNodes_;
    if (numNodes_ <= 0) {
        adjNumNodes = 1;
    } else if (scanRanges_.size() < numNodes_) {
        // TODO: Empirically evaluate whether there is more Hdfs block skew for relatively
        // small files, i.e., whether this estimate is too optimistic.
        adjNumNodes = scanRanges_.size();
    }
    Preconditions.checkNotNull(desc_);
    Preconditions.checkNotNull(desc_.getTable() instanceof HdfsTable);
    HdfsTable table = (HdfsTable) desc_.getTable();
    int perHostScanRanges;
    if (table.getMajorityFormat() == HdfsFileFormat.PARQUET) {
        // For the purpose of this estimation, the number of per-host scan ranges for
        // Parquet files are equal to the number of non-partition columns scanned.
        perHostScanRanges = 0;
        for (SlotDescriptor slot : desc_.getSlots()) {
            if (slot.getColumn() == null || slot.getColumn().getPosition() >= table.getNumClusteringCols()) {
                ++perHostScanRanges;
            }
        }
    } else {
        perHostScanRanges = (int) Math.ceil(((double) scanRanges_.size() / (double) adjNumNodes) * SCAN_RANGE_SKEW_FACTOR);
    }
    // TODO: The total memory consumption for a particular query depends on the number
    // of *available* cores, i.e., it depends the resource consumption of other
    // concurrent queries. Figure out how to account for that.
    int maxScannerThreads = Math.min(perHostScanRanges, RuntimeEnv.INSTANCE.getNumCores() * THREADS_PER_CORE);
    // Account for the max scanner threads query option.
    if (queryOptions.isSetNum_scanner_threads() && queryOptions.getNum_scanner_threads() > 0) {
        maxScannerThreads = Math.min(maxScannerThreads, queryOptions.getNum_scanner_threads());
    }
    long avgScanRangeBytes = (long) Math.ceil(totalBytes_ / (double) scanRanges_.size());
    // The +1 accounts for an extra I/O buffer to read past the scan range due to a
    // trailing record spanning Hdfs blocks.
    long perThreadIoBuffers = Math.min((long) Math.ceil(avgScanRangeBytes / (double) IO_MGR_BUFFER_SIZE), MAX_IO_BUFFERS_PER_THREAD) + 1;
    perHostMemCost_ = maxScannerThreads * perThreadIoBuffers * IO_MGR_BUFFER_SIZE;
    // Sanity check: the tighter estimation should not exceed the per-host maximum.
    long perHostUpperBound = getPerHostMemUpperBound();
    if (perHostMemCost_ > perHostUpperBound) {
        LOG.warn(String.format("Per-host mem cost %s exceeded per-host upper bound %s.", PrintUtils.printBytes(perHostMemCost_), PrintUtils.printBytes(perHostUpperBound)));
        perHostMemCost_ = perHostUpperBound;
    }
}
#end_block

#method_before
protected void addTestUda(String name, Type retType, Type... argTypes) {
    FunctionName fnName = new FunctionName("default", name);
    catalog_.addFunction(new AggregateFunction(fnName, new FunctionArgs(Lists.newArrayList(argTypes), false), retType));
}
#method_after
protected void addTestUda(String name, Type retType, Type... argTypes) {
    FunctionName fnName = new FunctionName("default", name);
    catalog_.addFunction(new AggregateFunction(fnName, Lists.newArrayList(argTypes), retType, false));
}
#end_block

#method_before
protected Table addTestTable(String createTableSql) {
    CreateTableStmt createTableStmt = (CreateTableStmt) AnalyzesOk(createTableSql);
    // Currently does not support partitioned tables.
    Preconditions.checkState(createTableStmt.getPartitionColumnDefs().isEmpty());
    Db db = catalog_.getDb(createTableStmt.getDb());
    Preconditions.checkNotNull(db, "Test tables must be created in an existing db.");
    HdfsTable dummyTable = new HdfsTable(null, null, db, createTableStmt.getTbl(), createTableStmt.getOwner());
    List<ColumnDesc> columnDefs = createTableStmt.getColumnDefs();
    for (int i = 0; i < columnDefs.size(); ++i) {
        ColumnDesc colDef = columnDefs.get(i);
        dummyTable.addColumn(new Column(colDef.getColName(), colDef.getType(), i));
    }
    db.addTable(dummyTable);
    testTables_.add(dummyTable);
    return dummyTable;
}
#method_after
protected Table addTestTable(String createTableSql) {
    CreateTableStmt createTableStmt = (CreateTableStmt) AnalyzesOk(createTableSql);
    // Currently does not support partitioned tables.
    Preconditions.checkState(createTableStmt.getPartitionColumnDefs().isEmpty());
    Db db = catalog_.getDb(createTableStmt.getDb());
    Preconditions.checkNotNull(db, "Test tables must be created in an existing db.");
    HdfsTable dummyTable = new HdfsTable(null, null, db, createTableStmt.getTbl(), createTableStmt.getOwner());
    List<ColumnDef> columnDefs = createTableStmt.getColumnDefs();
    for (int i = 0; i < columnDefs.size(); ++i) {
        ColumnDef colDef = columnDefs.get(i);
        dummyTable.addColumn(new Column(colDef.getColName(), colDef.getType(), i));
    }
    db.addTable(dummyTable);
    testTables_.add(dummyTable);
    return dummyTable;
}
#end_block

#method_before
public void AnalysisError(String stmt, Analyzer analyzer, String expectedErrorString) {
    Preconditions.checkNotNull(expectedErrorString, "No expected error message given.");
    LOG.info("processing " + stmt);
    try {
        AnalysisContext analysisCtx = new AnalysisContext(catalog_, TestUtils.createQueryContext(Catalog.DEFAULT_DB, System.getProperty("user.name")), AuthorizationConfig.createAuthDisabledConfig());
        analysisCtx.analyze(stmt, analyzer);
        AnalysisContext.AnalysisResult analysisResult = analysisCtx.getAnalysisResult();
        Preconditions.checkNotNull(analysisResult.getStmt());
    } catch (Exception e) {
        e.printStackTrace();
        String errorString = e.getMessage();
        Assert.assertTrue("got error:\n" + errorString + "\nexpected:\n" + expectedErrorString, errorString.startsWith(expectedErrorString));
        return;
    }
    fail("Stmt didn't result in analysis error: " + stmt);
}
#method_after
public void AnalysisError(String stmt, Analyzer analyzer, String expectedErrorString) {
    Preconditions.checkNotNull(expectedErrorString, "No expected error message given.");
    LOG.info("processing " + stmt);
    try {
        AnalysisContext analysisCtx = new AnalysisContext(catalog_, TestUtils.createQueryContext(Catalog.DEFAULT_DB, System.getProperty("user.name")), AuthorizationConfig.createAuthDisabledConfig());
        analysisCtx.analyze(stmt, analyzer);
        AnalysisContext.AnalysisResult analysisResult = analysisCtx.getAnalysisResult();
        Preconditions.checkNotNull(analysisResult.getStmt());
    } catch (Exception e) {
        String errorString = e.getMessage();
        Assert.assertTrue("got error:\n" + errorString + "\nexpected:\n" + expectedErrorString, errorString.startsWith(expectedErrorString));
        return;
    }
    fail("Stmt didn't result in analysis error: " + stmt);
}
#end_block

#method_before
protected void TblsAnalysisError(String query, TableName tbl, String expectedError) {
    Preconditions.checkState(tbl.isFullyQualified());
    Preconditions.checkState(query.contains("$TBL"));
    String uqQuery = query.replace("$TBL", tbl.getTbl());
    System.out.println(uqQuery);
    AnalysisError(uqQuery, createAnalyzer(tbl.getDb()), expectedError);
    String fqQuery = query.replace("$TBL", tbl.toString());
    System.out.println(fqQuery);
    AnalysisError(fqQuery, expectedError);
}
#method_after
protected void TblsAnalysisError(String query, TableName tbl, String expectedError) {
    Preconditions.checkState(tbl.isFullyQualified());
    Preconditions.checkState(query.contains("$TBL"));
    String uqQuery = query.replace("$TBL", tbl.getTbl());
    AnalysisError(uqQuery, createAnalyzer(tbl.getDb()), expectedError);
    String fqQuery = query.replace("$TBL", tbl.toString());
    AnalysisError(fqQuery, expectedError);
}
#end_block

#method_before
private static void rewriteWhereClauseSubqueries(SelectStmt stmt, Analyzer analyzer) throws AnalysisException {
    int numTableRefs = stmt.tableRefs_.size();
    ArrayList<Expr> exprsWithSubqueries = Lists.newArrayList();
    ExprSubstitutionMap smap = new ExprSubstitutionMap();
    // Replace all BetweenPredicates that contain subqueries with their
    // equivalent compound predicates.
    stmt.whereClause_ = replaceBetweenPredicates(stmt.whereClause_);
    // can currently be rewritten as a join.
    for (Expr conjunct : stmt.whereClause_.getConjuncts()) {
        List<Subquery> subqueries = Lists.newArrayList();
        conjunct.collectAll(Predicates.instanceOf(Subquery.class), subqueries);
        if (subqueries.size() == 0)
            continue;
        if (subqueries.size() > 1) {
            throw new AnalysisException("Multiple subqueries are not supported in " + "expression: " + conjunct.toSql());
        }
        if (!(conjunct instanceof InPredicate) && !(conjunct instanceof ExistsPredicate) && !(conjunct instanceof BinaryPredicate) && !conjunct.contains(Expr.IS_SCALAR_SUBQUERY)) {
            throw new AnalysisException("Non-scalar subquery is not supported in " + "expression: " + conjunct.toSql());
        }
        // Replace all the supported exprs with subqueries with true BoolLiterals
        // using an smap.
        BoolLiteral boolLiteral = new BoolLiteral(true);
        boolLiteral.analyze(analyzer);
        smap.put(conjunct, boolLiteral);
        exprsWithSubqueries.add(conjunct);
    }
    stmt.whereClause_ = stmt.whereClause_.substitute(smap, analyzer, false);
    boolean hasNewVisibleTuple = false;
    // with 'stmt'.
    for (Expr expr : exprsWithSubqueries) {
        if (mergeExpr(stmt, rewriteExpr(expr, analyzer), analyzer)) {
            hasNewVisibleTuple = true;
        }
    }
    if (canEliminate(stmt.whereClause_))
        stmt.whereClause_ = null;
    if (hasNewVisibleTuple)
        replaceUnqualifiedStarItems(stmt, numTableRefs);
}
#method_after
private static void rewriteWhereClauseSubqueries(SelectStmt stmt, Analyzer analyzer) throws AnalysisException {
    int numTableRefs = stmt.tableRefs_.size();
    ArrayList<Expr> exprsWithSubqueries = Lists.newArrayList();
    ExprSubstitutionMap smap = new ExprSubstitutionMap();
    // Replace all BetweenPredicates that contain subqueries with their
    // equivalent compound predicates.
    stmt.whereClause_ = replaceBetweenPredicates(stmt.whereClause_);
    // can currently be rewritten as a join.
    for (Expr conjunct : stmt.whereClause_.getConjuncts()) {
        List<Subquery> subqueries = Lists.newArrayList();
        conjunct.collectAll(Predicates.instanceOf(Subquery.class), subqueries);
        if (subqueries.size() == 0)
            continue;
        if (subqueries.size() > 1) {
            throw new AnalysisException("Multiple subqueries are not supported in " + "expression: " + conjunct.toSql());
        }
        if (!(conjunct instanceof InPredicate) && !(conjunct instanceof ExistsPredicate) && !(conjunct instanceof BinaryPredicate) && !conjunct.contains(Expr.IS_SCALAR_SUBQUERY)) {
            throw new AnalysisException("Non-scalar subquery is not supported in " + "expression: " + conjunct.toSql());
        }
        if (conjunct instanceof ExistsPredicate) {
            // Check if we can determine the result of an ExistsPredicate during analysis.
            // If so, replace the predicate with a BoolLiteral predicate and remove it from
            // the list of predicates to be rewritten.
            BoolLiteral boolLiteral = replaceExistsPredicate((ExistsPredicate) conjunct);
            if (boolLiteral != null) {
                boolLiteral.analyze(analyzer);
                smap.put(conjunct, boolLiteral);
                continue;
            }
        }
        // Replace all the supported exprs with subqueries with true BoolLiterals
        // using an smap.
        BoolLiteral boolLiteral = new BoolLiteral(true);
        boolLiteral.analyze(analyzer);
        smap.put(conjunct, boolLiteral);
        exprsWithSubqueries.add(conjunct);
    }
    stmt.whereClause_ = stmt.whereClause_.substitute(smap, analyzer, false);
    boolean hasNewVisibleTuple = false;
    // with 'stmt'.
    for (Expr expr : exprsWithSubqueries) {
        if (mergeExpr(stmt, rewriteExpr(expr, analyzer), analyzer)) {
            hasNewVisibleTuple = true;
        }
    }
    if (canEliminate(stmt.whereClause_))
        stmt.whereClause_ = null;
    if (hasNewVisibleTuple)
        replaceUnqualifiedStarItems(stmt, numTableRefs);
}
#end_block

#method_before
private static boolean mergeExpr(SelectStmt stmt, Expr expr, Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(expr);
    Preconditions.checkNotNull(analyzer);
    boolean updateSelectList = false;
    SelectStmt subqueryStmt = (SelectStmt) expr.getSubquery().getStatement();
    // Create a new inline view from the subquery stmt. The inline view will be added
    // to the stmt's table refs later. Explicitly set the inline view's column labels
    // to eliminate any chance that column aliases from the parent query could reference
    // select items from the inline view after the rewrite.
    List<String> colLabels = Lists.newArrayList();
    for (int i = 0; i < subqueryStmt.getColLabels().size(); ++i) {
        colLabels.add(subqueryStmt.getColumnAliasGenerator().getNextAlias());
    }
    InlineViewRef inlineView = new InlineViewRef(stmt.getTableAliasGenerator().getNextAlias(), subqueryStmt, colLabels);
    // Extract all correlated predicates from the subquery.
    List<Expr> onClauseConjuncts = extractCorrelatedPredicates(subqueryStmt);
    if (!onClauseConjuncts.isEmpty()) {
        canRewriteCorrelatedSubquery(expr);
        // For correlated subqueries that are eligible for rewrite by transforming
        // into a join, a LIMIT clause has no effect on the results, so we can
        // safely remove it.
        subqueryStmt.limitElement_ = null;
    }
    // Update the subquery's select list and/or its GROUP BY clause by adding
    // exprs from the extracted correlated predicates.
    boolean updateGroupBy = expr.getSubquery().isScalarSubquery() || (expr instanceof ExistsPredicate && subqueryStmt.hasAggInfo());
    List<Expr> lhsExprs = Lists.newArrayList();
    List<Expr> rhsExprs = Lists.newArrayList();
    for (Expr conjunct : onClauseConjuncts) {
        updateInlineView(inlineView, conjunct, stmt.getTableRefIds(), lhsExprs, rhsExprs, updateGroupBy);
    }
    if (expr instanceof ExistsPredicate && onClauseConjuncts.isEmpty()) {
        // For uncorrelated subqueries, we limit the number of rows returned by the
        // subquery.
        subqueryStmt.setLimit(1);
    }
    // Analyzing the inline view trigger reanalysis of the subquery's select statement.
    // However the statement is already analyzed and since statement analysis is not
    // idempotent, the analysis needs to be reset (by a call to clone()).
    inlineView = (InlineViewRef) inlineView.clone();
    inlineView.analyze(analyzer);
    inlineView.setLeftTblRef(stmt.tableRefs_.get(stmt.tableRefs_.size() - 1));
    stmt.tableRefs_.add(inlineView);
    JoinOperator joinOp = JoinOperator.LEFT_SEMI_JOIN;
    // Create a join conjunct from the expr that contains a subquery.
    Expr joinConjunct = createJoinConjunct(expr, inlineView, analyzer, !onClauseConjuncts.isEmpty());
    if (joinConjunct != null) {
        SelectListItem firstItem = ((SelectStmt) inlineView.getViewStmt()).getSelectList().getItems().get(0);
        if (!onClauseConjuncts.isEmpty() && firstItem.getExpr().contains(Expr.NON_NULL_EMPTY_AGG)) {
            // Correlated subqueries with an aggregate function that returns non-null on
            // an empty input are rewritten using a LEFT OUTER JOIN because we
            // need to ensure that there is one agg value for every tuple of 'stmt'
            // (parent select block), even for those tuples of 'stmt' that get rejected
            // by the subquery due to some predicate. The new join conjunct is added to
            // stmt's WHERE clause because it needs to be applied to the result of the
            // LEFT OUTER JOIN (both matched and unmatched tuples).
            // 
            // TODO Handle other aggregate functions and UDAs that return a non-NULL value
            // on an empty set.
            // TODO Handle count aggregate functions in an expression in subqueries
            // select list.
            stmt.whereClause_ = CompoundPredicate.createConjunction(joinConjunct, stmt.whereClause_);
            joinConjunct = null;
            joinOp = JoinOperator.LEFT_OUTER_JOIN;
            updateSelectList = true;
        }
        if (joinConjunct != null)
            onClauseConjuncts.add(joinConjunct);
    }
    // Create the ON clause from the extracted correlated predicates.
    Expr onClausePredicate = CompoundPredicate.createConjunctivePredicate(onClauseConjuncts);
    if (onClausePredicate == null) {
        Preconditions.checkState(expr instanceof ExistsPredicate);
        // TODO: Remove this when we support independent subquery evaluation.
        if (((ExistsPredicate) expr).isNotExists()) {
            throw new AnalysisException("Unsupported uncorrelated NOT EXISTS subquery: " + subqueryStmt.toSql());
        }
        // We don't have an ON clause predicate to create an equi-join. Rewrite the
        // subquery using a CROSS JOIN.
        // TODO This is very expensive. Remove it when we implement independent
        // subquery evaluation.
        inlineView.setJoinOp(JoinOperator.CROSS_JOIN);
        LOG.warn("uncorrelated subquery rewritten using a cross join");
        // Indicate that new visible tuples may be added in stmt's select list.
        return true;
    }
    // Create an smap from the original select-list exprs of the select list to
    // the corresponding inline-view columns.
    ExprSubstitutionMap smap = new ExprSubstitutionMap();
    Preconditions.checkState(lhsExprs.size() == rhsExprs.size());
    for (int i = 0; i < lhsExprs.size(); ++i) {
        Expr lhsExpr = lhsExprs.get(i);
        Expr rhsExpr = rhsExprs.get(i);
        rhsExpr.analyze(analyzer);
        smap.put(lhsExpr, rhsExpr);
    }
    onClausePredicate = onClausePredicate.substitute(smap, analyzer, false);
    // graph of query blocks are not supported).
    if (!onClausePredicate.isBoundByTupleIds(stmt.getTableRefIds())) {
        throw new AnalysisException("Unsupported correlated subquery: " + subqueryStmt.toSql());
    }
    // Check if we have a valid ON clause for an equi-join.
    boolean hasEqJoinPred = false;
    for (Expr conjunct : onClausePredicate.getConjuncts()) {
        if (!(conjunct instanceof BinaryPredicate) || ((BinaryPredicate) conjunct).getOp() != BinaryPredicate.Operator.EQ) {
            continue;
        }
        List<TupleId> lhsTupleIds = Lists.newArrayList();
        conjunct.getChild(0).getIds(lhsTupleIds, null);
        if (lhsTupleIds.isEmpty())
            continue;
        List<TupleId> rhsTupleIds = Lists.newArrayList();
        conjunct.getChild(1).getIds(rhsTupleIds, null);
        if (rhsTupleIds.isEmpty())
            continue;
        // of the binary predicate.
        if ((lhsTupleIds.contains(inlineView.getDesc().getId()) && lhsTupleIds.size() > 1) || (rhsTupleIds.contains(inlineView.getDesc().getId()) && rhsTupleIds.size() > 1)) {
            continue;
        }
        hasEqJoinPred = true;
        break;
    }
    if (!hasEqJoinPred) {
        // TODO: Remove this when independent subquery evaluation is implemented.
        // TODO: Requires support for non-equi joins.
        boolean hasGroupBy = ((SelectStmt) inlineView.getViewStmt()).hasGroupByClause();
        if (!expr.getSubquery().isScalarSubquery() || (!(hasGroupBy && stmt.selectList_.isDistinct()) && hasGroupBy)) {
            throw new AnalysisException("Unsupported predicate with subquery: " + expr.toSql());
        }
        // We can rewrite the aggregate subquery using a cross join. All conjuncts
        // that were extracted from the subquery are added to stmt's WHERE clause.
        stmt.whereClause_ = CompoundPredicate.createConjunction(onClausePredicate, stmt.whereClause_);
        inlineView.setJoinOp(JoinOperator.CROSS_JOIN);
        // select list (if the latter contains an unqualified star item '*')
        return true;
    }
    // We have a valid equi-join conjunct.
    if (expr instanceof InPredicate && ((InPredicate) expr).isNotIn() || expr instanceof ExistsPredicate && ((ExistsPredicate) expr).isNotExists()) {
        // conjunct with a conjunct that uses the null-matching eq operator.
        if (expr instanceof InPredicate) {
            joinOp = JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN;
            List<TupleId> tIds = Lists.newArrayList();
            joinConjunct.getIds(tIds, null);
            if (tIds.size() <= 1 || !tIds.contains(inlineView.getDesc().getId())) {
                throw new AnalysisException("Unsupported NOT IN predicate with subquery: " + expr.toSql());
            }
            // null-matching EQ operator.
            for (Expr conjunct : onClausePredicate.getConjuncts()) {
                if (conjunct.equals(joinConjunct)) {
                    Preconditions.checkState(conjunct instanceof BinaryPredicate);
                    Preconditions.checkState(((BinaryPredicate) conjunct).getOp() == BinaryPredicate.Operator.EQ);
                    ((BinaryPredicate) conjunct).setOp(BinaryPredicate.Operator.NULL_MATCHING_EQ);
                    break;
                }
            }
        } else {
            joinOp = JoinOperator.LEFT_ANTI_JOIN;
        }
    }
    inlineView.setJoinOp(joinOp);
    inlineView.setOnClause(onClausePredicate);
    return updateSelectList;
}
#method_after
private static boolean mergeExpr(SelectStmt stmt, Expr expr, Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(expr);
    Preconditions.checkNotNull(analyzer);
    boolean updateSelectList = false;
    SelectStmt subqueryStmt = (SelectStmt) expr.getSubquery().getStatement();
    // Create a new inline view from the subquery stmt. The inline view will be added
    // to the stmt's table refs later. Explicitly set the inline view's column labels
    // to eliminate any chance that column aliases from the parent query could reference
    // select items from the inline view after the rewrite.
    List<String> colLabels = Lists.newArrayList();
    for (int i = 0; i < subqueryStmt.getColLabels().size(); ++i) {
        colLabels.add(subqueryStmt.getColumnAliasGenerator().getNextAlias());
    }
    InlineViewRef inlineView = new InlineViewRef(stmt.getTableAliasGenerator().getNextAlias(), subqueryStmt, colLabels);
    // Extract all correlated predicates from the subquery.
    List<Expr> onClauseConjuncts = extractCorrelatedPredicates(subqueryStmt);
    if (!onClauseConjuncts.isEmpty()) {
        canRewriteCorrelatedSubquery(expr);
        // For correlated subqueries that are eligible for rewrite by transforming
        // into a join, a LIMIT clause has no effect on the results, so we can
        // safely remove it.
        subqueryStmt.limitElement_ = null;
    }
    if (expr instanceof ExistsPredicate) {
        // subquery.
        if (onClauseConjuncts.isEmpty())
            subqueryStmt.setLimit(1);
    }
    // Update the subquery's select list and/or its GROUP BY clause by adding
    // exprs from the extracted correlated predicates.
    boolean updateGroupBy = expr.getSubquery().isScalarSubquery() || (expr instanceof ExistsPredicate && subqueryStmt.hasAggInfo() && !subqueryStmt.getSelectList().isDistinct());
    List<Expr> lhsExprs = Lists.newArrayList();
    List<Expr> rhsExprs = Lists.newArrayList();
    for (Expr conjunct : onClauseConjuncts) {
        updateInlineView(inlineView, conjunct, stmt.getTableRefIds(), lhsExprs, rhsExprs, updateGroupBy);
    }
    // Analyzing the inline view trigger reanalysis of the subquery's select statement.
    // However the statement is already analyzed and since statement analysis is not
    // idempotent, the analysis needs to be reset (by a call to clone()).
    inlineView = (InlineViewRef) inlineView.clone();
    inlineView.analyze(analyzer);
    inlineView.setLeftTblRef(stmt.tableRefs_.get(stmt.tableRefs_.size() - 1));
    stmt.tableRefs_.add(inlineView);
    JoinOperator joinOp = JoinOperator.LEFT_SEMI_JOIN;
    // Create a join conjunct from the expr that contains a subquery.
    Expr joinConjunct = createJoinConjunct(expr, inlineView, analyzer, !onClauseConjuncts.isEmpty());
    if (joinConjunct != null) {
        SelectListItem firstItem = ((SelectStmt) inlineView.getViewStmt()).getSelectList().getItems().get(0);
        if (!onClauseConjuncts.isEmpty() && firstItem.getExpr().contains(Expr.NON_NULL_EMPTY_AGG)) {
            // Correlated subqueries with an aggregate function that returns non-null on
            // an empty input are rewritten using a LEFT OUTER JOIN because we
            // need to ensure that there is one agg value for every tuple of 'stmt'
            // (parent select block), even for those tuples of 'stmt' that get rejected
            // by the subquery due to some predicate. The new join conjunct is added to
            // stmt's WHERE clause because it needs to be applied to the result of the
            // LEFT OUTER JOIN (both matched and unmatched tuples).
            // 
            // TODO Handle other aggregate functions and UDAs that return a non-NULL value
            // on an empty set.
            // TODO Handle count aggregate functions in an expression in subqueries
            // select list.
            stmt.whereClause_ = CompoundPredicate.createConjunction(joinConjunct, stmt.whereClause_);
            joinConjunct = null;
            joinOp = JoinOperator.LEFT_OUTER_JOIN;
            updateSelectList = true;
        }
        if (joinConjunct != null)
            onClauseConjuncts.add(joinConjunct);
    }
    // Create the ON clause from the extracted correlated predicates.
    Expr onClausePredicate = CompoundPredicate.createConjunctivePredicate(onClauseConjuncts);
    if (onClausePredicate == null) {
        Preconditions.checkState(expr instanceof ExistsPredicate);
        // TODO: Remove this when we support independent subquery evaluation.
        if (((ExistsPredicate) expr).isNotExists()) {
            throw new AnalysisException("Unsupported uncorrelated NOT EXISTS subquery: " + subqueryStmt.toSql());
        }
        // We don't have an ON clause predicate to create an equi-join. Rewrite the
        // subquery using a CROSS JOIN.
        // TODO This is very expensive. Remove it when we implement independent
        // subquery evaluation.
        inlineView.setJoinOp(JoinOperator.CROSS_JOIN);
        LOG.warn("uncorrelated subquery rewritten using a cross join");
        // Indicate that new visible tuples may be added in stmt's select list.
        return true;
    }
    // Create an smap from the original select-list exprs of the select list to
    // the corresponding inline-view columns.
    ExprSubstitutionMap smap = new ExprSubstitutionMap();
    Preconditions.checkState(lhsExprs.size() == rhsExprs.size());
    for (int i = 0; i < lhsExprs.size(); ++i) {
        Expr lhsExpr = lhsExprs.get(i);
        Expr rhsExpr = rhsExprs.get(i);
        rhsExpr.analyze(analyzer);
        smap.put(lhsExpr, rhsExpr);
    }
    onClausePredicate = onClausePredicate.substitute(smap, analyzer, false);
    // graph of query blocks are not supported).
    if (!onClausePredicate.isBoundByTupleIds(stmt.getTableRefIds())) {
        throw new AnalysisException("Unsupported correlated subquery: " + subqueryStmt.toSql());
    }
    // Check if we have a valid ON clause for an equi-join.
    boolean hasEqJoinPred = false;
    for (Expr conjunct : onClausePredicate.getConjuncts()) {
        if (!(conjunct instanceof BinaryPredicate) || ((BinaryPredicate) conjunct).getOp() != BinaryPredicate.Operator.EQ) {
            continue;
        }
        List<TupleId> lhsTupleIds = Lists.newArrayList();
        conjunct.getChild(0).getIds(lhsTupleIds, null);
        if (lhsTupleIds.isEmpty())
            continue;
        List<TupleId> rhsTupleIds = Lists.newArrayList();
        conjunct.getChild(1).getIds(rhsTupleIds, null);
        if (rhsTupleIds.isEmpty())
            continue;
        // of the binary predicate.
        if ((lhsTupleIds.contains(inlineView.getDesc().getId()) && lhsTupleIds.size() > 1) || (rhsTupleIds.contains(inlineView.getDesc().getId()) && rhsTupleIds.size() > 1)) {
            continue;
        }
        hasEqJoinPred = true;
        break;
    }
    if (!hasEqJoinPred) {
        // TODO: Remove this when independent subquery evaluation is implemented.
        // TODO: Requires support for non-equi joins.
        boolean hasGroupBy = ((SelectStmt) inlineView.getViewStmt()).hasGroupByClause();
        if (!expr.getSubquery().isScalarSubquery() || (!(hasGroupBy && stmt.selectList_.isDistinct()) && hasGroupBy)) {
            throw new AnalysisException("Unsupported predicate with subquery: " + expr.toSql());
        }
        // We can rewrite the aggregate subquery using a cross join. All conjuncts
        // that were extracted from the subquery are added to stmt's WHERE clause.
        stmt.whereClause_ = CompoundPredicate.createConjunction(onClausePredicate, stmt.whereClause_);
        inlineView.setJoinOp(JoinOperator.CROSS_JOIN);
        // select list (if the latter contains an unqualified star item '*')
        return true;
    }
    // We have a valid equi-join conjunct.
    if (expr instanceof InPredicate && ((InPredicate) expr).isNotIn() || expr instanceof ExistsPredicate && ((ExistsPredicate) expr).isNotExists()) {
        // conjunct with a conjunct that uses the null-matching eq operator.
        if (expr instanceof InPredicate) {
            joinOp = JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN;
            List<TupleId> tIds = Lists.newArrayList();
            joinConjunct.getIds(tIds, null);
            if (tIds.size() <= 1 || !tIds.contains(inlineView.getDesc().getId())) {
                throw new AnalysisException("Unsupported NOT IN predicate with subquery: " + expr.toSql());
            }
            // null-matching EQ operator.
            for (Expr conjunct : onClausePredicate.getConjuncts()) {
                if (conjunct.equals(joinConjunct)) {
                    Preconditions.checkState(conjunct instanceof BinaryPredicate);
                    Preconditions.checkState(((BinaryPredicate) conjunct).getOp() == BinaryPredicate.Operator.EQ);
                    ((BinaryPredicate) conjunct).setOp(BinaryPredicate.Operator.NULL_MATCHING_EQ);
                    break;
                }
            }
        } else {
            joinOp = JoinOperator.LEFT_ANTI_JOIN;
        }
    }
    inlineView.setJoinOp(joinOp);
    inlineView.setOnClause(onClausePredicate);
    return updateSelectList;
}
#end_block

#method_before
private static void replaceUnqualifiedStarItems(SelectStmt stmt, int tableIdx) {
    Preconditions.checkState(tableIdx < stmt.tableRefs_.size());
    ArrayList<SelectListItem> newItems = Lists.newArrayList();
    for (int i = 0; i < stmt.selectList_.getItems().size(); ++i) {
        SelectListItem item = stmt.selectList_.getItems().get(i);
        if (!item.isStar() || item.getPath() != null) {
            newItems.add(item);
            continue;
        }
        // tbl1,...,tbln are the visible tableRefs in stmt.
        for (int j = 0; j < tableIdx; ++j) {
            TableRef tableRef = stmt.tableRefs_.get(j);
            if (tableRef.getJoinOp() == JoinOperator.LEFT_SEMI_JOIN || tableRef.getJoinOp() == JoinOperator.LEFT_ANTI_JOIN) {
                continue;
            }
            newItems.add(SelectListItem.createStarItem(Lists.newArrayList(tableRef.getUniqueAlias())));
        }
    }
    Preconditions.checkState(!newItems.isEmpty());
    boolean isDistinct = stmt.selectList_.isDistinct();
    stmt.selectList_ = new SelectList(newItems, isDistinct, stmt.selectList_.getPlanHints());
}
#method_after
private static void replaceUnqualifiedStarItems(SelectStmt stmt, int tableIdx) {
    Preconditions.checkState(tableIdx < stmt.tableRefs_.size());
    ArrayList<SelectListItem> newItems = Lists.newArrayList();
    for (int i = 0; i < stmt.selectList_.getItems().size(); ++i) {
        SelectListItem item = stmt.selectList_.getItems().get(i);
        if (!item.isStar() || item.getRawPath() != null) {
            newItems.add(item);
            continue;
        }
        // tbl1,...,tbln are the visible tableRefs in stmt.
        for (int j = 0; j < tableIdx; ++j) {
            TableRef tableRef = stmt.tableRefs_.get(j);
            if (tableRef.getJoinOp() == JoinOperator.LEFT_SEMI_JOIN || tableRef.getJoinOp() == JoinOperator.LEFT_ANTI_JOIN) {
                continue;
            }
            newItems.add(SelectListItem.createStarItem(Lists.newArrayList(tableRef.getUniqueAlias())));
        }
    }
    Preconditions.checkState(!newItems.isEmpty());
    boolean isDistinct = stmt.selectList_.isDistinct();
    stmt.selectList_ = new SelectList(newItems, isDistinct, stmt.selectList_.getPlanHints());
}
#end_block

#method_before
private static void updateInlineView(InlineViewRef inlineView, Expr expr, List<TupleId> parentQueryTids, List<Expr> lhsExprs, List<Expr> rhsExprs, boolean updateGroupBy) throws AnalysisException {
    SelectStmt stmt = (SelectStmt) inlineView.getViewStmt();
    List<TupleId> subqueryTblIds = stmt.getTableRefIds();
    ArrayList<Expr> groupByExprs = null;
    if (updateGroupBy)
        groupByExprs = Lists.newArrayList();
    List<SelectListItem> items = stmt.selectList_.getItems();
    // Collect all the SlotRefs from 'expr' and identify those that are bound by
    // subquery tuple ids.
    ArrayList<Expr> slotRefs = Lists.newArrayList();
    expr.collectAll(Predicates.instanceOf(SlotRef.class), slotRefs);
    List<Expr> exprsBoundBySubqueryTids = Lists.newArrayList();
    for (Expr slotRef : slotRefs) {
        if (slotRef.isBoundByTupleIds(subqueryTblIds)) {
            exprsBoundBySubqueryTids.add(slotRef);
        }
    }
    // no need to update the subquery's select or group by list.
    if (exprsBoundBySubqueryTids.isEmpty())
        return;
    if (updateGroupBy) {
        Preconditions.checkState(expr instanceof BinaryPredicate);
        Expr exprBoundBySubqueryTids = null;
        if (exprsBoundBySubqueryTids.size() > 1) {
            // ids, they must all be on the same side of that predicate.
            if (expr.getChild(0).isBoundByTupleIds(subqueryTblIds) && expr.getChild(1).isBoundByTupleIds(parentQueryTids)) {
                exprBoundBySubqueryTids = expr.getChild(0);
            } else if (expr.getChild(0).isBoundByTupleIds(parentQueryTids) && expr.getChild(1).isBoundByTupleIds(subqueryTblIds)) {
                exprBoundBySubqueryTids = expr.getChild(1);
            } else {
                throw new AnalysisException("All subquery columns " + "that participate in a predicate must be on the same side of " + "that predicate: " + expr.toSql());
            }
        } else {
            Preconditions.checkState(exprsBoundBySubqueryTids.size() == 1);
            exprBoundBySubqueryTids = exprsBoundBySubqueryTids.get(0);
        }
        exprsBoundBySubqueryTids.clear();
        exprsBoundBySubqueryTids.add(exprBoundBySubqueryTids);
    }
    // added to an ExprSubstitutionMap.
    for (Expr boundExpr : exprsBoundBySubqueryTids) {
        String colAlias = stmt.getColumnAliasGenerator().getNextAlias();
        items.add(new SelectListItem(boundExpr, null));
        inlineView.getExplicitColLabels().add(colAlias);
        lhsExprs.add(boundExpr);
        rhsExprs.add(new SlotRef(Lists.newArrayList(inlineView.getUniqueAlias(), colAlias)));
        if (groupByExprs != null)
            groupByExprs.add(boundExpr);
    }
    // Update the subquery's select list.
    boolean isDistinct = stmt.selectList_.isDistinct();
    Preconditions.checkState(!isDistinct);
    stmt.selectList_ = new SelectList(items, isDistinct, stmt.selectList_.getPlanHints());
    // Update subquery's GROUP BY clause
    if (groupByExprs != null && !groupByExprs.isEmpty()) {
        if (stmt.hasGroupByClause()) {
            stmt.groupingExprs_.addAll(groupByExprs);
        } else {
            stmt.groupingExprs_ = groupByExprs;
        }
    }
}
#method_after
private static void updateInlineView(InlineViewRef inlineView, Expr expr, List<TupleId> parentQueryTids, List<Expr> lhsExprs, List<Expr> rhsExprs, boolean updateGroupBy) throws AnalysisException {
    SelectStmt stmt = (SelectStmt) inlineView.getViewStmt();
    List<TupleId> subqueryTblIds = stmt.getTableRefIds();
    ArrayList<Expr> groupByExprs = null;
    if (updateGroupBy)
        groupByExprs = Lists.newArrayList();
    List<SelectListItem> items = stmt.selectList_.getItems();
    // Collect all the SlotRefs from 'expr' and identify those that are bound by
    // subquery tuple ids.
    ArrayList<Expr> slotRefs = Lists.newArrayList();
    expr.collectAll(Predicates.instanceOf(SlotRef.class), slotRefs);
    List<Expr> exprsBoundBySubqueryTids = Lists.newArrayList();
    for (Expr slotRef : slotRefs) {
        if (slotRef.isBoundByTupleIds(subqueryTblIds)) {
            exprsBoundBySubqueryTids.add(slotRef);
        }
    }
    // no need to update the subquery's select or group by list.
    if (exprsBoundBySubqueryTids.isEmpty())
        return;
    if (updateGroupBy) {
        Preconditions.checkState(expr instanceof BinaryPredicate);
        Expr exprBoundBySubqueryTids = null;
        if (exprsBoundBySubqueryTids.size() > 1) {
            // ids, they must all be on the same side of that predicate.
            if (expr.getChild(0).isBoundByTupleIds(subqueryTblIds) && expr.getChild(1).isBoundByTupleIds(parentQueryTids)) {
                exprBoundBySubqueryTids = expr.getChild(0);
            } else if (expr.getChild(0).isBoundByTupleIds(parentQueryTids) && expr.getChild(1).isBoundByTupleIds(subqueryTblIds)) {
                exprBoundBySubqueryTids = expr.getChild(1);
            } else {
                throw new AnalysisException("All subquery columns " + "that participate in a predicate must be on the same side of " + "that predicate: " + expr.toSql());
            }
        } else {
            Preconditions.checkState(exprsBoundBySubqueryTids.size() == 1);
            exprBoundBySubqueryTids = exprsBoundBySubqueryTids.get(0);
        }
        exprsBoundBySubqueryTids.clear();
        exprsBoundBySubqueryTids.add(exprBoundBySubqueryTids);
    }
    // added to an ExprSubstitutionMap.
    for (Expr boundExpr : exprsBoundBySubqueryTids) {
        String colAlias = stmt.getColumnAliasGenerator().getNextAlias();
        items.add(new SelectListItem(boundExpr, null));
        inlineView.getExplicitColLabels().add(colAlias);
        lhsExprs.add(boundExpr);
        rhsExprs.add(new SlotRef(Lists.newArrayList(inlineView.getUniqueAlias(), colAlias)));
        if (groupByExprs != null)
            groupByExprs.add(boundExpr);
    }
    // Update the subquery's select list.
    boolean isDistinct = stmt.selectList_.isDistinct();
    stmt.selectList_ = new SelectList(items, isDistinct, stmt.selectList_.getPlanHints());
    // Update subquery's GROUP BY clause
    if (groupByExprs != null && !groupByExprs.isEmpty()) {
        if (stmt.hasGroupByClause()) {
            stmt.groupingExprs_.addAll(groupByExprs);
        } else {
            stmt.groupingExprs_ = groupByExprs;
        }
    }
}
#end_block

#method_before
// select list item corresponding to "[[[db.]tbl.]col.]*"
static public SelectListItem createStarItem(List<String> path) {
    return new SelectListItem(path);
}
#method_after
static public SelectListItem createStarItem(List<String> rawPath) {
    return new SelectListItem(rawPath);
}
#end_block

#method_before
@Override
public String toString() {
    if (!isStar_) {
        Preconditions.checkNotNull(expr_);
        return expr_.toSql() + ((alias_ != null) ? " " + alias_ : "");
    } else if (path_ != null) {
        Preconditions.checkState(isStar_);
        return Joiner.on(".").join(path_) + ".*";
    } else {
        return "*";
    }
}
#method_after
@Override
public String toString() {
    if (!isStar_) {
        Preconditions.checkNotNull(expr_);
        return expr_.toSql() + ((alias_ != null) ? " " + alias_ : "");
    } else if (rawPath_ != null) {
        Preconditions.checkState(isStar_);
        return Joiner.on(".").join(rawPath_) + ".*";
    } else {
        return "*";
    }
}
#end_block

#method_before
public String toSql() {
    if (!isStar_) {
        Preconditions.checkNotNull(expr_);
        // Enclose aliases in quotes if Hive cannot parse them without quotes.
        // This is needed for view compatibility between Impala and Hive.
        String aliasSql = null;
        if (alias_ != null)
            aliasSql = ToSqlUtils.getIdentSql(alias_);
        return expr_.toSql() + ((aliasSql != null) ? " " + aliasSql : "");
    } else if (path_ != null) {
        Preconditions.checkState(isStar_);
        StringBuilder result = new StringBuilder();
        for (String p : path_) {
            if (result.length() > 0)
                result.append(".");
            result.append(ToSqlUtils.getIdentSql(p.toLowerCase()));
        }
        result.append(".*");
        return result.toString();
    } else {
        return "*";
    }
}
#method_after
public String toSql() {
    if (!isStar_) {
        Preconditions.checkNotNull(expr_);
        // Enclose aliases in quotes if Hive cannot parse them without quotes.
        // This is needed for view compatibility between Impala and Hive.
        String aliasSql = null;
        if (alias_ != null)
            aliasSql = ToSqlUtils.getIdentSql(alias_);
        return expr_.toSql() + ((aliasSql != null) ? " " + aliasSql : "");
    } else if (rawPath_ != null) {
        Preconditions.checkState(isStar_);
        StringBuilder result = new StringBuilder();
        for (String p : rawPath_) {
            if (result.length() > 0)
                result.append(".");
            result.append(ToSqlUtils.getIdentSql(p.toLowerCase()));
        }
        result.append(".*");
        return result.toString();
    } else {
        return "*";
    }
}
#end_block

#method_before
@Override
public SelectListItem clone() {
    if (isStar_)
        return createStarItem(path_);
    return new SelectListItem(expr_.clone().reset(), alias_);
}
#method_after
@Override
public SelectListItem clone() {
    if (isStar_)
        return createStarItem(rawPath_);
    return new SelectListItem(expr_.clone().reset(), alias_);
}
#end_block

#method_before
private PlanNode createEmptyNode(QueryStmt stmt, Analyzer analyzer) throws InternalException {
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    stmt.getMaterializedTupleIds(tupleIds);
    EmptySetNode node = new EmptySetNode(ctx_.getNextNodeId(), tupleIds);
    node.init(analyzer);
    return node;
}
#method_after
private PlanNode createEmptyNode(QueryStmt stmt, Analyzer analyzer) throws InternalException {
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    stmt.getMaterializedTupleIds(tupleIds);
    EmptySetNode node = new EmptySetNode(ctx_.getNextNodeId(), tupleIds);
    node.init(analyzer);
    // Not needed for a UnionStmt because it materializes its input operands.
    if (stmt instanceof SelectStmt) {
        node.setOutputSmap(((SelectStmt) stmt).getBaseTblSmap());
    }
    return node;
}
#end_block

#method_before
private PlanNode createSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    // no from clause -> materialize the select's exprs with a UnionNode
    if (selectStmt.getTableRefs().isEmpty()) {
        return createConstantSelectPlan(selectStmt, analyzer);
    }
    // collect output tuples of subtrees
    ArrayList<TupleId> rowTuples = Lists.newArrayList();
    for (TableRef tblRef : selectStmt.getTableRefs()) {
        rowTuples.addAll(tblRef.getMaterializedTupleIds());
    }
    // Slot materialization:
    // We need to mark all slots as materialized that are needed during the execution
    // of selectStmt, and we need to do that prior to creating plans for the TableRefs
    // (because createTableRefNode() might end up calling computeMemLayout() on one or
    // more TupleDescriptors, at which point all referenced slots need to be marked).
    // 
    // For non-join predicates, slots are marked as follows:
    // - for base table scan predicates, this is done directly by ScanNode.init(), which
    // can do a better job because it doesn't need to materialize slots that are only
    // referenced for partition pruning, for instance
    // - for inline views, non-join predicates are pushed down, at which point the
    // process repeats itself.
    selectStmt.materializeRequiredSlots(analyzer);
    // if the selectStmt's select-project-join portion returns an empty result set
    if (analyzer.hasEmptySpjResultSet()) {
        PlanNode emptySetNode = new EmptySetNode(ctx_.getNextNodeId(), rowTuples);
        emptySetNode.init(analyzer);
        return createAggregationPlan(selectStmt, analyzer, emptySetNode);
    }
    // create plans for our table refs; use a list here instead of a map to
    // maintain a deterministic order of traversing the TableRefs during join
    // plan generation (helps with tests)
    List<Pair<TableRef, PlanNode>> refPlans = Lists.newArrayList();
    for (TableRef ref : selectStmt.getTableRefs()) {
        PlanNode plan = createTableRefNode(analyzer, ref);
        Preconditions.checkState(plan != null);
        refPlans.add(new Pair(ref, plan));
    }
    // save state of conjunct assignment; needed for join plan generation
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        entry.second.setAssignedConjuncts(analyzer.getAssignedConjuncts());
    }
    PlanNode root = null;
    if (!selectStmt.getSelectList().isStraightJoin()) {
        Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
        root = createCheapestJoinPlan(analyzer, refPlans);
        if (root == null)
            analyzer.setAssignedConjuncts(assignedConjuncts);
    }
    if (selectStmt.getSelectList().isStraightJoin() || root == null) {
        // we didn't have enough stats to do a cost-based join plan, or the STRAIGHT_JOIN
        // keyword was in the select list: use the FROM clause order instead
        root = createFromClauseJoinPlan(analyzer, refPlans);
        Preconditions.checkNotNull(root);
    }
    // add aggregation, if any
    if (selectStmt.getAggInfo() != null) {
        root = createAggregationPlan(selectStmt, analyzer, root);
    }
    // Preconditions.checkState(!analyzer.hasUnassignedConjuncts());
    return root;
}
#method_after
private PlanNode createSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    // no from clause -> materialize the select's exprs with a UnionNode
    if (selectStmt.getTableRefs().isEmpty()) {
        return createConstantSelectPlan(selectStmt, analyzer);
    }
    // Slot materialization:
    // We need to mark all slots as materialized that are needed during the execution
    // of selectStmt, and we need to do that prior to creating plans for the TableRefs
    // (because createTableRefNode() might end up calling computeMemLayout() on one or
    // more TupleDescriptors, at which point all referenced slots need to be marked).
    // 
    // For non-join predicates, slots are marked as follows:
    // - for base table scan predicates, this is done directly by ScanNode.init(), which
    // can do a better job because it doesn't need to materialize slots that are only
    // referenced for partition pruning, for instance
    // - for inline views, non-join predicates are pushed down, at which point the
    // process repeats itself.
    selectStmt.materializeRequiredSlots(analyzer);
    ArrayList<TupleId> rowTuples = Lists.newArrayList();
    // collect output tuples of subtrees
    for (TableRef tblRef : selectStmt.getTableRefs()) {
        rowTuples.addAll(tblRef.getMaterializedTupleIds());
    }
    // are materialized (see IMPALA-1960).
    if (analyzer.hasEmptySpjResultSet()) {
        PlanNode emptySetNode = new EmptySetNode(ctx_.getNextNodeId(), rowTuples);
        emptySetNode.init(analyzer);
        emptySetNode.setOutputSmap(selectStmt.getBaseTblSmap());
        return createAggregationPlan(selectStmt, analyzer, emptySetNode);
    }
    // create plans for our table refs; use a list here instead of a map to
    // maintain a deterministic order of traversing the TableRefs during join
    // plan generation (helps with tests)
    List<Pair<TableRef, PlanNode>> refPlans = Lists.newArrayList();
    for (TableRef ref : selectStmt.getTableRefs()) {
        PlanNode plan = createTableRefNode(analyzer, ref);
        Preconditions.checkState(plan != null);
        refPlans.add(new Pair(ref, plan));
    }
    // save state of conjunct assignment; needed for join plan generation
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        entry.second.setAssignedConjuncts(analyzer.getAssignedConjuncts());
    }
    PlanNode root = null;
    if (!selectStmt.getSelectList().isStraightJoin()) {
        Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
        root = createCheapestJoinPlan(analyzer, refPlans);
        if (root == null)
            analyzer.setAssignedConjuncts(assignedConjuncts);
    }
    if (selectStmt.getSelectList().isStraightJoin() || root == null) {
        // we didn't have enough stats to do a cost-based join plan, or the STRAIGHT_JOIN
        // keyword was in the select list: use the FROM clause order instead
        root = createFromClauseJoinPlan(analyzer, refPlans);
        Preconditions.checkNotNull(root);
    }
    // add aggregation, if any
    if (selectStmt.getAggInfo() != null) {
        root = createAggregationPlan(selectStmt, analyzer, root);
    }
    // Preconditions.checkState(!analyzer.hasUnassignedConjuncts());
    return root;
}
#end_block

#method_before
private PlanNode createConstantSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws InternalException {
    Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
    ArrayList<Expr> resultExprs = selectStmt.getBaseTblResultExprs();
    ArrayList<String> colLabels = selectStmt.getColLabels();
    // Create tuple descriptor for materialized tuple.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
    // Analysis guarantees that selects without a FROM clause only have constant exprs.
    unionNode.addConstExprList(Lists.newArrayList(resultExprs));
    // Replace the select stmt's resultExprs with SlotRefs into tupleDesc.
    for (int i = 0; i < resultExprs.size(); ++i) {
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(colLabels.get(i));
        slotDesc.setSourceExpr(resultExprs.get(i));
        slotDesc.setType(resultExprs.get(i).getType());
        slotDesc.setStats(ColumnStats.fromExpr(resultExprs.get(i)));
        slotDesc.setIsMaterialized(true);
        SlotRef slotRef = new SlotRef(slotDesc);
        resultExprs.set(i, slotRef);
    }
    tupleDesc.computeMemLayout();
    // UnionNode.init() needs tupleDesc to have been initialized
    unionNode.init(analyzer);
    return unionNode;
}
#method_after
private PlanNode createConstantSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws InternalException {
    Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
    ArrayList<Expr> resultExprs = selectStmt.getResultExprs();
    ArrayList<String> colLabels = selectStmt.getColLabels();
    // Create tuple descriptor for materialized tuple.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
    // Analysis guarantees that selects without a FROM clause only have constant exprs.
    unionNode.addConstExprList(Lists.newArrayList(resultExprs));
    // Replace the select stmt's resultExprs with SlotRefs into tupleDesc.
    for (int i = 0; i < resultExprs.size(); ++i) {
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(colLabels.get(i));
        slotDesc.setSourceExpr(resultExprs.get(i));
        slotDesc.setType(resultExprs.get(i).getType());
        slotDesc.setStats(ColumnStats.fromExpr(resultExprs.get(i)));
        slotDesc.setIsMaterialized(true);
        SlotRef slotRef = new SlotRef(slotDesc);
        resultExprs.set(i, slotRef);
    }
    tupleDesc.computeMemLayout();
    // UnionNode.init() needs tupleDesc to have been initialized
    unionNode.init(analyzer);
    return unionNode;
}
#end_block

#method_before
private PlanNode createInlineViewPlan(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    // If possible, "push down" view predicates; this is needed in order to ensure
    // that predicates such as "x + y = 10" are evaluated in the view's plan tree
    // rather than a SelectNode grafted on top of that plan tree.
    // This doesn't prevent predicate propagation, because predicates like
    // "x = 10" that get pushed down are still connected to equivalent slots
    // via the equality predicates created for the view's select list.
    // Include outer join conjuncts here as well because predicates from the
    // On-clause of an outer join may be pushed into the inline view as well.
    // 
    // Limitations on predicate propagation into inline views:
    // If the inline view computes analytic functions, we cannot push any
    // predicate into the inline view tree (see IMPALA-1243). The reason is that
    // analytic functions compute aggregates over their entire input, and applying
    // filters from the enclosing scope *before* the aggregate computation would
    // alter the results. This is unlike regular aggregate computation, which only
    // makes the *output* of the computation visible to the enclosing scope, so that
    // filters from the enclosing scope can be safely applied (to the grouping cols, say)
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(inlineViewRef.getId().asList(), true);
    boolean migrateConjuncts = !inlineViewRef.getViewStmt().hasLimit() && !inlineViewRef.getViewStmt().hasOffset() && (!(inlineViewRef.getViewStmt() instanceof SelectStmt) || !((SelectStmt) (inlineViewRef.getViewStmt())).hasAnalyticInfo());
    if (migrateConjuncts) {
        // check if we can evaluate them
        List<Expr> preds = Lists.newArrayList();
        for (Expr e : unassigned) {
            if (analyzer.canEvalPredicate(inlineViewRef.getId().asList(), e))
                preds.add(e);
        }
        unassigned.removeAll(preds);
        // Generate predicates to enforce equivalences among slots of the inline view
        // tuple. These predicates are also migrated into the inline view.
        analyzer.createEquivConjuncts(inlineViewRef.getId(), preds);
        // create new predicates against the inline view's unresolved result exprs, not
        // the resolved result exprs, in order to avoid skipping scopes (and ignoring
        // limit clauses on the way)
        List<Expr> viewPredicates = Expr.substituteList(preds, inlineViewRef.getSmap(), analyzer, false);
        // Remove unregistered predicates that reference the same slot on
        // both sides (e.g. a = a). Such predicates have been generated from slot
        // equivalences and may incorrectly reject rows with nulls (IMPALA-1412).
        Predicate<Expr> isIdentityPredicate = new Predicate<Expr>() {

            @Override
            public boolean apply(Expr expr) {
                if (!(expr instanceof BinaryPredicate) || ((BinaryPredicate) expr).getOp() != BinaryPredicate.Operator.EQ) {
                    return false;
                }
                if (!expr.isRegisteredPredicate() && expr.getChild(0) instanceof SlotRef && expr.getChild(1) instanceof SlotRef && (((SlotRef) expr.getChild(0)).getSlotId() == ((SlotRef) expr.getChild(1)).getSlotId())) {
                    return true;
                }
                return false;
            }
        };
        Iterables.removeIf(viewPredicates, isIdentityPredicate);
        // "migrate" conjuncts_ by marking them as assigned and re-registering them with
        // new ids.
        // Mark pre-substitution conjuncts as assigned, since the ids of the new exprs may
        // have changed.
        analyzer.markConjunctsAssigned(preds);
        inlineViewRef.getAnalyzer().registerConjuncts(viewPredicates);
    }
    // mark (fully resolve) slots referenced by remaining unassigned conjuncts_ as
    // materialized
    List<Expr> substUnassigned = Expr.substituteList(unassigned, inlineViewRef.getBaseTblSmap(), analyzer, false);
    analyzer.materializeSlots(substUnassigned);
    // Turn a constant select into a UnionNode that materializes the exprs.
    // TODO: unify this with createConstantSelectPlan(), this is basically the
    // same thing
    QueryStmt viewStmt = inlineViewRef.getViewStmt();
    if (viewStmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) viewStmt;
        if (selectStmt.getTableRefs().isEmpty()) {
            if (inlineViewRef.getAnalyzer().hasEmptyResultSet()) {
                return createEmptyNode(viewStmt, inlineViewRef.getAnalyzer());
            }
            // Analysis should have generated a tuple id_ into which to materialize the exprs.
            Preconditions.checkState(inlineViewRef.getMaterializedTupleIds().size() == 1);
            // we need to materialize all slots of our inline view tuple
            analyzer.getTupleDesc(inlineViewRef.getId()).materializeSlots();
            UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), inlineViewRef.getMaterializedTupleIds().get(0));
            if (analyzer.hasEmptyResultSet())
                return unionNode;
            unionNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
            unionNode.addConstExprList(selectStmt.getBaseTblResultExprs());
            unionNode.init(analyzer);
            return unionNode;
        }
    }
    PlanNode rootNode = createQueryPlan(inlineViewRef.getViewStmt(), inlineViewRef.getAnalyzer(), false);
    // TODO: we should compute the "physical layout" of the view's descriptor, so that
    // the avg row size is availble during optimization; however, that means we need to
    // select references to its resultExprs from the enclosing scope(s)
    rootNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
    // Set smap *before* creating a SelectNode in order to allow proper resolution.
    // Analytics have an additional level of logical to physical slot remapping.
    // The composition creates a mapping from the logical output of the inline view
    // to the physical analytic output. In addition, it retains the logical to
    // physical analytic slot mappings which are needed to resolve exprs that already
    // reference the logical analytic tuple (and not the inline view tuple), e.g.,
    // the result exprs set in the coordinator fragment.
    rootNode.setOutputSmap(ExprSubstitutionMap.compose(inlineViewRef.getBaseTblSmap(), rootNode.getOutputSmap(), analyzer));
    // if the view has a limit we may have conjuncts_ from the enclosing scope left
    if (!migrateConjuncts) {
        rootNode = addUnassignedConjuncts(analyzer, inlineViewRef.getDesc().getId().asList(), rootNode);
    }
    return rootNode;
}
#method_after
private PlanNode createInlineViewPlan(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    // If possible, "push down" view predicates; this is needed in order to ensure
    // that predicates such as "x + y = 10" are evaluated in the view's plan tree
    // rather than a SelectNode grafted on top of that plan tree.
    // This doesn't prevent predicate propagation, because predicates like
    // "x = 10" that get pushed down are still connected to equivalent slots
    // via the equality predicates created for the view's select list.
    // Include outer join conjuncts here as well because predicates from the
    // On-clause of an outer join may be pushed into the inline view as well.
    migrateConjunctsToInlineView(analyzer, inlineViewRef);
    // Turn a constant select into a UnionNode that materializes the exprs.
    // TODO: unify this with createConstantSelectPlan(), this is basically the
    // same thing
    QueryStmt viewStmt = inlineViewRef.getViewStmt();
    if (viewStmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) viewStmt;
        if (selectStmt.getTableRefs().isEmpty()) {
            if (inlineViewRef.getAnalyzer().hasEmptyResultSet()) {
                return createEmptyNode(viewStmt, inlineViewRef.getAnalyzer());
            }
            // Analysis should have generated a tuple id_ into which to materialize the exprs.
            Preconditions.checkState(inlineViewRef.getMaterializedTupleIds().size() == 1);
            // we need to materialize all slots of our inline view tuple
            analyzer.getTupleDesc(inlineViewRef.getId()).materializeSlots();
            UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), inlineViewRef.getMaterializedTupleIds().get(0));
            if (analyzer.hasEmptyResultSet())
                return unionNode;
            unionNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
            unionNode.addConstExprList(selectStmt.getBaseTblResultExprs());
            unionNode.init(analyzer);
            return unionNode;
        }
    }
    PlanNode rootNode = createQueryPlan(inlineViewRef.getViewStmt(), inlineViewRef.getAnalyzer(), false);
    // TODO: we should compute the "physical layout" of the view's descriptor, so that
    // the avg row size is availble during optimization; however, that means we need to
    // select references to its resultExprs from the enclosing scope(s)
    rootNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
    ExprSubstitutionMap inlineViewSmap = inlineViewRef.getSmap();
    if (analyzer.isOuterJoined(inlineViewRef.getId())) {
        // Exprs against non-matched rows of an outer join should always return NULL.
        // Make the rhs exprs of the inline view's smap nullable, if necessary.
        List<Expr> nullableRhs = TupleIsNullPredicate.wrapExprs(inlineViewSmap.getRhs(), rootNode.getTupleIds(), analyzer);
        inlineViewSmap = new ExprSubstitutionMap(inlineViewSmap.getLhs(), nullableRhs);
    }
    // Set output smap of rootNode *before* creating a SelectNode for proper resolution.
    // The output smap is the composition of the inline view's smap and the output smap
    // of the inline view's plan root. This ensures that all downstream exprs referencing
    // the inline view are replaced with exprs referencing the physical output of
    // the inline view's plan.
    ExprSubstitutionMap composedSmap = ExprSubstitutionMap.compose(inlineViewSmap, rootNode.getOutputSmap(), analyzer);
    rootNode.setOutputSmap(composedSmap);
    // place.
    if (!canMigrateConjuncts(inlineViewRef)) {
        rootNode = addUnassignedConjuncts(analyzer, inlineViewRef.getDesc().getId().asList(), rootNode);
    }
    return rootNode;
}
#end_block

#method_before
@Test
public void TestCreateTableAsSelect() throws AnalysisException {
    // Constant select.
    AnalyzesOk("create table newtbl as select 1+2, 'abc'");
    // Select from partitioned and unpartitioned tables using different
    // queries.
    AnalyzesOk("create table newtbl stored as textfile " + "as select * from functional.jointbl");
    AnalyzesOk("create table newtbl stored as parquetfile " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl stored as parquet " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl as select int_col from functional.alltypes");
    AnalyzesOk("create table functional.newtbl " + "as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.tbl as select a.* from functional.alltypes a " + "join functional.alltypes b on (a.int_col=b.int_col) limit 1000");
    // Caching operations
    AnalyzesOk("create table functional.newtbl cached in 'testPool'" + " as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.newtbl uncached" + " as select count(*) as CNT from functional.alltypes");
    // Table already exists with and without IF NOT EXISTS
    AnalysisError("create table functional.alltypes as select 1", "Table already exists: functional.alltypes");
    AnalyzesOk("create table if not exists functional.alltypes as select 1");
    // Database does not exist
    AnalysisError("create table db_does_not_exist.new_table as select 1", "Database does not exist: db_does_not_exist");
    // Analysis errors in the SELECT statement
    AnalysisError("create table newtbl as select * from tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    AnalysisError("create table newtbl as select 1 as c1, 2 as c1", "Duplicate column name: c1");
    // TODO: Enable this once we can execute it.
    AnalysisError("create table newtbl as select * from functional.allcomplextypes", "Target table 'default.newtbl' is incompatible with " + "SELECT / PARTITION expressions.\n" + "Expression 'functional.allcomplextypes.int_array_col' (type: ARRAY<INT>) is " + "not compatible with column 'int_array_col' (type: ARRAY<INT>)");
    // Unsupported file formats
    AnalysisError("create table foo stored as sequencefile as select 1", "CREATE TABLE AS SELECT does not support (SEQUENCEFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE)");
    AnalysisError("create table foo stored as RCFILE as select 1", "CREATE TABLE AS SELECT does not support (RCFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE)");
    // CTAS with a WITH clause and inline view (IMPALA-1100)
    AnalyzesOk("create table test_with as with with_1 as (select 1 as int_col from " + "functional.alltypes as t1 right join (select 1 as int_col from " + "functional.alltypestiny as t1) as t2 on t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
}
#method_after
@Test
public void TestCreateTableAsSelect() throws AnalysisException {
    // Constant select.
    AnalyzesOk("create table newtbl as select 1+2, 'abc'");
    // Select from partitioned and unpartitioned tables using different
    // queries.
    AnalyzesOk("create table newtbl stored as textfile " + "as select * from functional.jointbl");
    AnalyzesOk("create table newtbl stored as parquetfile " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl stored as parquet " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl as select int_col from functional.alltypes");
    AnalyzesOk("create table functional.newtbl " + "as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.tbl as select a.* from functional.alltypes a " + "join functional.alltypes b on (a.int_col=b.int_col) limit 1000");
    // Caching operations
    AnalyzesOk("create table functional.newtbl cached in 'testPool'" + " as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.newtbl uncached" + " as select count(*) as CNT from functional.alltypes");
    // Table already exists with and without IF NOT EXISTS
    AnalysisError("create table functional.alltypes as select 1", "Table already exists: functional.alltypes");
    AnalyzesOk("create table if not exists functional.alltypes as select 1");
    // Database does not exist
    AnalysisError("create table db_does_not_exist.new_table as select 1", "Database does not exist: db_does_not_exist");
    // Analysis errors in the SELECT statement
    AnalysisError("create table newtbl as select * from tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    AnalysisError("create table newtbl as select 1 as c1, 2 as c1", "Duplicate column name: c1");
    // Unsupported file formats
    AnalysisError("create table foo stored as sequencefile as select 1", "CREATE TABLE AS SELECT does not support (SEQUENCEFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE)");
    AnalysisError("create table foo stored as RCFILE as select 1", "CREATE TABLE AS SELECT does not support (RCFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE)");
    // CTAS with a WITH clause and inline view (IMPALA-1100)
    AnalyzesOk("create table test_with as with with_1 as (select 1 as int_col from " + "functional.alltypes as t1 right join (select 1 as int_col from " + "functional.alltypestiny as t1) as t2 on t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
}
#end_block

#method_before
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38.");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0. Size is too small: 0.");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355. Size is too large: 65356.");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0. Size is too small: 0.");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255. Size is too large: 256.");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    for (String format : fileFormats) {
        AnalyzesOk(String.format("create table new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", format));
        // No column definitions.
        AnalysisError(String.format("create table new_table " + "partitioned by (d decimal) comment 'c' stored as %s", format), "Table requires at least 1 column");
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: I");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d date)", "Type 'DATE' is not supported as partition-column type in column: d");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d datetime)", "Type 'DATETIME' is not supported as partition-column type in column: d");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#method_after
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    for (String format : fileFormats) {
        AnalyzesOk(String.format("create table new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", format));
        // No column definitions.
        AnalysisError(String.format("create table new_table " + "partitioned by (d decimal) comment 'c' stored as %s", format), "Table requires at least 1 column");
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: I");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d date)", "Type 'DATE' is not supported as partition-column type in column: d");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d datetime)", "Type 'DATETIME' is not supported as partition-column type in column: d");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#end_block

#method_before
@Test
public void TestCreateView() throws AnalysisException {
    AnalyzesOk("create view foo_new as select int_col, string_col from functional.alltypes");
    AnalyzesOk("create view functional.foo as select * from functional.alltypes");
    AnalyzesOk("create view if not exists foo as select * from functional.alltypes");
    AnalyzesOk("create view foo (a, b) as select int_col, string_col " + "from functional.alltypes");
    AnalyzesOk("create view functional.foo (a, b) as select int_col x, double_col y " + "from functional.alltypes");
    // View can have complex-typed columns.
    AnalyzesOk("create view functional.foo (a, b, c) as " + "select int_array_col, int_map_col, int_struct_col " + "from functional.allcomplextypes");
    // Creating a view on a view is ok (alltypes_view is a view on alltypes).
    AnalyzesOk("create view foo as select * from functional.alltypes_view");
    AnalyzesOk("create view foo (aaa, bbb) as select * from functional.complex_view");
    // Create a view resulting in Hive-style auto-generated column names.
    AnalyzesOk("create view foo as select trim('abc'), 17 * 7");
    // Creating a view on an HBase table is ok.
    AnalyzesOk("create view foo as select * from functional_hbase.alltypesagg");
    // Complex view definition with joins and aggregates.
    AnalyzesOk("create view foo (cnt) as " + "select count(distinct x.int_col) from functional.alltypessmall x " + "inner join functional.alltypessmall y on (x.id = y.id) group by x.bigint_col");
    // Test different query-statement types as view definition.
    AnalyzesOk("create view foo (a, b) as values(1, 'a'), (2, 'b')");
    AnalyzesOk("create view foo (a, b) as select 1, 'a' union all select 2, 'b'");
    // Mismatching number of columns in column definition and view-definition statement.
    AnalysisError("create view foo (a) as select int_col, string_col " + "from functional.alltypes", "Column-definition list has fewer columns (1) than the " + "view-definition query statement returns (2).");
    AnalysisError("create view foo (a, b, c) as select int_col " + "from functional.alltypes", "Column-definition list has more columns (3) than the " + "view-definition query statement returns (1).");
    // Duplicate columns in the view-definition statement.
    AnalysisError("create view foo as select * from functional.alltypessmall a " + "inner join functional.alltypessmall b on a.id = b.id", "Duplicate column name: id");
    // Duplicate columns in the column definition.
    AnalysisError("create view foo (a, b, a) as select int_col, int_col, int_col " + "from functional.alltypes", "Duplicate column name: a");
    // Invalid database/view/column names.
    AnalysisError("create view `???`.new_view as select 1, 2, 3", "Invalid database name: ???");
    AnalysisError("create view `^%&` as select 1, 2, 3", "Invalid table/view name: ^%&");
    AnalysisError("create view foo as select 1 as `???`", "Invalid column/field name: ???");
    AnalysisError("create view foo(`%^&`) as select 1", "Invalid column/field name: %^&");
    // Table/view already exists.
    AnalysisError("create view functional.alltypes as " + "select * from functional.alltypessmall ", "Table already exists: functional.alltypes");
    // Target database does not exist.
    AnalysisError("create view wrongdb.test as " + "select * from functional.alltypessmall ", "Database does not exist: wrongdb");
    // Source database does not exist,
    AnalysisError("create view foo as " + "select * from wrongdb.alltypessmall ", "Could not resolve table reference: 'wrongdb.alltypessmall'");
    // Source table does not exist,
    AnalysisError("create view foo as " + "select * from wrongdb.alltypessmall ", "Could not resolve table reference: 'wrongdb.alltypessmall'");
    // Analysis error in view-definition statement.
    AnalysisError("create view foo as " + "select int_col from functional.alltypessmall union all " + "select string_col from functional.alltypes", "Incompatible return types 'INT' and 'STRING' of exprs " + "'int_col' and 'string_col'.");
    // View with a subquery
    AnalyzesOk("create view test_view_with_subquery as " + "select * from functional.alltypestiny t where exists " + "(select * from functional.alltypessmall s where s.id = t.id)");
}
#method_after
@Test
public void TestCreateView() throws AnalysisException {
    AnalyzesOk("create view foo_new as select int_col, string_col from functional.alltypes");
    AnalyzesOk("create view functional.foo as select * from functional.alltypes");
    AnalyzesOk("create view if not exists foo as select * from functional.alltypes");
    AnalyzesOk("create view foo (a, b) as select int_col, string_col " + "from functional.alltypes");
    AnalyzesOk("create view functional.foo (a, b) as select int_col x, double_col y " + "from functional.alltypes");
    // Creating a view on a view is ok (alltypes_view is a view on alltypes).
    AnalyzesOk("create view foo as select * from functional.alltypes_view");
    AnalyzesOk("create view foo (aaa, bbb) as select * from functional.complex_view");
    // Create a view resulting in Hive-style auto-generated column names.
    AnalyzesOk("create view foo as select trim('abc'), 17 * 7");
    // Creating a view on an HBase table is ok.
    AnalyzesOk("create view foo as select * from functional_hbase.alltypesagg");
    // Complex view definition with joins and aggregates.
    AnalyzesOk("create view foo (cnt) as " + "select count(distinct x.int_col) from functional.alltypessmall x " + "inner join functional.alltypessmall y on (x.id = y.id) group by x.bigint_col");
    // Test different query-statement types as view definition.
    AnalyzesOk("create view foo (a, b) as values(1, 'a'), (2, 'b')");
    AnalyzesOk("create view foo (a, b) as select 1, 'a' union all select 2, 'b'");
    // View with a subquery
    AnalyzesOk("create view test_view_with_subquery as " + "select * from functional.alltypestiny t where exists " + "(select * from functional.alltypessmall s where s.id = t.id)");
    // Mismatching number of columns in column definition and view-definition statement.
    AnalysisError("create view foo (a) as select int_col, string_col " + "from functional.alltypes", "Column-definition list has fewer columns (1) than the " + "view-definition query statement returns (2).");
    AnalysisError("create view foo (a, b, c) as select int_col " + "from functional.alltypes", "Column-definition list has more columns (3) than the " + "view-definition query statement returns (1).");
    // Duplicate columns in the view-definition statement.
    AnalysisError("create view foo as select * from functional.alltypessmall a " + "inner join functional.alltypessmall b on a.id = b.id", "Duplicate column name: id");
    // Duplicate columns in the column definition.
    AnalysisError("create view foo (a, b, a) as select int_col, int_col, int_col " + "from functional.alltypes", "Duplicate column name: a");
    // Invalid database/view/column names.
    AnalysisError("create view `???`.new_view as select 1, 2, 3", "Invalid database name: ???");
    AnalysisError("create view `^%&` as select 1, 2, 3", "Invalid table/view name: ^%&");
    AnalysisError("create view foo as select 1 as `???`", "Invalid column/field name: ???");
    AnalysisError("create view foo(`%^&`) as select 1", "Invalid column/field name: %^&");
    // Table/view already exists.
    AnalysisError("create view functional.alltypes as " + "select * from functional.alltypessmall ", "Table already exists: functional.alltypes");
    // Target database does not exist.
    AnalysisError("create view wrongdb.test as " + "select * from functional.alltypessmall ", "Database does not exist: wrongdb");
    // Source database does not exist,
    AnalysisError("create view foo as " + "select * from wrongdb.alltypessmall ", "Could not resolve table reference: 'wrongdb.alltypessmall'");
    // Source table does not exist,
    AnalysisError("create view foo as " + "select * from wrongdb.alltypessmall ", "Could not resolve table reference: 'wrongdb.alltypessmall'");
    // Analysis error in view-definition statement.
    AnalysisError("create view foo as " + "select int_col from functional.alltypessmall union all " + "select string_col from functional.alltypes", "Incompatible return types 'INT' and 'STRING' of exprs " + "'int_col' and 'string_col'.");
    // View cannot have complex-typed columns because complex-typed exprs are
    // not supported in the select list.
    AnalysisError("create view functional.foo (a, b, c) as " + "select int_array_col, int_map_col, int_struct_col " + "from functional.allcomplextypes", "Expr 'int_array_col' in select list returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
}
#end_block

#method_before
@Test
public void TestUdf() throws AnalysisException {
    final String symbol = "'_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'";
    final String udfSuffix = " LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=" + symbol;
    final String udfSuffixIr = " LOCATION '/test-warehouse/test-udfs.ll' " + "SYMBOL=" + symbol;
    final String hdfsPath = "hdfs://localhost:20500/test-warehouse/libTestUdfs.so";
    AnalyzesOk("create function foo() RETURNS int" + udfSuffix);
    AnalyzesOk("create function foo(int, int, string) RETURNS int" + udfSuffix);
    // Try some fully qualified function names
    AnalyzesOk("create function functional.B() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.B1() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.`B1C`() RETURNS int" + udfSuffix);
    // Name with underscore
    AnalyzesOk("create function A_B() RETURNS int" + udfSuffix);
    // Locations for all the udfs types.
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.so' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'", "Could not load binary: /test-warehouse/libTestUdfs.ll");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo(int) RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' SYMBOL='Identity'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.SO' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/hive-exec.jar' SYMBOL='a'");
    // Test hive UDFs for unsupported types
    AnalysisError("create function foo() RETURNS timestamp LOCATION '/a.jar'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo(timestamp) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo() RETURNS decimal LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(Decimal) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(char(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(varchar(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5) LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5) LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5)" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5)" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo(CHAR(5)) RETURNS int" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(VARCHAR(5)) RETURNS int" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalyzesOk("create function foo() RETURNS decimal" + udfSuffix);
    AnalyzesOk("create function foo() RETURNS decimal(38,10)" + udfSuffix);
    AnalyzesOk("create function foo(Decimal, decimal(10, 2)) RETURNS int" + udfSuffix);
    AnalysisError("create function foo() RETURNS decimal(100)" + udfSuffix, "Decimal precision must be <= 38.");
    AnalysisError("create function foo(Decimal(2, 3)) RETURNS int" + udfSuffix, "Decimal scale (3) must be <= precision (2).");
    // Varargs
    AnalyzesOk("create function foo(INT...) RETURNS int" + udfSuffix);
    // Prepare/Close functions
    AnalyzesOk("create function foo() returns int" + udfSuffix + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='_Z19ValidateOpenPreparePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'" + " close_fn='_Z17ValidateOpenClosePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " close_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn='FakePrepare'", "Could not find function FakePrepare(impala_udf::FunctionContext*, " + "impala_udf::FunctionContext::FunctionStateScope) in: ");
    // Try to create a function with the same name as a builtin
    AnalysisError("create function sin(double) RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    AnalysisError("create function sin() RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    // Try to create with a bad location
    AnalysisError("create function foo() RETURNS int LOCATION 'bad-location' SYMBOL='c'", "URI path must be absolute: bad-location");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'blah://localhost:50200/bad-location' SYMBOL='c'", "No FileSystem for scheme: blah");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'file:///foo.jar' SYMBOL='c'", "Could not load binary: file:///foo.jar");
    // Try creating udfs with unknown extensions
    AnalysisError("create function foo() RETURNS int LOCATION '/binary' SYMBOL='a'", "Unknown binary type: '/binary'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.a' SYMBOL='a'", "Unknown binary type: '/binary.a'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so.' SYMBOL='a'", "Unknown binary type: '/binary.so.'. Binary must end in .jar, .so or .ll");
    // Try with missing symbol
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so'", "Argument 'SYMBOL' must be set.");
    // Try with symbols missing in binary and symbols
    AnalysisError("create function foo() RETURNS int LOCATION '/blah.so' " + "SYMBOL='ab'", "Could not load binary: /blah.so");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.JAR' SYMBOL='a'", "Could not load binary: /binary.JAR");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='b'", "Could not find function b() in: " + hdfsPath);
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=''", "Could not find symbol ''");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='_ZAB'", "Could not find symbol '_ZAB' in: " + hdfsPath);
    // Infer the fully mangled symbol from the signature
    AnalyzesOk("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // We can't get the return type so any of those will match
    AnalyzesOk("create function foo() RETURNS double " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // The part the user specifies is case sensitive
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='noArgs'", "Could not find function noArgs() in: " + hdfsPath);
    // Types no longer match
    AnalysisError("create function foo(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'", "Could not find function NoArgs(INT) in: " + hdfsPath);
    // Check we can match identity for all types
    AnalyzesOk("create function identity(boolean) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(tinyint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(smallint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(bigint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(float) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(double) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(string) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function all_types_fn(string, boolean, tinyint, " + "smallint, int, bigint, float, double, decimal) returns int " + "location '/test-warehouse/libTestUdfs.so' symbol='AllTypes'");
    // Try creating functions with illegal function names.
    AnalysisError("create function 123A() RETURNS int" + udfSuffix, "Function cannot start with a digit: 123a");
    AnalysisError("create function A.`1A`() RETURNS int" + udfSuffix, "Function cannot start with a digit: 1a");
    AnalysisError("create function A.`ABC-D`() RETURNS int" + udfSuffix, "Function names must be all alphanumeric or underscore. Invalid name: abc-d");
    AnalysisError("create function baddb.f() RETURNS int" + udfSuffix, "Database does not exist: baddb");
    AnalysisError("create function a.b.c() RETURNS int" + udfSuffix, "Invalid function name: 'a.b.c'. Expected [dbname].funcname.");
    AnalysisError("create function a.b.c.d(smallint) RETURNS int" + udfSuffix, "Invalid function name: 'a.b.c.d'. Expected [dbname].funcname.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try dropping functions.
    AnalyzesOk("drop function if exists foo()");
    AnalysisError("drop function foo()", "Function does not exist: foo()");
    AnalyzesOk("drop function if exists a.foo()");
    AnalysisError("drop function a.foo()", "Database does not exist: a");
    AnalyzesOk("drop function if exists foo()");
    AnalyzesOk("drop function if exists foo(int...)");
    AnalyzesOk("drop function if exists foo(double, int...)");
    // Add functions default.TestFn(), default.TestFn(double), default.TestFn(String...),
    addTestFunction("TestFn", new ArrayList<ScalarType>(), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.DOUBLE), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.STRING), true);
    AnalysisError("create function TestFn() RETURNS INT " + udfSuffix, "Function already exists: testfn()");
    AnalysisError("create function TestFn(double) RETURNS INT " + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Fn(Double) and Fn(Double...) should be a conflict.
    AnalysisError("create function TestFn(double...) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    AnalysisError("create function TestFn(double) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Add default.TestFn(int, int)
    addTestFunction("TestFn", Lists.newArrayList(Type.INT, Type.INT), false);
    AnalyzesOk("drop function TestFn(int, int)");
    AnalysisError("drop function TestFn(int, int, int)", "Function does not exist: testfn(INT, INT, INT)");
    // Fn(String...) was already added.
    AnalysisError("create function TestFn(String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String...) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String, String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalyzesOk("create function TestFn(String, String, Int) RETURNS INT" + udfSuffix);
    // Check function overloading.
    AnalyzesOk("create function TestFn(int) RETURNS INT " + udfSuffix);
    // Create a function with the same signature in a different db
    AnalyzesOk("create function functional.TestFn() RETURNS INT " + udfSuffix);
    AnalyzesOk("drop function TestFn()");
    AnalyzesOk("drop function TestFn(double)");
    AnalyzesOk("drop function TestFn(string...)");
    AnalysisError("drop function TestFn(double...)", "Function does not exist: testfn(DOUBLE...)");
    AnalysisError("drop function TestFn(int)", "Function does not exist: testfn(INT)");
    AnalysisError("drop function functional.TestFn()", "Function does not exist: testfn()");
    AnalysisError("create function f() returns int " + udfSuffix + "init_fn='a'", "Optional argument 'INIT_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "serialize_fn='a'", "Optional argument 'SERIALIZE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "merge_fn='a'", "Optional argument 'MERGE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "finalize_fn='a'", "Optional argument 'FINALIZE_FN' should not be set");
}
#method_after
@Test
public void TestUdf() throws AnalysisException {
    final String symbol = "'_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'";
    final String udfSuffix = " LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=" + symbol;
    final String udfSuffixIr = " LOCATION '/test-warehouse/test-udfs.ll' " + "SYMBOL=" + symbol;
    final String hdfsPath = "hdfs://localhost:20500/test-warehouse/libTestUdfs.so";
    AnalyzesOk("create function foo() RETURNS int" + udfSuffix);
    AnalyzesOk("create function foo(int, int, string) RETURNS int" + udfSuffix);
    // Try some fully qualified function names
    AnalyzesOk("create function functional.B() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.B1() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.`B1C`() RETURNS int" + udfSuffix);
    // Name with underscore
    AnalyzesOk("create function A_B() RETURNS int" + udfSuffix);
    // Locations for all the udfs types.
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.so' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'", "Could not load binary: /test-warehouse/libTestUdfs.ll");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo(int) RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' SYMBOL='Identity'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.SO' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/hive-exec.jar' SYMBOL='a'");
    // Test hive UDFs for unsupported types
    AnalysisError("create function foo() RETURNS timestamp LOCATION '/test-warehouse/hive-exec.jar' SYMBOL='a'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo(timestamp) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo() RETURNS decimal LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(Decimal) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(char(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(varchar(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5) LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5) LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5)" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5)" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo(CHAR(5)) RETURNS int" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(VARCHAR(5)) RETURNS int" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalyzesOk("create function foo() RETURNS decimal" + udfSuffix);
    AnalyzesOk("create function foo() RETURNS decimal(38,10)" + udfSuffix);
    AnalyzesOk("create function foo(Decimal, decimal(10, 2)) RETURNS int" + udfSuffix);
    AnalysisError("create function foo() RETURNS decimal(100)" + udfSuffix, "Decimal precision must be <= 38: 100");
    AnalysisError("create function foo(Decimal(2, 3)) RETURNS int" + udfSuffix, "Decimal scale (3) must be <= precision (2)");
    // Varargs
    AnalyzesOk("create function foo(INT...) RETURNS int" + udfSuffix);
    // Prepare/Close functions
    AnalyzesOk("create function foo() returns int" + udfSuffix + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='_Z19ValidateOpenPreparePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'" + " close_fn='_Z17ValidateOpenClosePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " close_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn='FakePrepare'", "Could not find function FakePrepare(impala_udf::FunctionContext*, " + "impala_udf::FunctionContext::FunctionStateScope) in: ");
    // Try to create a function with the same name as a builtin
    AnalysisError("create function sin(double) RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    AnalysisError("create function sin() RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    // Try to create with a bad location
    AnalysisError("create function foo() RETURNS int LOCATION 'bad-location' SYMBOL='c'", "URI path must be absolute: bad-location");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'blah://localhost:50200/bad-location' SYMBOL='c'", "No FileSystem for scheme: blah");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'file:///foo.jar' SYMBOL='c'", "Could not load binary: file:///foo.jar");
    // Try creating udfs with unknown extensions
    AnalysisError("create function foo() RETURNS int LOCATION '/binary' SYMBOL='a'", "Unknown binary type: '/binary'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.a' SYMBOL='a'", "Unknown binary type: '/binary.a'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so.' SYMBOL='a'", "Unknown binary type: '/binary.so.'. Binary must end in .jar, .so or .ll");
    // Try with missing symbol
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so'", "Argument 'SYMBOL' must be set.");
    // Try with symbols missing in binary and symbols
    AnalysisError("create function foo() RETURNS int LOCATION '/blah.so' " + "SYMBOL='ab'", "Could not load binary: /blah.so");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.JAR' SYMBOL='a'", "Could not load binary: /binary.JAR");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='b'", "Could not find function b() in: " + hdfsPath);
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=''", "Could not find symbol ''");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='_ZAB'", "Could not find symbol '_ZAB' in: " + hdfsPath);
    // Infer the fully mangled symbol from the signature
    AnalyzesOk("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // We can't get the return type so any of those will match
    AnalyzesOk("create function foo() RETURNS double " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // The part the user specifies is case sensitive
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='noArgs'", "Could not find function noArgs() in: " + hdfsPath);
    // Types no longer match
    AnalysisError("create function foo(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'", "Could not find function NoArgs(INT) in: " + hdfsPath);
    // Check we can match identity for all types
    AnalyzesOk("create function identity(boolean) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(tinyint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(smallint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(bigint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(float) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(double) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(string) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function all_types_fn(string, boolean, tinyint, " + "smallint, int, bigint, float, double, decimal) returns int " + "location '/test-warehouse/libTestUdfs.so' symbol='AllTypes'");
    // Try creating functions with illegal function names.
    AnalysisError("create function 123A() RETURNS int" + udfSuffix, "Function cannot start with a digit: 123a");
    AnalysisError("create function A.`1A`() RETURNS int" + udfSuffix, "Function cannot start with a digit: 1a");
    AnalysisError("create function A.`ABC-D`() RETURNS int" + udfSuffix, "Function names must be all alphanumeric or underscore. Invalid name: abc-d");
    AnalysisError("create function baddb.f() RETURNS int" + udfSuffix, "Database does not exist: baddb");
    AnalysisError("create function a.b.c() RETURNS int" + udfSuffix, "Invalid function name: 'a.b.c'. Expected [dbname].funcname.");
    AnalysisError("create function a.b.c.d(smallint) RETURNS int" + udfSuffix, "Invalid function name: 'a.b.c.d'. Expected [dbname].funcname.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try dropping functions.
    AnalyzesOk("drop function if exists foo()");
    AnalysisError("drop function foo()", "Function does not exist: foo()");
    AnalyzesOk("drop function if exists a.foo()");
    AnalysisError("drop function a.foo()", "Database does not exist: a");
    AnalyzesOk("drop function if exists foo()");
    AnalyzesOk("drop function if exists foo(int...)");
    AnalyzesOk("drop function if exists foo(double, int...)");
    // Add functions default.TestFn(), default.TestFn(double), default.TestFn(String...),
    addTestFunction("TestFn", new ArrayList<ScalarType>(), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.DOUBLE), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.STRING), true);
    AnalysisError("create function TestFn() RETURNS INT " + udfSuffix, "Function already exists: testfn()");
    AnalysisError("create function TestFn(double) RETURNS INT " + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Fn(Double) and Fn(Double...) should be a conflict.
    AnalysisError("create function TestFn(double...) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    AnalysisError("create function TestFn(double) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Add default.TestFn(int, int)
    addTestFunction("TestFn", Lists.newArrayList(Type.INT, Type.INT), false);
    AnalyzesOk("drop function TestFn(int, int)");
    AnalysisError("drop function TestFn(int, int, int)", "Function does not exist: testfn(INT, INT, INT)");
    // Fn(String...) was already added.
    AnalysisError("create function TestFn(String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String...) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String, String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalyzesOk("create function TestFn(String, String, Int) RETURNS INT" + udfSuffix);
    // Check function overloading.
    AnalyzesOk("create function TestFn(int) RETURNS INT " + udfSuffix);
    // Create a function with the same signature in a different db
    AnalyzesOk("create function functional.TestFn() RETURNS INT " + udfSuffix);
    AnalyzesOk("drop function TestFn()");
    AnalyzesOk("drop function TestFn(double)");
    AnalyzesOk("drop function TestFn(string...)");
    AnalysisError("drop function TestFn(double...)", "Function does not exist: testfn(DOUBLE...)");
    AnalysisError("drop function TestFn(int)", "Function does not exist: testfn(INT)");
    AnalysisError("drop function functional.TestFn()", "Function does not exist: testfn()");
    AnalysisError("create function f() returns int " + udfSuffix + "init_fn='a'", "Optional argument 'INIT_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "serialize_fn='a'", "Optional argument 'SERIALIZE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "merge_fn='a'", "Optional argument 'MERGE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "finalize_fn='a'", "Optional argument 'FINALIZE_FN' should not be set");
}
#end_block

#method_before
@Test
public void TestUda() throws AnalysisException {
    final String loc = " LOCATION '/test-warehouse/libTestUdas.so' ";
    final String hdfsLoc = "hdfs://localhost:20500/test-warehouse/libTestUdas.so";
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AGgInit'", "Could not find function AGgInit() returns INT in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(int, int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate'");
    // TODO: remove these when the BE can execute them
    AnalysisError("create aggregate function foo(int...) RETURNS int" + loc, "UDAs with varargs are not yet supported.");
    AnalysisError("create aggregate function " + "foo(int, int, int, int, int, int, int , int, int) " + "RETURNS int" + loc, "UDAs with more than 8 arguments are not yet supported.");
    // Check that CHAR and VARCHAR are not valid UDA argument or return types
    String symbols = " UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'";
    AnalysisError("create aggregate function foo(CHAR(5)) RETURNS int" + loc + symbols, "UDAs with CHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(VARCHAR(5)) RETURNS int" + loc + symbols, "UDAs with VARCHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS CHAR(5)" + loc + symbols, "UDAs with CHAR return type are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS VARCHAR(5)" + loc + symbols, "UDAs with VARCHAR return type are not yet supported.");
    // Specify the complete symbol. If the user does this, we can't guess the
    // other function names.
    // TODO: think about these error messages more. Perhaps they can be made
    // more actionable.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'");
    // Try with intermediate type
    // TODO: this is currently not supported. Remove these tests and re-enable
    // the commented out ones when we do.
    AnalyzesOk("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE int" + loc + "UPDATE_FN='AggUpdate'");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE double" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DOUBLE, that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE char(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, CHAR(10), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DECIMAL(10,0), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(40)" + loc + "UPDATE_FN='AggUpdate'", "Decimal precision must be <= 38.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='AggUpdate'");
    // AnalysisError("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge'" ,
    // "Finalize() is required for this UDA.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge' FINALIZE_FN='AggFinalize'");
    // Udf only arguments must not be set.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "SYMBOL='Bad'", "Optional argument 'SYMBOL' should not be set.");
    // Invalid char(0) type.
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE CHAR(0) LOCATION '/foo.so' UPDATE_FN='b'", "Char size must be > 0. Size is too small: 0.");
    AnalysisError("create aggregate function foo() RETURNS int" + loc, "UDAs must take at least one argument.");
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.jar' UPDATE_FN='b'", "Java UDAs are not supported.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create aggregate function foo(string, double) RETURNS array<int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(map<string,int>) RETURNS int " + loc + "UPDATE_FN='AggUpdate'", "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(int) RETURNS struct<f:int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Test missing .ll file. TODO: reenable when we can run IR UDAs
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.ll' UPDATE_FN='Fn'", "IR UDAs are not yet supported.");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='Fn'", "Could not load binary: /foo.ll");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='_ZABCD'", "Could not load binary: /foo.ll");
    // Test cases where the UPDATE_FN doesn't contain "Update" in which case the user has
    // to explicitly specify the other functions.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggFinalize'");
    // Serialize and Finalize have the same signature, make sure that's possible.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggFinalize' FINALIZE_FN='AggFinalize'");
    // If you don't specify the full symbol, we look for it in the binary. This should
    // prevent mismatched names by accident.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggSerialize'", "Could not find function AggSerialize() returns STRING in: " + hdfsLoc);
    // If you specify a mangled name, we just check it exists.
    // TODO: we should be able to validate better. This is almost certainly going
    // to crash everything.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' " + "INIT_FN='_Z12AggSerializePN10impala_udf15FunctionContextERKNS_6IntValE'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='_ZAggSerialize'", "Could not find symbol '_ZAggSerialize' in: " + hdfsLoc);
    // Tests for checking the symbol exists
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update'", "Could not find function Agg2Init() returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit'", "Could not find function Agg2Merge(STRING) returns STRING in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='BadFn'", "Could not find function BadFn(STRING) returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "FINALIZE_FN='not there'", "Could not find function not there(STRING) in: " + hdfsLoc);
}
#method_after
@Test
public void TestUda() throws AnalysisException {
    final String loc = " LOCATION '/test-warehouse/libTestUdas.so' ";
    final String hdfsLoc = "hdfs://localhost:20500/test-warehouse/libTestUdas.so";
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AGgInit'", "Could not find function AGgInit() returns INT in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(int, int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate'");
    // TODO: remove these when the BE can execute them
    AnalysisError("create aggregate function foo(int...) RETURNS int" + loc, "UDAs with varargs are not yet supported.");
    AnalysisError("create aggregate function " + "foo(int, int, int, int, int, int, int , int, int) " + "RETURNS int" + loc, "UDAs with more than 8 arguments are not yet supported.");
    // Check that CHAR and VARCHAR are not valid UDA argument or return types
    String symbols = " UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'";
    AnalysisError("create aggregate function foo(CHAR(5)) RETURNS int" + loc + symbols, "UDAs with CHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(VARCHAR(5)) RETURNS int" + loc + symbols, "UDAs with VARCHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS CHAR(5)" + loc + symbols, "UDAs with CHAR return type are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS VARCHAR(5)" + loc + symbols, "UDAs with VARCHAR return type are not yet supported.");
    // Specify the complete symbol. If the user does this, we can't guess the
    // other function names.
    // TODO: think about these error messages more. Perhaps they can be made
    // more actionable.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'");
    // Try with intermediate type
    // TODO: this is currently not supported. Remove these tests and re-enable
    // the commented out ones when we do.
    AnalyzesOk("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE int" + loc + "UPDATE_FN='AggUpdate'");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE double" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DOUBLE, that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE char(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, CHAR(10), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DECIMAL(10,0), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(40)" + loc + "UPDATE_FN='AggUpdate'", "Decimal precision must be <= 38: 40");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='AggUpdate'");
    // AnalysisError("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge'" ,
    // "Finalize() is required for this UDA.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge' FINALIZE_FN='AggFinalize'");
    // Udf only arguments must not be set.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "SYMBOL='Bad'", "Optional argument 'SYMBOL' should not be set.");
    // Invalid char(0) type.
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE CHAR(0) LOCATION '/foo.so' UPDATE_FN='b'", "Char size must be > 0: 0");
    AnalysisError("create aggregate function foo() RETURNS int" + loc, "UDAs must take at least one argument.");
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.jar' UPDATE_FN='b'", "Java UDAs are not supported.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create aggregate function foo(string, double) RETURNS array<int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(map<string,int>) RETURNS int " + loc + "UPDATE_FN='AggUpdate'", "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(int) RETURNS struct<f:int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Test missing .ll file. TODO: reenable when we can run IR UDAs
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.ll' UPDATE_FN='Fn'", "IR UDAs are not yet supported.");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='Fn'", "Could not load binary: /foo.ll");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='_ZABCD'", "Could not load binary: /foo.ll");
    // Test cases where the UPDATE_FN doesn't contain "Update" in which case the user has
    // to explicitly specify the other functions.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggFinalize'");
    // Serialize and Finalize have the same signature, make sure that's possible.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggFinalize' FINALIZE_FN='AggFinalize'");
    // If you don't specify the full symbol, we look for it in the binary. This should
    // prevent mismatched names by accident.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggSerialize'", "Could not find function AggSerialize() returns STRING in: " + hdfsLoc);
    // If you specify a mangled name, we just check it exists.
    // TODO: we should be able to validate better. This is almost certainly going
    // to crash everything.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' " + "INIT_FN='_Z12AggSerializePN10impala_udf15FunctionContextERKNS_6IntValE'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='_ZAggSerialize'", "Could not find symbol '_ZAggSerialize' in: " + hdfsLoc);
    // Tests for checking the symbol exists
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update'", "Could not find function Agg2Init() returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit'", "Could not find function Agg2Merge(STRING) returns STRING in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='BadFn'", "Could not find function BadFn(STRING) returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "FINALIZE_FN='not there'", "Could not find function not there(STRING) in: " + hdfsLoc);
}
#end_block

#method_before
@Test
public void TestTypes() {
    // Test primitive types.
    TypeDefsAnalyzeOk("BOOLEAN");
    TypeDefsAnalyzeOk("TINYINT");
    TypeDefsAnalyzeOk("SMALLINT");
    TypeDefsAnalyzeOk("INT", "INTEGER");
    TypeDefsAnalyzeOk("BIGINT");
    TypeDefsAnalyzeOk("FLOAT");
    TypeDefsAnalyzeOk("DOUBLE", "REAL");
    TypeDefsAnalyzeOk("STRING");
    TypeDefsAnalyzeOk("CHAR(1)", "CHAR(20)");
    TypeDefsAnalyzeOk("BINARY");
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("TIMESTAMP");
    // Test decimal.
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("DECIMAL(1)");
    TypeDefsAnalyzeOk("DECIMAL(12, 7)");
    TypeDefsAnalyzeOk("DECIMAL(38)");
    TypeDefsAnalyzeOk("DECIMAL(38, 1)");
    TypeDefsAnalyzeOk("DECIMAL(38, 38)");
    TypeDefAnalysisError("DECIMAL(1, 10)", "Decimal scale (10) must be <= precision (1).");
    TypeDefAnalysisError("DECIMAL(0, 0)", "Decimal precision must be greater than 0.");
    TypeDefAnalysisError("DECIMAL(39, 0)", "Decimal precision must be <= 38.");
    // Test complex types.
    TypeDefsAnalyzeOk("ARRAY<BIGINT>");
    TypeDefsAnalyzeOk("MAP<TINYINT, DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<f:TINYINT>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT, b:BIGINT, c:DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT COMMENT 'x', b:BIGINT, c:DOUBLE COMMENT 'y'>");
    // Map keys can't be complex types.
    TypeDefAnalysisError("map<array<int>, int>", "Map type cannot have a complex-typed key: MAP<ARRAY<INT>,INT>");
    // Duplicate struct-field name.
    TypeDefAnalysisError("STRUCT<f1: int, f2: string, f1: float>", "Duplicate field name 'f1' in struct 'STRUCT<f1:INT,f2:STRING,f1:FLOAT>'");
    // Invalid struct-field name.
    TypeDefAnalysisError("STRUCT<`???`: int>", "Invalid struct field name: ???");
}
#method_after
@Test
public void TestTypes() {
    // Test primitive types.
    TypeDefsAnalyzeOk("BOOLEAN");
    TypeDefsAnalyzeOk("TINYINT");
    TypeDefsAnalyzeOk("SMALLINT");
    TypeDefsAnalyzeOk("INT", "INTEGER");
    TypeDefsAnalyzeOk("BIGINT");
    TypeDefsAnalyzeOk("FLOAT");
    TypeDefsAnalyzeOk("DOUBLE", "REAL");
    TypeDefsAnalyzeOk("STRING");
    TypeDefsAnalyzeOk("CHAR(1)", "CHAR(20)");
    TypeDefsAnalyzeOk("BINARY");
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("TIMESTAMP");
    // Test decimal.
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("DECIMAL(1)");
    TypeDefsAnalyzeOk("DECIMAL(12, 7)");
    TypeDefsAnalyzeOk("DECIMAL(38)");
    TypeDefsAnalyzeOk("DECIMAL(38, 1)");
    TypeDefsAnalyzeOk("DECIMAL(38, 38)");
    TypeDefAnalysisError("DECIMAL(1, 10)", "Decimal scale (10) must be <= precision (1)");
    TypeDefAnalysisError("DECIMAL(0, 0)", "Decimal precision must be > 0: 0");
    TypeDefAnalysisError("DECIMAL(39, 0)", "Decimal precision must be <= 38");
    // Test complex types.
    TypeDefsAnalyzeOk("ARRAY<BIGINT>");
    TypeDefsAnalyzeOk("MAP<TINYINT, DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<f:TINYINT>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT, b:BIGINT, c:DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT COMMENT 'x', b:BIGINT, c:DOUBLE COMMENT 'y'>");
    // Map keys can't be complex types.
    TypeDefAnalysisError("map<array<int>, int>", "Map type cannot have a complex-typed key: MAP<ARRAY<INT>,INT>");
    // Duplicate struct-field name.
    TypeDefAnalysisError("STRUCT<f1: int, f2: string, f1: float>", "Duplicate field name 'f1' in struct 'STRUCT<f1:INT,f2:STRING,f1:FLOAT>'");
    // Invalid struct-field name.
    TypeDefAnalysisError("STRUCT<`???`: int>", "Invalid struct field name: ???");
}
#end_block

#method_before
@Test
public void TestPermissionValidation() throws AnalysisException {
    String location = "/test-warehouse/.tmp_" + UUID.randomUUID().toString();
    Path parentPath = FileSystemUtil.createFullyQualifiedPath(new Path(location));
    FileSystem fs = null;
    try {
        fs = parentPath.getFileSystem(FileSystemUtil.getConfiguration());
        // Test location doesn't exist
        AnalyzesOk(String.format("create table new_table (col INT) location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        // Test localtion path with trailing slash.
        AnalyzesOk(String.format("create table new_table (col INT) location " + "'%s/new_table/'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table location '%s/new_table' " + "as select 1, 1", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table like functional.alltypes " + "location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create database new_db location '%s/new_db'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        fs.mkdirs(parentPath);
        // Create a test data file for load data test
        FSDataOutputStream out = fs.create(new Path(parentPath, "test_loaddata/testdata.txt"));
        out.close();
        fs.setPermission(parentPath, new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));
        // Test location exists but Impala doesn't have sufficient permission
        AnalyzesOk(String.format("create data Source serverlog location " + "'%s/foo.jar' class 'foo.Bar' API_VERSION 'V1'", location), String.format("Impala does not have READ access to path '%s'", parentPath));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "add partition (s2='hello') location '%s/new_partition'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "partition(s2=NULL) set location '%s/new_part_loc'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        // Test location exists and Impala does have sufficient permission
        fs.setPermission(parentPath, new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location));
    } catch (IOException e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        // Clean up
        try {
            if (fs != null && fs.exists(parentPath)) {
                fs.delete(parentPath, true);
            }
        } catch (IOException e) {
        // Ignore
        }
    }
}
#method_after
@Test
public void TestPermissionValidation() throws AnalysisException {
    String location = "/test-warehouse/.tmp_" + UUID.randomUUID().toString();
    Path parentPath = FileSystemUtil.createFullyQualifiedPath(new Path(location));
    FileSystem fs = null;
    try {
        fs = parentPath.getFileSystem(FileSystemUtil.getConfiguration());
        // Test location doesn't exist
        AnalyzesOk(String.format("create table new_table (col INT) location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        // Test localtion path with trailing slash.
        AnalyzesOk(String.format("create table new_table (col INT) location " + "'%s/new_table/'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table location '%s/new_table' " + "as select 1, 1", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table like functional.alltypes " + "location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create database new_db location '%s/new_db'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        fs.mkdirs(parentPath);
        // Create a test data file for load data test
        FSDataOutputStream out = fs.create(new Path(parentPath, "test_loaddata/testdata.txt"));
        out.close();
        fs.setPermission(parentPath, new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));
        // Test location exists but Impala doesn't have sufficient permission
        AnalyzesOk(String.format("create data Source serverlog location " + "'%s/foo.jar' class 'foo.Bar' API_VERSION 'V1'", location), String.format("Impala does not have READ access to path '%s'", parentPath));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "add partition (s2='hello') location '%s/new_partition'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.stringpartitionkey " + "partition(string_col = 'partition1') set location '%s/new_part_loc'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        // Test location exists and Impala does have sufficient permission
        fs.setPermission(parentPath, new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location));
    } catch (IOException e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        // Clean up
        try {
            if (fs != null && fs.exists(parentPath)) {
                fs.delete(parentPath, true);
            }
        } catch (IOException e) {
        // Ignore
        }
    }
}
#end_block

#method_before
private void computeResultPredicateDependencies(Analyzer analyzer) {
    Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
    for (ExprId exprId : assignedConjuncts) {
        if (exprId == null)
            continue;
        Expr conjunct = analyzer.getConjunct(exprId);
        Preconditions.checkNotNull(conjunct);
        resultDependencyPredicates_.add(conjunct);
    }
    Set<String> predicateBaseCols = Sets.newHashSet();
    for (Expr expr : resultDependencyPredicates_) {
        getSourceBaseCols(expr, predicateBaseCols, null, true);
    }
    if (predicateBaseCols.isEmpty())
        return;
    Set<String> targets = Sets.newHashSet(targetColumnLabels_);
    createMultiEdge(targets, predicateBaseCols, MultiEdge.EdgeType.PREDICATE);
}
#method_after
private void computeResultPredicateDependencies(Analyzer analyzer) {
    List<Expr> conjuncts = analyzer.getConjuncts();
    for (Expr expr : conjuncts) {
        if (expr.isAuxExpr())
            continue;
        resultDependencyPredicates_.add(expr);
    }
    Set<String> predicateBaseCols = Sets.newHashSet();
    for (Expr expr : resultDependencyPredicates_) {
        getSourceBaseCols(expr, predicateBaseCols, null, true);
    }
    if (predicateBaseCols.isEmpty())
        return;
    Set<String> targets = Sets.newHashSet(targetColumnLabels_);
    createMultiEdge(targets, predicateBaseCols, MultiEdge.EdgeType.PREDICATE);
}
#end_block

#method_before
private void getSourceBaseCols(Expr expr, Set<String> sourceBaseCols, List<Expr> directPredDeps, boolean traversePredDeps) {
    List<Expr> exprsToTraverse = getProjectionDeps(expr);
    List<Expr> predicateDepExprs = getPredicateDeps(expr);
    if (directPredDeps != null)
        directPredDeps.addAll(predicateDepExprs);
    if (traversePredDeps)
        exprsToTraverse.addAll(predicateDepExprs);
    List<SlotId> slotIds = Lists.newArrayList();
    for (Expr e : exprsToTraverse) {
        e.getIds(null, slotIds);
    }
    for (SlotId slotId : slotIds) {
        SlotDescriptor slotDesc = descTbl_.getSlotDesc(slotId);
        List<Expr> sourceExprs = slotDesc.getSourceExprs();
        if (sourceExprs.isEmpty()) {
            // slot should correspond to a materialized base table column
            Preconditions.checkState(slotDesc.getParent().isMaterialized() && slotDesc.getParent().getParentTable() != null && slotDesc.getColumn() != null);
            String colName = slotDesc.getParent().getTableName() + "." + slotDesc.getColumn().getName();
            sourceBaseCols.add(colName);
        } else {
            for (Expr sourceExpr : sourceExprs) {
                getSourceBaseCols(sourceExpr, sourceBaseCols, directPredDeps, traversePredDeps);
            }
        }
    }
}
#method_after
private void getSourceBaseCols(Expr expr, Set<String> sourceBaseCols, List<Expr> directPredDeps, boolean traversePredDeps) {
    List<Expr> exprsToTraverse = getProjectionDeps(expr);
    List<Expr> predicateDepExprs = getPredicateDeps(expr);
    if (directPredDeps != null)
        directPredDeps.addAll(predicateDepExprs);
    if (traversePredDeps)
        exprsToTraverse.addAll(predicateDepExprs);
    List<SlotId> slotIds = Lists.newArrayList();
    for (Expr e : exprsToTraverse) {
        e.getIds(null, slotIds);
    }
    for (SlotId slotId : slotIds) {
        SlotDescriptor slotDesc = descTbl_.getSlotDesc(slotId);
        List<Expr> sourceExprs = slotDesc.getSourceExprs();
        if (sourceExprs.isEmpty()) {
            // slot should correspond to a materialized base table column
            Preconditions.checkState(slotDesc.getParent().isMaterialized() && slotDesc.getParent().getTable() != null && slotDesc.getColumn() != null);
            String colName = slotDesc.getParent().getTableName() + "." + slotDesc.getColumn().getName();
            sourceBaseCols.add(colName);
        } else {
            for (Expr sourceExpr : sourceExprs) {
                getSourceBaseCols(sourceExpr, sourceBaseCols, directPredDeps, traversePredDeps);
            }
        }
    }
}
#end_block

#method_before
public TSlotDescriptor toThrift() {
    List<Integer> slotPath = Lists.newArrayList();
    ;
    if (path_ != null) {
        slotPath.addAll(path_);
    } else if (column_ != null) {
        slotPath.add(column_.getPosition());
    }
    TSlotDescriptor result = new TSlotDescriptor(id_.asInt(), parent_.getId().asInt(), type_.toThrift(), slotPath, byteOffset_, nullIndicatorByte_, nullIndicatorBit_, slotIdx_, isMaterialized_);
    return result;
}
#method_after
public TSlotDescriptor toThrift() {
    List<Integer> slotPath = getAbsolutePath();
    TSlotDescriptor result = new TSlotDescriptor(id_.asInt(), parent_.getId().asInt(), type_.toThrift(), slotPath, byteOffset_, nullIndicatorByte_, nullIndicatorBit_, slotIdx_, isMaterialized_);
    return result;
}
#end_block

#method_before
private void testChildTableRefs(String childTable, String childColumn, boolean testInlineView) {
    TableName tbl = new TableName("functional", "allcomplextypes");
    // Child table uses unqualified implicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL, allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    // Child table uses fully qualified implicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL, functional.allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    // Child table uses explicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL a, a.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s a, a.%s", childColumn, tbl.toSql(), childTable));
    if (testInlineView) {
        // Parent table is an inline view.
        TblsTestToSql(String.format("select %s from (select %s from $TBL) a, a.%s", childColumn, childTable, childTable), tbl, String.format("SELECT %s FROM (SELECT %s FROM %s) a, a.%s", childColumn, childTable, tbl.toSql(), childTable));
    }
    // Parent/child/child join.
    TblsTestToSql(String.format("select b.%s from $TBL a, a.%s b, a.int_map_col c", childColumn, childTable), tbl, String.format("SELECT b.%s FROM %s a, a.%s b, a.int_map_col c", childColumn, tbl.toSql(), childTable));
    TblsTestToSql(String.format("select c.%s from $TBL a, a.int_array_col b, a.%s c", childColumn, childTable), tbl, String.format("SELECT c.%s FROM %s a, a.int_array_col b, a.%s c", childColumn, tbl.toSql(), childTable));
    // Test join types. Parent/child joins do not require an ON or USING clause.
    for (String joinType : joinTypes_) {
        TblsTestToSql(String.format("select 1 from $TBL %s allcomplextypes.%s", joinType, childTable), tbl, String.format("SELECT 1 FROM %s %s functional.allcomplextypes.%s", tbl.toSql(), joinType, childTable));
        TblsTestToSql(String.format("select 1 from $TBL a %s a.%s", joinType, childTable), tbl, String.format("SELECT 1 FROM %s a %s a.%s", tbl.toSql(), joinType, childTable));
    }
    // Legal, but not a parent/child join.
    TblsTestToSql(String.format("select %s from $TBL a, functional.allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s a, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    TblsTestToSql(String.format("select %s from $TBL.%s, functional.allcomplextypes", childColumn, childTable), tbl, String.format("SELECT %s FROM %s.%s, functional.allcomplextypes", childColumn, tbl.toSql(), childTable));
}
#method_after
private void testChildTableRefs(String childTable, String childColumn) {
    TableName tbl = new TableName("functional", "allcomplextypes");
    // Child table uses unqualified implicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL, allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    // Child table uses fully qualified implicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL, functional.allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    // Child table uses explicit alias of parent table.
    TblsTestToSql(String.format("select %s from $TBL a, a.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s a, a.%s", childColumn, tbl.toSql(), childTable));
    // Parent/child/child join.
    TblsTestToSql(String.format("select b.%s from $TBL a, a.%s b, a.int_map_col c", childColumn, childTable), tbl, String.format("SELECT b.%s FROM %s a, a.%s b, a.int_map_col c", childColumn, tbl.toSql(), childTable));
    TblsTestToSql(String.format("select c.%s from $TBL a, a.int_array_col b, a.%s c", childColumn, childTable), tbl, String.format("SELECT c.%s FROM %s a, a.int_array_col b, a.%s c", childColumn, tbl.toSql(), childTable));
    // Test join types. Parent/child joins do not require an ON or USING clause.
    for (String joinType : joinTypes_) {
        TblsTestToSql(String.format("select 1 from $TBL %s allcomplextypes.%s", joinType, childTable), tbl, String.format("SELECT 1 FROM %s %s functional.allcomplextypes.%s", tbl.toSql(), joinType, childTable));
        TblsTestToSql(String.format("select 1 from $TBL a %s a.%s", joinType, childTable), tbl, String.format("SELECT 1 FROM %s a %s a.%s", tbl.toSql(), joinType, childTable));
    }
    // Legal, but not a parent/child join.
    TblsTestToSql(String.format("select %s from $TBL a, functional.allcomplextypes.%s", childColumn, childTable), tbl, String.format("SELECT %s FROM %s a, functional.allcomplextypes.%s", childColumn, tbl.toSql(), childTable));
    TblsTestToSql(String.format("select %s from $TBL.%s, functional.allcomplextypes", childColumn, childTable), tbl, String.format("SELECT %s FROM %s.%s, functional.allcomplextypes", childColumn, tbl.toSql(), childTable));
}
#end_block

#method_before
@Test
public void TestStructFields() throws AnalysisException {
    String[] tables = new String[] { "allcomplextypes", "allcomplextypes_view" };
    String[] columns = new String[] { "id", "int_struct_col.f1", "nested_struct_col.f2.f12.f21" };
    testAllTableAliases(tables, columns);
}
#method_after
@Test
public void TestStructFields() throws AnalysisException {
    String[] tables = new String[] { "allcomplextypes" };
    String[] columns = new String[] { "id", "int_struct_col.f1", "nested_struct_col.f2.f12.f21" };
    testAllTableAliases(tables, columns);
}
#end_block

#method_before
@Test
public void TestCollectionTableRefs() throws AnalysisException {
    // Test ARRAY type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_array_col", "allcomplextypes_view.int_array_col" }, new String[] { "item", "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_array_col", "allcomplextypes_view.struct_array_col" }, new String[] { "f1", "f2", "*" });
    // Test MAP type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_map_col", "allcomplextypes_view.int_map_col" }, new String[] { "key", "value", "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_map_col", "allcomplextypes_view.struct_map_col" }, new String[] { "key", "f1", "f2", "*" });
    // Test complex table ref path with structs and multiple collections.
    testAllTableAliases(new String[] { "allcomplextypes.complex_nested_struct_col.f2.f12", "allcomplextypes_view.complex_nested_struct_col.f2.f12" }, new String[] { "key", "f21", "*" });
    // Test toSql() of child table refs.
    testChildTableRefs("int_array_col", "item", true);
    testChildTableRefs("int_map_col", "key", true);
    testChildTableRefs("complex_nested_struct_col.f2.f12", "f21", false);
}
#method_after
@Test
public void TestCollectionTableRefs() throws AnalysisException {
    // Test ARRAY type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_array_col" }, new String[] { Path.ARRAY_ITEM_FIELD_NAME, "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_array_col" }, new String[] { "f1", "f2", "*" });
    // Test MAP type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_map_col" }, new String[] { Path.MAP_KEY_FIELD_NAME, Path.MAP_VALUE_FIELD_NAME, "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_map_col" }, new String[] { Path.MAP_KEY_FIELD_NAME, "f1", "f2", "*" });
    // Test complex table ref path with structs and multiple collections.
    testAllTableAliases(new String[] { "allcomplextypes.complex_nested_struct_col.f2.f12" }, new String[] { Path.MAP_KEY_FIELD_NAME, "f21", "*" });
    // Test toSql() of child table refs.
    testChildTableRefs("int_array_col", Path.ARRAY_ITEM_FIELD_NAME);
    testChildTableRefs("int_map_col", Path.MAP_KEY_FIELD_NAME);
    testChildTableRefs("complex_nested_struct_col.f2.f12", "f21");
}
#end_block

#method_before
@Test
public void TestDecimalCasts() throws AnalysisException {
    AnalyzesOk("select cast(1.1 as boolean)");
    AnalyzesOk("select cast(1.1 as timestamp)");
    AnalysisError("select cast(true as decimal)", "Invalid type cast of TRUE from BOOLEAN to DECIMAL(9,0)");
    AnalysisError("select cast(cast(1 as timestamp) as decimal)", "Invalid type cast of CAST(1 AS TIMESTAMP) from TIMESTAMP to DECIMAL(9,0)");
    for (Type type : Type.getSupportedTypes()) {
        if (type.isNull() || type.isDecimal() || type.isBoolean() || type.isDateType() || type.getPrimitiveType() == PrimitiveType.VARCHAR || type.getPrimitiveType() == PrimitiveType.CHAR) {
            continue;
        }
        AnalyzesOk("select cast(1.1 as " + type + ")");
        AnalyzesOk("select cast(cast(1 as " + type + ") as decimal)");
    }
    // Casts to all other decimals are supported.
    for (int precision = 1; precision <= ScalarType.MAX_PRECISION; ++precision) {
        for (int scale = 0; scale < precision; ++scale) {
            Type t = ScalarType.createDecimalType(precision, scale);
            AnalyzesOk("select cast(1.1 as " + t.toSql() + ")");
            AnalyzesOk("select cast(cast(1 as " + t.toSql() + ") as decimal)");
        }
    }
    AnalysisError("select cast(1 as decimal(0, 1))", "Decimal precision must be greater than 0.");
}
#method_after
@Test
public void TestDecimalCasts() throws AnalysisException {
    AnalyzesOk("select cast(1.1 as boolean)");
    AnalyzesOk("select cast(1.1 as timestamp)");
    AnalysisError("select cast(true as decimal)", "Invalid type cast of TRUE from BOOLEAN to DECIMAL(9,0)");
    AnalysisError("select cast(cast(1 as timestamp) as decimal)", "Invalid type cast of CAST(1 AS TIMESTAMP) from TIMESTAMP to DECIMAL(9,0)");
    for (Type type : Type.getSupportedTypes()) {
        if (type.isNull() || type.isDecimal() || type.isBoolean() || type.isDateType() || type.getPrimitiveType() == PrimitiveType.VARCHAR || type.getPrimitiveType() == PrimitiveType.CHAR) {
            continue;
        }
        AnalyzesOk("select cast(1.1 as " + type + ")");
        AnalyzesOk("select cast(cast(1 as " + type + ") as decimal)");
    }
    // Casts to all other decimals are supported.
    for (int precision = 1; precision <= ScalarType.MAX_PRECISION; ++precision) {
        for (int scale = 0; scale < precision; ++scale) {
            Type t = ScalarType.createDecimalType(precision, scale);
            AnalyzesOk("select cast(1.1 as " + t.toSql() + ")");
            AnalyzesOk("select cast(cast(1 as " + t.toSql() + ") as decimal)");
        }
    }
    AnalysisError("select cast(1 as decimal(0, 1))", "Decimal precision must be > 0: 0");
}
#end_block

#method_before
@Test
public void TestStringCasts() throws AnalysisException {
    // No implicit cast from STRING to numeric and boolean
    AnalysisError("select * from functional.alltypes where tinyint_col = '1'", "operands of type TINYINT and STRING are not comparable: tinyint_col = '1'");
    AnalysisError("select * from functional.alltypes where bool_col = '0'", "operands of type BOOLEAN and STRING are not comparable: bool_col = '0'");
    // No explicit cast from STRING to boolean.
    AnalysisError("select cast('false' as boolean) from functional.alltypes", "Invalid type cast of 'false' from STRING to BOOLEAN");
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('0.5' as float)");
    AnalyzesOk("select * from functional.alltypes where " + "smallint_col = cast('0.5' as float)");
    AnalyzesOk("select * from functional.alltypes where int_col = cast('0.5' as float)");
    AnalyzesOk("select * from functional.alltypes where " + "bigint_col = cast('0.5' as float)");
    AnalyzesOk("select 1.0 = cast('" + Double.toString(Double.MIN_VALUE) + "' as double)");
    AnalyzesOk("select 1.0 = cast('-" + Double.toString(Double.MIN_VALUE) + "' as double)");
    AnalyzesOk("select 1.0 = cast('" + Double.toString(Double.MAX_VALUE) + "' as double)");
    AnalyzesOk("select 1.0 = cast('-" + Double.toString(Double.MAX_VALUE) + "' as double)");
    // Test chains of minus. Note that "--" is the a comment symbol.
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('-1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('- -1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('- - -1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('- - - -1' as tinyint)");
    // Test correct casting to compatible type on bitwise ops.
    AnalyzesOk("select 1 | cast('" + Byte.toString(Byte.MIN_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Byte.toString(Byte.MAX_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Short.toString(Short.MIN_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Short.toString(Short.MAX_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Integer.toString(Integer.MIN_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Integer.toString(Integer.MAX_VALUE) + "' as int)");
    // We need to add 1 to MIN_VALUE because there are no negative integer literals.
    // The reason is that whether a minus belongs to an
    // arithmetic expr or a literal must be decided by the parser, not the lexer.
    AnalyzesOk("select 1 | cast('" + Long.toString(Long.MIN_VALUE + 1) + "' as bigint)");
    AnalyzesOk("select 1 | cast('" + Long.toString(Long.MAX_VALUE) + "' as bigint)");
    // Cast to numeric never overflow
    AnalyzesOk("select * from functional.alltypes where tinyint_col = " + "cast('" + Long.toString(Long.MIN_VALUE) + "1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where tinyint_col = " + "cast('" + Long.toString(Long.MAX_VALUE) + "1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where tinyint_col = " + "cast('" + Double.toString(Double.MAX_VALUE) + "1' as tinyint)");
    // Java converts a float underflow to 0.0.
    // Since there is no easy, reliable way to detect underflow,
    // we don't consider it an error.
    AnalyzesOk("select * from functional.alltypes where tinyint_col = " + "cast('" + Double.toString(Double.MIN_VALUE) + "1' as tinyint)");
    // Cast never raise analysis exception
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('--1' as tinyint)");
    // Cast string literal to string
    AnalyzesOk("select cast('abc' as string)");
    // Cast decimal to string
    AnalyzesOk("select cast(cast('1.234' as decimal) as string)");
    // Cast to / from VARCHAR
    AnalyzesOk("select cast('helloworld' as VARCHAR(3))");
    AnalyzesOk("select cast(cast('helloworld' as VARCHAR(3)) as string)");
    AnalyzesOk("select cast(cast('3.0' as VARCHAR(5)) as float)");
    AnalyzesOk("select NULL = cast('123' as CHAR(3))");
    AnalysisError("select now() = cast('hi' as CHAR(3))", "operands of type TIMESTAMP and CHAR(3) are not comparable: " + "now() = CAST('hi' AS CHAR(3))");
    testExprCast("cast('Hello' as VARCHAR(5))", ScalarType.createVarcharType(7));
    testExprCast("cast('Hello' as VARCHAR(5))", ScalarType.createVarcharType(3));
    AnalysisError("select cast('foo' as varchar(0))", "Varchar size must be > 0. Size is too small: 0.");
    AnalysisError("select cast('foo' as varchar(65356))", "Varchar size must be <= 65355. Size is too large: 65356.");
    AnalysisError("select cast('foo' as char(0))", "Char size must be > 0. Size is too small: 0.");
    AnalysisError("select cast('foo' as char(256))", "Char size must be <= 255. Size is too large: 256.");
    testExprCast("'Hello'", ScalarType.createCharType(5));
    testExprCast("cast('Hello' as CHAR(5))", ScalarType.STRING);
    testExprCast("cast('Hello' as CHAR(5))", ScalarType.createVarcharType(7));
    testExprCast("cast('Hello' as VARCHAR(5))", ScalarType.createCharType(7));
    testExprCast("cast('Hello' as CHAR(7))", ScalarType.createVarcharType(5));
    testExprCast("cast('Hello' as VARCHAR(7))", ScalarType.createCharType(5));
    testExprCast("cast('Hello' as CHAR(5))", ScalarType.createVarcharType(5));
    testExprCast("cast('Hello' as VARCHAR(5))", ScalarType.createCharType(5));
    testExprCast("1", ScalarType.createCharType(5));
    testExprCast("cast('abcde' as char(10)) IN " + "(cast('abcde' as CHAR(20)), cast('abcde' as VARCHAR(10)), 'abcde')", ScalarType.createCharType(10));
    testExprCast("'abcde' IN " + "(cast('abcde' as CHAR(20)), cast('abcde' as VARCHAR(10)), 'abcde')", ScalarType.STRING);
    testExprCast("cast('abcde' as varchar(10)) IN " + "(cast('abcde' as CHAR(20)), cast('abcde' as VARCHAR(10)), 'abcde')", ScalarType.createVarcharType(10));
}
#method_after
@Test
public void TestStringCasts() throws AnalysisException {
    // No implicit cast from STRING to numeric and boolean
    AnalysisError("select * from functional.alltypes where tinyint_col = '1'", "operands of type TINYINT and STRING are not comparable: tinyint_col = '1'");
    AnalysisError("select * from functional.alltypes where bool_col = '0'", "operands of type BOOLEAN and STRING are not comparable: bool_col = '0'");
    // No explicit cast from STRING to boolean.
    AnalysisError("select cast('false' as boolean) from functional.alltypes", "Invalid type cast of 'false' from STRING to BOOLEAN");
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('0.5' as float)");
    AnalyzesOk("select * from functional.alltypes where " + "smallint_col = cast('0.5' as float)");
    AnalyzesOk("select * from functional.alltypes where int_col = cast('0.5' as float)");
    AnalyzesOk("select * from functional.alltypes where " + "bigint_col = cast('0.5' as float)");
    AnalyzesOk("select 1.0 = cast('" + Double.toString(Double.MIN_VALUE) + "' as double)");
    AnalyzesOk("select 1.0 = cast('-" + Double.toString(Double.MIN_VALUE) + "' as double)");
    AnalyzesOk("select 1.0 = cast('" + Double.toString(Double.MAX_VALUE) + "' as double)");
    AnalyzesOk("select 1.0 = cast('-" + Double.toString(Double.MAX_VALUE) + "' as double)");
    // Test chains of minus. Note that "--" is the a comment symbol.
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('-1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('- -1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('- - -1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('- - - -1' as tinyint)");
    // Test correct casting to compatible type on bitwise ops.
    AnalyzesOk("select 1 | cast('" + Byte.toString(Byte.MIN_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Byte.toString(Byte.MAX_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Short.toString(Short.MIN_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Short.toString(Short.MAX_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Integer.toString(Integer.MIN_VALUE) + "' as int)");
    AnalyzesOk("select 1 | cast('" + Integer.toString(Integer.MAX_VALUE) + "' as int)");
    // We need to add 1 to MIN_VALUE because there are no negative integer literals.
    // The reason is that whether a minus belongs to an
    // arithmetic expr or a literal must be decided by the parser, not the lexer.
    AnalyzesOk("select 1 | cast('" + Long.toString(Long.MIN_VALUE + 1) + "' as bigint)");
    AnalyzesOk("select 1 | cast('" + Long.toString(Long.MAX_VALUE) + "' as bigint)");
    // Cast to numeric never overflow
    AnalyzesOk("select * from functional.alltypes where tinyint_col = " + "cast('" + Long.toString(Long.MIN_VALUE) + "1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where tinyint_col = " + "cast('" + Long.toString(Long.MAX_VALUE) + "1' as tinyint)");
    AnalyzesOk("select * from functional.alltypes where tinyint_col = " + "cast('" + Double.toString(Double.MAX_VALUE) + "1' as tinyint)");
    // Java converts a float underflow to 0.0.
    // Since there is no easy, reliable way to detect underflow,
    // we don't consider it an error.
    AnalyzesOk("select * from functional.alltypes where tinyint_col = " + "cast('" + Double.toString(Double.MIN_VALUE) + "1' as tinyint)");
    // Cast never raise analysis exception
    AnalyzesOk("select * from functional.alltypes where " + "tinyint_col = cast('--1' as tinyint)");
    // Cast string literal to string
    AnalyzesOk("select cast('abc' as string)");
    // Cast decimal to string
    AnalyzesOk("select cast(cast('1.234' as decimal) as string)");
    // Cast to / from VARCHAR
    AnalyzesOk("select cast('helloworld' as VARCHAR(3))");
    AnalyzesOk("select cast(cast('helloworld' as VARCHAR(3)) as string)");
    AnalyzesOk("select cast(cast('3.0' as VARCHAR(5)) as float)");
    AnalyzesOk("select NULL = cast('123' as CHAR(3))");
    AnalysisError("select now() = cast('hi' as CHAR(3))", "operands of type TIMESTAMP and CHAR(3) are not comparable: " + "now() = CAST('hi' AS CHAR(3))");
    testExprCast("cast('Hello' as VARCHAR(5))", ScalarType.createVarcharType(7));
    testExprCast("cast('Hello' as VARCHAR(5))", ScalarType.createVarcharType(3));
    AnalysisError("select cast('foo' as varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("select cast('foo' as varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("select cast('foo' as char(0))", "Char size must be > 0: 0");
    AnalysisError("select cast('foo' as char(256))", "Char size must be <= 255: 256");
    testExprCast("'Hello'", ScalarType.createCharType(5));
    testExprCast("cast('Hello' as CHAR(5))", ScalarType.STRING);
    testExprCast("cast('Hello' as CHAR(5))", ScalarType.createVarcharType(7));
    testExprCast("cast('Hello' as VARCHAR(5))", ScalarType.createCharType(7));
    testExprCast("cast('Hello' as CHAR(7))", ScalarType.createVarcharType(5));
    testExprCast("cast('Hello' as VARCHAR(7))", ScalarType.createCharType(5));
    testExprCast("cast('Hello' as CHAR(5))", ScalarType.createVarcharType(5));
    testExprCast("cast('Hello' as VARCHAR(5))", ScalarType.createCharType(5));
    testExprCast("1", ScalarType.createCharType(5));
    testExprCast("cast('abcde' as char(10)) IN " + "(cast('abcde' as CHAR(20)), cast('abcde' as VARCHAR(10)), 'abcde')", ScalarType.createCharType(10));
    testExprCast("'abcde' IN " + "(cast('abcde' as CHAR(20)), cast('abcde' as VARCHAR(10)), 'abcde')", ScalarType.STRING);
    testExprCast("cast('abcde' as varchar(10)) IN " + "(cast('abcde' as CHAR(20)), cast('abcde' as VARCHAR(10)), 'abcde')", ScalarType.createVarcharType(10));
}
#end_block

#method_before
@Test
public void TestInPredicates() throws AnalysisException {
    AnalyzesOk("select * from functional.alltypes where int_col in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where int_col not in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where " + "string_col in ('a', 'b', 'c', 'd')");
    AnalyzesOk("select * from functional.alltypes where " + "string_col not in ('a', 'b', 'c', 'd')");
    // Test booleans.
    AnalyzesOk("select * from functional.alltypes where " + "true in (bool_col, true and false)");
    AnalyzesOk("select * from functional.alltypes where " + "true not in (bool_col, true and false)");
    // In list requires implicit casts.
    AnalyzesOk("select * from functional.alltypes where " + "double_col in (int_col, bigint_col)");
    // Comparison expr requires implicit cast.
    AnalyzesOk("select * from functional.alltypes where " + "int_col in (double_col, bigint_col)");
    // Test predicates.
    AnalyzesOk("select * from functional.alltypes where " + "!true in (false or true, true and false)");
    // Test NULLs.
    AnalyzesOk("select * from functional.alltypes where " + "NULL in (NULL, NULL)");
    // Incompatible types.
    AnalysisError("select * from functional.alltypes where " + "string_col in (bool_col, double_col)", "Incompatible return types 'STRING' and 'BOOLEAN' " + "of exprs 'string_col' and 'bool_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (int_col, double_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (NULL, int_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select 1 from functional.allcomplextypes where " + "int_array_col in (id, NULL)", "Incompatible return types 'ARRAY<INT>' and 'INT' " + "of exprs 'int_array_col' and 'id'.");
}
#method_after
@Test
public void TestInPredicates() throws AnalysisException {
    AnalyzesOk("select * from functional.alltypes where int_col in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where int_col not in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where " + "string_col in ('a', 'b', 'c', 'd')");
    AnalyzesOk("select * from functional.alltypes where " + "string_col not in ('a', 'b', 'c', 'd')");
    // Test booleans.
    AnalyzesOk("select * from functional.alltypes where " + "true in (bool_col, true and false)");
    AnalyzesOk("select * from functional.alltypes where " + "true not in (bool_col, true and false)");
    // In list requires implicit casts.
    AnalyzesOk("select * from functional.alltypes where " + "double_col in (int_col, bigint_col)");
    // Comparison expr requires implicit cast.
    AnalyzesOk("select * from functional.alltypes where " + "int_col in (double_col, bigint_col)");
    // Test predicates.
    AnalyzesOk("select * from functional.alltypes where " + "!true in (false or true, true and false)");
    // Test NULLs.
    AnalyzesOk("select * from functional.alltypes where " + "NULL in (NULL, NULL)");
    // Test IN in binary predicates
    AnalyzesOk("select bool_col = (int_col in (1,2)), " + "case when tinyint_col in (10, NULL) then tinyint_col else NULL end " + "from functional.alltypestiny where int_col > (bool_col in (false)) " + "and (int_col in (1,2)) = (select min(bool_col) from functional.alltypes) " + "and (int_col in (3,4)) = (tinyint_col in (4,5))");
    // Incompatible types.
    AnalysisError("select * from functional.alltypes where " + "string_col in (bool_col, double_col)", "Incompatible return types 'STRING' and 'BOOLEAN' " + "of exprs 'string_col' and 'bool_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (int_col, double_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (NULL, int_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select 1 from functional.allcomplextypes where " + "int_array_col in (id, NULL)", "Incompatible return types 'ARRAY<INT>' and 'INT' " + "of exprs 'int_array_col' and 'id'.");
}
#end_block

#method_before
@Test
public void TestDecimalCast() throws AnalysisException {
    AnalyzesOk("select cast(1 as decimal)");
    AnalyzesOk("select cast(1 as decimal(1))");
    AnalyzesOk("select cast(1 as decimal(38))");
    AnalyzesOk("select cast(1 as decimal(1, 0))");
    AnalyzesOk("select cast(1 as decimal(10, 5))");
    AnalyzesOk("select cast(1 as decimal(38, 0))");
    AnalyzesOk("select cast(1 as decimal(38, 38))");
    AnalysisError("select cast(1 as decimal(0))", "Decimal precision must be greater than 0.");
    AnalysisError("select cast(1 as decimal(39))", "Decimal precision must be <= 38.");
    AnalysisError("select cast(1 as decimal(1, 2))", "Decimal scale (2) must be <= precision (1).");
}
#method_after
@Test
public void TestDecimalCast() throws AnalysisException {
    AnalyzesOk("select cast(1 as decimal)");
    AnalyzesOk("select cast(1 as decimal(1))");
    AnalyzesOk("select cast(1 as decimal(38))");
    AnalyzesOk("select cast(1 as decimal(1, 0))");
    AnalyzesOk("select cast(1 as decimal(10, 5))");
    AnalyzesOk("select cast(1 as decimal(38, 0))");
    AnalyzesOk("select cast(1 as decimal(38, 38))");
    AnalysisError("select cast(1 as decimal(0))", "Decimal precision must be > 0: 0");
    AnalysisError("select cast(1 as decimal(39))", "Decimal precision must be <= 38: 39");
    AnalysisError("select cast(1 as decimal(1, 2))", "Decimal scale (2) must be <= precision (1)");
}
#end_block

#method_before
// Provide better error message for some aggregate builtins. These can be
// a bit more user friendly than a generic function not found.
// TODO: should we bother to do this? We could also improve the general
protected String getFunctionNotFoundError(Type[] argTypes) {
    if (fnName_.isBuiltin_) {
        // Some custom error message for builtins
        if (params_.isStar()) {
            return "'*' can only be used in conjunction with COUNT";
        }
        if (fnName_.getFunction().equalsIgnoreCase("count")) {
            if (!params_.isDistinct() && argTypes.length > 1) {
                return "COUNT must have DISTINCT for multiple arguments: " + toSql();
            }
        }
        if (fnName_.getFunction().equalsIgnoreCase("sum")) {
            return "SUM requires a numeric parameter: " + toSql();
        }
        if (fnName_.getFunction().equalsIgnoreCase("avg")) {
            return "AVG requires a numeric or timestamp parameter: " + toSql();
        }
    }
    String[] argTypesSql = new String[argTypes.length];
    for (int i = 0; i < argTypes.length; ++i) {
        argTypesSql[i] = argTypes[i].toSql();
    }
    return String.format("No matching function with signature: %s(%s).", fnName_, params_.isStar() ? "*" : Joiner.on(", ").join(argTypesSql));
}
#method_after
// Provide better error message for some aggregate builtins. These can be
// a bit more user friendly than a generic function not found.
// TODO: should we bother to do this? We could also improve the general
protected String getFunctionNotFoundError(Type[] argTypes) {
    if (fnName_.isBuiltin()) {
        // Some custom error message for builtins
        if (params_.isStar()) {
            return "'*' can only be used in conjunction with COUNT";
        }
        if (fnName_.getFunction().equalsIgnoreCase("count")) {
            if (!params_.isDistinct() && argTypes.length > 1) {
                return "COUNT must have DISTINCT for multiple arguments: " + toSql();
            }
        }
        if (fnName_.getFunction().equalsIgnoreCase("sum")) {
            return "SUM requires a numeric parameter: " + toSql();
        }
        if (fnName_.getFunction().equalsIgnoreCase("avg")) {
            return "AVG requires a numeric or timestamp parameter: " + toSql();
        }
    }
    String[] argTypesSql = new String[argTypes.length];
    for (int i = 0; i < argTypes.length; ++i) {
        argTypesSql[i] = argTypes[i].toSql();
    }
    return String.format("No matching function with signature: %s(%s).", fnName_, params_.isStar() ? "*" : Joiner.on(", ").join(argTypesSql));
}
#end_block

#method_before
public TableName getTableName() {
    if (parentTable_ == null)
        return null;
    return new TableName(parentTable_.getDb() != null ? parentTable_.getDb().getName() : null, parentTable_.getName());
}
#method_after
public TableName getTableName() {
    Table t = getTable();
    return (t == null) ? null : t.getTableName();
}
#end_block

#method_before
public void setPath(List<Integer> path) {
    path_ = path;
}
#method_after
public void setPath(Path p) {
    Preconditions.checkNotNull(p);
    Preconditions.checkState(p.isResolved());
    Preconditions.checkState(p.destType().isCollectionType());
    path_ = p;
    if (p.destTable() != null) {
        // Do not use Path.getTypeAsStruct() to only allow implicit path resolutions,
        // because this tuple desc belongs to a base table ref.
        type_ = (StructType) p.destTable().getType().getItemType();
    } else {
        // Also allow explicit path resolutions.
        type_ = Path.getTypeAsStruct(p.destType());
    }
}
#end_block

#method_before
public List<Integer> getPath() {
    return path_;
}
#method_after
public Path getPath() {
    return path_;
}
#end_block

#method_before
public String debugString() {
    String tblStr = (parentTable_ == null ? "null" : parentTable_.getFullName());
    List<String> slotStrings = Lists.newArrayList();
    for (SlotDescriptor slot : slots_) {
        slotStrings.add(slot.debugString());
    }
    return Objects.toStringHelper(this).add("id", id_.asInt()).add("name", debugName_).add("tbl", tblStr).add("byte_size", byteSize_).add("is_materialized", isMaterialized_).add("slots", "[" + Joiner.on(", ").join(slotStrings) + "]").toString();
}
#method_after
public String debugString() {
    String tblStr = (getTable() == null ? "null" : getTable().getFullName());
    List<String> slotStrings = Lists.newArrayList();
    for (SlotDescriptor slot : slots_) {
        slotStrings.add(slot.debugString());
    }
    return Objects.toStringHelper(this).add("id", id_.asInt()).add("name", debugName_).add("tbl", tblStr).add("byte_size", byteSize_).add("is_materialized", isMaterialized_).add("slots", "[" + Joiner.on(", ").join(slotStrings) + "]").toString();
}
#end_block

#method_before
public TTupleDescriptor toThrift() {
    TTupleDescriptor ttupleDesc = new TTupleDescriptor(id_.asInt(), byteSize_, numNullBytes_);
    // do not set the table id for views
    if (parentTable_ != null && !(parentTable_ instanceof View)) {
        ttupleDesc.setTableId(parentTable_.getId().asInt());
    }
    return ttupleDesc;
}
#method_after
public TTupleDescriptor toThrift() {
    TTupleDescriptor ttupleDesc = new TTupleDescriptor(id_.asInt(), byteSize_, numNullBytes_);
    // do not set the table id for views
    if (getTable() != null && !(getTable() instanceof View)) {
        ttupleDesc.setTableId(getTable().getId().asInt());
    }
    return ttupleDesc;
}
#end_block

#method_before
public static String getCreateTableSql(CreateTableStmt stmt) {
    ArrayList<String> colsSql = Lists.newArrayList();
    for (ColumnDesc col : stmt.getColumnDefs()) {
        colsSql.add(col.toString());
    }
    ArrayList<String> partitionColsSql = Lists.newArrayList();
    for (ColumnDesc col : stmt.getPartitionColumnDefs()) {
        partitionColsSql.add(col.toString());
    }
    // TODO: Pass the correct compression, if applicable.
    return getCreateTableSql(stmt.getDb(), stmt.getTbl(), stmt.getComment(), colsSql, partitionColsSql, stmt.getTblProperties(), stmt.getSerdeProperties(), stmt.isExternal(), stmt.getIfNotExists(), stmt.getRowFormat(), HdfsFileFormat.fromThrift(stmt.getFileFormat()), HdfsCompression.NONE, null, stmt.getLocation().toString());
}
#method_after
public static String getCreateTableSql(CreateTableStmt stmt) {
    ArrayList<String> colsSql = Lists.newArrayList();
    for (ColumnDef col : stmt.getColumnDefs()) {
        colsSql.add(col.toString());
    }
    ArrayList<String> partitionColsSql = Lists.newArrayList();
    for (ColumnDef col : stmt.getPartitionColumnDefs()) {
        partitionColsSql.add(col.toString());
    }
    // TODO: Pass the correct compression, if applicable.
    return getCreateTableSql(stmt.getDb(), stmt.getTbl(), stmt.getComment(), colsSql, partitionColsSql, stmt.getTblProperties(), stmt.getSerdeProperties(), stmt.isExternal(), stmt.getIfNotExists(), stmt.getRowFormat(), HdfsFileFormat.fromThrift(stmt.getFileFormat()), HdfsCompression.NONE, null, stmt.getLocation().toString());
}
#end_block

#method_before
public TDescriptorTable toThrift() {
    TDescriptorTable result = new TDescriptorTable();
    HashSet<Table> referencedTbls = Sets.newHashSet();
    HashSet<Table> allPartitionsTbls = Sets.newHashSet();
    for (TupleDescriptor tupleDesc : tupleDescs_.values()) {
        // in the descriptor table just for type checking, which we need to skip
        if (tupleDesc.isMaterialized()) {
            result.addToTupleDescriptors(tupleDesc.toThrift());
            Table table = tupleDesc.getParentTable();
            if (table != null && !(table instanceof View)) {
                referencedTbls.add(table);
            }
            for (SlotDescriptor slotD : tupleDesc.getSlots()) {
                result.addToSlotDescriptors(slotD.toThrift());
            }
        }
    }
    for (Table table : referencedTables_) {
        referencedTbls.add(table);
        // We don't know which partitions are needed for INSERT, so include them all.
        allPartitionsTbls.add(table);
    }
    for (Table tbl : referencedTbls) {
        // null means include all partitions.
        HashSet<Long> referencedPartitions = null;
        if (!allPartitionsTbls.contains(tbl)) {
            referencedPartitions = getReferencedPartitions(tbl);
        }
        result.addToTableDescriptors(tbl.toThriftDescriptor(referencedPartitions));
    }
    return result;
}
#method_after
public TDescriptorTable toThrift() {
    TDescriptorTable result = new TDescriptorTable();
    HashSet<Table> referencedTbls = Sets.newHashSet();
    HashSet<Table> allPartitionsTbls = Sets.newHashSet();
    for (TupleDescriptor tupleDesc : tupleDescs_.values()) {
        // in the descriptor table just for type checking, which we need to skip
        if (tupleDesc.isMaterialized()) {
            result.addToTupleDescriptors(tupleDesc.toThrift());
            Table table = tupleDesc.getTable();
            if (table != null && !(table instanceof View))
                referencedTbls.add(table);
            for (SlotDescriptor slotD : tupleDesc.getSlots()) {
                result.addToSlotDescriptors(slotD.toThrift());
            }
        }
    }
    for (Table table : referencedTables_) {
        referencedTbls.add(table);
        // We don't know which partitions are needed for INSERT, so include them all.
        allPartitionsTbls.add(table);
    }
    for (Table tbl : referencedTbls) {
        // null means include all partitions.
        HashSet<Long> referencedPartitions = null;
        if (!allPartitionsTbls.contains(tbl)) {
            referencedPartitions = getReferencedPartitions(tbl);
        }
        result.addToTableDescriptors(tbl.toThriftDescriptor(referencedPartitions));
    }
    return result;
}
#end_block

#method_before
public static Type parseColumnType(FieldSchema fs) {
    // Wrap the type string in a CREATE TABLE stmt and use Impala's Parser
    // to get the ColumnType.
    // Pick a table name that can't be used.
    String stmt = String.format("CREATE TABLE $DUMMY ($DUMMY %s)", fs.getType());
    SqlScanner input = new SqlScanner(new StringReader(stmt));
    SqlParser parser = new SqlParser(input);
    CreateTableStmt createTableStmt;
    try {
        Object o = parser.parse().value;
        if (!(o instanceof CreateTableStmt)) {
            // Should never get here.
            throw new IllegalStateException("Couldn't parse create table stmt.");
        }
        createTableStmt = (CreateTableStmt) o;
        if (createTableStmt.getColumnDefs().isEmpty()) {
            // Should never get here.
            throw new IllegalStateException("Invalid create table stmt.");
        }
    } catch (Exception e) {
        return null;
    }
    return createTableStmt.getColumnDefs().get(0).getType();
}
#method_after
public static Type parseColumnType(FieldSchema fs) {
    // Wrap the type string in a CREATE TABLE stmt and use Impala's Parser
    // to get the ColumnType.
    // Pick a table name that can't be used.
    String stmt = String.format("CREATE TABLE $DUMMY ($DUMMY %s)", fs.getType());
    SqlScanner input = new SqlScanner(new StringReader(stmt));
    SqlParser parser = new SqlParser(input);
    CreateTableStmt createTableStmt;
    try {
        Object o = parser.parse().value;
        if (!(o instanceof CreateTableStmt)) {
            // Should never get here.
            throw new IllegalStateException("Couldn't parse create table stmt.");
        }
        createTableStmt = (CreateTableStmt) o;
        if (createTableStmt.getColumnDefs().isEmpty()) {
            // Should never get here.
            throw new IllegalStateException("Invalid create table stmt.");
        }
    } catch (Exception e) {
        return null;
    }
    TypeDef typeDef = createTableStmt.getColumnDefs().get(0).getTypeDef();
    return typeDef.getType();
}
#end_block

#method_before
public TupleDescriptor registerTableRef(TableRef ref) throws AnalysisException {
    String uniqueAlias = ref.getUniqueAlias();
    if (aliasMap_.containsKey(uniqueAlias)) {
        throw new AnalysisException("Duplicate table alias: '" + uniqueAlias + "'");
    }
    // If ref has no explicit alias, then the unqualified and the fully-qualified table
    // names are legal implicit aliases. Column references against unqualified implicit
    // aliases can be ambiguous, therefore, we register such ambiguous aliases here.
    String unqualifiedAlias = null;
    String[] aliases = ref.getAliases();
    if (aliases.length > 1) {
        unqualifiedAlias = aliases[1];
        TupleDescriptor tupleDesc = aliasMap_.get(unqualifiedAlias);
        if (tupleDesc != null) {
            if (tupleDesc.hasExplicitAlias()) {
                throw new AnalysisException("Duplicate table alias: '" + unqualifiedAlias + "'");
            } else {
                ambiguousAliases_.add(unqualifiedAlias);
            }
        }
    }
    // aliases for a concrete (resolved) table ref.
    if (!(ref instanceof InlineViewRef)) {
        Preconditions.checkNotNull(ref.getResolvedPath());
        Type destType = ref.getResolvedPath().destType();
        if (!destType.isCollectionType()) {
            throw new AnalysisException(String.format("Illegal table reference to non-collection type: '%s'\n" + "Path resolved to type: %s", Joiner.on(".").join(ref.getPath()), destType.toSql()));
        }
    }
    // Delegate creation of the tuple descriptor to the concrete table ref.
    TupleDescriptor result = ref.createTupleDescriptor(this);
    result.setAliases(aliases, ref.hasExplicitAlias());
    // Register all legal aliases.
    for (String alias : aliases) {
        aliasMap_.put(alias, result);
    }
    tableRefMap_.put(result.getId(), ref);
    return result;
}
#method_after
public TupleDescriptor registerTableRef(TableRef ref) throws AnalysisException {
    String uniqueAlias = ref.getUniqueAlias();
    if (aliasMap_.containsKey(uniqueAlias)) {
        throw new AnalysisException("Duplicate table alias: '" + uniqueAlias + "'");
    }
    // If ref has no explicit alias, then the unqualified and the fully-qualified table
    // names are legal implicit aliases. Column references against unqualified implicit
    // aliases can be ambiguous, therefore, we register such ambiguous aliases here.
    String unqualifiedAlias = null;
    String[] aliases = ref.getAliases();
    if (aliases.length > 1) {
        unqualifiedAlias = aliases[1];
        TupleDescriptor tupleDesc = aliasMap_.get(unqualifiedAlias);
        if (tupleDesc != null) {
            if (tupleDesc.hasExplicitAlias()) {
                throw new AnalysisException("Duplicate table alias: '" + unqualifiedAlias + "'");
            } else {
                ambiguousAliases_.add(unqualifiedAlias);
            }
        }
    }
    // Delegate creation of the tuple descriptor to the concrete table ref.
    TupleDescriptor result = ref.createTupleDescriptor(this);
    result.setAliases(aliases, ref.hasExplicitAlias());
    // Register all legal aliases.
    for (String alias : aliases) {
        aliasMap_.put(alias, result);
    }
    tableRefMap_.put(result.getId(), ref);
    return result;
}
#end_block

#method_before
public Path resolvePath(List<String> rawPath, PathType pathType) throws AnalysisException, TableLoadingException {
    boolean resolveInAncestors = (pathType == PathType.SLOT_REF) ? isSubquery_ : true;
    return resolvePath(rawPath, pathType, resolveInAncestors);
}
#method_after
public Path resolvePath(List<String> rawPath, PathType pathType) throws AnalysisException, TableLoadingException {
    // We only allow correlated references in predicates of a subquery.
    boolean resolveInAncestors = (pathType == PathType.SLOT_REF) ? isSubquery_ : false;
    // Convert all path elements to lower case.
    ArrayList<String> lcRawPath = Lists.newArrayListWithCapacity(rawPath.size());
    for (String s : rawPath) lcRawPath.add(s.toLowerCase());
    return resolvePath(lcRawPath, pathType, resolveInAncestors);
}
#end_block

#method_before
private Path resolvePath(List<String> rawPath, PathType pathType, boolean resolveInAncestors) throws AnalysisException, TableLoadingException {
    // Convert all path elements to lower case.
    ArrayList<String> lcRawPath = Lists.newArrayListWithCapacity(rawPath.size());
    for (String s : rawPath) {
        lcRawPath.add(s.toLowerCase());
    }
    // List of all successful path resolutions. A path in this list may be illegal with
    // respect to the pathType.
    ArrayList<Path> resolutions = Lists.newArrayList();
    // Resolve explicit alias or implicit unqualified alias.
    TupleDescriptor rootDesc = getDescriptor(lcRawPath.get(0));
    if (rootDesc != null) {
        Path p = new Path(rootDesc, lcRawPath.subList(1, lcRawPath.size()));
        if (p.resolve())
            resolutions.add(p);
    }
    // Resolve implicit qualified alias.
    if (lcRawPath.size() > 1) {
        rootDesc = getDescriptor(lcRawPath.get(0) + "." + lcRawPath.get(1));
        if (rootDesc != null) {
            Path p = new Path(rootDesc, lcRawPath.subList(2, lcRawPath.size()));
            if (p.resolve())
                resolutions.add(p);
        }
    }
    if (pathType == PathType.SLOT_REF || pathType == PathType.STAR) {
        // Resolve path in the context of all of the registered tuple descriptors.
        for (Map.Entry<String, TupleDescriptor> entry : aliasMap_.entrySet()) {
            Path p = new Path(entry.getValue(), lcRawPath);
            if (p.resolve())
                resolutions.add(p);
        }
    }
    Path result = getResultPath(rawPath, resolutions, pathType);
    if (result != null)
        return result;
    Path errPath = getErrorPath(resolutions, pathType);
    if (errPath != null)
        return errPath;
    if (pathType == PathType.SLOT_REF || pathType == PathType.STAR) {
        if (resolveInAncestors && hasAncestors()) {
            return getParentAnalyzer().resolvePath(rawPath, pathType, resolveInAncestors);
        } else {
            return null;
        }
    }
    Preconditions.checkState(pathType == PathType.TABLE_REF);
    resolutions.clear();
    // Resolve catalog table with an unqualified table name.
    Table uqTable = null;
    try {
        uqTable = getTable(getDefaultDb(), lcRawPath.get(0));
    } catch (AnalysisException e) {
        if (hasMissingTbls())
            throw e;
    // Ignore other exceptions to allow path resolution to continue.
    }
    if (uqTable != null) {
        Path p = new Path(uqTable, lcRawPath.subList(1, lcRawPath.size()));
        if (p.resolve())
            resolutions.add(p);
    }
    // Resolve catalog table with a fully-qualified table name.
    if (lcRawPath.size() > 1) {
        Table fqTable = null;
        try {
            fqTable = getTable(lcRawPath.get(0), lcRawPath.get(1));
        } catch (AnalysisException e) {
            if (hasMissingTbls())
                throw e;
        // Ignore other exceptions to allow path resolution to continue.
        }
        if (fqTable != null) {
            Path p = new Path(fqTable, lcRawPath.subList(2, lcRawPath.size()));
            if (p.resolve())
                resolutions.add(p);
        }
    }
    result = getResultPath(rawPath, resolutions, pathType);
    if (result != null)
        return result;
    return getErrorPath(resolutions, pathType);
}
#method_after
private Path resolvePath(List<String> rawPath, PathType pathType, boolean resolveInAncestors) throws AnalysisException, TableLoadingException {
    // List of all candidate paths with different roots. Paths in this list are initially
    // unresolved and may be illegal with respect to the pathType.
    ArrayList<Path> candidates = Lists.newArrayList();
    // Path rooted at a tuple desc with an explicit or implicit unqualified alias.
    TupleDescriptor rootDesc = getDescriptor(rawPath.get(0));
    if (rootDesc != null) {
        candidates.add(new Path(rootDesc, rawPath.subList(1, rawPath.size())));
    }
    // Path rooted at a tuple desc with an implicit qualified alias.
    if (rawPath.size() > 1) {
        rootDesc = getDescriptor(rawPath.get(0) + "." + rawPath.get(1));
        if (rootDesc != null) {
            candidates.add(new Path(rootDesc, rawPath.subList(2, rawPath.size())));
        }
    }
    LinkedList<String> errors = Lists.newLinkedList();
    if (pathType == PathType.SLOT_REF || pathType == PathType.STAR) {
        // Paths rooted at all of the unique registered tuple descriptors.
        for (TableRef tblRef : tableRefMap_.values()) {
            candidates.add(new Path(tblRef.getDesc(), rawPath));
        }
    } else {
        // Always prefer table ref paths rooted at a registered tuples descriptor.
        Preconditions.checkState(pathType == PathType.TABLE_REF);
        Path result = resolvePaths(rawPath, candidates, pathType, errors);
        if (result != null)
            return result;
        candidates.clear();
        // Add paths rooted at a table with an unqualified and fully-qualified table name.
        int end = Math.min(2, rawPath.size());
        for (int tblNameIdx = 0; tblNameIdx < end; ++tblNameIdx) {
            String dbName = (tblNameIdx == 0) ? getDefaultDb() : rawPath.get(0);
            String tblName = rawPath.get(tblNameIdx);
            Table tbl = null;
            try {
                tbl = getTable(dbName, tblName);
            } catch (AnalysisException e) {
                if (hasMissingTbls())
                    throw e;
            // Ignore other exceptions to allow path resolution to continue.
            }
            if (tbl != null) {
                candidates.add(new Path(tbl, rawPath.subList(tblNameIdx + 1, rawPath.size())));
            }
        }
    }
    Path result = resolvePaths(rawPath, candidates, pathType, errors);
    if (result == null && resolveInAncestors && hasAncestors()) {
        result = getParentAnalyzer().resolvePath(rawPath, pathType, true);
    }
    if (result == null) {
        Preconditions.checkState(!errors.isEmpty());
        throw new AnalysisException(errors.getFirst());
    }
    return result;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    try {
        resolvedPath_ = analyzer.resolvePath(rawPath_, PathType.TABLE_REF);
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath_)), e);
    }
    if (resolvedPath_ == null) {
        // reveal the non-existence of a table/database if the user is not authorized.
        if (rawPath_.size() > 1) {
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath_.get(0), rawPath_.get(1)).allOf(getPrivilegeRequirement()).toRequest());
        }
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(analyzer.getDefaultDb(), rawPath_.get(0)).allOf(getPrivilegeRequirement()).toRequest());
        throw new AnalysisException(String.format("Could not resolve table reference: '%s'", Joiner.on(".").join(rawPath_)));
    }
    if (resolvedPath_.getRootDesc() != null) {
        if (!analyzer.isVisible(resolvedPath_.getRootDesc().getId())) {
            throw new AnalysisException(String.format("Illegal table reference '%s' of semi-/anti-joined table '%s'", Joiner.on(".").join(rawPath_), resolvedPath_.getRootDesc().getAlias()));
        }
    }
    if (resolvedPath_.getRootTable() != null) {
        Table table = resolvedPath_.getRootTable();
        // Add access event for auditing.
        if (table instanceof View) {
            View view = (View) table;
            if (!view.isLocalView()) {
                analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, getPrivilegeRequirement().toString()));
            }
        } else {
            analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, getPrivilegeRequirement().toString()));
        }
        // Add privilege requests for authorization.
        TableName tableName = table.getTableName();
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(getPrivilegeRequirement()).toRequest());
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    try {
        resolvedPath_ = analyzer.resolvePath(rawPath_, PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!analyzer.hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath_.size() > 1) {
                analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath_.get(0), rawPath_.get(1)).allOf(getPrivilegeRequirement()).toRequest());
            }
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(analyzer.getDefaultDb(), rawPath_.get(0)).allOf(getPrivilegeRequirement()).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath_)), e);
    }
    if (resolvedPath_.getRootTable() != null) {
        // Add access event for auditing.
        Table table = resolvedPath_.getRootTable();
        if (table instanceof View) {
            View view = (View) table;
            if (!view.isLocalView()) {
                analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, getPrivilegeRequirement().toString()));
            }
        } else {
            analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, getPrivilegeRequirement().toString()));
        }
        // Add privilege requests for authorization.
        TableName tableName = table.getTableName();
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(getPrivilegeRequirement()).toRequest());
    }
}
#end_block

#method_before
public TupleDescriptor createTupleDescriptor(Analyzer analyzer) throws AnalysisException {
    throw new AnalysisException("Unresolved table reference: " + tableRefToSql());
}
#method_after
public TupleDescriptor createTupleDescriptor(Analyzer analyzer) throws AnalysisException {
    TupleDescriptor result = analyzer.getDescTbl().createTupleDescriptor(getClass().getSimpleName() + " " + getUniqueAlias());
    result.setPath(resolvedPath_);
    return result;
}
#end_block

#method_before
public void analyzeJoin(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(desc_ != null);
    analyzeJoinHints(analyzer);
    if (joinOp_ == JoinOperator.CROSS_JOIN) {
        // A CROSS JOIN is always a broadcast join, regardless of the join hints
        isBroadcastJoin_ = true;
    }
    if (usingColNames_ != null) {
        Preconditions.checkState(joinOp_ != JoinOperator.CROSS_JOIN);
        // Turn USING clause into equivalent ON clause.
        onClause_ = null;
        for (String colName : usingColNames_) {
            // check whether colName exists both for our table and the one
            // to the left of us
            Path leftColPath = new Path(leftTblRef_.getDesc(), Lists.newArrayList(colName));
            if (!leftColPath.resolve()) {
                throw new AnalysisException("unknown column " + colName + " for alias " + leftTblRef_.getUniqueAlias() + " (in \"" + this.toSql() + "\")");
            }
            Path rightColPath = new Path(desc_, Lists.newArrayList(colName));
            if (!rightColPath.resolve()) {
                throw new AnalysisException("unknown column " + colName + " for alias " + getUniqueAlias() + " (in \"" + this.toSql() + "\")");
            }
            // create predicate "<left>.colName = <right>.colName"
            BinaryPredicate eqPred = new BinaryPredicate(BinaryPredicate.Operator.EQ, new SlotRef(Lists.newArrayList(leftTblRef_.getUniqueAlias(), colName)), new SlotRef(Lists.newArrayList(getUniqueAlias(), colName)));
            onClause_ = CompoundPredicate.createConjunction(eqPred, onClause_);
        }
    }
    // at this point, both 'this' and leftTblRef have been analyzed and registered;
    // register the tuple ids of the TableRefs on the nullable side of an outer join
    boolean lhsIsNullable = false;
    boolean rhsIsNullable = false;
    if (joinOp_ == JoinOperator.LEFT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerOuterJoinedTids(getId().asList(), this);
        rhsIsNullable = true;
    }
    if (joinOp_ == JoinOperator.RIGHT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerOuterJoinedTids(leftTblRef_.getAllTupleIds(), this);
        lhsIsNullable = true;
    }
    // register the tuple ids of a full outer join
    if (joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerFullOuterJoinedTids(leftTblRef_.getAllTupleIds(), this);
        analyzer.registerFullOuterJoinedTids(getId().asList(), this);
    }
    // register the tuple id of the rhs of a left semi join
    TupleId semiJoinedTupleId = null;
    if (joinOp_ == JoinOperator.LEFT_SEMI_JOIN || joinOp_ == JoinOperator.LEFT_ANTI_JOIN || joinOp_ == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
        analyzer.registerSemiJoinedTid(getId(), this);
        semiJoinedTupleId = getId();
    }
    // register the tuple id of the lhs of a right semi join
    if (joinOp_ == JoinOperator.RIGHT_SEMI_JOIN || joinOp_ == JoinOperator.RIGHT_ANTI_JOIN) {
        analyzer.registerSemiJoinedTid(leftTblRef_.getId(), this);
        semiJoinedTupleId = leftTblRef_.getId();
    }
    if (onClause_ != null) {
        Preconditions.checkState(joinOp_ != JoinOperator.CROSS_JOIN);
        analyzer.setVisibleSemiJoinedTuple(semiJoinedTupleId);
        onClause_.analyze(analyzer);
        analyzer.setVisibleSemiJoinedTuple(null);
        onClause_.checkReturnsBool("ON clause", true);
        if (onClause_.contains(Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function not allowed in ON clause: " + toSql());
        }
        if (onClause_.contains(AnalyticExpr.class)) {
            throw new AnalysisException("analytic expression not allowed in ON clause: " + toSql());
        }
        Set<TupleId> onClauseTupleIds = Sets.newHashSet();
        for (Expr e : onClause_.getConjuncts()) {
            // Outer join clause conjuncts are registered for this particular table ref
            // (ie, can only be evaluated by the plan node that implements this join).
            // The exception are conjuncts that only pertain to the nullable side
            // of the outer join; those can be evaluated directly when materializing tuples
            // without violating outer join semantics.
            analyzer.registerOnClauseConjuncts(e, this);
            List<TupleId> tupleIds = Lists.newArrayList();
            e.getIds(tupleIds, null);
            onClauseTupleIds.addAll(tupleIds);
        }
        onClauseTupleIds_.addAll(onClauseTupleIds);
    } else if (!isChildRef() && (getJoinOp().isOuterJoin() || getJoinOp().isSemiJoin())) {
        throw new AnalysisException(joinOp_.toString() + " requires an ON or USING clause.");
    }
    // Make constant expressions from inline view refs nullable in its substitution map.
    if (lhsIsNullable && leftTblRef_ instanceof InlineViewRef) {
        ((InlineViewRef) leftTblRef_).makeOutputNullable(analyzer);
    }
    if (rhsIsNullable && this instanceof InlineViewRef) {
        ((InlineViewRef) this).makeOutputNullable(analyzer);
    }
}
#method_after
public void analyzeJoin(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(desc_ != null);
    analyzeJoinHints(analyzer);
    if (joinOp_ == JoinOperator.CROSS_JOIN) {
        // A CROSS JOIN is always a broadcast join, regardless of the join hints
        isBroadcastJoin_ = true;
    }
    if (usingColNames_ != null) {
        Preconditions.checkState(joinOp_ != JoinOperator.CROSS_JOIN);
        // Turn USING clause into equivalent ON clause.
        onClause_ = null;
        for (String colName : usingColNames_) {
            // check whether colName exists both for our table and the one
            // to the left of us
            Path leftColPath = new Path(leftTblRef_.getDesc(), Lists.newArrayList(colName.toLowerCase()));
            if (!leftColPath.resolve()) {
                throw new AnalysisException("unknown column " + colName + " for alias " + leftTblRef_.getUniqueAlias() + " (in \"" + this.toSql() + "\")");
            }
            Path rightColPath = new Path(desc_, Lists.newArrayList(colName.toLowerCase()));
            if (!rightColPath.resolve()) {
                throw new AnalysisException("unknown column " + colName + " for alias " + getUniqueAlias() + " (in \"" + this.toSql() + "\")");
            }
            // create predicate "<left>.colName = <right>.colName"
            BinaryPredicate eqPred = new BinaryPredicate(BinaryPredicate.Operator.EQ, new SlotRef(Path.createRawPath(leftTblRef_.getUniqueAlias(), colName)), new SlotRef(Path.createRawPath(getUniqueAlias(), colName)));
            onClause_ = CompoundPredicate.createConjunction(eqPred, onClause_);
        }
    }
    // register the tuple ids of the TableRefs on the nullable side of an outer join
    if (joinOp_ == JoinOperator.LEFT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerOuterJoinedTids(getId().asList(), this);
    }
    if (joinOp_ == JoinOperator.RIGHT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerOuterJoinedTids(leftTblRef_.getAllTupleIds(), this);
    }
    // register the tuple ids of a full outer join
    if (joinOp_ == JoinOperator.FULL_OUTER_JOIN) {
        analyzer.registerFullOuterJoinedTids(leftTblRef_.getAllTupleIds(), this);
        analyzer.registerFullOuterJoinedTids(getId().asList(), this);
    }
    // register the tuple id of the rhs of a left semi join
    TupleId semiJoinedTupleId = null;
    if (joinOp_ == JoinOperator.LEFT_SEMI_JOIN || joinOp_ == JoinOperator.LEFT_ANTI_JOIN || joinOp_ == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
        analyzer.registerSemiJoinedTid(getId(), this);
        semiJoinedTupleId = getId();
    }
    // register the tuple id of the lhs of a right semi join
    if (joinOp_ == JoinOperator.RIGHT_SEMI_JOIN || joinOp_ == JoinOperator.RIGHT_ANTI_JOIN) {
        analyzer.registerSemiJoinedTid(leftTblRef_.getId(), this);
        semiJoinedTupleId = leftTblRef_.getId();
    }
    if (onClause_ != null) {
        Preconditions.checkState(joinOp_ != JoinOperator.CROSS_JOIN);
        analyzer.setVisibleSemiJoinedTuple(semiJoinedTupleId);
        onClause_.analyze(analyzer);
        analyzer.setVisibleSemiJoinedTuple(null);
        onClause_.checkReturnsBool("ON clause", true);
        if (onClause_.contains(Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function not allowed in ON clause: " + toSql());
        }
        if (onClause_.contains(AnalyticExpr.class)) {
            throw new AnalysisException("analytic expression not allowed in ON clause: " + toSql());
        }
        Set<TupleId> onClauseTupleIds = Sets.newHashSet();
        for (Expr e : onClause_.getConjuncts()) {
            // Outer join clause conjuncts are registered for this particular table ref
            // (ie, can only be evaluated by the plan node that implements this join).
            // The exception are conjuncts that only pertain to the nullable side
            // of the outer join; those can be evaluated directly when materializing tuples
            // without violating outer join semantics.
            analyzer.registerOnClauseConjuncts(e, this);
            List<TupleId> tupleIds = Lists.newArrayList();
            e.getIds(tupleIds, null);
            onClauseTupleIds.addAll(tupleIds);
        }
        onClauseTupleIds_.addAll(onClauseTupleIds);
    } else if (!isChildRef() && (getJoinOp().isOuterJoin() || getJoinOp().isSemiJoin())) {
        throw new AnalysisException(joinOp_.toString() + " requires an ON or USING clause.");
    }
}
#end_block

#method_before
@Override
public String toString() {
    // The function may be null before analyze().
    if (fn_ == null)
        return Joiner.on(".").join(fnNamePath_);
    if (db_ == null || isBuiltin_)
        return fn_;
    return db_ + "." + fn_;
}
#method_after
@Override
public String toString() {
    // The fnNamePath_ is not always set.
    if (!isAnalyzed_ && fnNamePath_ != null)
        return Joiner.on(".").join(fnNamePath_);
    if (db_ == null || isBuiltin_)
        return fn_;
    return db_ + "." + fn_;
}
#end_block

#method_before
public void analyze(Analyzer analyzer) throws AnalysisException {
    analyzeFnNamePath();
    if (fn_.isEmpty())
        throw new AnalysisException("Function name cannot be empty.");
    for (int i = 0; i < fn_.length(); ++i) {
        if (!isValidCharacter(fn_.charAt(i))) {
            throw new AnalysisException("Function names must be all alphanumeric or underscore. " + "Invalid name: " + fn_);
        }
    }
    if (Character.isDigit(fn_.charAt(0))) {
        throw new AnalysisException("Function cannot start with a digit: " + fn_);
    }
    // Resolve the database for this function.
    if (!isFullyQualified()) {
        Db builtinDb = analyzer.getCatalog().getBuiltinsDb();
        if (builtinDb.containsFunction(fn_)) {
            // If it isn't fully qualified and is the same name as a builtin, use
            // the builtin.
            db_ = Catalog.BUILTINS_DB;
            isBuiltin_ = true;
        } else {
            db_ = analyzer.getDefaultDb();
            isBuiltin_ = false;
        }
    } else {
        isBuiltin_ = db_.equals(Catalog.BUILTINS_DB);
    }
}
#method_after
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    analyzeFnNamePath();
    if (fn_.isEmpty())
        throw new AnalysisException("Function name cannot be empty.");
    for (int i = 0; i < fn_.length(); ++i) {
        if (!isValidCharacter(fn_.charAt(i))) {
            throw new AnalysisException("Function names must be all alphanumeric or underscore. " + "Invalid name: " + fn_);
        }
    }
    if (Character.isDigit(fn_.charAt(0))) {
        throw new AnalysisException("Function cannot start with a digit: " + fn_);
    }
    // Resolve the database for this function.
    if (!isFullyQualified()) {
        Db builtinDb = analyzer.getCatalog().getBuiltinsDb();
        if (builtinDb.containsFunction(fn_)) {
            // If it isn't fully qualified and is the same name as a builtin, use
            // the builtin.
            db_ = Catalog.BUILTINS_DB;
            isBuiltin_ = true;
        } else {
            db_ = analyzer.getDefaultDb();
            isBuiltin_ = false;
        }
    } else {
        isBuiltin_ = db_.equals(Catalog.BUILTINS_DB);
    }
    isAnalyzed_ = true;
}
#end_block

#method_before
@Test
public void TestInSubqueries() throws AnalysisException {
    String[] colNames = { "bool_col", "tinyint_col", "smallint_col", "int_col", "bigint_col", "float_col", "double_col", "string_col", "date_string_col", "timestamp_col" };
    String[] joinOperators = { "inner join", "left outer join", "right outer join", "left semi join", "left anti join" };
    // [NOT] IN subquery predicates
    String[] operators = { "in", "not in" };
    for (String op : operators) {
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select id from functional.alltypestiny)", op));
        // Using column and table aliases similar to the ones produced by the
        // column/table alias generators during a rewrite.
        AnalyzesOk(String.format("select id `$c$1` from functional.alltypestiny `$a$1` " + "where id %s (select id from functional.alltypessmall)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "t.id %s (select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select t.id, max(t.int_col) from " + "functional.alltypes t where t.int_col %s (select int_col from " + "functional.alltypesagg) group by t.id having count(*) < 10", op));
        AnalyzesOk(String.format("select t.bigint_col, t.string_col from " + "functional.alltypes t where t.id %s (select id from " + "functional.alltypesagg where int_col < 10) order by bigint_col", op));
        AnalyzesOk(String.format("select * from functional.alltypes a where a.id %s " + "(select id from functional.alltypes b where a.id = b.id)", op));
        // Complex expressions
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id + int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "t.int_col + 1 %s (select int_col - 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "abs(t.double_col) %s (select int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select NULL from functional.alltypes t where " + "cast(t.double_col as int) %s (select int_col from " + "functional.alltypestiny)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes where id %s " + "(select 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select 1 + 1 from functional.alltypestiny group by int_col)", op));
        AnalyzesOk(String.format("select max(id) from functional.alltypes where id %s " + "(select max(id) from functional.alltypesagg a where a.int_col < 10) " + "and bool_col = false", op));
        // Subquery returns multiple columns
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select id, int_col from functional.alltypessmall)", op), "Subquery must return a single column: (SELECT id, int_col " + "FROM functional.alltypessmall)");
        // Subquery returns an incompatible column type
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select timestamp_col from functional.alltypessmall)", op), "Incompatible return types 'INT' and 'TIMESTAMP' of exprs 'id' and " + "'timestamp_col'.");
        // Different column types in the subquery predicate
        for (String col : colNames) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.%s %s " + "(select a.%s from functional.alltypestiny a)", col, op, col));
        }
        // Decimal in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.double_col %s (select d3 from functional.decimal_tbl a)", op));
        // Varchar in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.string_col %s (select cast(a.string_col as varchar(1)) from " + "functional.alltypestiny a)", op));
        // Subqueries with multiple predicates in the WHERE clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col > 10 and " + "a.tinyint_col < 5)", op));
        // Subqueries with a GROUP BY clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.double_col < 10.1 " + "group by a.id)", op));
        // Subqueries with GROUP BY and HAVING clauses
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.bool_col = true and " + "int_col < 10 group by id having count(*) < 10)", op));
        // Subqueries with a LIMIT clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where id < 100 limit 10)", op));
        // Subqueries with multiple tables in the FROM clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, functional.alltypessmall s " + "where a.int_col = s.int_col and s.bigint_col < 100 and a.tinyint_col < 10)", op));
        // Different join operators between the tables in subquery's FROM clause
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a %s functional.alltypessmall " + "s on a.int_col = s.int_col where a.bool_col = false)", op, joinOp));
        }
        // Correlated predicates in the subquery's ON clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.bigint_col = a.bigint_col and " + "s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on a.bool_col = s.bool_col and t.int_col = 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on ifnull(s.int_col, s.int_col + 20) = " + "t.int_col + t.bigint_col)", op));
        // Subqueries with inline views
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, " + "(select * from functional.alltypessmall) s where s.int_col = a.int_col " + "and s.bool_col = false)", op));
        // Subqueries with inline views that contain subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select id from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select g.* from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a where " + "a.bigint_col = 100)", op));
        // Multiple tables in the FROM clause of the outer query block
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t %s " + "functional.alltypessmall s on t.int_col = s.int_col where " + "t.tinyint_col %s (select tinyint_col from functional.alltypesagg) " + "and t.bool_col = false and t.bigint_col = 10", joinOp, op));
        }
        // Subqueries in WITH clause
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a where " + "id %s (select id from functional.alltypestiny)) select * from t where " + "t.bool_col = false and t.int_col = 10", op));
        // Subqueries in WITH and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny s)) select * from t " + "where t.int_col in (select int_col from functional.alltypessmall) and " + "t.bool_col = false", op));
        // Subqueries in WITH, FROM and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny)) select t.* from t, " + "(select * from functional.alltypesagg g where g.id in " + "(select id from functional.alltypes)) s where s.string_col = t.string_col " + "and t.int_col in (select int_col from functional.alltypessmall) and " + "s.bool_col = false", op));
        // Correlated subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col) " + "and t.bool_col = false", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col + 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + a.int_col  = " + "a.bigint_col and a.bool_col = true)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col = false and " + "a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col)", op));
        // Multiple nesting levels (uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg where int_col %s " + "(select int_col from functional.alltypestiny) and bool_col = false) " + "and bigint_col < 1000", op, op));
        // Multiple nesting levels (correlated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where a.int_col = t.int_col " + "and a.tinyint_col %s (select tinyint_col from functional.alltypestiny s " + "where s.bigint_col = a.bigint_col))", op, op));
        // Multiple nesting levels (correlated and uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col " + "and a.int_col %s (select int_col from functional.alltypestiny s))", op, op));
        // NOT ([NOT] IN predicate)
        AnalyzesOk(String.format("select * from functional.alltypes t where not (id %s " + "(select id from functional.alltypesagg))", op));
        // Different cmp operators in the correlation predicate
        for (String cmpOp : cmpOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t " + "where t.id %s (select a.id from functional.alltypesagg a where " + "t.int_col %s a.int_col)", op, cmpOp));
        }
        // Uncorrelated IN subquery with analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col %s (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", op));
    }
    // Constant on the left hand side
    AnalyzesOk("select * from functional.alltypes a where 1 in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)");
    AnalysisError("select * from functional.alltypes a where 1 not in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)", "Unsupported NOT IN predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypesagg s WHERE s.int_col = a.int_col)");
    // IN subquery that is equivalent to an uncorrelated EXISTS subquery
    AnalysisError("select * from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg)", "Unsupported " + "predicate with subquery: 1 IN (SELECT int_col FROM functional.alltypesagg)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        AnalysisError(String.format("select 1 from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg g where g.id %s t.id)", cmpOp), String.format("Unsupported predicate with subquery: 1 " + "IN (SELECT int_col FROM functional.alltypesagg g WHERE g.id %s t.id)", cmpOp));
    }
    // NOT IN subquery with a correlated predicate that can't be used in an equi
    // join
    AnalysisError("select 1 from functional.alltypes t where 1 not in " + "(select id from functional.alltypestiny g where g.id < t.id)", "Unsupported predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypestiny g WHERE g.id < t.id)");
    // Statement with a GROUP BY and a correlated IN subquery that has
    // correlated predicate that cannot be transformed into an equi-join.
    AnalysisError("select id, count(*) from functional.alltypes t " + "where 1 IN (select id from functional.alltypesagg g where t.int_col < " + "g.int_col) group by id", "Unsupported predicate with subquery: 1 IN " + "(SELECT id FROM functional.alltypesagg g WHERE t.int_col < g.int_col)");
    // Reference a non-existing table in the subquery
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s left outer join p on " + "(s.int_col = p.int_col))", "Could not resolve table reference: 'p'");
    // Reference a non-existing column from a table in the outer scope
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s where s.int_col = t.bad_col)", "Could not resolve column/field reference: 't.bad_col'");
    // Referencing the same table in the inner and the outer query block
    // No explicit alias
    AnalyzesOk("select id from functional.alltypestiny where int_col in " + "(select int_col from functional.alltypestiny)");
    // Different alias between inner and outer block referencing the same table
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny p)");
    // Alias only in the outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny)");
    // Same alias in both inner and outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny t)");
    // Binary predicate with non-comparable operands
    AnalysisError("select * from functional.alltypes t where " + "(id in (select id from functional.alltypestiny)) = 'string_val'", "operands of type BOOLEAN and STRING are not comparable: " + "(id IN (SELECT id FROM functional.alltypestiny)) = 'string_val'");
    // OR with subquery predicates
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select id from functional.alltypesagg) or t.bool_col = false", "Subqueries in OR predicates are not supported: t.id IN " + "(SELECT id FROM functional.alltypesagg) OR t.bool_col = FALSE");
    AnalysisError("select * from functional.alltypes t where not (t.id in " + "(select id from functional.alltypesagg) and t.int_col = 10)", "Subqueries in OR predicates are not supported: t.id NOT IN " + "(SELECT id FROM functional.alltypesagg) OR t.int_col != 10");
    AnalysisError("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg g where g.bool_col = false) " + "or t.bool_col = true", "Subqueries in OR predicates are not " + "supported: EXISTS (SELECT * FROM functional.alltypesagg g WHERE " + "g.bool_col = FALSE) OR t.bool_col = TRUE");
    AnalysisError("select * from functional.alltypes t where t.id = " + "(select min(id) from functional.alltypesagg g) or t.id = 10", "Subqueries in OR predicates are not supported: t.id = " + "(SELECT min(id) FROM functional.alltypesagg g) OR t.id = 10");
    // Correlated subquery with OR predicate
    AnalysisError("select * from functional.alltypes t where id in " + "(select id from functional.alltypesagg a where " + "a.int_col = t.int_col or a.bool_col = false)", "Disjunctions " + "with correlated predicates are not supported: a.int_col = " + "t.int_col OR a.bool_col = FALSE");
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny) and (bool_col = false or " + "int_col = 10)");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select max(a.id) from functional.alltypesagg a where " + "t.int_col = a.int_col)", "Unsupported correlated subquery with grouping " + "and/or aggregation: SELECT max(a.id) FROM functional.alltypesagg a " + "WHERE t.int_col = a.int_col");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select a.id from functional.alltypesagg a where " + "t.int_col = a.int_col group by a.id)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT a.id FROM " + "functional.alltypesagg a WHERE t.int_col = a.int_col GROUP BY a.id");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select distinct a.id from functional.alltypesagg a where " + "a.bigint_col = t.bigint_col)", "Unsupported correlated subquery with " + "grouping and/or aggregation: SELECT DISTINCT a.id FROM " + "functional.alltypesagg a WHERE a.bigint_col = t.bigint_col");
    // NOT compound predicates with OR
    AnalyzesOk("select * from functional.alltypes t where not (" + "id in (select id from functional.alltypesagg) or int_col < 10)");
    AnalyzesOk("select * from functional.alltypes t where not (" + "t.id < 10 or not (t.int_col in (select int_col from " + "functional.alltypesagg) and t.bool_col = false))");
    // Multiple subquery predicates
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny where int_col = 10) and int_col in " + "(select int_col from functional.alltypessmall where bigint_col = 1000) and " + "string_col not in (select string_col from functional.alltypesagg where " + "tinyint_col > 10) and bool_col = false");
    // Correlated subquery with a LIMIT clause
    AnalysisError("select * from functional.alltypes t where id in " + "(select s.id from functional.alltypesagg s where s.int_col = t.int_col " + "limit 1)", "Unsupported correlated subquery with a LIMIT clause: " + "SELECT s.id FROM functional.alltypesagg s WHERE s.int_col = t.int_col " + "LIMIT 1");
    // Correlated IN with an analytic function
    AnalysisError("select id, int_col, bool_col from functional.alltypestiny t1 " + "where int_col in (select min(bigint_col) over (partition by bool_col) " + "from functional.alltypessmall t2 where t1.id < t2.id)", "Unsupported " + "correlated subquery with grouping and/or aggregation: SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE t1.id < t2.id");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "int_col in (select 1 as int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "int_col not in (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#method_after
@Test
public void TestInSubqueries() throws AnalysisException {
    String[] colNames = { "bool_col", "tinyint_col", "smallint_col", "int_col", "bigint_col", "float_col", "double_col", "string_col", "date_string_col", "timestamp_col" };
    String[] joinOperators = { "inner join", "left outer join", "right outer join", "left semi join", "left anti join" };
    // [NOT] IN subquery predicates
    String[] operators = { "in", "not in" };
    for (String op : operators) {
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select id from functional.alltypestiny)", op));
        // Using column and table aliases similar to the ones produced by the
        // column/table alias generators during a rewrite.
        AnalyzesOk(String.format("select id `$c$1` from functional.alltypestiny `$a$1` " + "where id %s (select id from functional.alltypessmall)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "t.id %s (select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select t.id, max(t.int_col) from " + "functional.alltypes t where t.int_col %s (select int_col from " + "functional.alltypesagg) group by t.id having count(*) < 10", op));
        AnalyzesOk(String.format("select t.bigint_col, t.string_col from " + "functional.alltypes t where t.id %s (select id from " + "functional.alltypesagg where int_col < 10) order by bigint_col", op));
        AnalyzesOk(String.format("select * from functional.alltypes a where a.id %s " + "(select id from functional.alltypes b where a.id = b.id)", op));
        // Complex expressions
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id + int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "t.int_col + 1 %s (select int_col - 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "abs(t.double_col) %s (select int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select NULL from functional.alltypes t where " + "cast(t.double_col as int) %s (select int_col from " + "functional.alltypestiny)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes where id %s " + "(select 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select 1 + 1 from functional.alltypestiny group by int_col)", op));
        AnalyzesOk(String.format("select max(id) from functional.alltypes where id %s " + "(select max(id) from functional.alltypesagg a where a.int_col < 10) " + "and bool_col = false", op));
        // Subquery returns multiple columns
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select id, int_col from functional.alltypessmall)", op), "Subquery must return a single column: (SELECT id, int_col " + "FROM functional.alltypessmall)");
        // Subquery returns an incompatible column type
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select timestamp_col from functional.alltypessmall)", op), "Incompatible return types 'INT' and 'TIMESTAMP' of exprs 'id' and " + "'timestamp_col'.");
        // Different column types in the subquery predicate
        for (String col : colNames) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.%s %s " + "(select a.%s from functional.alltypestiny a)", col, op, col));
        }
        // Decimal in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.double_col %s (select d3 from functional.decimal_tbl a)", op));
        // Varchar in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.string_col %s (select cast(a.string_col as varchar(1)) from " + "functional.alltypestiny a)", op));
        // Subqueries with multiple predicates in the WHERE clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col > 10 and " + "a.tinyint_col < 5)", op));
        // Subqueries with a GROUP BY clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.double_col < 10.1 " + "group by a.id)", op));
        // Subqueries with GROUP BY and HAVING clauses
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.bool_col = true and " + "int_col < 10 group by id having count(*) < 10)", op));
        // Subqueries with a LIMIT clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where id < 100 limit 10)", op));
        // Subqueries with multiple tables in the FROM clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, functional.alltypessmall s " + "where a.int_col = s.int_col and s.bigint_col < 100 and a.tinyint_col < 10)", op));
        // Different join operators between the tables in subquery's FROM clause
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a %s functional.alltypessmall " + "s on a.int_col = s.int_col where a.bool_col = false)", op, joinOp));
        }
        // Correlated predicates in the subquery's ON clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.bigint_col = a.bigint_col and " + "s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on a.bool_col = s.bool_col and t.int_col = 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on ifnull(s.int_col, s.int_col + 20) = " + "t.int_col + t.bigint_col)", op));
        // Subqueries with inline views
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, " + "(select * from functional.alltypessmall) s where s.int_col = a.int_col " + "and s.bool_col = false)", op));
        // Subqueries with inline views that contain subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select id from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select g.* from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a where " + "a.bigint_col = 100)", op));
        // Multiple tables in the FROM clause of the outer query block
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t %s " + "functional.alltypessmall s on t.int_col = s.int_col where " + "t.tinyint_col %s (select tinyint_col from functional.alltypesagg) " + "and t.bool_col = false and t.bigint_col = 10", joinOp, op));
        }
        // Subqueries in WITH clause
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a where " + "id %s (select id from functional.alltypestiny)) select * from t where " + "t.bool_col = false and t.int_col = 10", op));
        // Subqueries in WITH and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny s)) select * from t " + "where t.int_col in (select int_col from functional.alltypessmall) and " + "t.bool_col = false", op));
        // Subqueries in WITH, FROM and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny)) select t.* from t, " + "(select * from functional.alltypesagg g where g.id in " + "(select id from functional.alltypes)) s where s.string_col = t.string_col " + "and t.int_col in (select int_col from functional.alltypessmall) and " + "s.bool_col = false", op));
        // Correlated subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col) " + "and t.bool_col = false", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col + 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + a.int_col  = " + "a.bigint_col and a.bool_col = true)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col = false and " + "a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col)", op));
        // Multiple nesting levels (uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg where int_col %s " + "(select int_col from functional.alltypestiny) and bool_col = false) " + "and bigint_col < 1000", op, op));
        // Multiple nesting levels (correlated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where a.int_col = t.int_col " + "and a.tinyint_col %s (select tinyint_col from functional.alltypestiny s " + "where s.bigint_col = a.bigint_col))", op, op));
        // Multiple nesting levels (correlated and uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col " + "and a.int_col %s (select int_col from functional.alltypestiny s))", op, op));
        // NOT ([NOT] IN predicate)
        AnalyzesOk(String.format("select * from functional.alltypes t where not (id %s " + "(select id from functional.alltypesagg))", op));
        // Different cmp operators in the correlation predicate
        for (String cmpOp : cmpOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t " + "where t.id %s (select a.id from functional.alltypesagg a where " + "t.int_col %s a.int_col)", op, cmpOp));
        }
        // Uncorrelated IN subquery with analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col %s (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", op));
    }
    // Constant on the left hand side
    AnalyzesOk("select * from functional.alltypes a where 1 in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)");
    AnalysisError("select * from functional.alltypes a where 1 not in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)", "Unsupported NOT IN predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypesagg s WHERE s.int_col = a.int_col)");
    // IN subquery that is equivalent to an uncorrelated EXISTS subquery
    AnalysisError("select * from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg)", "Unsupported " + "predicate with subquery: 1 IN (SELECT int_col FROM functional.alltypesagg)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        AnalysisError(String.format("select 1 from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg g where g.id %s t.id)", cmpOp), String.format("Unsupported predicate with subquery: 1 " + "IN (SELECT int_col FROM functional.alltypesagg g WHERE g.id %s t.id)", cmpOp));
    }
    // NOT IN subquery with a correlated predicate that can't be used in an equi
    // join
    AnalysisError("select 1 from functional.alltypes t where 1 not in " + "(select id from functional.alltypestiny g where g.id < t.id)", "Unsupported predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypestiny g WHERE g.id < t.id)");
    // Statement with a GROUP BY and a correlated IN subquery that has
    // correlated predicate that cannot be transformed into an equi-join.
    AnalysisError("select id, count(*) from functional.alltypes t " + "where 1 IN (select id from functional.alltypesagg g where t.int_col < " + "g.int_col) group by id", "Unsupported predicate with subquery: 1 IN " + "(SELECT id FROM functional.alltypesagg g WHERE t.int_col < g.int_col)");
    // Reference a non-existing table in the subquery
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s left outer join p on " + "(s.int_col = p.int_col))", "Could not resolve table reference: 'p'");
    // Reference a non-existing column from a table in the outer scope
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s where s.int_col = t.bad_col)", "Could not resolve column/field reference: 't.bad_col'");
    // Referencing the same table in the inner and the outer query block
    // No explicit alias
    AnalyzesOk("select id from functional.alltypestiny where int_col in " + "(select int_col from functional.alltypestiny)");
    // Different alias between inner and outer block referencing the same table
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny p)");
    // Alias only in the outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny)");
    // Same alias in both inner and outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny t)");
    // Binary predicate with non-comparable operands
    AnalysisError("select * from functional.alltypes t where " + "(id in (select id from functional.alltypestiny)) = 'string_val'", "operands of type BOOLEAN and STRING are not comparable: " + "(id IN (SELECT id FROM functional.alltypestiny)) = 'string_val'");
    // OR with subquery predicates
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select id from functional.alltypesagg) or t.bool_col = false", "Subqueries in OR predicates are not supported: t.id IN " + "(SELECT id FROM functional.alltypesagg) OR t.bool_col = FALSE");
    AnalysisError("select * from functional.alltypes t where not (t.id in " + "(select id from functional.alltypesagg) and t.int_col = 10)", "Subqueries in OR predicates are not supported: t.id NOT IN " + "(SELECT id FROM functional.alltypesagg) OR t.int_col != 10");
    AnalysisError("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg g where g.bool_col = false) " + "or t.bool_col = true", "Subqueries in OR predicates are not " + "supported: EXISTS (SELECT * FROM functional.alltypesagg g WHERE " + "g.bool_col = FALSE) OR t.bool_col = TRUE");
    AnalysisError("select * from functional.alltypes t where t.id = " + "(select min(id) from functional.alltypesagg g) or t.id = 10", "Subqueries in OR predicates are not supported: t.id = " + "(SELECT min(id) FROM functional.alltypesagg g) OR t.id = 10");
    // Correlated subquery with OR predicate
    AnalysisError("select * from functional.alltypes t where id in " + "(select id from functional.alltypesagg a where " + "a.int_col = t.int_col or a.bool_col = false)", "Disjunctions " + "with correlated predicates are not supported: a.int_col = " + "t.int_col OR a.bool_col = FALSE");
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny) and (bool_col = false or " + "int_col = 10)");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select max(a.id) from functional.alltypesagg a where " + "t.int_col = a.int_col)", "Unsupported correlated subquery with grouping " + "and/or aggregation: SELECT max(a.id) FROM functional.alltypesagg a " + "WHERE t.int_col = a.int_col");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select a.id from functional.alltypesagg a where " + "t.int_col = a.int_col group by a.id)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT a.id FROM " + "functional.alltypesagg a WHERE t.int_col = a.int_col GROUP BY a.id");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select distinct a.id from functional.alltypesagg a where " + "a.bigint_col = t.bigint_col)", "Unsupported correlated subquery with " + "grouping and/or aggregation: SELECT DISTINCT a.id FROM " + "functional.alltypesagg a WHERE a.bigint_col = t.bigint_col");
    // NOT compound predicates with OR
    AnalyzesOk("select * from functional.alltypes t where not (" + "id in (select id from functional.alltypesagg) or int_col < 10)");
    AnalyzesOk("select * from functional.alltypes t where not (" + "t.id < 10 or not (t.int_col in (select int_col from " + "functional.alltypesagg) and t.bool_col = false))");
    // Multiple subquery predicates
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny where int_col = 10) and int_col in " + "(select int_col from functional.alltypessmall where bigint_col = 1000) and " + "string_col not in (select string_col from functional.alltypesagg where " + "tinyint_col > 10) and bool_col = false");
    // Correlated subquery with a LIMIT clause
    AnalysisError("select * from functional.alltypes t where id in " + "(select s.id from functional.alltypesagg s where s.int_col = t.int_col " + "limit 1)", "Unsupported correlated subquery with a LIMIT clause: " + "SELECT s.id FROM functional.alltypesagg s WHERE s.int_col = t.int_col " + "LIMIT 1");
    // Correlated IN with an analytic function
    AnalysisError("select id, int_col, bool_col from functional.alltypestiny t1 " + "where int_col in (select min(bigint_col) over (partition by bool_col) " + "from functional.alltypessmall t2 where t1.id < t2.id)", "Unsupported " + "correlated subquery with grouping and/or aggregation: SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE t1.id < t2.id");
    // IN subquery in binary predicate
    AnalysisError("select * from functional.alltypestiny where " + "(tinyint_col in (1,2)) = (bool_col in (select bool_col from " + "functional.alltypes))", "IN subquery predicates are not supported " + "in binary predicates: (tinyint_col IN (1, 2)) = (bool_col IN (SELECT " + "bool_col FROM functional.alltypes))");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "int_col in (select 1 as int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "int_col not in (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#end_block

#method_before
@Test
public void TestExistsSubqueries() throws AnalysisException {
    String[] existsOperators = { "exists", "not exists" };
    for (String op : existsOperators) {
        // [NOT] EXISTS predicate (correlated)
        AnalyzesOk(String.format("select * from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.id = t.id)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.int_col = t.int_col and p.bool_col = false)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col and a.bool_col = " + "t.bool_col)", op));
        // Multiple [NOT] EXISTS predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypessmall s where s.id = t.id) and " + "%s (select NULL from functional.alltypesagg g where t.int_col = g.int_col)", op, op));
        // OR between two subqueries
        AnalysisError(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id) or %s " + "(select * from functional.alltypessmall s where s.int_col = t.int_col)", op, op), String.format("Subqueries in OR predicates are not supported: %s " + "(SELECT * FROM functional.alltypesagg a WHERE a.id = t.id) OR %s (SELECT " + "* FROM functional.alltypessmall s WHERE s.int_col = t.int_col)", op.toUpperCase(), op.toUpperCase()));
        // Complex correlation predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id + 1) and " + "%s (select 1 from functional.alltypes s where s.int_col + s.bigint_col = " + "t.bigint_col + 1)", op, op));
        // Correlated predicates
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg g where t.int_col = g.int_col " + "and t.bool_col = false)", op));
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select id from functional.alltypessmall s where t.tinyint_col = " + "s.tinyint_col and t.bool_col)", op));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.int_col = s.int_col))", op, op));
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.bool_col = " + "s.bool_col))", op, op));
        // Correlated EXISTS subquery with a group by and aggregation
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t " + "where %s (select id, count(*) from functional.alltypesagg g where " + "t.id = g.id group by id)", op));
        // Correlated EXISTS subquery with an analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where %s (select min(bigint_col) over " + "(partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id)", op));
        // Correlated EXISTS subquery with an analytic function and a group by
        // clause
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where exists (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 " + "where t1.id = t2.id group by bigint_col, bool_col)", op));
        String[] nullOps = { "is null", "is not null" };
        for (String nullOp : nullOps) {
            // Uncorrelated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes where %s " + "(select * from functional.alltypestiny) %s and id < 5", op, nullOp));
            // Correlated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes t where " + "%s (select 1 from functional.alltypestiny s where t.id = s.id) " + "%s and t.bool_col = false", op, nullOp));
        }
    }
    // Uncorrelated EXISTS subquery with an analytic function
    AnalyzesOk("select * from functional.alltypestiny t " + "where EXISTS (select id, min(int_col) over (partition by bool_col) " + "from functional.alltypesagg a where bigint_col < 10)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        AnalysisError(String.format("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where t.id %s a.id)", cmpOp), String.format("Unsupported predicate with subquery: EXISTS (SELECT * FROM " + "functional.alltypesagg a WHERE t.id %s a.id)", cmpOp));
    }
    // Uncorrelated EXISTS in a query with GROUP BY
    AnalyzesOk("select id, count(*) from functional.alltypes t " + "where exists (select 1 from functional.alltypestiny where id < 5) group by id");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select int_col + 1 from functional.alltypessmall s where " + "t.int_col = 10)", "Unsupported predicate with subquery: EXISTS " + "(SELECT int_col + 1 FROM functional.alltypessmall s WHERE t.int_col = 10)");
    // Uncorrelated EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where exists " + "(select * from functional.alltypesagg where id < 10)");
    AnalyzesOk("select id from functional.alltypestiny where exists " + "(select id from functional.alltypessmall where bool_col = false)");
    AnalyzesOk("select 1 from functional.alltypestiny t where exists " + "(select 1 from functional.alltypessmall where id < 5)");
    AnalyzesOk("select 1 + 1 from functional.alltypestiny where exists " + "(select null from functional.alltypessmall where id != 5)");
    // Multiple nesting levels with uncorrelated EXISTS
    AnalyzesOk("select id from functional.alltypes where exists " + "(select id from functional.alltypestiny where int_col < 10 and exists (" + "select id from functional.alltypessmall where bool_col = true))");
    // Uncorrelated NOT EXISTS subquery
    AnalysisError("select * from functional.alltypestiny where not exists " + "(select 1 from functional.alltypessmall where bool_col = false)", "Unsupported uncorrelated NOT EXISTS subquery: SELECT 1 FROM " + "functional.alltypessmall WHERE bool_col = FALSE");
    // Subquery references an explicit alias from the outer block in the FROM
    // clause
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select * from t)", "Could not resolve table reference: 't'");
    // Uncorrelated subquery with no FROM clause
    AnalyzesOk("select * from functional.alltypes where exists (select 1,2)");
    // EXISTS subquery in a binary predicate
    AnalysisError("select * from functional.alltypes where " + "if(exists(select * from functional.alltypesagg), 1, 0) = 1", "IN and/or EXISTS subquery predicates are not supported in binary predicates: " + "if(EXISTS (SELECT * FROM functional.alltypesagg), 1, 0) = 1");
    // Correlated subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select 1 from functional.alltypesagg g where t.id = g.id limit 1)");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "exists (select int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "not exists (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#method_after
@Test
public void TestExistsSubqueries() throws AnalysisException {
    String[] existsOperators = { "exists", "not exists" };
    for (String op : existsOperators) {
        // [NOT] EXISTS predicate (correlated)
        AnalyzesOk(String.format("select * from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.id = t.id)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.int_col = t.int_col and p.bool_col = false)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col and a.bool_col = " + "t.bool_col)", op));
        // Multiple [NOT] EXISTS predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypessmall s where s.id = t.id) and " + "%s (select NULL from functional.alltypesagg g where t.int_col = g.int_col)", op, op));
        // OR between two subqueries
        AnalysisError(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id) or %s " + "(select * from functional.alltypessmall s where s.int_col = t.int_col)", op, op), String.format("Subqueries in OR predicates are not supported: %s " + "(SELECT * FROM functional.alltypesagg a WHERE a.id = t.id) OR %s (SELECT " + "* FROM functional.alltypessmall s WHERE s.int_col = t.int_col)", op.toUpperCase(), op.toUpperCase()));
        // Complex correlation predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id + 1) and " + "%s (select 1 from functional.alltypes s where s.int_col + s.bigint_col = " + "t.bigint_col + 1)", op, op));
        // Correlated predicates
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg g where t.int_col = g.int_col " + "and t.bool_col = false)", op));
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select id from functional.alltypessmall s where t.tinyint_col = " + "s.tinyint_col and t.bool_col)", op));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.int_col = s.int_col))", op, op));
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.bool_col = " + "s.bool_col))", op, op));
        // Correlated EXISTS subquery with a group by and aggregation
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t " + "where %s (select id, count(*) from functional.alltypesagg g where " + "t.id = g.id group by id)", op));
        // Correlated EXISTS subquery with an analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where %s (select min(bigint_col) over " + "(partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id)", op));
        // Correlated EXISTS subquery with an analytic function and a group by
        // clause
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where exists (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 " + "where t1.id = t2.id group by bigint_col, bool_col)", op));
        String[] nullOps = { "is null", "is not null" };
        for (String nullOp : nullOps) {
            // Uncorrelated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes where %s " + "(select * from functional.alltypestiny) %s and id < 5", op, nullOp));
            // Correlated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes t where " + "%s (select 1 from functional.alltypestiny s where t.id = s.id) " + "%s and t.bool_col = false", op, nullOp));
        }
    }
    // Uncorrelated EXISTS subquery with an analytic function
    AnalyzesOk("select * from functional.alltypestiny t " + "where EXISTS (select id, min(int_col) over (partition by bool_col) " + "from functional.alltypesagg a where bigint_col < 10)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        AnalysisError(String.format("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where t.id %s a.id)", cmpOp), String.format("Unsupported predicate with subquery: EXISTS (SELECT * FROM " + "functional.alltypesagg a WHERE t.id %s a.id)", cmpOp));
    }
    // Uncorrelated EXISTS in a query with GROUP BY
    AnalyzesOk("select id, count(*) from functional.alltypes t " + "where exists (select 1 from functional.alltypestiny where id < 5) group by id");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select int_col + 1 from functional.alltypessmall s where " + "t.int_col = 10)", "Unsupported predicate with subquery: EXISTS " + "(SELECT int_col + 1 FROM functional.alltypessmall s WHERE t.int_col = 10)");
    // Uncorrelated EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where exists " + "(select * from functional.alltypesagg where id < 10)");
    AnalyzesOk("select id from functional.alltypestiny where exists " + "(select id from functional.alltypessmall where bool_col = false)");
    AnalyzesOk("select 1 from functional.alltypestiny t where exists " + "(select 1 from functional.alltypessmall where id < 5)");
    AnalyzesOk("select 1 + 1 from functional.alltypestiny where exists " + "(select null from functional.alltypessmall where id != 5)");
    // Multiple nesting levels with uncorrelated EXISTS
    AnalyzesOk("select id from functional.alltypes where exists " + "(select id from functional.alltypestiny where int_col < 10 and exists (" + "select id from functional.alltypessmall where bool_col = true))");
    // Uncorrelated NOT EXISTS subquery
    AnalysisError("select * from functional.alltypestiny where not exists " + "(select 1 from functional.alltypessmall where bool_col = false)", "Unsupported uncorrelated NOT EXISTS subquery: SELECT 1 FROM " + "functional.alltypessmall WHERE bool_col = FALSE");
    // Subquery references an explicit alias from the outer block in the FROM
    // clause
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select * from t)", "Could not resolve table reference: 't'");
    // Uncorrelated subquery with no FROM clause
    AnalyzesOk("select * from functional.alltypes where exists (select 1,2)");
    // EXISTS subquery in a binary predicate
    AnalysisError("select * from functional.alltypes where " + "if(exists(select * from functional.alltypesagg), 1, 0) = 1", "EXISTS subquery predicates are not supported in binary predicates: " + "if(EXISTS (SELECT * FROM functional.alltypesagg), 1, 0) = 1");
    // Correlated subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select 1 from functional.alltypesagg g where t.id = g.id limit 1)");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "exists (select int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "not exists (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#end_block

#method_before
@Test
public void TestSubqueries() throws AnalysisException {
    // Test resolution of column references inside subqueries.
    // Correlated column references can be qualified or unqualified.
    AnalyzesOk("select * from functional.jointbl t where exists " + "(select id from functional.alltypes where id = test_id and id = t.test_id)");
    // Correlated column references are invalid outside of WHERE and ON clauses.
    AnalysisError("select * from functional.jointbl t where exists " + "(select t.test_id = id from functional.alltypes)", "Could not resolve column/field reference: 't.test_id'");
    AnalysisError("select * from functional.jointbl t where exists " + "(select count(*) from functional.alltypes group by t.test_id)", "Could not resolve column/field reference: 't.test_id'");
    AnalysisError("select * from functional.jointbl t where exists " + "(select 1 from functional.alltypes order by t.test_id limit 1)", "Could not resolve column/field reference: 't.test_id'");
    // EXISTS, IN and aggregate subqueries
    AnalyzesOk("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where a.int_col = " + "t.int_col) and t.bigint_col in (select bigint_col from " + "functional.alltypestiny s) and t.bool_col = false and " + "t.int_col = (select min(int_col) from functional.alltypesagg)");
    // Nested IN with an EXISTS subquery that contains an aggregate subquery
    AnalyzesOk("select count(*) from functional.alltypes t where t.id " + "in (select id from functional.alltypesagg a where a.int_col = " + "t.int_col and exists (select * from functional.alltypestiny s " + "where s.bool_col = a.bool_col and s.int_col = (select min(int_col) " + "from functional.alltypessmall where bigint_col = 10)))");
    // Nested EXISTS with an IN subquery that has a nested aggregate subquery
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where a.id in (select id " + "from functional.alltypestiny s where bool_col = false and " + "s.int_col < (select max(int_col) from functional.alltypessmall where " + "bigint_col < 100)) and a.int_col = t.int_col)");
    // Nested aggregate subqueries with EXISTS and IN subqueries
    AnalyzesOk("select count(*) from functional.alltypes t where t.int_col = " + "(select avg(g.int_col) * 2 from functional.alltypesagg g where g.id in " + "(select id from functional.alltypessmall s where exists (select " + "* from functional.alltypestiny a where a.int_col = s.int_col and " + "a.bigint_col < 10)))");
    // INSERT SELECT
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypes where id in (select id from " + "functional.alltypesagg a where a.bool_col = false)");
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypes t where int_col in (select int_col " + "from functional.alltypesagg a where a.id = t.id) and exists " + "(select * from functional.alltypestiny s where s.bigint_col = " + "t.bigint_col) and int_col < (select min(int_col) from functional.alltypes)");
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypestiny where id = (select 1) " + "union select * from functional.alltypestiny where id = (select 2)");
    // CTAS with correlated subqueries
    AnalyzesOk("create table functional.test_tbl as select * from " + "functional.alltypes t where t.id in (select id from functional.alltypesagg " + "a where a.int_col = t.int_col and a.bool_col = false) and not exists " + "(select * from functional.alltypestiny s where s.int_col = t.int_col) " + "and t.bigint_col = (select count(*) from functional.alltypessmall)");
    AnalyzesOk("create table functional.test_tbl as " + "select * from functional.alltypestiny where id = (select 1) " + "union select * from functional.alltypestiny where id = (select 2)");
    // Predicate with a child subquery in the HAVING clause
    AnalysisError("select id, count(*) from functional.alltypestiny t group by " + "id having count(*) > (select count(*) from functional.alltypesagg)", "Subqueries are not supported in the HAVING clause.");
    AnalysisError("select id, count(*) from functional.alltypestiny t group by " + "id having (select count(*) from functional.alltypesagg) > 10", "Subqueries are not supported in the HAVING clause.");
    // Subquery in the select list
    AnalysisError("select id, (select int_col from functional.alltypestiny) " + "from functional.alltypestiny", "Subqueries are not supported in the select list.");
    // Subquery in the GROUP BY clause
    AnalysisError("select id, count(*) from functional.alltypestiny " + "group by (select int_col from functional.alltypestiny)", "Subqueries are not supported in the GROUP BY clause.");
    // Subquery in the ORDER BY clause
    AnalysisError("select id from functional.alltypestiny " + "order by (select int_col from functional.alltypestiny)", "Subqueries are not supported in the ORDER BY clause.");
    // Subquery with an inline view
    AnalyzesOk("select id from functional.alltypestiny t where exists " + "(select * from (select id, int_col from functional.alltypesagg) a where " + "a.id < 10 and a.int_col = t.int_col)");
    // Inner block references an inline view in the outer block
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where t.int_col = (select count(*) from t)", "Could not resolve table reference: 't'");
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where t.int_col = (select count(*) from t) and " + "t.string_col in (select string_col from t)", "Could not resolve table reference: 't'");
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where exists (select * from t, functional.alltypesagg p where " + "t.id = p.id)", "Could not resolve table reference: 't'");
    // Subquery referencing a view
    AnalyzesOk("select * from functional.alltypes a where exists " + "(select * from functional.alltypes_view b where b.id = a.id)");
    // Same view referenced in both the inner and outer block
    AnalyzesOk("select * from functional.alltypes_view a where exists " + "(select * from functional.alltypes_view b where a.id = b.id)");
    // Union query with subqueries
    AnalyzesOk("select * from functional.alltypes where id = " + "(select max(id) from functional.alltypestiny) union " + "select * from functional.alltypes where id = " + "(select min(id) from functional.alltypessmall)");
    AnalyzesOk("select * from functional.alltypes where id = (select 1) " + "union all select * from functional.alltypes where id in " + "(select int_col from functional.alltypestiny)");
    AnalyzesOk("select * from functional.alltypes where id = (select 1) " + "union select * from (select * from functional.alltypes where id in " + "(select int_col from functional.alltypestiny)) t");
    // Union in the subquery
    AnalysisError("select * from functional.alltypes where exists " + "(select id from functional.alltypestiny union " + "select id from functional.alltypesagg)", "A subquery must contain a single select block: " + "(SELECT id FROM functional.alltypestiny UNION " + "SELECT id FROM functional.alltypesagg)");
    AnalysisError("select * from functional.alltypes where exists (values(1))", "A subquery must contain a single select block: (VALUES(1))");
    // Subquery in LIMIT
    AnalysisError("select * from functional.alltypes limit " + "(select count(*) from functional.alltypesagg)", "LIMIT expression must be a constant expression: " + "(SELECT count(*) FROM functional.alltypesagg)");
    // NOT predicates in conjunction with subqueries
    AnalyzesOk("select * from functional.alltypes t where t.id not in " + "(select id from functional.alltypesagg g where g.bool_col = false) " + "and t.string_col not like '%1%' and not (t.int_col < 5) " + "and not (t.int_col is null) and not (t.int_col between 5 and 10)");
    // IS NULL with an InPredicate that contains a subquery
    AnalysisError("select * from functional.alltypestiny t where (id in " + "(select id from functional.alltypes)) is null", "Unsupported IS NULL " + "predicate that contains a subquery: (id IN (SELECT id FROM " + "functional.alltypes)) IS NULL");
    // IS NULL with a BinaryPredicate that contains a subquery
    AnalyzesOk("select * from functional.alltypestiny where (id = " + "(select max(id) from functional.alltypessmall)) is null");
    // between predicates with subqueries
    AnalyzesOk("select * from functional.alltypestiny where " + "(select avg(id) from functional.alltypesagg where bool_col = true) " + "between 1 and 100 and int_col < 10");
    AnalyzesOk("select count(*) from functional.alltypestiny t where " + "(select count(id) from functional.alltypesagg g where t.id = g.id " + "and g.bigint_col < 10) between 1 and 1000");
    AnalyzesOk("select id from functional.alltypestiny where " + "int_col between (select min(int_col) from functional.alltypesagg where " + "id < 10) and 100 and bool_col = false");
    AnalyzesOk("select * from functional.alltypessmall s where " + "int_col between (select count(t.id) from functional.alltypestiny t where " + "t.int_col = s.int_col) and (select max(int_col) from " + "functional.alltypes a where a.id = s.id and a.bool_col = false)");
    AnalyzesOk("select * from functional.alltypessmall where " + "int_col between (select min(int_col) from functional.alltypestiny) and " + "(select max(int_col) from functional.alltypestiny) and bigint_col between " + "(select min(bigint_col) from functional.alltypesagg) and (select " + "max(bigint_col) from functional.alltypesagg)");
    AnalysisError("select * from functional.alltypestiny where (select min(id) " + "from functional.alltypes) between 1 and (select max(id) from " + "functional.alltypes)", "Comparison between subqueries is not supported " + "in a between predicate: (SELECT min(id) FROM functional.alltypes) BETWEEN " + "1 AND (SELECT max(id) FROM functional.alltypes)");
    AnalyzesOk("select * from functional.alltypestiny where " + "int_col between 0 and 10 and exists (select 1)");
    AnalyzesOk("select * from functional.alltypestiny a where " + "double_col between cast(1 as double) and cast(10 as double) and " + "exists (select 1 from functional.alltypessmall b where a.id = b.id)");
}
#method_after
@Test
public void TestSubqueries() throws AnalysisException {
    // Test resolution of column references inside subqueries.
    // Correlated column references can be qualified or unqualified.
    AnalyzesOk("select * from functional.jointbl t where exists " + "(select id from functional.alltypes where id = test_id and id = t.test_id)");
    // Correlated column references are invalid outside of WHERE and ON clauses.
    AnalysisError("select * from functional.jointbl t where exists " + "(select t.test_id = id from functional.alltypes)", "Could not resolve column/field reference: 't.test_id'");
    AnalysisError("select * from functional.jointbl t where test_zip in " + "(select count(*) from functional.alltypes group by t.test_id)", "Could not resolve column/field reference: 't.test_id'");
    AnalysisError("select * from functional.jointbl t where exists " + "(select 1 from functional.alltypes order by t.test_id limit 1)", "Could not resolve column/field reference: 't.test_id'");
    // Star exprs cannot reference an alias from a parent block.
    AnalysisError("select * from functional.jointbl t where exists " + "(select t.* from functional.alltypes)", "Could not resolve star expression: 't.*'");
    // EXISTS, IN and aggregate subqueries
    AnalyzesOk("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where a.int_col = " + "t.int_col) and t.bigint_col in (select bigint_col from " + "functional.alltypestiny s) and t.bool_col = false and " + "t.int_col = (select min(int_col) from functional.alltypesagg)");
    // Nested IN with an EXISTS subquery that contains an aggregate subquery
    AnalyzesOk("select count(*) from functional.alltypes t where t.id " + "in (select id from functional.alltypesagg a where a.int_col = " + "t.int_col and exists (select * from functional.alltypestiny s " + "where s.bool_col = a.bool_col and s.int_col = (select min(int_col) " + "from functional.alltypessmall where bigint_col = 10)))");
    // Nested EXISTS with an IN subquery that has a nested aggregate subquery
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where a.id in (select id " + "from functional.alltypestiny s where bool_col = false and " + "s.int_col < (select max(int_col) from functional.alltypessmall where " + "bigint_col < 100)) and a.int_col = t.int_col)");
    // Nested aggregate subqueries with EXISTS and IN subqueries
    AnalyzesOk("select count(*) from functional.alltypes t where t.int_col = " + "(select avg(g.int_col) * 2 from functional.alltypesagg g where g.id in " + "(select id from functional.alltypessmall s where exists (select " + "* from functional.alltypestiny a where a.int_col = s.int_col and " + "a.bigint_col < 10)))");
    // INSERT SELECT
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypes where id in (select id from " + "functional.alltypesagg a where a.bool_col = false)");
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypes t where int_col in (select int_col " + "from functional.alltypesagg a where a.id = t.id) and exists " + "(select * from functional.alltypestiny s where s.bigint_col = " + "t.bigint_col) and int_col < (select min(int_col) from functional.alltypes)");
    AnalyzesOk("insert into functional.alltypessmall partition (year, month) " + "select * from functional.alltypestiny where id = (select 1) " + "union select * from functional.alltypestiny where id = (select 2)");
    // CTAS with correlated subqueries
    AnalyzesOk("create table functional.test_tbl as select * from " + "functional.alltypes t where t.id in (select id from functional.alltypesagg " + "a where a.int_col = t.int_col and a.bool_col = false) and not exists " + "(select * from functional.alltypestiny s where s.int_col = t.int_col) " + "and t.bigint_col = (select count(*) from functional.alltypessmall)");
    AnalyzesOk("create table functional.test_tbl as " + "select * from functional.alltypestiny where id = (select 1) " + "union select * from functional.alltypestiny where id = (select 2)");
    // Predicate with a child subquery in the HAVING clause
    AnalysisError("select id, count(*) from functional.alltypestiny t group by " + "id having count(*) > (select count(*) from functional.alltypesagg)", "Subqueries are not supported in the HAVING clause.");
    AnalysisError("select id, count(*) from functional.alltypestiny t group by " + "id having (select count(*) from functional.alltypesagg) > 10", "Subqueries are not supported in the HAVING clause.");
    // Subquery in the select list
    AnalysisError("select id, (select int_col from functional.alltypestiny) " + "from functional.alltypestiny", "Subqueries are not supported in the select list.");
    // Subquery in the GROUP BY clause
    AnalysisError("select id, count(*) from functional.alltypestiny " + "group by (select int_col from functional.alltypestiny)", "Subqueries are not supported in the GROUP BY clause.");
    // Subquery in the ORDER BY clause
    AnalysisError("select id from functional.alltypestiny " + "order by (select int_col from functional.alltypestiny)", "Subqueries are not supported in the ORDER BY clause.");
    // Subquery with an inline view
    AnalyzesOk("select id from functional.alltypestiny t where exists " + "(select * from (select id, int_col from functional.alltypesagg) a where " + "a.id < 10 and a.int_col = t.int_col)");
    // Inner block references an inline view in the outer block
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where t.int_col = (select count(*) from t)", "Could not resolve table reference: 't'");
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where t.int_col = (select count(*) from t) and " + "t.string_col in (select string_col from t)", "Could not resolve table reference: 't'");
    AnalysisError("select id from (select * from functional.alltypestiny) t " + "where exists (select * from t, functional.alltypesagg p where " + "t.id = p.id)", "Could not resolve table reference: 't'");
    // Subquery referencing a view
    AnalyzesOk("select * from functional.alltypes a where exists " + "(select * from functional.alltypes_view b where b.id = a.id)");
    // Same view referenced in both the inner and outer block
    AnalyzesOk("select * from functional.alltypes_view a where exists " + "(select * from functional.alltypes_view b where a.id = b.id)");
    // Union query with subqueries
    AnalyzesOk("select * from functional.alltypes where id = " + "(select max(id) from functional.alltypestiny) union " + "select * from functional.alltypes where id = " + "(select min(id) from functional.alltypessmall)");
    AnalyzesOk("select * from functional.alltypes where id = (select 1) " + "union all select * from functional.alltypes where id in " + "(select int_col from functional.alltypestiny)");
    AnalyzesOk("select * from functional.alltypes where id = (select 1) " + "union select * from (select * from functional.alltypes where id in " + "(select int_col from functional.alltypestiny)) t");
    // Union in the subquery
    AnalysisError("select * from functional.alltypes where exists " + "(select id from functional.alltypestiny union " + "select id from functional.alltypesagg)", "A subquery must contain a single select block: " + "(SELECT id FROM functional.alltypestiny UNION " + "SELECT id FROM functional.alltypesagg)");
    AnalysisError("select * from functional.alltypes where exists (values(1))", "A subquery must contain a single select block: (VALUES(1))");
    // Subquery in LIMIT
    AnalysisError("select * from functional.alltypes limit " + "(select count(*) from functional.alltypesagg)", "LIMIT expression must be a constant expression: " + "(SELECT count(*) FROM functional.alltypesagg)");
    // NOT predicates in conjunction with subqueries
    AnalyzesOk("select * from functional.alltypes t where t.id not in " + "(select id from functional.alltypesagg g where g.bool_col = false) " + "and t.string_col not like '%1%' and not (t.int_col < 5) " + "and not (t.int_col is null) and not (t.int_col between 5 and 10)");
    // IS NULL with an InPredicate that contains a subquery
    AnalysisError("select * from functional.alltypestiny t where (id in " + "(select id from functional.alltypes)) is null", "Unsupported IS NULL " + "predicate that contains a subquery: (id IN (SELECT id FROM " + "functional.alltypes)) IS NULL");
    // IS NULL with a BinaryPredicate that contains a subquery
    AnalyzesOk("select * from functional.alltypestiny where (id = " + "(select max(id) from functional.alltypessmall)) is null");
    // between predicates with subqueries
    AnalyzesOk("select * from functional.alltypestiny where " + "(select avg(id) from functional.alltypesagg where bool_col = true) " + "between 1 and 100 and int_col < 10");
    AnalyzesOk("select count(*) from functional.alltypestiny t where " + "(select count(id) from functional.alltypesagg g where t.id = g.id " + "and g.bigint_col < 10) between 1 and 1000");
    AnalyzesOk("select id from functional.alltypestiny where " + "int_col between (select min(int_col) from functional.alltypesagg where " + "id < 10) and 100 and bool_col = false");
    AnalyzesOk("select * from functional.alltypessmall s where " + "int_col between (select count(t.id) from functional.alltypestiny t where " + "t.int_col = s.int_col) and (select max(int_col) from " + "functional.alltypes a where a.id = s.id and a.bool_col = false)");
    AnalyzesOk("select * from functional.alltypessmall where " + "int_col between (select min(int_col) from functional.alltypestiny) and " + "(select max(int_col) from functional.alltypestiny) and bigint_col between " + "(select min(bigint_col) from functional.alltypesagg) and (select " + "max(bigint_col) from functional.alltypesagg)");
    AnalysisError("select * from functional.alltypestiny where (select min(id) " + "from functional.alltypes) between 1 and (select max(id) from " + "functional.alltypes)", "Comparison between subqueries is not supported " + "in a between predicate: (SELECT min(id) FROM functional.alltypes) BETWEEN " + "1 AND (SELECT max(id) FROM functional.alltypes)");
    AnalyzesOk("select * from functional.alltypestiny where " + "int_col between 0 and 10 and exists (select 1)");
    AnalyzesOk("select * from functional.alltypestiny a where " + "double_col between cast(1 as double) and cast(10 as double) and " + "exists (select 1 from functional.alltypessmall b where a.id = b.id)");
}
#end_block

#method_before
@Override
protected String debugString() {
    return Objects.toStringHelper(this).add("tid", desc_.getId().asInt()).add("tblName", desc_.getParentTable().getFullName()).add("keyRanges", "").addValue(super.debugString()).toString();
}
#method_after
@Override
protected String debugString() {
    return Objects.toStringHelper(this).add("tid", desc_.getId().asInt()).add("tblName", desc_.getTable().getFullName()).add("keyRanges", "").addValue(super.debugString()).toString();
}
#end_block

#method_before
protected String getStatsExplainString(String prefix, TExplainLevel detailLevel) {
    StringBuilder output = new StringBuilder();
    // Table stats.
    if (desc_.getParentTable().getNumRows() == -1) {
        output.append(prefix + "table stats: unavailable");
    } else {
        output.append(prefix + "table stats: " + desc_.getParentTable().getNumRows() + " rows total");
        if (numPartitionsMissingStats_ > 0) {
            output.append(" (" + numPartitionsMissingStats_ + " partition(s) missing stats)");
        }
    }
    output.append("\n");
    // Column stats.
    List<String> columnsMissingStats = Lists.newArrayList();
    for (SlotDescriptor slot : desc_.getSlots()) {
        if (!slot.getStats().hasStats() && slot.getColumn() != null) {
            columnsMissingStats.add(slot.getColumn().getName());
        }
    }
    if (columnsMissingStats.isEmpty()) {
        output.append(prefix + "column stats: all");
    } else if (columnsMissingStats.size() == desc_.getSlots().size()) {
        output.append(prefix + "column stats: unavailable");
    } else {
        output.append(String.format("%scolumns missing stats: %s", prefix, Joiner.on(", ").join(columnsMissingStats)));
    }
    return output.toString();
}
#method_after
protected String getStatsExplainString(String prefix, TExplainLevel detailLevel) {
    StringBuilder output = new StringBuilder();
    // Table stats.
    if (desc_.getTable().getNumRows() == -1) {
        output.append(prefix + "table stats: unavailable");
    } else {
        output.append(prefix + "table stats: " + desc_.getTable().getNumRows() + " rows total");
        if (numPartitionsMissingStats_ > 0) {
            output.append(" (" + numPartitionsMissingStats_ + " partition(s) missing stats)");
        }
    }
    output.append("\n");
    // Column stats.
    List<String> columnsMissingStats = Lists.newArrayList();
    for (SlotDescriptor slot : desc_.getSlots()) {
        if (!slot.getStats().hasStats() && slot.getColumn() != null) {
            columnsMissingStats.add(slot.getColumn().getName());
        }
    }
    if (columnsMissingStats.isEmpty()) {
        output.append(prefix + "column stats: all");
    } else if (columnsMissingStats.size() == desc_.getSlots().size()) {
        output.append(prefix + "column stats: unavailable");
    } else {
        output.append(String.format("%scolumns missing stats: %s", prefix, Joiner.on(", ").join(columnsMissingStats)));
    }
    return output.toString();
}
#end_block

#method_before
public boolean isTableMissingTableStats() {
    if (desc_.getParentTable().getNumRows() == -1)
        return true;
    return numPartitionsMissingStats_ > 0;
}
#method_after
public boolean isTableMissingTableStats() {
    if (desc_.getTable().getNumRows() == -1)
        return true;
    return numPartitionsMissingStats_ > 0;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(getPrivilegeRequirement());
    desc_ = analyzer.registerTableRef(this);
    if (resolvedPath_.getRootDesc() != null) {
        SlotDescriptor parentSlotDesc = analyzer.registerSlotRef(resolvedPath_);
        SlotRef parentSlotRef = new SlotRef(parentSlotDesc);
        TableRef parentTableRef = analyzer.getTableRef(resolvedPath_.getRootDesc().getId());
        if (parentTableRef instanceof InlineViewRef) {
            InlineViewRef parentInlineViewRef = (InlineViewRef) parentTableRef;
            collectionExpr_ = parentSlotRef.substitute(parentInlineViewRef.getBaseTblSmap(), analyzer, false);
        } else {
            collectionExpr_ = parentSlotRef;
        }
        // Must always be materialized to ensure the correct cardinality after unnesting.
        analyzer.materializeSlots(collectionExpr_);
    }
    isAnalyzed_ = true;
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(getPrivilegeRequirement());
    desc_ = analyzer.registerTableRef(this);
    if (resolvedPath_.getRootDesc() != null) {
        SlotDescriptor parentSlotDesc = analyzer.registerSlotRef(resolvedPath_);
        collectionExpr_ = new SlotRef(parentSlotDesc);
        // Must always be materialized to ensure the correct cardinality after unnesting.
        analyzer.materializeSlots(collectionExpr_);
    }
    isAnalyzed_ = true;
    analyzeJoin(analyzer);
}
#end_block

#method_before
public TResultSet getTableStats() {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    for (int i = 0; i < numClusteringCols_; ++i) {
        // Add the partition-key values as strings for simplicity.
        Column partCol = getColumns().get(i);
        TColumn colDesc = new TColumn(partCol.getName(), Type.STRING.toThrift());
        resultSchema.addToColumns(colDesc);
    }
    resultSchema.addToColumns(new TColumn("#Rows", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("#Files", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("Size", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Bytes Cached", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Cache Replication", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Format", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Incremental stats", Type.STRING.toThrift()));
    // Pretty print partitions and their stats.
    ArrayList<HdfsPartition> orderedPartitions = Lists.newArrayList(partitions_);
    Collections.sort(orderedPartitions);
    long totalCachedBytes = 0L;
    for (HdfsPartition p : orderedPartitions) {
        // Ignore dummy default partition.
        if (p.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID)
            continue;
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        // Add the partition-key values (as strings for simplicity).
        for (LiteralExpr expr : p.getPartitionValues()) {
            rowBuilder.add(expr.getStringValue());
        }
        // Add number of rows, files, bytes, cache stats, and file format.
        rowBuilder.add(p.getNumRows()).add(p.getFileDescriptors().size()).addBytes(p.getSize());
        if (!p.isMarkedCached()) {
            // Helps to differentiate partitions that have 0B cached versus partitions
            // that are not marked as cached.
            rowBuilder.add("NOT CACHED");
            rowBuilder.add("NOT CACHED");
        } else {
            // Calculate the number the number of bytes that are cached.
            long cachedBytes = 0L;
            for (FileDescriptor fd : p.getFileDescriptors()) {
                for (THdfsFileBlock fb : fd.getFileBlocks()) {
                    if (fb.getIs_replica_cached().contains(true)) {
                        cachedBytes += fb.getLength();
                    }
                }
            }
            totalCachedBytes += cachedBytes;
            rowBuilder.addBytes(cachedBytes);
            // Extract cache replication factor from the parameters of the table
            // if the table is not partitioned or directly from the partition.
            Short rep = HdfsCachingUtil.getCachedCacheReplication(numClusteringCols_ == 0 ? p.getTable().getMetaStoreTable().getParameters() : p.getParameters());
            rowBuilder.add(rep.toString());
        }
        rowBuilder.add(p.getInputFormatDescriptor().getFileFormat().toString());
        rowBuilder.add(String.valueOf(p.hasIncrementalStats()));
        result.addToRows(rowBuilder.get());
    }
    // For partitioned tables add a summary row at the bottom.
    if (numClusteringCols_ > 0) {
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        int numEmptyCells = numClusteringCols_ - 1;
        rowBuilder.add("Total");
        for (int i = 0; i < numEmptyCells; ++i) {
            rowBuilder.add("");
        }
        // Total num rows, files, and bytes (leave format empty).
        rowBuilder.add(numRows_).add(numHdfsFiles_).addBytes(totalHdfsBytes_).addBytes(totalCachedBytes).add("").add("").add("");
        result.addToRows(rowBuilder.get());
    }
    return result;
}
#method_after
public TResultSet getTableStats() {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    for (int i = 0; i < numClusteringCols_; ++i) {
        // Add the partition-key values as strings for simplicity.
        Column partCol = getColumns().get(i);
        TColumn colDesc = new TColumn(partCol.getName(), Type.STRING.toThrift());
        resultSchema.addToColumns(colDesc);
    }
    resultSchema.addToColumns(new TColumn("#Rows", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("#Files", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("Size", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Bytes Cached", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Cache Replication", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Format", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Incremental stats", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Location", Type.STRING.toThrift()));
    // Pretty print partitions and their stats.
    ArrayList<HdfsPartition> orderedPartitions = Lists.newArrayList(partitions_);
    Collections.sort(orderedPartitions);
    long totalCachedBytes = 0L;
    for (HdfsPartition p : orderedPartitions) {
        // Ignore dummy default partition.
        if (p.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID)
            continue;
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        // Add the partition-key values (as strings for simplicity).
        for (LiteralExpr expr : p.getPartitionValues()) {
            rowBuilder.add(expr.getStringValue());
        }
        // Add number of rows, files, bytes, cache stats, and file format.
        rowBuilder.add(p.getNumRows()).add(p.getFileDescriptors().size()).addBytes(p.getSize());
        if (!p.isMarkedCached()) {
            // Helps to differentiate partitions that have 0B cached versus partitions
            // that are not marked as cached.
            rowBuilder.add("NOT CACHED");
            rowBuilder.add("NOT CACHED");
        } else {
            // Calculate the number the number of bytes that are cached.
            long cachedBytes = 0L;
            for (FileDescriptor fd : p.getFileDescriptors()) {
                for (THdfsFileBlock fb : fd.getFileBlocks()) {
                    if (fb.getIs_replica_cached().contains(true)) {
                        cachedBytes += fb.getLength();
                    }
                }
            }
            totalCachedBytes += cachedBytes;
            rowBuilder.addBytes(cachedBytes);
            // Extract cache replication factor from the parameters of the table
            // if the table is not partitioned or directly from the partition.
            Short rep = HdfsCachingUtil.getCachedCacheReplication(numClusteringCols_ == 0 ? p.getTable().getMetaStoreTable().getParameters() : p.getParameters());
            rowBuilder.add(rep.toString());
        }
        rowBuilder.add(p.getInputFormatDescriptor().getFileFormat().toString());
        rowBuilder.add(String.valueOf(p.hasIncrementalStats()));
        rowBuilder.add(p.getLocation());
        result.addToRows(rowBuilder.get());
    }
    // For partitioned tables add a summary row at the bottom.
    if (numClusteringCols_ > 0) {
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        int numEmptyCells = numClusteringCols_ - 1;
        rowBuilder.add("Total");
        for (int i = 0; i < numEmptyCells; ++i) {
            rowBuilder.add("");
        }
        // Total num rows, files, and bytes (leave format empty).
        rowBuilder.add(numRows_).add(numHdfsFiles_).addBytes(totalHdfsBytes_).addBytes(totalCachedBytes).add("").add("").add("").add("");
        result.addToRows(rowBuilder.get());
    }
    return result;
}
#end_block

#method_before
public boolean resolve() {
    Preconditions.checkState(rootDesc_ != null || rootTable_ != null);
    Type currentType = null;
    if (rootDesc_ != null) {
        currentType = rootDesc_.getType();
    } else {
        currentType = rootTable_.getType();
    }
    // Map all raw-path elements to field types and positions.
    for (int rawPathIdx = 0; rawPathIdx < rawPath_.size(); ++rawPathIdx) {
        if (!currentType.isComplexType())
            return false;
        StructType structType = getFields(currentType);
        StructField field = structType.getField(rawPath_.get(rawPathIdx));
        if (field == null)
            return false;
        matches_.add(field.getType());
        matchedPositions_.add(field.getPosition());
        if (field.getType().isCollectionType() && firstCollectionIdx_ == -1) {
            firstCollectionIdx_ = rawPathIdx;
        }
        currentType = field.getType();
    }
    return true;
}
#method_after
public boolean resolve() {
    if (isResolved_)
        return true;
    Preconditions.checkState(rootDesc_ != null || rootTable_ != null);
    Type currentType = null;
    int rawPathIdx = 0;
    if (rootPath_ != null) {
        // Continue resolving this path relative to the rootPath_.
        currentType = rootPath_.destType();
        rawPathIdx = rootPath_.getRawPath().size();
    } else if (rootDesc_ != null) {
        currentType = rootDesc_.getType();
    } else {
        // Directly start from the item type because only implicit paths are allowed.
        currentType = rootTable_.getType().getItemType();
    }
    // Map all remaining raw-path elements to field types and positions.
    while (rawPathIdx < rawPath_.size()) {
        if (!currentType.isComplexType())
            return false;
        StructType structType = getTypeAsStruct(currentType);
        // Resolve explicit path.
        StructField field = structType.getField(rawPath_.get(rawPathIdx));
        if (field == null) {
            // Resolve implicit path.
            if (structType instanceof CollectionStructType) {
                field = ((CollectionStructType) structType).getOptionalField();
            } else {
                // Failed to resolve implicit or explicit path.
                return false;
            }
            // Update the physical types/positions.
            matchedTypes_.add(field.getType());
            matchedPositions_.add(field.getPosition());
            currentType = field.getType();
            // Do not consume a raw-path element.
            continue;
        }
        matchedTypes_.add(field.getType());
        matchedPositions_.add(field.getPosition());
        if (field.getType().isCollectionType() && firstCollectionPathIdx_ == -1) {
            Preconditions.checkState(firstCollectionTypeIdx_ == -1);
            firstCollectionPathIdx_ = rawPathIdx;
            firstCollectionTypeIdx_ = matchedTypes_.size() - 1;
        }
        currentType = field.getType();
        ++rawPathIdx;
    }
    Preconditions.checkState(matchedTypes_.size() == matchedPositions_.size());
    Preconditions.checkState(matchedTypes_.size() >= rawPath_.size());
    isResolved_ = true;
    return true;
}
#end_block

#method_before
public List<Integer> getMatchedPositions() {
    return matchedPositions_;
}
#method_after
public List<Integer> getMatchedPositions() {
    Preconditions.checkState(isResolved_);
    return matchedPositions_;
}
#end_block

#method_before
public Type destType() {
    if (!matches_.isEmpty())
        return matches_.get(matches_.size() - 1);
    if (rootDesc_ != null)
        return rootDesc_.getType();
    if (rootTable_ != null)
        return rootTable_.getType();
    return null;
}
#method_after
public Type destType() {
    Preconditions.checkState(isResolved_);
    if (!matchedTypes_.isEmpty())
        return matchedTypes_.get(matchedTypes_.size() - 1);
    if (rootDesc_ != null)
        return rootDesc_.getType();
    if (rootTable_ != null)
        return rootTable_.getType();
    return null;
}
#end_block

#method_before
public Table destTable() {
    if (rootTable_ != null && rootDesc_ == null && matches_.isEmpty()) {
        return rootTable_;
    }
    return null;
}
#method_after
public Table destTable() {
    Preconditions.checkState(isResolved_);
    if (rootTable_ != null && rootDesc_ == null && matchedTypes_.isEmpty()) {
        return rootTable_;
    }
    return null;
}
#end_block

#method_before
public Column destColumn() {
    if (rootTable_ == null)
        return null;
    return rootTable_.getColumn(rawPath_.get(rawPath_.size() - 1));
}
#method_after
public Column destColumn() {
    Preconditions.checkState(isResolved_);
    if (rootTable_ == null || rawPath_.size() != 1)
        return null;
    return rootTable_.getColumn(rawPath_.get(rawPath_.size() - 1));
}
#end_block

#method_before
public TupleDescriptor destTupleDesc() {
    if (rootDesc_ != null && matches_.isEmpty())
        return rootDesc_;
    return null;
}
#method_after
public TupleDescriptor destTupleDesc() {
    Preconditions.checkState(isResolved_);
    if (rootDesc_ != null && matchedTypes_.isEmpty())
        return rootDesc_;
    return null;
}
#end_block

#method_before
private void testCollectionTableRefs(String collectionTable, String collectionField, boolean testInlineView) {
    TableName tbl = new TableName("functional", "allcomplextypes");
    // Collection table uses unqualified implicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL, allcomplextypes.%s", collectionField, collectionTable), tbl);
    // Collection table uses fully qualified implicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL, functional.allcomplextypes.%s", collectionField, collectionTable), tbl);
    // Collection table uses explicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL a, a.%s", collectionField, collectionTable), tbl);
    if (testInlineView) {
        // Parent table is an inline view.
        TblsAnalyzeOk(String.format("select %s from (select %s from $TBL) a, a.%s", collectionField, collectionTable, collectionTable), tbl);
    }
    // Parent/collection/collection join.
    TblsAnalyzeOk(String.format("select b.%s from $TBL a, a.%s b, a.int_map_col c", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select c.%s from $TBL a, a.int_array_col b, a.%s c", collectionField, collectionTable), tbl);
    // Test join types. Parent/collection joins do not require an ON or USING clause.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin())
            continue;
        TblsAnalyzeOk(String.format("select 1 from $TBL %s allcomplextypes.%s", joinOp, collectionTable), tbl);
        TblsAnalyzeOk(String.format("select 1 from $TBL a %s a.%s", joinOp, collectionTable), tbl);
    }
    // Legal, but not a parent/collection join.
    TblsAnalyzeOk(String.format("select %s from $TBL a, functional.allcomplextypes.%s", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select %s from $TBL.%s, functional.allcomplextypes", collectionField, collectionTable), tbl);
    // Non parent/collection outer or semi  joins require an ON or USING clause.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin() || joinOp.isCrossJoin() || joinOp.isInnerJoin()) {
            continue;
        }
        AnalysisError(String.format("select 1 from functional.allcomplextypes.%s %s functional.allcomplextypes", collectionTable, joinOp), String.format("%s requires an ON or USING clause", joinOp));
    }
    // Duplicate explicit alias.
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s a", collectionField, collectionTable), tbl, "Duplicate table alias: 'a'");
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s b, a.%s b", collectionField, collectionTable, collectionTable), tbl, "Duplicate table alias: 'b'");
    // Duplicate implicit alias.
    String[] childTblPath = collectionTable.split("\\.");
    String childTblAlias = childTblPath[childTblPath.length - 1];
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s, a.%s", collectionField, collectionTable, collectionTable), tbl, String.format("Duplicate table alias: '%s'", childTblAlias));
    TblsAnalysisError(String.format("select 1 from $TBL, allcomplextypes.%s, functional.allcomplextypes.%s", collectionTable, collectionTable), tbl, String.format("Duplicate table alias: '%s'", childTblAlias));
    // Duplicate implicit/explicit alias.
    TblsAnalysisError(String.format("select %s from $TBL, functional.allcomplextypes.%s allcomplextypes", collectionField, collectionTable), tbl, "Duplicate table alias: 'allcomplextypes'");
    // Parent/collection join requires the child to use an alias of the parent.
    AnalysisError(String.format("select %s from allcomplextypes, %s", collectionField, collectionTable), createAnalyzer("functional"), String.format("Could not resolve table reference: '%s'", collectionTable));
    AnalysisError(String.format("select %s from functional.allcomplextypes, %s", collectionField, collectionTable), String.format("Could not resolve table reference: '%s'", collectionTable));
    // Collection table must use explicit alias of the parent.
    AnalysisError(String.format("select item from allcomplextypes a, allcomplextypes.%s", collectionField), createAnalyzer("functional"), String.format("Could not resolve table reference: 'allcomplextypes.%s'", collectionField));
    AnalysisError(String.format("select item from functional.allcomplextypes a, allcomplextypes.%s", collectionField), String.format("Could not resolve table reference: 'allcomplextypes.%s'", collectionField));
    // Ambiguous collection table ref.
    AnalysisError(String.format("select %s from functional.allcomplextypes, " + "functional_parquet.allcomplextypes, allcomplextypes.%s", collectionField, collectionTable), "Unqualified table alias is ambiguous: 'allcomplextypes'");
}
#method_after
private void testCollectionTableRefs(String collectionTable, String collectionField) {
    TableName tbl = new TableName("functional", "allcomplextypes");
    // Collection table uses unqualified implicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL, allcomplextypes.%s", collectionField, collectionTable), tbl);
    // Collection table uses fully qualified implicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL, functional.allcomplextypes.%s", collectionField, collectionTable), tbl);
    // Collection table uses explicit alias of parent table.
    TblsAnalyzeOk(String.format("select %s from $TBL a, a.%s", collectionField, collectionTable), tbl);
    // Parent/collection/collection join.
    TblsAnalyzeOk(String.format("select b.%s from $TBL a, a.%s b, a.int_map_col c", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select c.%s from $TBL a, a.int_array_col b, a.%s c", collectionField, collectionTable), tbl);
    // Test join types. Parent/collection joins do not require an ON or USING clause.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin())
            continue;
        TblsAnalyzeOk(String.format("select 1 from $TBL %s allcomplextypes.%s", joinOp, collectionTable), tbl);
        TblsAnalyzeOk(String.format("select 1 from $TBL a %s a.%s", joinOp, collectionTable), tbl);
    }
    // Legal, but not a parent/collection join.
    TblsAnalyzeOk(String.format("select %s from $TBL a, functional.allcomplextypes.%s", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select %s from $TBL.%s, functional.allcomplextypes", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select %s from functional.allcomplextypes a, $TBL.%s", collectionField, collectionTable), tbl);
    TblsAnalyzeOk(String.format("select %s from functional.allcomplextypes.%s, $TBL", collectionField, collectionTable), tbl);
    // Non parent/collection outer or semi  joins require an ON or USING clause.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin() || joinOp.isCrossJoin() || joinOp.isInnerJoin()) {
            continue;
        }
        AnalysisError(String.format("select 1 from functional.allcomplextypes.%s %s functional.allcomplextypes", collectionTable, joinOp), String.format("%s requires an ON or USING clause", joinOp));
    }
    // Duplicate explicit alias.
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s a", collectionField, collectionTable), tbl, "Duplicate table alias: 'a'");
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s b, a.%s b", collectionField, collectionTable, collectionTable), tbl, "Duplicate table alias: 'b'");
    // Duplicate implicit alias.
    String[] childTblPath = collectionTable.split("\\.");
    String childTblAlias = childTblPath[childTblPath.length - 1];
    TblsAnalysisError(String.format("select %s from $TBL a, a.%s, a.%s", collectionField, collectionTable, collectionTable), tbl, String.format("Duplicate table alias: '%s'", childTblAlias));
    TblsAnalysisError(String.format("select 1 from $TBL, allcomplextypes.%s, functional.allcomplextypes.%s", collectionTable, collectionTable), tbl, String.format("Duplicate table alias: '%s'", childTblAlias));
    // Duplicate implicit/explicit alias.
    TblsAnalysisError(String.format("select %s from $TBL, functional.allcomplextypes.%s allcomplextypes", collectionField, collectionTable), tbl, "Duplicate table alias: 'allcomplextypes'");
    // Parent/collection join requires the child to use an alias of the parent.
    AnalysisError(String.format("select %s from allcomplextypes, %s", collectionField, collectionTable), createAnalyzer("functional"), String.format("Could not resolve table reference: '%s'", collectionTable));
    AnalysisError(String.format("select %s from functional.allcomplextypes, %s", collectionField, collectionTable), String.format("Could not resolve table reference: '%s'", collectionTable));
    // Ambiguous collection table ref.
    AnalysisError(String.format("select %s from functional.allcomplextypes, " + "functional_parquet.allcomplextypes, allcomplextypes.%s", collectionField, collectionTable), "Unqualified table alias is ambiguous: 'allcomplextypes'");
}
#end_block

#method_before
private void testAllTableAliases(String[] tables, String[] columns) throws AnalysisException {
    for (String tbl : tables) {
        TableName tblName = new TableName("functional", tbl);
        String uqAlias = tbl.substring(tbl.lastIndexOf(".") + 1);
        String fqAlias = "functional." + tbl;
        // True if 'tbl' refers to a collection, false otherwise. A value of false implies
        // the table must be a base table or view.
        boolean isCollectionTblRef = isCollectionTableRef(tbl);
        for (String col : columns) {
            // Test implicit table aliases with unqualified and fully-qualified table names.
            TblsAnalyzeOk(String.format("select %s from $TBL", col), tblName);
            TblsAnalyzeOk(String.format("select %s.%s from $TBL", uqAlias, col), tblName);
            // Only references to base tables/views have a fully-qualified implicit alias.
            if (!isCollectionTblRef) {
                TblsAnalyzeOk(String.format("select %s.%s from $TBL", fqAlias, col), tblName);
            }
            // Explicit table alias.
            TblsAnalyzeOk(String.format("select %s from $TBL a", col), tblName);
            TblsAnalyzeOk(String.format("select a.%s from $TBL a", col), tblName);
            String errRefStr = "column/field reference";
            if (col.endsWith("*"))
                errRefStr = "star expression";
            // Explicit table alias must be used.
            TblsAnalysisError(String.format("select %s.%s from $TBL a", uqAlias, col, tbl), tblName, String.format("Could not resolve %s: '%s.%s'", errRefStr, uqAlias, col));
            TblsAnalysisError(String.format("select %s.%s from $TBL a", uqAlias, col, tbl), tblName, String.format("Could not resolve %s: '%s.%s'", errRefStr, uqAlias, col));
        }
    }
    // Test that multiple implicit fully-qualified aliases work.
    for (String t1 : tables) {
        for (String t2 : tables) {
            if (t1.equals(t2))
                continue;
            // Collection tables do not have a fully-qualified implicit alias.
            if (isCollectionTableRef(t1) && isCollectionTableRef(t2))
                continue;
            for (String col : columns) {
                AnalyzesOk(String.format("select functional.%s.%s, functional.%s.%s " + "from functional.%s, functional.%s", t1, col, t2, col, t1, t2));
            }
        }
    }
    String col = columns[0];
    for (String tbl : tables) {
        TableName tblName = new TableName("functional", tbl);
        // Make sure a column reference requires an existing table alias.
        TblsAnalysisError("select alltypessmall.int_col from $TBL", tblName, "Could not resolve column/field reference: 'alltypessmall.int_col'");
        // Duplicate explicit alias.
        TblsAnalysisError(String.format("select a.%s from $TBL a, functional.testtbl a", col), tblName, "Duplicate table alias");
        // Duplicate implicit alias.
        TblsAnalysisError(String.format("select %s from $TBL, $TBL", col), tblName, "Duplicate table alias");
        // Duplicate implicit/explicit alias.
        String uqAlias = tbl.substring(tbl.lastIndexOf(".") + 1);
        TblsAnalysisError(String.format("select %s.%s from $TBL, functional.testtbl %s", tbl, col, uqAlias), tblName, "Duplicate table alias");
    }
}
#method_after
private void testAllTableAliases(String[] tables, String[] columns) throws AnalysisException {
    for (String tbl : tables) {
        TableName tblName = new TableName("functional", tbl);
        String uqAlias = tbl.substring(tbl.lastIndexOf(".") + 1);
        String fqAlias = "functional." + tbl;
        // True if 'tbl' refers to a collection, false otherwise. A value of false implies
        // the table must be a base table or view.
        boolean isCollectionTblRef = isCollectionTableRef(tbl);
        for (String col : columns) {
            // Test implicit table aliases with unqualified and fully-qualified table names.
            TblsAnalyzeOk(String.format("select %s from $TBL", col), tblName);
            TblsAnalyzeOk(String.format("select %s.%s from $TBL", uqAlias, col), tblName);
            // Only references to base tables/views have a fully-qualified implicit alias.
            if (!isCollectionTblRef) {
                TblsAnalyzeOk(String.format("select %s.%s from $TBL", fqAlias, col), tblName);
            }
            // Explicit table alias.
            TblsAnalyzeOk(String.format("select %s from $TBL a", col), tblName);
            TblsAnalyzeOk(String.format("select a.%s from $TBL a", col), tblName);
            String errRefStr = "column/field reference";
            if (col.endsWith("*"))
                errRefStr = "star expression";
            // Explicit table alias must be used.
            TblsAnalysisError(String.format("select %s.%s from $TBL a", uqAlias, col, tbl), tblName, String.format("Could not resolve %s: '%s.%s'", errRefStr, uqAlias, col));
            TblsAnalysisError(String.format("select %s.%s from $TBL a", fqAlias, col, tbl), tblName, String.format("Could not resolve %s: '%s.%s'", errRefStr, fqAlias, col));
        }
    }
    // Test that multiple implicit fully-qualified aliases work.
    for (String t1 : tables) {
        for (String t2 : tables) {
            if (t1.equals(t2))
                continue;
            // Collection tables do not have a fully-qualified implicit alias.
            if (isCollectionTableRef(t1) && isCollectionTableRef(t2))
                continue;
            for (String col : columns) {
                AnalyzesOk(String.format("select functional.%s.%s, functional.%s.%s " + "from functional.%s, functional.%s", t1, col, t2, col, t1, t2));
            }
        }
    }
    String col = columns[0];
    for (String tbl : tables) {
        TableName tblName = new TableName("functional", tbl);
        // Make sure a column reference requires an existing table alias.
        TblsAnalysisError("select alltypessmall.int_col from $TBL", tblName, "Could not resolve column/field reference: 'alltypessmall.int_col'");
        // Duplicate explicit alias.
        TblsAnalysisError(String.format("select a.%s from $TBL a, functional.testtbl a", col), tblName, "Duplicate table alias");
        // Duplicate implicit alias.
        TblsAnalysisError(String.format("select %s from $TBL, $TBL", col), tblName, "Duplicate table alias");
        // Duplicate implicit/explicit alias.
        String uqAlias = tbl.substring(tbl.lastIndexOf(".") + 1);
        TblsAnalysisError(String.format("select %s.%s from $TBL, functional.testtbl %s", tbl, col, uqAlias), tblName, "Duplicate table alias");
    }
}
#end_block

#method_before
@Test
public void TestCollectionTableRefs() throws AnalysisException {
    // Test ARRAY type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_array_col", "allcomplextypes_view.int_array_col" }, new String[] { "item", "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_array_col", "allcomplextypes_view.struct_array_col" }, new String[] { "f1", "f2", "*" });
    // Test MAP type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_map_col", "allcomplextypes_view.int_map_col" }, new String[] { "key", "value", "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_map_col", "allcomplextypes_view.struct_map_col" }, new String[] { "key", "f1", "f2", "*" });
    // Test complex table ref path with structs and multiple collections.
    testAllTableAliases(new String[] { "allcomplextypes.complex_nested_struct_col.f2.f12", "allcomplextypes_view.complex_nested_struct_col.f2.f12" }, new String[] { "key", "f21", "*" });
    // Test resolution of collection table refs.
    testCollectionTableRefs("int_array_col", "item", true);
    testCollectionTableRefs("int_map_col", "key", true);
    testCollectionTableRefs("complex_nested_struct_col.f2.f12", "f21", false);
    // Prefer reporting duplicate alias over non-collection type.
    AnalysisError("select 1 from functional.allcomplextypes a, a", "Duplicate table alias: 'a'");
    // Invalid reference to non-collection type.
    AnalysisError("select 1 from functional.allcomplextypes.int_struct_col", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_struct_col'\n" + "Path resolved to type: STRUCT<f1:INT,f2:INT>");
    AnalysisError("select 1 from functional.allcomplextypes a, a.int_struct_col", "Illegal table reference to non-collection type: 'a.int_struct_col'\n" + "Path resolved to type: STRUCT<f1:INT,f2:INT>");
    AnalysisError("select 1 from functional.allcomplextypes.int_array_col.item", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_array_col.item'\n" + "Path resolved to type: INT");
    AnalysisError("select 1 from functional.allcomplextypes.int_array_col a, a.item", "Illegal table reference to non-collection type: 'a.item'\n" + "Path resolved to type: INT");
    AnalysisError("select 1 from functional.allcomplextypes.int_map_col.key", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_map_col.key'\n" + "Path resolved to type: STRING");
    AnalysisError("select 1 from functional.allcomplextypes.int_map_col a, a.key", "Illegal table reference to non-collection type: 'a.key'\n" + "Path resolved to type: STRING");
    // Test that parent/collection joins without an ON clause analyze ok.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin())
            continue;
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.int_array_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.struct_array_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.int_map_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.struct_map_col", joinOp));
    }
}
#method_after
@Test
public void TestCollectionTableRefs() throws AnalysisException {
    // Test ARRAY type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_array_col" }, new String[] { Path.ARRAY_POS_FIELD_NAME, Path.ARRAY_ITEM_FIELD_NAME, "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_array_col" }, new String[] { "f1", "f2", "*" });
    // Test MAP type referenced as a table.
    testAllTableAliases(new String[] { "allcomplextypes.int_map_col" }, new String[] { Path.MAP_KEY_FIELD_NAME, Path.MAP_VALUE_FIELD_NAME, "*" });
    testAllTableAliases(new String[] { "allcomplextypes.struct_map_col" }, new String[] { Path.MAP_KEY_FIELD_NAME, "f1", "f2", "*" });
    // Test complex table ref path with structs and multiple collections.
    testAllTableAliases(new String[] { "allcomplextypes.complex_nested_struct_col.f2.f12" }, new String[] { Path.MAP_KEY_FIELD_NAME, "f21", "*" });
    // Test resolution of collection table refs.
    testCollectionTableRefs("int_array_col", Path.ARRAY_POS_FIELD_NAME);
    testCollectionTableRefs("int_array_col", Path.ARRAY_ITEM_FIELD_NAME);
    testCollectionTableRefs("int_map_col", Path.MAP_KEY_FIELD_NAME);
    testCollectionTableRefs("complex_nested_struct_col.f2.f12", "f21");
    // Path resolution error is reported before duplicate alias.
    AnalysisError("select 1 from functional.allcomplextypes a, a", "Illegal table reference to non-collection type: 'a'");
    // Invalid reference to non-collection type.
    AnalysisError("select 1 from functional.allcomplextypes.int_struct_col", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_struct_col'\n" + "Path resolved to type: STRUCT<f1:INT,f2:INT>");
    AnalysisError("select 1 from functional.allcomplextypes a, a.int_struct_col", "Illegal table reference to non-collection type: 'a.int_struct_col'\n" + "Path resolved to type: STRUCT<f1:INT,f2:INT>");
    AnalysisError("select 1 from functional.allcomplextypes.int_array_col.item", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_array_col.item'\n" + "Path resolved to type: INT");
    AnalysisError("select 1 from functional.allcomplextypes.int_array_col a, a.pos", "Illegal table reference to non-collection type: 'a.pos'\n" + "Path resolved to type: BIGINT");
    AnalysisError("select 1 from functional.allcomplextypes.int_map_col.key", "Illegal table reference to non-collection type: " + "'functional.allcomplextypes.int_map_col.key'\n" + "Path resolved to type: STRING");
    AnalysisError("select 1 from functional.allcomplextypes.int_map_col a, a.key", "Illegal table reference to non-collection type: 'a.key'\n" + "Path resolved to type: STRING");
    // Test that parent/collection joins without an ON clause analyze ok.
    for (JoinOperator joinOp : JoinOperator.values()) {
        if (joinOp.isNullAwareLeftAntiJoin())
            continue;
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.int_array_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.struct_array_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.int_map_col b", joinOp));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes a %s a.struct_map_col", joinOp));
    }
}
#end_block

#method_before
@Test
public void TestCatalogTableRefs() throws AnalysisException {
    String[] tables = new String[] { "alltypes", "alltypes_view" };
    String[] columns = new String[] { "int_col", "*" };
    testAllTableAliases(tables, columns);
    // Unqualified '*' is not ambiguous.
    AnalyzesOk("select * from functional.alltypes " + "cross join functional_parquet.alltypes");
    // Ambiguous unqualified column reference.
    AnalysisError("select int_col from functional.alltypes " + "cross join functional_parquet.alltypes", "Column/field reference is ambiguous: 'int_col'");
    // Ambiguous implicit unqualified table alias.
    AnalysisError("select alltypes.int_col from functional.alltypes " + "cross join functional_parquet.alltypes", "Unqualified table alias is ambiguous: 'alltypes'");
    AnalysisError("select alltypes.* from functional.alltypes " + "cross join functional_parquet.alltypes", "Unqualified table alias is ambiguous: 'alltypes'");
    // Mixing unqualified and fully-qualified table refs without explicit aliases is an
    // error because we'd expect a consistent result if we created a view of this stmt
    // (table names are fully qualified during view creation).
    AnalysisError("select alltypes.smallint_col, functional.alltypes.int_col " + "from alltypes inner join functional.alltypes " + "on (alltypes.id = functional.alltypes.id)", createAnalyzer("functional"), "Duplicate table alias: 'alltypes'");
}
#method_after
@Test
public void TestCatalogTableRefs() throws AnalysisException {
    String[] tables = new String[] { "alltypes", "alltypes_view" };
    String[] columns = new String[] { "int_col", "*" };
    testAllTableAliases(tables, columns);
    // Unqualified '*' is not ambiguous.
    AnalyzesOk("select * from functional.alltypes " + "cross join functional_parquet.alltypes");
    // Ambiguous unqualified column reference.
    AnalysisError("select int_col from functional.alltypes " + "cross join functional_parquet.alltypes", "Column/field reference is ambiguous: 'int_col'");
    // Ambiguous implicit unqualified table alias.
    AnalysisError("select alltypes.int_col from functional.alltypes " + "cross join functional_parquet.alltypes", "Unqualified table alias is ambiguous: 'alltypes'");
    AnalysisError("select alltypes.* from functional.alltypes " + "cross join functional_parquet.alltypes", "Unqualified table alias is ambiguous: 'alltypes'");
    // Mixing unqualified and fully-qualified table refs without explicit aliases is an
    // error because we'd expect a consistent result if we created a view of this stmt
    // (table names are fully qualified during view creation).
    AnalysisError("select alltypes.smallint_col, functional.alltypes.int_col " + "from alltypes inner join functional.alltypes " + "on (alltypes.id = functional.alltypes.id)", createAnalyzer("functional"), "Duplicate table alias: 'functional.alltypes'");
}
#end_block

#method_before
@Test
public void TestStructFields() throws AnalysisException {
    String[] tables = new String[] { "allcomplextypes", "allcomplextypes_view" };
    String[] columns = new String[] { "id", "int_struct_col.f1", "nested_struct_col.f2.f12.f21" };
    testAllTableAliases(tables, columns);
    // Unknown struct fields.
    AnalysisError("select nested_struct_col.badfield from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.badfield'");
    AnalysisError("select nested_struct_col.f2.badfield from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.f2.badfield'");
    AnalysisError("select nested_struct_col.badfield.f2 from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.badfield.f2'");
    // Illegal intermediate reference to collection type.
    AnalysisError("select int_array_col.item from functional.allcomplextypes", "Illegal column/field reference 'int_array_col.item' with intermediate " + "collection 'int_array_col' of type 'ARRAY<INT>'");
    AnalysisError("select struct_array_col.f1 from functional.allcomplextypes", "Illegal column/field reference 'struct_array_col.f1' with intermediate " + "collection 'struct_array_col' of type 'ARRAY<STRUCT<f1:BIGINT,f2:STRING>>'");
    AnalysisError("select int_map_col.key from functional.allcomplextypes", "Illegal column/field reference 'int_map_col.key' with intermediate " + "collection 'int_map_col' of type 'MAP<STRING,INT>'");
    AnalysisError("select struct_map_col.f1 from functional.allcomplextypes", "Illegal column/field reference 'struct_map_col.f1' with intermediate " + "collection 'struct_map_col' of type 'MAP<STRING,STRUCT<f1:BIGINT,f2:STRING>>'");
    AnalysisError("select complex_nested_struct_col.f2.f11 from functional.allcomplextypes", "Illegal column/field reference 'complex_nested_struct_col.f2.f11' with " + "intermediate collection 'f2' of type " + "'ARRAY<STRUCT<f11:BIGINT,f12:MAP<STRING,STRUCT<f21:BIGINT>>>>'");
    AnalysisError("select complex_nested_struct_col.f2.f11 from functional.allcomplextypes", "Illegal column/field reference 'complex_nested_struct_col.f2.f11' with " + "intermediate collection 'f2' of type " + "'ARRAY<STRUCT<f11:BIGINT,f12:MAP<STRING,STRUCT<f21:BIGINT>>>>'");
}
#method_after
@Test
public void TestStructFields() throws AnalysisException {
    String[] tables = new String[] { "allcomplextypes" };
    String[] columns = new String[] { "id", "int_struct_col.f1", "nested_struct_col.f2.f12.f21" };
    testAllTableAliases(tables, columns);
    // Unknown struct fields.
    AnalysisError("select nested_struct_col.badfield from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.badfield'");
    AnalysisError("select nested_struct_col.f2.badfield from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.f2.badfield'");
    AnalysisError("select nested_struct_col.badfield.f2 from functional.allcomplextypes", "Could not resolve column/field reference: 'nested_struct_col.badfield.f2'");
    // Illegal intermediate reference to collection type.
    AnalysisError("select int_array_col.item from functional.allcomplextypes", "Illegal column/field reference 'int_array_col.item' with intermediate " + "collection 'int_array_col' of type 'ARRAY<INT>'");
    AnalysisError("select struct_array_col.f1 from functional.allcomplextypes", "Illegal column/field reference 'struct_array_col.f1' with intermediate " + "collection 'struct_array_col' of type 'ARRAY<STRUCT<f1:BIGINT,f2:STRING>>'");
    AnalysisError("select int_map_col.key from functional.allcomplextypes", "Illegal column/field reference 'int_map_col.key' with intermediate " + "collection 'int_map_col' of type 'MAP<STRING,INT>'");
    AnalysisError("select struct_map_col.f1 from functional.allcomplextypes", "Illegal column/field reference 'struct_map_col.f1' with intermediate " + "collection 'struct_map_col' of type 'MAP<STRING,STRUCT<f1:BIGINT,f2:STRING>>'");
    AnalysisError("select complex_nested_struct_col.f2.f11 from functional.allcomplextypes", "Illegal column/field reference 'complex_nested_struct_col.f2.f11' with " + "intermediate collection 'f2' of type " + "'ARRAY<STRUCT<f11:BIGINT,f12:MAP<STRING,STRUCT<f21:BIGINT>>>>'");
    AnalysisError("select complex_nested_struct_col.f2.f11 from functional.allcomplextypes", "Illegal column/field reference 'complex_nested_struct_col.f2.f11' with " + "intermediate collection 'f2' of type " + "'ARRAY<STRUCT<f11:BIGINT,f12:MAP<STRING,STRUCT<f21:BIGINT>>>>'");
}
#end_block

#method_before
@Test
public void TestStar() throws AnalysisException {
    AnalyzesOk("select * from functional.AllTypes");
    AnalyzesOk("select functional.alltypes.* from functional.AllTypes");
    // different db
    AnalyzesOk("select functional_seq.alltypes.* from functional_seq.alltypes");
    // two tables w/ identical names from different dbs
    AnalyzesOk("select functional.alltypes.*, functional_seq.alltypes.* " + "from functional.alltypes, functional_seq.alltypes");
    AnalyzesOk("select * from functional.alltypes, functional_seq.alltypes");
    // result includes complex types (star does not auto-expand structs)
    AnalysisError("select * from functional.allcomplextypes", "Expr 'functional.allcomplextypes.int_array_col' in select list of " + "root statement returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
    // '*' without from clause has no meaning.
    AnalysisError("select *", "'*' expression in select list requires FROM clause.");
    AnalysisError("select 1, *, 2+4", "'*' expression in select list requires FROM clause.");
    AnalysisError("select a.*", "Could not resolve star expression: 'a.*'");
    // invalid star expansions
    AnalysisError("select functional.* from functional.alltypes", "Could not resolve star expression: 'functional.*'");
    AnalysisError("select int_col.* from functional.alltypes", "Cannot expand star in 'int_col.*' because " + "path 'int_col' resolved to type 'INT'.\n" + "Star expansion is only valid for paths to a struct type.");
    AnalysisError("select complex_struct_col.f2.* from functional.allcomplextypes", "Cannot expand star in 'complex_struct_col.f2.*' because " + "path 'complex_struct_col.f2' resolved to type 'ARRAY<INT>'.\n" + "Star expansion is only valid for paths to a struct type.");
    for (String joinType : new String[] { "left semi join", "left anti join" }) {
        // ignore semi-/anti-joined tables in unqualified '*' expansion
        SelectStmt stmt = (SelectStmt) AnalyzesOk(String.format("select * from functional.alltypes a " + "%s functional.testtbl b on (a.id = b.id)", joinType));
        // expect to have as many result exprs as alltypes has columns
        assertEquals(13, stmt.getResultExprs().size());
        // cannot expand '*" for a semi-/anti-joined table
        AnalysisError(String.format("select a.*, b.* from functional.alltypes a " + "%s functional.alltypes b on (a.id = b.id)", joinType), "Illegal star expression 'b.*' referencing semi-/anti-joined table 'b'");
    }
    for (String joinType : new String[] { "right semi join", "right anti join" }) {
        // ignore semi-/anti-joined tables in unqualified '*' expansion
        SelectStmt stmt = (SelectStmt) AnalyzesOk(String.format("select * from functional.alltypes a " + "%s functional.testtbl b on (a.id = b.id)", joinType));
        // expect to have as many result exprs as testtbl has columns
        assertEquals(3, stmt.getResultExprs().size());
        // cannot expand '*" for a semi-/anti-joined table
        AnalysisError(String.format("select a.*, b.* from functional.alltypes a " + "%s functional.alltypes b on (a.id = b.id)", joinType), "Illegal star expression 'a.*' referencing semi-/anti-joined table 'a'");
    }
}
#method_after
@Test
public void TestStar() throws AnalysisException {
    AnalyzesOk("select * from functional.AllTypes");
    AnalyzesOk("select functional.alltypes.* from functional.AllTypes");
    // different db
    AnalyzesOk("select functional_seq.alltypes.* from functional_seq.alltypes");
    // two tables w/ identical names from different dbs
    AnalyzesOk("select functional.alltypes.*, functional_seq.alltypes.* " + "from functional.alltypes, functional_seq.alltypes");
    AnalyzesOk("select * from functional.alltypes, functional_seq.alltypes");
    // expand '*' on a struct-typed column
    AnalyzesOk("select int_struct_col.* from functional.allcomplextypes");
    AnalyzesOk("select a.int_struct_col.* from functional.allcomplextypes a");
    AnalyzesOk("select allcomplextypes.int_struct_col.* from functional.allcomplextypes");
    AnalyzesOk("select functional.allcomplextypes.int_struct_col.* " + "from functional.allcomplextypes");
    // '*' without from clause has no meaning.
    AnalysisError("select *", "'*' expression in select list requires FROM clause.");
    AnalysisError("select 1, *, 2+4", "'*' expression in select list requires FROM clause.");
    AnalysisError("select a.*", "Could not resolve star expression: 'a.*'");
    // invalid star expansions
    AnalysisError("select functional.* from functional.alltypes", "Could not resolve star expression: 'functional.*'");
    AnalysisError("select int_col.* from functional.alltypes", "Cannot expand star in 'int_col.*' because " + "path 'int_col' resolved to type 'INT'.\n" + "Star expansion is only valid for paths to a struct type.");
    AnalysisError("select complex_struct_col.f2.* from functional.allcomplextypes", "Cannot expand star in 'complex_struct_col.f2.*' because " + "path 'complex_struct_col.f2' resolved to type 'ARRAY<INT>'.\n" + "Star expansion is only valid for paths to a struct type.");
    for (String joinType : new String[] { "left semi join", "left anti join" }) {
        // ignore semi-/anti-joined tables in unqualified '*' expansion
        SelectStmt stmt = (SelectStmt) AnalyzesOk(String.format("select * from functional.alltypes a " + "%s functional.testtbl b on (a.id = b.id)", joinType));
        // expect to have as many result exprs as alltypes has columns
        assertEquals(13, stmt.getResultExprs().size());
        // cannot expand '*" for a semi-/anti-joined table
        AnalysisError(String.format("select a.*, b.* from functional.alltypes a " + "%s functional.alltypes b on (a.id = b.id)", joinType), "Illegal star expression 'b.*' of semi-/anti-joined table 'b'");
    }
    for (String joinType : new String[] { "right semi join", "right anti join" }) {
        // ignore semi-/anti-joined tables in unqualified '*' expansion
        SelectStmt stmt = (SelectStmt) AnalyzesOk(String.format("select * from functional.alltypes a " + "%s functional.testtbl b on (a.id = b.id)", joinType));
        // expect to have as many result exprs as testtbl has columns
        assertEquals(3, stmt.getResultExprs().size());
        // cannot expand '*" for a semi-/anti-joined table
        AnalysisError(String.format("select a.*, b.* from functional.alltypes a " + "%s functional.alltypes b on (a.id = b.id)", joinType), "Illegal star expression 'a.*' of semi-/anti-joined table 'a'");
    }
}
#end_block

#method_before
@Test
public void TestComplexTypesInSelectList() {
    // Legal complex-types result exprs in views.
    AnalyzesOk("with t as (select * from functional.allcomplextypes) " + "select t.id, t.int_struct_col.f1 from t");
    AnalyzesOk("with t as (select * from functional.allcomplextypes.struct_map_col) " + "select t.f1, t.f2 from t");
    AnalyzesOk("select t.id, t.int_struct_col.f1 " + "from (select * from functional.allcomplextypes) t");
    AnalyzesOk("select t2.id, t2.int_struct_col.f1 " + "from (select * from (select * from functional.allcomplextypes) t1) t2");
    AnalyzesOk("select t.f1, t.f2 " + "from (select * from functional.allcomplextypes.struct_array_col) t");
    AnalyzesOk("select id, int_struct_col.f1 from functional.allcomplextypes_view");
    AnalyzesOk("select t.id, t.int_struct_col.f1 " + "from functional.allcomplextypes_view t");
    // Illegal complex-typed result expr in root stmt.
    AnalysisError("select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list of root statement returns " + "a complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
    AnalysisError("select int_array_col from functional.allcomplextypes_view", "Expr 'int_array_col' in select list of root statement returns a " + "complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
    // Legal star expansion adds illegal complex-typed result expr in root stmt.
    AnalysisError("select * from functional.allcomplextypes " + "cross join functional_parquet.alltypes", "Expr 'functional.allcomplextypes.int_array_col' in select list of " + "root statement returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
    AnalysisError("select * from functional.allcomplextypes_view ", "Expr 'functional.allcomplextypes_view.int_array_col' in select list " + "of root statement returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list of the root statement.");
}
#method_after
@Test
public void TestComplexTypesInSelectList() {
    // Illegal complex-typed expr in select list.
    AnalysisError("select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Illegal complex-typed expr in a union.
    AnalysisError("select int_struct_col from functional.allcomplextypes " + "union all select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Illegal complex-typed expr inside inline view.
    AnalysisError("select 1 from " + "(select int_struct_col from functional.allcomplextypes) v", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Illegal complex-typed expr in an insert.
    AnalysisError("insert into functional.allcomplextypes " + "select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Illegal complex-typed expr in a CTAS.
    AnalysisError("create table new_tbl as " + "select int_struct_col from functional.allcomplextypes", "Expr 'int_struct_col' in select list returns a " + "complex type 'STRUCT<f1:INT,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Legal star expansion adds illegal complex-typed expr.
    AnalysisError("select * from functional.allcomplextypes " + "cross join functional_parquet.alltypes", "Expr 'functional.allcomplextypes.int_array_col' in select list returns a " + "complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("select complex_struct_col.* from functional.allcomplextypes", "Expr 'functional.allcomplextypes.complex_struct_col.f2' in select list " + "returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
}
#end_block

#method_before
@Test
public void TestViews() throws AnalysisException {
    // Simple selects on our pre-defined views.
    AnalyzesOk("select * from functional.alltypes_view");
    AnalyzesOk("select x, y, z from functional.alltypes_view_sub");
    AnalyzesOk("select abc, xyz from functional.complex_view");
    // Test a view on a view.
    AnalyzesOk("select * from functional.view_view");
    // Aliases of views.
    AnalyzesOk("select t.x, t.y, t.z from functional.alltypes_view_sub t");
    // Views in a union.
    AnalyzesOk("select * from functional.alltypes_view_sub union all " + "select * from functional.alltypes_view_sub");
    // View in a subquery.
    AnalyzesOk("select t.* from (select * from functional.alltypes_view_sub) t");
    // View in a WITH-clause view.
    AnalyzesOk("with t as (select * from functional.complex_view) " + "select abc, xyz from t");
    // Complex query on a complex view with a join and an aggregate.
    AnalyzesOk("select sum(t1.abc), t2.xyz from functional.complex_view t1 " + "inner join functional.complex_view t2 on (t1.abc = t2.abc) " + "group by t2.xyz");
    // Cannot insert into a view.
    AnalysisError("insert into functional.alltypes_view partition(year, month) " + "select * from functional.alltypes", "Impala does not support inserting into views: functional.alltypes_view");
    // Cannot load into a view.
    AnalysisError("load data inpath '/test-warehouse/tpch.lineitem/lineitem.tbl' " + "into table functional.alltypes_view", "LOAD DATA only supported for HDFS tables: functional.alltypes_view");
    // Need to give view-references an explicit alias.
    AnalysisError("select * from functional.alltypes_view_sub " + "inner join functional.alltypes_view_sub", "Duplicate table alias: 'alltypes_view_sub'");
    // Column names were redefined in view.
    AnalysisError("select int_col from functional.alltypes_view_sub", "Could not resolve column/field reference: 'int_col'");
}
#method_after
@Test
public void TestViews() throws AnalysisException {
    // Simple selects on our pre-defined views.
    AnalyzesOk("select * from functional.alltypes_view");
    AnalyzesOk("select x, y, z from functional.alltypes_view_sub");
    AnalyzesOk("select abc, xyz from functional.complex_view");
    // Test a view on a view.
    AnalyzesOk("select * from functional.view_view");
    // Aliases of views.
    AnalyzesOk("select t.x, t.y, t.z from functional.alltypes_view_sub t");
    // Views in a union.
    AnalyzesOk("select * from functional.alltypes_view_sub union all " + "select * from functional.alltypes_view_sub");
    // View in a subquery.
    AnalyzesOk("select t.* from (select * from functional.alltypes_view_sub) t");
    // View in a WITH-clause view.
    AnalyzesOk("with t as (select * from functional.complex_view) " + "select abc, xyz from t");
    // Complex query on a complex view with a join and an aggregate.
    AnalyzesOk("select sum(t1.abc), t2.xyz from functional.complex_view t1 " + "inner join functional.complex_view t2 on (t1.abc = t2.abc) " + "group by t2.xyz");
    // Cannot insert into a view.
    AnalysisError("insert into functional.alltypes_view partition(year, month) " + "select * from functional.alltypes", "Impala does not support inserting into views: functional.alltypes_view");
    // Cannot load into a view.
    AnalysisError("load data inpath '/test-warehouse/tpch.lineitem/lineitem.tbl' " + "into table functional.alltypes_view", "LOAD DATA only supported for HDFS tables: functional.alltypes_view");
    // Need to give view-references an explicit alias.
    AnalysisError("select * from functional.alltypes_view_sub " + "inner join functional.alltypes_view_sub", "Duplicate table alias: 'functional.alltypes_view_sub'");
    // Column names were redefined in view.
    AnalysisError("select int_col from functional.alltypes_view_sub", "Could not resolve column/field reference: 'int_col'");
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    Path resolvedPath = null;
    try {
        resolvedPath = analyzer.resolvePath(rawPath_, PathType.SLOT_REF);
    } catch (TableLoadingException e) {
        // Should never happen because we only check registered table aliases.
        Preconditions.checkState(false);
    }
    if (resolvedPath == null) {
        throw new AnalysisException(String.format("Could not resolve column/field reference: '%s'", Joiner.on(".").join(rawPath_)));
    } else if (resolvedPath.getRootDesc() != null && !analyzer.isVisible(resolvedPath.getRootDesc().getId())) {
        throw new AnalysisException(String.format("Illegal column/field reference '%s' of semi-/anti-joined table '%s'", Joiner.on(".").join(rawPath_), resolvedPath.getRootDesc().getAlias()));
    } else if (resolvedPath.hasMidCollection()) {
        int firstCollectionIdx = resolvedPath.getFirstCollectionIdx();
        throw new AnalysisException(String.format("Illegal column/field reference '%s' with intermediate " + "collection '%s' of type '%s'", Joiner.on(".").join(rawPath_), resolvedPath.getRawPath().get(firstCollectionIdx), resolvedPath.getMatches().get(firstCollectionIdx).toSql()));
    }
    matchedPath_ = Joiner.on(".").join(resolvedPath.getRawPath());
    desc_ = analyzer.registerSlotRef(resolvedPath);
    type_ = desc_.getType();
    if (!type_.isSupported()) {
        throw new AnalysisException("Unsupported type '" + type_.toSql() + "' in '" + toSql() + "'.");
    }
    if (type_.isInvalid()) {
        // HMS string.
        throw new AnalysisException("Unsupported type in '" + toSql() + "'.");
    }
    type_.analyze();
    numDistinctValues_ = desc_.getStats().getNumDistinctValues();
    if (type_.isBoolean())
        selectivity_ = DEFAULT_SELECTIVITY;
    isAnalyzed_ = true;
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    if (resolvedPath_ == null) {
        try {
            resolvedPath_ = analyzer.resolvePath(rawPath_, PathType.SLOT_REF);
        } catch (TableLoadingException e) {
            // Should never happen because we only check registered table aliases.
            Preconditions.checkState(false);
        }
    }
    Preconditions.checkNotNull(resolvedPath_);
    matchedPath_ = Joiner.on(".").join(resolvedPath_.getRawPath());
    desc_ = analyzer.registerSlotRef(resolvedPath_);
    type_ = desc_.getType();
    if (!type_.isSupported()) {
        throw new AnalysisException("Unsupported type '" + type_.toSql() + "' in '" + toSql() + "'.");
    }
    if (type_.isInvalid()) {
        // HMS string.
        throw new AnalysisException("Unsupported type in '" + toSql() + "'.");
    }
    numDistinctValues_ = desc_.getStats().getNumDistinctValues();
    if (type_.isBoolean())
        selectivity_ = DEFAULT_SELECTIVITY;
    isAnalyzed_ = true;
}
#end_block

#method_before
@Override
public String toSqlImpl() {
    if (rawPath_ != null)
        return ToSqlUtils.getPathSql(rawPath_);
    if (label_ != null)
        return label_;
    return "<slot " + Integer.toString(desc_.getId().asInt()) + ">";
}
#method_after
@Override
public String toSqlImpl() {
    if (label_ != null)
        return label_;
    if (rawPath_ != null)
        return ToSqlUtils.getPathSql(rawPath_);
    return "<slot " + Integer.toString(desc_.getId().asInt()) + ">";
}
#end_block

#method_before
private long getJoinCardinality(Analyzer analyzer) {
    Preconditions.checkState(joinOp_ == JoinOperator.INNER_JOIN || joinOp_.isOuterJoin());
    long maxNumDistinct = 0;
    for (Expr eqJoinPredicate : eqJoinConjuncts_) {
        if (eqJoinPredicate.getChild(0).unwrapSlotRef(false) == null)
            continue;
        SlotRef rhsSlotRef = eqJoinPredicate.getChild(1).unwrapSlotRef(false);
        if (rhsSlotRef == null)
            continue;
        SlotDescriptor slotDesc = rhsSlotRef.getDesc();
        if (slotDesc == null)
            continue;
        ColumnStats stats = slotDesc.getStats();
        if (!stats.hasNumDistinctValues())
            continue;
        long numDistinct = stats.getNumDistinctValues();
        Table rhsTbl = slotDesc.getParent().getParentTable();
        if (rhsTbl != null && rhsTbl.getNumRows() != -1) {
            // we can't have more distinct values than rows in the table, even though
            // the metastore stats may think so
            LOG.debug("#distinct=" + numDistinct + " #rows=" + Long.toString(rhsTbl.getNumRows()));
            numDistinct = Math.min(numDistinct, rhsTbl.getNumRows());
        }
        if (getChild(1).cardinality_ != -1 && numDistinct != -1) {
            // The number of distinct values of a slot cannot exceed the cardinality_
            // of the plan node the slot is coming from.
            numDistinct = Math.min(numDistinct, getChild(1).cardinality_);
        }
        maxNumDistinct = Math.max(maxNumDistinct, numDistinct);
        LOG.debug("min slotref=" + rhsSlotRef.toSql() + " #distinct=" + Long.toString(numDistinct));
    }
    long result = -1;
    if (maxNumDistinct == 0) {
        // if we didn't find any suitable join predicates or don't have stats
        // on the relevant columns, we very optimistically assume we're doing an
        // FK/PK join (which doesn't alter the cardinality of the left-hand side)
        result = getChild(0).cardinality_;
    } else if (getChild(0).cardinality_ != -1 && getChild(1).cardinality_ != -1) {
        result = multiplyCardinalities(getChild(0).cardinality_, getChild(1).cardinality_);
        result = Math.round((double) result / (double) maxNumDistinct);
    }
    return result;
}
#method_after
private long getJoinCardinality(Analyzer analyzer) {
    Preconditions.checkState(joinOp_ == JoinOperator.INNER_JOIN || joinOp_.isOuterJoin());
    long maxNumDistinct = 0;
    for (Expr eqJoinPredicate : eqJoinConjuncts_) {
        if (eqJoinPredicate.getChild(0).unwrapSlotRef(false) == null)
            continue;
        SlotRef rhsSlotRef = eqJoinPredicate.getChild(1).unwrapSlotRef(false);
        if (rhsSlotRef == null)
            continue;
        SlotDescriptor slotDesc = rhsSlotRef.getDesc();
        if (slotDesc == null)
            continue;
        ColumnStats stats = slotDesc.getStats();
        if (!stats.hasNumDistinctValues())
            continue;
        long numDistinct = stats.getNumDistinctValues();
        Table rhsTbl = slotDesc.getParent().getTable();
        if (rhsTbl != null && rhsTbl.getNumRows() != -1) {
            // we can't have more distinct values than rows in the table, even though
            // the metastore stats may think so
            LOG.debug("#distinct=" + numDistinct + " #rows=" + Long.toString(rhsTbl.getNumRows()));
            numDistinct = Math.min(numDistinct, rhsTbl.getNumRows());
        }
        if (getChild(1).cardinality_ != -1 && numDistinct != -1) {
            // The number of distinct values of a slot cannot exceed the cardinality_
            // of the plan node the slot is coming from.
            numDistinct = Math.min(numDistinct, getChild(1).cardinality_);
        }
        maxNumDistinct = Math.max(maxNumDistinct, numDistinct);
        LOG.debug("min slotref=" + rhsSlotRef.toSql() + " #distinct=" + Long.toString(numDistinct));
    }
    long result = -1;
    if (maxNumDistinct == 0) {
        // if we didn't find any suitable join predicates or don't have stats
        // on the relevant columns, we very optimistically assume we're doing an
        // FK/PK join (which doesn't alter the cardinality of the left-hand side)
        result = getChild(0).cardinality_;
    } else if (getChild(0).cardinality_ != -1 && getChild(1).cardinality_ != -1) {
        result = multiplyCardinalities(getChild(0).cardinality_, getChild(1).cardinality_);
        result = Math.round((double) result / (double) maxNumDistinct);
    }
    return result;
}
#end_block

#method_before
private PlanNode createSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    // no from clause -> materialize the select's exprs with a UnionNode
    if (selectStmt.getTableRefs().isEmpty()) {
        return createConstantSelectPlan(selectStmt, analyzer);
    }
    // Slot materialization:
    // We need to mark all slots as materialized that are needed during the execution
    // of selectStmt, and we need to do that prior to creating plans for the TableRefs
    // (because createTableRefNode() might end up calling computeMemLayout() on one or
    // more TupleDescriptors, at which point all referenced slots need to be marked).
    // 
    // For non-join predicates, slots are marked as follows:
    // - for base table scan predicates, this is done directly by ScanNode.init(), which
    // can do a better job because it doesn't need to materialize slots that are only
    // referenced for partition pruning, for instance
    // - for inline views, non-join predicates are pushed down, at which point the
    // process repeats itself.
    selectStmt.materializeRequiredSlots(analyzer);
    ArrayList<TupleId> rowTuples = Lists.newArrayList();
    // collect output tuples of subtrees
    for (TableRef tblRef : selectStmt.getTableRefs()) {
        rowTuples.addAll(tblRef.getMaterializedTupleIds());
    }
    // are materialized (see IMPALA-1960).
    if (analyzer.hasEmptySpjResultSet()) {
        for (TableRef tblRef : selectStmt.getTableRefs()) {
            if (tblRef instanceof InlineViewRef) {
                rowTuples.removeAll(tblRef.getMaterializedTupleIds());
            }
        }
        Preconditions.checkState(selectStmt.hasAggInfo());
        AggregateInfo aggInfo = selectStmt.getAggInfo();
        List<Expr> exprs = Lists.newArrayList(aggInfo.getGroupingExprs());
        exprs.addAll(aggInfo.getAggregateExprs());
        Set<TupleId> tupleIds = Sets.newHashSet();
        for (Expr expr : exprs) {
            List<SlotId> slotIds = Lists.newArrayList();
            expr.getIds(null, slotIds);
            for (SlotId slotId : slotIds) {
                SlotDescriptor slotDesc = analyzer.getDescTbl().getSlotDesc(slotId);
                slotDesc.setIsMaterialized(true);
                tupleIds.add(slotDesc.getParent().getId());
            }
        }
        // Materialize the referenced tuples.
        for (TupleId tupleId : tupleIds) {
            TupleDescriptor tupleDesc = analyzer.getTupleDesc(tupleId);
            tupleDesc.setIsMaterialized(true);
            tupleDesc.computeMemLayout();
        }
        rowTuples.addAll(tupleIds);
        PlanNode emptySetNode = new EmptySetNode(ctx_.getNextNodeId(), rowTuples);
        emptySetNode.init(analyzer);
        return createAggregationPlan(selectStmt, analyzer, emptySetNode);
    }
    // create plans for our table refs; use a list here instead of a map to
    // maintain a deterministic order of traversing the TableRefs during join
    // plan generation (helps with tests)
    List<Pair<TableRef, PlanNode>> refPlans = Lists.newArrayList();
    for (TableRef ref : selectStmt.getTableRefs()) {
        PlanNode plan = createTableRefNode(analyzer, ref);
        Preconditions.checkState(plan != null);
        refPlans.add(new Pair(ref, plan));
    }
    // save state of conjunct assignment; needed for join plan generation
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        entry.second.setAssignedConjuncts(analyzer.getAssignedConjuncts());
    }
    PlanNode root = null;
    if (!selectStmt.getSelectList().isStraightJoin()) {
        Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
        root = createCheapestJoinPlan(analyzer, refPlans);
        if (root == null)
            analyzer.setAssignedConjuncts(assignedConjuncts);
    }
    if (selectStmt.getSelectList().isStraightJoin() || root == null) {
        // we didn't have enough stats to do a cost-based join plan, or the STRAIGHT_JOIN
        // keyword was in the select list: use the FROM clause order instead
        root = createFromClauseJoinPlan(analyzer, refPlans);
        Preconditions.checkNotNull(root);
    }
    // add aggregation, if any
    if (selectStmt.getAggInfo() != null) {
        root = createAggregationPlan(selectStmt, analyzer, root);
    }
    // Preconditions.checkState(!analyzer.hasUnassignedConjuncts());
    return root;
}
#method_after
private PlanNode createSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    // no from clause -> materialize the select's exprs with a UnionNode
    if (selectStmt.getTableRefs().isEmpty()) {
        return createConstantSelectPlan(selectStmt, analyzer);
    }
    // Slot materialization:
    // We need to mark all slots as materialized that are needed during the execution
    // of selectStmt, and we need to do that prior to creating plans for the TableRefs
    // (because createTableRefNode() might end up calling computeMemLayout() on one or
    // more TupleDescriptors, at which point all referenced slots need to be marked).
    // 
    // For non-join predicates, slots are marked as follows:
    // - for base table scan predicates, this is done directly by ScanNode.init(), which
    // can do a better job because it doesn't need to materialize slots that are only
    // referenced for partition pruning, for instance
    // - for inline views, non-join predicates are pushed down, at which point the
    // process repeats itself.
    selectStmt.materializeRequiredSlots(analyzer);
    ArrayList<TupleId> rowTuples = Lists.newArrayList();
    // collect output tuples of subtrees
    for (TableRef tblRef : selectStmt.getTableRefs()) {
        rowTuples.addAll(tblRef.getMaterializedTupleIds());
    }
    // are materialized (see IMPALA-1960).
    if (analyzer.hasEmptySpjResultSet()) {
        PlanNode emptySetNode = new EmptySetNode(ctx_.getNextNodeId(), rowTuples);
        emptySetNode.init(analyzer);
        emptySetNode.setOutputSmap(selectStmt.getBaseTblSmap());
        return createAggregationPlan(selectStmt, analyzer, emptySetNode);
    }
    // create plans for our table refs; use a list here instead of a map to
    // maintain a deterministic order of traversing the TableRefs during join
    // plan generation (helps with tests)
    List<Pair<TableRef, PlanNode>> refPlans = Lists.newArrayList();
    for (TableRef ref : selectStmt.getTableRefs()) {
        PlanNode plan = createTableRefNode(analyzer, ref);
        Preconditions.checkState(plan != null);
        refPlans.add(new Pair(ref, plan));
    }
    // save state of conjunct assignment; needed for join plan generation
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        entry.second.setAssignedConjuncts(analyzer.getAssignedConjuncts());
    }
    PlanNode root = null;
    if (!selectStmt.getSelectList().isStraightJoin()) {
        Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
        root = createCheapestJoinPlan(analyzer, refPlans);
        if (root == null)
            analyzer.setAssignedConjuncts(assignedConjuncts);
    }
    if (selectStmt.getSelectList().isStraightJoin() || root == null) {
        // we didn't have enough stats to do a cost-based join plan, or the STRAIGHT_JOIN
        // keyword was in the select list: use the FROM clause order instead
        root = createFromClauseJoinPlan(analyzer, refPlans);
        Preconditions.checkNotNull(root);
    }
    // add aggregation, if any
    if (selectStmt.getAggInfo() != null) {
        root = createAggregationPlan(selectStmt, analyzer, root);
    }
    // Preconditions.checkState(!analyzer.hasUnassignedConjuncts());
    return root;
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws InternalException {
    assignConjuncts(analyzer);
    // Compute the memory layout for the generated tuple.
    computeMemLayout(analyzer);
    computeStats(analyzer);
    // populate resolvedTupleExprs_ and outputSmap_
    List<SlotDescriptor> sortTupleSlots = info_.getSortTupleDescriptor().getSlots();
    List<Expr> slotExprs = info_.getSortTupleSlotExprs();
    Preconditions.checkState(sortTupleSlots.size() == slotExprs.size());
    resolvedTupleExprs_ = Lists.newArrayList();
    outputSmap_ = new ExprSubstitutionMap();
    for (int i = 0; i < slotExprs.size(); ++i) {
        if (!sortTupleSlots.get(i).isMaterialized())
            continue;
        resolvedTupleExprs_.add(slotExprs.get(i));
        outputSmap_.put(slotExprs.get(i), new SlotRef(sortTupleSlots.get(i)));
    }
    ExprSubstitutionMap childSmap = getCombinedChildSmap();
    resolvedTupleExprs_ = Expr.substituteList(resolvedTupleExprs_, childSmap, analyzer, false);
    // be unwrapped to the original expr.
    if (childSmap != null) {
        ArrayList<Expr> rhsExprs = childSmap.getRhs();
        for (int i = 0; i < rhsExprs.size(); ++i) {
            rhsExprs.set(i, TupleIsNullPredicate.unwrapExprs(rhsExprs.get(i)));
        }
    }
    // Remap the ordering exprs to the tuple materialized by this sort node. The mapping
    // is a composition of the childSmap and the outputSmap_ because the child node may
    // have also remapped its input (e.g., as in a a series of (sort->analytic)* nodes).
    // Parent nodes have have to do the same so set the composition as the outputSmap_.
    outputSmap_ = ExprSubstitutionMap.compose(childSmap, outputSmap_, analyzer);
    info_.substituteOrderingExprs(outputSmap_, analyzer);
    info_.checkConsistency();
    LOG.trace("sort id " + tupleIds_.get(0).toString() + " smap: " + outputSmap_.debugString());
    LOG.trace("sort input exprs: " + Expr.debugString(resolvedTupleExprs_));
}
#method_after
@Override
public void init(Analyzer analyzer) throws InternalException {
    assignConjuncts(analyzer);
    // Compute the memory layout for the generated tuple.
    computeMemLayout(analyzer);
    computeStats(analyzer);
    // populate resolvedTupleExprs_ and outputSmap_
    List<SlotDescriptor> sortTupleSlots = info_.getSortTupleDescriptor().getSlots();
    List<Expr> slotExprs = info_.getSortTupleSlotExprs();
    Preconditions.checkState(sortTupleSlots.size() == slotExprs.size());
    resolvedTupleExprs_ = Lists.newArrayList();
    outputSmap_ = new ExprSubstitutionMap();
    for (int i = 0; i < slotExprs.size(); ++i) {
        if (!sortTupleSlots.get(i).isMaterialized())
            continue;
        resolvedTupleExprs_.add(slotExprs.get(i));
        outputSmap_.put(slotExprs.get(i), new SlotRef(sortTupleSlots.get(i)));
    }
    ExprSubstitutionMap childSmap = getCombinedChildSmap();
    resolvedTupleExprs_ = Expr.substituteList(resolvedTupleExprs_, childSmap, analyzer, false);
    // Remap the ordering exprs to the tuple materialized by this sort node. The mapping
    // is a composition of the childSmap and the outputSmap_ because the child node may
    // have also remapped its input (e.g., as in a a series of (sort->analytic)* nodes).
    // Parent nodes have have to do the same so set the composition as the outputSmap_.
    outputSmap_ = ExprSubstitutionMap.compose(childSmap, outputSmap_, analyzer);
    info_.substituteOrderingExprs(outputSmap_, analyzer);
    info_.checkConsistency();
    LOG.trace("sort id " + tupleIds_.get(0).toString() + " smap: " + outputSmap_.debugString());
    LOG.trace("sort input exprs: " + Expr.debugString(resolvedTupleExprs_));
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // create smap_ and baseTblSmap_ and register auxiliary eq predicates between our
    // tuple descriptor's slots and our *unresolved* select list exprs;
    // we create these auxiliary predicates so that the analyzer can compute the value
    // transfer graph through this inline view correctly (ie, predicates can get
    // propagated through the view);
    // if the view stmt contains analytic functions, we cannot propagate predicates
    // into the view, because those extra filters would alter the results of the
    // analytic functions (see IMPALA-1243)
    // TODO: relax this a bit by allowing propagation out of the inline view (but
    // not into it)
    boolean createAuxPredicates = !(queryStmt_ instanceof SelectStmt) || !(((SelectStmt) queryStmt_).hasAnalyticInfo());
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i);
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        SlotDescriptor slotDesc = analyzer.registerColumnRef(getAliasAsName(), colName);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicates) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // not into it)
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i);
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        SlotDescriptor slotDesc = analyzer.registerColumnRef(getAliasAsName(), colName);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicate(colExpr)) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#end_block

#method_before
@Override
public boolean equals(Object o) {
    if (!super.equals(o))
        return false;
    if (!(o instanceof TupleIsNullPredicate))
        return false;
    TupleIsNullPredicate other = (TupleIsNullPredicate) o;
    return other.tupleIds_.containsAll(tupleIds_) && tupleIds_.containsAll(other.tupleIds_);
}
#method_after
@Override
public boolean equals(Object o) {
    if (!super.equals(o))
        return false;
    TupleIsNullPredicate other = (TupleIsNullPredicate) o;
    return other.tupleIds_.containsAll(tupleIds_) && tupleIds_.containsAll(other.tupleIds_);
}
#end_block

#method_before
@Override
protected String toSqlImpl() {
    return "TupleIsNull(" + Joiner.on(",").join(tupleIds_) + ")";
}
#method_after
@Override
protected String toSqlImpl() {
    return "TupleIsNull()";
}
#end_block

#method_before
public static ArrayList<Expr> wrapExprs(ArrayList<Expr> inputExprs, List<TupleId> tids, Analyzer analyzer) throws InternalException {
    ArrayList<Expr> result = Lists.newArrayListWithCapacity(inputExprs.size());
    for (Expr e : inputExprs) {
        result.add(wrapExpr(e, tids, analyzer));
    }
    return result;
}
#method_after
public static List<Expr> wrapExprs(List<Expr> inputExprs, List<TupleId> tids, Analyzer analyzer) throws InternalException {
    // Assert that all tids are materialized.
    for (TupleId tid : tids) {
        TupleDescriptor tupleDesc = analyzer.getTupleDesc(tid);
        Preconditions.checkState(tupleDesc.isMaterialized());
    }
    // Perform the wrapping.
    List<Expr> result = Lists.newArrayListWithCapacity(inputExprs.size());
    for (Expr e : inputExprs) {
        result.add(wrapExpr(e, tids, analyzer));
    }
    return result;
}
#end_block

#method_before
private static boolean requiresNullWrapping(Expr expr, Analyzer analyzer) throws InternalException {
    // TODO: return true in this case?
    if (expr.contains(Predicates.instanceOf(TupleIsNullPredicate.class)))
        return true;
    // Map for substituting SlotRefs with NullLiterals.
    ExprSubstitutionMap nullSmap = new ExprSubstitutionMap();
    List<SlotRef> slotRefs = Lists.newArrayList();
    expr.collect(Predicates.instanceOf(SlotRef.class), slotRefs);
    for (SlotRef slotRef : slotRefs) {
        // The rhs null literal should have the same type as the lhs SlotRef to ensure
        // exprs resolve to the same signature after applying this smap.
        nullSmap.put(slotRef, NullLiteral.create(slotRef.getType()));
    }
    // Replace all SlotRefs in expr with NullLiterals, and wrap the result
    // with an IS NOT NULL predicate.
    Expr isNotNullLiteralPred = new IsNullPredicate(expr.substitute(nullSmap, analyzer, false), true);
    Preconditions.checkState(isNotNullLiteralPred.isConstant());
    // analyze to insert casts, etc.
    isNotNullLiteralPred.analyzeNoThrow(analyzer);
    return FeSupport.EvalPredicate(isNotNullLiteralPred, analyzer.getQueryCtx());
}
#method_after
private static boolean requiresNullWrapping(Expr expr, Analyzer analyzer) throws InternalException {
    Preconditions.checkNotNull(expr);
    // Do not try to execute expr because a TupleIsNullPredicate is not constant.
    if (expr.contains(TupleIsNullPredicate.class))
        return true;
    // Map for substituting SlotRefs with NullLiterals.
    ExprSubstitutionMap nullSmap = new ExprSubstitutionMap();
    List<SlotRef> slotRefs = Lists.newArrayList();
    expr.collect(SlotRef.class, slotRefs);
    for (SlotRef slotRef : slotRefs) {
        // The rhs null literal should have the same type as the lhs SlotRef to ensure
        // exprs resolve to the same signature after applying this smap.
        nullSmap.put(slotRef, NullLiteral.create(slotRef.getType()));
    }
    // Replace all SlotRefs in expr with NullLiterals, and wrap the result
    // with an IS NOT NULL predicate.
    Expr isNotNullLiteralPred = new IsNullPredicate(expr.substitute(nullSmap, analyzer, false), true);
    Preconditions.checkState(isNotNullLiteralPred.isConstant());
    // analyze to insert casts, etc.
    isNotNullLiteralPred.analyzeNoThrow(analyzer);
    return FeSupport.EvalPredicate(isNotNullLiteralPred, analyzer.getQueryCtx());
}
#end_block

#method_before
private SortInfo createSortInfo(PlanNode input, List<Expr> sortExprs, List<Boolean> isAsc, List<Boolean> nullsFirst) {
    // create tuple for sort output = the entire materialized input in a single tuple
    TupleDescriptor sortTupleDesc = analyzer_.getDescTbl().createTupleDescriptor("sort-tuple");
    ExprSubstitutionMap sortSmap = new ExprSubstitutionMap();
    List<Expr> sortSlotExprs = Lists.newArrayList();
    sortTupleDesc.setIsMaterialized(true);
    for (TupleId tid : input.getTupleIds()) {
        TupleDescriptor tupleDesc = analyzer_.getTupleDesc(tid);
        for (SlotDescriptor inputSlotDesc : tupleDesc.getSlots()) {
            if (!inputSlotDesc.isMaterialized())
                continue;
            SlotDescriptor sortSlotDesc = analyzer_.copySlotDescriptor(inputSlotDesc, sortTupleDesc);
            // all output slots need to be materialized
            sortSlotDesc.setIsMaterialized(true);
            sortSmap.put(new SlotRef(inputSlotDesc), new SlotRef(sortSlotDesc));
            sortSlotExprs.add(new SlotRef(inputSlotDesc));
        }
    }
    SortInfo sortInfo = new SortInfo(Expr.substituteList(sortExprs, sortSmap, analyzer_, false), isAsc, nullsFirst);
    LOG.trace("sortinfo exprs: " + Expr.debugString(sortInfo.getOrderingExprs()));
    sortInfo.setMaterializedTupleInfo(sortTupleDesc, sortSlotExprs);
    return sortInfo;
}
#method_after
private SortInfo createSortInfo(PlanNode input, List<Expr> sortExprs, List<Boolean> isAsc, List<Boolean> nullsFirst) {
    // create tuple for sort output = the entire materialized input in a single tuple
    TupleDescriptor sortTupleDesc = analyzer_.getDescTbl().createTupleDescriptor("sort-tuple");
    ExprSubstitutionMap sortSmap = new ExprSubstitutionMap();
    List<Expr> sortSlotExprs = Lists.newArrayList();
    sortTupleDesc.setIsMaterialized(true);
    for (TupleId tid : input.getTupleIds()) {
        TupleDescriptor tupleDesc = analyzer_.getTupleDesc(tid);
        for (SlotDescriptor inputSlotDesc : tupleDesc.getSlots()) {
            if (!inputSlotDesc.isMaterialized())
                continue;
            SlotDescriptor sortSlotDesc = analyzer_.copySlotDescriptor(inputSlotDesc, sortTupleDesc);
            // all output slots need to be materialized
            sortSlotDesc.setIsMaterialized(true);
            sortSmap.put(new SlotRef(inputSlotDesc), new SlotRef(sortSlotDesc));
            sortSlotExprs.add(new SlotRef(inputSlotDesc));
        }
    }
    // Lhs exprs to be substituted in ancestor plan nodes could have a rhs that contains
    // TupleIsNullPredicates. TupleIsNullPredicates require specific tuple ids for
    // evaluation. Since this sort materializes a new tuple, it's impossible to evaluate
    // TupleIsNullPredicates referring to this sort's input after this sort,
    // To preserve the information whether an input tuple was null or not this sort node,
    // we materialize those rhs TupleIsNullPredicates, which are then substituted
    // by a SlotRef into the sort's tuple in ancestor nodes (IMPALA-1519).
    ExprSubstitutionMap inputSmap = input.getOutputSmap();
    if (inputSmap != null) {
        List<Expr> tupleIsNullPredsToMaterialize = Lists.newArrayList();
        for (int i = 0; i < inputSmap.size(); ++i) {
            Expr rhsExpr = inputSmap.getRhs().get(i);
            // Ignore substitutions that are irrelevant at this plan node and its ancestors.
            if (!rhsExpr.isBoundByTupleIds(input.getTupleIds()))
                continue;
            rhsExpr.collect(TupleIsNullPredicate.class, tupleIsNullPredsToMaterialize);
        }
        Expr.removeDuplicates(tupleIsNullPredsToMaterialize);
        // Materialize relevant unique TupleIsNullPredicates.
        for (Expr tupleIsNullPred : tupleIsNullPredsToMaterialize) {
            SlotDescriptor sortSlotDesc = analyzer_.addSlotDescriptor(sortTupleDesc);
            sortSlotDesc.setType(tupleIsNullPred.getType());
            sortSlotDesc.setIsMaterialized(true);
            sortSlotDesc.setSourceExpr(tupleIsNullPred);
            sortSlotDesc.setLabel(tupleIsNullPred.toSql());
            sortSlotExprs.add(tupleIsNullPred.clone());
        }
    }
    SortInfo sortInfo = new SortInfo(Expr.substituteList(sortExprs, sortSmap, analyzer_, false), isAsc, nullsFirst);
    LOG.trace("sortinfo exprs: " + Expr.debugString(sortInfo.getOrderingExprs()));
    sortInfo.setMaterializedTupleInfo(sortTupleDesc, sortSlotExprs);
    return sortInfo;
}
#end_block

#method_before
private PlanNode createConstantSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws InternalException {
    Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
    ArrayList<Expr> resultExprs = selectStmt.getBaseTblResultExprs();
    ArrayList<String> colLabels = selectStmt.getColLabels();
    // Create tuple descriptor for materialized tuple.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
    // Analysis guarantees that selects without a FROM clause only have constant exprs.
    unionNode.addConstExprList(Lists.newArrayList(resultExprs));
    // Replace the select stmt's resultExprs with SlotRefs into tupleDesc.
    for (int i = 0; i < resultExprs.size(); ++i) {
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(colLabels.get(i));
        slotDesc.setSourceExpr(resultExprs.get(i));
        slotDesc.setType(resultExprs.get(i).getType());
        slotDesc.setStats(ColumnStats.fromExpr(resultExprs.get(i)));
        slotDesc.setIsMaterialized(true);
        SlotRef slotRef = new SlotRef(slotDesc);
        resultExprs.set(i, slotRef);
    }
    tupleDesc.computeMemLayout();
    // UnionNode.init() needs tupleDesc to have been initialized
    unionNode.init(analyzer);
    return unionNode;
}
#method_after
private PlanNode createConstantSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws InternalException {
    Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
    ArrayList<Expr> resultExprs = selectStmt.getResultExprs();
    ArrayList<String> colLabels = selectStmt.getColLabels();
    // Create tuple descriptor for materialized tuple.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
    // Analysis guarantees that selects without a FROM clause only have constant exprs.
    unionNode.addConstExprList(Lists.newArrayList(resultExprs));
    // Replace the select stmt's resultExprs with SlotRefs into tupleDesc.
    for (int i = 0; i < resultExprs.size(); ++i) {
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(colLabels.get(i));
        slotDesc.setSourceExpr(resultExprs.get(i));
        slotDesc.setType(resultExprs.get(i).getType());
        slotDesc.setStats(ColumnStats.fromExpr(resultExprs.get(i)));
        slotDesc.setIsMaterialized(true);
        SlotRef slotRef = new SlotRef(slotDesc);
        resultExprs.set(i, slotRef);
    }
    tupleDesc.computeMemLayout();
    // UnionNode.init() needs tupleDesc to have been initialized
    unionNode.init(analyzer);
    return unionNode;
}
#end_block

#method_before
private PlanNode createInlineViewPlan(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    // If possible, "push down" view predicates; this is needed in order to ensure
    // that predicates such as "x + y = 10" are evaluated in the view's plan tree
    // rather than a SelectNode grafted on top of that plan tree.
    // This doesn't prevent predicate propagation, because predicates like
    // "x = 10" that get pushed down are still connected to equivalent slots
    // via the equality predicates created for the view's select list.
    // Include outer join conjuncts here as well because predicates from the
    // On-clause of an outer join may be pushed into the inline view as well.
    // 
    // Limitations on predicate propagation into inline views:
    // If the inline view computes analytic functions, we cannot push any
    // predicate into the inline view tree (see IMPALA-1243). The reason is that
    // analytic functions compute aggregates over their entire input, and applying
    // filters from the enclosing scope *before* the aggregate computation would
    // alter the results. This is unlike regular aggregate computation, which only
    // makes the *output* of the computation visible to the enclosing scope, so that
    // filters from the enclosing scope can be safely applied (to the grouping cols, say)
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(inlineViewRef.getId().asList(), true);
    boolean migrateConjuncts = !inlineViewRef.getViewStmt().hasLimit() && !inlineViewRef.getViewStmt().hasOffset() && (!(inlineViewRef.getViewStmt() instanceof SelectStmt) || !((SelectStmt) (inlineViewRef.getViewStmt())).hasAnalyticInfo());
    if (migrateConjuncts) {
        // check if we can evaluate them
        List<Expr> preds = Lists.newArrayList();
        for (Expr e : unassigned) {
            if (analyzer.canEvalPredicate(inlineViewRef.getId().asList(), e))
                preds.add(e);
        }
        unassigned.removeAll(preds);
        // Generate predicates to enforce equivalences among slots of the inline view
        // tuple. These predicates are also migrated into the inline view.
        analyzer.createEquivConjuncts(inlineViewRef.getId(), preds);
        // create new predicates against the inline view's unresolved result exprs, not
        // the resolved result exprs, in order to avoid skipping scopes (and ignoring
        // limit clauses on the way)
        List<Expr> viewPredicates = Expr.substituteList(preds, inlineViewRef.getSmap(), analyzer, false);
        // Remove unregistered predicates that reference the same slot on
        // both sides (e.g. a = a). Such predicates have been generated from slot
        // equivalences and may incorrectly reject rows with nulls (IMPALA-1412).
        Predicate<Expr> isIdentityPredicate = new Predicate<Expr>() {

            @Override
            public boolean apply(Expr expr) {
                if (!(expr instanceof BinaryPredicate) || ((BinaryPredicate) expr).getOp() != BinaryPredicate.Operator.EQ) {
                    return false;
                }
                if (!expr.isRegisteredPredicate() && expr.getChild(0) instanceof SlotRef && expr.getChild(1) instanceof SlotRef && (((SlotRef) expr.getChild(0)).getSlotId() == ((SlotRef) expr.getChild(1)).getSlotId())) {
                    return true;
                }
                return false;
            }
        };
        Iterables.removeIf(viewPredicates, isIdentityPredicate);
        // "migrate" conjuncts_ by marking them as assigned and re-registering them with
        // new ids.
        // Mark pre-substitution conjuncts as assigned, since the ids of the new exprs may
        // have changed.
        analyzer.markConjunctsAssigned(preds);
        inlineViewRef.getAnalyzer().registerConjuncts(viewPredicates);
    }
    // mark (fully resolve) slots referenced by remaining unassigned conjuncts_ as
    // materialized
    List<Expr> substUnassigned = Expr.substituteList(unassigned, inlineViewRef.getBaseTblSmap(), analyzer, false);
    analyzer.materializeSlots(substUnassigned);
    // Turn a constant select into a UnionNode that materializes the exprs.
    // TODO: unify this with createConstantSelectPlan(), this is basically the
    // same thing
    QueryStmt viewStmt = inlineViewRef.getViewStmt();
    if (viewStmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) viewStmt;
        if (selectStmt.getTableRefs().isEmpty()) {
            if (inlineViewRef.getAnalyzer().hasEmptyResultSet()) {
                return createEmptyNode(viewStmt, inlineViewRef.getAnalyzer());
            }
            // Analysis should have generated a tuple id_ into which to materialize the exprs.
            Preconditions.checkState(inlineViewRef.getMaterializedTupleIds().size() == 1);
            // we need to materialize all slots of our inline view tuple
            analyzer.getTupleDesc(inlineViewRef.getId()).materializeSlots();
            UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), inlineViewRef.getMaterializedTupleIds().get(0));
            if (analyzer.hasEmptyResultSet())
                return unionNode;
            unionNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
            unionNode.addConstExprList(selectStmt.getBaseTblResultExprs());
            unionNode.init(analyzer);
            return unionNode;
        }
    }
    PlanNode rootNode = createQueryPlan(inlineViewRef.getViewStmt(), inlineViewRef.getAnalyzer(), false);
    // TODO: we should compute the "physical layout" of the view's descriptor, so that
    // the avg row size is available during optimization; however, that means we need to
    // select references to its resultExprs from the enclosing scope(s)
    rootNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
    ExprSubstitutionMap inlineViewSmap = inlineViewRef.getSmap();
    if (analyzer.isOuterJoined(inlineViewRef.getId())) {
        // Exprs against non-matched rows of an outer join should always return NULL.
        // Make the rhs exprs of the inline view's smap nullable, if necessary.
        ArrayList<Expr> nullableRhs = TupleIsNullPredicate.wrapExprs(inlineViewSmap.getRhs(), rootNode.getTupleIds(), analyzer);
        inlineViewSmap = new ExprSubstitutionMap(inlineViewSmap.getLhs(), nullableRhs);
    }
    // Set output smap of rootNode *before* creating a SelectNode for proper resolution.
    // The output smap is the composition of the inline view's smap and the output smap
    // of the inline view's plan root. This ensures that all downstream exprs referencing
    // the inline view are replaced with exprs referencing the physical output of
    // the inline view's plan.
    ExprSubstitutionMap composedSmap = ExprSubstitutionMap.compose(inlineViewSmap, rootNode.getOutputSmap(), analyzer);
    rootNode.setOutputSmap(composedSmap);
    if (!migrateConjuncts) {
        rootNode = addUnassignedConjuncts(analyzer, inlineViewRef.getDesc().getId().asList(), rootNode);
    }
    return rootNode;
}
#method_after
private PlanNode createInlineViewPlan(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    // If possible, "push down" view predicates; this is needed in order to ensure
    // that predicates such as "x + y = 10" are evaluated in the view's plan tree
    // rather than a SelectNode grafted on top of that plan tree.
    // This doesn't prevent predicate propagation, because predicates like
    // "x = 10" that get pushed down are still connected to equivalent slots
    // via the equality predicates created for the view's select list.
    // Include outer join conjuncts here as well because predicates from the
    // On-clause of an outer join may be pushed into the inline view as well.
    migrateConjunctsToInlineView(analyzer, inlineViewRef);
    // Turn a constant select into a UnionNode that materializes the exprs.
    // TODO: unify this with createConstantSelectPlan(), this is basically the
    // same thing
    QueryStmt viewStmt = inlineViewRef.getViewStmt();
    if (viewStmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) viewStmt;
        if (selectStmt.getTableRefs().isEmpty()) {
            if (inlineViewRef.getAnalyzer().hasEmptyResultSet()) {
                return createEmptyNode(viewStmt, inlineViewRef.getAnalyzer());
            }
            // Analysis should have generated a tuple id_ into which to materialize the exprs.
            Preconditions.checkState(inlineViewRef.getMaterializedTupleIds().size() == 1);
            // we need to materialize all slots of our inline view tuple
            analyzer.getTupleDesc(inlineViewRef.getId()).materializeSlots();
            UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), inlineViewRef.getMaterializedTupleIds().get(0));
            if (analyzer.hasEmptyResultSet())
                return unionNode;
            unionNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
            unionNode.addConstExprList(selectStmt.getBaseTblResultExprs());
            unionNode.init(analyzer);
            return unionNode;
        }
    }
    PlanNode rootNode = createQueryPlan(inlineViewRef.getViewStmt(), inlineViewRef.getAnalyzer(), false);
    // TODO: we should compute the "physical layout" of the view's descriptor, so that
    // the avg row size is availble during optimization; however, that means we need to
    // select references to its resultExprs from the enclosing scope(s)
    rootNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
    ExprSubstitutionMap inlineViewSmap = inlineViewRef.getSmap();
    if (analyzer.isOuterJoined(inlineViewRef.getId())) {
        // Exprs against non-matched rows of an outer join should always return NULL.
        // Make the rhs exprs of the inline view's smap nullable, if necessary.
        List<Expr> nullableRhs = TupleIsNullPredicate.wrapExprs(inlineViewSmap.getRhs(), rootNode.getTupleIds(), analyzer);
        inlineViewSmap = new ExprSubstitutionMap(inlineViewSmap.getLhs(), nullableRhs);
    }
    // Set output smap of rootNode *before* creating a SelectNode for proper resolution.
    // The output smap is the composition of the inline view's smap and the output smap
    // of the inline view's plan root. This ensures that all downstream exprs referencing
    // the inline view are replaced with exprs referencing the physical output of
    // the inline view's plan.
    ExprSubstitutionMap composedSmap = ExprSubstitutionMap.compose(inlineViewSmap, rootNode.getOutputSmap(), analyzer);
    rootNode.setOutputSmap(composedSmap);
    // place.
    if (!canMigrateConjuncts(inlineViewRef)) {
        rootNode = addUnassignedConjuncts(analyzer, inlineViewRef.getDesc().getId().asList(), rootNode);
    }
    return rootNode;
}
#end_block

#method_before
public ArrayList<Expr> getLhs() {
    return lhs_;
}
#method_after
public List<Expr> getLhs() {
    return lhs_;
}
#end_block

#method_before
public ArrayList<Expr> getRhs() {
    return rhs_;
}
#method_after
public List<Expr> getRhs() {
    return rhs_;
}
#end_block

#method_before
@Test
public void TestInPredicates() throws AnalysisException {
    AnalyzesOk("select * from functional.alltypes where int_col in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where int_col not in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where " + "string_col in ('a', 'b', 'c', 'd')");
    AnalyzesOk("select * from functional.alltypes where " + "string_col not in ('a', 'b', 'c', 'd')");
    // Test booleans.
    AnalyzesOk("select * from functional.alltypes where " + "true in (bool_col, true and false)");
    AnalyzesOk("select * from functional.alltypes where " + "true not in (bool_col, true and false)");
    // In list requires implicit casts.
    AnalyzesOk("select * from functional.alltypes where " + "double_col in (int_col, bigint_col)");
    // Comparison expr requires implicit cast.
    AnalyzesOk("select * from functional.alltypes where " + "int_col in (double_col, bigint_col)");
    // Test predicates.
    AnalyzesOk("select * from functional.alltypes where " + "!true in (false or true, true and false)");
    // Test NULLs.
    AnalyzesOk("select * from functional.alltypes where " + "NULL in (NULL, NULL)");
    // Test IN in binary predicates
    AnalyzesOk("select bool_col = (int_col in (1,2)), " + "case when tinyint_col in (10, NULL) then tinyint_col else NULL end " + "from functional.alltypestiny where int_col > (bool_col in (false))");
    // Incompatible types.
    AnalysisError("select * from functional.alltypes where " + "string_col in (bool_col, double_col)", "Incompatible return types 'STRING' and 'BOOLEAN' " + "of exprs 'string_col' and 'bool_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (int_col, double_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (NULL, int_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select 1 from functional.allcomplextypes where " + "int_array_col in (id, NULL)", "Incompatible return types 'ARRAY<INT>' and 'INT' " + "of exprs 'int_array_col' and 'id'.");
}
#method_after
@Test
public void TestInPredicates() throws AnalysisException {
    AnalyzesOk("select * from functional.alltypes where int_col in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where int_col not in (1, 2, 3, 4)");
    AnalyzesOk("select * from functional.alltypes where " + "string_col in ('a', 'b', 'c', 'd')");
    AnalyzesOk("select * from functional.alltypes where " + "string_col not in ('a', 'b', 'c', 'd')");
    // Test booleans.
    AnalyzesOk("select * from functional.alltypes where " + "true in (bool_col, true and false)");
    AnalyzesOk("select * from functional.alltypes where " + "true not in (bool_col, true and false)");
    // In list requires implicit casts.
    AnalyzesOk("select * from functional.alltypes where " + "double_col in (int_col, bigint_col)");
    // Comparison expr requires implicit cast.
    AnalyzesOk("select * from functional.alltypes where " + "int_col in (double_col, bigint_col)");
    // Test predicates.
    AnalyzesOk("select * from functional.alltypes where " + "!true in (false or true, true and false)");
    // Test NULLs.
    AnalyzesOk("select * from functional.alltypes where " + "NULL in (NULL, NULL)");
    // Test IN in binary predicates
    AnalyzesOk("select bool_col = (int_col in (1,2)), " + "case when tinyint_col in (10, NULL) then tinyint_col else NULL end " + "from functional.alltypestiny where int_col > (bool_col in (false)) " + "and (int_col in (1,2)) = (select min(bool_col) from functional.alltypes) " + "and (int_col in (3,4)) = (tinyint_col in (4,5))");
    // Incompatible types.
    AnalysisError("select * from functional.alltypes where " + "string_col in (bool_col, double_col)", "Incompatible return types 'STRING' and 'BOOLEAN' " + "of exprs 'string_col' and 'bool_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (int_col, double_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select * from functional.alltypes where " + "timestamp_col in (NULL, int_col)", "Incompatible return types 'TIMESTAMP' and 'INT' " + "of exprs 'timestamp_col' and 'int_col'.");
    AnalysisError("select 1 from functional.allcomplextypes where " + "int_array_col in (id, NULL)", "Incompatible return types 'ARRAY<INT>' and 'INT' " + "of exprs 'int_array_col' and 'id'.");
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    convertNumericLiteralsFromDecimal(analyzer);
    String opName = op_.getName().equals("null_matching_eq") ? "eq" : op_.getName();
    fn_ = getBuiltinFunction(analyzer, opName, collectChildReturnTypes(), CompareMode.IS_SUPERTYPE_OF);
    if (fn_ == null) {
        // Construct an appropriate error message and throw an AnalysisException.
        String errMsg = "operands of type " + getChild(0).getType().toSql() + " and " + getChild(1).getType().toSql() + " are not comparable: " + toSql();
        // scalar.
        for (Expr expr : children_) {
            if (expr instanceof Subquery && !expr.getType().isScalarType()) {
                errMsg = "Subquery must return a single row: " + expr.toSql();
                break;
            }
        }
        throw new AnalysisException(errMsg);
    }
    Preconditions.checkState(fn_.getReturnType().isBoolean());
    ArrayList<Expr> subqueries = Lists.newArrayList();
    collectAll(Predicates.instanceOf(Subquery.class), subqueries);
    if (subqueries.size() > 1) {
        // evaluation.
        throw new AnalysisException("Multiple subqueries are not supported in binary " + "predicates: " + toSql());
    }
    if ((contains(InPredicate.class) && !subqueries.isEmpty()) || contains(ExistsPredicate.class)) {
        throw new AnalysisException("IN and/or EXISTS subquery predicates are not " + "supported in binary predicates: " + toSql());
    }
    // required will be performed when the subquery is unnested.
    if (!contains(Subquery.class))
        castForFunctionCall(true);
    // determine selectivity
    // TODO: Compute selectivity for nested predicates
    Reference<SlotRef> slotRefRef = new Reference<SlotRef>();
    if (op_ == Operator.EQ && isSingleColumnPredicate(slotRefRef, null) && slotRefRef.getRef().getNumDistinctValues() > 0) {
        Preconditions.checkState(slotRefRef.getRef() != null);
        selectivity_ = 1.0 / slotRefRef.getRef().getNumDistinctValues();
        selectivity_ = Math.max(0, Math.min(1, selectivity_));
    } else {
        // TODO: improve using histograms, once they show up
        selectivity_ = Expr.DEFAULT_SELECTIVITY;
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    convertNumericLiteralsFromDecimal(analyzer);
    String opName = op_.getName().equals("null_matching_eq") ? "eq" : op_.getName();
    fn_ = getBuiltinFunction(analyzer, opName, collectChildReturnTypes(), CompareMode.IS_SUPERTYPE_OF);
    if (fn_ == null) {
        // Construct an appropriate error message and throw an AnalysisException.
        String errMsg = "operands of type " + getChild(0).getType().toSql() + " and " + getChild(1).getType().toSql() + " are not comparable: " + toSql();
        // scalar.
        for (Expr expr : children_) {
            if (expr instanceof Subquery && !expr.getType().isScalarType()) {
                errMsg = "Subquery must return a single row: " + expr.toSql();
                break;
            }
        }
        throw new AnalysisException(errMsg);
    }
    Preconditions.checkState(fn_.getReturnType().isBoolean());
    ArrayList<Expr> subqueries = Lists.newArrayList();
    collectAll(Predicates.instanceOf(Subquery.class), subqueries);
    if (subqueries.size() > 1) {
        // evaluation.
        throw new AnalysisException("Multiple subqueries are not supported in binary " + "predicates: " + toSql());
    }
    if (contains(ExistsPredicate.class)) {
        throw new AnalysisException("EXISTS subquery predicates are not " + "supported in binary predicates: " + toSql());
    }
    List<InPredicate> inPredicates = Lists.newArrayList();
    collect(InPredicate.class, inPredicates);
    for (InPredicate inPredicate : inPredicates) {
        if (inPredicate.contains(Subquery.class)) {
            throw new AnalysisException("IN subquery predicates are not supported in " + "binary predicates: " + toSql());
        }
    }
    // required will be performed when the subquery is unnested.
    if (!contains(Subquery.class))
        castForFunctionCall(true);
    // determine selectivity
    // TODO: Compute selectivity for nested predicates
    Reference<SlotRef> slotRefRef = new Reference<SlotRef>();
    if (op_ == Operator.EQ && isSingleColumnPredicate(slotRefRef, null) && slotRefRef.getRef().getNumDistinctValues() > 0) {
        Preconditions.checkState(slotRefRef.getRef() != null);
        selectivity_ = 1.0 / slotRefRef.getRef().getNumDistinctValues();
        selectivity_ = Math.max(0, Math.min(1, selectivity_));
    } else {
        // TODO: improve using histograms, once they show up
        selectivity_ = Expr.DEFAULT_SELECTIVITY;
    }
}
#end_block

#method_before
private void createDatabase(TCreateDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    String dbName = params.getDb();
    Preconditions.checkState(dbName != null && !dbName.isEmpty(), "Null or empty database name passed as argument to Catalog.createDatabase");
    if (params.if_not_exists && catalog_.getDb(dbName) != null) {
        LOG.debug("Skipping database creation because " + dbName + " already exists and " + "IF NOT EXISTS was specified.");
        resp.getResult().setVersion(catalog_.getCatalogVersion());
        return;
    }
    org.apache.hadoop.hive.metastore.api.Database db = new org.apache.hadoop.hive.metastore.api.Database();
    db.setName(dbName);
    if (params.getComment() != null) {
        db.setDescription(params.getComment());
    }
    if (params.getLocation() != null) {
        db.setLocationUri(params.getLocation());
    }
    LOG.debug("Creating database " + dbName);
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().createDatabase(db);
        } catch (AlreadyExistsException e) {
            if (!params.if_not_exists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating database %s because " + "IF NOT EXISTS was specified.", e, dbName));
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
        } finally {
            msClient.release();
        }
        Db newDb = catalog_.addDb(dbName);
        TCatalogObject thriftDb = new TCatalogObject(TCatalogObjectType.DATABASE, Catalog.INITIAL_CATALOG_VERSION);
        thriftDb.setDb(newDb.toThrift());
        thriftDb.setCatalog_version(newDb.getCatalogVersion());
        resp.result.setUpdated_catalog_object(thriftDb);
    }
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#method_after
private void createDatabase(TCreateDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    String dbName = params.getDb();
    Preconditions.checkState(dbName != null && !dbName.isEmpty(), "Null or empty database name passed as argument to Catalog.createDatabase");
    if (params.if_not_exists && catalog_.getDb(dbName) != null) {
        LOG.debug("Skipping database creation because " + dbName + " already exists and " + "IF NOT EXISTS was specified.");
        resp.getResult().setVersion(catalog_.getCatalogVersion());
        return;
    }
    org.apache.hadoop.hive.metastore.api.Database db = new org.apache.hadoop.hive.metastore.api.Database();
    db.setName(dbName);
    if (params.getComment() != null) {
        db.setDescription(params.getComment());
    }
    if (params.getLocation() != null) {
        db.setLocationUri(params.getLocation());
    }
    LOG.debug("Creating database " + dbName);
    Db newDb = null;
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().createDatabase(db);
            newDb = catalog_.addDb(dbName);
        } catch (AlreadyExistsException e) {
            if (!params.if_not_exists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating database %s because " + "IF NOT EXISTS was specified.", e, dbName));
            newDb = catalog_.getDb(dbName);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
        } finally {
            msClient.release();
        }
        Preconditions.checkNotNull(newDb);
        TCatalogObject thriftDb = new TCatalogObject(TCatalogObjectType.DATABASE, Catalog.INITIAL_CATALOG_VERSION);
        thriftDb.setDb(newDb.toThrift());
        thriftDb.setCatalog_version(newDb.getCatalogVersion());
        resp.result.setUpdated_catalog_object(thriftDb);
    }
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#end_block

#method_before
private void dropStats(TDropStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Table table = getExistingTable(params.getTable_name().getDb_name(), params.getTable_name().getTable_name());
    Preconditions.checkNotNull(table);
    if (params.getPartition_spec() == null) {
        // TODO: Report the number of updated partitions/columns to the user?
        dropColumnStats(table);
        dropTableStats(table);
    } else {
        HdfsPartition partition = ((HdfsTable) table).getPartitionFromThriftPartitionSpec(params.getPartition_spec());
        if (partition == null) {
            List<String> partitionDescription = Lists.newArrayList();
            for (TPartitionKeyValue v : params.getPartition_spec()) {
                partitionDescription.add(v.name + " = " + v.value);
            }
            throw new ImpalaRuntimeException("Could not find partition: " + Joiner.on("/").join(partitionDescription));
        }
        if (partition.getPartitionStats() != null) {
            synchronized (metastoreDdlLock_) {
                PartitionStatsUtil.deletePartStats(partition);
                // Remove the ROW_COUNT parameter if it has been set.
                partition.getParameters().remove(StatsSetupConst.ROW_COUNT);
                try {
                    applyAlterPartition(table.getTableName(), partition);
                } finally {
                    partition.markDirty();
                }
            }
        }
    }
    Table refreshedTable = catalog_.reloadTable(params.getTable_name());
    resp.result.setUpdated_catalog_object(TableToTCatalogObject(refreshedTable));
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#method_after
private void dropStats(TDropStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Table table = getExistingTable(params.getTable_name().getDb_name(), params.getTable_name().getTable_name());
    Preconditions.checkNotNull(table);
    if (params.getPartition_spec() == null) {
        // TODO: Report the number of updated partitions/columns to the user?
        dropColumnStats(table);
        dropTableStats(table);
    } else {
        HdfsPartition partition = ((HdfsTable) table).getPartitionFromThriftPartitionSpec(params.getPartition_spec());
        if (partition == null) {
            List<String> partitionDescription = Lists.newArrayList();
            for (TPartitionKeyValue v : params.getPartition_spec()) {
                partitionDescription.add(v.name + " = " + v.value);
            }
            throw new ImpalaRuntimeException("Could not find partition: " + Joiner.on("/").join(partitionDescription));
        }
        if (partition.getPartitionStats() != null) {
            synchronized (metastoreDdlLock_) {
                PartitionStatsUtil.deletePartStats(partition);
                try {
                    applyAlterPartition(table.getTableName(), partition);
                } finally {
                    partition.markDirty();
                }
            }
        }
    }
    Table refreshedTable = catalog_.reloadTable(params.getTable_name());
    resp.result.setUpdated_catalog_object(TableToTCatalogObject(refreshedTable));
    resp.result.setVersion(resp.result.getUpdated_catalog_object().getCatalog_version());
}
#end_block

#method_before
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    // Collects the cache directive IDs of any cached table/partitions that were
    // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
    // and the table will be refreshed asynchronously after all cache directives
    // complete.
    List<Long> cacheDirIds = Lists.<Long>newArrayList();
    // If the table is cached, get its cache pool name and replication factor. New
    // partitions will inherit this property.
    String cachePoolName = null;
    Short cacheReplication = 0;
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(table.getMetaStoreTable().getParameters());
    if (cacheDirId != null) {
        try {
            cachePoolName = HdfsCachingUtil.getCachePool(cacheDirId);
            cacheReplication = HdfsCachingUtil.getCacheReplication(cacheDirId);
            Preconditions.checkNotNull(cacheReplication);
            if (table.getNumClusteringCols() == 0)
                cacheDirIds.add(cacheDirId);
        } catch (ImpalaRuntimeException e) {
            // Catch the error so that the actual update to the catalog can progress,
            // this resets caching for the table though
            LOG.error(String.format("Cache directive %d was not found, uncache the table %s.%s" + "to remove this message.", cacheDirId, update.getDb_name(), update.getTarget_table()));
            cacheDirId = null;
        }
    }
    TableName tblName = new TableName(table.getDb().getName(), table.getName());
    if (table.getNumClusteringCols() > 0) {
        // Set of all partition names targeted by the insert that that need to be created
        // in the Metastore (partitions that do not currently exist in the catalog).
        // In the BE, we don't currently distinguish between which targeted partitions are
        // new and which already exist, so initialize the set with all targeted partition
        // names and remove the ones that are found to exist.
        Set<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
        for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
            // Skip dummy default partition.
            if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                continue;
            }
            // TODO: In the BE we build partition names without a trailing char. In FE we
            // build partition name with a trailing char. We should make this consistent.
            String partName = partition.getPartitionName() + "/";
            // returns true, it indicates the partition already exists.
            if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                // The partition was targeted by the insert and is also a cached. Since data
                // was written to the partition, a watch needs to be placed on the cache
                // cache directive so the TableLoadingMgr can perform an async refresh once
                // all data becomes cached.
                cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
            }
            if (partsToCreate.size() == 0)
                break;
        }
        if (!partsToCreate.isEmpty()) {
            MetaStoreClient msClient = catalog_.getMetaStoreClient();
            try {
                org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
                List<Partition> parts = Lists.newArrayList();
                for (String partName : partsToCreate) {
                    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
                    parts.add(partition);
                    partition.setDbName(tableName.getDb());
                    partition.setTableName(tableName.getTbl());
                    LinkedHashMap<String, String> hm = org.apache.hadoop.hive.metastore.Warehouse.makeSpecFromName(partName);
                    List<String> partVals = Lists.newArrayList();
                    for (FieldSchema fs : msTbl.getPartitionKeys()) {
                        String key = field.getName();
                        String val = hm.get(key);
                        if (val == null) {
                            throw new CatalogException("Incomplete partition name - missing " + key);
                        }
                        partVals.add(val);
                    }
                    partition.setValues(partVals);
                }
                List<Partition> addedParts = client.getHiveClient().add_partitions(parts, true, true);
                if (addedParts.size() > 0) {
                    if (cachePoolName != null) {
                        // directive id.
                        for (org.apache.hadoop.hive.metastore.api.Partition part : addedParts) {
                            cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(part, cachePoolName, cacheReplication));
                        }
                        msClient.getHiveClient().alter_partitions(tableName.getDb(), tableName.getTbl(), addedParts);
                    }
                    updateLastDdlTime(msTbl, msClient);
                }
            } catch (AlreadyExistsException e) {
            // Ignore since partition already exists.
            } catch (Exception e) {
                throw new InternalException("Error adding partitions", e);
            } finally {
                msClient.release();
            }
        }
    }
    // Submit the watch request for the given cache directives.
    if (!cacheDirIds.isEmpty())
        catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    // Perform an incremental refresh to load new/modified partitions and files.
    Table refreshedTbl = catalog_.reloadTable(tblName.toThrift());
    response.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    response.getResult().setVersion(response.getResult().getUpdated_catalog_object().getCatalog_version());
    return response;
}
#method_after
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    // Collects the cache directive IDs of any cached table/partitions that were
    // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
    // and the table will be refreshed asynchronously after all cache directives
    // complete.
    List<Long> cacheDirIds = Lists.<Long>newArrayList();
    // If the table is cached, get its cache pool name and replication factor. New
    // partitions will inherit this property.
    String cachePoolName = null;
    Short cacheReplication = 0;
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(table.getMetaStoreTable().getParameters());
    if (cacheDirId != null) {
        try {
            cachePoolName = HdfsCachingUtil.getCachePool(cacheDirId);
            cacheReplication = HdfsCachingUtil.getCacheReplication(cacheDirId);
            Preconditions.checkNotNull(cacheReplication);
            if (table.getNumClusteringCols() == 0)
                cacheDirIds.add(cacheDirId);
        } catch (ImpalaRuntimeException e) {
            // Catch the error so that the actual update to the catalog can progress,
            // this resets caching for the table though
            LOG.error(String.format("Cache directive %d was not found, uncache the table %s.%s" + "to remove this message.", cacheDirId, update.getDb_name(), update.getTarget_table()));
            cacheDirId = null;
        }
    }
    TableName tblName = new TableName(table.getDb().getName(), table.getName());
    List<String> errorMessages = Lists.newArrayList();
    if (table.getNumClusteringCols() > 0) {
        // Set of all partition names targeted by the insert that that need to be created
        // in the Metastore (partitions that do not currently exist in the catalog).
        // In the BE, we don't currently distinguish between which targeted partitions are
        // new and which already exist, so initialize the set with all targeted partition
        // names and remove the ones that are found to exist.
        Set<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
        for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
            // Skip dummy default partition.
            if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                continue;
            }
            // TODO: In the BE we build partition names without a trailing char. In FE we
            // build partition name with a trailing char. We should make this consistent.
            String partName = partition.getPartitionName() + "/";
            // returns true, it indicates the partition already exists.
            if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                // The partition was targeted by the insert and is also a cached. Since data
                // was written to the partition, a watch needs to be placed on the cache
                // cache directive so the TableLoadingMgr can perform an async refresh once
                // all data becomes cached.
                cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
            }
            if (partsToCreate.size() == 0)
                break;
        }
        if (!partsToCreate.isEmpty()) {
            MetaStoreClient msClient = catalog_.getMetaStoreClient();
            try {
                org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tblName);
                List<org.apache.hadoop.hive.metastore.api.Partition> hmsParts = Lists.newArrayList();
                HiveConf hiveConf = new HiveConf(this.getClass());
                Warehouse warehouse = new Warehouse(hiveConf);
                for (String partName : partsToCreate) {
                    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
                    hmsParts.add(partition);
                    partition.setDbName(tblName.getDb());
                    partition.setTableName(tblName.getTbl());
                    partition.setValues(getPartValsFromName(msTbl, partName));
                    partition.setParameters(new HashMap<String, String>());
                    partition.setSd(msTbl.getSd().deepCopy());
                    partition.getSd().setSerdeInfo(msTbl.getSd().getSerdeInfo().deepCopy());
                    partition.getSd().setLocation(msTbl.getSd().getLocation() + "/" + partName.substring(0, partName.length() - 1));
                    MetaStoreUtils.updatePartitionStatsFast(partition, warehouse);
                }
                // First add_partitions and then alter_partitions the successful ones with
                // caching directives. The reason is that some partitions could have been
                // added concurrently, and we want to avoid caching a partition twice and
                // leaking a caching directive.
                List<org.apache.hadoop.hive.metastore.api.Partition> addedHmsParts = msClient.getHiveClient().add_partitions(hmsParts, true, true);
                if (addedHmsParts.size() > 0) {
                    if (cachePoolName != null) {
                        List<org.apache.hadoop.hive.metastore.api.Partition> cachedHmsParts = Lists.newArrayList();
                        // the directive id.
                        for (org.apache.hadoop.hive.metastore.api.Partition part : addedHmsParts) {
                            try {
                                cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(part, cachePoolName, cacheReplication));
                                cachedHmsParts.add(part);
                            } catch (ImpalaRuntimeException e) {
                                String msg = String.format("Partition %s.%s(%s): State: Not cached." + " Action: Cache manully via 'ALTER TABLE'.", part.getDbName(), part.getTableName(), part.getValues());
                                LOG.error(msg, e);
                                errorMessages.add(msg);
                            }
                        }
                        try {
                            msClient.getHiveClient().alter_partitions(tblName.getDb(), tblName.getTbl(), cachedHmsParts);
                        } catch (Exception e) {
                            LOG.error("Failed in alter_partitions: ", e);
                            // Try to uncache the partitions when the alteration in the HMS failed.
                            for (org.apache.hadoop.hive.metastore.api.Partition part : cachedHmsParts) {
                                try {
                                    HdfsCachingUtil.uncachePartition(part);
                                } catch (ImpalaException e1) {
                                    String msg = String.format("Partition %s.%s(%s): State: Leaked caching directive. " + "Action: Manually uncache directory %s via hdfs cacheAdmin.", part.getDbName(), part.getTableName(), part.getValues(), part.getSd().getLocation());
                                    LOG.error(msg, e);
                                    errorMessages.add(msg);
                                }
                            }
                        }
                    }
                    updateLastDdlTime(msTbl, msClient);
                }
            } catch (AlreadyExistsException e) {
                throw new InternalException("AlreadyExistsException thrown although ifNotExists given", e);
            } catch (Exception e) {
                throw new InternalException("Error adding partitions", e);
            } finally {
                msClient.release();
            }
        }
    }
    // Submit the watch request for the given cache directives.
    if (!cacheDirIds.isEmpty())
        catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (errorMessages.size() > 0) {
        errorMessages.add("Please refer to the catalogd error log for details " + "regarding the failed un/caching operations.");
        response.getResult().setStatus(new TStatus(TErrorCode.INTERNAL_ERROR, errorMessages));
    } else {
        response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    }
    // Perform an incremental refresh to load new/modified partitions and files.
    Table refreshedTbl = catalog_.reloadTable(tblName.toThrift());
    response.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    response.getResult().setVersion(response.getResult().getUpdated_catalog_object().getCatalog_version());
    return response;
}
#end_block

#method_before
private PlanNode createInlineViewPlan(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    // If possible, "push down" view predicates; this is needed in order to ensure
    // that predicates such as "x + y = 10" are evaluated in the view's plan tree
    // rather than a SelectNode grafted on top of that plan tree.
    // This doesn't prevent predicate propagation, because predicates like
    // "x = 10" that get pushed down are still connected to equivalent slots
    // via the equality predicates created for the view's select list.
    // Include outer join conjuncts here as well because predicates from the
    // On-clause of an outer join may be pushed into the inline view as well.
    List<Expr> unassigned = inlineViewRef.propagateUnassignedConjuncts(analyzer);
    // mark (fully resolve) slots referenced by remaining unassigned conjuncts_ as
    // materialized
    List<Expr> substUnassigned = Expr.substituteList(unassigned, inlineViewRef.getBaseTblSmap(), analyzer, false);
    analyzer.materializeSlots(substUnassigned);
    // Turn a constant select into a UnionNode that materializes the exprs.
    // TODO: unify this with createConstantSelectPlan(), this is basically the
    // same thing
    QueryStmt viewStmt = inlineViewRef.getViewStmt();
    if (viewStmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) viewStmt;
        if (selectStmt.getTableRefs().isEmpty()) {
            if (inlineViewRef.getAnalyzer().hasEmptyResultSet()) {
                return createEmptyNode(viewStmt, inlineViewRef.getAnalyzer());
            }
            // Analysis should have generated a tuple id_ into which to materialize the exprs.
            Preconditions.checkState(inlineViewRef.getMaterializedTupleIds().size() == 1);
            // we need to materialize all slots of our inline view tuple
            analyzer.getTupleDesc(inlineViewRef.getId()).materializeSlots();
            UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), inlineViewRef.getMaterializedTupleIds().get(0));
            if (analyzer.hasEmptyResultSet())
                return unionNode;
            unionNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
            unionNode.addConstExprList(selectStmt.getBaseTblResultExprs());
            unionNode.init(analyzer);
            return unionNode;
        }
    }
    PlanNode rootNode = createQueryPlan(inlineViewRef.getViewStmt(), inlineViewRef.getAnalyzer(), false);
    // TODO: we should compute the "physical layout" of the view's descriptor, so that
    // the avg row size is availble during optimization; however, that means we need to
    // select references to its resultExprs from the enclosing scope(s)
    rootNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
    // Set smap *before* creating a SelectNode in order to allow proper resolution.
    // Analytics have an additional level of logical to physical slot remapping.
    // The composition creates a mapping from the logical output of the inline view
    // to the physical analytic output. In addition, it retains the logical to
    // physical analytic slot mappings which are needed to resolve exprs that already
    // reference the logical analytic tuple (and not the inline view tuple), e.g.,
    // the result exprs set in the coordinator fragment.
    rootNode.setOutputSmap(ExprSubstitutionMap.compose(inlineViewRef.getBaseTblSmap(), rootNode.getOutputSmap(), analyzer));
    // root node.
    if (inlineViewRef.getViewStmt().hasLimit() || inlineViewRef.getViewStmt().hasOffset() || !unassigned.isEmpty()) {
        rootNode = addUnassignedConjuncts(analyzer, inlineViewRef.getDesc().getId().asList(), rootNode);
    }
    return rootNode;
}
#method_after
private PlanNode createInlineViewPlan(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    // If possible, "push down" view predicates; this is needed in order to ensure
    // that predicates such as "x + y = 10" are evaluated in the view's plan tree
    // rather than a SelectNode grafted on top of that plan tree.
    // This doesn't prevent predicate propagation, because predicates like
    // "x = 10" that get pushed down are still connected to equivalent slots
    // via the equality predicates created for the view's select list.
    // Include outer join conjuncts here as well because predicates from the
    // On-clause of an outer join may be pushed into the inline view as well.
    migrateConjunctsToInlineView(analyzer, inlineViewRef);
    // Turn a constant select into a UnionNode that materializes the exprs.
    // TODO: unify this with createConstantSelectPlan(), this is basically the
    // same thing
    QueryStmt viewStmt = inlineViewRef.getViewStmt();
    if (viewStmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) viewStmt;
        if (selectStmt.getTableRefs().isEmpty()) {
            if (inlineViewRef.getAnalyzer().hasEmptyResultSet()) {
                return createEmptyNode(viewStmt, inlineViewRef.getAnalyzer());
            }
            // Analysis should have generated a tuple id_ into which to materialize the exprs.
            Preconditions.checkState(inlineViewRef.getMaterializedTupleIds().size() == 1);
            // we need to materialize all slots of our inline view tuple
            analyzer.getTupleDesc(inlineViewRef.getId()).materializeSlots();
            UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), inlineViewRef.getMaterializedTupleIds().get(0));
            if (analyzer.hasEmptyResultSet())
                return unionNode;
            unionNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
            unionNode.addConstExprList(selectStmt.getBaseTblResultExprs());
            unionNode.init(analyzer);
            return unionNode;
        }
    }
    PlanNode rootNode = createQueryPlan(inlineViewRef.getViewStmt(), inlineViewRef.getAnalyzer(), false);
    // TODO: we should compute the "physical layout" of the view's descriptor, so that
    // the avg row size is availble during optimization; however, that means we need to
    // select references to its resultExprs from the enclosing scope(s)
    rootNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
    // Set smap *before* creating a SelectNode in order to allow proper resolution.
    // Analytics have an additional level of logical to physical slot remapping.
    // The composition creates a mapping from the logical output of the inline view
    // to the physical analytic output. In addition, it retains the logical to
    // physical analytic slot mappings which are needed to resolve exprs that already
    // reference the logical analytic tuple (and not the inline view tuple), e.g.,
    // the result exprs set in the coordinator fragment.
    rootNode.setOutputSmap(ExprSubstitutionMap.compose(inlineViewRef.getBaseTblSmap(), rootNode.getOutputSmap(), analyzer));
    // place.
    if (!canMigrateConjuncts(inlineViewRef)) {
        rootNode = addUnassignedConjuncts(analyzer, inlineViewRef.getDesc().getId().asList(), rootNode);
    }
    return rootNode;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // not into it)
    if (queryStmt_ instanceof SelectStmt && ((SelectStmt) queryStmt_).hasAnalyticInfo()) {
        ((SelectStmt) queryStmt_).getAnalyticInfo().computeCommonGroupingSlotRefs();
    }
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i);
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        SlotDescriptor slotDesc = analyzer.registerColumnRef(getAliasAsName(), colName);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (canPushExprIntoInlineView(colExpr)) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // not into it)
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i);
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        SlotDescriptor slotDesc = analyzer.registerColumnRef(getAliasAsName(), colName);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicate(colExpr)) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#end_block

#method_before
private static BoolLiteral replaceExistsPredicate(ExistsPredicate predicate) {
    Subquery subquery = predicate.getSubquery();
    Preconditions.checkNotNull(subquery);
    SelectStmt subqueryStmt = (SelectStmt) subquery.getStatement();
    BoolLiteral boolLiteral = null;
    boolean result;
    if (subqueryStmt.getLimit() == 0) {
        result = predicate.isNotExists() ? true : false;
        boolLiteral = new BoolLiteral(result);
    } else if (subqueryStmt.hasAggInfo() && subqueryStmt.getAggInfo().hasAggregateExprs() && !subqueryStmt.hasAnalyticInfo()) {
        result = predicate.isNotExists() ? false : true;
        boolLiteral = new BoolLiteral(result);
    }
    return boolLiteral;
}
#method_after
private static BoolLiteral replaceExistsPredicate(ExistsPredicate predicate) {
    Subquery subquery = predicate.getSubquery();
    Preconditions.checkNotNull(subquery);
    SelectStmt subqueryStmt = (SelectStmt) subquery.getStatement();
    BoolLiteral boolLiteral = null;
    if (subqueryStmt.getAnalyzer().hasEmptyResultSet()) {
        boolLiteral = new BoolLiteral(predicate.isNotExists());
    } else if (subqueryStmt.hasAggInfo() && subqueryStmt.getAggInfo().hasAggregateExprs() && !subqueryStmt.hasAnalyticInfo() && subqueryStmt.getHavingPred() == null) {
        boolLiteral = new BoolLiteral(!predicate.isNotExists());
    }
    return boolLiteral;
}
#end_block

#method_before
private static boolean mergeExpr(SelectStmt stmt, Expr expr, Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(expr);
    Preconditions.checkNotNull(analyzer);
    boolean updateSelectList = false;
    SelectStmt subqueryStmt = (SelectStmt) expr.getSubquery().getStatement();
    // Create a new inline view from the subquery stmt. The inline view will be added
    // to the stmt's table refs later. Explicitly set the inline view's column labels
    // to eliminate any chance that column aliases from the parent query could reference
    // select items from the inline view after the rewrite.
    List<String> colLabels = Lists.newArrayList();
    for (int i = 0; i < subqueryStmt.getColLabels().size(); ++i) {
        colLabels.add(subqueryStmt.getColumnAliasGenerator().getNextAlias());
    }
    InlineViewRef inlineView = new InlineViewRef(stmt.getTableAliasGenerator().getNextAlias(), subqueryStmt, colLabels);
    // Extract all correlated predicates from the subquery.
    List<Expr> onClauseConjuncts = extractCorrelatedPredicates(subqueryStmt);
    if (!onClauseConjuncts.isEmpty()) {
        canRewriteCorrelatedSubquery(expr);
        // For correlated subqueries that are eligible for rewrite by transforming
        // into a join, a LIMIT clause has no effect on the results, so we can
        // safely remove it.
        subqueryStmt.limitElement_ = null;
    }
    if (expr instanceof ExistsPredicate) {
        // Remove any DISTINCT clause as it impedes certain rewrites and does not affect
        // the result of EXISTS subqueries.
        subqueryStmt.getSelectList().setIsDistinct(false);
        // subquery.
        if (onClauseConjuncts.isEmpty())
            subqueryStmt.setLimit(1);
    }
    // Update the subquery's select list and/or its GROUP BY clause by adding
    // exprs from the extracted correlated predicates.
    boolean updateGroupBy = expr.getSubquery().isScalarSubquery() || (expr instanceof ExistsPredicate && ((subqueryStmt.hasAggInfo() && subqueryStmt.getAggInfo().hasAggregateExprs()) || subqueryStmt.hasGroupByClause()));
    List<Expr> lhsExprs = Lists.newArrayList();
    List<Expr> rhsExprs = Lists.newArrayList();
    for (Expr conjunct : onClauseConjuncts) {
        updateInlineView(inlineView, conjunct, stmt.getTableRefIds(), lhsExprs, rhsExprs, updateGroupBy);
    }
    // Analyzing the inline view trigger reanalysis of the subquery's select statement.
    // However the statement is already analyzed and since statement analysis is not
    // idempotent, the analysis needs to be reset (by a call to clone()).
    inlineView = (InlineViewRef) inlineView.clone();
    inlineView.analyze(analyzer);
    inlineView.setLeftTblRef(stmt.tableRefs_.get(stmt.tableRefs_.size() - 1));
    stmt.tableRefs_.add(inlineView);
    JoinOperator joinOp = JoinOperator.LEFT_SEMI_JOIN;
    // Create a join conjunct from the expr that contains a subquery.
    Expr joinConjunct = createJoinConjunct(expr, inlineView, analyzer, !onClauseConjuncts.isEmpty());
    if (joinConjunct != null) {
        SelectListItem firstItem = ((SelectStmt) inlineView.getViewStmt()).getSelectList().getItems().get(0);
        if (!onClauseConjuncts.isEmpty() && firstItem.getExpr().contains(Expr.NON_NULL_EMPTY_AGG)) {
            // Correlated subqueries with an aggregate function that returns non-null on
            // an empty input are rewritten using a LEFT OUTER JOIN because we
            // need to ensure that there is one agg value for every tuple of 'stmt'
            // (parent select block), even for those tuples of 'stmt' that get rejected
            // by the subquery due to some predicate. The new join conjunct is added to
            // stmt's WHERE clause because it needs to be applied to the result of the
            // LEFT OUTER JOIN (both matched and unmatched tuples).
            // 
            // TODO Handle other aggregate functions and UDAs that return a non-NULL value
            // on an empty set.
            // TODO Handle count aggregate functions in an expression in subqueries
            // select list.
            stmt.whereClause_ = CompoundPredicate.createConjunction(joinConjunct, stmt.whereClause_);
            joinConjunct = null;
            joinOp = JoinOperator.LEFT_OUTER_JOIN;
            updateSelectList = true;
        }
        if (joinConjunct != null)
            onClauseConjuncts.add(joinConjunct);
    }
    // Create the ON clause from the extracted correlated predicates.
    Expr onClausePredicate = CompoundPredicate.createConjunctivePredicate(onClauseConjuncts);
    if (onClausePredicate == null) {
        Preconditions.checkState(expr instanceof ExistsPredicate);
        // TODO: Remove this when we support independent subquery evaluation.
        if (((ExistsPredicate) expr).isNotExists()) {
            throw new AnalysisException("Unsupported uncorrelated NOT EXISTS subquery: " + subqueryStmt.toSql());
        }
        // We don't have an ON clause predicate to create an equi-join. Rewrite the
        // subquery using a CROSS JOIN.
        // TODO This is very expensive. Remove it when we implement independent
        // subquery evaluation.
        inlineView.setJoinOp(JoinOperator.CROSS_JOIN);
        LOG.warn("uncorrelated subquery rewritten using a cross join");
        // Indicate that new visible tuples may be added in stmt's select list.
        return true;
    }
    // Create an smap from the original select-list exprs of the select list to
    // the corresponding inline-view columns.
    ExprSubstitutionMap smap = new ExprSubstitutionMap();
    Preconditions.checkState(lhsExprs.size() == rhsExprs.size());
    for (int i = 0; i < lhsExprs.size(); ++i) {
        Expr lhsExpr = lhsExprs.get(i);
        Expr rhsExpr = rhsExprs.get(i);
        rhsExpr.analyze(analyzer);
        smap.put(lhsExpr, rhsExpr);
    }
    onClausePredicate = onClausePredicate.substitute(smap, analyzer, false);
    // graph of query blocks are not supported).
    if (!onClausePredicate.isBoundByTupleIds(stmt.getTableRefIds())) {
        throw new AnalysisException("Unsupported correlated subquery: " + subqueryStmt.toSql());
    }
    // Check if we have a valid ON clause for an equi-join.
    boolean hasEqJoinPred = false;
    for (Expr conjunct : onClausePredicate.getConjuncts()) {
        if (!(conjunct instanceof BinaryPredicate) || ((BinaryPredicate) conjunct).getOp() != BinaryPredicate.Operator.EQ) {
            continue;
        }
        List<TupleId> lhsTupleIds = Lists.newArrayList();
        conjunct.getChild(0).getIds(lhsTupleIds, null);
        if (lhsTupleIds.isEmpty())
            continue;
        List<TupleId> rhsTupleIds = Lists.newArrayList();
        conjunct.getChild(1).getIds(rhsTupleIds, null);
        if (rhsTupleIds.isEmpty())
            continue;
        // of the binary predicate.
        if ((lhsTupleIds.contains(inlineView.getDesc().getId()) && lhsTupleIds.size() > 1) || (rhsTupleIds.contains(inlineView.getDesc().getId()) && rhsTupleIds.size() > 1)) {
            continue;
        }
        hasEqJoinPred = true;
        break;
    }
    if (!hasEqJoinPred) {
        // TODO: Remove this when independent subquery evaluation is implemented.
        // TODO: Requires support for non-equi joins.
        boolean hasGroupBy = ((SelectStmt) inlineView.getViewStmt()).hasGroupByClause();
        if (!expr.getSubquery().isScalarSubquery() || (!(hasGroupBy && stmt.selectList_.isDistinct()) && hasGroupBy)) {
            throw new AnalysisException("Unsupported predicate with subquery: " + expr.toSql());
        }
        // We can rewrite the aggregate subquery using a cross join. All conjuncts
        // that were extracted from the subquery are added to stmt's WHERE clause.
        stmt.whereClause_ = CompoundPredicate.createConjunction(onClausePredicate, stmt.whereClause_);
        inlineView.setJoinOp(JoinOperator.CROSS_JOIN);
        // select list (if the latter contains an unqualified star item '*')
        return true;
    }
    // We have a valid equi-join conjunct.
    if (expr instanceof InPredicate && ((InPredicate) expr).isNotIn() || expr instanceof ExistsPredicate && ((ExistsPredicate) expr).isNotExists()) {
        // conjunct with a conjunct that uses the null-matching eq operator.
        if (expr instanceof InPredicate) {
            joinOp = JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN;
            List<TupleId> tIds = Lists.newArrayList();
            joinConjunct.getIds(tIds, null);
            if (tIds.size() <= 1 || !tIds.contains(inlineView.getDesc().getId())) {
                throw new AnalysisException("Unsupported NOT IN predicate with subquery: " + expr.toSql());
            }
            // null-matching EQ operator.
            for (Expr conjunct : onClausePredicate.getConjuncts()) {
                if (conjunct.equals(joinConjunct)) {
                    Preconditions.checkState(conjunct instanceof BinaryPredicate);
                    Preconditions.checkState(((BinaryPredicate) conjunct).getOp() == BinaryPredicate.Operator.EQ);
                    ((BinaryPredicate) conjunct).setOp(BinaryPredicate.Operator.NULL_MATCHING_EQ);
                    break;
                }
            }
        } else {
            joinOp = JoinOperator.LEFT_ANTI_JOIN;
        }
    }
    inlineView.setJoinOp(joinOp);
    inlineView.setOnClause(onClausePredicate);
    return updateSelectList;
}
#method_after
private static boolean mergeExpr(SelectStmt stmt, Expr expr, Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(expr);
    Preconditions.checkNotNull(analyzer);
    boolean updateSelectList = false;
    SelectStmt subqueryStmt = (SelectStmt) expr.getSubquery().getStatement();
    // Create a new inline view from the subquery stmt. The inline view will be added
    // to the stmt's table refs later. Explicitly set the inline view's column labels
    // to eliminate any chance that column aliases from the parent query could reference
    // select items from the inline view after the rewrite.
    List<String> colLabels = Lists.newArrayList();
    for (int i = 0; i < subqueryStmt.getColLabels().size(); ++i) {
        colLabels.add(subqueryStmt.getColumnAliasGenerator().getNextAlias());
    }
    InlineViewRef inlineView = new InlineViewRef(stmt.getTableAliasGenerator().getNextAlias(), subqueryStmt, colLabels);
    // Extract all correlated predicates from the subquery.
    List<Expr> onClauseConjuncts = extractCorrelatedPredicates(subqueryStmt);
    if (!onClauseConjuncts.isEmpty()) {
        canRewriteCorrelatedSubquery(expr);
        // For correlated subqueries that are eligible for rewrite by transforming
        // into a join, a LIMIT clause has no effect on the results, so we can
        // safely remove it.
        subqueryStmt.limitElement_ = null;
    }
    if (expr instanceof ExistsPredicate) {
        // subquery.
        if (onClauseConjuncts.isEmpty())
            subqueryStmt.setLimit(1);
    }
    // Update the subquery's select list and/or its GROUP BY clause by adding
    // exprs from the extracted correlated predicates.
    boolean updateGroupBy = expr.getSubquery().isScalarSubquery() || (expr instanceof ExistsPredicate && subqueryStmt.hasAggInfo() && !subqueryStmt.getSelectList().isDistinct());
    List<Expr> lhsExprs = Lists.newArrayList();
    List<Expr> rhsExprs = Lists.newArrayList();
    for (Expr conjunct : onClauseConjuncts) {
        updateInlineView(inlineView, conjunct, stmt.getTableRefIds(), lhsExprs, rhsExprs, updateGroupBy);
    }
    // Analyzing the inline view trigger reanalysis of the subquery's select statement.
    // However the statement is already analyzed and since statement analysis is not
    // idempotent, the analysis needs to be reset (by a call to clone()).
    inlineView = (InlineViewRef) inlineView.clone();
    inlineView.analyze(analyzer);
    inlineView.setLeftTblRef(stmt.tableRefs_.get(stmt.tableRefs_.size() - 1));
    stmt.tableRefs_.add(inlineView);
    JoinOperator joinOp = JoinOperator.LEFT_SEMI_JOIN;
    // Create a join conjunct from the expr that contains a subquery.
    Expr joinConjunct = createJoinConjunct(expr, inlineView, analyzer, !onClauseConjuncts.isEmpty());
    if (joinConjunct != null) {
        SelectListItem firstItem = ((SelectStmt) inlineView.getViewStmt()).getSelectList().getItems().get(0);
        if (!onClauseConjuncts.isEmpty() && firstItem.getExpr().contains(Expr.NON_NULL_EMPTY_AGG)) {
            // Correlated subqueries with an aggregate function that returns non-null on
            // an empty input are rewritten using a LEFT OUTER JOIN because we
            // need to ensure that there is one agg value for every tuple of 'stmt'
            // (parent select block), even for those tuples of 'stmt' that get rejected
            // by the subquery due to some predicate. The new join conjunct is added to
            // stmt's WHERE clause because it needs to be applied to the result of the
            // LEFT OUTER JOIN (both matched and unmatched tuples).
            // 
            // TODO Handle other aggregate functions and UDAs that return a non-NULL value
            // on an empty set.
            // TODO Handle count aggregate functions in an expression in subqueries
            // select list.
            stmt.whereClause_ = CompoundPredicate.createConjunction(joinConjunct, stmt.whereClause_);
            joinConjunct = null;
            joinOp = JoinOperator.LEFT_OUTER_JOIN;
            updateSelectList = true;
        }
        if (joinConjunct != null)
            onClauseConjuncts.add(joinConjunct);
    }
    // Create the ON clause from the extracted correlated predicates.
    Expr onClausePredicate = CompoundPredicate.createConjunctivePredicate(onClauseConjuncts);
    if (onClausePredicate == null) {
        Preconditions.checkState(expr instanceof ExistsPredicate);
        // TODO: Remove this when we support independent subquery evaluation.
        if (((ExistsPredicate) expr).isNotExists()) {
            throw new AnalysisException("Unsupported uncorrelated NOT EXISTS subquery: " + subqueryStmt.toSql());
        }
        // We don't have an ON clause predicate to create an equi-join. Rewrite the
        // subquery using a CROSS JOIN.
        // TODO This is very expensive. Remove it when we implement independent
        // subquery evaluation.
        inlineView.setJoinOp(JoinOperator.CROSS_JOIN);
        LOG.warn("uncorrelated subquery rewritten using a cross join");
        // Indicate that new visible tuples may be added in stmt's select list.
        return true;
    }
    // Create an smap from the original select-list exprs of the select list to
    // the corresponding inline-view columns.
    ExprSubstitutionMap smap = new ExprSubstitutionMap();
    Preconditions.checkState(lhsExprs.size() == rhsExprs.size());
    for (int i = 0; i < lhsExprs.size(); ++i) {
        Expr lhsExpr = lhsExprs.get(i);
        Expr rhsExpr = rhsExprs.get(i);
        rhsExpr.analyze(analyzer);
        smap.put(lhsExpr, rhsExpr);
    }
    onClausePredicate = onClausePredicate.substitute(smap, analyzer, false);
    // graph of query blocks are not supported).
    if (!onClausePredicate.isBoundByTupleIds(stmt.getTableRefIds())) {
        throw new AnalysisException("Unsupported correlated subquery: " + subqueryStmt.toSql());
    }
    // Check if we have a valid ON clause for an equi-join.
    boolean hasEqJoinPred = false;
    for (Expr conjunct : onClausePredicate.getConjuncts()) {
        if (!(conjunct instanceof BinaryPredicate) || ((BinaryPredicate) conjunct).getOp() != BinaryPredicate.Operator.EQ) {
            continue;
        }
        List<TupleId> lhsTupleIds = Lists.newArrayList();
        conjunct.getChild(0).getIds(lhsTupleIds, null);
        if (lhsTupleIds.isEmpty())
            continue;
        List<TupleId> rhsTupleIds = Lists.newArrayList();
        conjunct.getChild(1).getIds(rhsTupleIds, null);
        if (rhsTupleIds.isEmpty())
            continue;
        // of the binary predicate.
        if ((lhsTupleIds.contains(inlineView.getDesc().getId()) && lhsTupleIds.size() > 1) || (rhsTupleIds.contains(inlineView.getDesc().getId()) && rhsTupleIds.size() > 1)) {
            continue;
        }
        hasEqJoinPred = true;
        break;
    }
    if (!hasEqJoinPred) {
        // TODO: Remove this when independent subquery evaluation is implemented.
        // TODO: Requires support for non-equi joins.
        boolean hasGroupBy = ((SelectStmt) inlineView.getViewStmt()).hasGroupByClause();
        if (!expr.getSubquery().isScalarSubquery() || (!(hasGroupBy && stmt.selectList_.isDistinct()) && hasGroupBy)) {
            throw new AnalysisException("Unsupported predicate with subquery: " + expr.toSql());
        }
        // We can rewrite the aggregate subquery using a cross join. All conjuncts
        // that were extracted from the subquery are added to stmt's WHERE clause.
        stmt.whereClause_ = CompoundPredicate.createConjunction(onClausePredicate, stmt.whereClause_);
        inlineView.setJoinOp(JoinOperator.CROSS_JOIN);
        // select list (if the latter contains an unqualified star item '*')
        return true;
    }
    // We have a valid equi-join conjunct.
    if (expr instanceof InPredicate && ((InPredicate) expr).isNotIn() || expr instanceof ExistsPredicate && ((ExistsPredicate) expr).isNotExists()) {
        // conjunct with a conjunct that uses the null-matching eq operator.
        if (expr instanceof InPredicate) {
            joinOp = JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN;
            List<TupleId> tIds = Lists.newArrayList();
            joinConjunct.getIds(tIds, null);
            if (tIds.size() <= 1 || !tIds.contains(inlineView.getDesc().getId())) {
                throw new AnalysisException("Unsupported NOT IN predicate with subquery: " + expr.toSql());
            }
            // null-matching EQ operator.
            for (Expr conjunct : onClausePredicate.getConjuncts()) {
                if (conjunct.equals(joinConjunct)) {
                    Preconditions.checkState(conjunct instanceof BinaryPredicate);
                    Preconditions.checkState(((BinaryPredicate) conjunct).getOp() == BinaryPredicate.Operator.EQ);
                    ((BinaryPredicate) conjunct).setOp(BinaryPredicate.Operator.NULL_MATCHING_EQ);
                    break;
                }
            }
        } else {
            joinOp = JoinOperator.LEFT_ANTI_JOIN;
        }
    }
    inlineView.setJoinOp(joinOp);
    inlineView.setOnClause(onClausePredicate);
    return updateSelectList;
}
#end_block

#method_before
private static void updateInlineView(InlineViewRef inlineView, Expr expr, List<TupleId> parentQueryTids, List<Expr> lhsExprs, List<Expr> rhsExprs, boolean updateGroupBy) throws AnalysisException {
    SelectStmt stmt = (SelectStmt) inlineView.getViewStmt();
    List<TupleId> subqueryTblIds = stmt.getTableRefIds();
    ArrayList<Expr> groupByExprs = null;
    if (updateGroupBy)
        groupByExprs = Lists.newArrayList();
    List<SelectListItem> items = stmt.selectList_.getItems();
    // Collect all the SlotRefs from 'expr' and identify those that are bound by
    // subquery tuple ids.
    ArrayList<Expr> slotRefs = Lists.newArrayList();
    expr.collectAll(Predicates.instanceOf(SlotRef.class), slotRefs);
    List<Expr> exprsBoundBySubqueryTids = Lists.newArrayList();
    for (Expr slotRef : slotRefs) {
        if (slotRef.isBoundByTupleIds(subqueryTblIds)) {
            exprsBoundBySubqueryTids.add(slotRef);
        }
    }
    // no need to update the subquery's select or group by list.
    if (exprsBoundBySubqueryTids.isEmpty())
        return;
    if (updateGroupBy) {
        Preconditions.checkState(expr instanceof BinaryPredicate);
        Expr exprBoundBySubqueryTids = null;
        if (exprsBoundBySubqueryTids.size() > 1) {
            // ids, they must all be on the same side of that predicate.
            if (expr.getChild(0).isBoundByTupleIds(subqueryTblIds) && expr.getChild(1).isBoundByTupleIds(parentQueryTids)) {
                exprBoundBySubqueryTids = expr.getChild(0);
            } else if (expr.getChild(0).isBoundByTupleIds(parentQueryTids) && expr.getChild(1).isBoundByTupleIds(subqueryTblIds)) {
                exprBoundBySubqueryTids = expr.getChild(1);
            } else {
                throw new AnalysisException("All subquery columns " + "that participate in a predicate must be on the same side of " + "that predicate: " + expr.toSql());
            }
        } else {
            Preconditions.checkState(exprsBoundBySubqueryTids.size() == 1);
            exprBoundBySubqueryTids = exprsBoundBySubqueryTids.get(0);
        }
        exprsBoundBySubqueryTids.clear();
        exprsBoundBySubqueryTids.add(exprBoundBySubqueryTids);
    }
    // added to an ExprSubstitutionMap.
    for (Expr boundExpr : exprsBoundBySubqueryTids) {
        String colAlias = stmt.getColumnAliasGenerator().getNextAlias();
        items.add(new SelectListItem(boundExpr, null));
        inlineView.getExplicitColLabels().add(colAlias);
        lhsExprs.add(boundExpr);
        rhsExprs.add(new SlotRef(inlineView.getAliasAsName(), colAlias));
        if (groupByExprs != null)
            groupByExprs.add(boundExpr);
    }
    // Update the subquery's select list.
    boolean isDistinct = stmt.selectList_.isDistinct();
    Preconditions.checkState(!isDistinct);
    stmt.selectList_ = new SelectList(items, isDistinct, stmt.selectList_.getPlanHints());
    // Update subquery's GROUP BY clause
    if (groupByExprs != null && !groupByExprs.isEmpty()) {
        if (stmt.hasGroupByClause()) {
            stmt.groupingExprs_.addAll(groupByExprs);
        } else {
            stmt.groupingExprs_ = groupByExprs;
        }
    }
}
#method_after
private static void updateInlineView(InlineViewRef inlineView, Expr expr, List<TupleId> parentQueryTids, List<Expr> lhsExprs, List<Expr> rhsExprs, boolean updateGroupBy) throws AnalysisException {
    SelectStmt stmt = (SelectStmt) inlineView.getViewStmt();
    List<TupleId> subqueryTblIds = stmt.getTableRefIds();
    ArrayList<Expr> groupByExprs = null;
    if (updateGroupBy)
        groupByExprs = Lists.newArrayList();
    List<SelectListItem> items = stmt.selectList_.getItems();
    // Collect all the SlotRefs from 'expr' and identify those that are bound by
    // subquery tuple ids.
    ArrayList<Expr> slotRefs = Lists.newArrayList();
    expr.collectAll(Predicates.instanceOf(SlotRef.class), slotRefs);
    List<Expr> exprsBoundBySubqueryTids = Lists.newArrayList();
    for (Expr slotRef : slotRefs) {
        if (slotRef.isBoundByTupleIds(subqueryTblIds)) {
            exprsBoundBySubqueryTids.add(slotRef);
        }
    }
    // no need to update the subquery's select or group by list.
    if (exprsBoundBySubqueryTids.isEmpty())
        return;
    if (updateGroupBy) {
        Preconditions.checkState(expr instanceof BinaryPredicate);
        Expr exprBoundBySubqueryTids = null;
        if (exprsBoundBySubqueryTids.size() > 1) {
            // ids, they must all be on the same side of that predicate.
            if (expr.getChild(0).isBoundByTupleIds(subqueryTblIds) && expr.getChild(1).isBoundByTupleIds(parentQueryTids)) {
                exprBoundBySubqueryTids = expr.getChild(0);
            } else if (expr.getChild(0).isBoundByTupleIds(parentQueryTids) && expr.getChild(1).isBoundByTupleIds(subqueryTblIds)) {
                exprBoundBySubqueryTids = expr.getChild(1);
            } else {
                throw new AnalysisException("All subquery columns " + "that participate in a predicate must be on the same side of " + "that predicate: " + expr.toSql());
            }
        } else {
            Preconditions.checkState(exprsBoundBySubqueryTids.size() == 1);
            exprBoundBySubqueryTids = exprsBoundBySubqueryTids.get(0);
        }
        exprsBoundBySubqueryTids.clear();
        exprsBoundBySubqueryTids.add(exprBoundBySubqueryTids);
    }
    // added to an ExprSubstitutionMap.
    for (Expr boundExpr : exprsBoundBySubqueryTids) {
        String colAlias = stmt.getColumnAliasGenerator().getNextAlias();
        items.add(new SelectListItem(boundExpr, null));
        inlineView.getExplicitColLabels().add(colAlias);
        lhsExprs.add(boundExpr);
        rhsExprs.add(new SlotRef(inlineView.getAliasAsName(), colAlias));
        if (groupByExprs != null)
            groupByExprs.add(boundExpr);
    }
    // Update the subquery's select list.
    boolean isDistinct = stmt.selectList_.isDistinct();
    stmt.selectList_ = new SelectList(items, isDistinct, stmt.selectList_.getPlanHints());
    // Update subquery's GROUP BY clause
    if (groupByExprs != null && !groupByExprs.isEmpty()) {
        if (stmt.hasGroupByClause()) {
            stmt.groupingExprs_.addAll(groupByExprs);
        } else {
            stmt.groupingExprs_ = groupByExprs;
        }
    }
}
#end_block

#method_before
@Override
public int compareTo(LiteralExpr o) {
    if (!(o instanceof StringLiteral))
        return -1;
    StringLiteral other = (StringLiteral) o;
    return value_.compareTo(other.getStringValue());
}
#method_after
@Override
public int compareTo(LiteralExpr o) {
    int ret = super.compareTo(o);
    if (ret != 0)
        return ret;
    StringLiteral other = (StringLiteral) o;
    return value_.compareTo(other.getStringValue());
}
#end_block

#method_before
private void analyzeScalarType(ScalarType scalarType, Analyzer analyzer) throws AnalysisException {
    PrimitiveType type = scalarType.getPrimitiveType();
    switch(type) {
        case CHAR:
        case VARCHAR:
            {
                String name;
                int maxLen;
                if (type == PrimitiveType.VARCHAR) {
                    name = "Varchar";
                    maxLen = ScalarType.MAX_VARCHAR_LENGTH;
                } else if (type == PrimitiveType.CHAR) {
                    name = "Char";
                    maxLen = ScalarType.MAX_CHAR_LENGTH;
                } else {
                    Preconditions.checkState(false);
                    return;
                }
                int len = scalarType.getLength();
                if (len <= 0) {
                    throw new AnalysisException(name + " size must be > 0. Size is too small: " + len + ".");
                }
                if (scalarType.getLength() > maxLen) {
                    throw new AnalysisException(name + " size must be <= " + maxLen + ". Size is too large: " + len + ".");
                }
                break;
            }
        case DECIMAL:
            {
                int precision = scalarType.decimalPrecision();
                int scale = scalarType.decimalScale();
                if (precision > ScalarType.MAX_PRECISION) {
                    throw new AnalysisException("Decimal precision must be <= " + ScalarType.MAX_PRECISION + ".");
                }
                if (precision == 0) {
                    throw new AnalysisException("Decimal precision must be greater than 0.");
                }
                if (scale > precision) {
                    throw new AnalysisException("Decimal scale (" + scale + ") must be <= " + "precision (" + precision + ").");
                }
            }
        default:
            break;
    }
}
#method_after
private void analyzeScalarType(ScalarType scalarType, Analyzer analyzer) throws AnalysisException {
    PrimitiveType type = scalarType.getPrimitiveType();
    switch(type) {
        case CHAR:
        case VARCHAR:
            {
                String name;
                int maxLen;
                if (type == PrimitiveType.VARCHAR) {
                    name = "Varchar";
                    maxLen = ScalarType.MAX_VARCHAR_LENGTH;
                } else if (type == PrimitiveType.CHAR) {
                    name = "Char";
                    maxLen = ScalarType.MAX_CHAR_LENGTH;
                } else {
                    Preconditions.checkState(false);
                    return;
                }
                int len = scalarType.getLength();
                if (len <= 0) {
                    throw new AnalysisException(name + " size must be > 0: " + len);
                }
                if (scalarType.getLength() > maxLen) {
                    throw new AnalysisException(name + " size must be <= " + maxLen + ": " + len);
                }
                break;
            }
        case DECIMAL:
            {
                int precision = scalarType.decimalPrecision();
                int scale = scalarType.decimalScale();
                if (precision > ScalarType.MAX_PRECISION) {
                    throw new AnalysisException("Decimal precision must be <= " + ScalarType.MAX_PRECISION + ": " + precision);
                }
                if (precision == 0) {
                    throw new AnalysisException("Decimal precision must be > 0: " + precision);
                }
                if (scale > precision) {
                    throw new AnalysisException("Decimal scale (" + scale + ") must be <= " + "precision (" + precision + ")");
                }
            }
        default:
            break;
    }
}
#end_block

#method_before
public void setHasVarArgs(boolean b) {
    hasVarArgs_ = b;
}
#method_after
public void setHasVarArgs(boolean b) {
    {
        Preconditions.checkState(argTypeDefs_.size() > 0);
        hasVarArgs_ = b;
    }
}
#end_block

#method_before
public List<ColumnDesc> getColumnDefs() {
    return columnDefs_;
}
#method_after
public List<ColumnDef> getColumnDefs() {
    return columnDefs_;
}
#end_block

#method_before
public List<ColumnDesc> getPartitionColumnDefs() {
    return partitionColDefs_;
}
#method_after
public List<ColumnDef> getPartitionColumnDefs() {
    return partitionColDefs_;
}
#end_block

#method_before
public TCreateTableParams toThrift() {
    TCreateTableParams params = new TCreateTableParams();
    params.setTable_name(new TTableName(getDb(), getTbl()));
    for (ColumnDesc col : getColumnDefs()) {
        params.addToColumns(col.toThrift());
    }
    for (ColumnDesc col : getPartitionColumnDefs()) {
        params.addToPartition_columns(col.toThrift());
    }
    params.setOwner(getOwner());
    params.setIs_external(isExternal());
    params.setComment(comment_);
    params.setLocation(location_ == null ? null : location_.toString());
    if (cachingOp_ != null)
        params.setCache_op(cachingOp_.toThrift());
    params.setRow_format(rowFormat_.toThrift());
    params.setFile_format(fileFormat_);
    params.setIf_not_exists(getIfNotExists());
    if (tblProperties_ != null)
        params.setTable_properties(tblProperties_);
    if (serdeProperties_ != null)
        params.setSerde_properties(serdeProperties_);
    return params;
}
#method_after
public TCreateTableParams toThrift() {
    TCreateTableParams params = new TCreateTableParams();
    params.setTable_name(new TTableName(getDb(), getTbl()));
    for (ColumnDef col : getColumnDefs()) {
        params.addToColumns(col.toThrift());
    }
    for (ColumnDef col : getPartitionColumnDefs()) {
        params.addToPartition_columns(col.toThrift());
    }
    params.setOwner(getOwner());
    params.setIs_external(isExternal());
    params.setComment(comment_);
    params.setLocation(location_ == null ? null : location_.toString());
    if (cachingOp_ != null)
        params.setCache_op(cachingOp_.toThrift());
    params.setRow_format(rowFormat_.toThrift());
    params.setFile_format(fileFormat_);
    params.setIf_not_exists(getIfNotExists());
    if (tblProperties_ != null)
        params.setTable_properties(tblProperties_);
    if (serdeProperties_ != null)
        params.setSerde_properties(serdeProperties_);
    return params;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(tableName_ != null && !tableName_.isEmpty());
    tableName_.analyze();
    dbName_ = analyzer.getTargetDbName(tableName_);
    owner_ = analyzer.getUser().getName();
    if (analyzer.dbContainsTable(dbName_, tableName_.getTbl(), Privilege.CREATE) && !ifNotExists_) {
        throw new AnalysisException(Analyzer.TBL_ALREADY_EXISTS_ERROR_MSG + String.format("%s.%s", dbName_, getTbl()));
    }
    analyzer.addAccessEvent(new TAccessEvent(dbName_ + "." + tableName_.getTbl(), TCatalogObjectType.TABLE, Privilege.CREATE.toString()));
    // the Avro schema.
    if (columnDefs_.isEmpty() && fileFormat_ != THdfsFileFormat.AVRO) {
        throw new AnalysisException("Table requires at least 1 column");
    }
    if (location_ != null) {
        location_.analyze(analyzer, Privilege.ALL, FsAction.READ_WRITE);
    }
    analyzeRowFormatValue(rowFormat_.getFieldDelimiter());
    analyzeRowFormatValue(rowFormat_.getLineDelimiter());
    analyzeRowFormatValue(rowFormat_.getEscapeChar());
    // Check that all the column names are valid and unique.
    analyzeColumnDefs(analyzer);
    if (fileFormat_ == THdfsFileFormat.AVRO) {
        List<ColumnDesc> newColumnDefs = analyzeAvroSchema(analyzer);
        if (newColumnDefs != columnDefs_) {
            // Replace the old column defs with the new ones and analyze them.
            columnDefs_.clear();
            columnDefs_.addAll(newColumnDefs);
            analyzeColumnDefs(analyzer);
        }
    }
    if (cachingOp_ != null)
        cachingOp_.analyze(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(tableName_ != null && !tableName_.isEmpty());
    tableName_.analyze();
    dbName_ = analyzer.getTargetDbName(tableName_);
    owner_ = analyzer.getUser().getName();
    if (analyzer.dbContainsTable(dbName_, tableName_.getTbl(), Privilege.CREATE) && !ifNotExists_) {
        throw new AnalysisException(Analyzer.TBL_ALREADY_EXISTS_ERROR_MSG + String.format("%s.%s", dbName_, getTbl()));
    }
    analyzer.addAccessEvent(new TAccessEvent(dbName_ + "." + tableName_.getTbl(), TCatalogObjectType.TABLE, Privilege.CREATE.toString()));
    // the Avro schema.
    if (columnDefs_.isEmpty() && fileFormat_ != THdfsFileFormat.AVRO) {
        throw new AnalysisException("Table requires at least 1 column");
    }
    if (location_ != null) {
        location_.analyze(analyzer, Privilege.ALL, FsAction.READ_WRITE);
    }
    analyzeRowFormatValue(rowFormat_.getFieldDelimiter());
    analyzeRowFormatValue(rowFormat_.getLineDelimiter());
    analyzeRowFormatValue(rowFormat_.getEscapeChar());
    // Check that all the column names are valid and unique.
    analyzeColumnDefs(analyzer);
    if (fileFormat_ == THdfsFileFormat.AVRO) {
        List<ColumnDef> newColumnDefs = analyzeAvroSchema(analyzer);
        if (newColumnDefs != columnDefs_) {
            // Replace the old column defs with the new ones and analyze them.
            columnDefs_.clear();
            columnDefs_.addAll(newColumnDefs);
            analyzeColumnDefs(analyzer);
        }
    }
    if (cachingOp_ != null)
        cachingOp_.analyze(analyzer);
}
#end_block

#method_before
private void analyzeColumnDefs(Analyzer analyzer) throws AnalysisException {
    Set<String> colNames = Sets.newHashSet();
    for (ColumnDesc colDef : columnDefs_) {
        colDef.analyze();
        if (!colNames.add(colDef.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDef.getColName());
        }
    }
    for (ColumnDesc colDef : partitionColDefs_) {
        colDef.analyze();
        if (!colDef.getType().supportsTablePartitioning()) {
            throw new AnalysisException(String.format("Type '%s' is not supported as partition-column type " + "in column: %s", colDef.getType().toSql(), colDef.getColName()));
        }
        if (!colNames.add(colDef.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDef.getColName());
        }
    }
}
#method_after
private void analyzeColumnDefs(Analyzer analyzer) throws AnalysisException {
    Set<String> colNames = Sets.newHashSet();
    for (ColumnDef colDef : columnDefs_) {
        colDef.analyze();
        if (!colNames.add(colDef.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDef.getColName());
        }
    }
    for (ColumnDef colDef : partitionColDefs_) {
        colDef.analyze();
        if (!colDef.getType().supportsTablePartitioning()) {
            throw new AnalysisException(String.format("Type '%s' is not supported as partition-column type " + "in column: %s", colDef.getType().toSql(), colDef.getColName()));
        }
        if (!colNames.add(colDef.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDef.getColName());
        }
    }
}
#end_block

#method_before
private List<ColumnDesc> analyzeAvroSchema(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(fileFormat_ == THdfsFileFormat.AVRO);
    // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
    // taking precedence.
    List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
    String fullTblName = dbName_ + "." + tableName_.getTbl();
    schemaSearchLocations.add(serdeProperties_);
    schemaSearchLocations.add(tblProperties_);
    String avroSchema = null;
    try {
        avroSchema = HdfsTable.getAvroSchema(schemaSearchLocations, dbName_ + "." + tableName_.getTbl());
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    if (Strings.isNullOrEmpty(avroSchema)) {
        throw new AnalysisException("Avro schema is null or empty: " + fullTblName);
    }
    // List of columns parsed from the Avro schema.
    List<Column> avroColumns = null;
    try {
        avroColumns = AvroSchemaParser.parse(avroSchema);
    } catch (Exception e) {
        throw new AnalysisException(String.format("Error parsing Avro schema for table '%s': %s", fullTblName, e.getMessage()));
    }
    Preconditions.checkNotNull(avroColumns);
    // Analyze the Avro schema to detect inconsistencies with the columnDefs_.
    // In case of inconsistencies, the column defs are ignored in favor of the Avro
    // schema for simplicity and, in particular, to enable COMPUTE STATS (IMPALA-1104).
    // set if inconsistency detected
    String warnStr = null;
    if (avroColumns.size() != columnDefs_.size() && !columnDefs_.isEmpty()) {
        warnStr = String.format("Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has %s column(s) but %s column definition(s) were given.", avroColumns.size(), columnDefs_.size());
    } else {
        // Determine whether the column names and the types match.
        for (int i = 0; i < columnDefs_.size(); ++i) {
            ColumnDesc colDesc = columnDefs_.get(i);
            Column avroCol = avroColumns.get(i);
            String warnDetail = null;
            if (!colDesc.getColName().equalsIgnoreCase(avroCol.getName())) {
                warnDetail = "name";
            }
            if (colDesc.getType().isStringType() && avroCol.getType().isStringType()) {
            // This is OK -- avro types for CHAR, VARCHAR, and STRING are "string"
            } else if (!colDesc.getType().equals(avroCol.getType())) {
                warnDetail = "type";
            }
            if (warnDetail != null) {
                warnStr = String.format("Ignoring column definitions in favor of Avro schema due to a " + "mismatched column %s at position %s.\n" + "Column definition: %s\n" + "Avro schema column: %s", warnDetail, i + 1, colDesc.getColName() + " " + colDesc.getType().toSql(), avroCol.getName() + " " + avroCol.getType().toSql());
                break;
            }
        }
    }
    if (warnStr != null || columnDefs_.isEmpty()) {
        analyzer.addWarning(warnStr);
        // Create new columnDefs_ based on the Avro schema and return them.
        List<ColumnDesc> avroSchemaColDefs = Lists.newArrayListWithCapacity(avroColumns.size());
        for (Column avroCol : avroColumns) {
            ColumnDesc colDef = new ColumnDesc(avroCol.getName(), null, avroCol.getComment());
            colDef.setType(avroCol.getType());
            avroSchemaColDefs.add(colDef);
        }
        return avroSchemaColDefs;
    }
    // The existing col defs are consistent with the Avro schema.
    return columnDefs_;
}
#method_after
private List<ColumnDef> analyzeAvroSchema(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(fileFormat_ == THdfsFileFormat.AVRO);
    // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
    // taking precedence.
    List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
    String fullTblName = dbName_ + "." + tableName_.getTbl();
    schemaSearchLocations.add(serdeProperties_);
    schemaSearchLocations.add(tblProperties_);
    String avroSchema = null;
    try {
        avroSchema = HdfsTable.getAvroSchema(schemaSearchLocations, dbName_ + "." + tableName_.getTbl());
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    if (Strings.isNullOrEmpty(avroSchema)) {
        throw new AnalysisException("Avro schema is null or empty: " + fullTblName);
    }
    // List of columns parsed from the Avro schema.
    List<Column> avroColumns = null;
    try {
        avroColumns = AvroSchemaParser.parse(avroSchema);
    } catch (Exception e) {
        throw new AnalysisException(String.format("Error parsing Avro schema for table '%s': %s", fullTblName, e.getMessage()));
    }
    Preconditions.checkNotNull(avroColumns);
    // Analyze the Avro schema to detect inconsistencies with the columnDefs_.
    // In case of inconsistencies, the column defs are ignored in favor of the Avro
    // schema for simplicity and, in particular, to enable COMPUTE STATS (IMPALA-1104).
    // set if inconsistency detected
    String warnStr = null;
    if (avroColumns.size() != columnDefs_.size() && !columnDefs_.isEmpty()) {
        warnStr = String.format("Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has %s column(s) but %s column definition(s) were given.", avroColumns.size(), columnDefs_.size());
    } else {
        // Determine whether the column names and the types match.
        for (int i = 0; i < columnDefs_.size(); ++i) {
            ColumnDef colDesc = columnDefs_.get(i);
            Column avroCol = avroColumns.get(i);
            String warnDetail = null;
            if (!colDesc.getColName().equalsIgnoreCase(avroCol.getName())) {
                warnDetail = "name";
            }
            if (colDesc.getType().isStringType() && avroCol.getType().isStringType()) {
            // This is OK -- avro types for CHAR, VARCHAR, and STRING are "string"
            } else if (!colDesc.getType().equals(avroCol.getType())) {
                warnDetail = "type";
            }
            if (warnDetail != null) {
                warnStr = String.format("Ignoring column definitions in favor of Avro schema due to a " + "mismatched column %s at position %s.\n" + "Column definition: %s\n" + "Avro schema column: %s", warnDetail, i + 1, colDesc.getColName() + " " + colDesc.getType().toSql(), avroCol.getName() + " " + avroCol.getType().toSql());
                break;
            }
        }
    }
    if (warnStr != null || columnDefs_.isEmpty()) {
        analyzer.addWarning(warnStr);
        // Create new columnDefs_ based on the Avro schema and return them.
        List<ColumnDef> avroSchemaColDefs = Lists.newArrayListWithCapacity(avroColumns.size());
        for (Column avroCol : avroColumns) {
            ColumnDef colDef = new ColumnDef(avroCol.getName(), null, avroCol.getComment());
            colDef.setType(avroCol.getType());
            avroSchemaColDefs.add(colDef);
        }
        return avroSchemaColDefs;
    }
    // The existing col defs are consistent with the Avro schema.
    return columnDefs_;
}
#end_block

#method_before
public static ScalarFunction createBuiltin(String name, ArrayList<Type> argTypes, boolean hasVarArgs, Type retType, String symbol, String prepareFnSymbol, String closeFnSymbol, boolean isOperator) {
    Preconditions.checkNotNull(symbol);
    ScalarFunction fn = new ScalarFunction(new FunctionName(Catalog.BUILTINS_DB, name), argTypes, retType, hasVarArgs);
    fn.setBinaryType(TFunctionBinaryType.BUILTIN);
    fn.setUserVisible(!isOperator);
    try {
        fn.symbolName_ = fn.lookupSymbol(symbol, TSymbolType.UDF_EVALUATE, null, fn.hasVarArgs(), fn.getArgs());
    } catch (AnalysisException e) {
        // This should never happen
        Preconditions.checkState(false, "Builtin symbol '" + symbol + "'" + argTypes + " not found!" + e.getStackTrace());
        throw new RuntimeException("Builtin symbol not found!", e);
    }
    fn.prepareFnSymbol_ = prepareFnSymbol;
    fn.closeFnSymbol_ = closeFnSymbol;
    return fn;
}
#method_after
public static ScalarFunction createBuiltin(String name, ArrayList<Type> argTypes, boolean hasVarArgs, Type retType, String symbol, String prepareFnSymbol, String closeFnSymbol, boolean isOperator) {
    Preconditions.checkNotNull(symbol);
    ScalarFunction fn = new ScalarFunction(new FunctionName(Catalog.BUILTINS_DB, name), argTypes, retType, hasVarArgs);
    fn.setBinaryType(TFunctionBinaryType.BUILTIN);
    fn.setUserVisible(!isOperator);
    try {
        fn.symbolName_ = fn.lookupSymbol(symbol, TSymbolType.UDF_EVALUATE, null, fn.hasVarArgs(), fn.getArgs());
    } catch (AnalysisException e) {
        // This should never happen
        throw new RuntimeException("Builtin symbol '" + symbol + "'" + argTypes + " not found!", e);
    }
    if (prepareFnSymbol != null) {
        try {
            fn.prepareFnSymbol_ = fn.lookupSymbol(prepareFnSymbol, TSymbolType.UDF_PREPARE);
        } catch (AnalysisException e) {
            // This should never happen
            throw new RuntimeException("Builtin symbol '" + prepareFnSymbol + "' not found!", e);
        }
    }
    if (closeFnSymbol != null) {
        try {
            fn.closeFnSymbol_ = fn.lookupSymbol(closeFnSymbol, TSymbolType.UDF_CLOSE);
        } catch (AnalysisException e) {
            // This should never happen
            throw new RuntimeException("Builtin symbol '" + closeFnSymbol + "' not found!", e);
        }
    }
    return fn;
}
#end_block

#method_before
@Override
protected Expr uncheckedCastTo(Type targetType) throws AnalysisException {
    Preconditions.checkState(targetType.isNumericType());
    type_ = targetType;
    return this;
}
#method_after
@Override
protected Expr uncheckedCastTo(Type targetType) throws AnalysisException {
    Preconditions.checkState(targetType.isNumericType());
    // expected byte size sent to the BE in toThrift().
    if (targetType.isDecimal()) {
        ScalarType decimalType = (ScalarType) targetType;
        // analyze() ensures that value_ never exceeds the maximum scale and precision.
        Preconditions.checkState(isAnalyzed_);
        // Sanity check that our implicit casting does not allow a reduced precision or
        // truncating values from the right of the decimal point.
        Preconditions.checkState(value_.precision() <= decimalType.decimalPrecision());
        Preconditions.checkState(value_.scale() <= decimalType.decimalScale());
        int valLeftDigits = value_.precision() - value_.scale();
        int typeLeftDigits = decimalType.decimalPrecision() - decimalType.decimalScale();
        if (typeLeftDigits < valLeftDigits)
            return new CastExpr(targetType, this);
    }
    type_ = targetType;
    return this;
}
#end_block

#method_before
@Override
public int compareTo(LiteralExpr o) {
    if (!(o instanceof NumericLiteral))
        return -1;
    NumericLiteral other = (NumericLiteral) o;
    return value_.compareTo(other.value_);
}
#method_after
@Override
public int compareTo(LiteralExpr o) {
    int ret = super.compareTo(o);
    if (ret != 0)
        return ret;
    NumericLiteral other = (NumericLiteral) o;
    return value_.compareTo(other.value_);
}
#end_block

#method_before
@Test
public void TestAlterView() {
    // View-definition references a table.
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypesagg");
    // View-definition references a view.
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypes_view");
    // View-definition resulting in Hive-style auto-generated column names.
    AnalyzesOk("alter view functional.alltypes_view as " + "select trim('abc'), 17 * 7");
    // Cannot ALTER VIEW a table.
    AnalysisError("alter view functional.alltypes as " + "select * from functional.alltypesagg", "ALTER VIEW not allowed on a table: functional.alltypes");
    AnalysisError("alter view functional_hbase.alltypesagg as " + "select * from functional.alltypesagg", "ALTER VIEW not allowed on a table: functional_hbase.alltypesagg");
    // Target database does not exist.
    AnalysisError("alter view baddb.alltypes_view as " + "select * from functional.alltypesagg", "Database does not exist: baddb");
    // Target view does not exist.
    AnalysisError("alter view functional.badview as " + "select * from functional.alltypesagg", "Table does not exist: functional.badview");
    // View-definition statement fails to analyze. Database does not exist.
    AnalysisError("alter view functional.alltypes_view as " + "select * from baddb.alltypesagg", "Database does not exist: baddb");
    // View-definition statement fails to analyze. Table does not exist.
    AnalysisError("alter view functional.alltypes_view as " + "select * from functional.badtable", "Table does not exist: functional.badtable");
    // Duplicate column name.
    AnalysisError("alter view functional.alltypes_view as " + "select * from functional.alltypessmall a inner join " + "functional.alltypessmall b on a.id = b.id", "Duplicate column name: id");
    // Invalid column name.
    AnalysisError("alter view functional.alltypes_view as select 'abc' as `???`", "Invalid column/field name: ???");
}
#method_after
@Test
public void TestAlterView() {
    // View-definition references a table.
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypesagg");
    // View-definition references a view.
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypes_view");
    // View-definition resulting in Hive-style auto-generated column names.
    AnalyzesOk("alter view functional.alltypes_view as " + "select trim('abc'), 17 * 7");
    // Cannot ALTER VIEW a table.
    AnalysisError("alter view functional.alltypes as " + "select * from functional.alltypesagg", "ALTER VIEW not allowed on a table: functional.alltypes");
    AnalysisError("alter view functional_hbase.alltypesagg as " + "select * from functional.alltypesagg", "ALTER VIEW not allowed on a table: functional_hbase.alltypesagg");
    // Target database does not exist.
    AnalysisError("alter view baddb.alltypes_view as " + "select * from functional.alltypesagg", "Database does not exist: baddb");
    // Target view does not exist.
    AnalysisError("alter view functional.badview as " + "select * from functional.alltypesagg", "Table does not exist: functional.badview");
    // View-definition statement fails to analyze. Database does not exist.
    AnalysisError("alter view functional.alltypes_view as " + "select * from baddb.alltypesagg", "Database does not exist: baddb");
    // View-definition statement fails to analyze. Table does not exist.
    AnalysisError("alter view functional.alltypes_view as " + "select * from functional.badtable", "Table does not exist: functional.badtable");
    // Duplicate column name.
    AnalysisError("alter view functional.alltypes_view as " + "select * from functional.alltypessmall a inner join " + "functional.alltypessmall b on a.id = b.id", "Duplicate column name: id");
    // Invalid column name.
    AnalysisError("alter view functional.alltypes_view as select 'abc' as `???`", "Invalid column/field name: ???");
    // Change the view definition to contain a subquery (IMPALA-1797)
    AnalyzesOk("alter view functional.alltypes_view as " + "select * from functional.alltypestiny where id in " + "(select id from functional.alltypessmall where int_col = 1)");
}
#end_block

#method_before
@Test
public void TestComputeStats() throws AnalysisException {
    // Analyze the stmt itself as well as the generated child queries.
    checkComputeStatsStmt("compute stats functional.alltypes");
    checkComputeStatsStmt("compute stats functional_hbase.alltypes");
    // Test that complex-typed columns are ignored.
    checkComputeStatsStmt("compute stats functional.allcomplextypes");
    // Cannot compute stats on a database.
    AnalysisError("compute stats tbl_does_not_exist", "Table does not exist: default.tbl_does_not_exist");
    // Cannot compute stats on a view.
    AnalysisError("compute stats functional.alltypes_view", "COMPUTE STATS not supported for view functional.alltypes_view");
    AnalyzesOk("compute stats functional_avro_snap.alltypes");
    // Test mismatched column definitions and Avro schema (HIVE-6308, IMPALA-867).
    // See testdata/avro_schema_resolution/create_table.sql for the CREATE TABLE stmts.
    // Mismatched column type is ok because the conflict is resolved in favor of
    // the type in the column definition list in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_type_mismatch");
    // Missing column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_missing_coldef");
    // Extra column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_extra_coldef");
    // Mismatched column name (tables were created by Hive).
    AnalysisError("compute stats functional_avro_snap.schema_resolution_test", "Cannot COMPUTE STATS on Avro table 'schema_resolution_test' because its " + "column definitions do not match those in the Avro schema.\nDefinition of " + "column 'col1' of type 'string' does not match the Avro-schema column " + "'boolean1' of type 'BOOLEAN' at position '0'.\nPlease re-create the table " + "with column definitions, e.g., using the result of 'SHOW CREATE TABLE'");
    // No column definitions were given at all. This case is broken in Hive (HIVE-6308),
    // but works when such a table is created through Impala.
    AnalysisError("compute stats functional_avro_snap.alltypes_no_coldef", "Cannot COMPUTE STATS on Avro table 'alltypes_no_coldef' because its column " + "definitions do not match those in the Avro schema.\nMissing column " + "definition corresponding to Avro-schema column 'id' of type 'INT' at " + "position '0'.\nPlease re-create the table with column definitions, e.g., " + "using the result of 'SHOW CREATE TABLE'");
}
#method_after
@Test
public void TestComputeStats() throws AnalysisException {
    // Analyze the stmt itself as well as the generated child queries.
    checkComputeStatsStmt("compute stats functional.alltypes");
    checkComputeStatsStmt("compute stats functional_hbase.alltypes");
    // Test that complex-typed columns are ignored.
    checkComputeStatsStmt("compute stats functional.allcomplextypes");
    // Cannot compute stats on a database.
    AnalysisError("compute stats tbl_does_not_exist", "Table does not exist: default.tbl_does_not_exist");
    // Cannot compute stats on a view.
    AnalysisError("compute stats functional.alltypes_view", "COMPUTE STATS not supported for view functional.alltypes_view");
    AnalyzesOk("compute stats functional_avro_snap.alltypes");
    // Test mismatched column definitions and Avro schema (HIVE-6308, IMPALA-867).
    // See testdata/avro_schema_resolution/create_table.sql for the CREATE TABLE stmts.
    // Mismatched column type is ok because the conflict is resolved in favor of
    // the type in the column definition list in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_type_mismatch");
    // Missing column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_missing_coldef");
    // Extra column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_extra_coldef");
    // No column definitions are ok.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_no_coldef");
    // Mismatched column name (table was created by Hive).
    AnalysisError("compute stats functional_avro_snap.schema_resolution_test", "Cannot COMPUTE STATS on Avro table 'schema_resolution_test' because its " + "column definitions do not match those in the Avro schema.\nDefinition of " + "column 'col1' of type 'string' does not match the Avro-schema column " + "'boolean1' of type 'BOOLEAN' at position '0'.\nPlease re-create the table " + "with column definitions, e.g., using the result of 'SHOW CREATE TABLE'");
}
#end_block

#method_before
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38.");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0. Size is too small: 0.");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355. Size is too large: 65356.");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0. Size is too small: 0.");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255. Size is too large: 256.");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    for (String format : fileFormats) {
        AnalyzesOk(String.format("create table new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", format));
        // No column definitions.
        AnalysisError(String.format("create table new_table " + "partitioned by (d decimal) comment 'c' stored as %s", format), "Table requires at least 1 column");
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: I");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d date)", "Type 'DATE' is not supported as partition-column type in column: d");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d datetime)", "Type 'DATETIME' is not supported as partition-column type in column: d");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#method_after
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    for (String format : fileFormats) {
        AnalyzesOk(String.format("create table new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", format));
        // No column definitions.
        AnalysisError(String.format("create table new_table " + "partitioned by (d decimal) comment 'c' stored as %s", format), "Table requires at least 1 column");
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: I");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d date)", "Type 'DATE' is not supported as partition-column type in column: d");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d datetime)", "Type 'DATETIME' is not supported as partition-column type in column: d");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#end_block

#method_before
@Test
public void TestUdf() throws AnalysisException {
    final String symbol = "'_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'";
    final String udfSuffix = " LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=" + symbol;
    final String udfSuffixIr = " LOCATION '/test-warehouse/test-udfs.ll' " + "SYMBOL=" + symbol;
    final String hdfsPath = "hdfs://localhost:20500/test-warehouse/libTestUdfs.so";
    AnalyzesOk("create function foo() RETURNS int" + udfSuffix);
    AnalyzesOk("create function foo(int, int, string) RETURNS int" + udfSuffix);
    // Try some fully qualified function names
    AnalyzesOk("create function functional.B() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.B1() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.`B1C`() RETURNS int" + udfSuffix);
    // Name with underscore
    AnalyzesOk("create function A_B() RETURNS int" + udfSuffix);
    // Locations for all the udfs types.
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.so' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'", "Could not load binary: /test-warehouse/libTestUdfs.ll");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo(int) RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' SYMBOL='Identity'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.SO' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/hive-exec.jar' SYMBOL='a'");
    // Test hive UDFs for unsupported types
    AnalysisError("create function foo() RETURNS timestamp LOCATION '/test-warehouse/hive-exec.jar' SYMBOL='a'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo(timestamp) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo() RETURNS decimal LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(Decimal) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(char(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(varchar(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5) LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5) LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5)" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5)" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo(CHAR(5)) RETURNS int" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(VARCHAR(5)) RETURNS int" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalyzesOk("create function foo() RETURNS decimal" + udfSuffix);
    AnalyzesOk("create function foo() RETURNS decimal(38,10)" + udfSuffix);
    AnalyzesOk("create function foo(Decimal, decimal(10, 2)) RETURNS int" + udfSuffix);
    AnalysisError("create function foo() RETURNS decimal(100)" + udfSuffix, "Decimal precision must be <= 38.");
    AnalysisError("create function foo(Decimal(2, 3)) RETURNS int" + udfSuffix, "Decimal scale (3) must be <= precision (2).");
    // Varargs
    AnalyzesOk("create function foo(INT...) RETURNS int" + udfSuffix);
    // Prepare/Close functions
    AnalyzesOk("create function foo() returns int" + udfSuffix + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='_Z19ValidateOpenPreparePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'" + " close_fn='_Z17ValidateOpenClosePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " close_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn='FakePrepare'", "Could not find function FakePrepare(impala_udf::FunctionContext*, " + "impala_udf::FunctionContext::FunctionStateScope) in: ");
    // Try to create a function with the same name as a builtin
    AnalysisError("create function sin(double) RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    AnalysisError("create function sin() RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    // Try to create with a bad location
    AnalysisError("create function foo() RETURNS int LOCATION 'bad-location' SYMBOL='c'", "URI path must be absolute: bad-location");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'blah://localhost:50200/bad-location' SYMBOL='c'", "No FileSystem for scheme: blah");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'file:///foo.jar' SYMBOL='c'", "Could not load binary: file:///foo.jar");
    // Try creating udfs with unknown extensions
    AnalysisError("create function foo() RETURNS int LOCATION '/binary' SYMBOL='a'", "Unknown binary type: '/binary'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.a' SYMBOL='a'", "Unknown binary type: '/binary.a'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so.' SYMBOL='a'", "Unknown binary type: '/binary.so.'. Binary must end in .jar, .so or .ll");
    // Try with missing symbol
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so'", "Argument 'SYMBOL' must be set.");
    // Try with symbols missing in binary and symbols
    AnalysisError("create function foo() RETURNS int LOCATION '/blah.so' " + "SYMBOL='ab'", "Could not load binary: /blah.so");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.JAR' SYMBOL='a'", "Could not load binary: /binary.JAR");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='b'", "Could not find function b() in: " + hdfsPath);
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=''", "Could not find symbol ''");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='_ZAB'", "Could not find symbol '_ZAB' in: " + hdfsPath);
    // Infer the fully mangled symbol from the signature
    AnalyzesOk("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // We can't get the return type so any of those will match
    AnalyzesOk("create function foo() RETURNS double " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // The part the user specifies is case sensitive
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='noArgs'", "Could not find function noArgs() in: " + hdfsPath);
    // Types no longer match
    AnalysisError("create function foo(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'", "Could not find function NoArgs(INT) in: " + hdfsPath);
    // Check we can match identity for all types
    AnalyzesOk("create function identity(boolean) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(tinyint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(smallint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(bigint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(float) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(double) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(string) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function all_types_fn(string, boolean, tinyint, " + "smallint, int, bigint, float, double, decimal) returns int " + "location '/test-warehouse/libTestUdfs.so' symbol='AllTypes'");
    // Try creating functions with illegal function names.
    AnalysisError("create function 123A() RETURNS int" + udfSuffix, "Function cannot start with a digit: 123a");
    AnalysisError("create function A.`1A`() RETURNS int" + udfSuffix, "Function cannot start with a digit: 1a");
    AnalysisError("create function A.`ABC-D`() RETURNS int" + udfSuffix, "Function names must be all alphanumeric or underscore. Invalid name: abc-d");
    AnalysisError("create function baddb.f() RETURNS int" + udfSuffix, "Database does not exist: baddb");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try dropping functions.
    AnalyzesOk("drop function if exists foo()");
    AnalysisError("drop function foo()", "Function does not exist: foo()");
    AnalyzesOk("drop function if exists a.foo()");
    AnalysisError("drop function a.foo()", "Database does not exist: a");
    AnalyzesOk("drop function if exists foo()");
    AnalyzesOk("drop function if exists foo(int...)");
    AnalyzesOk("drop function if exists foo(double, int...)");
    // Add functions default.TestFn(), default.TestFn(double), default.TestFn(String...),
    addTestFunction("TestFn", new ArrayList<ScalarType>(), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.DOUBLE), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.STRING), true);
    AnalysisError("create function TestFn() RETURNS INT " + udfSuffix, "Function already exists: testfn()");
    AnalysisError("create function TestFn(double) RETURNS INT " + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Fn(Double) and Fn(Double...) should be a conflict.
    AnalysisError("create function TestFn(double...) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    AnalysisError("create function TestFn(double) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Add default.TestFn(int, int)
    addTestFunction("TestFn", Lists.newArrayList(Type.INT, Type.INT), false);
    AnalyzesOk("drop function TestFn(int, int)");
    AnalysisError("drop function TestFn(int, int, int)", "Function does not exist: testfn(INT, INT, INT)");
    // Fn(String...) was already added.
    AnalysisError("create function TestFn(String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String...) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String, String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalyzesOk("create function TestFn(String, String, Int) RETURNS INT" + udfSuffix);
    // Check function overloading.
    AnalyzesOk("create function TestFn(int) RETURNS INT " + udfSuffix);
    // Create a function with the same signature in a different db
    AnalyzesOk("create function functional.TestFn() RETURNS INT " + udfSuffix);
    AnalyzesOk("drop function TestFn()");
    AnalyzesOk("drop function TestFn(double)");
    AnalyzesOk("drop function TestFn(string...)");
    AnalysisError("drop function TestFn(double...)", "Function does not exist: testfn(DOUBLE...)");
    AnalysisError("drop function TestFn(int)", "Function does not exist: testfn(INT)");
    AnalysisError("drop function functional.TestFn()", "Function does not exist: testfn()");
    AnalysisError("create function f() returns int " + udfSuffix + "init_fn='a'", "Optional argument 'INIT_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "serialize_fn='a'", "Optional argument 'SERIALIZE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "merge_fn='a'", "Optional argument 'MERGE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "finalize_fn='a'", "Optional argument 'FINALIZE_FN' should not be set");
}
#method_after
@Test
public void TestUdf() throws AnalysisException {
    final String symbol = "'_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'";
    final String udfSuffix = " LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=" + symbol;
    final String udfSuffixIr = " LOCATION '/test-warehouse/test-udfs.ll' " + "SYMBOL=" + symbol;
    final String hdfsPath = "hdfs://localhost:20500/test-warehouse/libTestUdfs.so";
    AnalyzesOk("create function foo() RETURNS int" + udfSuffix);
    AnalyzesOk("create function foo(int, int, string) RETURNS int" + udfSuffix);
    // Try some fully qualified function names
    AnalyzesOk("create function functional.B() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.B1() RETURNS int" + udfSuffix);
    AnalyzesOk("create function functional.`B1C`() RETURNS int" + udfSuffix);
    // Name with underscore
    AnalyzesOk("create function A_B() RETURNS int" + udfSuffix);
    // Locations for all the udfs types.
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.so' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'", "Could not load binary: /test-warehouse/libTestUdfs.ll");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo(int) RETURNS int LOCATION " + "'/test-warehouse/test-udfs.ll' SYMBOL='Identity'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/libTestUdfs.SO' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'");
    AnalyzesOk("create function foo() RETURNS int LOCATION " + "'/test-warehouse/hive-exec.jar' SYMBOL='a'");
    // Test hive UDFs for unsupported types
    AnalysisError("create function foo() RETURNS timestamp LOCATION '/test-warehouse/hive-exec.jar' SYMBOL='a'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo(timestamp) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use TIMESTAMP are not yet supported.");
    AnalysisError("create function foo() RETURNS decimal LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(Decimal) RETURNS int LOCATION '/a.jar'", "Hive UDFs that use DECIMAL are not yet supported.");
    AnalysisError("create function foo(char(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(varchar(5)) RETURNS int LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5) LOCATION '/a.jar'", "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5) LOCATION '/a.jar'", "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS CHAR(5)" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo() RETURNS VARCHAR(5)" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalysisError("create function foo(CHAR(5)) RETURNS int" + udfSuffix, "UDFs that use CHAR are not yet supported.");
    AnalysisError("create function foo(VARCHAR(5)) RETURNS int" + udfSuffix, "UDFs that use VARCHAR are not yet supported.");
    AnalyzesOk("create function foo() RETURNS decimal" + udfSuffix);
    AnalyzesOk("create function foo() RETURNS decimal(38,10)" + udfSuffix);
    AnalyzesOk("create function foo(Decimal, decimal(10, 2)) RETURNS int" + udfSuffix);
    AnalysisError("create function foo() RETURNS decimal(100)" + udfSuffix, "Decimal precision must be <= 38: 100");
    AnalysisError("create function foo(Decimal(2, 3)) RETURNS int" + udfSuffix, "Decimal scale (3) must be <= precision (2)");
    // Varargs
    AnalyzesOk("create function foo(INT...) RETURNS int" + udfSuffix);
    // Prepare/Close functions
    AnalyzesOk("create function foo() returns int" + udfSuffix + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='ValidateOpenPrepare'" + " close_fn='ValidateOpenClose'");
    AnalyzesOk("create function foo() returns int" + udfSuffixIr + " prepare_fn='_Z19ValidateOpenPreparePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'" + " close_fn='_Z17ValidateOpenClosePN10impala_udf15FunctionContextENS0_18FunctionStateScopeE'");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " close_fn=''", "Could not find symbol ''");
    AnalysisError("create function foo() returns int" + udfSuffix + " prepare_fn='FakePrepare'", "Could not find function FakePrepare(impala_udf::FunctionContext*, " + "impala_udf::FunctionContext::FunctionStateScope) in: ");
    // Try to create a function with the same name as a builtin
    AnalysisError("create function sin(double) RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    AnalysisError("create function sin() RETURNS double" + udfSuffix, "Function cannot have the same name as a builtin: sin");
    // Try to create with a bad location
    AnalysisError("create function foo() RETURNS int LOCATION 'bad-location' SYMBOL='c'", "URI path must be absolute: bad-location");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'blah://localhost:50200/bad-location' SYMBOL='c'", "No FileSystem for scheme: blah");
    AnalysisError("create function foo() RETURNS int LOCATION " + "'file:///foo.jar' SYMBOL='c'", "Could not load binary: file:///foo.jar");
    // Try creating udfs with unknown extensions
    AnalysisError("create function foo() RETURNS int LOCATION '/binary' SYMBOL='a'", "Unknown binary type: '/binary'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.a' SYMBOL='a'", "Unknown binary type: '/binary.a'. Binary must end in .jar, .so or .ll");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so.' SYMBOL='a'", "Unknown binary type: '/binary.so.'. Binary must end in .jar, .so or .ll");
    // Try with missing symbol
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.so'", "Argument 'SYMBOL' must be set.");
    // Try with symbols missing in binary and symbols
    AnalysisError("create function foo() RETURNS int LOCATION '/blah.so' " + "SYMBOL='ab'", "Could not load binary: /blah.so");
    AnalysisError("create function foo() RETURNS int LOCATION '/binary.JAR' SYMBOL='a'", "Could not load binary: /binary.JAR");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='b'", "Could not find function b() in: " + hdfsPath);
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL=''", "Could not find symbol ''");
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='_ZAB'", "Could not find symbol '_ZAB' in: " + hdfsPath);
    // Infer the fully mangled symbol from the signature
    AnalyzesOk("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // We can't get the return type so any of those will match
    AnalyzesOk("create function foo() RETURNS double " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'");
    // The part the user specifies is case sensitive
    AnalysisError("create function foo() RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='noArgs'", "Could not find function noArgs() in: " + hdfsPath);
    // Types no longer match
    AnalysisError("create function foo(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='NoArgs'", "Could not find function NoArgs(INT) in: " + hdfsPath);
    // Check we can match identity for all types
    AnalyzesOk("create function identity(boolean) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(tinyint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(smallint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(int) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(bigint) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(float) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(double) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function identity(string) RETURNS int " + "LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='Identity'");
    AnalyzesOk("create function all_types_fn(string, boolean, tinyint, " + "smallint, int, bigint, float, double, decimal) returns int " + "location '/test-warehouse/libTestUdfs.so' symbol='AllTypes'");
    // Try creating functions with illegal function names.
    AnalysisError("create function 123A() RETURNS int" + udfSuffix, "Function cannot start with a digit: 123a");
    AnalysisError("create function A.`1A`() RETURNS int" + udfSuffix, "Function cannot start with a digit: 1a");
    AnalysisError("create function A.`ABC-D`() RETURNS int" + udfSuffix, "Function names must be all alphanumeric or underscore. Invalid name: abc-d");
    AnalysisError("create function baddb.f() RETURNS int" + udfSuffix, "Database does not exist: baddb");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create function f() RETURNS array<int>" + udfSuffix, "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f(map<string,int>) RETURNS int" + udfSuffix, "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create function f() RETURNS struct<f:int>" + udfSuffix, "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Try dropping functions.
    AnalyzesOk("drop function if exists foo()");
    AnalysisError("drop function foo()", "Function does not exist: foo()");
    AnalyzesOk("drop function if exists a.foo()");
    AnalysisError("drop function a.foo()", "Database does not exist: a");
    AnalyzesOk("drop function if exists foo()");
    AnalyzesOk("drop function if exists foo(int...)");
    AnalyzesOk("drop function if exists foo(double, int...)");
    // Add functions default.TestFn(), default.TestFn(double), default.TestFn(String...),
    addTestFunction("TestFn", new ArrayList<ScalarType>(), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.DOUBLE), false);
    addTestFunction("TestFn", Lists.newArrayList(Type.STRING), true);
    AnalysisError("create function TestFn() RETURNS INT " + udfSuffix, "Function already exists: testfn()");
    AnalysisError("create function TestFn(double) RETURNS INT " + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Fn(Double) and Fn(Double...) should be a conflict.
    AnalysisError("create function TestFn(double...) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    AnalysisError("create function TestFn(double) RETURNS INT" + udfSuffix, "Function already exists: testfn(DOUBLE)");
    // Add default.TestFn(int, int)
    addTestFunction("TestFn", Lists.newArrayList(Type.INT, Type.INT), false);
    AnalyzesOk("drop function TestFn(int, int)");
    AnalysisError("drop function TestFn(int, int, int)", "Function does not exist: testfn(INT, INT, INT)");
    // Fn(String...) was already added.
    AnalysisError("create function TestFn(String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String...) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalysisError("create function TestFn(String, String) RETURNS INT" + udfSuffix, "Function already exists: testfn(STRING...)");
    AnalyzesOk("create function TestFn(String, String, Int) RETURNS INT" + udfSuffix);
    // Check function overloading.
    AnalyzesOk("create function TestFn(int) RETURNS INT " + udfSuffix);
    // Create a function with the same signature in a different db
    AnalyzesOk("create function functional.TestFn() RETURNS INT " + udfSuffix);
    AnalyzesOk("drop function TestFn()");
    AnalyzesOk("drop function TestFn(double)");
    AnalyzesOk("drop function TestFn(string...)");
    AnalysisError("drop function TestFn(double...)", "Function does not exist: testfn(DOUBLE...)");
    AnalysisError("drop function TestFn(int)", "Function does not exist: testfn(INT)");
    AnalysisError("drop function functional.TestFn()", "Function does not exist: testfn()");
    AnalysisError("create function f() returns int " + udfSuffix + "init_fn='a'", "Optional argument 'INIT_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "serialize_fn='a'", "Optional argument 'SERIALIZE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "merge_fn='a'", "Optional argument 'MERGE_FN' should not be set");
    AnalysisError("create function f() returns int " + udfSuffix + "finalize_fn='a'", "Optional argument 'FINALIZE_FN' should not be set");
}
#end_block

#method_before
@Test
public void TestUda() throws AnalysisException {
    final String loc = " LOCATION '/test-warehouse/libTestUdas.so' ";
    final String hdfsLoc = "hdfs://localhost:20500/test-warehouse/libTestUdas.so";
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AGgInit'", "Could not find function AGgInit() returns INT in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(int, int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate'");
    // TODO: remove these when the BE can execute them
    AnalysisError("create aggregate function foo(int...) RETURNS int" + loc, "UDAs with varargs are not yet supported.");
    AnalysisError("create aggregate function " + "foo(int, int, int, int, int, int, int , int, int) " + "RETURNS int" + loc, "UDAs with more than 8 arguments are not yet supported.");
    // Check that CHAR and VARCHAR are not valid UDA argument or return types
    String symbols = " UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'";
    AnalysisError("create aggregate function foo(CHAR(5)) RETURNS int" + loc + symbols, "UDAs with CHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(VARCHAR(5)) RETURNS int" + loc + symbols, "UDAs with VARCHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS CHAR(5)" + loc + symbols, "UDAs with CHAR return type are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS VARCHAR(5)" + loc + symbols, "UDAs with VARCHAR return type are not yet supported.");
    // Specify the complete symbol. If the user does this, we can't guess the
    // other function names.
    // TODO: think about these error messages more. Perhaps they can be made
    // more actionable.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'");
    // Try with intermediate type
    // TODO: this is currently not supported. Remove these tests and re-enable
    // the commented out ones when we do.
    AnalyzesOk("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE int" + loc + "UPDATE_FN='AggUpdate'");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE double" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DOUBLE, that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE char(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, CHAR(10), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DECIMAL(10,0), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(40)" + loc + "UPDATE_FN='AggUpdate'", "Decimal precision must be <= 38.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='AggUpdate'");
    // AnalysisError("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge'" ,
    // "Finalize() is required for this UDA.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge' FINALIZE_FN='AggFinalize'");
    // Udf only arguments must not be set.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "SYMBOL='Bad'", "Optional argument 'SYMBOL' should not be set.");
    // Invalid char(0) type.
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE CHAR(0) LOCATION '/foo.so' UPDATE_FN='b'", "Char size must be > 0. Size is too small: 0.");
    AnalysisError("create aggregate function foo() RETURNS int" + loc, "UDAs must take at least one argument.");
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.jar' UPDATE_FN='b'", "Java UDAs are not supported.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create aggregate function foo(string, double) RETURNS array<int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(map<string,int>) RETURNS int " + loc + "UPDATE_FN='AggUpdate'", "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(int) RETURNS struct<f:int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Test missing .ll file. TODO: reenable when we can run IR UDAs
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.ll' UPDATE_FN='Fn'", "IR UDAs are not yet supported.");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='Fn'", "Could not load binary: /foo.ll");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='_ZABCD'", "Could not load binary: /foo.ll");
    // Test cases where the UPDATE_FN doesn't contain "Update" in which case the user has
    // to explicitly specify the other functions.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggFinalize'");
    // Serialize and Finalize have the same signature, make sure that's possible.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggFinalize' FINALIZE_FN='AggFinalize'");
    // If you don't specify the full symbol, we look for it in the binary. This should
    // prevent mismatched names by accident.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggSerialize'", "Could not find function AggSerialize() returns STRING in: " + hdfsLoc);
    // If you specify a mangled name, we just check it exists.
    // TODO: we should be able to validate better. This is almost certainly going
    // to crash everything.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' " + "INIT_FN='_Z12AggSerializePN10impala_udf15FunctionContextERKNS_6IntValE'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='_ZAggSerialize'", "Could not find symbol '_ZAggSerialize' in: " + hdfsLoc);
    // Tests for checking the symbol exists
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update'", "Could not find function Agg2Init() returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit'", "Could not find function Agg2Merge(STRING) returns STRING in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='BadFn'", "Could not find function BadFn(STRING) returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "FINALIZE_FN='not there'", "Could not find function not there(STRING) in: " + hdfsLoc);
}
#method_after
@Test
public void TestUda() throws AnalysisException {
    final String loc = " LOCATION '/test-warehouse/libTestUdas.so' ";
    final String hdfsLoc = "hdfs://localhost:20500/test-warehouse/libTestUdas.so";
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit'");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AGgInit'", "Could not find function AGgInit() returns INT in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(int, int) RETURNS int" + loc + "UPDATE_FN='AggUpdate'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate'");
    // TODO: remove these when the BE can execute them
    AnalysisError("create aggregate function foo(int...) RETURNS int" + loc, "UDAs with varargs are not yet supported.");
    AnalysisError("create aggregate function " + "foo(int, int, int, int, int, int, int , int, int) " + "RETURNS int" + loc, "UDAs with more than 8 arguments are not yet supported.");
    // Check that CHAR and VARCHAR are not valid UDA argument or return types
    String symbols = " UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'";
    AnalysisError("create aggregate function foo(CHAR(5)) RETURNS int" + loc + symbols, "UDAs with CHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(VARCHAR(5)) RETURNS int" + loc + symbols, "UDAs with VARCHAR arguments are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS CHAR(5)" + loc + symbols, "UDAs with CHAR return type are not yet supported.");
    AnalysisError("create aggregate function foo(int) RETURNS VARCHAR(5)" + loc + symbols, "UDAs with VARCHAR return type are not yet supported.");
    // Specify the complete symbol. If the user does this, we can't guess the
    // other function names.
    // TODO: think about these error messages more. Perhaps they can be made
    // more actionable.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(int) RETURNS int" + loc + "UPDATE_FN='_Z9AggUpdatePN10impala_udf15FunctionContextERKNS_6IntValEPS2_' " + "INIT_FN='_Z7AggInitPN10impala_udf15FunctionContextEPNS_6IntValE' " + "MERGE_FN='_Z8AggMergePN10impala_udf15FunctionContextERKNS_6IntValEPS2_'");
    // Try with intermediate type
    // TODO: this is currently not supported. Remove these tests and re-enable
    // the commented out ones when we do.
    AnalyzesOk("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE int" + loc + "UPDATE_FN='AggUpdate'");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE double" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DOUBLE, that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE char(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, CHAR(10), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(10)" + loc + "UPDATE_FN='AggUpdate'", "UDAs with an intermediate type, DECIMAL(10,0), that is different from the " + "return type, INT, are currently not supported.");
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE decimal(40)" + loc + "UPDATE_FN='AggUpdate'", "Decimal precision must be <= 38: 40");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='AggUpdate'");
    // AnalysisError("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge'" ,
    // "Finalize() is required for this UDA.");
    // AnalyzesOk("create aggregate function foo(int) RETURNS int " +
    // "INTERMEDIATE CHAR(10)" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' " +
    // "MERGE_FN='AggMerge' FINALIZE_FN='AggFinalize'");
    // Udf only arguments must not be set.
    AnalysisError("create aggregate function foo(int) RETURNS int" + loc + "SYMBOL='Bad'", "Optional argument 'SYMBOL' should not be set.");
    // Invalid char(0) type.
    AnalysisError("create aggregate function foo(int) RETURNS int " + "INTERMEDIATE CHAR(0) LOCATION '/foo.so' UPDATE_FN='b'", "Char size must be > 0: 0");
    AnalysisError("create aggregate function foo() RETURNS int" + loc, "UDAs must take at least one argument.");
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.jar' UPDATE_FN='b'", "Java UDAs are not supported.");
    // Try creating functions with unsupported return/arg types.
    AnalysisError("create aggregate function foo(string, double) RETURNS array<int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'ARRAY<INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(map<string,int>) RETURNS int " + loc + "UPDATE_FN='AggUpdate'", "Type 'MAP<STRING,INT>' is not supported in UDFs/UDAs.");
    AnalysisError("create aggregate function foo(int) RETURNS struct<f:int> " + loc + "UPDATE_FN='AggUpdate'", "Type 'STRUCT<f:INT>' is not supported in UDFs/UDAs.");
    // Test missing .ll file. TODO: reenable when we can run IR UDAs
    AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " + "'/foo.ll' UPDATE_FN='Fn'", "IR UDAs are not yet supported.");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='Fn'", "Could not load binary: /foo.ll");
    // AnalysisError("create aggregate function foo(int) RETURNS int LOCATION " +
    // "'/foo.ll' UPDATE_FN='_ZABCD'", "Could not load binary: /foo.ll");
    // Test cases where the UPDATE_FN doesn't contain "Update" in which case the user has
    // to explicitly specify the other functions.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg'", "Could not infer symbol for init() function.");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit'", "Could not infer symbol for merge() function.");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggFinalize'");
    // Serialize and Finalize have the same signature, make sure that's possible.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggSerialize' FINALIZE_FN='AggSerialize'");
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' SERIALIZE_FN='AggFinalize' FINALIZE_FN='AggFinalize'");
    // If you don't specify the full symbol, we look for it in the binary. This should
    // prevent mismatched names by accident.
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='AggSerialize'", "Could not find function AggSerialize() returns STRING in: " + hdfsLoc);
    // If you specify a mangled name, we just check it exists.
    // TODO: we should be able to validate better. This is almost certainly going
    // to crash everything.
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' " + "INIT_FN='_Z12AggSerializePN10impala_udf15FunctionContextERKNS_6IntValE'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='AggUpdate' INIT_FN='_ZAggSerialize'", "Could not find symbol '_ZAggSerialize' in: " + hdfsLoc);
    // Tests for checking the symbol exists
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update'", "Could not find function Agg2Init() returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit'", "Could not find function Agg2Merge(STRING) returns STRING in: " + hdfsLoc);
    AnalyzesOk("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge'");
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='BadFn'", "Could not find function BadFn(STRING) returns STRING in: " + hdfsLoc);
    AnalysisError("create aggregate function foo(string, double) RETURNS string" + loc + "UPDATE_FN='Agg2Update' INIT_FN='AggInit' MERGE_FN='AggMerge' " + "FINALIZE_FN='not there'", "Could not find function not there(STRING) in: " + hdfsLoc);
}
#end_block

#method_before
@Test
public void TestTypes() {
    // Test primitive types.
    TypeDefsAnalyzeOk("BOOLEAN");
    TypeDefsAnalyzeOk("TINYINT");
    TypeDefsAnalyzeOk("SMALLINT");
    TypeDefsAnalyzeOk("INT", "INTEGER");
    TypeDefsAnalyzeOk("BIGINT");
    TypeDefsAnalyzeOk("FLOAT");
    TypeDefsAnalyzeOk("DOUBLE", "REAL");
    TypeDefsAnalyzeOk("STRING");
    TypeDefsAnalyzeOk("CHAR(1)", "CHAR(20)");
    TypeDefsAnalyzeOk("BINARY");
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("TIMESTAMP");
    // Test decimal.
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("DECIMAL(1)");
    TypeDefsAnalyzeOk("DECIMAL(12, 7)");
    TypeDefsAnalyzeOk("DECIMAL(38)");
    TypeDefsAnalyzeOk("DECIMAL(38, 1)");
    TypeDefsAnalyzeOk("DECIMAL(38, 38)");
    TypeDefAnalysisError("DECIMAL(1, 10)", "Decimal scale (10) must be <= precision (1).");
    TypeDefAnalysisError("DECIMAL(0, 0)", "Decimal precision must be greater than 0.");
    TypeDefAnalysisError("DECIMAL(39, 0)", "Decimal precision must be <= 38.");
    // Test complex types.
    TypeDefsAnalyzeOk("ARRAY<BIGINT>");
    TypeDefsAnalyzeOk("MAP<TINYINT, DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<f:TINYINT>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT, b:BIGINT, c:DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT COMMENT 'x', b:BIGINT, c:DOUBLE COMMENT 'y'>");
    // Map keys can't be complex types.
    TypeDefAnalysisError("map<array<int>, int>", "Map type cannot have a complex-typed key: MAP<ARRAY<INT>,INT>");
    // Duplicate struct-field name.
    TypeDefAnalysisError("STRUCT<f1: int, f2: string, f1: float>", "Duplicate field name 'f1' in struct 'STRUCT<f1:INT,f2:STRING,f1:FLOAT>'");
    // Invalid struct-field name.
    TypeDefAnalysisError("STRUCT<`???`: int>", "Invalid struct field name: ???");
}
#method_after
@Test
public void TestTypes() {
    // Test primitive types.
    TypeDefsAnalyzeOk("BOOLEAN");
    TypeDefsAnalyzeOk("TINYINT");
    TypeDefsAnalyzeOk("SMALLINT");
    TypeDefsAnalyzeOk("INT", "INTEGER");
    TypeDefsAnalyzeOk("BIGINT");
    TypeDefsAnalyzeOk("FLOAT");
    TypeDefsAnalyzeOk("DOUBLE", "REAL");
    TypeDefsAnalyzeOk("STRING");
    TypeDefsAnalyzeOk("CHAR(1)", "CHAR(20)");
    TypeDefsAnalyzeOk("BINARY");
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("TIMESTAMP");
    // Test decimal.
    TypeDefsAnalyzeOk("DECIMAL");
    TypeDefsAnalyzeOk("DECIMAL(1)");
    TypeDefsAnalyzeOk("DECIMAL(12, 7)");
    TypeDefsAnalyzeOk("DECIMAL(38)");
    TypeDefsAnalyzeOk("DECIMAL(38, 1)");
    TypeDefsAnalyzeOk("DECIMAL(38, 38)");
    TypeDefAnalysisError("DECIMAL(1, 10)", "Decimal scale (10) must be <= precision (1)");
    TypeDefAnalysisError("DECIMAL(0, 0)", "Decimal precision must be > 0: 0");
    TypeDefAnalysisError("DECIMAL(39, 0)", "Decimal precision must be <= 38");
    // Test complex types.
    TypeDefsAnalyzeOk("ARRAY<BIGINT>");
    TypeDefsAnalyzeOk("MAP<TINYINT, DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<f:TINYINT>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT, b:BIGINT, c:DOUBLE>");
    TypeDefsAnalyzeOk("STRUCT<a:TINYINT COMMENT 'x', b:BIGINT, c:DOUBLE COMMENT 'y'>");
    // Map keys can't be complex types.
    TypeDefAnalysisError("map<array<int>, int>", "Map type cannot have a complex-typed key: MAP<ARRAY<INT>,INT>");
    // Duplicate struct-field name.
    TypeDefAnalysisError("STRUCT<f1: int, f2: string, f1: float>", "Duplicate field name 'f1' in struct 'STRUCT<f1:INT,f2:STRING,f1:FLOAT>'");
    // Invalid struct-field name.
    TypeDefAnalysisError("STRUCT<`???`: int>", "Invalid struct field name: ???");
}
#end_block

#method_before
@Test
public void TestPermissionValidation() throws AnalysisException {
    String location = "/test-warehouse/.tmp_" + UUID.randomUUID().toString();
    Path parentPath = FileSystemUtil.createFullyQualifiedPath(new Path(location));
    FileSystem fs = null;
    try {
        fs = parentPath.getFileSystem(FileSystemUtil.getConfiguration());
        // Test location doesn't exist
        AnalyzesOk(String.format("create table new_table (col INT) location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        // Test localtion path with trailing slash.
        AnalyzesOk(String.format("create table new_table (col INT) location " + "'%s/new_table/'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table location '%s/new_table' " + "as select 1, 1", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table like functional.alltypes " + "location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create database new_db location '%s/new_db'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        fs.mkdirs(parentPath);
        // Create a test data file for load data test
        FSDataOutputStream out = fs.create(new Path(parentPath, "test_loaddata/testdata.txt"));
        out.close();
        fs.setPermission(parentPath, new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));
        // Test location exists but Impala doesn't have sufficient permission
        AnalyzesOk(String.format("create data Source serverlog location " + "'%s/foo.jar' class 'foo.Bar' API_VERSION 'V1'", location), String.format("Impala does not have READ access to path '%s'", parentPath));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "add partition (s2='hello') location '%s/new_partition'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "partition(s2=NULL) set location '%s/new_part_loc'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        // Test location exists and Impala does have sufficient permission
        fs.setPermission(parentPath, new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location));
    } catch (IOException e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        // Clean up
        try {
            if (fs != null && fs.exists(parentPath)) {
                fs.delete(parentPath, true);
            }
        } catch (IOException e) {
        // Ignore
        }
    }
}
#method_after
@Test
public void TestPermissionValidation() throws AnalysisException {
    String location = "/test-warehouse/.tmp_" + UUID.randomUUID().toString();
    Path parentPath = FileSystemUtil.createFullyQualifiedPath(new Path(location));
    FileSystem fs = null;
    try {
        fs = parentPath.getFileSystem(FileSystemUtil.getConfiguration());
        // Test location doesn't exist
        AnalyzesOk(String.format("create table new_table (col INT) location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        // Test localtion path with trailing slash.
        AnalyzesOk(String.format("create table new_table (col INT) location " + "'%s/new_table/'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table location '%s/new_table' " + "as select 1, 1", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create table new_table like functional.alltypes " + "location '%s/new_table'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        AnalyzesOk(String.format("create database new_db location '%s/new_db'", location), String.format("Path '%s' cannot be reached: Path does not exist.", parentPath));
        fs.mkdirs(parentPath);
        // Create a test data file for load data test
        FSDataOutputStream out = fs.create(new Path(parentPath, "test_loaddata/testdata.txt"));
        out.close();
        fs.setPermission(parentPath, new FsPermission(FsAction.NONE, FsAction.NONE, FsAction.NONE));
        // Test location exists but Impala doesn't have sufficient permission
        AnalyzesOk(String.format("create data Source serverlog location " + "'%s/foo.jar' class 'foo.Bar' API_VERSION 'V1'", location), String.format("Impala does not have READ access to path '%s'", parentPath));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.insert_string_partitioned " + "add partition (s2='hello') location '%s/new_partition'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        AnalyzesOk(String.format("alter table functional.stringpartitionkey " + "partition(string_col = 'partition1') set location '%s/new_part_loc'", location), String.format("Impala does not have READ_WRITE access to path '%s'", parentPath));
        // Test location exists and Impala does have sufficient permission
        fs.setPermission(parentPath, new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE));
        AnalyzesOk(String.format("create external table new_table (col INT) location " + "'%s/new_table'", location));
    } catch (IOException e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        // Clean up
        try {
            if (fs != null && fs.exists(parentPath)) {
                fs.delete(parentPath, true);
            }
        } catch (IOException e) {
        // Ignore
        }
    }
}
#end_block

#method_before
@Override
public int compareTo(LiteralExpr o) {
    if (!(o instanceof BoolLiteral))
        return -1;
    BoolLiteral other = (BoolLiteral) o;
    if (value_ && !other.getValue())
        return 1;
    if (!value_ && other.getValue())
        return -1;
    return 0;
}
#method_after
@Override
public int compareTo(LiteralExpr o) {
    int ret = super.compareTo(o);
    if (ret != 0)
        return ret;
    BoolLiteral other = (BoolLiteral) o;
    if (value_ && !other.getValue())
        return 1;
    if (!value_ && other.getValue())
        return -1;
    return 0;
}
#end_block

#method_before
protected void createColumnAndViewDefs(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(dbName_);
    Preconditions.checkNotNull(owner_);
    // Set the finalColDefs to reflect the given column definitions.
    if (columnDefs_ != null) {
        Preconditions.checkState(!columnDefs_.isEmpty());
        if (columnDefs_.size() != viewDefStmt_.getColLabels().size()) {
            String cmp = (columnDefs_.size() > viewDefStmt_.getColLabels().size()) ? "more" : "fewer";
            throw new AnalysisException(String.format("Column-definition list has " + "%s columns (%s) than the view-definition query statement returns (%s).", cmp, columnDefs_.size(), viewDefStmt_.getColLabels().size()));
        }
        finalColDefs_ = columnDefs_;
        Preconditions.checkState(columnDefs_.size() == viewDefStmt_.getBaseTblResultExprs().size());
        for (int i = 0; i < columnDefs_.size(); ++i) {
            // Set type in the column definition from the view-definition statement.
            columnDefs_.get(i).setType(viewDefStmt_.getBaseTblResultExprs().get(i).getType());
        }
    } else {
        // Create list of column definitions from the view-definition statement.
        finalColDefs_ = Lists.newArrayList();
        List<Expr> exprs = viewDefStmt_.getBaseTblResultExprs();
        List<String> labels = viewDefStmt_.getColLabels();
        Preconditions.checkState(exprs.size() == labels.size());
        for (int i = 0; i < viewDefStmt_.getColLabels().size(); ++i) {
            ColumnDesc colDef = new ColumnDesc(labels.get(i), null, null);
            colDef.setType(exprs.get(i).getType());
            finalColDefs_.add(colDef);
        }
    }
    // Check that the column definitions have valid names, and that there are no
    // duplicate column names.
    Set<String> distinctColNames = Sets.newHashSet();
    for (ColumnDesc colDesc : finalColDefs_) {
        colDesc.analyze();
        if (!distinctColNames.add(colDesc.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDesc.getColName());
        }
    }
    // Set original and expanded view-definition SQL strings.
    originalViewDef_ = viewDefStmt_.toSql();
    // as the original one.
    if (columnDefs_ == null) {
        inlineViewDef_ = originalViewDef_;
        return;
    }
    // Wrap the original view-definition statement into a SELECT to enforce the
    // given column definitions.
    StringBuilder sb = new StringBuilder();
    sb.append("SELECT ");
    for (int i = 0; i < finalColDefs_.size(); ++i) {
        String colRef = ToSqlUtils.getIdentSql(viewDefStmt_.getColLabels().get(i));
        String colAlias = ToSqlUtils.getIdentSql(finalColDefs_.get(i).getColName());
        sb.append(String.format("%s.%s AS %s", tableName_.getTbl(), colRef, colAlias));
        sb.append((i + 1 != finalColDefs_.size()) ? ", " : "");
    }
    // Do not use 'AS' for table aliases because Hive only accepts them without 'AS'.
    sb.append(String.format(" FROM (%s) %s", originalViewDef_, tableName_.getTbl()));
    inlineViewDef_ = sb.toString();
}
#method_after
protected void createColumnAndViewDefs(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(dbName_);
    Preconditions.checkNotNull(owner_);
    // Set the finalColDefs to reflect the given column definitions.
    if (columnDefs_ != null) {
        Preconditions.checkState(!columnDefs_.isEmpty());
        if (columnDefs_.size() != viewDefStmt_.getColLabels().size()) {
            String cmp = (columnDefs_.size() > viewDefStmt_.getColLabels().size()) ? "more" : "fewer";
            throw new AnalysisException(String.format("Column-definition list has " + "%s columns (%s) than the view-definition query statement returns (%s).", cmp, columnDefs_.size(), viewDefStmt_.getColLabels().size()));
        }
        finalColDefs_ = columnDefs_;
        Preconditions.checkState(columnDefs_.size() == viewDefStmt_.getBaseTblResultExprs().size());
        for (int i = 0; i < columnDefs_.size(); ++i) {
            // Set type in the column definition from the view-definition statement.
            columnDefs_.get(i).setType(viewDefStmt_.getBaseTblResultExprs().get(i).getType());
        }
    } else {
        // Create list of column definitions from the view-definition statement.
        finalColDefs_ = Lists.newArrayList();
        List<Expr> exprs = viewDefStmt_.getBaseTblResultExprs();
        List<String> labels = viewDefStmt_.getColLabels();
        Preconditions.checkState(exprs.size() == labels.size());
        for (int i = 0; i < viewDefStmt_.getColLabels().size(); ++i) {
            ColumnDef colDef = new ColumnDef(labels.get(i), null, null);
            colDef.setType(exprs.get(i).getType());
            finalColDefs_.add(colDef);
        }
    }
    // Check that the column definitions have valid names, and that there are no
    // duplicate column names.
    Set<String> distinctColNames = Sets.newHashSet();
    for (ColumnDef colDesc : finalColDefs_) {
        colDesc.analyze();
        if (!distinctColNames.add(colDesc.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDesc.getColName());
        }
    }
    // Set original and expanded view-definition SQL strings.
    originalViewDef_ = viewDefStmt_.toSql();
    // as the original one.
    if (columnDefs_ == null) {
        inlineViewDef_ = originalViewDef_;
        return;
    }
    // Wrap the original view-definition statement into a SELECT to enforce the
    // given column definitions.
    StringBuilder sb = new StringBuilder();
    sb.append("SELECT ");
    for (int i = 0; i < finalColDefs_.size(); ++i) {
        String colRef = ToSqlUtils.getIdentSql(viewDefStmt_.getColLabels().get(i));
        String colAlias = ToSqlUtils.getIdentSql(finalColDefs_.get(i).getColName());
        sb.append(String.format("%s.%s AS %s", tableName_.getTbl(), colRef, colAlias));
        sb.append((i + 1 != finalColDefs_.size()) ? ", " : "");
    }
    // Do not use 'AS' for table aliases because Hive only accepts them without 'AS'.
    sb.append(String.format(" FROM (%s) %s", originalViewDef_, tableName_.getTbl()));
    inlineViewDef_ = sb.toString();
}
#end_block

#method_before
public TCreateOrAlterViewParams toThrift() {
    TCreateOrAlterViewParams params = new TCreateOrAlterViewParams();
    params.setView_name(new TTableName(getDb(), getTbl()));
    for (ColumnDesc col : finalColDefs_) {
        params.addToColumns(col.toThrift());
    }
    params.setOwner(getOwner());
    params.setIf_not_exists(getIfNotExists());
    params.setOriginal_view_def(originalViewDef_);
    params.setExpanded_view_def(inlineViewDef_);
    if (comment_ != null)
        params.setComment(comment_);
    return params;
}
#method_after
public TCreateOrAlterViewParams toThrift() {
    TCreateOrAlterViewParams params = new TCreateOrAlterViewParams();
    params.setView_name(new TTableName(getDb(), getTbl()));
    for (ColumnDef col : finalColDefs_) {
        params.addToColumns(col.toThrift());
    }
    params.setOwner(getOwner());
    params.setIf_not_exists(getIfNotExists());
    params.setOriginal_view_def(originalViewDef_);
    params.setExpanded_view_def(inlineViewDef_);
    if (comment_ != null)
        params.setComment(comment_);
    return params;
}
#end_block

#method_before
public List<ColumnDesc> getColumnDescs() {
    return columnDefs_;
}
#method_after
public List<ColumnDef> getColumnDescs() {
    return columnDefs_;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        if (analyzer.containsSubquery()) {
            // The select statement of this CTAS is nested. Rewrite the
            // statement to unnest all subqueries and re-analyze using a new analyzer.
            StmtRewriter.rewriteQueryStatement(tmpQueryStmt, tmpAnalyzer);
            // Update the insert statement with the unanalyzed rewritten select stmt.
            insertStmt_.setQueryStmt(tmpQueryStmt.clone());
            // Re-analyze the select statement of the CTAS.
            tmpQueryStmt = insertStmt_.getQueryStmt().clone();
            tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
            tmpAnalyzer.setUseHiveColLabels(true);
            tmpQueryStmt.analyze(tmpAnalyzer);
        }
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDesc colDef = new ColumnDesc(tmpQueryStmt.getColLabels().get(i), null, null);
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE"));
    }
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient();
    try {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        // target location for the INSERT statement, it is important the two match.
        if (createStmt_.getLocation() == null) {
            createStmt_.setLocation(new HdfsUri(msTbl.getSd().getLocation()));
        }
        // Create a "temp" table based off the given metastore.api.Table object. Normally,
        // the CatalogService assigns all table IDs, but in this case we need to assign the
        // "temp" table an ID locally. This table ID cannot conflict with any table in the
        // SelectStmt (or the BE will be very confused). To ensure the ID is unique within
        // this query, just assign it the invalid table ID. The CatalogServer will assign
        // this table a proper ID once it is created there as part of the CTAS execution.
        Table table = Table.fromMetastoreTable(TableId.createInvalidId(), db, msTbl);
        Preconditions.checkState(table != null && table instanceof HdfsTable);
        HdfsTable hdfsTable = (HdfsTable) table;
        hdfsTable.load(hdfsTable, client.getHiveClient(), msTbl);
        insertStmt_.setTargetTable(table);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        client.release();
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        if (analyzer.containsSubquery()) {
            // The select statement of this CTAS is nested. Rewrite the
            // statement to unnest all subqueries and re-analyze using a new analyzer.
            StmtRewriter.rewriteQueryStatement(tmpQueryStmt, tmpAnalyzer);
            // Update the insert statement with the unanalyzed rewritten select stmt.
            insertStmt_.setQueryStmt(tmpQueryStmt.clone());
            // Re-analyze the select statement of the CTAS.
            tmpQueryStmt = insertStmt_.getQueryStmt().clone();
            tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
            tmpAnalyzer.setUseHiveColLabels(true);
            tmpQueryStmt.analyze(tmpAnalyzer);
        }
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, null);
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE"));
    }
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient();
    try {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        // target location for the INSERT statement, it is important the two match.
        if (createStmt_.getLocation() == null) {
            createStmt_.setLocation(new HdfsUri(msTbl.getSd().getLocation()));
        }
        // Create a "temp" table based off the given metastore.api.Table object. Normally,
        // the CatalogService assigns all table IDs, but in this case we need to assign the
        // "temp" table an ID locally. This table ID cannot conflict with any table in the
        // SelectStmt (or the BE will be very confused). To ensure the ID is unique within
        // this query, just assign it the invalid table ID. The CatalogServer will assign
        // this table a proper ID once it is created there as part of the CTAS execution.
        Table table = Table.fromMetastoreTable(TableId.createInvalidId(), db, msTbl);
        Preconditions.checkState(table != null && table instanceof HdfsTable);
        HdfsTable hdfsTable = (HdfsTable) table;
        hdfsTable.load(hdfsTable, client.getHiveClient(), msTbl);
        insertStmt_.setTargetTable(table);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        client.release();
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#end_block

#method_before
private static List<ColumnDesc> extractParquetSchema(HdfsUri location) throws AnalysisException {
    parquet.schema.MessageType parquetSchema = loadParquetSchema(location.getPath());
    List<parquet.schema.Type> fields = parquetSchema.getFields();
    List<ColumnDesc> schema = new ArrayList<ColumnDesc>();
    for (parquet.schema.Type field : fields) {
        Type type = null;
        if (field.getOriginalType() != null) {
            type = convertLogicalParquetType(field);
        } else if (field.isPrimitive()) {
            type = convertPrimitiveParquetType(field);
        } else {
            throw new AnalysisException("Unsupported parquet type for field " + field.getName());
        }
        String colName = field.getName();
        schema.add(new ColumnDesc(colName, new TypeDef(type), "inferred from: " + field.toString()));
    }
    return schema;
}
#method_after
private static List<ColumnDef> extractParquetSchema(HdfsUri location) throws AnalysisException {
    parquet.schema.MessageType parquetSchema = loadParquetSchema(location.getPath());
    List<parquet.schema.Type> fields = parquetSchema.getFields();
    List<ColumnDef> schema = new ArrayList<ColumnDef>();
    for (parquet.schema.Type field : fields) {
        Type type = null;
        if (field.getOriginalType() != null) {
            type = convertLogicalParquetType(field);
        } else if (field.isPrimitive()) {
            type = convertPrimitiveParquetType(field);
        } else {
            throw new AnalysisException("Unsupported parquet type for field " + field.getName());
        }
        String colName = field.getName();
        schema.add(new ColumnDef(colName, new TypeDef(type), "inferred from: " + field.toString()));
    }
    return schema;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    if (contains(Subquery.class)) {
        // which is a Subquery.
        if (children_.size() != 2 || !(getChild(1) instanceof Subquery)) {
            throw new AnalysisException("Unsupported IN predicate with a subquery: " + toSqlImpl());
        }
        Subquery subquery = (Subquery) getChild(1);
        if (!subquery.returnsScalarColumn()) {
            throw new AnalysisException("Subquery must return a single column: " + subquery.toSql());
        }
        // Ensure that the column in the lhs of the IN predicate and the result of
        // the subquery are type compatible. No need to perform any
        // casting at this point. Any casting needed will be performed when the
        // subquery is unnested.
        ArrayList<Expr> subqueryExprs = subquery.getStatement().getResultExprs();
        Expr compareExpr = children_.get(0);
        Expr subqueryExpr = subqueryExprs.get(0);
        analyzer.getCompatibleType(compareExpr.getType(), compareExpr, subqueryExpr);
    } else {
        Preconditions.checkState(getChildren().size() >= 2);
        analyzer.castAllToCompatibleType(children_);
        if (children_.get(0).getType().isNull()) {
            // Make sure the BE never sees TYPE_NULL by picking an arbitrary type
            for (int i = 0; i < children_.size(); ++i) {
                uncheckedCastChild(Type.BOOLEAN, i);
            }
        }
        // Choose SetLookup or Iterate strategy. SetLookup can be used if all the exprs in
        // the IN list are constant, and is faster than iterating if the IN list is big
        // enough.
        boolean allConstant = true;
        for (int i = 1; i < children_.size(); ++i) {
            if (!children_.get(i).isConstant()) {
                allConstant = false;
                break;
            }
        }
        boolean useSetLookup = allConstant;
        // Threshold based on InPredicateBenchmark results
        // TODO: if the IN list is a single int, make this an equality predicate
        int setLookupThreshold = children_.get(0).getType().isStringType() ? 9 : 2;
        if (children_.size() - 1 < setLookupThreshold)
            useSetLookup = false;
        // NYI
        if (getChild(0).type_.isTimestamp() || getChild(0).type_.isDecimal()) {
            useSetLookup = false;
        }
        // Only lookup fn_ if all subqueries have been rewritten. If the second child is a
        // subquery, it will have type ArrayType, which cannot be resolved to a builtin
        // function and will fail analysis.
        Type[] argTypes = { getChild(0).type_, getChild(1).type_ };
        if (useSetLookup) {
            if (isNotIn_) {
                fn_ = getBuiltinFunction(analyzer, NOT_IN_SET_LOOKUP, argTypes, CompareMode.IS_SUPERTYPE_OF);
            } else {
                fn_ = getBuiltinFunction(analyzer, IN_SET_LOOKUP, argTypes, CompareMode.IS_SUPERTYPE_OF);
            }
        } else {
            if (isNotIn_) {
                fn_ = getBuiltinFunction(analyzer, NOT_IN_ITERATE, argTypes, CompareMode.IS_SUPERTYPE_OF);
            } else {
                fn_ = getBuiltinFunction(analyzer, IN_ITERATE, argTypes, CompareMode.IS_SUPERTYPE_OF);
            }
        }
        Preconditions.checkNotNull(fn_);
        Preconditions.checkState(fn_.getReturnType().isBoolean());
        castForFunctionCall(false);
    }
    // TODO: Fix selectivity_ for nested predicate
    Reference<SlotRef> slotRefRef = new Reference<SlotRef>();
    Reference<Integer> idxRef = new Reference<Integer>();
    if (isSingleColumnPredicate(slotRefRef, idxRef) && idxRef.getRef() == 0 && slotRefRef.getRef().getNumDistinctValues() > 0) {
        selectivity_ = (double) (getChildren().size() - 1) / (double) slotRefRef.getRef().getNumDistinctValues();
        selectivity_ = Math.max(0.0, Math.min(1.0, selectivity_));
    } else {
        selectivity_ = Expr.DEFAULT_SELECTIVITY;
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    super.analyze(analyzer);
    if (contains(Subquery.class)) {
        // which is a Subquery.
        if (children_.size() != 2 || !(getChild(1) instanceof Subquery)) {
            throw new AnalysisException("Unsupported IN predicate with a subquery: " + toSqlImpl());
        }
        Subquery subquery = (Subquery) getChild(1);
        if (!subquery.returnsScalarColumn()) {
            throw new AnalysisException("Subquery must return a single column: " + subquery.toSql());
        }
        // Ensure that the column in the lhs of the IN predicate and the result of
        // the subquery are type compatible. No need to perform any
        // casting at this point. Any casting needed will be performed when the
        // subquery is unnested.
        ArrayList<Expr> subqueryExprs = subquery.getStatement().getResultExprs();
        Expr compareExpr = children_.get(0);
        Expr subqueryExpr = subqueryExprs.get(0);
        analyzer.getCompatibleType(compareExpr.getType(), compareExpr, subqueryExpr);
    } else {
        Preconditions.checkState(getChildren().size() >= 2);
        analyzer.castAllToCompatibleType(children_);
        if (children_.get(0).getType().isNull()) {
            // Make sure the BE never sees TYPE_NULL by picking an arbitrary type
            for (int i = 0; i < children_.size(); ++i) {
                uncheckedCastChild(Type.BOOLEAN, i);
            }
        }
        // Choose SetLookup or Iterate strategy. SetLookup can be used if all the exprs in
        // the IN list are constant, and is faster than iterating if the IN list is big
        // enough.
        boolean allConstant = true;
        for (int i = 1; i < children_.size(); ++i) {
            if (!children_.get(i).isConstant()) {
                allConstant = false;
                break;
            }
        }
        boolean useSetLookup = allConstant;
        // Threshold based on InPredicateBenchmark results
        int setLookupThreshold = children_.get(0).getType().isStringType() ? 9 : 2;
        if (children_.size() - 1 < setLookupThreshold)
            useSetLookup = false;
        // NYI
        if (getChild(0).type_.isTimestamp() || getChild(0).type_.isDecimal()) {
            useSetLookup = false;
        }
        // Only lookup fn_ if all subqueries have been rewritten. If the second child is a
        // subquery, it will have type ArrayType, which cannot be resolved to a builtin
        // function and will fail analysis.
        Type[] argTypes = { getChild(0).type_, getChild(1).type_ };
        if (useSetLookup) {
            fn_ = getBuiltinFunction(analyzer, isNotIn_ ? NOT_IN_SET_LOOKUP : IN_SET_LOOKUP, argTypes, CompareMode.IS_SUPERTYPE_OF);
        } else {
            fn_ = getBuiltinFunction(analyzer, isNotIn_ ? NOT_IN_ITERATE : IN_ITERATE, argTypes, CompareMode.IS_SUPERTYPE_OF);
        }
        Preconditions.checkNotNull(fn_);
        Preconditions.checkState(fn_.getReturnType().isBoolean());
        castForFunctionCall(false);
    }
    // TODO: Fix selectivity_ for nested predicate
    Reference<SlotRef> slotRefRef = new Reference<SlotRef>();
    Reference<Integer> idxRef = new Reference<Integer>();
    if (isSingleColumnPredicate(slotRefRef, idxRef) && idxRef.getRef() == 0 && slotRefRef.getRef().getNumDistinctValues() > 0) {
        selectivity_ = (double) (getChildren().size() - 1) / (double) slotRefRef.getRef().getNumDistinctValues();
        selectivity_ = Math.max(0.0, Math.min(1.0, selectivity_));
    } else {
        selectivity_ = Expr.DEFAULT_SELECTIVITY;
    }
}
#end_block

#method_before
public static ScalarFunction createBuiltin(String name, ArrayList<Type> argTypes, boolean hasVarArgs, Type retType, String symbol, String prepareFnSymbol, String closeFnSymbol, boolean isOperator) {
    Preconditions.checkNotNull(symbol);
    FunctionArgs fnArgs = new FunctionArgs(argTypes, hasVarArgs);
    ScalarFunction fn = new ScalarFunction(new FunctionName(Catalog.BUILTINS_DB, name), fnArgs, retType);
    fn.setBinaryType(TFunctionBinaryType.BUILTIN);
    fn.setUserVisible(!isOperator);
    try {
        fn.symbolName_ = fn.lookupSymbol(symbol, TSymbolType.UDF_EVALUATE, null, fn.hasVarArgs(), fn.getArgs());
        if (prepareFnSymbol != null) {
            fn.prepareFnSymbol_ = fn.lookupSymbol(prepareFnSymbol, TSymbolType.UDF_PREPARE);
        }
        if (closeFnSymbol != null) {
            fn.closeFnSymbol_ = fn.lookupSymbol(closeFnSymbol, TSymbolType.UDF_CLOSE);
        }
    } catch (AnalysisException e) {
        // This should never happen
        Preconditions.checkState(false, "Builtin symbol '" + symbol + "'" + argTypes + " not found!" + e.getStackTrace());
        throw new RuntimeException("Builtin symbol not found!", e);
    }
    return fn;
}
#method_after
public static ScalarFunction createBuiltin(String name, ArrayList<Type> argTypes, boolean hasVarArgs, Type retType, String symbol, String prepareFnSymbol, String closeFnSymbol, boolean isOperator) {
    Preconditions.checkNotNull(symbol);
    FunctionArgs fnArgs = new FunctionArgs(argTypes, hasVarArgs);
    ScalarFunction fn = new ScalarFunction(new FunctionName(Catalog.BUILTINS_DB, name), fnArgs, retType);
    fn.setBinaryType(TFunctionBinaryType.BUILTIN);
    fn.setUserVisible(!isOperator);
    try {
        fn.symbolName_ = fn.lookupSymbol(symbol, TSymbolType.UDF_EVALUATE, null, fn.hasVarArgs(), fn.getArgs());
    } catch (AnalysisException e) {
        // This should never happen
        throw new RuntimeException("Builtin symbol '" + symbol + "'" + argTypes + " not found!", e);
    }
    if (prepareFnSymbol != null) {
        try {
            fn.prepareFnSymbol_ = fn.lookupSymbol(prepareFnSymbol, TSymbolType.UDF_PREPARE);
        } catch (AnalysisException e) {
            // This should never happen
            throw new RuntimeException("Builtin symbol '" + prepareFnSymbol + "' not found!", e);
        }
    }
    if (closeFnSymbol != null) {
        try {
            fn.closeFnSymbol_ = fn.lookupSymbol(closeFnSymbol, TSymbolType.UDF_CLOSE);
        } catch (AnalysisException e) {
            // This should never happen
            throw new RuntimeException("Builtin symbol '" + closeFnSymbol + "' not found!", e);
        }
    }
    return fn;
}
#end_block

#method_before
@Override
protected Expr uncheckedCastTo(Type targetType) throws AnalysisException {
    Preconditions.checkState(targetType.isNumericType());
    // consistent with casting/overflow of non-constant exprs that return decimal.
    if (targetType.isDecimal()) {
        ScalarType decimalType = (ScalarType) targetType;
        int valLeftDigits = value_.precision() - value_.scale();
        int typeLeftDigits = decimalType.decimalPrecision() - decimalType.decimalScale();
        if (typeLeftDigits < valLeftDigits)
            return new CastExpr(targetType, this, true);
    }
    type_ = targetType;
    return this;
}
#method_after
@Override
protected Expr uncheckedCastTo(Type targetType) throws AnalysisException {
    Preconditions.checkState(targetType.isNumericType());
    // expected byte size sent to the BE in toThrift().
    if (targetType.isDecimal()) {
        ScalarType decimalType = (ScalarType) targetType;
        // analyze() ensures that value_ never exceeds the maximum scale and precision.
        Preconditions.checkState(isAnalyzed_);
        // Sanity check that our implicit casting does not allow a reduced precision or
        // truncating values from the right of the decimal point.
        Preconditions.checkState(value_.precision() <= decimalType.decimalPrecision());
        Preconditions.checkState(value_.scale() <= decimalType.decimalScale());
        int valLeftDigits = value_.precision() - value_.scale();
        int typeLeftDigits = decimalType.decimalPrecision() - decimalType.decimalScale();
        if (typeLeftDigits < valLeftDigits)
            return new CastExpr(targetType, this, true);
    }
    type_ = targetType;
    return this;
}
#end_block

#method_before
public String getExplainString(ArrayList<PlanFragment> fragments, TQueryExecRequest request, TExplainLevel explainLevel) {
    StringBuilder str = new StringBuilder();
    boolean hasHeader = false;
    if (request.isSetPer_host_mem_req() && request.isSetPer_host_vcores()) {
        str.append(String.format("Estimated Per-Host Requirements: Memory=%s VCores=%s\n", PrintUtils.printBytes(request.getPer_host_mem_req()), request.per_host_vcores));
        hasHeader = true;
    }
    // Append warning about tables missing stats.
    if (!request.query_ctx.isSetParent_query_id() && request.query_ctx.isSetTables_missing_stats() && !request.query_ctx.getTables_missing_stats().isEmpty()) {
        List<String> tableNames = Lists.newArrayList();
        for (TTableName tableName : request.query_ctx.getTables_missing_stats()) {
            tableNames.add(tableName.db_name + "." + tableName.table_name);
        }
        str.append("WARNING: The following tables are missing relevant table " + "and/or column statistics.\n" + Joiner.on(", ").join(tableNames) + "\n");
        hasHeader = true;
    }
    if (request.query_ctx.isDisable_spilling()) {
        str.append("WARNING: Spilling is disabled for this query as a safety guard.\n" + "Reason: Query option disable_unsafe_spills is set, at least one table\n" + "is missing relevant stats, and no plan hints were given.\n");
        hasHeader = true;
    }
    if (hasHeader)
        str.append("\n");
    if (explainLevel.ordinal() < TExplainLevel.VERBOSE.ordinal()) {
        // Print the non-fragmented parallel plan.
        str.append(fragments.get(0).getExplainString(explainLevel));
    } else {
        // Print the fragmented parallel plan.
        for (int i = 0; i < fragments.size(); ++i) {
            PlanFragment fragment = fragments.get(i);
            str.append(fragment.getExplainString(explainLevel));
            if (explainLevel == TExplainLevel.VERBOSE && i + 1 != fragments.size()) {
                str.append("\n");
            }
        }
    }
    return str.toString();
}
#method_after
public String getExplainString(ArrayList<PlanFragment> fragments, TQueryExecRequest request, TExplainLevel explainLevel) {
    StringBuilder str = new StringBuilder();
    boolean hasHeader = false;
    if (request.isSetPer_host_mem_req() && request.isSetPer_host_vcores()) {
        str.append(String.format("Estimated Per-Host Requirements: Memory=%s VCores=%s\n", PrintUtils.printBytes(request.getPer_host_mem_req()), request.per_host_vcores));
        hasHeader = true;
    }
    // 'compute stats'. The parent_query_id is only set for compute stats child queries.
    if (!request.query_ctx.isSetParent_query_id() && request.query_ctx.isSetTables_missing_stats() && !request.query_ctx.getTables_missing_stats().isEmpty()) {
        List<String> tableNames = Lists.newArrayList();
        for (TTableName tableName : request.query_ctx.getTables_missing_stats()) {
            tableNames.add(tableName.db_name + "." + tableName.table_name);
        }
        str.append("WARNING: The following tables are missing relevant table " + "and/or column statistics.\n" + Joiner.on(", ").join(tableNames) + "\n");
        hasHeader = true;
    }
    if (request.query_ctx.isDisable_spilling()) {
        str.append("WARNING: Spilling is disabled for this query as a safety guard.\n" + "Reason: Query option disable_unsafe_spills is set, at least one table\n" + "is missing relevant stats, and no plan hints were given.\n");
        hasHeader = true;
    }
    if (hasHeader)
        str.append("\n");
    if (explainLevel.ordinal() < TExplainLevel.VERBOSE.ordinal()) {
        // Print the non-fragmented parallel plan.
        str.append(fragments.get(0).getExplainString(explainLevel));
    } else {
        // Print the fragmented parallel plan.
        for (int i = 0; i < fragments.size(); ++i) {
            PlanFragment fragment = fragments.get(i);
            str.append(fragment.getExplainString(explainLevel));
            if (explainLevel == TExplainLevel.VERBOSE && i + 1 != fragments.size()) {
                str.append("\n");
            }
        }
    }
    return str.toString();
}
#end_block

#method_before
public PlanNode createSingleNodePlan() throws ImpalaException {
    QueryStmt queryStmt = ctx_.getQueryStmt();
    // Use the stmt's analyzer which is not necessarily the root analyzer, in particular,
    // to detect empty result sets.
    Analyzer analyzer = queryStmt.getAnalyzer();
    analyzer.computeEquivClasses();
    analyzer.getTimeline().markEvent("Equivalence classes computed");
    // TODO 2: should the materialization decision be cost-based?
    if (queryStmt.getBaseTblResultExprs() != null) {
        analyzer.materializeSlots(queryStmt.getBaseTblResultExprs());
    }
    LOG.trace("desctbl: " + analyzer.getDescTbl().debugString());
    PlanNode singleNodePlan = createQueryPlan(queryStmt, analyzer, ctx_.getQueryOptions().isDisable_outermost_topn());
    Preconditions.checkNotNull(singleNodePlan);
    return singleNodePlan;
}
#method_after
public PlanNode createSingleNodePlan() throws ImpalaException {
    QueryStmt queryStmt = ctx_.getQueryStmt();
    // Use the stmt's analyzer which is not necessarily the root analyzer
    // to detect empty result sets.
    Analyzer analyzer = queryStmt.getAnalyzer();
    analyzer.computeEquivClasses();
    analyzer.getTimeline().markEvent("Equivalence classes computed");
    // TODO 2: should the materialization decision be cost-based?
    if (queryStmt.getBaseTblResultExprs() != null) {
        analyzer.materializeSlots(queryStmt.getBaseTblResultExprs());
    }
    LOG.trace("desctbl: " + analyzer.getDescTbl().debugString());
    PlanNode singleNodePlan = createQueryPlan(queryStmt, analyzer, ctx_.getQueryOptions().isDisable_outermost_topn());
    Preconditions.checkNotNull(singleNodePlan);
    return singleNodePlan;
}
#end_block

#method_before
private boolean canEvalUsingPartitionMd(Expr expr, Analyzer analyzer) {
    Preconditions.checkNotNull(expr);
    if (expr instanceof BinaryPredicate) {
        BinaryPredicate bp = (BinaryPredicate) expr;
        SlotRef slot = bp.getBoundSlot();
        if (slot == null)
            return false;
        Expr bindingExpr = bp.getSlotBinding(slot.getSlotId());
        if (bindingExpr == null)
            return false;
        // PrimitiveType. If not, the expr needs to be evaluated in the BE.
        if (bindingExpr.isLiteral())
            return hasIdenticalType(slot, bindingExpr);
        if (!bindingExpr.isConstant())
            return false;
        // Evaluate constant expressions in the BE and modify the binary predicate
        try {
            LiteralExpr literalExpr = LiteralExpr.create(bindingExpr, analyzer.getQueryCtx());
            Preconditions.checkNotNull(literalExpr);
            // Ensure that the generated LiteralExpr and Slot have compatible types
            analyzer.getCompatibleType(slot.getType(), slot, literalExpr);
            ExprSubstitutionMap smap = new ExprSubstitutionMap();
            if (bindingExpr.isImplicitCast()) {
                smap.put(bindingExpr.getChild(0), literalExpr);
            } else {
                smap.put(bindingExpr, literalExpr);
            }
            for (int i = 0; i < expr.getChildren().size(); ++i) {
                expr.setChild(i, expr.getChild(i).substitute(smap, analyzer, false));
            }
            return true;
        } catch (AnalysisException e) {
            LOG.error("Error evaluating constant expression in BE: " + e.getMessage());
            return false;
        }
    } else if (expr instanceof CompoundPredicate) {
        boolean res = canEvalUsingPartitionMd(expr.getChild(0), analyzer);
        if (expr.getChild(1) != null) {
            res &= canEvalUsingPartitionMd(expr.getChild(1), analyzer);
        }
        return res;
    } else if (expr instanceof IsNullPredicate) {
        // Check for SlotRef IS [NOT] NULL case
        IsNullPredicate nullPredicate = (IsNullPredicate) expr;
        return nullPredicate.getBoundSlot() != null;
    } else if (expr instanceof InPredicate) {
        // Check for SlotRef [NOT] IN (Literal, ... Literal) case
        SlotRef slot = ((InPredicate) expr).getBoundSlot();
        if (slot == null)
            return false;
        for (int i = 1; i < expr.getChildren().size(); ++i) {
            Expr rhs = expr.getChild(i);
            if (!(rhs.isLiteral())) {
                if (!rhs.isConstant())
                    return false;
                // comparison values
                try {
                    LiteralExpr literalExpr = LiteralExpr.create(rhs, analyzer.getQueryCtx());
                    Preconditions.checkNotNull(literalExpr);
                    // Ensure that the generated LiteralExpr and Slot have compatible types
                    analyzer.getCompatibleType(slot.getType(), slot, literalExpr);
                    expr.setChild(i, literalExpr);
                } catch (AnalysisException e) {
                    LOG.error("Error evaluating constant expression in BE: " + e.getMessage());
                    return false;
                }
            } else {
                // PrimitiveType. If not, the expr needs to be evaluated in the BE.
                if (!hasIdenticalType(slot, rhs))
                    return false;
            }
        }
        return true;
    }
    return false;
}
#method_after
private boolean canEvalUsingPartitionMd(Expr expr, Analyzer analyzer) {
    Preconditions.checkNotNull(expr);
    if (expr instanceof BinaryPredicate) {
        // Evaluate any constant expression in the BE
        try {
            expr.foldConstantChildren(analyzer);
        } catch (AnalysisException e) {
            LOG.error("Error evaluating constant expressions in the BE: " + e.getMessage());
            return false;
        }
        BinaryPredicate bp = (BinaryPredicate) expr;
        SlotRef slot = bp.getBoundSlot();
        if (slot == null)
            return false;
        Expr bindingExpr = bp.getSlotBinding(slot.getSlotId());
        if (bindingExpr == null || !bindingExpr.isLiteral())
            return false;
        return true;
    } else if (expr instanceof CompoundPredicate) {
        boolean res = canEvalUsingPartitionMd(expr.getChild(0), analyzer);
        if (expr.getChild(1) != null) {
            res &= canEvalUsingPartitionMd(expr.getChild(1), analyzer);
        }
        return res;
    } else if (expr instanceof IsNullPredicate) {
        // Check for SlotRef IS [NOT] NULL case
        IsNullPredicate nullPredicate = (IsNullPredicate) expr;
        return nullPredicate.getBoundSlot() != null;
    } else if (expr instanceof InPredicate) {
        // Evaluate any constant expressions in the BE
        try {
            expr.foldConstantChildren(analyzer);
        } catch (AnalysisException e) {
            LOG.error("Error evaluating constant expressions in the BE: " + e.getMessage());
            return false;
        }
        // Check for SlotRef [NOT] IN (Literal, ... Literal) case
        SlotRef slot = ((InPredicate) expr).getBoundSlot();
        if (slot == null)
            return false;
        for (int i = 1; i < expr.getChildren().size(); ++i) {
            if (!(expr.getChild(i).isLiteral()))
                return false;
        }
        return true;
    }
    return false;
}
#end_block

#method_before
private void prunePartitions(Analyzer analyzer) throws InternalException {
    DescriptorTable descTbl = analyzer.getDescTbl();
    // loop through all partitions and prune based on applicable conjuncts;
    // start with creating a collection of partition filters for the applicable conjuncts
    List<SlotId> partitionSlots = Lists.newArrayList();
    for (SlotDescriptor slotDesc : descTbl.getTupleDesc(tupleIds_.get(0)).getSlots()) {
        Preconditions.checkState(slotDesc.getColumn() != null);
        if (slotDesc.getColumn().getPosition() < tbl_.getNumClusteringCols()) {
            partitionSlots.add(slotDesc.getId());
        }
    }
    List<HdfsPartitionFilter> partitionFilters = Lists.newArrayList();
    // Conjuncts that can be evaluated from the partition key values.
    List<Expr> simpleFilterConjuncts = Lists.newArrayList();
    // Simple predicates (e.g. binary predicates of the form
    // <SlotRef> <op> <LiteralExpr>) can be used to derive lists
    // of matching partition ids directly from the partition key values.
    // Split conjuncts among those that can be evaluated from partition
    // key values and those that need to be evaluated in the BE.
    Iterator<Expr> it = conjuncts_.iterator();
    while (it.hasNext()) {
        Expr conjunct = it.next();
        if (conjunct.isBoundBySlotIds(partitionSlots)) {
            if (canEvalUsingPartitionMd(conjunct, analyzer)) {
                simpleFilterConjuncts.add(Expr.pushNegationToOperands(conjunct));
            } else {
                partitionFilters.add(new HdfsPartitionFilter(conjunct, tbl_, analyzer));
            }
            it.remove();
        }
    }
    // Set of matching partition ids, i.e. partitions that pass all filters
    HashSet<Long> matchingPartitionIds = null;
    // The result is the intersection of the associated partition id sets.
    for (Expr filter : simpleFilterConjuncts) {
        // Evaluate the filter
        HashSet<Long> matchingIds = evalSlotBindingFilter(filter);
        if (matchingPartitionIds == null) {
            matchingPartitionIds = matchingIds;
        } else {
            matchingPartitionIds.retainAll(matchingIds);
        }
    }
    // Check if we need to initialize the set of valid partition ids.
    if (simpleFilterConjuncts.size() == 0) {
        Preconditions.checkState(matchingPartitionIds == null);
        matchingPartitionIds = Sets.newHashSet(tbl_.getPartitionIds());
    }
    // Evaluate the 'complex' partition filters in the BE.
    evalPartitionFiltersInBe(partitionFilters, matchingPartitionIds, analyzer);
    // Populate the list of valid, non-empty partitions to process
    HashMap<Long, HdfsPartition> partitionMap = tbl_.getPartitionMap();
    for (Long id : matchingPartitionIds) {
        HdfsPartition partition = partitionMap.get(id);
        Preconditions.checkNotNull(partition);
        if (partition.hasFileDescriptors()) {
            partitions_.add(partition);
            descTbl.addReferencedPartition(tbl_, partition.getId());
        }
    }
}
#method_after
private void prunePartitions(Analyzer analyzer) throws InternalException {
    DescriptorTable descTbl = analyzer.getDescTbl();
    // loop through all partitions and prune based on applicable conjuncts;
    // start with creating a collection of partition filters for the applicable conjuncts
    List<SlotId> partitionSlots = Lists.newArrayList();
    for (SlotDescriptor slotDesc : descTbl.getTupleDesc(tupleIds_.get(0)).getSlots()) {
        Preconditions.checkState(slotDesc.getColumn() != null);
        if (slotDesc.getColumn().getPosition() < tbl_.getNumClusteringCols()) {
            partitionSlots.add(slotDesc.getId());
        }
    }
    List<HdfsPartitionFilter> partitionFilters = Lists.newArrayList();
    // Conjuncts that can be evaluated from the partition key values.
    List<Expr> simpleFilterConjuncts = Lists.newArrayList();
    // Simple predicates (e.g. binary predicates of the form
    // <SlotRef> <op> <LiteralExpr>) can be used to derive lists
    // of matching partition ids directly from the partition key values.
    // Split conjuncts among those that can be evaluated from partition
    // key values and those that need to be evaluated in the BE.
    Iterator<Expr> it = conjuncts_.iterator();
    while (it.hasNext()) {
        Expr conjunct = it.next();
        if (conjunct.isBoundBySlotIds(partitionSlots)) {
            // Check if the conjunct can be evaluated from the partition metadata.
            // canEvalUsingPartitionMd() operates on a cloned conjunct which may get
            // modified if it contains constant expressions. If the cloned conjunct
            // cannot be evaluated from the partition metadata, the original unmodified
            // conjuct is evaluated in the BE.
            Expr clonedConjunct = conjunct.clone();
            if (canEvalUsingPartitionMd(clonedConjunct, analyzer)) {
                simpleFilterConjuncts.add(Expr.pushNegationToOperands(clonedConjunct));
            } else {
                partitionFilters.add(new HdfsPartitionFilter(conjunct, tbl_, analyzer));
            }
            it.remove();
        }
    }
    // Set of matching partition ids, i.e. partitions that pass all filters
    HashSet<Long> matchingPartitionIds = null;
    // The result is the intersection of the associated partition id sets.
    for (Expr filter : simpleFilterConjuncts) {
        // Evaluate the filter
        HashSet<Long> matchingIds = evalSlotBindingFilter(filter);
        if (matchingPartitionIds == null) {
            matchingPartitionIds = matchingIds;
        } else {
            matchingPartitionIds.retainAll(matchingIds);
        }
    }
    // Check if we need to initialize the set of valid partition ids.
    if (simpleFilterConjuncts.size() == 0) {
        Preconditions.checkState(matchingPartitionIds == null);
        matchingPartitionIds = Sets.newHashSet(tbl_.getPartitionIds());
    }
    // Evaluate the 'complex' partition filters in the BE.
    evalPartitionFiltersInBe(partitionFilters, matchingPartitionIds, analyzer);
    // Populate the list of valid, non-empty partitions to process
    HashMap<Long, HdfsPartition> partitionMap = tbl_.getPartitionMap();
    for (Long id : matchingPartitionIds) {
        HdfsPartition partition = partitionMap.get(id);
        Preconditions.checkNotNull(partition);
        if (partition.hasFileDescriptors()) {
            partitions_.add(partition);
            descTbl.addReferencedPartition(tbl_, partition.getId());
        }
    }
}
#end_block

#method_before
public TLoadDataResp loadTableData(TLoadDataReq request) throws ImpalaException, IOException {
    TableName tableName = TableName.fromThrift(request.getTable_name());
    // Get the destination for the load. If the load is targeting a partition,
    // this the partition location. Otherwise this is the table location.
    String destPathString = null;
    if (request.isSetPartition_spec()) {
        destPathString = impaladCatalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), request.getPartition_spec()).getLocation();
    } else {
        destPathString = impaladCatalog_.getTable(tableName.getDb(), tableName.getTbl()).getMetaStoreTable().getSd().getLocation();
    }
    Path destPath = new Path(destPathString);
    DistributedFileSystem dfs = FileSystemUtil.getDistributedFileSystem(destPath);
    // Create a temporary directory within the final destination directory to stage the
    // file move.
    Path tmpDestPath = FileSystemUtil.makeTmpSubdirectory(destPath);
    Path sourcePath = new Path(request.source_path);
    int filesLoaded = 0;
    if (dfs.isDirectory(sourcePath)) {
        filesLoaded = FileSystemUtil.moveAllVisibleFiles(sourcePath, tmpDestPath);
    } else {
        FileSystemUtil.moveFile(sourcePath, tmpDestPath, true);
        filesLoaded = 1;
    }
    // If this is an OVERWRITE, delete all files in the destination.
    if (request.isOverwrite()) {
        FileSystemUtil.deleteAllVisibleFiles(destPath);
    }
    // Move the files from the temporary location to the final destination.
    FileSystemUtil.moveAllVisibleFiles(tmpDestPath, destPath);
    // Cleanup the tmp directory.
    dfs.delete(tmpDestPath, true);
    TLoadDataResp response = new TLoadDataResp();
    TColumnValue col = new TColumnValue();
    String loadMsg = String.format("Loaded %d file(s). Total files in destination location: %d", filesLoaded, FileSystemUtil.getTotalNumVisibleFiles(destPath));
    col.setString_val(loadMsg);
    response.setLoad_summary(new TResultRow(Lists.newArrayList(col)));
    return response;
}
#method_after
public TLoadDataResp loadTableData(TLoadDataReq request) throws ImpalaException, IOException {
    TableName tableName = TableName.fromThrift(request.getTable_name());
    // Get the destination for the load. If the load is targeting a partition,
    // this the partition location. Otherwise this is the table location.
    String destPathString = null;
    if (request.isSetPartition_spec()) {
        destPathString = impaladCatalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), request.getPartition_spec()).getLocation();
    } else {
        destPathString = impaladCatalog_.getTable(tableName.getDb(), tableName.getTbl()).getMetaStoreTable().getSd().getLocation();
    }
    Path destPath = new Path(destPathString);
    FileSystem fs = destPath.getFileSystem(FileSystemUtil.getConfiguration());
    // Create a temporary directory within the final destination directory to stage the
    // file move.
    Path tmpDestPath = FileSystemUtil.makeTmpSubdirectory(destPath);
    Path sourcePath = new Path(request.source_path);
    int filesLoaded = 0;
    if (fs.isDirectory(sourcePath)) {
        filesLoaded = FileSystemUtil.relocateAllVisibleFiles(sourcePath, tmpDestPath);
    } else {
        FileSystemUtil.relocateFile(sourcePath, tmpDestPath, true);
        filesLoaded = 1;
    }
    // If this is an OVERWRITE, delete all files in the destination.
    if (request.isOverwrite()) {
        FileSystemUtil.deleteAllVisibleFiles(destPath);
    }
    // Move the files from the temporary location to the final destination.
    FileSystemUtil.relocateAllVisibleFiles(tmpDestPath, destPath);
    // Cleanup the tmp directory.
    fs.delete(tmpDestPath, true);
    TLoadDataResp response = new TLoadDataResp();
    TColumnValue col = new TColumnValue();
    String loadMsg = String.format("Loaded %d file(s). Total files in destination location: %d", filesLoaded, FileSystemUtil.getTotalNumVisibleFiles(destPath));
    col.setString_val(loadMsg);
    response.setLoad_summary(new TResultRow(Lists.newArrayList(col)));
    return response;
}
#end_block

#method_before
private boolean requestTblLoadAndWait(Set<TableName> requestedTbls, long timeoutMs) throws InternalException {
    Set<TableName> missingTbls = getMissingTbls(requestedTbls);
    // There are no missing tables, return and avoid making an RPC to the CatalogServer.
    if (missingTbls.isEmpty())
        return true;
    // Call into the CatalogServer and request the required tables be loaded.
    LOG.info(String.format("Requesting prioritized load of table(s): %s", Joiner.on(", ").join(missingTbls)));
    TStatus status = FeSupport.PrioritizeLoad(missingTbls);
    if (status.getStatus_code() != TStatusCode.OK) {
        throw new InternalException("Error requesting prioritized load: " + Joiner.on("\n").join(status.getError_msgs()));
    }
    long startTimeMs = System.currentTimeMillis();
    // Wait until all the required tables are loaded in the Impalad's catalog cache.
    while (!missingTbls.isEmpty()) {
        // Check if the timeout has been reached.
        if (timeoutMs > 0 && System.currentTimeMillis() - startTimeMs > timeoutMs) {
            return false;
        }
        LOG.trace(String.format("Waiting for table(s) to complete loading: %s", Joiner.on(", ").join(missingTbls)));
        getCatalog().waitForCatalogUpdate(MAX_CATALOG_UPDATE_WAIT_TIME_MS);
        missingTbls = getMissingTbls(missingTbls);
    // TODO: Check for query cancellation here.
    }
    return true;
}
#method_after
private boolean requestTblLoadAndWait(Set<TableName> requestedTbls, long timeoutMs) throws InternalException {
    Set<TableName> missingTbls = getMissingTbls(requestedTbls);
    // There are no missing tables, return and avoid making an RPC to the CatalogServer.
    if (missingTbls.isEmpty())
        return true;
    // Call into the CatalogServer and request the required tables be loaded.
    LOG.info(String.format("Requesting prioritized load of table(s): %s", Joiner.on(", ").join(missingTbls)));
    TStatus status = FeSupport.PrioritizeLoad(missingTbls);
    if (status.getStatus_code() != TErrorCode.OK) {
        throw new InternalException("Error requesting prioritized load: " + Joiner.on("\n").join(status.getError_msgs()));
    }
    long startTimeMs = System.currentTimeMillis();
    // Wait until all the required tables are loaded in the Impalad's catalog cache.
    while (!missingTbls.isEmpty()) {
        // Check if the timeout has been reached.
        if (timeoutMs > 0 && System.currentTimeMillis() - startTimeMs > timeoutMs) {
            return false;
        }
        LOG.trace(String.format("Waiting for table(s) to complete loading: %s", Joiner.on(", ").join(missingTbls)));
        getCatalog().waitForCatalogUpdate(MAX_CATALOG_UPDATE_WAIT_TIME_MS);
        missingTbls = getMissingTbls(missingTbls);
    // TODO: Check for query cancellation here.
    }
    return true;
}
#end_block

#method_before
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int fragmentId = 0; fragmentId < fragments.size(); ++fragmentId) {
        PlanFragment fragment = fragments.get(fragmentId);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, fragmentId);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use VERBOSE by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.VERBOSE;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else {
        Preconditions.checkState(analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt());
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    }
    return result;
}
#method_after
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int fragmentId = 0; fragmentId < fragments.size(); ++fragmentId) {
        PlanFragment fragment = fragments.get(fragmentId);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, fragmentId);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use VERBOSE by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.VERBOSE;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    String jsonLineageGraph = analysisResult.getJsonLineageGraph();
    if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
        queryExecRequest.setLineage_graph(jsonLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else {
        Preconditions.checkState(analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt());
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    }
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#end_block

#method_before
public TResultSet getTableFiles(TShowFilesParams request) throws ImpalaException {
    Table table = impaladCatalog_.getTable(request.getTable_name().getDb_name(), request.getTable_name().getTable_name());
    return table.getAllFiles(request.getPartition_spec());
}
#method_after
public TResultSet getTableFiles(TShowFilesParams request) throws ImpalaException {
    Table table = impaladCatalog_.getTable(request.getTable_name().getDb_name(), request.getTable_name().getTable_name());
    if (table instanceof HdfsTable) {
        return ((HdfsTable) table).getFiles(request.getPartition_spec());
    } else {
        throw new InternalException("SHOW FILES only supports Hdfs table. " + "Unsupported table class: " + table.getClass());
    }
}
#end_block

#method_before
private String checkFileSystem(Configuration conf) {
    try {
        FileSystem fs = FileSystem.get(CONF);
        if (!(fs instanceof DistributedFileSystem)) {
            return "Unsupported filesystem. Impala only supports DistributedFileSystem " + "but the configured filesystem is: " + fs.getClass().getSimpleName() + "." + CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY + "(" + CONF.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ")" + " might be set incorrectly";
        }
    } catch (IOException e) {
        return "couldn't retrieve FileSystem:\n" + e.getMessage();
    }
    try {
        FileSystemUtil.getTotalNumVisibleFiles(new Path("/"));
    } catch (IOException e) {
        return "Could not read the HDFS root directory at " + CONF.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ". Error was: \n" + e.getMessage();
    }
    return "";
}
#method_after
private String checkFileSystem(Configuration conf) {
    try {
        FileSystem fs = FileSystem.get(CONF);
        if (!(fs instanceof DistributedFileSystem)) {
            return "Unsupported default filesystem. The default filesystem must be " + "a DistributedFileSystem but the configured default filesystem is " + fs.getClass().getSimpleName() + ". " + CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY + " (" + CONF.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ")" + " might be set incorrectly.";
        }
    } catch (IOException e) {
        return "couldn't retrieve FileSystem:\n" + e.getMessage();
    }
    try {
        FileSystemUtil.getTotalNumVisibleFiles(new Path("/"));
    } catch (IOException e) {
        return "Could not read the HDFS root directory at " + CONF.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ". Error was: \n" + e.getMessage();
    }
    return "";
}
#end_block

#method_before
private void loadDiskIds(DistributedFileSystem dfs, List<BlockLocation> blockLocations, Map<String, List<FileDescriptor>> fileDescriptors) {
    // BlockStorageLocations for all the blocks
    // block described by blockMetadataList[i] is located at locations[i]
    BlockStorageLocation[] locations = null;
    try {
        // Get the BlockStorageLocations for all the blocks
        locations = dfs.getFileBlockStorageLocations(blockLocations);
    } catch (IOException e) {
        LOG.error("Couldn't determine block storage locations:\n" + e.getMessage());
        return;
    }
    if (locations == null || locations.length == 0) {
        LOG.warn("Attempted to get block locations but the call returned nulls");
        return;
    }
    if (locations.length != blockLocations.size()) {
        // blocks and locations don't match up
        LOG.error("Number of block locations not equal to number of blocks: " + "#locations=" + Long.toString(locations.length) + " #blocks=" + Long.toString(blockLocations.size()));
        return;
    }
    int locationsIdx = 0;
    int unknownDiskIdCount = 0;
    for (String parentPath : fileDescriptors.keySet()) {
        for (FileDescriptor fileDescriptor : fileDescriptors.get(parentPath)) {
            for (THdfsFileBlock blockMd : fileDescriptor.getFileBlocks()) {
                VolumeId[] volumeIds = locations[locationsIdx++].getVolumeIds();
                // Convert opaque VolumeId to 0 based ids.
                // TODO: the diskId should be eventually retrievable from Hdfs when
                // the community agrees this API is useful.
                int[] diskIds = new int[volumeIds.length];
                for (int i = 0; i < volumeIds.length; ++i) {
                    diskIds[i] = getDiskId(volumeIds[i]);
                    if (diskIds[i] < 0)
                        ++unknownDiskIdCount;
                }
                FileBlock.setDiskIds(diskIds, blockMd);
            }
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("unknown disk id count " + unknownDiskIdCount);
        }
    }
}
#method_after
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    if (!SUPPORTS_VOLUME_ID)
        return;
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        // part of the FileSystem interface, so we'll need to downcast.
        if (!(fs instanceof DistributedFileSystem))
            continue;
        LOG.trace("Loading disk ids for: " + getFullName() + ". nodes: " + getNumNodes() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        BlockStorageLocation[] storageLocs = null;
        try {
            // Get the BlockStorageLocations for all the blocks
            storageLocs = dfs.getFileBlockStorageLocations(blockLists.locations);
        } catch (IOException e) {
            LOG.error("Couldn't determine block storage locations for filesystem " + fs + ":\n" + e.getMessage());
            continue;
        }
        if (storageLocs == null || storageLocs.length == 0) {
            LOG.warn("Attempted to get block locations for filesystem " + fs + " but the call returned no results");
            continue;
        }
        if (storageLocs.length != blockLists.locations.size()) {
            // Block locations and storage locations didn't match up.
            LOG.error("Number of block storage locations not equal to number of blocks: " + "#storage locations=" + Long.toString(storageLocs.length) + " #blocks=" + Long.toString(blockLists.locations.size()));
            continue;
        }
        long unknownDiskIdCount = 0;
        // THdfsFileBlocks.
        for (int locIdx = 0; locIdx < storageLocs.length; ++locIdx) {
            VolumeId[] volumeIds = storageLocs[locIdx].getVolumeIds();
            THdfsFileBlock block = blockLists.blocks.get(locIdx);
            // Convert opaque VolumeId to 0 based ids.
            // TODO: the diskId should be eventually retrievable from Hdfs when the
            // community agrees this API is useful.
            int[] diskIds = new int[volumeIds.length];
            for (int i = 0; i < volumeIds.length; ++i) {
                diskIds[i] = getDiskId(volumeIds[i]);
                if (diskIds[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIds, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#end_block

#method_before
private void loadPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl, Map<String, List<FileDescriptor>> oldFileDescMap) throws IOException, CatalogException {
    resetPartitionMd();
    partitions_.clear();
    hdfsBaseDir_ = msTbl.getSd().getLocation();
    // Map of filesystem to parent path to a list of new/modified
    // FileDescriptors. FileDescriptors in this Map will have their block location
    // information (re)loaded. This is used to speed up the incremental refresh of a
    // table's metadata by skipping unmodified, previously loaded FileDescriptors.
    Map<FsKey, Map<String, List<FileDescriptor>>> fileDescsToLoad = Maps.newHashMap();
    // INSERT statements need to refer to this if they try to write to new partitions
    // Scans don't refer to this because by definition all partitions they refer to
    // exist.
    addDefaultPartition(msTbl.getSd());
    Long cacheDirectiveId = HdfsCachingUtil.getCacheDirIdFromParams(msTbl.getParameters());
    isMarkedCached_ = cacheDirectiveId != null;
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null, oldFileDescMap, fileDescsToLoad);
        addPartition(part);
        if (isMarkedCached_)
            part.markCached();
        Path location = new Path(hdfsBaseDir_);
        FileSystem fs = location.getFileSystem(CONF);
        if (fs.exists(location)) {
            accessLevel_ = getAvailableAccessLevel(fs, location);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, oldFileDescMap, fileDescsToLoad);
            addPartition(partition);
            // this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null)
                ;
            {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
        }
    }
    loadBlockMd(fileDescsToLoad);
}
#method_after
private void loadPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl, Map<String, List<FileDescriptor>> oldFileDescMap) throws IOException, CatalogException {
    resetPartitionMd();
    partitions_.clear();
    hdfsBaseDir_ = msTbl.getSd().getLocation();
    // Map of filesystem to the file blocks for new/modified FileDescriptors. Blocks in
    // this map will have their disk volume IDs information (re)loaded. This is used to
    // speed up the incremental refresh of a table's metadata by skipping unmodified,
    // previously loaded blocks.
    Map<FsKey, FileBlocksInfo> blocksToLoad = Maps.newHashMap();
    // INSERT statements need to refer to this if they try to write to new partitions
    // Scans don't refer to this because by definition all partitions they refer to
    // exist.
    addDefaultPartition(msTbl.getSd());
    // We silently ignore cache directives that no longer exist in HDFS, and remove
    // non-existing cache directives from the parameters.
    isMarkedCached_ = HdfsCachingUtil.validateCacheParams(msTbl.getParameters());
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null, oldFileDescMap, blocksToLoad);
        addPartition(part);
        if (isMarkedCached_)
            part.markCached();
        Path location = new Path(hdfsBaseDir_);
        FileSystem fs = location.getFileSystem(CONF);
        if (fs.exists(location)) {
            accessLevel_ = getAvailableAccessLevel(fs, location);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, oldFileDescMap, blocksToLoad);
            addPartition(partition);
            // this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null)
                ;
            {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
        }
    }
    loadDiskIds(blocksToLoad);
}
#end_block

#method_before
public HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws CatalogException {
    Map<FsKey, Map<String, List<FileDescriptor>>> fileDescsToLoad = Maps.newHashMap();
    HdfsPartition hdfsPartition = createPartition(storageDescriptor, msPartition, fileDescMap_, fileDescsToLoad);
    loadBlockMd(fileDescsToLoad);
    return hdfsPartition;
}
#method_after
public HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws CatalogException {
    Map<FsKey, FileBlocksInfo> blocksToLoad = Maps.newHashMap();
    HdfsPartition hdfsPartition = createPartition(storageDescriptor, msPartition, fileDescMap_, blocksToLoad);
    loadDiskIds(blocksToLoad);
    return hdfsPartition;
}
#end_block

#method_before
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition, Map<String, List<FileDescriptor>> oldFileDescMap, Map<FsKey, Map<String, List<FileDescriptor>>> perFsFileDescMap) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    Path partDirPath = new Path(storageDescriptor.getLocation());
    List<FileDescriptor> fileDescriptors = Lists.newArrayList();
    // If the partition is marked as cached, the block location metadata must be
    // reloaded, even if the file times have not changed.
    boolean isMarkedCached = isMarkedCached_;
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        isMarkedCached = HdfsCachingUtil.getCacheDirIdFromParams(msPartition.getParameters()) != null;
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        try {
            Expr.analyze(keyValues, null);
        } catch (AnalysisException e) {
            // should never happen
            throw new IllegalStateException(e);
        }
    }
    try {
        // Each partition could reside on a different filesystem.
        FileSystem fs = partDirPath.getFileSystem(CONF);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (fs.exists(partDirPath)) {
            // fs.listStatus() to list all the files.
            for (FileStatus fileStatus : fs.listStatus(partDirPath)) {
                String fileName = fileStatus.getPath().getName().toString();
                if (fileStatus.isDirectory() || FileSystemUtil.isHiddenFile(fileName) || HdfsCompression.fromFileName(fileName) == HdfsCompression.LZO_INDEX) {
                    // Skip index files, these are read by the LZO scanner directly.
                    continue;
                }
                String partitionDir = fileStatus.getPath().getParent().toString();
                FileDescriptor fd = null;
                // is found, it will be chosen as a candidate to reuse.
                if (oldFileDescMap != null && oldFileDescMap.get(partitionDir) != null) {
                    for (FileDescriptor oldFileDesc : oldFileDescMap.get(partitionDir)) {
                        if (oldFileDesc.getFileName().equals(fileName)) {
                            fd = oldFileDesc;
                            break;
                        }
                    }
                }
                // value can be reused.
                if (fd == null || isMarkedCached || fd.getFileLength() != fileStatus.getLen() || fd.getModificationTime() != fileStatus.getModificationTime()) {
                    // Create a new file descriptor, the block metadata will be populated by
                    // loadBlockMd.
                    fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
                    addPerFsFileDesc(perFsFileDescMap, fs, partitionDir, fd);
                }
                List<FileDescriptor> fds = fileDescMap_.get(partitionDir);
                if (fds == null) {
                    fds = Lists.newArrayList();
                    fileDescMap_.put(partitionDir, fds);
                }
                fds.add(fd);
                // Add to the list of FileDescriptors for this partition.
                fileDescriptors.add(fd);
            }
            numHdfsFiles_ += fileDescriptors.size();
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, fileDescriptors, getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (Exception e) {
        throw new CatalogException("Failed to create partition: ", e);
    }
}
#method_after
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition, Map<String, List<FileDescriptor>> oldFileDescMap, Map<FsKey, FileBlocksInfo> perFsFileBlocks) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    Path partDirPath = new Path(storageDescriptor.getLocation());
    List<FileDescriptor> fileDescriptors = Lists.newArrayList();
    // If the partition is marked as cached, the block location metadata must be
    // reloaded, even if the file times have not changed.
    boolean isMarkedCached = isMarkedCached_;
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        isMarkedCached = HdfsCachingUtil.validateCacheParams(msPartition.getParameters());
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        try {
            Expr.analyze(keyValues, null);
        } catch (AnalysisException e) {
            // should never happen
            throw new IllegalStateException(e);
        }
    }
    try {
        // Each partition could reside on a different filesystem.
        FileSystem fs = partDirPath.getFileSystem(CONF);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (fs.exists(partDirPath)) {
            // fs.listStatus() to list all the files.
            for (FileStatus fileStatus : fs.listStatus(partDirPath)) {
                String fileName = fileStatus.getPath().getName().toString();
                if (fileStatus.isDirectory() || FileSystemUtil.isHiddenFile(fileName) || HdfsCompression.fromFileName(fileName) == HdfsCompression.LZO_INDEX) {
                    // Skip index files, these are read by the LZO scanner directly.
                    continue;
                }
                String partitionDir = fileStatus.getPath().getParent().toString();
                FileDescriptor fd = null;
                // is found, it will be chosen as a candidate to reuse.
                if (oldFileDescMap != null && oldFileDescMap.get(partitionDir) != null) {
                    for (FileDescriptor oldFileDesc : oldFileDescMap.get(partitionDir)) {
                        if (oldFileDesc.getFileName().equals(fileName)) {
                            fd = oldFileDesc;
                            break;
                        }
                    }
                }
                // value can be reused.
                if (fd == null || isMarkedCached || fd.getFileLength() != fileStatus.getLen() || fd.getModificationTime() != fileStatus.getModificationTime()) {
                    // Create a new file descriptor and load the file block metadata,
                    // collecting the block metadata into perFsFileBlocks.  The disk IDs for
                    // all the blocks of each filesystem will be loaded by loadDiskIds().
                    fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
                    loadBlockMetadata(fs, fileStatus, fd, fileFormatDescriptor.getFileFormat(), perFsFileBlocks);
                }
                List<FileDescriptor> fds = fileDescMap_.get(partitionDir);
                if (fds == null) {
                    fds = Lists.newArrayList();
                    fileDescMap_.put(partitionDir, fds);
                }
                fds.add(fd);
                // Add to the list of FileDescriptors for this partition.
                fileDescriptors.add(fd);
            }
            numHdfsFiles_ += fileDescriptors.size();
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, fileDescriptors, getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (Exception e) {
        throw new CatalogException("Failed to create partition: ", e);
    }
}
#end_block

#method_before
@Override
public /**
 * Load the table metadata and reuse metadata to speed up metadata loading.
 * If the lastDdlTime has not been changed, that means the Hive metastore metadata has
 * not been changed. Reuses the old Hive partition metadata from cachedEntry.
 * To speed up Hdfs metadata loading, if a file's mtime has not been changed, reuses
 * the old file block metadata from old value.
 *
 * There are several cases where the cachedEntry might be reused incorrectly:
 * 1. an ALTER TABLE ADD PARTITION or dynamic partition insert is executed through
 *    Hive. This does not update the lastDdlTime.
 * 2. Hdfs rebalancer is executed. This changes the block locations but won't update
 *    the mtime (file modification time).
 * If any of these occurs, user has to execute "invalidate metadata" to invalidate the
 * metadata cache of the table to trigger a fresh load.
 */
void load(Table cachedEntry, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    LOG.debug("load table: " + db_.getName() + "." + name_);
    // turn all exceptions into TableLoadingException
    try {
        // set nullPartitionKeyValue from the hive conf.
        nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
        // set NULL indicator string from table properties
        nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
        if (nullColumnValue_ == null)
            nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
        // populate with both partition keys and regular columns
        List<FieldSchema> partKeys = msTbl.getPartitionKeys();
        List<FieldSchema> tblFields = Lists.newArrayList();
        String inputFormat = msTbl.getSd().getInputFormat();
        if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO) {
            // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
            // taking precedence.
            List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
            schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
            schemaSearchLocations.add(getMetaStoreTable().getParameters());
            avroSchema_ = HdfsTable.getAvroSchema(schemaSearchLocations, getFullName(), true);
            String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
            if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
                // If the SerDe library is null or set to LazySimpleSerDe or is null, it
                // indicates there is an issue with the table metadata since Avro table need a
                // non-native serde. Instead of failing to load the table, fall back to
                // using the fields from the storage descriptor (same as Hive).
                tblFields.addAll(msTbl.getSd().getCols());
            } else {
                // Load the fields from the Avro schema.
                // Since Avro does not include meta-data for CHAR or VARCHAR, an Avro type of
                // "string" is used for CHAR, VARCHAR and STRING. Default back to the storage
                // descriptor to determine the the type for "string"
                List<FieldSchema> sdTypes = msTbl.getSd().getCols();
                int i = 0;
                List<Column> avroTypeList = AvroSchemaParser.parse(avroSchema_);
                boolean canFallBack = sdTypes.size() == avroTypeList.size();
                for (Column parsedCol : avroTypeList) {
                    FieldSchema fs = new FieldSchema();
                    fs.setName(parsedCol.getName());
                    String avroType = parsedCol.getType().toSql();
                    if (avroType.toLowerCase().equals("string") && canFallBack) {
                        fs.setType(sdTypes.get(i).getType());
                    } else {
                        fs.setType(avroType);
                    }
                    fs.setComment("from deserializer");
                    tblFields.add(fs);
                    i++;
                }
            }
        } else {
            tblFields.addAll(msTbl.getSd().getCols());
        }
        List<FieldSchema> fieldSchemas = new ArrayList<FieldSchema>(partKeys.size() + tblFields.size());
        fieldSchemas.addAll(partKeys);
        fieldSchemas.addAll(tblFields);
        // The number of clustering columns is the number of partition keys.
        numClusteringCols_ = partKeys.size();
        loadColumns(fieldSchemas, client);
        // Collect the list of partitions to use for the table. Partitions may be reused
        // from the existing cached table entry (if one exists), read from the metastore,
        // or a mix of both. Whether or not a partition is reused depends on whether
        // the table or partition has been modified.
        List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
        if (cachedEntry == null || !(cachedEntry instanceof HdfsTable) || cachedEntry.lastDdlTime_ != lastDdlTime_) {
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
        } else {
            // The table was already in the metadata cache and it has not been modified.
            Preconditions.checkArgument(cachedEntry instanceof HdfsTable);
            HdfsTable cachedHdfsTableEntry = (HdfsTable) cachedEntry;
            // Set of partition names that have been modified. Partitions in this Set need to
            // be reloaded from the metastore.
            Set<String> modifiedPartitionNames = Sets.newHashSet();
            // "temp" table that doesn't actually exist in the metastore.
            if (cachedEntry != this) {
                // Since the table has not been modified, we might be able to reuse some of the
                // old partition metadata if the individual partitions have not been modified.
                // First get a list of all the partition names for this table from the
                // metastore, this is much faster than listing all the Partition objects.
                modifiedPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
            }
            int totalPartitions = modifiedPartitionNames.size();
            // Get all the partitions from the cached entry that have not been modified.
            for (HdfsPartition cachedPart : cachedHdfsTableEntry.getPartitions()) {
                // Skip the default partition and any partitions that have been modified.
                if (cachedPart.isDirty() || cachedPart.getMetaStorePartition() == null || cachedPart.getId() == DEFAULT_PARTITION_ID) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition cachedMsPart = cachedPart.getMetaStorePartition();
                Preconditions.checkNotNull(cachedMsPart);
                // This is a partition we already know about and it hasn't been modified.
                // No need to reload the metadata.
                String cachedPartName = cachedPart.getPartitionName();
                if (modifiedPartitionNames.contains(cachedPartName)) {
                    msPartitions.add(cachedMsPart);
                    modifiedPartitionNames.remove(cachedPartName);
                }
            }
            LOG.info(String.format("Incrementally refreshing %d/%d partitions.", modifiedPartitionNames.size(), totalPartitions));
            // No need to make the metastore call if no partitions are to be updated.
            if (modifiedPartitionNames.size() > 0) {
                // Now reload the the remaining partitions.
                msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(modifiedPartitionNames), db_.getName(), name_));
            }
        }
        Map<String, List<FileDescriptor>> oldFileDescMap = null;
        if (cachedEntry != null && cachedEntry instanceof HdfsTable) {
            HdfsTable cachedHdfsTable = (HdfsTable) cachedEntry;
            oldFileDescMap = cachedHdfsTable.fileDescMap_;
            hostIndex_.populate(cachedHdfsTable.hostIndex_.getList());
        }
        loadPartitions(msPartitions, msTbl, oldFileDescMap);
        // load table stats
        numRows_ = getRowCount(msTbl.getParameters());
        LOG.debug("table #rows=" + Long.toString(numRows_));
        // to the table's numRows.
        if (numClusteringCols_ == 0 && !partitions_.isEmpty()) {
            // Unpartitioned tables have a 'dummy' partition and a default partition.
            // Temp tables used in CTAS statements have one partition.
            Preconditions.checkState(partitions_.size() == 2 || partitions_.size() == 1);
            for (HdfsPartition p : partitions_) {
                p.setNumRows(numRows_);
            }
        }
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#method_after
@Override
public /**
 * Load the table metadata and reuse metadata to speed up metadata loading.
 * If the lastDdlTime has not been changed, that means the Hive metastore metadata has
 * not been changed. Reuses the old Hive partition metadata from cachedEntry.
 * To speed up Hdfs metadata loading, if a file's mtime has not been changed, reuses
 * the old file block metadata from old value.
 *
 * There are several cases where the cachedEntry might be reused incorrectly:
 * 1. an ALTER TABLE ADD PARTITION or dynamic partition insert is executed through
 *    Hive. This does not update the lastDdlTime.
 * 2. Hdfs rebalancer is executed. This changes the block locations but won't update
 *    the mtime (file modification time).
 * If any of these occurs, user has to execute "invalidate metadata" to invalidate the
 * metadata cache of the table to trigger a fresh load.
 */
void load(Table cachedEntry, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    LOG.debug("load table: " + db_.getName() + "." + name_);
    // turn all exceptions into TableLoadingException
    try {
        // set nullPartitionKeyValue from the hive conf.
        nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
        // set NULL indicator string from table properties
        nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
        if (nullColumnValue_ == null)
            nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
        // populate with both partition keys and regular columns
        List<FieldSchema> partKeys = msTbl.getPartitionKeys();
        List<FieldSchema> tblFields = Lists.newArrayList();
        String inputFormat = msTbl.getSd().getInputFormat();
        if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO) {
            // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
            // taking precedence.
            List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
            schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
            schemaSearchLocations.add(getMetaStoreTable().getParameters());
            avroSchema_ = HdfsTable.getAvroSchema(schemaSearchLocations, getFullName());
            String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
            if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
                // If the SerDe library is null or set to LazySimpleSerDe or is null, it
                // indicates there is an issue with the table metadata since Avro table need a
                // non-native serde. Instead of failing to load the table, fall back to
                // using the fields from the storage descriptor (same as Hive).
                tblFields.addAll(msTbl.getSd().getCols());
            } else {
                // Load the fields from the Avro schema.
                // Since Avro does not include meta-data for CHAR or VARCHAR, an Avro type of
                // "string" is used for CHAR, VARCHAR and STRING. Default back to the storage
                // descriptor to determine the the type for "string"
                List<FieldSchema> sdTypes = msTbl.getSd().getCols();
                int i = 0;
                List<Column> avroTypeList = AvroSchemaParser.parse(avroSchema_);
                boolean canFallBack = sdTypes.size() == avroTypeList.size();
                for (Column parsedCol : avroTypeList) {
                    FieldSchema fs = new FieldSchema();
                    fs.setName(parsedCol.getName());
                    String avroType = parsedCol.getType().toSql();
                    if (avroType.toLowerCase().equals("string") && canFallBack) {
                        fs.setType(sdTypes.get(i).getType());
                    } else {
                        fs.setType(avroType);
                    }
                    fs.setComment("from deserializer");
                    tblFields.add(fs);
                    i++;
                }
            }
        } else {
            tblFields.addAll(msTbl.getSd().getCols());
        }
        List<FieldSchema> fieldSchemas = new ArrayList<FieldSchema>(partKeys.size() + tblFields.size());
        fieldSchemas.addAll(partKeys);
        fieldSchemas.addAll(tblFields);
        // The number of clustering columns is the number of partition keys.
        numClusteringCols_ = partKeys.size();
        loadColumns(fieldSchemas, client);
        // Collect the list of partitions to use for the table. Partitions may be reused
        // from the existing cached table entry (if one exists), read from the metastore,
        // or a mix of both. Whether or not a partition is reused depends on whether
        // the table or partition has been modified.
        List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
        if (cachedEntry == null || !(cachedEntry instanceof HdfsTable) || cachedEntry.lastDdlTime_ != lastDdlTime_) {
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
        } else {
            // The table was already in the metadata cache and it has not been modified.
            Preconditions.checkArgument(cachedEntry instanceof HdfsTable);
            HdfsTable cachedHdfsTableEntry = (HdfsTable) cachedEntry;
            // Set of partition names that have been modified. Partitions in this Set need to
            // be reloaded from the metastore.
            Set<String> modifiedPartitionNames = Sets.newHashSet();
            // "temp" table that doesn't actually exist in the metastore.
            if (cachedEntry != this) {
                // Since the table has not been modified, we might be able to reuse some of the
                // old partition metadata if the individual partitions have not been modified.
                // First get a list of all the partition names for this table from the
                // metastore, this is much faster than listing all the Partition objects.
                modifiedPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
            }
            int totalPartitions = modifiedPartitionNames.size();
            // Get all the partitions from the cached entry that have not been modified.
            for (HdfsPartition cachedPart : cachedHdfsTableEntry.getPartitions()) {
                // Skip the default partition and any partitions that have been modified.
                if (cachedPart.isDirty() || cachedPart.isDefaultPartition()) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition cachedMsPart = cachedPart.toHmsPartition();
                if (cachedMsPart == null)
                    continue;
                // This is a partition we already know about and it hasn't been modified.
                // No need to reload the metadata.
                String cachedPartName = cachedPart.getPartitionName();
                if (modifiedPartitionNames.contains(cachedPartName)) {
                    msPartitions.add(cachedMsPart);
                    modifiedPartitionNames.remove(cachedPartName);
                }
            }
            LOG.info(String.format("Incrementally refreshing %d/%d partitions.", modifiedPartitionNames.size(), totalPartitions));
            // No need to make the metastore call if no partitions are to be updated.
            if (modifiedPartitionNames.size() > 0) {
                // Now reload the the remaining partitions.
                msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(modifiedPartitionNames), db_.getName(), name_));
            }
        }
        Map<String, List<FileDescriptor>> oldFileDescMap = null;
        if (cachedEntry != null && cachedEntry instanceof HdfsTable) {
            HdfsTable cachedHdfsTable = (HdfsTable) cachedEntry;
            oldFileDescMap = cachedHdfsTable.fileDescMap_;
            hostIndex_.populate(cachedHdfsTable.hostIndex_.getList());
        }
        loadPartitions(msPartitions, msTbl, oldFileDescMap);
        // load table stats
        numRows_ = getRowCount(msTbl.getParameters());
        LOG.debug("table #rows=" + Long.toString(numRows_));
        // to the table's numRows.
        if (numClusteringCols_ == 0 && !partitions_.isEmpty()) {
            // Unpartitioned tables have a 'dummy' partition and a default partition.
            // Temp tables used in CTAS statements have one partition.
            Preconditions.checkState(partitions_.size() == 2 || partitions_.size() == 1);
            for (HdfsPartition p : partitions_) {
                p.setNumRows(numRows_);
            }
        }
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#end_block

#method_before
public static String getAvroSchema(List<Map<String, String>> schemaSearchLocations, String tableName, boolean downloadSchema) throws TableLoadingException {
    String url = null;
    // Search all locations and break out on the first valid schema found.
    for (Map<String, String> schemaLocation : schemaSearchLocations) {
        if (schemaLocation == null)
            continue;
        String literal = schemaLocation.get(AvroSerdeUtils.SCHEMA_LITERAL);
        if (literal != null && !literal.equals(AvroSerdeUtils.SCHEMA_NONE))
            return literal;
        url = schemaLocation.get(AvroSerdeUtils.SCHEMA_URL);
        if (url != null) {
            url = url.trim();
            break;
        }
    }
    if (url == null || url.equals(AvroSerdeUtils.SCHEMA_NONE)) {
        throw new TableLoadingException(String.format("No Avro schema provided in " + "SERDEPROPERTIES or TBLPROPERTIES for table: %s ", tableName));
    }
    if (!url.toLowerCase().startsWith("hdfs://") && !url.toLowerCase().startsWith("http://")) {
        throw new TableLoadingException("avro.schema.url must be of form " + "\"http://path/to/schema/file\" or " + "\"hdfs://namenode:port/path/to/schema/file\", got " + url);
    }
    return downloadSchema ? loadAvroSchemaFromUrl(url) : url;
}
#method_after
public static String getAvroSchema(List<Map<String, String>> schemaSearchLocations, String tableName) throws TableLoadingException {
    String url = null;
    // Search all locations and break out on the first valid schema found.
    for (Map<String, String> schemaLocation : schemaSearchLocations) {
        if (schemaLocation == null)
            continue;
        String literal = schemaLocation.get(AvroSerdeUtils.SCHEMA_LITERAL);
        if (literal != null && !literal.equals(AvroSerdeUtils.SCHEMA_NONE))
            return literal;
        url = schemaLocation.get(AvroSerdeUtils.SCHEMA_URL);
        if (url != null) {
            url = url.trim();
            break;
        }
    }
    if (url == null || url.equals(AvroSerdeUtils.SCHEMA_NONE)) {
        throw new TableLoadingException(String.format("No Avro schema provided in " + "SERDEPROPERTIES or TBLPROPERTIES for table: %s ", tableName));
    }
    String schema = null;
    if (url.toLowerCase().startsWith("http://")) {
        InputStream urlStream = null;
        try {
            urlStream = new URL(url).openStream();
            schema = IOUtils.toString(urlStream);
        } catch (IOException e) {
            throw new TableLoadingException("Problem reading Avro schema from: " + url, e);
        } finally {
            IOUtils.closeQuietly(urlStream);
        }
    } else {
        Path path = new Path(url);
        FileSystem fs = null;
        try {
            fs = path.getFileSystem(FileSystemUtil.getConfiguration());
        } catch (Exception e) {
            throw new TableLoadingException(String.format("Invalid avro.schema.url: %s. %s", path, e.getMessage()));
        }
        StringBuilder errorMsg = new StringBuilder();
        if (!FileSystemUtil.isPathReachable(path, fs, errorMsg)) {
            throw new TableLoadingException(String.format("Invalid avro.schema.url: %s. %s", path, errorMsg));
        }
        try {
            schema = FileSystemUtil.readFile(path);
        } catch (IOException e) {
            throw new TableLoadingException("Problem reading Avro schema at: " + url, e);
        }
    }
    return schema;
}
#end_block

#method_before
@Override
protected void loadFromThrift(TTable thriftTable) throws TableLoadingException {
    super.loadFromThrift(thriftTable);
    THdfsTable hdfsTable = thriftTable.getHdfs_table();
    hdfsBaseDir_ = hdfsTable.getHdfsBaseDir();
    nullColumnValue_ = hdfsTable.nullColumnValue;
    nullPartitionKeyValue_ = hdfsTable.nullPartitionKeyValue;
    multipleFileSystems_ = hdfsTable.multiple_filesystems;
    hostIndex_.populate(hdfsTable.getNetwork_addresses());
    resetPartitionMd();
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    for (Map.Entry<Long, THdfsPartition> part : hdfsTable.getPartitions().entrySet()) {
        HdfsPartition hdfsPart = HdfsPartition.fromThrift(this, part.getKey(), part.getValue());
        numHdfsFiles_ += hdfsPart.getFileDescriptors().size();
        totalHdfsBytes_ += hdfsPart.getSize();
        partitions_.add(hdfsPart);
    }
    avroSchema_ = hdfsTable.isSetAvroSchema() ? hdfsTable.getAvroSchema() : null;
    isMarkedCached_ = HdfsCachingUtil.getCacheDirIdFromParams(getMetaStoreTable().getParameters()) != null;
    populatePartitionMd();
}
#method_after
@Override
protected void loadFromThrift(TTable thriftTable) throws TableLoadingException {
    super.loadFromThrift(thriftTable);
    THdfsTable hdfsTable = thriftTable.getHdfs_table();
    hdfsBaseDir_ = hdfsTable.getHdfsBaseDir();
    nullColumnValue_ = hdfsTable.nullColumnValue;
    nullPartitionKeyValue_ = hdfsTable.nullPartitionKeyValue;
    multipleFileSystems_ = hdfsTable.multiple_filesystems;
    hostIndex_.populate(hdfsTable.getNetwork_addresses());
    resetPartitionMd();
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    for (Map.Entry<Long, THdfsPartition> part : hdfsTable.getPartitions().entrySet()) {
        HdfsPartition hdfsPart = HdfsPartition.fromThrift(this, part.getKey(), part.getValue());
        numHdfsFiles_ += hdfsPart.getFileDescriptors().size();
        totalHdfsBytes_ += hdfsPart.getSize();
        partitions_.add(hdfsPart);
    }
    avroSchema_ = hdfsTable.isSetAvroSchema() ? hdfsTable.getAvroSchema() : null;
    isMarkedCached_ = HdfsCachingUtil.getCacheDirectiveId(getMetaStoreTable().getParameters()) != null;
    populatePartitionMd();
}
#end_block

#method_before
public TResultSet getTableStats() {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    for (int i = 0; i < numClusteringCols_; ++i) {
        // Add the partition-key values as strings for simplicity.
        Column partCol = getColumns().get(i);
        TColumn colDesc = new TColumn(partCol.getName(), Type.STRING.toThrift());
        resultSchema.addToColumns(colDesc);
    }
    resultSchema.addToColumns(new TColumn("#Rows", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("#Files", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("Size", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Bytes Cached", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Format", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Incremental stats", Type.STRING.toThrift()));
    // Pretty print partitions and their stats.
    ArrayList<HdfsPartition> orderedPartitions = Lists.newArrayList(partitions_);
    Collections.sort(orderedPartitions);
    long totalCachedBytes = 0L;
    for (HdfsPartition p : orderedPartitions) {
        // Ignore dummy default partition.
        if (p.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID)
            continue;
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        // Add the partition-key values (as strings for simplicity).
        for (LiteralExpr expr : p.getPartitionValues()) {
            rowBuilder.add(expr.getStringValue());
        }
        // Add number of rows, files, bytes, cache stats, and file format.
        rowBuilder.add(p.getNumRows()).add(p.getFileDescriptors().size()).addBytes(p.getSize());
        if (!p.isMarkedCached()) {
            // Helps to differentiate partitions that have 0B cached versus partitions
            // that are not marked as cached.
            rowBuilder.add("NOT CACHED");
        } else {
            // Calculate the number the number of bytes that are cached.
            long cachedBytes = 0L;
            for (FileDescriptor fd : p.getFileDescriptors()) {
                for (THdfsFileBlock fb : fd.getFileBlocks()) {
                    if (fb.getIs_replica_cached().contains(true)) {
                        cachedBytes += fb.getLength();
                    }
                }
            }
            totalCachedBytes += cachedBytes;
            rowBuilder.addBytes(cachedBytes);
        }
        rowBuilder.add(p.getInputFormatDescriptor().getFileFormat().toString());
        rowBuilder.add(String.valueOf(p.hasIncrementalStats()));
        result.addToRows(rowBuilder.get());
    }
    // For partitioned tables add a summary row at the bottom.
    if (numClusteringCols_ > 0) {
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        int numEmptyCells = numClusteringCols_ - 1;
        rowBuilder.add("Total");
        for (int i = 0; i < numEmptyCells; ++i) {
            rowBuilder.add("");
        }
        // Total num rows, files, and bytes (leave format empty).
        rowBuilder.add(numRows_).add(numHdfsFiles_).addBytes(totalHdfsBytes_).addBytes(totalCachedBytes).add("").add("");
        result.addToRows(rowBuilder.get());
    }
    return result;
}
#method_after
public TResultSet getTableStats() {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    for (int i = 0; i < numClusteringCols_; ++i) {
        // Add the partition-key values as strings for simplicity.
        Column partCol = getColumns().get(i);
        TColumn colDesc = new TColumn(partCol.getName(), Type.STRING.toThrift());
        resultSchema.addToColumns(colDesc);
    }
    resultSchema.addToColumns(new TColumn("#Rows", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("#Files", Type.BIGINT.toThrift()));
    resultSchema.addToColumns(new TColumn("Size", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Bytes Cached", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Cache Replication", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Format", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Incremental stats", Type.STRING.toThrift()));
    // Pretty print partitions and their stats.
    ArrayList<HdfsPartition> orderedPartitions = Lists.newArrayList(partitions_);
    Collections.sort(orderedPartitions);
    long totalCachedBytes = 0L;
    for (HdfsPartition p : orderedPartitions) {
        // Ignore dummy default partition.
        if (p.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID)
            continue;
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        // Add the partition-key values (as strings for simplicity).
        for (LiteralExpr expr : p.getPartitionValues()) {
            rowBuilder.add(expr.getStringValue());
        }
        // Add number of rows, files, bytes, cache stats, and file format.
        rowBuilder.add(p.getNumRows()).add(p.getFileDescriptors().size()).addBytes(p.getSize());
        if (!p.isMarkedCached()) {
            // Helps to differentiate partitions that have 0B cached versus partitions
            // that are not marked as cached.
            rowBuilder.add("NOT CACHED");
            rowBuilder.add("NOT CACHED");
        } else {
            // Calculate the number the number of bytes that are cached.
            long cachedBytes = 0L;
            for (FileDescriptor fd : p.getFileDescriptors()) {
                for (THdfsFileBlock fb : fd.getFileBlocks()) {
                    if (fb.getIs_replica_cached().contains(true)) {
                        cachedBytes += fb.getLength();
                    }
                }
            }
            totalCachedBytes += cachedBytes;
            rowBuilder.addBytes(cachedBytes);
            // Extract cache replication factor from the parameters of the table
            // if the table is not partitioned or directly from the partition.
            Short rep = HdfsCachingUtil.getCachedCacheReplication(numClusteringCols_ == 0 ? p.getTable().getMetaStoreTable().getParameters() : p.getParameters());
            rowBuilder.add(rep.toString());
        }
        rowBuilder.add(p.getInputFormatDescriptor().getFileFormat().toString());
        rowBuilder.add(String.valueOf(p.hasIncrementalStats()));
        result.addToRows(rowBuilder.get());
    }
    // For partitioned tables add a summary row at the bottom.
    if (numClusteringCols_ > 0) {
        TResultRowBuilder rowBuilder = new TResultRowBuilder();
        int numEmptyCells = numClusteringCols_ - 1;
        rowBuilder.add("Total");
        for (int i = 0; i < numEmptyCells; ++i) {
            rowBuilder.add("");
        }
        // Total num rows, files, and bytes (leave format empty).
        rowBuilder.add(numRows_).add(numHdfsFiles_).addBytes(totalHdfsBytes_).addBytes(totalCachedBytes).add("").add("").add("");
        result.addToRows(rowBuilder.get());
    }
    return result;
}
#end_block

#method_before
public List<TAccessEvent> getAccessEvents() {
    return analyzer_.getAccessEvents();
}
#method_after
public Set<TAccessEvent> getAccessEvents() {
    return analyzer_.getAccessEvents();
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    strBuilder.append("SHOW FILES ");
    strBuilder.append(tableName_.toString());
    if (partitionSpec_ != null)
        strBuilder.append(" " + partitionSpec_.toSql());
    return strBuilder.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    strBuilder.append("SHOW FILES IN " + tableName_.toString());
    if (partitionSpec_ != null)
        strBuilder.append(" " + partitionSpec_.toSql());
    return strBuilder.toString();
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    postAnalysisDb_ = tableName_.getDb();
    if (postAnalysisDb_ == null) {
        postAnalysisDb_ = analyzer.getDefaultDb();
    }
    if (analyzer.getDb(postAnalysisDb_, Privilege.ANY) == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + postAnalysisDb_);
    }
    // Analyze the partition spec, if one was specified.
    if (partitionSpec_ != null) {
        partitionSpec_.setTableName(tableName_);
        partitionSpec_.setPartitionShouldExist();
        partitionSpec_.setPrivilegeRequirement(Privilege.ANY);
        partitionSpec_.analyze(analyzer);
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (!tableName_.isFullyQualified()) {
        tableName_ = new TableName(analyzer.getDefaultDb(), tableName_.getTbl());
    }
    table_ = analyzer.getTable(tableName_, Privilege.VIEW_METADATA);
    if (!(table_ instanceof HdfsTable)) {
        throw new AnalysisException(String.format("SHOW FILES not applicable to a non hdfs table: %s", table_.getFullName()));
    }
    // Analyze the partition spec, if one was specified.
    if (partitionSpec_ != null) {
        partitionSpec_.setTableName(tableName_);
        partitionSpec_.setPartitionShouldExist();
        partitionSpec_.setPrivilegeRequirement(Privilege.VIEW_METADATA);
        partitionSpec_.analyze(analyzer);
    }
}
#end_block

#method_before
public TShowFilesParams toThrift() {
    TShowFilesParams params = new TShowFilesParams();
    params.setTable_name(new TTableName(postAnalysisDb_, tableName_.getTbl()));
    if (partitionSpec_ != null) {
        params.setPartition_spec(partitionSpec_.toThrift());
    }
    return params;
}
#method_after
public TShowFilesParams toThrift() {
    TShowFilesParams params = new TShowFilesParams();
    params.setTable_name(new TTableName(tableName_.getDb(), tableName_.getTbl()));
    if (partitionSpec_ != null) {
        params.setPartition_spec(partitionSpec_.toThrift());
    }
    return params;
}
#end_block

#method_before
private void standardize(Analyzer analyzer) {
    FunctionName analyticFnName = getFnCall().getFnName();
    // Set a window from UNBOUNDED PRECEDING to CURRENT_ROW for row_number().
    if (analyticFnName.getFunction().equals(ROWNUMBER)) {
        Preconditions.checkState(window_ == null, "Unexpected window set for row_numer()");
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), new Boundary(BoundaryType.CURRENT_ROW, null));
        resetWindow_ = true;
        return;
    }
    // Set a window for lag(): UNBOUNDED PRECEDING to OFFSET PRECEDING.
    if (isOffsetFn(getFnCall().getFn())) {
        Preconditions.checkState(window_ == null);
        // If necessary, create a new fn call with the default args explicitly set.
        List<Expr> newExprParams = null;
        if (getFnCall().getChildren().size() == 1) {
            newExprParams = Lists.newArrayListWithExpectedSize(3);
            newExprParams.addAll(getFnCall().getChildren());
            // Default offset is 1.
            newExprParams.add(new NumericLiteral(BigDecimal.valueOf(1)));
            // Default default value is NULL.
            newExprParams.add(new NullLiteral());
        } else if (getFnCall().getChildren().size() == 2) {
            newExprParams = Lists.newArrayListWithExpectedSize(3);
            newExprParams.addAll(getFnCall().getChildren());
            // Default default value is NULL.
            newExprParams.add(new NullLiteral());
        } else {
            Preconditions.checkState(getFnCall().getChildren().size() == 3);
        }
        if (newExprParams != null) {
            fnCall_ = new FunctionCallExpr(getFnCall().getFnName(), new FunctionParams(newExprParams));
            fnCall_.setIsAnalyticFnCall(true);
            fnCall_.analyzeNoThrow(analyzer);
        }
        // Set the window.
        BoundaryType rightBoundaryType = BoundaryType.FOLLOWING;
        if (analyticFnName.getFunction().equals(LAG)) {
            rightBoundaryType = BoundaryType.PRECEDING;
        }
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), new Boundary(rightBoundaryType, getOffsetExpr(getFnCall())));
        try {
            window_.analyze(analyzer);
        } catch (AnalysisException e) {
            throw new IllegalStateException(e);
        }
        resetWindow_ = true;
        return;
    }
    if (analyticFnName.getFunction().equals(FIRSTVALUE) && window_ != null && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING) {
        if (window_.getLeftBoundary().getType() != BoundaryType.PRECEDING) {
            window_ = new AnalyticWindow(window_.getType(), window_.getLeftBoundary(), window_.getLeftBoundary());
            fnCall_ = new FunctionCallExpr(new FunctionName("last_value"), getFnCall().getParams());
        } else {
            List<Expr> paramExprs = Expr.cloneList(getFnCall().getParams().exprs());
            if (window_.getRightBoundary().getType() == BoundaryType.PRECEDING) {
                // The number of rows preceding for the end bound determines the number of
                // rows at the beginning of each partition that should have a NULL value.
                paramExprs.add(window_.getRightBoundary().getExpr());
            } else {
                // -1 indicates that no NULL values are inserted even though we set the end
                // bound to the start bound (which is PRECEDING) below; this is different from
                // the default behavior of windows with an end bound PRECEDING.
                paramExprs.add(new NumericLiteral(BigInteger.valueOf(-1), Type.TINYINT));
            }
            window_ = new AnalyticWindow(window_.getType(), new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), window_.getLeftBoundary());
            fnCall_ = new FunctionCallExpr(new FunctionName("first_value_rewrite"), new FunctionParams(paramExprs));
            fnCall_.setIsInternalFnCall(true);
        }
        fnCall_.setIsAnalyticFnCall(true);
        fnCall_.analyzeNoThrow(analyzer);
        type_ = fnCall_.getReturnType();
        analyticFnName = getFnCall().getFnName();
    }
    // and and not starting with UNBOUNDED PRECEDING.
    if (window_ != null && window_.getRightBoundary().getType() == BoundaryType.UNBOUNDED_FOLLOWING && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING) {
        orderByElements_ = OrderByElement.reverse(orderByElements_);
        window_ = window_.reverse();
        // Also flip first_value()/last_value(). For other analytic functions there is no
        // need to also change the function.
        FunctionName reversedFnName = null;
        if (analyticFnName.getFunction().equals(FIRSTVALUE)) {
            reversedFnName = new FunctionName(LASTVALUE);
        } else if (analyticFnName.getFunction().equals(LASTVALUE)) {
            reversedFnName = new FunctionName(FIRSTVALUE);
        }
        if (reversedFnName != null) {
            fnCall_ = new FunctionCallExpr(reversedFnName, getFnCall().getParams());
            fnCall_.setIsAnalyticFnCall(true);
            fnCall_.analyzeNoThrow(analyzer);
        }
        analyticFnName = getFnCall().getFnName();
    }
    // is UNBOUNDED_PRECEDING.
    if (window_ != null && window_.getLeftBoundary().getType() == BoundaryType.UNBOUNDED_PRECEDING && window_.getRightBoundary().getType() != BoundaryType.PRECEDING && analyticFnName.getFunction().equals(FIRSTVALUE)) {
        window_.setRightBoundary(new Boundary(BoundaryType.CURRENT_ROW, null));
    }
    // Set the default window.
    if (!orderByElements_.isEmpty() && window_ == null) {
        window_ = AnalyticWindow.DEFAULT_WINDOW;
        resetWindow_ = true;
    }
}
#method_after
private void standardize(Analyzer analyzer) {
    FunctionName analyticFnName = getFnCall().getFnName();
    // Set a window from UNBOUNDED PRECEDING to CURRENT_ROW for row_number().
    if (analyticFnName.getFunction().equals(ROWNUMBER)) {
        Preconditions.checkState(window_ == null, "Unexpected window set for row_numer()");
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), new Boundary(BoundaryType.CURRENT_ROW, null));
        resetWindow_ = true;
        return;
    }
    // Set a window for lag(): UNBOUNDED PRECEDING to OFFSET PRECEDING.
    if (isOffsetFn(getFnCall().getFn())) {
        Preconditions.checkState(window_ == null);
        // If necessary, create a new fn call with the default args explicitly set.
        List<Expr> newExprParams = null;
        if (getFnCall().getChildren().size() == 1) {
            newExprParams = Lists.newArrayListWithExpectedSize(3);
            newExprParams.addAll(getFnCall().getChildren());
            // Default offset is 1.
            newExprParams.add(new NumericLiteral(BigDecimal.valueOf(1)));
            // Default default value is NULL.
            newExprParams.add(new NullLiteral());
        } else if (getFnCall().getChildren().size() == 2) {
            newExprParams = Lists.newArrayListWithExpectedSize(3);
            newExprParams.addAll(getFnCall().getChildren());
            // Default default value is NULL.
            newExprParams.add(new NullLiteral());
        } else {
            Preconditions.checkState(getFnCall().getChildren().size() == 3);
        }
        if (newExprParams != null) {
            fnCall_ = new FunctionCallExpr(getFnCall().getFnName(), new FunctionParams(newExprParams));
            fnCall_.setIsAnalyticFnCall(true);
            fnCall_.analyzeNoThrow(analyzer);
        }
        // Set the window.
        BoundaryType rightBoundaryType = BoundaryType.FOLLOWING;
        if (analyticFnName.getFunction().equals(LAG)) {
            rightBoundaryType = BoundaryType.PRECEDING;
        }
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), new Boundary(rightBoundaryType, getOffsetExpr(getFnCall())));
        try {
            window_.analyze(analyzer);
        } catch (AnalysisException e) {
            throw new IllegalStateException(e);
        }
        resetWindow_ = true;
        return;
    }
    if (analyticFnName.getFunction().equals(FIRSTVALUE) && window_ != null && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING) {
        if (window_.getLeftBoundary().getType() != BoundaryType.PRECEDING) {
            window_ = new AnalyticWindow(window_.getType(), window_.getLeftBoundary(), window_.getLeftBoundary());
            fnCall_ = new FunctionCallExpr(new FunctionName("last_value"), getFnCall().getParams());
        } else {
            List<Expr> paramExprs = Expr.cloneList(getFnCall().getParams().exprs());
            if (window_.getRightBoundary().getType() == BoundaryType.PRECEDING) {
                // The number of rows preceding for the end bound determines the number of
                // rows at the beginning of each partition that should have a NULL value.
                paramExprs.add(new NumericLiteral(window_.getRightBoundary().getOffsetValue(), Type.BIGINT));
            } else {
                // -1 indicates that no NULL values are inserted even though we set the end
                // bound to the start bound (which is PRECEDING) below; this is different from
                // the default behavior of windows with an end bound PRECEDING.
                paramExprs.add(new NumericLiteral(BigInteger.valueOf(-1), Type.BIGINT));
            }
            window_ = new AnalyticWindow(window_.getType(), new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), window_.getLeftBoundary());
            fnCall_ = new FunctionCallExpr(new FunctionName("first_value_rewrite"), new FunctionParams(paramExprs));
            fnCall_.setIsInternalFnCall(true);
        }
        fnCall_.setIsAnalyticFnCall(true);
        fnCall_.analyzeNoThrow(analyzer);
        type_ = fnCall_.getReturnType();
        analyticFnName = getFnCall().getFnName();
    }
    // and and not starting with UNBOUNDED PRECEDING.
    if (window_ != null && window_.getRightBoundary().getType() == BoundaryType.UNBOUNDED_FOLLOWING && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING) {
        orderByElements_ = OrderByElement.reverse(orderByElements_);
        window_ = window_.reverse();
        // Also flip first_value()/last_value(). For other analytic functions there is no
        // need to also change the function.
        FunctionName reversedFnName = null;
        if (analyticFnName.getFunction().equals(FIRSTVALUE)) {
            reversedFnName = new FunctionName(LASTVALUE);
        } else if (analyticFnName.getFunction().equals(LASTVALUE)) {
            reversedFnName = new FunctionName(FIRSTVALUE);
        }
        if (reversedFnName != null) {
            fnCall_ = new FunctionCallExpr(reversedFnName, getFnCall().getParams());
            fnCall_.setIsAnalyticFnCall(true);
            fnCall_.analyzeNoThrow(analyzer);
        }
        analyticFnName = getFnCall().getFnName();
    }
    // is UNBOUNDED_PRECEDING.
    if (window_ != null && window_.getLeftBoundary().getType() == BoundaryType.UNBOUNDED_PRECEDING && window_.getRightBoundary().getType() != BoundaryType.PRECEDING && analyticFnName.getFunction().equals(FIRSTVALUE)) {
        window_.setRightBoundary(new Boundary(BoundaryType.CURRENT_ROW, null));
    }
    // Set the default window.
    if (!orderByElements_.isEmpty() && window_ == null) {
        window_ = AnalyticWindow.DEFAULT_WINDOW;
        resetWindow_ = true;
    }
}
#end_block

#method_before
@Test
public void testCompare() {
    verifyReflexive(valuesNull_);
    verifyReflexive(valuesSmall_);
    verifyReflexive(valuesMixed_);
    verifySymmetric(valuesSmall_, valuesLarge_);
    verifySymmetric(valuesSmall_, valuesNull_);
    verifySymmetric(valuesNull_, valuesMixed_);
    verifySymmetric(valuesNull_, valuesNull_);
    verifySymmetric(valuesMixed_, valuesMixedSmall_);
    verifySymmetric(valuesMixedLarge_, valuesMixedSmall_);
    verifyTransitive(valuesLarge_, valuesMedium_, valuesSmall_);
    verifyTransitive(valuesLarge_, valuesSmall_, valuesNull_);
    verifyTransitive(valuesMixedLarge_, valuesMixedSmall_, valuesSmall_);
    verifyTransitive(valuesMixedLarge_, valuesMixedSmall_, valuesNull_);
    List<LiteralExpr> valuesTest = Lists.newArrayList();
    valuesTest.add(new NumericLiteral(BigDecimal.valueOf(3)));
    verifyAntiSymmetric(valuesMedium_, valuesTest, valuesNull_);
    valuesTest.add(NullLiteral.create(Type.BIGINT));
    verifyAntiSymmetric(valuesMixed_, valuesTest, valuesSmall_);
}
#method_after
@Test
public void testCompare() {
    List<List<LiteralExpr>> allLists = Lists.newArrayList();
    allLists.add(valuesNull_);
    allLists.add(valuesDecimal_);
    allLists.add(valuesDecimal1_);
    allLists.add(valuesDecimal2_);
    allLists.add(valuesMixed_);
    allLists.add(valuesMixed1_);
    allLists.add(valuesMixed2_);
    for (List<LiteralExpr> l1 : allLists) {
        verifyReflexive(l1);
        for (List<LiteralExpr> l2 : allLists) {
            verifySymmetric(l1, l2);
            for (List<LiteralExpr> l3 : allLists) {
                verifyTransitive(l1, l2, l3);
            }
        }
    }
    List<LiteralExpr> valuesTest = Lists.newArrayList();
    valuesTest.add(new NumericLiteral(BigDecimal.valueOf(3)));
    verifyAntiSymmetric(valuesDecimal1_, valuesTest, valuesNull_);
    valuesTest.add(NullLiteral.create(Type.BIGINT));
    verifyAntiSymmetric(valuesMixed_, valuesTest, valuesDecimal_);
}
#end_block

#method_before
public String getPartitionName() {
    List<String> partitionCols = Lists.newArrayList();
    List<String> partitionValues = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(getTable().getColumns().get(i).getName());
    }
    for (LiteralExpr partValue : getPartitionValues()) {
        partitionValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, getTable().getNullPartitionKeyValue()));
    }
    return org.apache.hadoop.hive.common.FileUtils.makePartName(partitionCols, partitionValues);
}
#method_after
public String getPartitionName() {
    List<String> partitionCols = Lists.newArrayList();
    List<String> partitionValues = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(getTable().getColumns().get(i).getName());
    }
    return org.apache.hadoop.hive.common.FileUtils.makePartName(partitionCols, getPartitionValuesAsStrings(true));
}
#end_block

#method_before
// Returns the HDFS permissions Impala has to this partition's directory - READ_ONLY,
public TAccessLevel getAccessLevel() {
    return accessLevel_;
}
#method_after
public TAccessLevel getAccessLevel() {
    return accessLevel_;
}
#end_block

#method_before
@VisibleForTesting
public static int comparePartitionKeyValues(List<LiteralExpr> lhs, List<LiteralExpr> rhs) {
    int sizeDiff = lhs.size() - rhs.size();
    if (sizeDiff != 0)
        return sizeDiff;
    for (int i = 0; i < lhs.size(); ++i) {
        if (!(lhs.get(i) instanceof NullLiteral) && (rhs.get(i) instanceof NullLiteral)) {
            return 1;
        }
        int cmp = lhs.get(i).compareTo(rhs.get(i));
        if (cmp != 0)
            return cmp;
    }
    return 0;
}
#method_after
@VisibleForTesting
public static int comparePartitionKeyValues(List<LiteralExpr> lhs, List<LiteralExpr> rhs) {
    int sizeDiff = lhs.size() - rhs.size();
    if (sizeDiff != 0)
        return sizeDiff;
    for (int i = 0; i < lhs.size(); ++i) {
        int cmp = lhs.get(i).compareTo(rhs.get(i));
        if (cmp != 0)
            return cmp;
    }
    return 0;
}
#end_block

#method_before
public TDdlExecResponse execDdlRequest(TDdlExecRequest ddlRequest) throws ImpalaException {
    TDdlExecResponse response = new TDdlExecResponse();
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    User requestingUser = null;
    if (ddlRequest.isSetHeader()) {
        requestingUser = new User(ddlRequest.getHeader().getRequesting_user());
    }
    switch(ddlRequest.ddl_type) {
        case ALTER_TABLE:
            alterTable(ddlRequest.getAlter_table_params(), response);
            break;
        case ALTER_VIEW:
            alterView(ddlRequest.getAlter_view_params(), response);
            break;
        case CREATE_DATABASE:
            createDatabase(ddlRequest.getCreate_db_params(), response);
            break;
        case CREATE_TABLE_AS_SELECT:
            response.setNew_table_created(createTable(ddlRequest.getCreate_table_params(), response));
            break;
        case CREATE_TABLE:
            createTable(ddlRequest.getCreate_table_params(), response);
            break;
        case CREATE_TABLE_LIKE:
            createTableLike(ddlRequest.getCreate_table_like_params(), response);
            break;
        case CREATE_VIEW:
            createView(ddlRequest.getCreate_view_params(), response);
            break;
        case CREATE_FUNCTION:
            createFunction(ddlRequest.getCreate_fn_params(), response);
            break;
        case CREATE_DATA_SOURCE:
            createDataSource(ddlRequest.getCreate_data_source_params(), response);
            break;
        case COMPUTE_STATS:
            Preconditions.checkState(false, "Compute stats should trigger an ALTER TABLE.");
            break;
        case DROP_STATS:
            dropStats(ddlRequest.getDrop_stats_params(), response);
            break;
        case DROP_DATABASE:
            dropDatabase(ddlRequest.getDrop_db_params(), response);
            break;
        case DROP_TABLE:
        case DROP_VIEW:
            dropTableOrView(ddlRequest.getDrop_table_or_view_params(), response);
            break;
        case DROP_FUNCTION:
            dropFunction(ddlRequest.getDrop_fn_params(), response);
            break;
        case DROP_DATA_SOURCE:
            dropDataSource(ddlRequest.getDrop_data_source_params(), response);
            break;
        case CREATE_ROLE:
        case DROP_ROLE:
            createDropRole(requestingUser, ddlRequest.getCreate_drop_role_params(), response);
            break;
        case GRANT_ROLE:
        case REVOKE_ROLE:
            grantRevokeRoleGroup(requestingUser, ddlRequest.getGrant_revoke_role_params(), response);
            break;
        case GRANT_PRIVILEGE:
        case REVOKE_PRIVILEGE:
            grantRevokeRolePrivilege(requestingUser, ddlRequest.getGrant_revoke_priv_params(), response);
            break;
        default:
            throw new IllegalStateException("Unexpected DDL exec request type: " + ddlRequest.ddl_type);
    }
    // At this point, the operation is considered successful. If any errors occurred
    // during execution, this function will throw an exception and the CatalogServer
    // will handle setting a bad status code.
    response.getResult().setStatus(new TStatus(TStatusCode.OK, new ArrayList<String>()));
    return response;
}
#method_after
public TDdlExecResponse execDdlRequest(TDdlExecRequest ddlRequest) throws ImpalaException {
    TDdlExecResponse response = new TDdlExecResponse();
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    User requestingUser = null;
    if (ddlRequest.isSetHeader()) {
        requestingUser = new User(ddlRequest.getHeader().getRequesting_user());
    }
    switch(ddlRequest.ddl_type) {
        case ALTER_TABLE:
            alterTable(ddlRequest.getAlter_table_params(), response);
            break;
        case ALTER_VIEW:
            alterView(ddlRequest.getAlter_view_params(), response);
            break;
        case CREATE_DATABASE:
            createDatabase(ddlRequest.getCreate_db_params(), response);
            break;
        case CREATE_TABLE_AS_SELECT:
            response.setNew_table_created(createTable(ddlRequest.getCreate_table_params(), response));
            break;
        case CREATE_TABLE:
            createTable(ddlRequest.getCreate_table_params(), response);
            break;
        case CREATE_TABLE_LIKE:
            createTableLike(ddlRequest.getCreate_table_like_params(), response);
            break;
        case CREATE_VIEW:
            createView(ddlRequest.getCreate_view_params(), response);
            break;
        case CREATE_FUNCTION:
            createFunction(ddlRequest.getCreate_fn_params(), response);
            break;
        case CREATE_DATA_SOURCE:
            createDataSource(ddlRequest.getCreate_data_source_params(), response);
            break;
        case COMPUTE_STATS:
            Preconditions.checkState(false, "Compute stats should trigger an ALTER TABLE.");
            break;
        case DROP_STATS:
            dropStats(ddlRequest.getDrop_stats_params(), response);
            break;
        case DROP_DATABASE:
            dropDatabase(ddlRequest.getDrop_db_params(), response);
            break;
        case DROP_TABLE:
        case DROP_VIEW:
            dropTableOrView(ddlRequest.getDrop_table_or_view_params(), response);
            break;
        case DROP_FUNCTION:
            dropFunction(ddlRequest.getDrop_fn_params(), response);
            break;
        case DROP_DATA_SOURCE:
            dropDataSource(ddlRequest.getDrop_data_source_params(), response);
            break;
        case CREATE_ROLE:
        case DROP_ROLE:
            createDropRole(requestingUser, ddlRequest.getCreate_drop_role_params(), response);
            break;
        case GRANT_ROLE:
        case REVOKE_ROLE:
            grantRevokeRoleGroup(requestingUser, ddlRequest.getGrant_revoke_role_params(), response);
            break;
        case GRANT_PRIVILEGE:
        case REVOKE_PRIVILEGE:
            grantRevokeRolePrivilege(requestingUser, ddlRequest.getGrant_revoke_priv_params(), response);
            break;
        default:
            throw new IllegalStateException("Unexpected DDL exec request type: " + ddlRequest.ddl_type);
    }
    // At this point, the operation is considered successful. If any errors occurred
    // during execution, this function will throw an exception and the CatalogServer
    // will handle setting a bad status code.
    response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    return response;
}
#end_block

#method_before
private int updateTableStats(Table table, TAlterTableUpdateStatsParams params, org.apache.hadoop.hive.metastore.api.Table msTbl, List<HdfsPartition> partitions, List<HdfsPartition> modifiedParts) throws ImpalaException {
    Preconditions.checkState(params.isSetPartition_stats());
    Preconditions.checkState(params.isSetTable_stats());
    // Update the partitions' ROW_COUNT parameter.
    int numTargetedPartitions = 0;
    for (HdfsPartition partition : partitions) {
        List<String> partitionValues = partition.getPartitionValuesAsStrings();
        TPartitionStats partitionStats = params.partition_stats.get(partitionValues);
        TPartitionStats existingPartStats = PartitionStatsUtil.partStatsFromParameters(partition.getParameters());
        if (partitionStats == null) {
            // TPartitionStats object.
            if (params.expect_all_partitions == false)
                continue;
            // If all partitions are expected, fill in any missing stats with an empty entry.
            partitionStats = new TPartitionStats();
            if (params.is_incremental) {
                partitionStats.intermediate_col_stats = Maps.newHashMap();
            }
            partitionStats.stats = new TTableStats();
            partitionStats.stats.setNum_rows(0L);
        }
        long numRows = partitionStats.stats.num_rows;
        LOG.debug(String.format("Updating stats for partition %s: numRows=%s", partition.getValuesAsString(), numRows));
        boolean updatedPartition = false;
        // partition, or they're different.
        if (existingPartStats == null || !existingPartStats.equals(partitionStats)) {
            PartitionStatsUtil.partStatsToParameters(partitionStats, partition);
            updatedPartition = true;
        }
        String existingRowCount = partition.getParameters().get(StatsSetupConst.ROW_COUNT);
        String newRowCount = String.valueOf(numRows);
        // Update table stats
        if (existingRowCount == null || !existingRowCount.equals(newRowCount)) {
            // The existing row count value wasn't set or has changed.
            partition.putToParameters(StatsSetupConst.ROW_COUNT, newRowCount);
            partition.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
            updatedPartition = true;
        }
        if (updatedPartition) {
            ++numTargetedPartitions;
            modifiedParts.add(partition);
        }
    }
    // For unpartitioned tables and HBase tables report a single updated partition.
    if (table.getNumClusteringCols() == 0 || table instanceof HBaseTable) {
        numTargetedPartitions = 1;
    }
    // Update the table's ROW_COUNT parameter.
    msTbl.putToParameters(StatsSetupConst.ROW_COUNT, String.valueOf(params.getTable_stats().num_rows));
    msTbl.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
    return numTargetedPartitions;
}
#method_after
private int updateTableStats(Table table, TAlterTableUpdateStatsParams params, org.apache.hadoop.hive.metastore.api.Table msTbl, List<HdfsPartition> partitions, List<HdfsPartition> modifiedParts) throws ImpalaException {
    Preconditions.checkState(params.isSetPartition_stats());
    Preconditions.checkState(params.isSetTable_stats());
    // Update the partitions' ROW_COUNT parameter.
    int numTargetedPartitions = 0;
    for (HdfsPartition partition : partitions) {
        // NULL keys are returned as 'NULL' in the partition_stats map, so don't substitute
        // this partition's keys with Hive's replacement value.
        List<String> partitionValues = partition.getPartitionValuesAsStrings(false);
        TPartitionStats partitionStats = params.partition_stats.get(partitionValues);
        TPartitionStats existingPartStats = PartitionStatsUtil.partStatsFromParameters(partition.getParameters());
        if (partitionStats == null) {
            // TPartitionStats object.
            if (params.expect_all_partitions == false)
                continue;
            // If all partitions are expected, fill in any missing stats with an empty entry.
            partitionStats = new TPartitionStats();
            if (params.is_incremental) {
                partitionStats.intermediate_col_stats = Maps.newHashMap();
            }
            partitionStats.stats = new TTableStats();
            partitionStats.stats.setNum_rows(0L);
        }
        long numRows = partitionStats.stats.num_rows;
        LOG.debug(String.format("Updating stats for partition %s: numRows=%s", partition.getValuesAsString(), numRows));
        boolean updatedPartition = false;
        // partition, or they're different.
        if (existingPartStats == null || !existingPartStats.equals(partitionStats)) {
            PartitionStatsUtil.partStatsToParameters(partitionStats, partition);
            updatedPartition = true;
        }
        String existingRowCount = partition.getParameters().get(StatsSetupConst.ROW_COUNT);
        String newRowCount = String.valueOf(numRows);
        // Update table stats
        if (existingRowCount == null || !existingRowCount.equals(newRowCount)) {
            // The existing row count value wasn't set or has changed.
            partition.putToParameters(StatsSetupConst.ROW_COUNT, newRowCount);
            partition.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
            updatedPartition = true;
        }
        if (updatedPartition) {
            ++numTargetedPartitions;
            modifiedParts.add(partition);
        }
    }
    // For unpartitioned tables and HBase tables report a single updated partition.
    if (table.getNumClusteringCols() == 0 || table instanceof HBaseTable) {
        numTargetedPartitions = 1;
    }
    // Update the table's ROW_COUNT parameter.
    msTbl.putToParameters(StatsSetupConst.ROW_COUNT, String.valueOf(params.getTable_stats().num_rows));
    msTbl.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
    return numTargetedPartitions;
}
#end_block

#method_before
private static ColumnStatisticsData createHiveColStatsData(TColumnStats colStats, Type colType) {
    ColumnStatisticsData colStatsData = new ColumnStatisticsData();
    long ndvs = colStats.getNum_distinct_values();
    long numNulls = colStats.getNum_nulls();
    switch(colType.getPrimitiveType()) {
        case BOOLEAN:
            // TODO: Gather and set the numTrues and numFalse stats as well. The planner
            // currently does not rely on them.
            colStatsData.setBooleanStats(new BooleanColumnStatsData(1, -1, numNulls));
            break;
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case // Hive and Impala use LongColumnStatsData for timestamps.
        TIMESTAMP:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setLongStats(new LongColumnStatsData(numNulls, ndvs));
            break;
        case FLOAT:
        case DOUBLE:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDoubleStats(new DoubleColumnStatsData(numNulls, ndvs));
            break;
        case STRING:
            long maxStrLen = colStats.getMax_size();
            double avgStrLen = colStats.getAvg_size();
            colStatsData.setStringStats(new StringColumnStatsData(maxStrLen, avgStrLen, numNulls, ndvs));
            break;
        case DECIMAL:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDecimalStats(new DecimalColumnStatsData(numNulls, ndvs));
            break;
        default:
            return null;
    }
    return colStatsData;
}
#method_after
private static ColumnStatisticsData createHiveColStatsData(TColumnStats colStats, Type colType) {
    ColumnStatisticsData colStatsData = new ColumnStatisticsData();
    long ndvs = colStats.getNum_distinct_values();
    long numNulls = colStats.getNum_nulls();
    switch(colType.getPrimitiveType()) {
        case BOOLEAN:
            // TODO: Gather and set the numTrues and numFalse stats as well. The planner
            // currently does not rely on them.
            colStatsData.setBooleanStats(new BooleanColumnStatsData(1, -1, numNulls));
            break;
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case // Hive and Impala use LongColumnStatsData for timestamps.
        TIMESTAMP:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setLongStats(new LongColumnStatsData(numNulls, ndvs));
            break;
        case FLOAT:
        case DOUBLE:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDoubleStats(new DoubleColumnStatsData(numNulls, ndvs));
            break;
        case CHAR:
        case VARCHAR:
        case STRING:
            long maxStrLen = colStats.getMax_size();
            double avgStrLen = colStats.getAvg_size();
            colStatsData.setStringStats(new StringColumnStatsData(maxStrLen, avgStrLen, numNulls, ndvs));
            break;
        case DECIMAL:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDecimalStats(new DecimalColumnStatsData(numNulls, ndvs));
            break;
        default:
            return null;
    }
    return colStatsData;
}
#end_block

#method_before
private void alterTableOrViewRename(TableName tableName, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        msTbl.setDbName(newTableName.getDb());
        msTbl.setTableName(newTableName.getTbl());
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
        } finally {
            msClient.release();
        }
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object(removedObject);
    response.result.setUpdated_catalog_object(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#method_after
private void alterTableOrViewRename(TableName tableName, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        msTbl.setDbName(newTableName.getDb());
        msTbl.setTableName(newTableName.getTbl());
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column stats
            // across databases, we save, drop and restore the column stats because the HMS
            // does not properly move them to the new table via alteration. The following
            // block needs to be protected by the metastoreDdlLock_ to avoid conflicts with
            // concurrent DDL on this same table (e.g., drop+add table with same db/name).
            ColumnStatistics hmsColStats = null;
            if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
                Table oldTbl = getExistingTable(tableName.getDb(), tableName.getTbl());
                Map<String, TColumnStats> colStats = Maps.newHashMap();
                for (Column c : oldTbl.getColumns()) {
                    colStats.put(c.getName(), c.getStats().toThrift());
                }
                hmsColStats = createHiveColStats(colStats, oldTbl);
                // Set the new db/table.
                hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
                LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
                // Delete all column stats of the original table from the HMS.
                msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
            }
            // Perform the table rename in any case.
            msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
            if (hmsColStats != null) {
                LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
                msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
            }
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
        } finally {
            msClient.release();
        }
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object(removedObject);
    response.result.setUpdated_catalog_object(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#end_block

#method_before
private void bulkAlterPartitions(String dbName, String tableName, List<HdfsPartition> modifiedParts) throws ImpalaRuntimeException {
    MetaStoreClient msClient = null;
    List<org.apache.hadoop.hive.metastore.api.Partition> hmsPartitions = Lists.newArrayList();
    for (HdfsPartition p : modifiedParts) hmsPartitions.add(p.toHmsPartition());
    try {
        msClient = catalog_.getMetaStoreClient();
        // Apply the updates in batches of 'MAX_PARTITION_UPDATES_PER_RPC'.
        for (int i = 0; i < hmsPartitions.size(); i += MAX_PARTITION_UPDATES_PER_RPC) {
            int numPartitionsToUpdate = Math.min(i + MAX_PARTITION_UPDATES_PER_RPC, hmsPartitions.size());
            synchronized (metastoreDdlLock_) {
                try {
                    // Alter partitions in bulk.
                    msClient.getHiveClient().alter_partitions(dbName, tableName, hmsPartitions.subList(i, numPartitionsToUpdate));
                } catch (TException e) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partitions"), e);
                }
            }
        }
    } finally {
        if (msClient != null)
            msClient.release();
    }
}
#method_after
private void bulkAlterPartitions(String dbName, String tableName, List<HdfsPartition> modifiedParts) throws ImpalaRuntimeException {
    MetaStoreClient msClient = null;
    List<org.apache.hadoop.hive.metastore.api.Partition> hmsPartitions = Lists.newArrayList();
    for (HdfsPartition p : modifiedParts) {
        org.apache.hadoop.hive.metastore.api.Partition msPart = p.toHmsPartition();
        if (msPart != null)
            hmsPartitions.add(msPart);
    }
    if (hmsPartitions.size() == 0)
        return;
    try {
        msClient = catalog_.getMetaStoreClient();
        // Apply the updates in batches of 'MAX_PARTITION_UPDATES_PER_RPC'.
        for (int i = 0; i < hmsPartitions.size(); i += MAX_PARTITION_UPDATES_PER_RPC) {
            int numPartitionsToUpdate = Math.min(i + MAX_PARTITION_UPDATES_PER_RPC, hmsPartitions.size());
            synchronized (metastoreDdlLock_) {
                try {
                    // Alter partitions in bulk.
                    msClient.getHiveClient().alter_partitions(dbName, tableName, hmsPartitions.subList(i, numPartitionsToUpdate));
                } catch (TException e) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partitions"), e);
                }
            }
        }
    } finally {
        if (msClient != null)
            msClient.release();
    }
}
#end_block

#method_before
public TResetMetadataResponse execResetMetadata(TResetMetadataRequest req) throws CatalogException {
    TResetMetadataResponse resp = new TResetMetadataResponse();
    resp.setResult(new TCatalogUpdateResult());
    resp.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (req.isSetTable_name()) {
        // Tracks any CatalogObjects updated/added/removed as a result of
        // the invalidate metadata or refresh call. For refresh() it is only expected
        // that a table be modified, but for invalidateTable() the table's parent database
        // may have also been added if it did not previously exist in the catalog.
        Pair<Db, Table> modifiedObjects = new Pair<Db, Table>(null, null);
        boolean wasRemoved = false;
        if (req.isIs_refresh()) {
            modifiedObjects.second = catalog_.reloadTable(req.getTable_name());
        } else {
            wasRemoved = catalog_.invalidateTable(req.getTable_name(), modifiedObjects);
        }
        if (modifiedObjects.first == null) {
            TCatalogObject thriftTable = TableToTCatalogObject(modifiedObjects.second);
            if (modifiedObjects.second != null) {
                // processed as a direct DDL operation.
                if (wasRemoved) {
                    resp.getResult().setRemoved_catalog_object(thriftTable);
                } else {
                    resp.getResult().setUpdated_catalog_object(thriftTable);
                }
            } else {
                // Table does not exist in the meta store and Impala catalog, throw error.
                throw new TableNotFoundException("Table not found: " + req.getTable_name().getDb_name() + "." + req.getTable_name().getTable_name());
            }
            resp.getResult().setVersion(thriftTable.getCatalog_version());
        } else {
            // If there were two catalog objects modified it indicates there was an
            // "invalidateTable()" call that added a new table AND database to the catalog.
            Preconditions.checkState(!req.isIs_refresh());
            Preconditions.checkNotNull(modifiedObjects.first);
            Preconditions.checkNotNull(modifiedObjects.second);
            // The database should always have a lower catalog version than the table because
            // it needs to be created before the table can be added.
            Preconditions.checkState(modifiedObjects.first.getCatalogVersion() < modifiedObjects.second.getCatalogVersion());
            // Since multiple catalog objects were modified, don't treat this as a direct DDL
            // operation. Just set the overall catalog version and the impalad will wait for
            // a statestore heartbeat that contains the update.
            resp.getResult().setVersion(modifiedObjects.second.getCatalogVersion());
        }
    } else {
        // Invalidate the entire catalog if no table name is provided.
        Preconditions.checkArgument(!req.isIs_refresh());
        catalog_.reset();
        resp.result.setVersion(catalog_.getCatalogVersion());
    }
    resp.getResult().setStatus(new TStatus(TStatusCode.OK, new ArrayList<String>()));
    return resp;
}
#method_after
public TResetMetadataResponse execResetMetadata(TResetMetadataRequest req) throws CatalogException {
    TResetMetadataResponse resp = new TResetMetadataResponse();
    resp.setResult(new TCatalogUpdateResult());
    resp.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (req.isSetTable_name()) {
        // Tracks any CatalogObjects updated/added/removed as a result of
        // the invalidate metadata or refresh call. For refresh() it is only expected
        // that a table be modified, but for invalidateTable() the table's parent database
        // may have also been added if it did not previously exist in the catalog.
        Pair<Db, Table> modifiedObjects = new Pair<Db, Table>(null, null);
        boolean wasRemoved = false;
        if (req.isIs_refresh()) {
            modifiedObjects.second = catalog_.reloadTable(req.getTable_name());
        } else {
            wasRemoved = catalog_.invalidateTable(req.getTable_name(), modifiedObjects);
        }
        if (modifiedObjects.first == null) {
            TCatalogObject thriftTable = TableToTCatalogObject(modifiedObjects.second);
            if (modifiedObjects.second != null) {
                // processed as a direct DDL operation.
                if (wasRemoved) {
                    resp.getResult().setRemoved_catalog_object(thriftTable);
                } else {
                    resp.getResult().setUpdated_catalog_object(thriftTable);
                }
            } else {
                // Table does not exist in the meta store and Impala catalog, throw error.
                throw new TableNotFoundException("Table not found: " + req.getTable_name().getDb_name() + "." + req.getTable_name().getTable_name());
            }
            resp.getResult().setVersion(thriftTable.getCatalog_version());
        } else {
            // If there were two catalog objects modified it indicates there was an
            // "invalidateTable()" call that added a new table AND database to the catalog.
            Preconditions.checkState(!req.isIs_refresh());
            Preconditions.checkNotNull(modifiedObjects.first);
            Preconditions.checkNotNull(modifiedObjects.second);
            // The database should always have a lower catalog version than the table because
            // it needs to be created before the table can be added.
            Preconditions.checkState(modifiedObjects.first.getCatalogVersion() < modifiedObjects.second.getCatalogVersion());
            // Since multiple catalog objects were modified, don't treat this as a direct DDL
            // operation. Just set the overall catalog version and the impalad will wait for
            // a statestore heartbeat that contains the update.
            resp.getResult().setVersion(modifiedObjects.second.getCatalogVersion());
        }
    } else {
        // Invalidate the entire catalog if no table name is provided.
        Preconditions.checkArgument(!req.isIs_refresh());
        catalog_.reset();
        resp.result.setVersion(catalog_.getCatalogVersion());
    }
    resp.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    return resp;
}
#end_block

#method_before
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    // Collects the cache directive IDs of any cached table/partitions that were
    // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
    // and the table will be refreshed asynchronously after all cache directives
    // complete.
    List<Long> cacheDirIds = Lists.<Long>newArrayList();
    // If the table is cached, get its cache pool name and replication factor. New
    // partitions will inherit this property.
    String cachePoolName = null;
    Short cacheReplication = 0;
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(table.getMetaStoreTable().getParameters());
    if (cacheDirId != null) {
        cachePoolName = HdfsCachingUtil.getCachePool(cacheDirId);
        cacheReplication = HdfsCachingUtil.getCacheReplication(cacheDirId);
        Preconditions.checkNotNull(cacheReplication);
        if (table.getNumClusteringCols() == 0)
            cacheDirIds.add(cacheDirId);
    }
    TableName tblName = new TableName(table.getDb().getName(), table.getName());
    AtomicBoolean addedNewPartition = new AtomicBoolean(false);
    if (table.getNumClusteringCols() > 0) {
        // Set of all partition names targeted by the insert that that need to be created
        // in the Metastore (partitions that do not currently exist in the catalog).
        // In the BE, we don't currently distinguish between which targeted partitions are
        // new and which already exist, so initialize the set with all targeted partition
        // names and remove the ones that are found to exist.
        Set<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
        for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
            // Skip dummy default partition.
            if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                continue;
            }
            // TODO: In the BE we build partition names without a trailing char. In FE we
            // build partition name with a trailing char. We should make this consistent.
            String partName = partition.getPartitionName() + "/";
            // returns true, it indicates the partition already exists.
            if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                // The partition was targeted by the insert and is also a cached. Since data
                // was written to the partition, a watch needs to be placed on the cache
                // cache directive so the TableLoadingMgr can perform an async refresh once
                // all data becomes cached.
                cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
            }
            if (partsToCreate.size() == 0)
                break;
        }
        if (!partsToCreate.isEmpty()) {
            SettableFuture<Void> allFinished = SettableFuture.create();
            AtomicInteger numPartitions = new AtomicInteger(partsToCreate.size());
            // Add all partitions to metastore.
            for (String partName : partsToCreate) {
                Preconditions.checkState(partName != null && !partName.isEmpty());
                CreatePartitionRunnable rbl = new CreatePartitionRunnable(tblName, partName, cachePoolName, addedNewPartition, allFinished, numPartitions, cacheDirIds, cacheReplication);
                executor_.execute(rbl);
            }
            try {
                // Will throw if any operation calls setException
                allFinished.get();
            } catch (Exception e) {
                throw new InternalException("Error updating metastore", e);
            }
        }
    }
    if (addedNewPartition.get()) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // Operate on a copy of msTbl to prevent our cached msTbl becoming inconsistent
            // if the alteration fails in the metastore.
            org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
            updateLastDdlTime(msTbl, msClient);
        } catch (Exception e) {
            throw new InternalException("Error updating lastDdlTime", e);
        } finally {
            msClient.release();
        }
    }
    // Submit the watch request for the given cache directives.
    if (!cacheDirIds.isEmpty())
        catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    response.getResult().setStatus(new TStatus(TStatusCode.OK, new ArrayList<String>()));
    // Perform an incremental refresh to load new/modified partitions and files.
    Table refreshedTbl = catalog_.reloadTable(tblName.toThrift());
    response.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    response.getResult().setVersion(response.getResult().getUpdated_catalog_object().getCatalog_version());
    return response;
}
#method_after
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    // Collects the cache directive IDs of any cached table/partitions that were
    // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
    // and the table will be refreshed asynchronously after all cache directives
    // complete.
    List<Long> cacheDirIds = Lists.<Long>newArrayList();
    // If the table is cached, get its cache pool name and replication factor. New
    // partitions will inherit this property.
    String cachePoolName = null;
    Short cacheReplication = 0;
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(table.getMetaStoreTable().getParameters());
    if (cacheDirId != null) {
        try {
            cachePoolName = HdfsCachingUtil.getCachePool(cacheDirId);
            cacheReplication = HdfsCachingUtil.getCacheReplication(cacheDirId);
            Preconditions.checkNotNull(cacheReplication);
            if (table.getNumClusteringCols() == 0)
                cacheDirIds.add(cacheDirId);
        } catch (ImpalaRuntimeException e) {
            // Catch the error so that the actual update to the catalog can progress,
            // this resets caching for the table though
            LOG.error(String.format("Cache directive %d was not found, uncache the table %s.%s" + "to remove this message.", cacheDirId, update.getDb_name(), update.getTarget_table()));
            cacheDirId = null;
        }
    }
    TableName tblName = new TableName(table.getDb().getName(), table.getName());
    AtomicBoolean addedNewPartition = new AtomicBoolean(false);
    if (table.getNumClusteringCols() > 0) {
        // Set of all partition names targeted by the insert that that need to be created
        // in the Metastore (partitions that do not currently exist in the catalog).
        // In the BE, we don't currently distinguish between which targeted partitions are
        // new and which already exist, so initialize the set with all targeted partition
        // names and remove the ones that are found to exist.
        Set<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
        for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
            // Skip dummy default partition.
            if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                continue;
            }
            // TODO: In the BE we build partition names without a trailing char. In FE we
            // build partition name with a trailing char. We should make this consistent.
            String partName = partition.getPartitionName() + "/";
            // returns true, it indicates the partition already exists.
            if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                // The partition was targeted by the insert and is also a cached. Since data
                // was written to the partition, a watch needs to be placed on the cache
                // cache directive so the TableLoadingMgr can perform an async refresh once
                // all data becomes cached.
                cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
            }
            if (partsToCreate.size() == 0)
                break;
        }
        if (!partsToCreate.isEmpty()) {
            SettableFuture<Void> allFinished = SettableFuture.create();
            AtomicInteger numPartitions = new AtomicInteger(partsToCreate.size());
            // Add all partitions to metastore.
            for (String partName : partsToCreate) {
                Preconditions.checkState(partName != null && !partName.isEmpty());
                CreatePartitionRunnable rbl = new CreatePartitionRunnable(tblName, partName, cachePoolName, addedNewPartition, allFinished, numPartitions, cacheDirIds, cacheReplication);
                executor_.execute(rbl);
            }
            try {
                // Will throw if any operation calls setException
                allFinished.get();
            } catch (Exception e) {
                throw new InternalException("Error updating metastore", e);
            }
        }
    }
    if (addedNewPartition.get()) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // Operate on a copy of msTbl to prevent our cached msTbl becoming inconsistent
            // if the alteration fails in the metastore.
            org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
            updateLastDdlTime(msTbl, msClient);
        } catch (Exception e) {
            throw new InternalException("Error updating lastDdlTime", e);
        } finally {
            msClient.release();
        }
    }
    // Submit the watch request for the given cache directives.
    if (!cacheDirIds.isEmpty())
        catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
    response.setResult(new TCatalogUpdateResult());
    response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    // Perform an incremental refresh to load new/modified partitions and files.
    Table refreshedTbl = catalog_.reloadTable(tblName.toThrift());
    response.getResult().setUpdated_catalog_object(TableToTCatalogObject(refreshedTbl));
    response.getResult().setVersion(response.getResult().getUpdated_catalog_object().getCatalog_version());
    return response;
}
#end_block

#method_before
private void loadBlockMetadata(FileSystem fs, FileStatus file, FileDescriptor fd, Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    Preconditions.checkNotNull(fd);
    Preconditions.checkNotNull(perFsFileBlocks);
    Preconditions.checkArgument(!file.isDirectory());
    LOG.debug("load block md for " + name_ + " file " + fd.getFileName());
    try {
        BlockLocation[] locations = fs.getFileBlockLocations(file, 0, file.getLen());
        Preconditions.checkNotNull(locations);
        // Loop over all blocks in the file.
        for (BlockLocation loc : locations) {
            Preconditions.checkNotNull(loc);
            // Get the location of all block replicas in ip:port format.
            String[] blockHostPorts = loc.getNames();
            // Get the hostnames for all block replicas. Used to resolve which hosts
            // contain cached data. The results are returned in the same order as
            // block.getNames() so it allows us to match a host specified as ip:port to
            // corresponding hostname using the same array index.
            String[] blockHostNames = loc.getHosts();
            Preconditions.checkState(blockHostNames.length == blockHostPorts.length);
            // Get the hostnames that contain cached replicas of this block.
            Set<String> cachedHosts = Sets.newHashSet(Arrays.asList(loc.getCachedHosts()));
            Preconditions.checkState(cachedHosts.size() <= blockHostNames.length);
            // Now enumerate all replicas of the block, adding any unknown hosts
            // to hostMap_/hostList_. The host ID (index in to the hostList_) for each
            // replica is stored in replicaHostIdxs.
            List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(blockHostPorts.length);
            for (int i = 0; i < blockHostPorts.length; ++i) {
                TNetworkAddress networkAddress = BlockReplica.parseLocation(blockHostPorts[i]);
                Preconditions.checkState(networkAddress != null);
                replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(blockHostNames[i])));
            }
            fd.addFileBlock(new FileBlock(loc.getOffset(), loc.getLength(), replicas));
        }
        // Remember the THdfsFileBlocks and corresponding BlockLocations.  Once all the
        // blocks are collected, the disk IDs will be queried in one batch per filesystem.
        addPerFsFileBlocks(perFsFileBlocks, fs, fd.getFileBlocks(), Arrays.asList(locations));
    } catch (IOException e) {
        throw new RuntimeException("couldn't determine block locations for path '" + file.getPath() + "':\n" + e.getMessage(), e);
    }
}
#method_after
private void loadBlockMetadata(FileSystem fs, FileStatus file, FileDescriptor fd, HdfsFileFormat fileFormat, Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    Preconditions.checkNotNull(fd);
    Preconditions.checkNotNull(perFsFileBlocks);
    Preconditions.checkArgument(!file.isDirectory());
    LOG.debug("load block md for " + name_ + " file " + fd.getFileName());
    if (!FileSystemUtil.hasGetFileBlockLocations(fs)) {
        synthesizeBlockMetadata(fs, fd, fileFormat);
        return;
    }
    try {
        BlockLocation[] locations = fs.getFileBlockLocations(file, 0, file.getLen());
        Preconditions.checkNotNull(locations);
        // Loop over all blocks in the file.
        for (BlockLocation loc : locations) {
            Preconditions.checkNotNull(loc);
            // Get the location of all block replicas in ip:port format.
            String[] blockHostPorts = loc.getNames();
            // Get the hostnames for all block replicas. Used to resolve which hosts
            // contain cached data. The results are returned in the same order as
            // block.getNames() so it allows us to match a host specified as ip:port to
            // corresponding hostname using the same array index.
            String[] blockHostNames = loc.getHosts();
            Preconditions.checkState(blockHostNames.length == blockHostPorts.length);
            // Get the hostnames that contain cached replicas of this block.
            Set<String> cachedHosts = Sets.newHashSet(Arrays.asList(loc.getCachedHosts()));
            Preconditions.checkState(cachedHosts.size() <= blockHostNames.length);
            // Now enumerate all replicas of the block, adding any unknown hosts
            // to hostMap_/hostList_. The host ID (index in to the hostList_) for each
            // replica is stored in replicaHostIdxs.
            List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(blockHostPorts.length);
            for (int i = 0; i < blockHostPorts.length; ++i) {
                TNetworkAddress networkAddress = BlockReplica.parseLocation(blockHostPorts[i]);
                Preconditions.checkState(networkAddress != null);
                replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(blockHostNames[i])));
            }
            fd.addFileBlock(new FileBlock(loc.getOffset(), loc.getLength(), replicas));
        }
        // Remember the THdfsFileBlocks and corresponding BlockLocations.  Once all the
        // blocks are collected, the disk IDs will be queried in one batch per filesystem.
        addPerFsFileBlocks(perFsFileBlocks, fs, fd.getFileBlocks(), Arrays.asList(locations));
    } catch (IOException e) {
        throw new RuntimeException("couldn't determine block locations for path '" + file.getPath() + "':\n" + e.getMessage(), e);
    }
}
#end_block

#method_before
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    if (!SUPPORTS_VOLUME_ID)
        return;
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        if (!(fs instanceof DistributedFileSystem))
            continue;
        LOG.trace("Loading disk ids for: " + getFullName() + ". nodes: " + getNumNodes() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        BlockStorageLocation[] storageLocs = null;
        try {
            // Get the BlockStorageLocations for all the blocks
            storageLocs = dfs.getFileBlockStorageLocations(blockLists.locations);
        } catch (IOException e) {
            LOG.error("Couldn't determine block storage locations for filesystem " + fs + ":\n" + e.getMessage());
            continue;
        }
        if (storageLocs == null || storageLocs.length == 0) {
            LOG.warn("Attempted to get block locations for filesystem " + fs + " but the call returned no results");
            continue;
        }
        if (storageLocs.length != blockLists.locations.size()) {
            // Block locations and storage locations didn't match up.
            LOG.error("Number of block storage locations not equal to number of blocks: " + "#storage locations=" + Long.toString(storageLocs.length) + " #blocks=" + Long.toString(blockLists.locations.size()));
            continue;
        }
        long unknownDiskIdCount = 0;
        // THdfsFileBlocks.
        for (int locIdx = 0; locIdx < storageLocs.length; ++locIdx) {
            VolumeId[] volumeIds = storageLocs[locIdx].getVolumeIds();
            THdfsFileBlock block = blockLists.blocks.get(locIdx);
            // Convert opaque VolumeId to 0 based ids.
            // TODO: the diskId should be eventually retrievable from Hdfs when the
            // community agrees this API is useful.
            int[] diskIds = new int[volumeIds.length];
            for (int i = 0; i < volumeIds.length; ++i) {
                diskIds[i] = getDiskId(volumeIds[i]);
                if (diskIds[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIds, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#method_after
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    if (!SUPPORTS_VOLUME_ID)
        return;
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        // part of the FileSystem interface, so we'll need to downcast.
        if (!(fs instanceof DistributedFileSystem))
            continue;
        LOG.trace("Loading disk ids for: " + getFullName() + ". nodes: " + getNumNodes() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        BlockStorageLocation[] storageLocs = null;
        try {
            // Get the BlockStorageLocations for all the blocks
            storageLocs = dfs.getFileBlockStorageLocations(blockLists.locations);
        } catch (IOException e) {
            LOG.error("Couldn't determine block storage locations for filesystem " + fs + ":\n" + e.getMessage());
            continue;
        }
        if (storageLocs == null || storageLocs.length == 0) {
            LOG.warn("Attempted to get block locations for filesystem " + fs + " but the call returned no results");
            continue;
        }
        if (storageLocs.length != blockLists.locations.size()) {
            // Block locations and storage locations didn't match up.
            LOG.error("Number of block storage locations not equal to number of blocks: " + "#storage locations=" + Long.toString(storageLocs.length) + " #blocks=" + Long.toString(blockLists.locations.size()));
            continue;
        }
        long unknownDiskIdCount = 0;
        // THdfsFileBlocks.
        for (int locIdx = 0; locIdx < storageLocs.length; ++locIdx) {
            VolumeId[] volumeIds = storageLocs[locIdx].getVolumeIds();
            THdfsFileBlock block = blockLists.blocks.get(locIdx);
            // Convert opaque VolumeId to 0 based ids.
            // TODO: the diskId should be eventually retrievable from Hdfs when the
            // community agrees this API is useful.
            int[] diskIds = new int[volumeIds.length];
            for (int i = 0; i < volumeIds.length; ++i) {
                diskIds[i] = getDiskId(volumeIds[i]);
                if (diskIds[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIds, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#end_block

#method_before
private void loadPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl, Map<String, List<FileDescriptor>> oldFileDescMap) throws IOException, CatalogException {
    resetPartitionMd();
    partitions_.clear();
    hdfsBaseDir_ = msTbl.getSd().getLocation();
    // Map of filesystem to the file blocks for new/modified FileDescriptors. Blocks in
    // this map will have their disk volume IDs information (re)loaded. This is used to
    // speed up the incremental refresh of a table's metadata by skipping unmodified,
    // previously loaded blocks.
    Map<FsKey, FileBlocksInfo> blocksToLoad = Maps.newHashMap();
    // INSERT statements need to refer to this if they try to write to new partitions
    // Scans don't refer to this because by definition all partitions they refer to
    // exist.
    addDefaultPartition(msTbl.getSd());
    // We silently ignore cache directives that no longer exist in HDFS
    isMarkedCached_ = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters()) != null;
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null, oldFileDescMap, blocksToLoad);
        addPartition(part);
        if (isMarkedCached_)
            part.markCached();
        Path location = new Path(hdfsBaseDir_);
        FileSystem fs = location.getFileSystem(CONF);
        if (fs.exists(location)) {
            accessLevel_ = getAvailableAccessLevel(fs, location);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, oldFileDescMap, blocksToLoad);
            addPartition(partition);
            // this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null)
                ;
            {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
        }
    }
    loadDiskIds(blocksToLoad);
}
#method_after
private void loadPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl, Map<String, List<FileDescriptor>> oldFileDescMap) throws IOException, CatalogException {
    resetPartitionMd();
    partitions_.clear();
    hdfsBaseDir_ = msTbl.getSd().getLocation();
    // Map of filesystem to the file blocks for new/modified FileDescriptors. Blocks in
    // this map will have their disk volume IDs information (re)loaded. This is used to
    // speed up the incremental refresh of a table's metadata by skipping unmodified,
    // previously loaded blocks.
    Map<FsKey, FileBlocksInfo> blocksToLoad = Maps.newHashMap();
    // INSERT statements need to refer to this if they try to write to new partitions
    // Scans don't refer to this because by definition all partitions they refer to
    // exist.
    addDefaultPartition(msTbl.getSd());
    // We silently ignore cache directives that no longer exist in HDFS, and remove
    // non-existing cache directives from the parameters.
    isMarkedCached_ = HdfsCachingUtil.validateCacheParams(msTbl.getParameters());
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null, oldFileDescMap, blocksToLoad);
        addPartition(part);
        if (isMarkedCached_)
            part.markCached();
        Path location = new Path(hdfsBaseDir_);
        FileSystem fs = location.getFileSystem(CONF);
        if (fs.exists(location)) {
            accessLevel_ = getAvailableAccessLevel(fs, location);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, oldFileDescMap, blocksToLoad);
            addPartition(partition);
            // this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null)
                ;
            {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
        }
    }
    loadDiskIds(blocksToLoad);
}
#end_block

#method_before
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition, Map<String, List<FileDescriptor>> oldFileDescMap, Map<FsKey, FileBlocksInfo> perFsFileBlocks) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    Path partDirPath = new Path(storageDescriptor.getLocation());
    List<FileDescriptor> fileDescriptors = Lists.newArrayList();
    // If the partition is marked as cached, the block location metadata must be
    // reloaded, even if the file times have not changed.
    boolean isMarkedCached = isMarkedCached_;
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        isMarkedCached = HdfsCachingUtil.getCacheDirectiveId(msPartition.getParameters()) != null;
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        try {
            Expr.analyze(keyValues, null);
        } catch (AnalysisException e) {
            // should never happen
            throw new IllegalStateException(e);
        }
    }
    try {
        // Each partition could reside on a different filesystem.
        FileSystem fs = partDirPath.getFileSystem(CONF);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (fs.exists(partDirPath)) {
            // fs.listStatus() to list all the files.
            for (FileStatus fileStatus : fs.listStatus(partDirPath)) {
                String fileName = fileStatus.getPath().getName().toString();
                if (fileStatus.isDirectory() || FileSystemUtil.isHiddenFile(fileName) || HdfsCompression.fromFileName(fileName) == HdfsCompression.LZO_INDEX) {
                    // Skip index files, these are read by the LZO scanner directly.
                    continue;
                }
                String partitionDir = fileStatus.getPath().getParent().toString();
                FileDescriptor fd = null;
                // is found, it will be chosen as a candidate to reuse.
                if (oldFileDescMap != null && oldFileDescMap.get(partitionDir) != null) {
                    for (FileDescriptor oldFileDesc : oldFileDescMap.get(partitionDir)) {
                        if (oldFileDesc.getFileName().equals(fileName)) {
                            fd = oldFileDesc;
                            break;
                        }
                    }
                }
                // value can be reused.
                if (fd == null || isMarkedCached || fd.getFileLength() != fileStatus.getLen() || fd.getModificationTime() != fileStatus.getModificationTime()) {
                    // Create a new file descriptor and load the file block metadata,
                    // collecting the block metadata into perFsFileBlocks.  The disk IDs for
                    // all the blocks of each filesystem will be loaded by loadDiskIds().
                    fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
                    loadBlockMetadata(fs, fileStatus, fd, perFsFileBlocks);
                }
                List<FileDescriptor> fds = fileDescMap_.get(partitionDir);
                if (fds == null) {
                    fds = Lists.newArrayList();
                    fileDescMap_.put(partitionDir, fds);
                }
                fds.add(fd);
                // Add to the list of FileDescriptors for this partition.
                fileDescriptors.add(fd);
            }
            numHdfsFiles_ += fileDescriptors.size();
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, fileDescriptors, getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (Exception e) {
        throw new CatalogException("Failed to create partition: ", e);
    }
}
#method_after
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition, Map<String, List<FileDescriptor>> oldFileDescMap, Map<FsKey, FileBlocksInfo> perFsFileBlocks) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    Path partDirPath = new Path(storageDescriptor.getLocation());
    List<FileDescriptor> fileDescriptors = Lists.newArrayList();
    // If the partition is marked as cached, the block location metadata must be
    // reloaded, even if the file times have not changed.
    boolean isMarkedCached = isMarkedCached_;
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        isMarkedCached = HdfsCachingUtil.validateCacheParams(msPartition.getParameters());
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        try {
            Expr.analyze(keyValues, null);
        } catch (AnalysisException e) {
            // should never happen
            throw new IllegalStateException(e);
        }
    }
    try {
        // Each partition could reside on a different filesystem.
        FileSystem fs = partDirPath.getFileSystem(CONF);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (fs.exists(partDirPath)) {
            // fs.listStatus() to list all the files.
            for (FileStatus fileStatus : fs.listStatus(partDirPath)) {
                String fileName = fileStatus.getPath().getName().toString();
                if (fileStatus.isDirectory() || FileSystemUtil.isHiddenFile(fileName) || HdfsCompression.fromFileName(fileName) == HdfsCompression.LZO_INDEX) {
                    // Skip index files, these are read by the LZO scanner directly.
                    continue;
                }
                String partitionDir = fileStatus.getPath().getParent().toString();
                FileDescriptor fd = null;
                // is found, it will be chosen as a candidate to reuse.
                if (oldFileDescMap != null && oldFileDescMap.get(partitionDir) != null) {
                    for (FileDescriptor oldFileDesc : oldFileDescMap.get(partitionDir)) {
                        if (oldFileDesc.getFileName().equals(fileName)) {
                            fd = oldFileDesc;
                            break;
                        }
                    }
                }
                // value can be reused.
                if (fd == null || isMarkedCached || fd.getFileLength() != fileStatus.getLen() || fd.getModificationTime() != fileStatus.getModificationTime()) {
                    // Create a new file descriptor and load the file block metadata,
                    // collecting the block metadata into perFsFileBlocks.  The disk IDs for
                    // all the blocks of each filesystem will be loaded by loadDiskIds().
                    fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
                    loadBlockMetadata(fs, fileStatus, fd, fileFormatDescriptor.getFileFormat(), perFsFileBlocks);
                }
                List<FileDescriptor> fds = fileDescMap_.get(partitionDir);
                if (fds == null) {
                    fds = Lists.newArrayList();
                    fileDescMap_.put(partitionDir, fds);
                }
                fds.add(fd);
                // Add to the list of FileDescriptors for this partition.
                fileDescriptors.add(fd);
            }
            numHdfsFiles_ += fileDescriptors.size();
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, fileDescriptors, getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (Exception e) {
        throw new CatalogException("Failed to create partition: ", e);
    }
}
#end_block

#method_before
@Override
public /**
 * Load the table metadata and reuse metadata to speed up metadata loading.
 * If the lastDdlTime has not been changed, that means the Hive metastore metadata has
 * not been changed. Reuses the old Hive partition metadata from cachedEntry.
 * To speed up Hdfs metadata loading, if a file's mtime has not been changed, reuses
 * the old file block metadata from old value.
 *
 * There are several cases where the cachedEntry might be reused incorrectly:
 * 1. an ALTER TABLE ADD PARTITION or dynamic partition insert is executed through
 *    Hive. This does not update the lastDdlTime.
 * 2. Hdfs rebalancer is executed. This changes the block locations but won't update
 *    the mtime (file modification time).
 * If any of these occurs, user has to execute "invalidate metadata" to invalidate the
 * metadata cache of the table to trigger a fresh load.
 */
void load(Table cachedEntry, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    LOG.debug("load table: " + db_.getName() + "." + name_);
    // turn all exceptions into TableLoadingException
    try {
        // set nullPartitionKeyValue from the hive conf.
        nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
        // set NULL indicator string from table properties
        nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
        if (nullColumnValue_ == null)
            nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
        // populate with both partition keys and regular columns
        List<FieldSchema> partKeys = msTbl.getPartitionKeys();
        List<FieldSchema> tblFields = Lists.newArrayList();
        String inputFormat = msTbl.getSd().getInputFormat();
        if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO) {
            // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
            // taking precedence.
            List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
            schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
            schemaSearchLocations.add(getMetaStoreTable().getParameters());
            avroSchema_ = HdfsTable.getAvroSchema(schemaSearchLocations, getFullName());
            String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
            if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
                // If the SerDe library is null or set to LazySimpleSerDe or is null, it
                // indicates there is an issue with the table metadata since Avro table need a
                // non-native serde. Instead of failing to load the table, fall back to
                // using the fields from the storage descriptor (same as Hive).
                tblFields.addAll(msTbl.getSd().getCols());
            } else {
                // Load the fields from the Avro schema.
                // Since Avro does not include meta-data for CHAR or VARCHAR, an Avro type of
                // "string" is used for CHAR, VARCHAR and STRING. Default back to the storage
                // descriptor to determine the the type for "string"
                List<FieldSchema> sdTypes = msTbl.getSd().getCols();
                int i = 0;
                List<Column> avroTypeList = AvroSchemaParser.parse(avroSchema_);
                boolean canFallBack = sdTypes.size() == avroTypeList.size();
                for (Column parsedCol : avroTypeList) {
                    FieldSchema fs = new FieldSchema();
                    fs.setName(parsedCol.getName());
                    String avroType = parsedCol.getType().toSql();
                    if (avroType.toLowerCase().equals("string") && canFallBack) {
                        fs.setType(sdTypes.get(i).getType());
                    } else {
                        fs.setType(avroType);
                    }
                    fs.setComment("from deserializer");
                    tblFields.add(fs);
                    i++;
                }
            }
        } else {
            tblFields.addAll(msTbl.getSd().getCols());
        }
        List<FieldSchema> fieldSchemas = new ArrayList<FieldSchema>(partKeys.size() + tblFields.size());
        fieldSchemas.addAll(partKeys);
        fieldSchemas.addAll(tblFields);
        // The number of clustering columns is the number of partition keys.
        numClusteringCols_ = partKeys.size();
        loadColumns(fieldSchemas, client);
        // Collect the list of partitions to use for the table. Partitions may be reused
        // from the existing cached table entry (if one exists), read from the metastore,
        // or a mix of both. Whether or not a partition is reused depends on whether
        // the table or partition has been modified.
        List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
        if (cachedEntry == null || !(cachedEntry instanceof HdfsTable) || cachedEntry.lastDdlTime_ != lastDdlTime_) {
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
        } else {
            // The table was already in the metadata cache and it has not been modified.
            Preconditions.checkArgument(cachedEntry instanceof HdfsTable);
            HdfsTable cachedHdfsTableEntry = (HdfsTable) cachedEntry;
            // Set of partition names that have been modified. Partitions in this Set need to
            // be reloaded from the metastore.
            Set<String> modifiedPartitionNames = Sets.newHashSet();
            // "temp" table that doesn't actually exist in the metastore.
            if (cachedEntry != this) {
                // Since the table has not been modified, we might be able to reuse some of the
                // old partition metadata if the individual partitions have not been modified.
                // First get a list of all the partition names for this table from the
                // metastore, this is much faster than listing all the Partition objects.
                modifiedPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
            }
            int totalPartitions = modifiedPartitionNames.size();
            // Get all the partitions from the cached entry that have not been modified.
            for (HdfsPartition cachedPart : cachedHdfsTableEntry.getPartitions()) {
                // Skip the default partition and any partitions that have been modified.
                if (cachedPart.isDirty() || cachedPart.isDefaultPartition()) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition cachedMsPart = cachedPart.toHmsPartition();
                Preconditions.checkNotNull(cachedMsPart);
                // This is a partition we already know about and it hasn't been modified.
                // No need to reload the metadata.
                String cachedPartName = cachedPart.getPartitionName();
                if (modifiedPartitionNames.contains(cachedPartName)) {
                    msPartitions.add(cachedMsPart);
                    modifiedPartitionNames.remove(cachedPartName);
                }
            }
            LOG.info(String.format("Incrementally refreshing %d/%d partitions.", modifiedPartitionNames.size(), totalPartitions));
            // No need to make the metastore call if no partitions are to be updated.
            if (modifiedPartitionNames.size() > 0) {
                // Now reload the the remaining partitions.
                msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(modifiedPartitionNames), db_.getName(), name_));
            }
        }
        Map<String, List<FileDescriptor>> oldFileDescMap = null;
        if (cachedEntry != null && cachedEntry instanceof HdfsTable) {
            HdfsTable cachedHdfsTable = (HdfsTable) cachedEntry;
            oldFileDescMap = cachedHdfsTable.fileDescMap_;
            hostIndex_.populate(cachedHdfsTable.hostIndex_.getList());
        }
        loadPartitions(msPartitions, msTbl, oldFileDescMap);
        // load table stats
        numRows_ = getRowCount(msTbl.getParameters());
        LOG.debug("table #rows=" + Long.toString(numRows_));
        // to the table's numRows.
        if (numClusteringCols_ == 0 && !partitions_.isEmpty()) {
            // Unpartitioned tables have a 'dummy' partition and a default partition.
            // Temp tables used in CTAS statements have one partition.
            Preconditions.checkState(partitions_.size() == 2 || partitions_.size() == 1);
            for (HdfsPartition p : partitions_) {
                p.setNumRows(numRows_);
            }
        }
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#method_after
@Override
public /**
 * Load the table metadata and reuse metadata to speed up metadata loading.
 * If the lastDdlTime has not been changed, that means the Hive metastore metadata has
 * not been changed. Reuses the old Hive partition metadata from cachedEntry.
 * To speed up Hdfs metadata loading, if a file's mtime has not been changed, reuses
 * the old file block metadata from old value.
 *
 * There are several cases where the cachedEntry might be reused incorrectly:
 * 1. an ALTER TABLE ADD PARTITION or dynamic partition insert is executed through
 *    Hive. This does not update the lastDdlTime.
 * 2. Hdfs rebalancer is executed. This changes the block locations but won't update
 *    the mtime (file modification time).
 * If any of these occurs, user has to execute "invalidate metadata" to invalidate the
 * metadata cache of the table to trigger a fresh load.
 */
void load(Table cachedEntry, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
    LOG.debug("load table: " + db_.getName() + "." + name_);
    // turn all exceptions into TableLoadingException
    try {
        // set nullPartitionKeyValue from the hive conf.
        nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
        // set NULL indicator string from table properties
        nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
        if (nullColumnValue_ == null)
            nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
        // populate with both partition keys and regular columns
        List<FieldSchema> partKeys = msTbl.getPartitionKeys();
        List<FieldSchema> tblFields = Lists.newArrayList();
        String inputFormat = msTbl.getSd().getInputFormat();
        if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO) {
            // Look for the schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
            // taking precedence.
            List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
            schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
            schemaSearchLocations.add(getMetaStoreTable().getParameters());
            avroSchema_ = HdfsTable.getAvroSchema(schemaSearchLocations, getFullName());
            String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
            if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
                // If the SerDe library is null or set to LazySimpleSerDe or is null, it
                // indicates there is an issue with the table metadata since Avro table need a
                // non-native serde. Instead of failing to load the table, fall back to
                // using the fields from the storage descriptor (same as Hive).
                tblFields.addAll(msTbl.getSd().getCols());
            } else {
                // Load the fields from the Avro schema.
                // Since Avro does not include meta-data for CHAR or VARCHAR, an Avro type of
                // "string" is used for CHAR, VARCHAR and STRING. Default back to the storage
                // descriptor to determine the the type for "string"
                List<FieldSchema> sdTypes = msTbl.getSd().getCols();
                int i = 0;
                List<Column> avroTypeList = AvroSchemaParser.parse(avroSchema_);
                boolean canFallBack = sdTypes.size() == avroTypeList.size();
                for (Column parsedCol : avroTypeList) {
                    FieldSchema fs = new FieldSchema();
                    fs.setName(parsedCol.getName());
                    String avroType = parsedCol.getType().toSql();
                    if (avroType.toLowerCase().equals("string") && canFallBack) {
                        fs.setType(sdTypes.get(i).getType());
                    } else {
                        fs.setType(avroType);
                    }
                    fs.setComment("from deserializer");
                    tblFields.add(fs);
                    i++;
                }
            }
        } else {
            tblFields.addAll(msTbl.getSd().getCols());
        }
        List<FieldSchema> fieldSchemas = new ArrayList<FieldSchema>(partKeys.size() + tblFields.size());
        fieldSchemas.addAll(partKeys);
        fieldSchemas.addAll(tblFields);
        // The number of clustering columns is the number of partition keys.
        numClusteringCols_ = partKeys.size();
        loadColumns(fieldSchemas, client);
        // Collect the list of partitions to use for the table. Partitions may be reused
        // from the existing cached table entry (if one exists), read from the metastore,
        // or a mix of both. Whether or not a partition is reused depends on whether
        // the table or partition has been modified.
        List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
        if (cachedEntry == null || !(cachedEntry instanceof HdfsTable) || cachedEntry.lastDdlTime_ != lastDdlTime_) {
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
        } else {
            // The table was already in the metadata cache and it has not been modified.
            Preconditions.checkArgument(cachedEntry instanceof HdfsTable);
            HdfsTable cachedHdfsTableEntry = (HdfsTable) cachedEntry;
            // Set of partition names that have been modified. Partitions in this Set need to
            // be reloaded from the metastore.
            Set<String> modifiedPartitionNames = Sets.newHashSet();
            // "temp" table that doesn't actually exist in the metastore.
            if (cachedEntry != this) {
                // Since the table has not been modified, we might be able to reuse some of the
                // old partition metadata if the individual partitions have not been modified.
                // First get a list of all the partition names for this table from the
                // metastore, this is much faster than listing all the Partition objects.
                modifiedPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
            }
            int totalPartitions = modifiedPartitionNames.size();
            // Get all the partitions from the cached entry that have not been modified.
            for (HdfsPartition cachedPart : cachedHdfsTableEntry.getPartitions()) {
                // Skip the default partition and any partitions that have been modified.
                if (cachedPart.isDirty() || cachedPart.isDefaultPartition()) {
                    continue;
                }
                org.apache.hadoop.hive.metastore.api.Partition cachedMsPart = cachedPart.toHmsPartition();
                if (cachedMsPart == null)
                    continue;
                // This is a partition we already know about and it hasn't been modified.
                // No need to reload the metadata.
                String cachedPartName = cachedPart.getPartitionName();
                if (modifiedPartitionNames.contains(cachedPartName)) {
                    msPartitions.add(cachedMsPart);
                    modifiedPartitionNames.remove(cachedPartName);
                }
            }
            LOG.info(String.format("Incrementally refreshing %d/%d partitions.", modifiedPartitionNames.size(), totalPartitions));
            // No need to make the metastore call if no partitions are to be updated.
            if (modifiedPartitionNames.size() > 0) {
                // Now reload the the remaining partitions.
                msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(modifiedPartitionNames), db_.getName(), name_));
            }
        }
        Map<String, List<FileDescriptor>> oldFileDescMap = null;
        if (cachedEntry != null && cachedEntry instanceof HdfsTable) {
            HdfsTable cachedHdfsTable = (HdfsTable) cachedEntry;
            oldFileDescMap = cachedHdfsTable.fileDescMap_;
            hostIndex_.populate(cachedHdfsTable.hostIndex_.getList());
        }
        loadPartitions(msPartitions, msTbl, oldFileDescMap);
        // load table stats
        numRows_ = getRowCount(msTbl.getParameters());
        LOG.debug("table #rows=" + Long.toString(numRows_));
        // to the table's numRows.
        if (numClusteringCols_ == 0 && !partitions_.isEmpty()) {
            // Unpartitioned tables have a 'dummy' partition and a default partition.
            // Temp tables used in CTAS statements have one partition.
            Preconditions.checkState(partitions_.size() == 2 || partitions_.size() == 1);
            for (HdfsPartition p : partitions_) {
                p.setNumRows(numRows_);
            }
        }
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#end_block

#method_before
public static Short getCachedCacheReplication(Map<String, String> params) {
    Preconditions.checkNotNull(params);
    String replication = params.get(CACHE_DIR_REPLICATION_PROP_NAME);
    // For compatibility with tables created before allowing a custom replication factor
    if (replication == null) {
        return JniCatalogConstants.HDFS_DEFAULT_CACHE_REPLICATION_FACTOR;
    }
    try {
        return Short.parseShort(replication);
    } catch (NumberFormatException e) {
        return JniCatalogConstants.HDFS_DEFAULT_CACHE_REPLICATION_FACTOR;
    }
}
#method_after
public static Short getCachedCacheReplication(Map<String, String> params) {
    Preconditions.checkNotNull(params);
    String replication = params.get(CACHE_DIR_REPLICATION_PROP_NAME);
    if (replication == null) {
        return JniCatalogConstants.HDFS_DEFAULT_CACHE_REPLICATION_FACTOR;
    }
    try {
        return Short.parseShort(replication);
    } catch (NumberFormatException e) {
        return JniCatalogConstants.HDFS_DEFAULT_CACHE_REPLICATION_FACTOR;
    }
}
#end_block

#method_before
private static CacheDirectiveEntry getDirective(long directiveId) throws ImpalaRuntimeException {
    LOG.trace("Getting cache directive id: " + directiveId);
    CacheDirectiveInfo filter = new CacheDirectiveInfo.Builder().setId(directiveId).build();
    try {
        RemoteIterator<CacheDirectiveEntry> itr = dfs.listCacheDirectives(filter);
        while (itr.hasNext()) {
            CacheDirectiveEntry entry = itr.next();
            if (entry.getInfo().getId() == directiveId)
                return entry;
        }
    } catch (IOException e) {
        throw new ImpalaRuntimeException(e.getMessage(), e);
    }
    return null;
}
#method_after
private static CacheDirectiveEntry getDirective(long directiveId) throws ImpalaRuntimeException {
    LOG.trace("Getting cache directive id: " + directiveId);
    CacheDirectiveInfo filter = new CacheDirectiveInfo.Builder().setId(directiveId).build();
    try {
        RemoteIterator<CacheDirectiveEntry> itr = dfs.listCacheDirectives(filter);
        if (itr.hasNext())
            return itr.next();
    } catch (IOException e) {
        // Handle connection issues with e.g. HDFS and possible not found errors
        throw new ImpalaRuntimeException(e.getMessage(), e);
    }
    throw new ImpalaRuntimeException("HDFS cache directive filter returned empty result. This must not happen");
}
#end_block

#method_before
public String getPartitionName() {
    List<String> partitionCols = Lists.newArrayList();
    List<String> partitionValues = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(getTable().getColumns().get(i).getName());
    }
    return org.apache.hadoop.hive.common.FileUtils.makePartName(partitionCols, getPartitionValuesAsStrings());
}
#method_after
public String getPartitionName() {
    List<String> partitionCols = Lists.newArrayList();
    List<String> partitionValues = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(getTable().getColumns().get(i).getName());
    }
    return org.apache.hadoop.hive.common.FileUtils.makePartName(partitionCols, getPartitionValuesAsStrings(true));
}
#end_block

#method_before
public List<String> getPartitionValuesAsStrings() {
    List<String> ret = Lists.newArrayList();
    for (LiteralExpr partValue : getPartitionValues()) {
        ret.add(PartitionKeyValue.getPartitionKeyValueString(partValue, getTable().getNullPartitionKeyValue()));
    }
    return ret;
}
#method_after
public List<String> getPartitionValuesAsStrings(boolean mapNullsToHiveKey) {
    List<String> ret = Lists.newArrayList();
    for (LiteralExpr partValue : getPartitionValues()) {
        if (mapNullsToHiveKey) {
            ret.add(PartitionKeyValue.getPartitionKeyValueString(partValue, getTable().getNullPartitionKeyValue()));
        } else {
            ret.add(partValue.getStringValue());
        }
    }
    return ret;
}
#end_block

#method_before
public org.apache.hadoop.hive.metastore.api.SerDeInfo getSerdeInfo() {
    return cachedHiveDescriptor_.sdSerdeInfo;
}
#method_after
public org.apache.hadoop.hive.metastore.api.SerDeInfo getSerdeInfo() {
    return cachedMsPartitionDescriptor_.sdSerdeInfo;
}
#end_block

#method_before
public org.apache.hadoop.hive.metastore.api.Partition toHmsPartition() {
    org.apache.hadoop.hive.metastore.api.StorageDescriptor storageDescriptor = new org.apache.hadoop.hive.metastore.api.StorageDescriptor(table_.getFieldSchemas(), location_, fileFormatDescriptor_.getFileFormat().toJavaClassName(), cachedHiveDescriptor_.sdOutputFormat, cachedHiveDescriptor_.sdCompressed, cachedHiveDescriptor_.sdNumBuckets, cachedHiveDescriptor_.sdSerdeInfo, cachedHiveDescriptor_.sdBucketCols, cachedHiveDescriptor_.sdSortCols, cachedHiveDescriptor_.sdParameters);
    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition(getPartitionValuesAsStrings(), getTable().getDb().getName(), getTable().getName(), cachedHiveDescriptor_.msCreateTime, cachedHiveDescriptor_.msLastAccessTime, storageDescriptor, getParameters());
    return partition;
}
#method_after
public org.apache.hadoop.hive.metastore.api.Partition toHmsPartition() {
    if (cachedMsPartitionDescriptor_ == null)
        return null;
    org.apache.hadoop.hive.metastore.api.StorageDescriptor storageDescriptor = new org.apache.hadoop.hive.metastore.api.StorageDescriptor(table_.getFieldSchemas(), location_, fileFormatDescriptor_.getFileFormat().toJavaClassName(), cachedMsPartitionDescriptor_.sdOutputFormat, cachedMsPartitionDescriptor_.sdCompressed, cachedMsPartitionDescriptor_.sdNumBuckets, cachedMsPartitionDescriptor_.sdSerdeInfo, cachedMsPartitionDescriptor_.sdBucketCols, cachedMsPartitionDescriptor_.sdSortCols, cachedMsPartitionDescriptor_.sdParameters);
    org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition(getPartitionValuesAsStrings(true), getTable().getDb().getName(), getTable().getName(), cachedMsPartitionDescriptor_.msCreateTime, cachedMsPartitionDescriptor_.msLastAccessTime, storageDescriptor, getParameters());
    return partition;
}
#end_block

#method_before
// TODO: implement this
/**
 * Substitute all the expressions (grouping expr, aggregate expr) and update our
 * substitution map according to the given substitution map:
 * - smap typically maps from tuple t1 to tuple t2 (example: the smap of an
 *   inline view maps the virtual table ref t1 into a base table ref t2)
 * - our grouping and aggregate exprs need to be substituted with the given
 *   smap so that they also reference t2
 * - aggTupleSMap needs to be recomputed to map exprs based on t2
 *   onto our aggTupleDesc (ie, the left-hand side needs to be substituted with
 *   smap)
 * - mergeAggInfo: this is not affected, because
 *   * its grouping and aggregate exprs only reference aggTupleDesc_
 *   * its smap is identical to aggTupleSMap_
 * - 2ndPhaseDistinctAggInfo:
 *   * its grouping and aggregate exprs also only reference aggTupleDesc_
 *     and are therefore not affected
 *   * its smap needs to be recomputed to map exprs based on t2 to its own
 *     aggTupleDesc
 *  public void substitute(ExprSubstitutionMap smap, Analyzer analyzer)
 *      throws InternalException {
 *    groupingExprs_ = Expr.substituteList(groupingExprs_, smap, analyzer);
 *    LOG.trace("AggInfo: grouping_exprs=" + Expr.debugString(groupingExprs_));
 *
 *    // The smap in this case should not substitute the aggs themselves, only
 *    // their subexpressions.
 *    List<Expr> substitutedAggs = Expr.substituteList(aggregateExprs_, smap, analyzer);
 *    aggregateExprs_.clear();
 *    for (Expr substitutedAgg: substitutedAggs) {
 *      aggregateExprs_.add((FunctionCallExpr) substitutedAgg);
 *    }
 *
 *    LOG.trace("AggInfo: agg_exprs=" + Expr.debugString(aggregateExprs_));
 *    aggTupleSMap_.substituteLhs(smap, analyzer);
 *    if (secondPhaseDistinctAggInfo_ != null) {
 *      secondPhaseDistinctAggInfo_.substitute(smap, analyzer);
 *    }
 *  }
 */
@Override
public void materializeRequiredSlots(Analyzer analyzer, ExprSubstitutionMap smap) {
    materializedSlots_.clear();
    List<Expr> exprs = Lists.newArrayList();
    for (int i = 0; i < analyticExprs_.size(); ++i) {
        SlotDescriptor outputSlotDesc = outputTupleDesc_.getSlots().get(i);
        if (!outputSlotDesc.isMaterialized())
            continue;
        intermediateTupleDesc_.getSlots().get(i).setIsMaterialized(true);
        exprs.add(analyticExprs_.get(i));
        materializedSlots_.add(i);
    }
    List<Expr> resolvedExprs = Expr.substituteList(exprs, smap, analyzer, false);
    analyzer.materializeSlots(resolvedExprs);
}
#method_after
@Override
public void materializeRequiredSlots(Analyzer analyzer, ExprSubstitutionMap smap) {
    materializedSlots_.clear();
    List<Expr> exprs = Lists.newArrayList();
    for (int i = 0; i < analyticExprs_.size(); ++i) {
        SlotDescriptor outputSlotDesc = outputTupleDesc_.getSlots().get(i);
        if (!outputSlotDesc.isMaterialized())
            continue;
        intermediateTupleDesc_.getSlots().get(i).setIsMaterialized(true);
        exprs.add(analyticExprs_.get(i));
        materializedSlots_.add(i);
    }
    List<Expr> resolvedExprs = Expr.substituteList(exprs, smap, analyzer, false);
    analyzer.materializeSlots(resolvedExprs);
}
#end_block

#method_before
public Type getType() {
    return type_;
}
#method_after
public String getType() {
    return type_;
}
#end_block

#method_before
public String toString() {
    return "(" + id_ + ":" + label_ + ":" + type_ + ")";
}
#method_after
@Override
public String toString() {
    return "(" + id_ + ":" + type_ + ":" + label_ + ")";
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (obj == null)
        return false;
    if (obj.getClass() != this.getClass())
        return false;
    Vertex vertex = (Vertex) obj;
    return this.id_.equals(vertex.id_) && this.label_.equals(vertex.label_) && this.type_ == vertex.type_;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (obj == null)
        return false;
    if (obj.getClass() != this.getClass())
        return false;
    Vertex vertex = (Vertex) obj;
    return this.id_.equals(vertex.id_) && this.label_.equals(vertex.label_);
}
#end_block

#method_before
public void setVertices(Set<Vertex> vertices) {
    for (Vertex vertex : vertices) {
        vertices_.put(vertex.getLabel(), vertex);
        idToVertexMap_.put(vertex.getVertexId(), vertex);
    }
}
#method_after
private void setVertices(Set<Vertex> vertices) {
    for (Vertex vertex : vertices) {
        vertices_.put(vertex.getLabel(), vertex);
        idToVertexMap_.put(vertex.getVertexId(), vertex);
    }
}
#end_block

#method_before
private Vertex createVertex(String label, Vertex.Type type) {
    Vertex newVertex = vertices_.get(label);
    if (newVertex != null)
        return newVertex;
    newVertex = new Vertex(vertexIdGenerator.getNextId(), label, type);
    vertices_.put(newVertex.getLabel(), newVertex);
    idToVertexMap_.put(newVertex.getVertexId(), newVertex);
    return newVertex;
}
#method_after
private Vertex createVertex(String label) {
    Vertex newVertex = vertices_.get(label);
    if (newVertex != null)
        return newVertex;
    newVertex = new Vertex(vertexIdGenerator.getNextId(), label);
    vertices_.put(newVertex.getLabel(), newVertex);
    idToVertexMap_.put(newVertex.getVertexId(), newVertex);
    return newVertex;
}
#end_block

#method_before
public void computeLineageGraph(List<Expr> resultExprs, Analyzer analyzer) {
    computeProjectionDependencies(resultExprs);
    computeResultPredicateDependencies(analyzer);
}
#method_after
public void computeLineageGraph(List<Expr> resultExprs, Analyzer rootAnalyzer) {
    init(rootAnalyzer);
    computeProjectionDependencies(resultExprs);
    computeResultPredicateDependencies(rootAnalyzer);
}
#end_block

#method_before
private void computeProjectionDependencies(List<Expr> resultExprs) {
    Preconditions.checkNotNull(resultExprs);
    Preconditions.checkState(!resultExprs.isEmpty());
    Preconditions.checkState(resultExprs.size() == targetColumnLabels_.size());
    for (int i = 0; i < resultExprs.size(); ++i) {
        Expr expr = resultExprs.get(i);
        Set<String> dependentBaseCols = Sets.newHashSet();
        List<Expr> dependentExprs = Lists.newArrayList();
        getDependentBaseColumns(expr, dependentBaseCols, true, dependentExprs);
        Set<String> targets = Sets.newHashSet(targetColumnLabels_.get(i));
        createEdge(targets, dependentBaseCols, Edge.EdgeType.PROJECTION);
        if (!dependentExprs.isEmpty()) {
            // We have additional exprs that have a predicate dependency with 'expr'.
            // Identify the base table columns these exprs are connected to and create a new
            // PREDICATE edge in the column lineage graph.
            Set<String> predicateBaseCols = Sets.newHashSet();
            for (Expr dependentExpr : dependentExprs) {
                getDependentBaseColumns(dependentExpr, predicateBaseCols, false, null);
            }
            dependentBaseCols.addAll(predicateBaseCols);
            createEdge(targets, dependentBaseCols, Edge.EdgeType.PREDICATE);
        }
    }
}
#method_after
private void computeProjectionDependencies(List<Expr> resultExprs) {
    Preconditions.checkNotNull(resultExprs);
    Preconditions.checkState(!resultExprs.isEmpty());
    Preconditions.checkState(resultExprs.size() == targetColumnLabels_.size());
    for (int i = 0; i < resultExprs.size(); ++i) {
        Expr expr = resultExprs.get(i);
        Set<String> sourceBaseCols = Sets.newHashSet();
        List<Expr> dependentExprs = Lists.newArrayList();
        getSourceBaseCols(expr, sourceBaseCols, dependentExprs, false);
        Set<String> targets = Sets.newHashSet(targetColumnLabels_.get(i));
        createMultiEdge(targets, sourceBaseCols, MultiEdge.EdgeType.PROJECTION);
        if (!dependentExprs.isEmpty()) {
            // We have additional exprs that 'expr' has a predicate dependency on.
            // Gather the transitive predicate dependencies of 'expr' based on its direct
            // predicate dependencies. For each direct predicate dependency p, 'expr' is
            // transitively predicate dependent on all exprs that p is projection and
            // predicate dependent on.
            Set<String> predicateBaseCols = Sets.newHashSet();
            for (Expr dependentExpr : dependentExprs) {
                getSourceBaseCols(dependentExpr, predicateBaseCols, null, true);
            }
            createMultiEdge(targets, predicateBaseCols, MultiEdge.EdgeType.PREDICATE);
        }
    }
}
#end_block

#method_before
private void computeResultPredicateDependencies(Analyzer analyzer) {
    Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
    for (ExprId exprId : assignedConjuncts) {
        if (exprId == null)
            continue;
        Expr conjunct = analyzer.getConjunct(exprId);
        Preconditions.checkNotNull(conjunct);
        resultDependencyPredicates_.add(conjunct);
    }
    Set<String> predicateBaseCols = Sets.newHashSet();
    for (Expr expr : resultDependencyPredicates_) {
        getDependentBaseColumns(expr, predicateBaseCols, false, null);
    }
    if (predicateBaseCols.isEmpty())
        return;
    Set<String> targets = Sets.newHashSet(targetColumnLabels_);
    createEdge(targets, predicateBaseCols, Edge.EdgeType.PREDICATE);
}
#method_after
private void computeResultPredicateDependencies(Analyzer analyzer) {
    Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
    for (ExprId exprId : assignedConjuncts) {
        if (exprId == null)
            continue;
        Expr conjunct = analyzer.getConjunct(exprId);
        Preconditions.checkNotNull(conjunct);
        resultDependencyPredicates_.add(conjunct);
    }
    Set<String> predicateBaseCols = Sets.newHashSet();
    for (Expr expr : resultDependencyPredicates_) {
        getSourceBaseCols(expr, predicateBaseCols, null, true);
    }
    if (predicateBaseCols.isEmpty())
        return;
    Set<String> targets = Sets.newHashSet(targetColumnLabels_);
    createMultiEdge(targets, predicateBaseCols, MultiEdge.EdgeType.PREDICATE);
}
#end_block

#method_before
public static ColumnLineageGraph createFromJSON(String json) {
    if (json == null || json.isEmpty())
        return null;
    JSONParser parser = new JSONParser();
    Object obj = null;
    try {
        obj = parser.parse(json);
    } catch (ParseException e) {
        LOG.error("Error parsing serialized column lineage graph: " + e.getMessage());
        return null;
    }
    if (!(obj instanceof JSONObject))
        return null;
    JSONObject jsonObj = (JSONObject) obj;
    String stmt = (String) jsonObj.get("queryText");
    ColumnLineageGraph graph = new ColumnLineageGraph(stmt, null);
    JSONArray serializedVertices = (JSONArray) jsonObj.get("vertices");
    Set<Vertex> vertices = Sets.newHashSet();
    for (int i = 0; i < serializedVertices.size(); ++i) {
        Vertex v = Vertex.fromJSONObj((JSONObject) serializedVertices.get(i));
        vertices.add(v);
    }
    graph.setVertices(vertices);
    JSONArray serializedEdges = (JSONArray) jsonObj.get("edges");
    List<Edge> edges = Lists.newArrayList();
    for (int i = 0; i < serializedEdges.size(); ++i) {
        Edge e = graph.createEdgeFromJSONObj((JSONObject) serializedEdges.get(i));
        graph.addEdge(e);
    }
    return graph;
}
#method_after
public static ColumnLineageGraph createFromJSON(String json) {
    if (json == null || json.isEmpty())
        return null;
    JSONParser parser = new JSONParser();
    Object obj = null;
    try {
        obj = parser.parse(json);
    } catch (ParseException e) {
        LOG.error("Error parsing serialized column lineage graph: " + e.getMessage());
        return null;
    }
    if (!(obj instanceof JSONObject))
        return null;
    JSONObject jsonObj = (JSONObject) obj;
    String stmt = (String) jsonObj.get("queryText");
    String hash = (String) jsonObj.get("hash");
    String user = (String) jsonObj.get("user");
    long timestamp = (Long) jsonObj.get("timestamp");
    ColumnLineageGraph graph = new ColumnLineageGraph(stmt, user, timestamp);
    JSONArray serializedVertices = (JSONArray) jsonObj.get("vertices");
    Set<Vertex> vertices = Sets.newHashSet();
    for (int i = 0; i < serializedVertices.size(); ++i) {
        Vertex v = Vertex.fromJsonObj((JSONObject) serializedVertices.get(i));
        vertices.add(v);
    }
    graph.setVertices(vertices);
    JSONArray serializedEdges = (JSONArray) jsonObj.get("edges");
    for (int i = 0; i < serializedEdges.size(); ++i) {
        MultiEdge e = graph.createMultiEdgeFromJSONObj((JSONObject) serializedEdges.get(i));
        graph.edges_.add(e);
    }
    return graph;
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (obj == null)
        return false;
    if (obj.getClass() != this.getClass())
        return false;
    ColumnLineageGraph g = (ColumnLineageGraph) obj;
    if (this.vertices_.size() != g.vertices_.size() || this.edges_.size() != g.edges_.size()) {
        return false;
    }
    if (!this.vertices_.equals(g.vertices_) || !this.edges_.equals(g.edges_)) {
        return false;
    }
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (obj == null)
        return false;
    if (obj.getClass() != this.getClass())
        return false;
    ColumnLineageGraph g = (ColumnLineageGraph) obj;
    if (!this.vertices_.equals(g.vertices_) || !this.edges_.equals(g.edges_)) {
        return false;
    }
    return true;
}
#end_block

#method_before
public String debugString() {
    StringBuilder builder = new StringBuilder();
    for (Edge edge : edges_) {
        builder.append(edge.toString() + "\n");
    }
    builder.append(toJSON());
    return builder.toString();
}
#method_after
public String debugString() {
    StringBuilder builder = new StringBuilder();
    for (MultiEdge edge : edges_) {
        builder.append(edge.toString() + "\n");
    }
    builder.append(toJson());
    return builder.toString();
}
#end_block

#method_before
public void addTargetColumnLabels(Collection<String> columnLabels) {
    Preconditions.checkNotNull(columnLabels);
    targetColumnLabels_.addAll(columnLabels);
}
#method_after
public void addTargetColumnLabels(Table dstTable) {
    Preconditions.checkNotNull(dstTable);
    String tblFullName = dstTable.getFullName();
    for (String columnName : dstTable.getColumnNames()) {
        targetColumnLabels_.add(tblFullName + "." + columnName);
    }
}
#end_block

#method_before
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
    }
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
    }
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        resultExprs = ctx_.getAnalysisResult().getInsertStmt().getResultExprs();
    } else {
        resultExprs = ctx_.getQueryStmt().getBaseTblResultExprs();
        graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
    }
    resultExprs = Expr.substituteList(resultExprs, rootFragment.getPlanRoot().getOutputSmap(), ctx_.getRootAnalyzer(), true);
    rootFragment.setOutputExprs(resultExprs);
    // Compute the column lineage graph
    graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
    LOG.trace("lineage: " + graph.debugString());
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    return fragments;
}
#method_after
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
    }
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
    }
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    List<Expr> resultExprs = null;
    Table targetTable = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        resultExprs = insertStmt.getResultExprs();
        targetTable = insertStmt.getTargetTable();
        graph.addTargetColumnLabels(targetTable);
    } else {
        resultExprs = ctx_.getQueryStmt().getBaseTblResultExprs();
        graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
    }
    resultExprs = Expr.substituteList(resultExprs, rootFragment.getPlanRoot().getOutputSmap(), ctx_.getRootAnalyzer(), true);
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    if (RuntimeEnv.INSTANCE.computeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.debug("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#end_block

#method_before
public void setLabel(String label) {
    this.label_ = label;
}
#method_after
public void setLabel(String label) {
    label_ = label;
}
#end_block

#method_before
public TSlotDescriptor toThrift() {
    return new TSlotDescriptor(id_.asInt(), parent_.getId().asInt(), type_.toThrift(), ((column_ != null) ? column_.getPosition() : -1), byteOffset_, nullIndicatorByte_, nullIndicatorBit_, slotIdx_, isMaterialized_);
}
#method_after
public TSlotDescriptor toThrift() {
    List<Integer> columnPath = Lists.newArrayList();
    if (column_ != null)
        columnPath.add(column_.getPosition());
    return new TSlotDescriptor(id_.asInt(), parent_.getId().asInt(), type_.toThrift(), columnPath, byteOffset_, nullIndicatorByte_, nullIndicatorBit_, slotIdx_, isMaterialized_);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    // Start out with table refs to establish aliases.
    // the one to the left of tblRef
    TableRef leftTblRef = null;
    for (int i = 0; i < tableRefs_.size(); ++i) {
        // Resolve and replace non-InlineViewRef table refs with a BaseTableRef or ViewRef.
        TableRef tblRef = tableRefs_.get(i);
        tblRef = analyzer.resolveTableRef(tblRef);
        Preconditions.checkNotNull(tblRef);
        tableRefs_.set(i, tblRef);
        tblRef.setLeftTblRef(leftTblRef);
        try {
            tblRef.analyze(analyzer);
        } catch (AnalysisException e) {
            // Only re-throw the exception if no tables are missing.
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
        leftTblRef = tblRef;
    }
    // There is no reason to proceed with analysis past this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    // analyze plan hints from select list
    selectList_.analyzePlanHints(analyzer);
    // populate resultExprs_, aliasSmap_, and colLabels_
    for (int i = 0; i < selectList_.getItems().size(); ++i) {
        SelectListItem item = selectList_.getItems().get(i);
        if (item.isStar()) {
            TableName tblName = item.getTblName();
            if (tblName == null) {
                expandStar(analyzer);
            } else {
                expandStar(analyzer, tblName);
            }
        } else {
            // Analyze the resultExpr before generating a label to ensure enforcement
            // of expr child and depth limits (toColumn() label may call toSql()).
            item.getExpr().analyze(analyzer);
            if (item.getExpr().contains(Predicates.instanceOf(Subquery.class))) {
                throw new AnalysisException("Subqueries are not supported in the select list.");
            }
            resultExprs_.add(item.getExpr());
            String label = item.toColumnLabel(i, analyzer.useHiveColLabels());
            SlotRef aliasRef = new SlotRef(null, label);
            Expr existingAliasExpr = aliasSmap_.get(aliasRef);
            if (existingAliasExpr != null && !existingAliasExpr.equals(item.getExpr())) {
                // If we have already seen this alias, it refers to more than one column and
                // therefore is ambiguous.
                ambiguousAliasList_.add(aliasRef);
            }
            aliasSmap_.put(aliasRef, item.getExpr().clone());
            colLabels_.add(label);
        }
    }
    // non-root stmts to support views.
    for (Expr expr : resultExprs_) {
        if (expr.getType().isComplexType() && analyzer.isRootAnalyzer()) {
            throw new AnalysisException(String.format("Expr '%s' in select list of root statement returns a complex type '%s'.\n" + "Only scalar types are allowed in the select list of the root statement.", expr.toSql(), expr.getType().toSql()));
        }
    }
    if (TreeNode.contains(resultExprs_, AnalyticExpr.class)) {
        if (tableRefs_.isEmpty()) {
            throw new AnalysisException("Analytic expressions require FROM clause.");
        }
        // will get substituted away
        if (selectList_.isDistinct()) {
            throw new AnalysisException("cannot combine SELECT DISTINCT with analytic functions");
        }
    }
    if (whereClause_ != null) {
        whereClause_.analyze(analyzer);
        if (whereClause_.contains(Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function not allowed in WHERE clause");
        }
        whereClause_.checkReturnsBool("WHERE clause", false);
        Expr e = whereClause_.findFirstOf(AnalyticExpr.class);
        if (e != null) {
            throw new AnalysisException("WHERE clause must not contain analytic expressions: " + e.toSql());
        }
        analyzer.registerConjuncts(whereClause_, false);
    }
    createSortInfo(analyzer);
    analyzeAggregation(analyzer);
    analyzeAnalytics(analyzer);
    if (evaluateOrderBy_)
        createSortTupleInfo(analyzer);
    // Remember the SQL string before inline-view expression substitution.
    sqlString_ = toSql();
    resolveInlineViewRefs(analyzer);
    // block has no aggregation, then mark this block as returning an empty result set.
    if (analyzer.hasEmptySpjResultSet() && aggInfo_ == null) {
        analyzer.setHasEmptyResultSet();
    }
    ColumnLineageGraph graph = analyzer.getColumnLineageGraph();
    if (aggInfo_ != null && !selectList_.isDistinct()) {
        graph.addDependencyPredicates(aggInfo_.getGroupingExprs());
    }
    if (aggInfo_ != null)
        LOG.debug("post-analysis " + aggInfo_.debugString());
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    // Start out with table refs to establish aliases.
    // the one to the left of tblRef
    TableRef leftTblRef = null;
    for (int i = 0; i < tableRefs_.size(); ++i) {
        // Resolve and replace non-InlineViewRef table refs with a BaseTableRef or ViewRef.
        TableRef tblRef = tableRefs_.get(i);
        tblRef = analyzer.resolveTableRef(tblRef);
        Preconditions.checkNotNull(tblRef);
        tableRefs_.set(i, tblRef);
        tblRef.setLeftTblRef(leftTblRef);
        try {
            tblRef.analyze(analyzer);
        } catch (AnalysisException e) {
            // Only re-throw the exception if no tables are missing.
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
        leftTblRef = tblRef;
    }
    // There is no reason to proceed with analysis past this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    // analyze plan hints from select list
    selectList_.analyzePlanHints(analyzer);
    // populate resultExprs_, aliasSmap_, and colLabels_
    for (int i = 0; i < selectList_.getItems().size(); ++i) {
        SelectListItem item = selectList_.getItems().get(i);
        if (item.isStar()) {
            TableName tblName = item.getTblName();
            if (tblName == null) {
                expandStar(analyzer);
            } else {
                expandStar(analyzer, tblName);
            }
        } else {
            // Analyze the resultExpr before generating a label to ensure enforcement
            // of expr child and depth limits (toColumn() label may call toSql()).
            item.getExpr().analyze(analyzer);
            if (item.getExpr().contains(Predicates.instanceOf(Subquery.class))) {
                throw new AnalysisException("Subqueries are not supported in the select list.");
            }
            resultExprs_.add(item.getExpr());
            String label = item.toColumnLabel(i, analyzer.useHiveColLabels());
            SlotRef aliasRef = new SlotRef(null, label);
            Expr existingAliasExpr = aliasSmap_.get(aliasRef);
            if (existingAliasExpr != null && !existingAliasExpr.equals(item.getExpr())) {
                // If we have already seen this alias, it refers to more than one column and
                // therefore is ambiguous.
                ambiguousAliasList_.add(aliasRef);
            }
            aliasSmap_.put(aliasRef, item.getExpr().clone());
            colLabels_.add(label);
        }
    }
    // non-root stmts to support views.
    for (Expr expr : resultExprs_) {
        if (expr.getType().isComplexType() && analyzer.isRootAnalyzer()) {
            throw new AnalysisException(String.format("Expr '%s' in select list of root statement returns a complex type '%s'.\n" + "Only scalar types are allowed in the select list of the root statement.", expr.toSql(), expr.getType().toSql()));
        }
    }
    if (TreeNode.contains(resultExprs_, AnalyticExpr.class)) {
        if (tableRefs_.isEmpty()) {
            throw new AnalysisException("Analytic expressions require FROM clause.");
        }
        // will get substituted away
        if (selectList_.isDistinct()) {
            throw new AnalysisException("cannot combine SELECT DISTINCT with analytic functions");
        }
    }
    if (whereClause_ != null) {
        whereClause_.analyze(analyzer);
        if (whereClause_.contains(Expr.isAggregatePredicate())) {
            throw new AnalysisException("aggregate function not allowed in WHERE clause");
        }
        whereClause_.checkReturnsBool("WHERE clause", false);
        Expr e = whereClause_.findFirstOf(AnalyticExpr.class);
        if (e != null) {
            throw new AnalysisException("WHERE clause must not contain analytic expressions: " + e.toSql());
        }
        analyzer.registerConjuncts(whereClause_, false);
    }
    createSortInfo(analyzer);
    analyzeAggregation(analyzer);
    analyzeAnalytics(analyzer);
    if (evaluateOrderBy_)
        createSortTupleInfo(analyzer);
    // Remember the SQL string before inline-view expression substitution.
    sqlString_ = toSql();
    resolveInlineViewRefs(analyzer);
    // block has no aggregation, then mark this block as returning an empty result set.
    if (analyzer.hasEmptySpjResultSet() && aggInfo_ == null) {
        analyzer.setHasEmptyResultSet();
    }
    ColumnLineageGraph graph = analyzer.getColumnLineageGraph();
    if (aggInfo_ != null && !aggInfo_.getAggregateExprs().isEmpty()) {
        graph.addDependencyPredicates(aggInfo_.getGroupingExprs());
    }
    if (sortInfo_ != null && hasLimit()) {
        // When there is a LIMIT clause in conjunction with an ORDER BY, the ordering exprs
        // must be added in the column lineage graph.
        graph.addDependencyPredicates(sortInfo_.getOrderingExprs());
    }
    if (aggInfo_ != null)
        LOG.debug("post-analysis " + aggInfo_.debugString());
}
#end_block

#method_before
private void createMetadata(Analyzer analyzer) throws AnalysisException {
    // Create tuple descriptor for materialized tuple created by the union.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    tupleId_ = tupleDesc.getId();
    LOG.trace("UnionStmt.createMetadata: tupleId=" + tupleId_.toString());
    // One slot per expr in the select blocks. Use first select block as representative.
    List<Expr> firstSelectExprs = operands_.get(0).getQueryStmt().getBaseTblResultExprs();
    // Compute column stats for the materialized slots from the source exprs.
    List<ColumnStats> columnStats = Lists.newArrayList();
    for (int i = 0; i < operands_.size(); ++i) {
        List<Expr> selectExprs = operands_.get(i).getQueryStmt().getBaseTblResultExprs();
        for (int j = 0; j < selectExprs.size(); ++j) {
            ColumnStats statsToAdd = ColumnStats.fromExpr(selectExprs.get(j));
            if (i == 0) {
                columnStats.add(statsToAdd);
            } else {
                columnStats.get(j).add(statsToAdd);
            }
        }
    }
    // Create tuple descriptor and slots.
    for (int i = 0; i < firstSelectExprs.size(); ++i) {
        Expr expr = firstSelectExprs.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(getColLabels().get(i));
        slotDesc.setType(expr.getType());
        slotDesc.setStats(columnStats.get(i));
        SlotRef outputSlotRef = new SlotRef(slotDesc);
        resultExprs_.add(outputSlotRef);
        // Add to aliasSMap so that column refs in "order by" can be resolved.
        if (orderByElements_ != null) {
            SlotRef aliasRef = new SlotRef(null, getColLabels().get(i));
            if (aliasSmap_.containsMappingFor(aliasRef)) {
                ambiguousAliasList_.add(aliasRef);
            } else {
                aliasSmap_.put(aliasRef, outputSlotRef);
            }
        }
        // (see Planner.createInlineViewPlan() for the reasoning)
        for (UnionOperand op : operands_) {
            if (op.hasAnalyticExprs())
                continue;
            Expr resultExpr = op.getQueryStmt().getBaseTblResultExprs().get(i);
            slotDesc.addSourceExpr(resultExpr);
            SlotRef slotRef = resultExpr.unwrapSlotRef(true);
            if (slotRef == null)
                continue;
            analyzer.registerValueTransfer(outputSlotRef.getSlotId(), slotRef.getSlotId());
        }
    }
    baseTblResultExprs_ = resultExprs_;
}
#method_after
private void createMetadata(Analyzer analyzer) throws AnalysisException {
    // Create tuple descriptor for materialized tuple created by the union.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    tupleId_ = tupleDesc.getId();
    LOG.trace("UnionStmt.createMetadata: tupleId=" + tupleId_.toString());
    // One slot per expr in the select blocks. Use first select block as representative.
    List<Expr> firstSelectExprs = operands_.get(0).getQueryStmt().getBaseTblResultExprs();
    // Compute column stats for the materialized slots from the source exprs.
    List<ColumnStats> columnStats = Lists.newArrayList();
    for (int i = 0; i < operands_.size(); ++i) {
        List<Expr> selectExprs = operands_.get(i).getQueryStmt().getBaseTblResultExprs();
        for (int j = 0; j < selectExprs.size(); ++j) {
            ColumnStats statsToAdd = ColumnStats.fromExpr(selectExprs.get(j));
            if (i == 0) {
                columnStats.add(statsToAdd);
            } else {
                columnStats.get(j).add(statsToAdd);
            }
        }
    }
    // Create tuple descriptor and slots.
    for (int i = 0; i < firstSelectExprs.size(); ++i) {
        Expr expr = firstSelectExprs.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(getColLabels().get(i));
        slotDesc.setType(expr.getType());
        slotDesc.setStats(columnStats.get(i));
        SlotRef outputSlotRef = new SlotRef(slotDesc);
        resultExprs_.add(outputSlotRef);
        // Add to aliasSMap so that column refs in "order by" can be resolved.
        if (orderByElements_ != null) {
            SlotRef aliasRef = new SlotRef(null, getColLabels().get(i));
            if (aliasSmap_.containsMappingFor(aliasRef)) {
                ambiguousAliasList_.add(aliasRef);
            } else {
                aliasSmap_.put(aliasRef, outputSlotRef);
            }
        }
        // (see Planner.createInlineViewPlan() for the reasoning)
        for (UnionOperand op : operands_) {
            Expr resultExpr = op.getQueryStmt().getBaseTblResultExprs().get(i);
            slotDesc.addSourceExpr(resultExpr);
            if (op.hasAnalyticExprs())
                continue;
            SlotRef slotRef = resultExpr.unwrapSlotRef(true);
            if (slotRef == null)
                continue;
            analyzer.registerValueTransfer(outputSlotRef.getSlotId(), slotRef.getSlotId());
        }
    }
    baseTblResultExprs_ = resultExprs_;
}
#end_block

#method_before
public SlotDescriptor copySlotDescriptor(SlotDescriptor srcSlotDesc, TupleDescriptor tupleDesc) {
    SlotDescriptor result = globalState_.descTbl.addSlotDescriptor(tupleDesc);
    globalState_.blockBySlot.put(result.getId(), this);
    result.setLabel(srcSlotDesc.getLabel());
    result.setSourceExprs(srcSlotDesc.getSourceExprs());
    result.setStats(srcSlotDesc.getStats());
    if (srcSlotDesc.getColumn() != null) {
        result.setColumn(srcSlotDesc.getColumn());
    } else {
        result.setType(srcSlotDesc.getType());
    }
    return result;
}
#method_after
public SlotDescriptor copySlotDescriptor(SlotDescriptor srcSlotDesc, TupleDescriptor tupleDesc) {
    SlotDescriptor result = globalState_.descTbl.addSlotDescriptor(tupleDesc);
    globalState_.blockBySlot.put(result.getId(), this);
    result.setSourceExprs(srcSlotDesc.getSourceExprs());
    result.setLabel(srcSlotDesc.getLabel());
    result.setStats(srcSlotDesc.getStats());
    if (srcSlotDesc.getColumn() != null) {
        result.setColumn(srcSlotDesc.getColumn());
    } else {
        result.setType(srcSlotDesc.getType());
    }
    return result;
}
#end_block

#method_before
public ColumnLineageGraph getColumnLineageGraph() {
    return globalState_.lineageGraph_;
}
#method_after
public ColumnLineageGraph getColumnLineageGraph() {
    return globalState_.lineageGraph;
}
#end_block

#method_before
public String getSerializedLineageGraph() {
    return globalState_.lineageGraph_.toJSON();
}
#method_after
public String getSerializedLineageGraph() {
    Preconditions.checkNotNull(globalState_.lineageGraph);
    return globalState_.lineageGraph.toJson();
}
#end_block

#method_before
public TLoadDataResp loadTableData(TLoadDataReq request) throws ImpalaException, IOException {
    TableName tableName = TableName.fromThrift(request.getTable_name());
    // Get the destination for the load. If the load is targeting a partition,
    // this the partition location. Otherwise this is the table location.
    String destPathString = null;
    if (request.isSetPartition_spec()) {
        destPathString = impaladCatalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), request.getPartition_spec()).getLocation();
    } else {
        destPathString = impaladCatalog_.getTable(tableName.getDb(), tableName.getTbl()).getMetaStoreTable().getSd().getLocation();
    }
    Path destPath = new Path(destPathString);
    DistributedFileSystem dfs = FileSystemUtil.getDistributedFileSystem(destPath);
    // Create a temporary directory within the final destination directory to stage the
    // file move.
    Path tmpDestPath = FileSystemUtil.makeTmpSubdirectory(destPath);
    Path sourcePath = new Path(request.source_path);
    int filesLoaded = 0;
    if (dfs.isDirectory(sourcePath)) {
        filesLoaded = FileSystemUtil.moveAllVisibleFiles(sourcePath, tmpDestPath);
    } else {
        FileSystemUtil.moveFile(sourcePath, tmpDestPath, true);
        filesLoaded = 1;
    }
    // If this is an OVERWRITE, delete all files in the destination.
    if (request.isOverwrite()) {
        FileSystemUtil.deleteAllVisibleFiles(destPath);
    }
    // Move the files from the temporary location to the final destination.
    FileSystemUtil.moveAllVisibleFiles(tmpDestPath, destPath);
    // Cleanup the tmp directory.
    dfs.delete(tmpDestPath, true);
    TLoadDataResp response = new TLoadDataResp();
    TColumnValue col = new TColumnValue();
    String loadMsg = String.format("Loaded %d file(s). Total files in destination location: %d", filesLoaded, FileSystemUtil.getTotalNumVisibleFiles(destPath));
    col.setString_val(loadMsg);
    response.setLoad_summary(new TResultRow(Lists.newArrayList(col)));
    return response;
}
#method_after
public TLoadDataResp loadTableData(TLoadDataReq request) throws ImpalaException, IOException {
    TableName tableName = TableName.fromThrift(request.getTable_name());
    // Get the destination for the load. If the load is targeting a partition,
    // this the partition location. Otherwise this is the table location.
    String destPathString = null;
    if (request.isSetPartition_spec()) {
        destPathString = impaladCatalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), request.getPartition_spec()).getLocation();
    } else {
        destPathString = impaladCatalog_.getTable(tableName.getDb(), tableName.getTbl()).getMetaStoreTable().getSd().getLocation();
    }
    Path destPath = new Path(destPathString);
    FileSystem fs = destPath.getFileSystem(FileSystemUtil.getConfiguration());
    // Create a temporary directory within the final destination directory to stage the
    // file move.
    Path tmpDestPath = FileSystemUtil.makeTmpSubdirectory(destPath);
    Path sourcePath = new Path(request.source_path);
    int filesLoaded = 0;
    if (fs.isDirectory(sourcePath)) {
        filesLoaded = FileSystemUtil.relocateAllVisibleFiles(sourcePath, tmpDestPath);
    } else {
        FileSystemUtil.relocateFile(sourcePath, tmpDestPath, true);
        filesLoaded = 1;
    }
    // If this is an OVERWRITE, delete all files in the destination.
    if (request.isOverwrite()) {
        FileSystemUtil.deleteAllVisibleFiles(destPath);
    }
    // Move the files from the temporary location to the final destination.
    FileSystemUtil.relocateAllVisibleFiles(tmpDestPath, destPath);
    // Cleanup the tmp directory.
    fs.delete(tmpDestPath, true);
    TLoadDataResp response = new TLoadDataResp();
    TColumnValue col = new TColumnValue();
    String loadMsg = String.format("Loaded %d file(s). Total files in destination location: %d", filesLoaded, FileSystemUtil.getTotalNumVisibleFiles(destPath));
    col.setString_val(loadMsg);
    response.setLoad_summary(new TResultRow(Lists.newArrayList(col)));
    return response;
}
#end_block

#method_before
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    ArrayList<PlanFragment> fragments = planner.createPlan();
    result.setLineage_graph(analysisResult.getLineageGraph());
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int fragmentId = 0; fragmentId < fragments.size(); ++fragmentId) {
        PlanFragment fragment = fragments.get(fragmentId);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, fragmentId);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use VERBOSE by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.VERBOSE;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else {
        Preconditions.checkState(analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt());
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    }
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#method_after
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int fragmentId = 0; fragmentId < fragments.size(); ++fragmentId) {
        PlanFragment fragment = fragments.get(fragmentId);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, fragmentId);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use VERBOSE by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.VERBOSE;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    String jsonLineageGraph = analysisResult.getJsonLineageGraph();
    if (jsonLineageGraph != null && !jsonLineageGraph.isEmpty()) {
        queryExecRequest.setLineage_graph(jsonLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else {
        Preconditions.checkState(analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt());
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    }
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#end_block

#method_before
private StringBuilder PrintScanRangeLocations(TQueryExecRequest execRequest) {
    StringBuilder result = new StringBuilder();
    if (execRequest.per_node_scan_ranges == null) {
        return result;
    }
    for (Map.Entry<Integer, List<TScanRangeLocations>> entry : execRequest.per_node_scan_ranges.entrySet()) {
        result.append("NODE " + entry.getKey().toString() + ":\n");
        if (entry.getValue() == null) {
            continue;
        }
        for (TScanRangeLocations locations : entry.getValue()) {
            // print scan range
            result.append("  ");
            if (locations.scan_range.isSetHdfs_file_split()) {
                THdfsFileSplit split = locations.scan_range.getHdfs_file_split();
                THdfsPartition partition = findPartition(entry.getKey(), split);
                Path filePath = new Path(partition.getLocation(), split.file_name);
                result.append("HDFS SPLIT " + filePath.toString() + " " + Long.toString(split.offset) + ":" + Long.toString(split.length));
            }
            if (locations.scan_range.isSetHbase_key_range()) {
                THBaseKeyRange keyRange = locations.scan_range.getHbase_key_range();
                Integer hostIdx = locations.locations.get(0).host_idx;
                TNetworkAddress networkAddress = execRequest.getHost_list().get(hostIdx);
                result.append("HBASE KEYRANGE ");
                result.append("port=" + networkAddress.port + " ");
                if (keyRange.isSetStartKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStartKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
                result.append(":");
                if (keyRange.isSetStopKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStopKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
            }
            result.append("\n");
        }
    }
    return result;
}
#method_after
private StringBuilder PrintScanRangeLocations(TQueryExecRequest execRequest) {
    StringBuilder result = new StringBuilder();
    if (execRequest.per_node_scan_ranges == null) {
        return result;
    }
    for (Map.Entry<Integer, List<TScanRangeLocations>> entry : execRequest.per_node_scan_ranges.entrySet()) {
        result.append("NODE " + entry.getKey().toString() + ":\n");
        if (entry.getValue() == null) {
            continue;
        }
        for (TScanRangeLocations locations : entry.getValue()) {
            // print scan range
            result.append("  ");
            if (locations.scan_range.isSetHdfs_file_split()) {
                THdfsFileSplit split = locations.scan_range.getHdfs_file_split();
                THdfsPartition partition = findPartition(entry.getKey(), split);
                Path filePath = new Path(partition.getLocation(), split.file_name);
                filePath = cleanseFilePath(filePath);
                result.append("HDFS SPLIT " + filePath.toString() + " " + Long.toString(split.offset) + ":" + Long.toString(split.length));
            }
            if (locations.scan_range.isSetHbase_key_range()) {
                THBaseKeyRange keyRange = locations.scan_range.getHbase_key_range();
                Integer hostIdx = locations.locations.get(0).host_idx;
                TNetworkAddress networkAddress = execRequest.getHost_list().get(hostIdx);
                result.append("HBASE KEYRANGE ");
                result.append("port=" + networkAddress.port + " ");
                if (keyRange.isSetStartKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStartKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
                result.append(":");
                if (keyRange.isSetStopKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStopKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
            }
            result.append("\n");
        }
    }
    return result;
}
#end_block

#method_before
private void testColumnLineageOutput(TestCase testCase, TQueryCtx queryCtx, StringBuilder errorLog, StringBuilder actualOutput) throws CatalogException {
    ArrayList<String> expectedLineage = testCase.getSectionContents(Section.LINEAGE);
    if (expectedLineage == null || expectedLineage.isEmpty())
        return;
    String query = testCase.getQuery();
    queryCtx.request.getQuery_options().setNum_nodes(1);
    queryCtx.request.setStmt(query);
    StringBuilder explainBuilder = new StringBuilder();
    TExecRequest execRequest = null;
    String lineageGraph = null;
    try {
        execRequest = frontend_.createExecRequest(queryCtx, explainBuilder);
        lineageGraph = execRequest.lineage_graph;
    } catch (ImpalaException e) {
        if (e instanceof AnalysisException) {
            e.printStackTrace();
            errorLog.append("query:\n" + query + "\nanalysis error: " + e.getMessage() + "\n");
            return;
        } else if (e instanceof InternalException) {
            errorLog.append("query:\n" + query + "\ninternal error: " + e.getMessage() + "\n");
            return;
        }
        if (e instanceof NotImplementedException) {
            handleNotImplException(query, "", errorLog, actualOutput, e);
        } else if (e instanceof CatalogException) {
            throw (CatalogException) e;
        } else {
            errorLog.append("query:\n" + query + "\nunhandled exception: " + e.getMessage() + "\n");
        }
    }
    LOG.info("lineage graph: " + lineageGraph);
    ArrayList<String> expected = testCase.getSectionContents(Section.LINEAGE);
    if (expected.size() > 0 && lineageGraph != null) {
        String serializedGraph = Joiner.on("\n").join(expected);
        ColumnLineageGraph expectedGraph = ColumnLineageGraph.createFromJSON(serializedGraph);
        ColumnLineageGraph outputGraph = ColumnLineageGraph.createFromJSON(lineageGraph);
        if (expectedGraph == null || outputGraph == null || !outputGraph.equals(expectedGraph)) {
            StringBuilder lineageError = new StringBuilder();
            lineageError.append("section " + Section.LINEAGE + " of query:\n" + query + "\n");
            lineageError.append("Output:\n");
            lineageError.append(lineageGraph + "\n");
            lineageError.append("Expected:\n");
            lineageError.append(serializedGraph + "\n");
            errorLog.append(lineageError.toString());
        }
        actualOutput.append(Section.LINEAGE.getHeader() + "\n");
        actualOutput.append(lineageGraph + "\n");
    }
}
#method_after
private void testColumnLineageOutput(TestCase testCase, TQueryCtx queryCtx, StringBuilder errorLog, StringBuilder actualOutput) throws CatalogException {
    ArrayList<String> expectedLineage = testCase.getSectionContents(Section.LINEAGE);
    if (expectedLineage == null || expectedLineage.isEmpty())
        return;
    String query = testCase.getQuery();
    queryCtx.request.getQuery_options().setNum_nodes(1);
    queryCtx.request.setStmt(query);
    StringBuilder explainBuilder = new StringBuilder();
    TExecRequest execRequest = null;
    String lineageGraph = null;
    try {
        execRequest = frontend_.createExecRequest(queryCtx, explainBuilder);
        lineageGraph = execRequest.query_exec_request.lineage_graph;
    } catch (ImpalaException e) {
        if (e instanceof AnalysisException) {
            e.printStackTrace();
            errorLog.append("query:\n" + query + "\nanalysis error: " + e.getMessage() + "\n");
            return;
        } else if (e instanceof InternalException) {
            errorLog.append("query:\n" + query + "\ninternal error: " + e.getMessage() + "\n");
            return;
        }
        if (e instanceof NotImplementedException) {
            handleNotImplException(query, "", errorLog, actualOutput, e);
        } else if (e instanceof CatalogException) {
            throw (CatalogException) e;
        } else {
            errorLog.append("query:\n" + query + "\nunhandled exception: " + e.getMessage() + "\n");
        }
    }
    LOG.info("lineage graph: " + lineageGraph);
    ArrayList<String> expected = testCase.getSectionContents(Section.LINEAGE);
    if (expected.size() > 0 && lineageGraph != null) {
        String serializedGraph = Joiner.on("\n").join(expected);
        ColumnLineageGraph expectedGraph = ColumnLineageGraph.createFromJSON(serializedGraph);
        ColumnLineageGraph outputGraph = ColumnLineageGraph.createFromJSON(lineageGraph);
        if (expectedGraph == null || outputGraph == null || !outputGraph.equals(expectedGraph)) {
            StringBuilder lineageError = new StringBuilder();
            lineageError.append("section " + Section.LINEAGE + " of query:\n" + query + "\n");
            lineageError.append("Output:\n");
            lineageError.append(lineageGraph + "\n");
            lineageError.append("Expected:\n");
            lineageError.append(serializedGraph + "\n");
            errorLog.append(lineageError.toString());
        }
        actualOutput.append(Section.LINEAGE.getHeader());
        actualOutput.append(TestUtils.prettyPrintJson(lineageGraph) + "\n");
    }
}
#end_block

#method_before
private String checkFileSystem(Configuration conf) {
    try {
        FileSystem fs = FileSystem.get(CONF);
        if (!(fs instanceof DistributedFileSystem)) {
            return "Unsupported default filesystem. The default filesystem must be " + "DistributedFileSystem but the configured default filesystem is " + fs.getClass().getSimpleName() + ". " + CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY + " (" + CONF.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ")" + " might be set incorrectly.";
        }
    } catch (IOException e) {
        return "couldn't retrieve FileSystem:\n" + e.getMessage();
    }
    try {
        FileSystemUtil.getTotalNumVisibleFiles(new Path("/"));
    } catch (IOException e) {
        return "Could not read the HDFS root directory at " + CONF.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ". Error was: \n" + e.getMessage();
    }
    return "";
}
#method_after
private String checkFileSystem(Configuration conf) {
    try {
        FileSystem fs = FileSystem.get(CONF);
        if (!(fs instanceof DistributedFileSystem)) {
            return "Unsupported default filesystem. The default filesystem must be " + "a DistributedFileSystem but the configured default filesystem is " + fs.getClass().getSimpleName() + ". " + CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY + " (" + CONF.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ")" + " might be set incorrectly.";
        }
    } catch (IOException e) {
        return "couldn't retrieve FileSystem:\n" + e.getMessage();
    }
    try {
        FileSystemUtil.getTotalNumVisibleFiles(new Path("/"));
    } catch (IOException e) {
        return "Could not read the HDFS root directory at " + CONF.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ". Error was: \n" + e.getMessage();
    }
    return "";
}
#end_block

#method_before
public TSlotDescriptor toThrift() {
    List<Integer> columnPath = Lists.newArrayList();
    if (column_ != null) {
        columnPath.add(column_.getPosition());
    }
    return new TSlotDescriptor(id_.asInt(), parent_.getId().asInt(), type_.toThrift(), columnPath, byteOffset_, nullIndicatorByte_, nullIndicatorBit_, slotIdx_, isMaterialized_);
}
#method_after
public TSlotDescriptor toThrift() {
    List<Integer> columnPath = Lists.newArrayList();
    if (column_ != null)
        columnPath.add(column_.getPosition());
    return new TSlotDescriptor(id_.asInt(), parent_.getId().asInt(), type_.toThrift(), columnPath, byteOffset_, nullIndicatorByte_, nullIndicatorBit_, slotIdx_, isMaterialized_);
}
#end_block

#method_before
private Boolean checkAcls(FsAction action) {
    // returning null causes us to fall back to standard ugo permissions.
    if (aclStatus_ == null)
        return null;
    Boolean foundMatch = false;
    for (AclEntryType t : ACL_TYPE_PRIORITY) {
        for (AclEntry e : entriesByTypes_.get(t)) {
            if (t == AclEntryType.OTHER) {
                if (!foundMatch) {
                    return e.getPermission().implies(action);
                } else {
                    break;
                }
            }
            // the underlying ACL permit it.
            if (shouldApplyMask(e)) {
                if (mask_.getPermission().implies(action) && e.getPermission().implies(action)) {
                    return true;
                }
            } else {
                if (e.getPermission().implies(action))
                    return true;
            }
            // User acl entry has priority, no need to continue check.
            if (t == AclEntryType.USER)
                return false;
            foundMatch = true;
        }
    }
    return false;
}
#method_after
private Boolean checkAcls(FsAction action) {
    // returning null causes us to fall back to standard ugo permissions.
    if (aclStatus_ == null)
        return null;
    // Remember if there is an applicable ACL entry, including owner user, named user,
    // owning group, named group.
    boolean foundMatch = false;
    for (AclEntryType t : ACL_TYPE_PRIORITY) {
        for (AclEntry e : entriesByTypes_.get(t)) {
            if (t == AclEntryType.OTHER) {
                // permission, deny access. Otherwise check OTHER entry.
                return foundMatch ? false : e.getPermission().implies(action);
            }
            // the underlying ACL permit it.
            if (e.getPermission().implies(action)) {
                if (shouldApplyMask(e)) {
                    if (mask_.getPermission().implies(action))
                        return true;
                } else {
                    return true;
                }
            }
            // User ACL entry has priority, no need to continue check.
            if (t == AclEntryType.USER)
                return false;
            foundMatch = true;
        }
    }
    return false;
}
#end_block

#method_before
public Integer getColumnSize() {
    if (!isScalarType())
        return null;
    if (isNumericType())
        return getPrecision();
    ScalarType t = (ScalarType) this;
    switch(t.getPrimitiveType()) {
        case STRING:
            return Integer.MAX_VALUE;
        case TIMESTAMP:
            return 30;
        case DECIMAL:
            return t.decimalPrecision();
        case CHAR:
        case VARCHAR:
            return t.getLength();
        default:
            return null;
    }
}
#method_after
public Integer getColumnSize() {
    if (!isScalarType())
        return null;
    if (isNumericType())
        return getPrecision();
    ScalarType t = (ScalarType) this;
    switch(t.getPrimitiveType()) {
        case STRING:
            return Integer.MAX_VALUE;
        case TIMESTAMP:
            return 29;
        case CHAR:
        case VARCHAR:
            return t.getLength();
        default:
            return null;
    }
}
#end_block

#method_before
public Integer getDecimalDigits() {
    if (!isScalarType())
        return null;
    ScalarType t = (ScalarType) this;
    switch(t.getPrimitiveType()) {
        case BOOLEAN:
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
            return 0;
        case FLOAT:
            return 7;
        case DOUBLE:
            return 15;
        case DECIMAL:
            return t.decimalScale();
        default:
            return null;
    }
}
#method_after
public Integer getDecimalDigits() {
    if (!isScalarType())
        return null;
    ScalarType t = (ScalarType) this;
    switch(t.getPrimitiveType()) {
        case BOOLEAN:
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
            return 0;
        case FLOAT:
            return 7;
        case DOUBLE:
            return 15;
        case TIMESTAMP:
            return 9;
        case DECIMAL:
            return t.decimalScale();
        default:
            return null;
    }
}
#end_block

#method_before
public Integer getNumPrecRadix() {
    if (!isScalarType())
        return null;
    ScalarType t = (ScalarType) this;
    switch(t.getPrimitiveType()) {
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
            return 10;
        case FLOAT:
        case DOUBLE:
            return 2;
        default:
            // everything else (including boolean and string) is null
            return null;
    }
}
#method_after
public Integer getNumPrecRadix() {
    if (!isScalarType())
        return null;
    ScalarType t = (ScalarType) this;
    switch(t.getPrimitiveType()) {
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case FLOAT:
        case DOUBLE:
        case DECIMAL:
            return 10;
        default:
            // everything else (including boolean and string) is null
            return null;
    }
}
#end_block

#method_before
@Test
public void testMetaDataGetColumns() throws SQLException {
    // It should return alltypessmall.string_col.
    ResultSet rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "s%rin%");
    // validate the metadata for the getColumns result set
    ResultSetMetaData rsmd = rs.getMetaData();
    assertEquals("TABLE_CAT", rsmd.getColumnName(1));
    assertTrue(rs.next());
    String columnname = rs.getString("COLUMN_NAME");
    int ordinalPos = rs.getInt("ORDINAL_POSITION");
    assertEquals("Incorrect column name", "string_col", columnname);
    assertEquals("Incorrect ordinal position", 12, ordinalPos);
    assertEquals("Incorrect type", Types.VARCHAR, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate bool_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "bool_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.BOOLEAN, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate tinyint_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "tinyint_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.TINYINT, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate smallint_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "smallint_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.SMALLINT, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate int_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "int_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.INTEGER, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate bigint_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "bigint_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.BIGINT, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate float_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "float_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.FLOAT, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate double_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "double_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DOUBLE, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate timestamp_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "timestamp_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.TIMESTAMP, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate null column name returns all columns.
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", null);
    int numCols = 0;
    while (rs.next()) {
        ++numCols;
    }
    assertEquals(13, numCols);
    rs.close();
    // validate DECIMAL columns
    rs = con_.getMetaData().getColumns(null, "functional", "decimal_tbl", null);
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(9, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(10, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(20, rs.getInt("COLUMN_SIZE"));
    assertEquals(10, rs.getInt("DECIMAL_DIGITS"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(38, rs.getInt("COLUMN_SIZE"));
    assertEquals(38, rs.getInt("DECIMAL_DIGITS"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(10, rs.getInt("COLUMN_SIZE"));
    assertEquals(5, rs.getInt("DECIMAL_DIGITS"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(9, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertFalse(rs.next());
    rs.close();
    // validate CHAR/VARCHAR columns
    rs = con_.getMetaData().getColumns(null, "functional", "chars_tiny", null);
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.CHAR, rs.getInt("DATA_TYPE"));
    assertEquals(5, rs.getInt("COLUMN_SIZE"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.CHAR, rs.getInt("DATA_TYPE"));
    assertEquals(140, rs.getInt("COLUMN_SIZE"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.VARCHAR, rs.getInt("DATA_TYPE"));
    assertEquals(32, rs.getInt("COLUMN_SIZE"));
    assertFalse(rs.next());
    rs.close();
}
#method_after
@Test
public void testMetaDataGetColumns() throws SQLException {
    // It should return alltypessmall.string_col.
    ResultSet rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "s%rin%");
    // validate the metadata for the getColumns result set
    ResultSetMetaData rsmd = rs.getMetaData();
    assertEquals("TABLE_CAT", rsmd.getColumnName(1));
    assertTrue(rs.next());
    String columnname = rs.getString("COLUMN_NAME");
    int ordinalPos = rs.getInt("ORDINAL_POSITION");
    assertEquals("Incorrect column name", "string_col", columnname);
    assertEquals("Incorrect ordinal position", 12, ordinalPos);
    assertEquals("Incorrect type", Types.VARCHAR, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate bool_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "bool_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.BOOLEAN, rs.getInt("DATA_TYPE"));
    assertFalse(rs.next());
    rs.close();
    // validate tinyint_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "tinyint_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.TINYINT, rs.getInt("DATA_TYPE"));
    assertEquals(3, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertFalse(rs.next());
    rs.close();
    // validate smallint_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "smallint_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.SMALLINT, rs.getInt("DATA_TYPE"));
    assertEquals(5, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertFalse(rs.next());
    rs.close();
    // validate int_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "int_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.INTEGER, rs.getInt("DATA_TYPE"));
    assertEquals(10, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertFalse(rs.next());
    rs.close();
    // validate bigint_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "bigint_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.BIGINT, rs.getInt("DATA_TYPE"));
    assertEquals(19, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertFalse(rs.next());
    rs.close();
    // validate float_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "float_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.FLOAT, rs.getInt("DATA_TYPE"));
    assertEquals(7, rs.getInt("COLUMN_SIZE"));
    assertEquals(7, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertFalse(rs.next());
    rs.close();
    // validate double_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "double_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DOUBLE, rs.getInt("DATA_TYPE"));
    assertEquals(15, rs.getInt("COLUMN_SIZE"));
    assertEquals(15, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertFalse(rs.next());
    rs.close();
    // validate timestamp_col
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", "timestamp_col");
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.TIMESTAMP, rs.getInt("DATA_TYPE"));
    assertEquals(29, rs.getInt("COLUMN_SIZE"));
    assertEquals(9, rs.getInt("DECIMAL_DIGITS"));
    // Use getString() to check the value is null (and not 0).
    assertEquals(null, rs.getString("NUM_PREC_RADIX"));
    assertFalse(rs.next());
    rs.close();
    // validate null column name returns all columns.
    rs = con_.getMetaData().getColumns(null, "functional", "alltypessmall", null);
    int numCols = 0;
    while (rs.next()) {
        ++numCols;
    }
    assertEquals(13, numCols);
    rs.close();
    // validate DECIMAL columns
    rs = con_.getMetaData().getColumns(null, "functional", "decimal_tbl", null);
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(9, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(10, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(20, rs.getInt("COLUMN_SIZE"));
    assertEquals(10, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(38, rs.getInt("COLUMN_SIZE"));
    assertEquals(38, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(10, rs.getInt("COLUMN_SIZE"));
    assertEquals(5, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.DECIMAL, rs.getInt("DATA_TYPE"));
    assertEquals(9, rs.getInt("COLUMN_SIZE"));
    assertEquals(0, rs.getInt("DECIMAL_DIGITS"));
    assertEquals(10, rs.getInt("NUM_PREC_RADIX"));
    assertFalse(rs.next());
    rs.close();
    // validate CHAR/VARCHAR columns
    rs = con_.getMetaData().getColumns(null, "functional", "chars_tiny", null);
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.CHAR, rs.getInt("DATA_TYPE"));
    assertEquals(5, rs.getInt("COLUMN_SIZE"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.CHAR, rs.getInt("DATA_TYPE"));
    assertEquals(140, rs.getInt("COLUMN_SIZE"));
    assertTrue(rs.next());
    assertEquals("Incorrect type", Types.VARCHAR, rs.getInt("DATA_TYPE"));
    assertEquals(32, rs.getInt("COLUMN_SIZE"));
    assertFalse(rs.next());
    rs.close();
}
#end_block

#method_before
public static void relocateFile(Path sourceFile, Path dest, boolean renameIfAlreadyExists) throws IOException {
    FileSystem fs = dest.getFileSystem(CONF);
    // TODO: Handle moving between file systems
    Preconditions.checkArgument(isPathOnFileSystem(sourceFile, fs));
    Path destFile = fs.isDirectory(dest) ? new Path(dest, sourceFile.getName()) : dest;
    // then use the same file name. Otherwise, generate a unique file name.
    if (renameIfAlreadyExists && fs.exists(destFile)) {
        Path destDir = fs.isDirectory(dest) ? dest : dest.getParent();
        destFile = new Path(destDir, appendToBaseFileName(destFile.getName(), UUID.randomUUID().toString()));
    }
    // operation and the files would not be encrypted/decrypted properly on the DNs.
    if (arePathsInDifferentEncryptionZones(fs, sourceFile, destFile)) {
        LOG.info(String.format("Copying source '%s' to '%s' because HDFS encryption zones are different", sourceFile, destFile));
        FileUtil.copy(sourceFile.getFileSystem(CONF), sourceFile, fs, destFile, true, true, CONF);
    } else {
        LOG.debug(String.format("Moving '%s' to '%s'", sourceFile.toString(), destFile.toString()));
        // Move (rename) the file.
        fs.rename(sourceFile, destFile);
    }
}
#method_after
public static void relocateFile(Path sourceFile, Path dest, boolean renameIfAlreadyExists) throws IOException {
    FileSystem fs = dest.getFileSystem(CONF);
    // TODO: Handle moving between file systems
    Preconditions.checkArgument(isPathOnFileSystem(sourceFile, fs));
    Path destFile = fs.isDirectory(dest) ? new Path(dest, sourceFile.getName()) : dest;
    // then use the same file name. Otherwise, generate a unique file name.
    if (renameIfAlreadyExists && fs.exists(destFile)) {
        Path destDir = fs.isDirectory(dest) ? dest : dest.getParent();
        destFile = new Path(destDir, appendToBaseFileName(destFile.getName(), UUID.randomUUID().toString()));
    }
    if (arePathsInSameEncryptionZone(fs, sourceFile, destFile)) {
        LOG.debug(String.format("Moving '%s' to '%s'", sourceFile.toString(), destFile.toString()));
        // Move (rename) the file.
        fs.rename(sourceFile, destFile);
    } else {
        // We must copy rather than move if the source and dest are in different encryption
        // zones. A move would return an error from the NN because a move is a metadata-only
        // operation and the files would not be encrypted/decrypted properly on the DNs.
        LOG.info(String.format("Copying source '%s' to '%s' because HDFS encryption zones are different", sourceFile, destFile));
        FileUtil.copy(sourceFile.getFileSystem(CONF), sourceFile, fs, destFile, true, true, CONF);
    }
}
#end_block

#method_before
public static boolean isHiddenFile(String fileName) {
    // Hidden files start with . or _
    return fileName.startsWith(".") || fileName.startsWith("_");
}
#method_after
public static boolean isHiddenFile(String fileName) {
    // Hidden files start with '.' or '_'. The '.copying' suffix is used by some
    // filesystem utilities (e.g. hdfs put) as a temporary destination when copying
    // files. The '.tmp' suffix is Flume's default for temporary files.
    String lcFileName = fileName.toLowerCase();
    return lcFileName.startsWith(".") || lcFileName.startsWith("_") || lcFileName.endsWith(".copying") || lcFileName.endsWith(".tmp");
}
#end_block

#method_before
public static DistributedFileSystem getDistributedFileSystem() throws IOException {
    return getDistributedFileSystem(new Path(FileSystem.getDefaultUri(CONF)));
}
#method_after
public static DistributedFileSystem getDistributedFileSystem() throws IOException {
    Path path = new Path(FileSystem.getDefaultUri(CONF));
    FileSystem fs = path.getFileSystem(CONF);
    Preconditions.checkState(fs instanceof DistributedFileSystem);
    return (DistributedFileSystem) fs;
}
#end_block

#method_before
private static ColumnStatisticsData createHiveColStatsData(TColumnStats colStats, Type colType) {
    ColumnStatisticsData colStatsData = new ColumnStatisticsData();
    long ndvs = colStats.getNum_distinct_values();
    long numNulls = colStats.getNum_nulls();
    switch(colType.getPrimitiveType()) {
        case BOOLEAN:
            // TODO: Gather and set the numTrues and numFalse stats as well. The planner
            // currently does not rely on them.
            colStatsData.setBooleanStats(new BooleanColumnStatsData(1, -1, numNulls));
            break;
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case // Hive and Impala use LongColumnStatsData for timestamps.
        TIMESTAMP:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setLongStats(new LongColumnStatsData(numNulls, ndvs));
            break;
        case FLOAT:
        case DOUBLE:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDoubleStats(new DoubleColumnStatsData(numNulls, ndvs));
            break;
        case STRING:
            long maxStrLen = colStats.getMax_size();
            double avgStrLen = colStats.getAvg_size();
            colStatsData.setStringStats(new StringColumnStatsData(maxStrLen, avgStrLen, numNulls, ndvs));
            break;
        case DECIMAL:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDecimalStats(new DecimalColumnStatsData(numNulls, ndvs));
            break;
        default:
            return null;
    }
    return colStatsData;
}
#method_after
private static ColumnStatisticsData createHiveColStatsData(TColumnStats colStats, Type colType) {
    ColumnStatisticsData colStatsData = new ColumnStatisticsData();
    long ndvs = colStats.getNum_distinct_values();
    long numNulls = colStats.getNum_nulls();
    switch(colType.getPrimitiveType()) {
        case BOOLEAN:
            // TODO: Gather and set the numTrues and numFalse stats as well. The planner
            // currently does not rely on them.
            colStatsData.setBooleanStats(new BooleanColumnStatsData(1, -1, numNulls));
            break;
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case // Hive and Impala use LongColumnStatsData for timestamps.
        TIMESTAMP:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setLongStats(new LongColumnStatsData(numNulls, ndvs));
            break;
        case FLOAT:
        case DOUBLE:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDoubleStats(new DoubleColumnStatsData(numNulls, ndvs));
            break;
        case CHAR:
        case VARCHAR:
        case STRING:
            long maxStrLen = colStats.getMax_size();
            double avgStrLen = colStats.getAvg_size();
            colStatsData.setStringStats(new StringColumnStatsData(maxStrLen, avgStrLen, numNulls, ndvs));
            break;
        case DECIMAL:
            // TODO: Gather and set the min/max values stats as well. The planner
            // currently does not rely on them.
            colStatsData.setDecimalStats(new DecimalColumnStatsData(numNulls, ndvs));
            break;
        default:
            return null;
    }
    return colStatsData;
}
#end_block

#method_before
private void alterTableOrViewRename(TableName tableName, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        msTbl.setDbName(newTableName.getDb());
        msTbl.setTableName(newTableName.getTbl());
        // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column stats
        // across databases, we drop+create the column stats because the HMS does not
        // properly move them to the new table via alteration.
        ColumnStatistics hmsColStats = null;
        if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
            Table oldTbl = getExistingTable(tableName.getDb(), tableName.getTbl());
            Map<String, TColumnStats> colStats = Maps.newHashMap();
            for (Column c : oldTbl.getColumns()) {
                colStats.put(c.getName(), c.getStats().toThrift());
            }
            hmsColStats = createHiveColStats(colStats, oldTbl);
            // Set the new db/table.
            hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
        }
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            if (hmsColStats != null) {
                LOG.info(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
                // Delete all column stats of the original table from the HMS.
                msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
            }
            // Perform the table rename in any case.
            msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
            if (hmsColStats != null) {
                LOG.info(String.format("Re-creating column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
                msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
            }
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
        } finally {
            msClient.release();
        }
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object(removedObject);
    response.result.setUpdated_catalog_object(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#method_after
private void alterTableOrViewRename(TableName tableName, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(tableName);
        msTbl.setDbName(newTableName.getDb());
        msTbl.setTableName(newTableName.getTbl());
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column stats
            // across databases, we save, drop and restore the column stats because the HMS
            // does not properly move them to the new table via alteration. The following
            // block needs to be protected by the metastoreDdlLock_ to avoid conflicts with
            // concurrent DDL on this same table (e.g., drop+add table with same db/name).
            ColumnStatistics hmsColStats = null;
            if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
                Table oldTbl = getExistingTable(tableName.getDb(), tableName.getTbl());
                Map<String, TColumnStats> colStats = Maps.newHashMap();
                for (Column c : oldTbl.getColumns()) {
                    colStats.put(c.getName(), c.getStats().toThrift());
                }
                hmsColStats = createHiveColStats(colStats, oldTbl);
                // Set the new db/table.
                hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
                LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
                // Delete all column stats of the original table from the HMS.
                msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
            }
            // Perform the table rename in any case.
            msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
            if (hmsColStats != null) {
                LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
                msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
            }
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
        } finally {
            msClient.release();
        }
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object(removedObject);
    response.result.setUpdated_catalog_object(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#end_block

#method_before
@Override
protected Path cleanseFilePath(Path path) {
    path = super.cleanseFilePath(path);
    URI fsURI = fsName.toUri();
    URI pathURI = path.toUri();
    Assert.assertTrue("error: " + path + " is not on filesystem " + fsName, fsURI.getScheme().equals(pathURI.getScheme()) && fsURI.getAuthority().equals(pathURI.getAuthority()));
    return Path.mergePaths(s3aCanonicalBucket, path);
}
#method_after
@Override
protected Path cleanseFilePath(Path path) {
    path = super.cleanseFilePath(path);
    URI fsURI = fsName.toUri();
    URI pathURI = path.toUri();
    Assert.assertTrue("error: " + path + " is not on filesystem " + fsName, fsURI.getScheme().equals(pathURI.getScheme()) && fsURI.getAuthority().equals(pathURI.getAuthority()));
    return Path.mergePaths(S3A_CANONICAL_BUCKET, path);
}
#end_block

#method_before
private void loadBlockMetadata(boolean hasGetFileBlockLocations, FileSystem fs, FileStatus file, FileDescriptor fd, HdfsFileFormat fileFormat, Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    Preconditions.checkNotNull(fd);
    Preconditions.checkNotNull(perFsFileBlocks);
    Preconditions.checkArgument(!file.isDirectory());
    LOG.debug("load block md for " + name_ + " file " + fd.getFileName());
    if (!hasGetFileBlockLocations) {
        synthesizeBlockMetadata(fs, fd, fileFormat);
        return;
    }
    try {
        BlockLocation[] locations = fs.getFileBlockLocations(file, 0, file.getLen());
        Preconditions.checkNotNull(locations);
        // Loop over all blocks in the file.
        for (BlockLocation loc : locations) {
            Preconditions.checkNotNull(loc);
            // Get the location of all block replicas in ip:port format.
            String[] blockHostPorts = loc.getNames();
            // Get the hostnames for all block replicas. Used to resolve which hosts
            // contain cached data. The results are returned in the same order as
            // block.getNames() so it allows us to match a host specified as ip:port to
            // corresponding hostname using the same array index.
            String[] blockHostNames = loc.getHosts();
            Preconditions.checkState(blockHostNames.length == blockHostPorts.length);
            // Get the hostnames that contain cached replicas of this block.
            Set<String> cachedHosts = Sets.newHashSet(Arrays.asList(loc.getCachedHosts()));
            Preconditions.checkState(cachedHosts.size() <= blockHostNames.length);
            // Now enumerate all replicas of the block, adding any unknown hosts
            // to hostMap_/hostList_. The host ID (index in to the hostList_) for each
            // replica is stored in replicaHostIdxs.
            List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(blockHostPorts.length);
            for (int i = 0; i < blockHostPorts.length; ++i) {
                TNetworkAddress networkAddress = BlockReplica.parseLocation(blockHostPorts[i]);
                Preconditions.checkState(networkAddress != null);
                replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(blockHostNames[i])));
            }
            fd.addFileBlock(new FileBlock(loc.getOffset(), loc.getLength(), replicas));
        }
        // Remember the THdfsFileBlocks and corresponding BlockLocations.  Once all the
        // blocks are collected, the disk IDs will be queried in one batch per filesystem.
        addPerFsFileBlocks(perFsFileBlocks, fs, fd.getFileBlocks(), Arrays.asList(locations));
    } catch (IOException e) {
        throw new RuntimeException("couldn't determine block locations for path '" + file.getPath() + "':\n" + e.getMessage(), e);
    }
}
#method_after
private void loadBlockMetadata(FileSystem fs, FileStatus file, FileDescriptor fd, HdfsFileFormat fileFormat, Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    Preconditions.checkNotNull(fd);
    Preconditions.checkNotNull(perFsFileBlocks);
    Preconditions.checkArgument(!file.isDirectory());
    LOG.debug("load block md for " + name_ + " file " + fd.getFileName());
    if (!FileSystemUtil.hasGetFileBlockLocations(fs)) {
        synthesizeBlockMetadata(fs, fd, fileFormat);
        return;
    }
    try {
        BlockLocation[] locations = fs.getFileBlockLocations(file, 0, file.getLen());
        Preconditions.checkNotNull(locations);
        // Loop over all blocks in the file.
        for (BlockLocation loc : locations) {
            Preconditions.checkNotNull(loc);
            // Get the location of all block replicas in ip:port format.
            String[] blockHostPorts = loc.getNames();
            // Get the hostnames for all block replicas. Used to resolve which hosts
            // contain cached data. The results are returned in the same order as
            // block.getNames() so it allows us to match a host specified as ip:port to
            // corresponding hostname using the same array index.
            String[] blockHostNames = loc.getHosts();
            Preconditions.checkState(blockHostNames.length == blockHostPorts.length);
            // Get the hostnames that contain cached replicas of this block.
            Set<String> cachedHosts = Sets.newHashSet(Arrays.asList(loc.getCachedHosts()));
            Preconditions.checkState(cachedHosts.size() <= blockHostNames.length);
            // Now enumerate all replicas of the block, adding any unknown hosts
            // to hostMap_/hostList_. The host ID (index in to the hostList_) for each
            // replica is stored in replicaHostIdxs.
            List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(blockHostPorts.length);
            for (int i = 0; i < blockHostPorts.length; ++i) {
                TNetworkAddress networkAddress = BlockReplica.parseLocation(blockHostPorts[i]);
                Preconditions.checkState(networkAddress != null);
                replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(blockHostNames[i])));
            }
            fd.addFileBlock(new FileBlock(loc.getOffset(), loc.getLength(), replicas));
        }
        // Remember the THdfsFileBlocks and corresponding BlockLocations.  Once all the
        // blocks are collected, the disk IDs will be queried in one batch per filesystem.
        addPerFsFileBlocks(perFsFileBlocks, fs, fd.getFileBlocks(), Arrays.asList(locations));
    } catch (IOException e) {
        throw new RuntimeException("couldn't determine block locations for path '" + file.getPath() + "':\n" + e.getMessage(), e);
    }
}
#end_block

#method_before
private void synthesizeBlockMetadata(FileSystem fs, FileDescriptor fd, HdfsFileFormat fileFormat) {
    long start = 0;
    long remaining = fd.getFileLength();
    // Workaround HADOOP-11584 by using the filesystem default block size rather than
    // the block size from the FileStatus.
    // TODO: after HADOOP-11584 is resolved, get the block size from the FileStatus.
    long blockSize = fs.getDefaultBlockSize();
    if (blockSize < MIN_SYNTHETIC_BLOCK_SIZE)
        blockSize = MIN_SYNTHETIC_BLOCK_SIZE;
    if (!fileFormat.isSplittable(fd.getFileName()))
        blockSize = remaining;
    while (remaining > 0) {
        long len = Math.min(remaining, blockSize);
        List<BlockReplica> replicas = Lists.newArrayList(new BlockReplica(hostIndex_.getIndex(remoteNetworkAddress_), false));
        fd.addFileBlock(new FileBlock(start, len, replicas));
        remaining -= len;
        start += len;
    }
}
#method_after
private void synthesizeBlockMetadata(FileSystem fs, FileDescriptor fd, HdfsFileFormat fileFormat) {
    long start = 0;
    long remaining = fd.getFileLength();
    // Workaround HADOOP-11584 by using the filesystem default block size rather than
    // the block size from the FileStatus.
    // TODO: after HADOOP-11584 is resolved, get the block size from the FileStatus.
    long blockSize = fs.getDefaultBlockSize();
    if (blockSize < MIN_SYNTHETIC_BLOCK_SIZE)
        blockSize = MIN_SYNTHETIC_BLOCK_SIZE;
    if (!fileFormat.isSplittable(HdfsCompression.fromFileName(fd.getFileName()))) {
        blockSize = remaining;
    }
    while (remaining > 0) {
        long len = Math.min(remaining, blockSize);
        List<BlockReplica> replicas = Lists.newArrayList(new BlockReplica(hostIndex_.getIndex(REMOTE_NETWORK_ADDRESS), false));
        fd.addFileBlock(new FileBlock(start, len, replicas));
        remaining -= len;
        start += len;
    }
}
#end_block

#method_before
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    if (!SUPPORTS_VOLUME_ID)
        return;
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        // part of the FileSystem inteface, so we'll need to downcast.
        if (!(fs instanceof DistributedFileSystem))
            continue;
        LOG.trace("Loading disk ids for: " + getFullName() + ". nodes: " + getNumNodes() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        BlockStorageLocation[] storageLocs = null;
        try {
            // Get the BlockStorageLocations for all the blocks
            storageLocs = dfs.getFileBlockStorageLocations(blockLists.locations);
        } catch (IOException e) {
            LOG.error("Couldn't determine block storage locations for filesystem " + fs + ":\n" + e.getMessage());
            continue;
        }
        if (storageLocs == null || storageLocs.length == 0) {
            LOG.warn("Attempted to get block locations for filesystem " + fs + " but the call returned no results");
            continue;
        }
        if (storageLocs.length != blockLists.locations.size()) {
            // Block locations and storage locations didn't match up.
            LOG.error("Number of block storage locations not equal to number of blocks: " + "#storage locations=" + Long.toString(storageLocs.length) + " #blocks=" + Long.toString(blockLists.locations.size()));
            continue;
        }
        long unknownDiskIdCount = 0;
        // THdfsFileBlocks.
        for (int locIdx = 0; locIdx < storageLocs.length; ++locIdx) {
            VolumeId[] volumeIds = storageLocs[locIdx].getVolumeIds();
            THdfsFileBlock block = blockLists.blocks.get(locIdx);
            // Convert opaque VolumeId to 0 based ids.
            // TODO: the diskId should be eventually retrievable from Hdfs when the
            // community agrees this API is useful.
            int[] diskIds = new int[volumeIds.length];
            for (int i = 0; i < volumeIds.length; ++i) {
                diskIds[i] = getDiskId(volumeIds[i]);
                if (diskIds[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIds, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#method_after
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    if (!SUPPORTS_VOLUME_ID)
        return;
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        // part of the FileSystem interface, so we'll need to downcast.
        if (!(fs instanceof DistributedFileSystem))
            continue;
        LOG.trace("Loading disk ids for: " + getFullName() + ". nodes: " + getNumNodes() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        BlockStorageLocation[] storageLocs = null;
        try {
            // Get the BlockStorageLocations for all the blocks
            storageLocs = dfs.getFileBlockStorageLocations(blockLists.locations);
        } catch (IOException e) {
            LOG.error("Couldn't determine block storage locations for filesystem " + fs + ":\n" + e.getMessage());
            continue;
        }
        if (storageLocs == null || storageLocs.length == 0) {
            LOG.warn("Attempted to get block locations for filesystem " + fs + " but the call returned no results");
            continue;
        }
        if (storageLocs.length != blockLists.locations.size()) {
            // Block locations and storage locations didn't match up.
            LOG.error("Number of block storage locations not equal to number of blocks: " + "#storage locations=" + Long.toString(storageLocs.length) + " #blocks=" + Long.toString(blockLists.locations.size()));
            continue;
        }
        long unknownDiskIdCount = 0;
        // THdfsFileBlocks.
        for (int locIdx = 0; locIdx < storageLocs.length; ++locIdx) {
            VolumeId[] volumeIds = storageLocs[locIdx].getVolumeIds();
            THdfsFileBlock block = blockLists.blocks.get(locIdx);
            // Convert opaque VolumeId to 0 based ids.
            // TODO: the diskId should be eventually retrievable from Hdfs when the
            // community agrees this API is useful.
            int[] diskIds = new int[volumeIds.length];
            for (int i = 0; i < volumeIds.length; ++i) {
                diskIds[i] = getDiskId(volumeIds[i]);
                if (diskIds[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIds, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#end_block

#method_before
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition, Map<String, List<FileDescriptor>> oldFileDescMap, Map<FsKey, FileBlocksInfo> perFsFileBlocks) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    Path partDirPath = new Path(storageDescriptor.getLocation());
    List<FileDescriptor> fileDescriptors = Lists.newArrayList();
    // If the partition is marked as cached, the block location metadata must be
    // reloaded, even if the file times have not changed.
    boolean isMarkedCached = isMarkedCached_;
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        isMarkedCached = HdfsCachingUtil.validateCacheParams(msPartition.getParameters());
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        try {
            Expr.analyze(keyValues, null);
        } catch (AnalysisException e) {
            // should never happen
            throw new IllegalStateException(e);
        }
    }
    try {
        // Each partition could reside on a different filesystem.
        FileSystem fs = partDirPath.getFileSystem(CONF);
        boolean hasGetFileBlockLocations = FileSystemUtil.hasGetFileBlockLocations(fs);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (fs.exists(partDirPath)) {
            // fs.listStatus() to list all the files.
            for (FileStatus fileStatus : fs.listStatus(partDirPath)) {
                String fileName = fileStatus.getPath().getName().toString();
                if (fileStatus.isDirectory() || FileSystemUtil.isHiddenFile(fileName) || HdfsCompression.fromFileName(fileName) == HdfsCompression.LZO_INDEX) {
                    // Skip index files, these are read by the LZO scanner directly.
                    continue;
                }
                String partitionDir = fileStatus.getPath().getParent().toString();
                FileDescriptor fd = null;
                // is found, it will be chosen as a candidate to reuse.
                if (oldFileDescMap != null && oldFileDescMap.get(partitionDir) != null) {
                    for (FileDescriptor oldFileDesc : oldFileDescMap.get(partitionDir)) {
                        if (oldFileDesc.getFileName().equals(fileName)) {
                            fd = oldFileDesc;
                            break;
                        }
                    }
                }
                // value can be reused.
                if (fd == null || isMarkedCached || fd.getFileLength() != fileStatus.getLen() || fd.getModificationTime() != fileStatus.getModificationTime()) {
                    // Create a new file descriptor and load the file block metadata,
                    // collecting the block metadata into perFsFileBlocks.  The disk IDs for
                    // all the blocks of each filesystem will be loaded by loadDiskIds().
                    fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
                    loadBlockMetadata(hasGetFileBlockLocations, fs, fileStatus, fd, fileFormatDescriptor.getFileFormat(), perFsFileBlocks);
                }
                List<FileDescriptor> fds = fileDescMap_.get(partitionDir);
                if (fds == null) {
                    fds = Lists.newArrayList();
                    fileDescMap_.put(partitionDir, fds);
                }
                fds.add(fd);
                // Add to the list of FileDescriptors for this partition.
                fileDescriptors.add(fd);
            }
            numHdfsFiles_ += fileDescriptors.size();
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, fileDescriptors, getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (Exception e) {
        throw new CatalogException("Failed to create partition: ", e);
    }
}
#method_after
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition, Map<String, List<FileDescriptor>> oldFileDescMap, Map<FsKey, FileBlocksInfo> perFsFileBlocks) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    Path partDirPath = new Path(storageDescriptor.getLocation());
    List<FileDescriptor> fileDescriptors = Lists.newArrayList();
    // If the partition is marked as cached, the block location metadata must be
    // reloaded, even if the file times have not changed.
    boolean isMarkedCached = isMarkedCached_;
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        isMarkedCached = HdfsCachingUtil.validateCacheParams(msPartition.getParameters());
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        try {
            Expr.analyze(keyValues, null);
        } catch (AnalysisException e) {
            // should never happen
            throw new IllegalStateException(e);
        }
    }
    try {
        // Each partition could reside on a different filesystem.
        FileSystem fs = partDirPath.getFileSystem(CONF);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (fs.exists(partDirPath)) {
            // fs.listStatus() to list all the files.
            for (FileStatus fileStatus : fs.listStatus(partDirPath)) {
                String fileName = fileStatus.getPath().getName().toString();
                if (fileStatus.isDirectory() || FileSystemUtil.isHiddenFile(fileName) || HdfsCompression.fromFileName(fileName) == HdfsCompression.LZO_INDEX) {
                    // Skip index files, these are read by the LZO scanner directly.
                    continue;
                }
                String partitionDir = fileStatus.getPath().getParent().toString();
                FileDescriptor fd = null;
                // is found, it will be chosen as a candidate to reuse.
                if (oldFileDescMap != null && oldFileDescMap.get(partitionDir) != null) {
                    for (FileDescriptor oldFileDesc : oldFileDescMap.get(partitionDir)) {
                        if (oldFileDesc.getFileName().equals(fileName)) {
                            fd = oldFileDesc;
                            break;
                        }
                    }
                }
                // value can be reused.
                if (fd == null || isMarkedCached || fd.getFileLength() != fileStatus.getLen() || fd.getModificationTime() != fileStatus.getModificationTime()) {
                    // Create a new file descriptor and load the file block metadata,
                    // collecting the block metadata into perFsFileBlocks.  The disk IDs for
                    // all the blocks of each filesystem will be loaded by loadDiskIds().
                    fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
                    loadBlockMetadata(fs, fileStatus, fd, fileFormatDescriptor.getFileFormat(), perFsFileBlocks);
                }
                List<FileDescriptor> fds = fileDescMap_.get(partitionDir);
                if (fds == null) {
                    fds = Lists.newArrayList();
                    fileDescMap_.put(partitionDir, fds);
                }
                fds.add(fd);
                // Add to the list of FileDescriptors for this partition.
                fileDescriptors.add(fd);
            }
            numHdfsFiles_ += fileDescriptors.size();
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, fileDescriptors, getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (Exception e) {
        throw new CatalogException("Failed to create partition: ", e);
    }
}
#end_block

#method_before
public static boolean validateCacheParams(Map<String, String> params) {
    Long directiveId = getCacheDirectiveId(params);
    if (directiveId == null)
        return false;
    CacheDirectiveEntry entry = null;
    try {
        entry = getDirective(directiveId);
    } catch (ImpalaRuntimeException e) {
        if (e.getCause() != null && e.getCause() instanceof RemoteException) {
            // This exception signals that the cache directive no longer exists.
            LOG.error("Cache directive does not exist", e);
            params.remove(CACHE_DIR_ID_PROP_NAME);
            params.remove(CACHE_DIR_REPLICATION_PROP_NAME);
        } else {
            // This exception signals that there was a connection problem with HDFS.
            LOG.error("IO Exception, possible connectivity issues with HDFS", e);
        }
        return false;
    }
    Preconditions.checkNotNull(entry);
    // and is different from the one from the meta store, issue a warning.
    if (params.get(CACHE_DIR_REPLICATION_PROP_NAME) != null && Short.parseShort(params.get(CACHE_DIR_REPLICATION_PROP_NAME)) != entry.getInfo().getReplication()) {
        LOG.info("Replication factor for entry in HDFS differs from value in Hive MS: " + entry.getInfo().getPath().toString() + " " + entry.getInfo().getReplication().toString() + " != " + params.get(CACHE_DIR_REPLICATION_PROP_NAME));
    }
    params.put(CACHE_DIR_REPLICATION_PROP_NAME, String.valueOf(entry.getInfo().getReplication()));
    return true;
}
#method_after
public static boolean validateCacheParams(Map<String, String> params) {
    Long directiveId = getCacheDirectiveId(params);
    if (directiveId == null)
        return false;
    CacheDirectiveEntry entry = null;
    try {
        entry = getDirective(directiveId);
    } catch (ImpalaRuntimeException e) {
        if (e.getCause() != null && e.getCause() instanceof RemoteException) {
            // This exception signals that the cache directive no longer exists.
            LOG.error("Cache directive does not exist", e);
            params.remove(CACHE_DIR_ID_PROP_NAME);
            params.remove(CACHE_DIR_REPLICATION_PROP_NAME);
        } else {
            // This exception signals that there was a connection problem with HDFS.
            LOG.error("IO Exception, possible connectivity issues with HDFS", e);
        }
        return false;
    }
    Preconditions.checkNotNull(entry);
    // On the upgrade path the property might not exist, if it exists
    // and is different from the one from the meta store, issue a warning.
    String replicationFactor = params.get(CACHE_DIR_REPLICATION_PROP_NAME);
    if (replicationFactor != null && Short.parseShort(replicationFactor) != entry.getInfo().getReplication()) {
        LOG.info("Replication factor for entry in HDFS differs from value in Hive MS: " + entry.getInfo().getPath().toString() + " " + entry.getInfo().getReplication().toString() + " != " + params.get(CACHE_DIR_REPLICATION_PROP_NAME));
    }
    params.put(CACHE_DIR_REPLICATION_PROP_NAME, String.valueOf(entry.getInfo().getReplication()));
    return true;
}
#end_block

