950
#method_before
@Test
public void testRenameKeyColumn() throws Exception {
    KuduTable table = createTable(ImmutableList.<Pair<Integer, Integer>>of());
    syncClient.alterTable(tableName, new AlterTableOptions().renameColumn("c0", "c0Key"));
    boolean done = syncClient.isAlterTableDone(tableName);
    assertTrue(done);
    // Reopen table for the new schema.
    table = syncClient.openTable(tableName);
    assertEquals("c0Key", table.getSchema().getPrimaryKeyColumns().get(0).getName());
    assertEquals(2, table.getSchema().getColumnCount());
    // Add a row
    KuduSession session = syncClient.newSession();
    Insert insert = table.newInsert();
    PartialRow row = insert.getRow();
    row.addInt("c0Key", 101);
    row.addInt("c1", 101);
    session.apply(insert);
    session.flush();
    RowError[] rowErrors = session.getPendingErrors().getRowErrors();
    assertEquals(String.format("row errors: %s", Arrays.toString(rowErrors)), 0, rowErrors.length);
}
#method_after
@Test
public void testRenameKeyColumn() throws Exception {
    KuduTable table = createTable(ImmutableList.<Pair<Integer, Integer>>of());
    insertRows(table, 0, 100);
    assertEquals(100, countRowsInTable(table));
    syncClient.alterTable(tableName, new AlterTableOptions().renameColumn("c0", "c0Key"));
    boolean done = syncClient.isAlterTableDone(tableName);
    assertTrue(done);
    // scanning with the old schema
    try {
        KuduScanner scanner = syncClient.newScannerBuilder(table).setProjectedColumnNames(Lists.newArrayList("c0", "c1")).build();
        while (scanner.hasMoreRows()) {
            scanner.nextRows();
        }
    } catch (KuduException e) {
        assertTrue(e.getStatus().isInvalidArgument());
        assertTrue(e.getStatus().getMessage().contains("Some columns are not present in the current schema: c0"));
    }
    // Reopen table for the new schema.
    table = syncClient.openTable(tableName);
    assertEquals("c0Key", table.getSchema().getPrimaryKeyColumns().get(0).getName());
    assertEquals(2, table.getSchema().getColumnCount());
    // Add a row
    KuduSession session = syncClient.newSession();
    Insert insert = table.newInsert();
    PartialRow row = insert.getRow();
    row.addInt("c0Key", 101);
    row.addInt("c1", 101);
    session.apply(insert);
    session.flush();
    RowError[] rowErrors = session.getPendingErrors().getRowErrors();
    assertEquals(String.format("row errors: %s", Arrays.toString(rowErrors)), 0, rowErrors.length);
    KuduScanner scanner = syncClient.newScannerBuilder(table).setProjectedColumnNames(Lists.newArrayList("c0Key", "c1")).build();
    while (scanner.hasMoreRows()) {
        RowResultIterator it = scanner.nextRows();
        assertTrue(it.hasNext());
        RowResult rr = it.next();
        assertEquals(rr.getInt(0), rr.getInt(1));
    }
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    conjuncts_.clear();
    // Add bound predicates.
    conjuncts_.addAll(analyzer.getBoundPredicates(desc_.getId()));
    // Add unassigned predicates.
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(this);
    conjuncts_.addAll(unassigned);
    analyzer.markConjunctsAssigned(unassigned);
    // Add equivalence predicates.
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    Expr.removeDuplicates(conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = KuduUtil.createKuduClient(kuduTable_.getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Compute mem layout before the scan range locations because creation of the Kudu
        // scan tokens depends on having a mem layout.
        computeMemLayout(analyzer);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeStats(analyzer);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    conjuncts_.clear();
    // Add bound predicates.
    conjuncts_.addAll(analyzer.getBoundPredicates(desc_.getId()));
    // Add unassigned predicates.
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(this);
    conjuncts_.addAll(unassigned);
    analyzer.markConjunctsAssigned(unassigned);
    // Add equivalence predicates.
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    Expr.removeDuplicates(conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = KuduUtil.createKuduClient(kuduTable_.getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Compute mem layout before the scan range locations because creation of the Kudu
        // scan tokens depends on having a mem layout.
        computeMemLayout(analyzer);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    // Determine backend scan node implementation to use.
    if (analyzer.getQueryOptions().isSetMt_dop() && analyzer.getQueryOptions().mt_dop > 0) {
        useMtScanNode_ = true;
    } else {
        useMtScanNode_ = false;
    }
    computeStats(analyzer);
}
#end_block

#method_before
private void validateSchema(org.apache.kudu.client.KuduTable rpcTable) throws ImpalaRuntimeException {
    Schema tableSchema = rpcTable.getSchema();
    for (SlotDescriptor desc : getTupleDesc().getSlots()) {
        String colName = desc.getColumn().getName();
        try {
            tableSchema.getColumn(colName);
        } catch (Exception e) {
            throw new ImpalaRuntimeException("Column '" + colName + "' not found in kudu " + "table " + rpcTable.getName());
        }
    }
}
#method_after
private void validateSchema(org.apache.kudu.client.KuduTable rpcTable) throws ImpalaRuntimeException {
    Schema tableSchema = rpcTable.getSchema();
    for (SlotDescriptor desc : getTupleDesc().getSlots()) {
        String colName = desc.getColumn().getName();
        Type colType = desc.getColumn().getType();
        ColumnSchema kuduCol = null;
        try {
            kuduCol = tableSchema.getColumn(colName);
        } catch (Exception e) {
            throw new ImpalaRuntimeException("Column '" + colName + "' not found in kudu " + "table " + rpcTable.getName() + ". The table metadata in Impala may be " + "outdated and need to be refreshed.");
        }
        Type kuduColType = KuduUtil.toImpalaType(kuduCol.getType());
        if (!colType.equals(kuduColType)) {
            throw new ImpalaRuntimeException("Column '" + colName + "' is type " + kuduColType.toSql() + " but Impala expected " + colType.toSql() + ". The table metadata in Impala may be outdated and need to be refreshed.");
        }
        if (desc.getIsNullable() != kuduCol.isNullable()) {
            String expected;
            String actual;
            if (desc.getIsNullable()) {
                expected = "nullable";
                actual = "not nullable";
            } else {
                expected = "not nullable";
                actual = "nullable";
            }
            throw new ImpalaRuntimeException("Column '" + colName + "' is " + actual + " but Impala expected it to be " + expected + ". The table metadata in Impala may be outdated and need to be refreshed.");
        }
    }
}
#end_block

#method_before
@Override
protected void toThrift(TPlanNode node) {
    node.node_type = TPlanNodeType.KUDU_SCAN_NODE;
    node.kudu_scan_node = new TKuduScanNode(desc_.getId().asInt());
}
#method_after
@Override
protected void toThrift(TPlanNode node) {
    node.node_type = TPlanNodeType.KUDU_SCAN_NODE;
    node.kudu_scan_node = new TKuduScanNode(desc_.getId().asInt());
    node.kudu_scan_node.setUse_mt_scan_node(useMtScanNode_);
}
#end_block

#method_before
private boolean tryConvertBinaryKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof BinaryPredicate))
        return false;
    BinaryPredicate predicate = (BinaryPredicate) expr;
    // TODO KUDU-931 look into handling implicit/explicit casts on the SlotRef.
    predicate = normalizeSlotRefComparison(predicate, analyzer);
    if (predicate == null)
        return false;
    ComparisonOp op = getKuduOperator(predicate.getOp());
    if (op == null)
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    LiteralExpr literal = (LiteralExpr) predicate.getChild(1);
    // Cannot push predicates with null literal values (KUDU-1595).
    if (literal instanceof NullLiteral)
        return false;
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    KuduPredicate kuduPredicate = null;
    switch(literal.getType().getPrimitiveType()) {
        case BOOLEAN:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((BoolLiteral) literal).getValue());
                break;
            }
        case TINYINT:
        case SMALLINT:
        case INT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getIntValue());
                break;
            }
        case BIGINT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getLongValue());
                break;
            }
        case FLOAT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, (float) ((NumericLiteral) literal).getDoubleValue());
                break;
            }
        case DOUBLE:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getDoubleValue());
                break;
            }
        case STRING:
        case VARCHAR:
        case CHAR:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((StringLiteral) literal).getStringValue());
                break;
            }
        default:
            break;
    }
    if (kuduPredicate == null)
        return false;
    kuduConjuncts_.add(predicate);
    kuduPredicates_.add(kuduPredicate);
    return true;
}
#method_after
private boolean tryConvertBinaryKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof BinaryPredicate))
        return false;
    BinaryPredicate predicate = (BinaryPredicate) expr;
    // TODO KUDU-931 look into handling implicit/explicit casts on the SlotRef.
    ComparisonOp op = getKuduOperator(predicate.getOp());
    if (op == null)
        return false;
    if (!(predicate.getChild(0) instanceof SlotRef))
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    if (!(predicate.getChild(1) instanceof LiteralExpr))
        return false;
    LiteralExpr literal = (LiteralExpr) predicate.getChild(1);
    // Cannot push predicates with null literal values (KUDU-1595).
    if (literal instanceof NullLiteral)
        return false;
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    KuduPredicate kuduPredicate = null;
    switch(literal.getType().getPrimitiveType()) {
        case BOOLEAN:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((BoolLiteral) literal).getValue());
                break;
            }
        case TINYINT:
        case SMALLINT:
        case INT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getIntValue());
                break;
            }
        case BIGINT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getLongValue());
                break;
            }
        case FLOAT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, (float) ((NumericLiteral) literal).getDoubleValue());
                break;
            }
        case DOUBLE:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getDoubleValue());
                break;
            }
        case STRING:
        case VARCHAR:
        case CHAR:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((StringLiteral) literal).getStringValue());
                break;
            }
        default:
            break;
    }
    if (kuduPredicate == null)
        return false;
    kuduConjuncts_.add(predicate);
    kuduPredicates_.add(kuduPredicate);
    return true;
}
#end_block

#method_before
private boolean tryConvertIsNullKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof IsNullPredicate))
        return false;
    IsNullPredicate predicate = (IsNullPredicate) expr;
    // Do not convert if expression is more than a SlotRef
    if (!(predicate.getChild(0) instanceof SlotRef))
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    KuduPredicate kuduPredicate = null;
    if (predicate.isNotNull()) {
        kuduPredicate = KuduPredicate.newIsNotNullPredicate(column);
    } else {
        kuduPredicate = KuduPredicate.newIsNullPredicate(column);
    }
    kuduConjuncts_.add(predicate);
    kuduPredicates_.add(kuduPredicate);
    return true;
}
#method_after
private boolean tryConvertIsNullKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof IsNullPredicate))
        return false;
    IsNullPredicate predicate = (IsNullPredicate) expr;
    // is NULL.
    if (!(predicate.getChild(0) instanceof SlotRef))
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    KuduPredicate kuduPredicate = null;
    if (predicate.isNotNull()) {
        kuduPredicate = KuduPredicate.newIsNotNullPredicate(column);
    } else {
        kuduPredicate = KuduPredicate.newIsNullPredicate(column);
    }
    kuduConjuncts_.add(predicate);
    kuduPredicates_.add(kuduPredicate);
    return true;
}
#end_block

#method_before
private UnionNode createUnionPlan(Analyzer analyzer, UnionStmt unionStmt, List<UnionOperand> unionOperands, PlanNode unionDistinctPlan) throws ImpalaException {
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), unionStmt.getTupleId(), unionStmt.getUnionNodeResultExprs());
    for (UnionOperand op : unionOperands) {
        if (op.getAnalyzer().hasEmptyResultSet()) {
            unmarkCollectionSlots(op.getQueryStmt());
            continue;
        }
        QueryStmt queryStmt = op.getQueryStmt();
        if (queryStmt instanceof SelectStmt) {
            SelectStmt selectStmt = (SelectStmt) queryStmt;
            if (selectStmt.getTableRefs().isEmpty()) {
                unionNode.addConstExprList(selectStmt.getResultExprs());
                continue;
            }
        }
        PlanNode opPlan = createQueryPlan(queryStmt, op.getAnalyzer(), false);
        // There may still be unassigned conjuncts if the operand has an order by + limit.
        // Place them into a SelectNode on top of the operand's plan.
        opPlan = addUnassignedConjuncts(analyzer, opPlan.getTupleIds(), opPlan);
        if (opPlan instanceof EmptySetNode)
            continue;
        unionNode.addChild(opPlan, op.getQueryStmt().getResultExprs());
    }
    if (unionDistinctPlan != null) {
        Preconditions.checkState(unionStmt.hasDistinctOps());
        Preconditions.checkState(unionDistinctPlan instanceof AggregationNode);
        unionNode.addChild(unionDistinctPlan, unionStmt.getDistinctAggInfo().getGroupingExprs());
    }
    unionNode.init(analyzer);
    return unionNode;
}
#method_after
private UnionNode createUnionPlan(Analyzer analyzer, UnionStmt unionStmt, List<UnionOperand> unionOperands, PlanNode unionDistinctPlan) throws ImpalaException {
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), unionStmt.getTupleId(), unionStmt.getUnionResultExprs(), ctx_.hasSubplan());
    for (UnionOperand op : unionOperands) {
        if (op.getAnalyzer().hasEmptyResultSet()) {
            unmarkCollectionSlots(op.getQueryStmt());
            continue;
        }
        QueryStmt queryStmt = op.getQueryStmt();
        if (queryStmt instanceof SelectStmt) {
            SelectStmt selectStmt = (SelectStmt) queryStmt;
            if (selectStmt.getTableRefs().isEmpty()) {
                unionNode.addConstExprList(selectStmt.getResultExprs());
                continue;
            }
        }
        PlanNode opPlan = createQueryPlan(queryStmt, op.getAnalyzer(), false);
        // There may still be unassigned conjuncts if the operand has an order by + limit.
        // Place them into a SelectNode on top of the operand's plan.
        opPlan = addUnassignedConjuncts(analyzer, opPlan.getTupleIds(), opPlan);
        if (opPlan instanceof EmptySetNode)
            continue;
        unionNode.addChild(opPlan, op.getQueryStmt().getResultExprs());
    }
    if (unionDistinctPlan != null) {
        Preconditions.checkState(unionStmt.hasDistinctOps());
        Preconditions.checkState(unionDistinctPlan instanceof AggregationNode);
        unionNode.addChild(unionDistinctPlan, unionStmt.getDistinctAggInfo().getGroupingExprs());
    }
    unionNode.init(analyzer);
    return unionNode;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    try {
        super.analyze(analyzer);
    } catch (AnalysisException e) {
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    Preconditions.checkState(operands_.size() > 0);
    // Propagates DISTINCT from right to left.
    propagateDistinct();
    // Analyze all operands and make sure they return an equal number of exprs.
    analyzeOperands(analyzer);
    // Remember the SQL string before unnesting operands.
    toSqlString_ = toSql();
    // Unnest the operands before casting the result exprs. Unnesting may add
    // additional entries to operands_ and the result exprs of those unnested
    // operands must also be cast properly.
    unnestOperands(analyzer);
    // Compute hasAnalyticExprs_
    hasAnalyticExprs_ = false;
    for (UnionOperand op : operands_) {
        if (op.hasAnalyticExprs()) {
            hasAnalyticExprs_ = true;
            break;
        }
    }
    // Collect all result expr lists and cast the exprs as necessary.
    List<List<Expr>> resultExprLists = Lists.newArrayList();
    for (UnionOperand op : operands_) {
        resultExprLists.add(op.getQueryStmt().getResultExprs());
    }
    analyzer.castToUnionCompatibleTypes(resultExprLists);
    // Create tuple descriptor materialized by this UnionStmt, its resultExprs, and
    // its sortInfo if necessary.
    createMetadata(analyzer);
    createSortInfo(analyzer);
    // Create unnested operands' smaps.
    for (UnionOperand operand : operands_) setOperandSmap(operand, analyzer);
    // Create distinctAggInfo, if necessary.
    if (!distinctOperands_.isEmpty()) {
        // Aggregate produces exactly the same tuple as the original union stmt.
        ArrayList<Expr> groupingExprs = Expr.cloneList(resultExprs_);
        try {
            distinctAggInfo_ = AggregateInfo.create(groupingExprs, null, analyzer.getDescTbl().getTupleDesc(tupleId_), analyzer);
        } catch (AnalysisException e) {
            // Should never happen.
            throw new IllegalStateException("Error creating agg info in UnionStmt.analyze()", e);
        }
    }
    unionNodeResultExprs_ = Expr.cloneList(resultExprs_);
    if (evaluateOrderBy_)
        createSortTupleInfo(analyzer);
    baseTblResultExprs_ = resultExprs_;
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    try {
        super.analyze(analyzer);
    } catch (AnalysisException e) {
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    Preconditions.checkState(operands_.size() > 0);
    // Propagates DISTINCT from right to left.
    propagateDistinct();
    // Analyze all operands and make sure they return an equal number of exprs.
    analyzeOperands(analyzer);
    // Remember the SQL string before unnesting operands.
    toSqlString_ = toSql();
    // Unnest the operands before casting the result exprs. Unnesting may add
    // additional entries to operands_ and the result exprs of those unnested
    // operands must also be cast properly.
    unnestOperands(analyzer);
    // Compute hasAnalyticExprs_
    hasAnalyticExprs_ = false;
    for (UnionOperand op : operands_) {
        if (op.hasAnalyticExprs()) {
            hasAnalyticExprs_ = true;
            break;
        }
    }
    // Collect all result expr lists and cast the exprs as necessary.
    List<List<Expr>> resultExprLists = Lists.newArrayList();
    for (UnionOperand op : operands_) {
        resultExprLists.add(op.getQueryStmt().getResultExprs());
    }
    analyzer.castToUnionCompatibleTypes(resultExprLists);
    // Create tuple descriptor materialized by this UnionStmt, its resultExprs, and
    // its sortInfo if necessary.
    createMetadata(analyzer);
    createSortInfo(analyzer);
    // Create unnested operands' smaps.
    for (UnionOperand operand : operands_) setOperandSmap(operand, analyzer);
    // Create distinctAggInfo, if necessary.
    if (!distinctOperands_.isEmpty()) {
        // Aggregate produces exactly the same tuple as the original union stmt.
        ArrayList<Expr> groupingExprs = Expr.cloneList(resultExprs_);
        try {
            distinctAggInfo_ = AggregateInfo.create(groupingExprs, null, analyzer.getDescTbl().getTupleDesc(tupleId_), analyzer);
        } catch (AnalysisException e) {
            // Should never happen.
            throw new IllegalStateException("Error creating agg info in UnionStmt.analyze()", e);
        }
    }
    unionResultExprs_ = Expr.cloneList(resultExprs_);
    if (evaluateOrderBy_)
        createSortTupleInfo(analyzer);
    baseTblResultExprs_ = resultExprs_;
}
#end_block

#method_before
private void createMetadata(Analyzer analyzer) throws AnalysisException {
    // Create tuple descriptor for materialized tuple created by the union.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    tupleId_ = tupleDesc.getId();
    if (LOG.isTraceEnabled()) {
        LOG.trace("UnionStmt.createMetadata: tupleId=" + tupleId_.toString());
    }
    // One slot per expr in the select blocks. Use first select block as representative.
    List<Expr> firstSelectExprs = operands_.get(0).getQueryStmt().getResultExprs();
    // Compute column stats for the materialized slots from the source exprs.
    List<ColumnStats> columnStats = Lists.newArrayList();
    for (int i = 0; i < operands_.size(); ++i) {
        List<Expr> selectExprs = operands_.get(i).getQueryStmt().getResultExprs();
        for (int j = 0; j < selectExprs.size(); ++j) {
            ColumnStats statsToAdd = ColumnStats.fromExpr(selectExprs.get(j));
            if (i == 0) {
                columnStats.add(statsToAdd);
            } else {
                columnStats.get(j).add(statsToAdd);
            }
        }
    }
    // Create tuple descriptor and slots.
    for (int i = 0; i < firstSelectExprs.size(); ++i) {
        Expr expr = firstSelectExprs.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(getColLabels().get(i));
        slotDesc.setType(expr.getType());
        slotDesc.setStats(columnStats.get(i));
        SlotRef outputSlotRef = new SlotRef(slotDesc);
        resultExprs_.add(outputSlotRef);
        // Add to aliasSMap so that column refs in "order by" can be resolved.
        if (orderByElements_ != null) {
            SlotRef aliasRef = new SlotRef(getColLabels().get(i));
            if (aliasSmap_.containsMappingFor(aliasRef)) {
                ambiguousAliasList_.add(aliasRef);
            } else {
                aliasSmap_.put(aliasRef, outputSlotRef);
            }
        }
        // (see Planner.createInlineViewPlan() for the reasoning)
        for (UnionOperand op : operands_) {
            Expr resultExpr = op.getQueryStmt().getResultExprs().get(i);
            slotDesc.addSourceExpr(resultExpr);
            if (op.hasAnalyticExprs())
                continue;
            SlotRef slotRef = resultExpr.unwrapSlotRef(true);
            if (slotRef == null)
                continue;
            analyzer.registerValueTransfer(outputSlotRef.getSlotId(), slotRef.getSlotId());
        }
    }
    baseTblResultExprs_ = resultExprs_;
}
#method_after
private void createMetadata(Analyzer analyzer) throws AnalysisException {
    // Create tuple descriptor for materialized tuple created by the union.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    tupleId_ = tupleDesc.getId();
    if (LOG.isTraceEnabled()) {
        LOG.trace("UnionStmt.createMetadata: tupleId=" + tupleId_.toString());
    }
    // One slot per expr in the select blocks. Use first select block as representative.
    List<Expr> firstSelectExprs = operands_.get(0).getQueryStmt().getResultExprs();
    // Compute column stats for the materialized slots from the source exprs.
    List<ColumnStats> columnStats = Lists.newArrayList();
    for (int i = 0; i < operands_.size(); ++i) {
        List<Expr> selectExprs = operands_.get(i).getQueryStmt().getResultExprs();
        for (int j = 0; j < selectExprs.size(); ++j) {
            ColumnStats statsToAdd = ColumnStats.fromExpr(selectExprs.get(j));
            if (i == 0) {
                columnStats.add(statsToAdd);
            } else {
                columnStats.get(j).add(statsToAdd);
            }
        }
    }
    // Create tuple descriptor and slots.
    for (int i = 0; i < firstSelectExprs.size(); ++i) {
        Expr expr = firstSelectExprs.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(getColLabels().get(i));
        slotDesc.setType(expr.getType());
        slotDesc.setStats(columnStats.get(i));
        SlotRef outputSlotRef = new SlotRef(slotDesc);
        resultExprs_.add(outputSlotRef);
        // Add to aliasSMap so that column refs in "order by" can be resolved.
        if (orderByElements_ != null) {
            SlotRef aliasRef = new SlotRef(getColLabels().get(i));
            if (aliasSmap_.containsMappingFor(aliasRef)) {
                ambiguousAliasList_.add(aliasRef);
            } else {
                aliasSmap_.put(aliasRef, outputSlotRef);
            }
        }
        boolean isNullable = false;
        // (see Planner.createInlineViewPlan() for the reasoning)
        for (UnionOperand op : operands_) {
            Expr resultExpr = op.getQueryStmt().getResultExprs().get(i);
            slotDesc.addSourceExpr(resultExpr);
            SlotRef slotRef = resultExpr.unwrapSlotRef(false);
            if (slotRef == null || slotRef.getDesc().getIsNullable())
                isNullable = true;
            if (op.hasAnalyticExprs())
                continue;
            slotRef = resultExpr.unwrapSlotRef(true);
            if (slotRef == null)
                continue;
            analyzer.registerValueTransfer(outputSlotRef.getSlotId(), slotRef.getSlotId());
        }
        // If all the child slots are not nullable, then the union output slot should not
        // be nullable as well.
        slotDesc.setIsNullable(isNullable);
    }
    baseTblResultExprs_ = resultExprs_;
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    for (UnionOperand op : operands_) op.reset();
    distinctOperands_.clear();
    allOperands_.clear();
    distinctAggInfo_ = null;
    tupleId_ = null;
    toSqlString_ = null;
    hasAnalyticExprs_ = false;
    unionNodeResultExprs_.clear();
}
#method_after
@Override
public void reset() {
    super.reset();
    for (UnionOperand op : operands_) op.reset();
    distinctOperands_.clear();
    allOperands_.clear();
    distinctAggInfo_ = null;
    tupleId_ = null;
    toSqlString_ = null;
    hasAnalyticExprs_ = false;
    unionResultExprs_.clear();
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) {
    Preconditions.checkState(conjuncts_.isEmpty());
    computeMemLayout(analyzer);
    computeStats(analyzer);
    // drop resultExprs/constExprs that aren't getting materialized (= where the
    // corresponding output slot isn't being materialized)
    materializedResultExprLists_.clear();
    Preconditions.checkState(resultExprLists_.size() == children_.size());
    List<SlotDescriptor> slots = analyzer.getDescTbl().getTupleDesc(tupleId_).getSlots();
    for (int i = 0; i < resultExprLists_.size(); ++i) {
        List<Expr> exprList = resultExprLists_.get(i);
        List<Expr> newExprList = Lists.newArrayList();
        Preconditions.checkState(exprList.size() == slots.size());
        for (int j = 0; j < exprList.size(); ++j) {
            if (slots.get(j).isMaterialized())
                newExprList.add(exprList.get(j));
        }
        materializedResultExprLists_.add(Expr.substituteList(newExprList, getChild(i).getOutputSmap(), analyzer, true));
    }
    Preconditions.checkState(materializedResultExprLists_.size() == getChildren().size());
    materializedConstExprLists_.clear();
    for (List<Expr> exprList : constExprLists_) {
        Preconditions.checkState(exprList.size() == slots.size());
        List<Expr> newExprList = Lists.newArrayList();
        for (int i = 0; i < exprList.size(); ++i) {
            if (slots.get(i).isMaterialized())
                newExprList.add(exprList.get(i));
        }
        materializedConstExprLists_.add(newExprList);
    }
    passThrough.clear();
    TupleDescriptor this_tuple_desc = analyzer.getDescTbl().getTupleDesc(tupleId_);
    for (int i = 0; i < resultExprLists_.size(); ++i) {
        passThrough.add(computePassThroughNodes(analyzer, children_.get(i).getTupleIds(), resultExprLists_.get(i)));
    }
}
#method_after
@Override
public void init(Analyzer analyzer) {
    Preconditions.checkState(conjuncts_.isEmpty());
    computeMemLayout(analyzer);
    computeStats(analyzer);
    computePassthrough(analyzer);
    // drop resultExprs/constExprs that aren't getting materialized (= where the
    // corresponding output slot isn't being materialized)
    materializedResultExprLists_.clear();
    Preconditions.checkState(resultExprLists_.size() == children_.size());
    List<SlotDescriptor> slots = analyzer.getDescTbl().getTupleDesc(tupleId_).getSlots();
    for (int i = 0; i < resultExprLists_.size(); ++i) {
        List<Expr> exprList = resultExprLists_.get(i);
        List<Expr> newExprList = Lists.newArrayList();
        Preconditions.checkState(exprList.size() == slots.size());
        for (int j = 0; j < exprList.size(); ++j) {
            if (slots.get(j).isMaterialized())
                newExprList.add(exprList.get(j));
        }
        materializedResultExprLists_.add(Expr.substituteList(newExprList, getChild(i).getOutputSmap(), analyzer, true));
    }
    Preconditions.checkState(materializedResultExprLists_.size() == getChildren().size());
    materializedConstExprLists_.clear();
    for (List<Expr> exprList : constExprLists_) {
        Preconditions.checkState(exprList.size() == slots.size());
        List<Expr> newExprList = Lists.newArrayList();
        for (int i = 0; i < exprList.size(); ++i) {
            if (slots.get(i).isMaterialized())
                newExprList.add(exprList.get(i));
        }
        materializedConstExprLists_.add(newExprList);
    }
}
#end_block

#method_before
@Override
protected void toThrift(TPlanNode msg) {
    Preconditions.checkState(materializedResultExprLists_.size() == children_.size());
    List<List<TExpr>> texprLists = Lists.newArrayList();
    for (List<Expr> exprList : materializedResultExprLists_) {
        texprLists.add(Expr.treesToThrift(exprList));
    }
    List<List<TExpr>> constTexprLists = Lists.newArrayList();
    for (List<Expr> constTexprList : materializedConstExprLists_) {
        constTexprLists.add(Expr.treesToThrift(constTexprList));
    }
    msg.union_node = new TUnionNode(tupleId_.asInt(), texprLists, constTexprLists, passThrough);
    msg.node_type = TPlanNodeType.UNION_NODE;
}
#method_after
@Override
protected void toThrift(TPlanNode msg) {
    Preconditions.checkState(materializedResultExprLists_.size() == children_.size());
    List<List<TExpr>> texprLists = Lists.newArrayList();
    for (List<Expr> exprList : materializedResultExprLists_) {
        texprLists.add(Expr.treesToThrift(exprList));
    }
    List<List<TExpr>> constTexprLists = Lists.newArrayList();
    for (List<Expr> constTexprList : materializedConstExprLists_) {
        constTexprLists.add(Expr.treesToThrift(constTexprList));
    }
    Preconditions.checkState(firstMaterializedChildIdx_ <= children_.size());
    msg.union_node = new TUnionNode(tupleId_.asInt(), texprLists, constTexprLists, firstMaterializedChildIdx_);
    msg.node_type = TPlanNodeType.UNION_NODE;
}
#end_block

#method_before
@Override
protected String getNodeExplainString(String prefix, String detailPrefix, TExplainLevel detailLevel) {
    StringBuilder output = new StringBuilder();
    output.append(String.format("%s%s:%s\n", prefix, id_.toString(), displayName_));
    // and the enclosing select stmt has predicates referring to the inline view.
    if (!conjuncts_.isEmpty()) {
        output.append(detailPrefix + "predicates: " + getExplainString(conjuncts_) + "\n");
    }
    if (!constExprLists_.isEmpty()) {
        output.append(detailPrefix + "constant-operands=" + constExprLists_.size() + "\n");
    }
    if (!passThrough.isEmpty()) {
        List<String> passThroughNodes = Lists.newArrayList();
        for (int i = 0; i < passThrough.size(); ++i) {
            if (passThrough.get(i)) {
                passThroughNodes.add(children_.get(i).getId().toString());
            }
        }
        if (!passThroughNodes.isEmpty()) {
            output.append(detailPrefix + "pass through nodes: " + Joiner.on(", ").join(passThroughNodes) + "\n");
        }
    }
    return output.toString();
}
#method_after
@Override
protected String getNodeExplainString(String prefix, String detailPrefix, TExplainLevel detailLevel) {
    StringBuilder output = new StringBuilder();
    output.append(String.format("%s%s:%s\n", prefix, id_.toString(), displayName_));
    // and the enclosing select stmt has predicates referring to the inline view.
    if (!conjuncts_.isEmpty()) {
        output.append(detailPrefix + "predicates: " + getExplainString(conjuncts_) + "\n");
    }
    if (!constExprLists_.isEmpty()) {
        output.append(detailPrefix + "constant-operands=" + constExprLists_.size() + "\n");
    }
    if (detailLevel.ordinal() > TExplainLevel.MINIMAL.ordinal()) {
        List<String> passThroughNodeIds = Lists.newArrayList();
        for (int i = 0; i < firstMaterializedChildIdx_; ++i) {
            passThroughNodeIds.add(children_.get(i).getId().toString());
        }
        if (!passThroughNodeIds.isEmpty()) {
            String result = detailPrefix + "pass-through-operands: ";
            if (passThroughNodeIds.size() == children_.size()) {
                output.append(result + "all\n");
            } else {
                output.append(result + Joiner.on(",").join(passThroughNodeIds) + "\n");
            }
        }
    }
    return output.toString();
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    conjuncts_.clear();
    // Add bound predicates.
    conjuncts_.addAll(analyzer.getBoundPredicates(desc_.getId()));
    // Add unassigned predicates.
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(this);
    conjuncts_.addAll(unassigned);
    analyzer.markConjunctsAssigned(unassigned);
    // Add equivalence predicates.
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    Expr.removeDuplicates(conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = KuduUtil.createKuduClient(kuduTable_.getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Compute mem layout before the scan range locations because creation of the Kudu
        // scan tokens depends on having a mem layout.
        computeMemLayout(analyzer);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    // is currently only supported for Parquet.
    if (analyzer.getQueryOptions().isSetMt_dop() && analyzer.getQueryOptions().mt_dop > 0) {
        useMtScanNode_ = true;
    } else {
        useMtScanNode_ = false;
    }
    computeStats(analyzer);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    conjuncts_.clear();
    // Add bound predicates.
    conjuncts_.addAll(analyzer.getBoundPredicates(desc_.getId()));
    // Add unassigned predicates.
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(this);
    conjuncts_.addAll(unassigned);
    analyzer.markConjunctsAssigned(unassigned);
    // Add equivalence predicates.
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    Expr.removeDuplicates(conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = KuduUtil.createKuduClient(kuduTable_.getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Compute mem layout before the scan range locations because creation of the Kudu
        // scan tokens depends on having a mem layout.
        computeMemLayout(analyzer);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    // Determine backend scan node implementation to use.
    if (analyzer.getQueryOptions().isSetMt_dop() && analyzer.getQueryOptions().mt_dop > 0) {
        useMtScanNode_ = true;
    } else {
        useMtScanNode_ = false;
    }
    computeStats(analyzer);
}
#end_block

#method_before
private void alterTableRecoverPartitions(Table tbl) throws ImpalaException {
    Preconditions.checkArgument(tbl.getLock().isHeldByCurrentThread());
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an HDFS table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    List<List<String>> partitionsNotInHms = hdfsTable.getPathsWithoutPartitions();
    if (partitionsNotInHms.isEmpty())
        return;
    List<Partition> hmsPartitions = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    TableName tableName = tbl.getTableName();
    for (List<String> partitionSpecValues : partitionsNotInHms) {
        hmsPartitions.add(createHmsPartitionFromValues(partitionSpecValues, msTbl, tableName, null));
    }
    String cachePoolName = null;
    Short replication = null;
    List<Long> cacheIds = Lists.newArrayList();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (parentTblCacheDirId != null) {
        // Inherit the HDFS cache value from the parent table.
        cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
        Preconditions.checkNotNull(cachePoolName);
        replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
        Preconditions.checkNotNull(replication);
    }
    // Add partitions to metastore.
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Apply the updates in batches of 'MAX_PARTITION_UPDATES_PER_RPC'.
        for (int i = 0; i < hmsPartitions.size(); i += MAX_PARTITION_UPDATES_PER_RPC) {
            int endPartitionIndex = Math.min(i + MAX_PARTITION_UPDATES_PER_RPC, hmsPartitions.size());
            List<Partition> hmsSublist = hmsPartitions.subList(i, endPartitionIndex);
            // ifNotExists and needResults are true.
            List<Partition> hmsAddedPartitions;
            try {
                hmsAddedPartitions = msClient.getHiveClient().add_partitions(hmsSublist, true, true);
            } catch (AlreadyExistsException e) {
                throw new InternalException("AlreadyExistsException thrown although ifNotExists given", e);
            }
            for (Partition partition : hmsAddedPartitions) {
                // Create and add the HdfsPartition. Return the table object with an updated
                // catalog version.
                addHdfsPartition(tbl, partition);
            }
            // Handle HDFS cache.
            if (cachePoolName != null) {
                for (Partition partition : hmsAddedPartitions) {
                    long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
                    cacheIds.add(id);
                }
                // Update the partition metadata to include the cache directive id.
                msClient.getHiveClient().alter_partitions(tableName.getDb(), tableName.getTbl(), hmsAddedPartitions);
            }
            updateLastDdlTime(msTbl, msClient);
        }
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    }
    if (!cacheIds.isEmpty()) {
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    }
}
#method_after
private void alterTableRecoverPartitions(Table tbl) throws ImpalaException {
    Preconditions.checkArgument(tbl.getLock().isHeldByCurrentThread());
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an HDFS table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    List<List<String>> partitionsNotInHms = hdfsTable.getPathsWithoutPartitions();
    if (partitionsNotInHms.isEmpty())
        return;
    List<Partition> hmsPartitions = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    TableName tableName = tbl.getTableName();
    for (List<String> partitionSpecValues : partitionsNotInHms) {
        hmsPartitions.add(createHmsPartitionFromValues(partitionSpecValues, msTbl, tableName, null));
    }
    String cachePoolName = null;
    Short replication = null;
    List<Long> cacheIds = Lists.newArrayList();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (parentTblCacheDirId != null) {
        // Inherit the HDFS cache value from the parent table.
        cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
        Preconditions.checkNotNull(cachePoolName);
        replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
        Preconditions.checkNotNull(replication);
    }
    // Add partitions to metastore.
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Apply the updates in batches of 'MAX_PARTITION_UPDATES_PER_RPC'.
        for (int i = 0; i < hmsPartitions.size(); i += MAX_PARTITION_UPDATES_PER_RPC) {
            int endPartitionIndex = Math.min(i + MAX_PARTITION_UPDATES_PER_RPC, hmsPartitions.size());
            List<Partition> hmsSublist = hmsPartitions.subList(i, endPartitionIndex);
            // ifNotExists and needResults are true.
            List<Partition> hmsAddedPartitions = msClient.getHiveClient().add_partitions(hmsSublist, true, true);
            for (Partition partition : hmsAddedPartitions) {
                // Create and add the HdfsPartition. Return the table object with an updated
                // catalog version.
                addHdfsPartition(tbl, partition);
            }
            // Handle HDFS cache.
            if (cachePoolName != null) {
                for (Partition partition : hmsAddedPartitions) {
                    long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
                    cacheIds.add(id);
                }
                // Update the partition metadata to include the cache directive id.
                msClient.getHiveClient().alter_partitions(tableName.getDb(), tableName.getTbl(), hmsAddedPartitions);
            }
            updateLastDdlTime(msTbl, msClient);
        }
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    }
    if (!cacheIds.isEmpty()) {
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    }
}
#end_block

#method_before
public static byte[] getJvmThreadsInfo(byte[] argument) throws ImpalaException {
    TGetJvmThreadsInfoRequest request = new TGetJvmThreadsInfoRequest();
    JniUtil.deserializeThrift(protocolFactory_, request, argument);
    TGetJvmThreadsInfoResponse response = new TGetJvmThreadsInfoResponse();
    ThreadMXBean threadBean = ManagementFactory.getThreadMXBean();
    int threadCount = threadBean.getThreadCount();
    response.setTotal_thread_count(threadCount);
    response.setDaemon_thread_count(threadBean.getDaemonThreadCount());
    response.setPeak_thread_count(threadBean.getPeakThreadCount());
    if (request.get_complete_info) {
        for (ThreadInfo threadInfo : threadBean.dumpAllThreads(true, true)) {
            TJvmThreadInfo tThreadInfo = new TJvmThreadInfo();
            long id = threadInfo.getThreadId();
            tThreadInfo.setSummary(threadInfo.toString());
            tThreadInfo.setCpu_time_in_ns(threadBean.getThreadCpuTime(id));
            tThreadInfo.setUser_time_in_ns(threadBean.getThreadUserTime(id));
            tThreadInfo.setBlocked_count(threadInfo.getBlockedCount());
            tThreadInfo.setBlocked_time_in_ms(threadInfo.getBlockedTime());
            tThreadInfo.setIs_in_native(threadInfo.isInNative());
            response.addToThreads(tThreadInfo);
        }
    }
    TSerializer serializer = new TSerializer(protocolFactory_);
    try {
        return serializer.serialize(response);
    } catch (TException e) {
        throw new InternalException(e.getMessage());
    }
}
#method_after
public static byte[] getJvmThreadsInfo(byte[] argument) throws ImpalaException {
    TGetJvmThreadsInfoRequest request = new TGetJvmThreadsInfoRequest();
    JniUtil.deserializeThrift(protocolFactory_, request, argument);
    TGetJvmThreadsInfoResponse response = new TGetJvmThreadsInfoResponse();
    ThreadMXBean threadBean = ManagementFactory.getThreadMXBean();
    response.setTotal_thread_count(threadBean.getThreadCount());
    response.setDaemon_thread_count(threadBean.getDaemonThreadCount());
    response.setPeak_thread_count(threadBean.getPeakThreadCount());
    if (request.get_complete_info) {
        for (ThreadInfo threadInfo : threadBean.dumpAllThreads(true, true)) {
            TJvmThreadInfo tThreadInfo = new TJvmThreadInfo();
            long id = threadInfo.getThreadId();
            tThreadInfo.setSummary(threadInfo.toString());
            tThreadInfo.setCpu_time_in_ns(threadBean.getThreadCpuTime(id));
            tThreadInfo.setUser_time_in_ns(threadBean.getThreadUserTime(id));
            tThreadInfo.setBlocked_count(threadInfo.getBlockedCount());
            tThreadInfo.setBlocked_time_in_ms(threadInfo.getBlockedTime());
            tThreadInfo.setIs_in_native(threadInfo.isInNative());
            response.addToThreads(tThreadInfo);
        }
    }
    TSerializer serializer = new TSerializer(protocolFactory_);
    try {
        return serializer.serialize(response);
    } catch (TException e) {
        throw new InternalException(e.getMessage());
    }
}
#end_block

#method_before
public static TableSink create(Table table, Op sinkAction, List<Expr> partitionKeyExprs, List<Integer> referencedColumns, boolean overwrite, boolean inputIsClustered, List<Integer> sortByColumns) {
    if (table instanceof HdfsTable) {
        // Hdfs only supports inserts.
        Preconditions.checkState(sinkAction == Op.INSERT);
        // Referenced columns don't make sense for an Hdfs table.
        Preconditions.checkState(referencedColumns.isEmpty());
        return new HdfsTableSink(table, partitionKeyExprs, overwrite, inputIsClustered, sortByColumns);
    } else if (table instanceof HBaseTable) {
        // HBase only supports inserts.
        Preconditions.checkState(sinkAction == Op.INSERT);
        // Partition clause doesn't make sense for an HBase table.
        Preconditions.checkState(partitionKeyExprs.isEmpty());
        // HBase doesn't have a way to perform INSERT OVERWRITE
        Preconditions.checkState(overwrite == false);
        // Referenced columns don't make sense for an HBase table.
        Preconditions.checkState(referencedColumns.isEmpty());
        // Create the HBaseTableSink and return it.
        return new HBaseTableSink(table);
    } else if (table instanceof KuduTable) {
        // Kudu doesn't have a way to perform INSERT OVERWRITE.
        Preconditions.checkState(overwrite == false);
        // Partition clauses don't make sense for Kudu inserts.
        Preconditions.checkState(partitionKeyExprs.isEmpty());
        return new KuduTableSink(table, sinkAction, referencedColumns);
    } else {
        throw new UnsupportedOperationException("Cannot create data sink into table of type: " + table.getClass().getName());
    }
}
#method_after
public static TableSink create(Table table, Op sinkAction, List<Expr> partitionKeyExprs, List<Integer> referencedColumns, boolean overwrite, boolean inputIsClustered, List<Integer> sortByColumns) {
    Preconditions.checkNotNull(partitionKeyExprs);
    Preconditions.checkNotNull(referencedColumns);
    Preconditions.checkNotNull(sortByColumns);
    if (table instanceof HdfsTable) {
        // Hdfs only supports inserts.
        Preconditions.checkState(sinkAction == Op.INSERT);
        // Referenced columns don't make sense for an Hdfs table.
        Preconditions.checkState(referencedColumns.isEmpty());
        return new HdfsTableSink(table, partitionKeyExprs, overwrite, inputIsClustered, sortByColumns);
    } else if (table instanceof HBaseTable) {
        // HBase only supports inserts.
        Preconditions.checkState(sinkAction == Op.INSERT);
        // Partition clause doesn't make sense for an HBase table.
        Preconditions.checkState(partitionKeyExprs.isEmpty());
        // HBase doesn't have a way to perform INSERT OVERWRITE
        Preconditions.checkState(overwrite == false);
        // Referenced columns don't make sense for an HBase table.
        Preconditions.checkState(referencedColumns.isEmpty());
        // sortby() hint is not supported for HBase tables.
        Preconditions.checkState(sortByColumns.isEmpty());
        // Create the HBaseTableSink and return it.
        return new HBaseTableSink(table);
    } else if (table instanceof KuduTable) {
        // Kudu doesn't have a way to perform INSERT OVERWRITE.
        Preconditions.checkState(overwrite == false);
        // Partition clauses don't make sense for Kudu inserts.
        Preconditions.checkState(partitionKeyExprs.isEmpty());
        // sortby() hint is not supported for Kudu tables.
        Preconditions.checkState(sortByColumns.isEmpty());
        return new KuduTableSink(table, sinkAction, referencedColumns);
    } else {
        throw new UnsupportedOperationException("Cannot create data sink into table of type: " + table.getClass().getName());
    }
}
#end_block

#method_before
public static void Install(TLogLevel impalaLogLevel, TLogLevel otherLogLevel) throws InternalException {
    Properties properties = new Properties();
    properties.setProperty("log4j.appender.glog", GlogAppender.class.getName());
    // These settings are relatively subtle. log4j provides many ways to filter log
    // messages, and configuring them in the right order is a bit of black magic.
    // 
    // The 'Threshold' property supercedes everything, so must be set to its most
    // permissive and applies to any message sent to the glog appender.
    // 
    // The 'rootLogger' property controls the default maximum logging level (where more
    // verbose->larger logging level) for the entire space of classes. This will apply to
    // all non-Impala classes, so is set to otherLogLevel.
    // 
    // Finally we can configure per-package logging which overrides the rootLogger
    // setting. In order to control Impala's logging independently of the rest of the
    // world, we set the log level for org.apache.impala.
    properties.setProperty("log4j.rootLogger", log4jLevelForTLogLevel(otherLogLevel) + ",glog");
    properties.setProperty("log4j.appender.glog.Threshold", "TRACE");
    properties.setProperty("log4j.logger.org.apache.impala", log4jLevelForTLogLevel(impalaLogLevel));
    PropertyConfigurator.configure(properties);
    Logger.getLogger(GlogAppender.class).info(String.format("Logging initialized. " + "Impala: %s, All other: %s", impalaLogLevel, otherLogLevel));
}
#method_after
public static void Install(TLogLevel impalaLogLevel, TLogLevel otherLogLevel) throws InternalException {
    Properties properties = new Properties();
    properties.setProperty("log4j.appender.glog", GlogAppender.class.getName());
    // These settings are relatively subtle. log4j provides many ways to filter log
    // messages, and configuring them in the right order is a bit of black magic.
    // 
    // The 'Threshold' property supercedes everything, so must be set to its most
    // permissive and applies to any message sent to the glog appender.
    // 
    // The 'rootLogger' property controls the default maximum logging level (where more
    // verbose->larger logging level) for the entire space of classes. This will apply to
    // all non-Impala classes, so is set to otherLogLevel.
    // 
    // Finally we can configure per-package logging which overrides the rootLogger
    // setting. In order to control Impala's logging independently of the rest of the
    // world, we set the log level for org.apache.impala.
    properties.setProperty("log4j.rootLogger", log4jLevelForTLogLevel(otherLogLevel) + ",glog");
    properties.setProperty("log4j.appender.glog.Threshold", "TRACE");
    properties.setProperty("log4j.logger.org.apache.impala", log4jLevelForTLogLevel(impalaLogLevel));
    PropertyConfigurator.configure(properties);
    Logger.getLogger(GlogAppender.class).info(String.format("Logging (re)initialized. " + "Impala: %s, All other: %s", impalaLogLevel, otherLogLevel));
}
#end_block

#method_before
public static String getLogLevel(byte[] serializedParams) throws ImpalaException {
    TGetLogLevelParams thriftParams = new TGetLogLevelParams();
    JniUtil.deserializeThrift(protocolFactory_, thriftParams, serializedParams);
    String className = thriftParams.getClass_name();
    if (Strings.isNullOrEmpty(className))
        return null;
    return Logger.getLogger(className).getEffectiveLevel().toString();
}
#method_after
public static String getLogLevel(byte[] serializedParams) throws ImpalaException {
    TGetJavaLogLevelParams thriftParams = new TGetJavaLogLevelParams();
    JniUtil.deserializeThrift(protocolFactory_, thriftParams, serializedParams);
    String className = thriftParams.getClass_name();
    if (Strings.isNullOrEmpty(className))
        return null;
    return Logger.getLogger(className).getEffectiveLevel().toString();
}
#end_block

#method_before
public static String setLogLevel(byte[] serializedParams) throws ImpalaException {
    TSetLogLevelParams thriftParams = new TSetLogLevelParams();
    JniUtil.deserializeThrift(protocolFactory_, thriftParams, serializedParams);
    String className = thriftParams.getClass_name();
    String logLevel = thriftParams.getLog_level();
    if (Strings.isNullOrEmpty(className) || Strings.isNullOrEmpty(logLevel))
        return null;
    // Level.toLevel() returns DEBUG for an incorrect logLevel input.
    Logger.getLogger(className).setLevel(Level.toLevel(logLevel));
    return Logger.getLogger(className).getEffectiveLevel().toString();
}
#method_after
public static String setLogLevel(byte[] serializedParams) throws ImpalaException {
    TSetJavaLogLevelParams thriftParams = new TSetJavaLogLevelParams();
    JniUtil.deserializeThrift(protocolFactory_, thriftParams, serializedParams);
    String className = thriftParams.getClass_name();
    String logLevel = thriftParams.getLog_level();
    if (Strings.isNullOrEmpty(className) || Strings.isNullOrEmpty(logLevel))
        return null;
    // Level.toLevel() returns DEBUG for an incorrect logLevel input.
    Logger.getLogger(className).setLevel(Level.toLevel(logLevel));
    return Logger.getLogger(className).getEffectiveLevel().toString();
}
#end_block

#method_before
private void computeMinMaxTupleAndConjuncts(Analyzer analyzer) throws ImpalaException {
    Preconditions.checkNotNull(desc_.getPath());
    String tupleName = desc_.getPath().toString() + " statistics";
    DescriptorTable descTbl = analyzer.getDescTbl();
    minMaxTuple_ = descTbl.createTupleDescriptor(tupleName);
    minMaxTuple_.setPath(desc_.getPath());
    for (Expr pred : conjuncts_) {
        if (!(pred instanceof BinaryPredicate))
            continue;
        BinaryPredicate binaryPred = (BinaryPredicate) pred;
        // We only support slot refs on the left hand side of the predicate, a rewriting
        // rule makes sure that all compatible exprs are rewritten into this form. Only
        // implicit casts are supported.
        SlotRef slot = binaryPred.getChild(0).unwrapSlotRef(true);
        if (slot == null)
            continue;
        // This node is a table scan, so this must be a scanning slot.
        Preconditions.checkState(slot.getDesc().isScanSlot());
        Expr constExpr = binaryPred.getChild(1);
        // LiteralExpr, but can also be an expr like "1 + 2".
        if (!constExpr.isConstant())
            continue;
        if (constExpr.isNullLiteral())
            continue;
        BinaryPredicate.Operator op = binaryPred.getOp();
        if (op == BinaryPredicate.Operator.LT || op == BinaryPredicate.Operator.LE || op == BinaryPredicate.Operator.GE || op == BinaryPredicate.Operator.GT) {
            minMaxOriginalConjuncts_.add(pred);
            buildStatsPredicate(analyzer, slot, binaryPred, op);
        } else if (op == BinaryPredicate.Operator.EQ) {
            minMaxOriginalConjuncts_.add(pred);
            // TODO: this could be optimized for boolean columns.
            buildStatsPredicate(analyzer, slot, binaryPred, BinaryPredicate.Operator.LE);
            buildStatsPredicate(analyzer, slot, binaryPred, BinaryPredicate.Operator.GE);
        }
    }
    minMaxTuple_.computeMemLayout();
}
#method_after
private void computeMinMaxTupleAndConjuncts(Analyzer analyzer) throws ImpalaException {
    Preconditions.checkNotNull(desc_.getPath());
    String tupleName = desc_.getPath().toString() + " statistics";
    DescriptorTable descTbl = analyzer.getDescTbl();
    minMaxTuple_ = descTbl.createTupleDescriptor(tupleName);
    minMaxTuple_.setPath(desc_.getPath());
    for (Expr pred : conjuncts_) {
        if (!(pred instanceof BinaryPredicate))
            continue;
        BinaryPredicate binaryPred = (BinaryPredicate) pred;
        // We only support slot refs on the left hand side of the predicate, a rewriting
        // rule makes sure that all compatible exprs are rewritten into this form. Only
        // implicit casts are supported.
        SlotRef slot = binaryPred.getChild(0).unwrapSlotRef(true);
        if (slot == null)
            continue;
        // This node is a table scan, so this must be a scanning slot.
        Preconditions.checkState(slot.getDesc().isScanSlot());
        // If the column is null, then this can be a 'pos' scanning slot of a nested type.
        if (slot.getDesc().getColumn() == null)
            continue;
        Expr constExpr = binaryPred.getChild(1);
        // LiteralExpr, but can also be an expr like "1 + 2".
        if (!constExpr.isConstant())
            continue;
        if (constExpr.isNullLiteral())
            continue;
        BinaryPredicate.Operator op = binaryPred.getOp();
        if (op == BinaryPredicate.Operator.LT || op == BinaryPredicate.Operator.LE || op == BinaryPredicate.Operator.GE || op == BinaryPredicate.Operator.GT) {
            minMaxOriginalConjuncts_.add(pred);
            buildStatsPredicate(analyzer, slot, binaryPred, op);
        } else if (op == BinaryPredicate.Operator.EQ) {
            minMaxOriginalConjuncts_.add(pred);
            // TODO: this could be optimized for boolean columns.
            buildStatsPredicate(analyzer, slot, binaryPred, BinaryPredicate.Operator.LE);
            buildStatsPredicate(analyzer, slot, binaryPred, BinaryPredicate.Operator.GE);
        }
    }
    minMaxTuple_.computeMemLayout();
}
#end_block

#method_before
private void handleTokenExchangeResponse(Channel chan, NegotiatePB response) throws SaslException {
    Preconditions.checkArgument(response.getStep() == NegotiateStep.TOKEN_EXCHANGE, "expected TOKEN_EXCHANGE, got step: {}", response.getStep());
    // The token response doesn't have any actual data in it, so we can just move on.
    handleSuccessResponse(chan, response);
}
#method_after
private void handleTokenExchangeResponse(Channel chan, NegotiatePB response) throws SaslException {
    Preconditions.checkArgument(response.getStep() == NegotiateStep.TOKEN_EXCHANGE, "expected TOKEN_EXCHANGE, got step: {}", response.getStep());
    // The token response doesn't have any actual data in it, so we can just move on.
    finish(chan);
}
#end_block

#method_before
private void handleSuccessResponse(Channel chan, NegotiatePB response) throws SaslException {
    if (chosenMech.equals("GSSAPI")) {
        nonce = response.getNonce().toByteArray();
        if (peerCert != null) {
            verifyChannelBindings(response);
        }
    }
    state = State.FINISHED;
    chan.getPipeline().remove(this);
    Channels.write(chan, makeConnectionContext());
    Channels.fireMessageReceived(chan, new Result(serverFeatures));
}
#method_after
private void handleSuccessResponse(Channel chan, NegotiatePB response) throws SaslException {
    Preconditions.checkState(saslClient.isComplete(), "server sent SASL_SUCCESS step, but SASL negotiation is not complete");
    if (chosenMech.equals("GSSAPI")) {
        if (response.hasNonce()) {
            // Grab the nonce from the server, if it has sent one. We'll send it back
            // later with SASL integrity protection as part of the connection context.
            nonce = response.getNonce().toByteArray();
        }
        if (peerCert != null) {
            // Check the channel bindings provided by the server against the expected channel bindings.
            verifyChannelBindings(response);
        }
    }
    finish(chan);
}
#end_block

#method_before
private RpcOutboundMessage makeConnectionContext() throws SaslException {
    RpcHeader.ConnectionContextPB.Builder builder = RpcHeader.ConnectionContextPB.newBuilder();
    // The UserInformationPB is deprecated, but used by servers prior to Kudu 1.1.
    RpcHeader.UserInformationPB.Builder userBuilder = RpcHeader.UserInformationPB.newBuilder();
    userBuilder.setEffectiveUser(Negotiator.USER_AND_PASSWORD);
    userBuilder.setRealUser(Negotiator.USER_AND_PASSWORD);
    builder.setDEPRECATEDUserInfo(userBuilder.build());
    if (nonce != null) {
        builder.setNonce(ZeroCopyLiteralByteString.wrap(saslClient.wrap(nonce, 0, nonce.length)));
    }
    RpcHeader.ConnectionContextPB pb = builder.build();
    RpcHeader.RequestHeader.Builder header = RpcHeader.RequestHeader.newBuilder().setCallId(CONNECTION_CTX_CALL_ID);
    return new RpcOutboundMessage(header, pb);
}
#method_after
private RpcOutboundMessage makeConnectionContext() throws SaslException {
    RpcHeader.ConnectionContextPB.Builder builder = RpcHeader.ConnectionContextPB.newBuilder();
    // The UserInformationPB is deprecated, but used by servers prior to Kudu 1.1.
    RpcHeader.UserInformationPB.Builder userBuilder = RpcHeader.UserInformationPB.newBuilder();
    String user = System.getProperty("user.name");
    userBuilder.setEffectiveUser(user);
    userBuilder.setRealUser(user);
    builder.setDEPRECATEDUserInfo(userBuilder.build());
    if (nonce != null) {
        // Reply with the SASL-protected nonce. We only set the nonce when using SASL GSSAPI.
        // The Java SASL client does not automatically add the length header,
        // so we have to do it ourselves.
        byte[] encodedNonce = saslClient.wrap(nonce, 0, nonce.length);
        ByteBuffer buf = ByteBuffer.allocate(encodedNonce.length + 4);
        buf.order(ByteOrder.BIG_ENDIAN);
        buf.putInt(encodedNonce.length);
        buf.put(encodedNonce);
        builder.setEncodedNonce(ZeroCopyLiteralByteString.wrap(buf.array()));
    }
    RpcHeader.ConnectionContextPB pb = builder.build();
    RpcHeader.RequestHeader.Builder header = RpcHeader.RequestHeader.newBuilder().setCallId(CONNECTION_CTX_CALL_ID);
    return new RpcOutboundMessage(header, pb);
}
#end_block

#method_before
public void handle(Callback[] callbacks) throws UnsupportedCallbackException {
    for (Callback callback : callbacks) {
        if (callback instanceof NameCallback) {
            ((NameCallback) callback).setName(USER_AND_PASSWORD);
        } else if (callback instanceof PasswordCallback) {
            ((PasswordCallback) callback).setPassword(USER_AND_PASSWORD.toCharArray());
        } else {
            throw new UnsupportedCallbackException(callback, "Unrecognized SASL client callback");
        }
    }
}
#method_after
public void handle(Callback[] callbacks) throws UnsupportedCallbackException {
    for (Callback callback : callbacks) {
        if (callback instanceof NameCallback) {
            ((NameCallback) callback).setName(System.getProperty("user.name"));
        } else if (callback instanceof PasswordCallback) {
            ((PasswordCallback) callback).setPassword(new char[0]);
        } else {
            throw new UnsupportedCallbackException(callback, "Unrecognized SASL client callback");
        }
    }
}
#end_block

#method_before
public TCatalogObject reloadTable(Table tbl) throws CatalogException {
    LOG.info(String.format("Refreshing table metadata: %s", tbl.getFullName()));
    TTableName tblName = new TTableName(tbl.getDb().getName().toLowerCase(), tbl.getName().toLowerCase());
    Db db = tbl.getDb();
    if (tbl instanceof IncompleteTable) {
        TableLoadingMgr.LoadRequest loadReq;
        long previousCatalogVersion;
        // Return the table if it is already loaded or submit a new load request.
        catalogLock_.readLock().lock();
        try {
            previousCatalogVersion = tbl.getCatalogVersion();
            loadReq = tableLoadingMgr_.loadAsync(tblName);
        } finally {
            catalogLock_.readLock().unlock();
        }
        Preconditions.checkNotNull(loadReq);
        try {
            // The table may have been dropped/modified while the load was in progress, so
            // only apply the update if the existing table hasn't changed.
            Table result = replaceTableIfUnchanged(loadReq.get(), previousCatalogVersion);
            return result.toTCatalogObject();
        } finally {
            loadReq.close();
            LOG.info(String.format("Refreshed table metadata: %s", tbl.getFullName()));
        }
    }
    if (!tryLockTable(tbl)) {
        throw new CatalogException(String.format("Error refreshing metadata for table " + "%s due to lock contention", tbl.getFullName()));
    }
    try {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = null;
            try {
                msTbl = msClient.getHiveClient().getTable(db.getName(), tblName.getTable_name());
            } catch (Exception e) {
                throw new TableLoadingException("Error loading metadata for table: " + db.getName() + "." + tblName.getTable_name(), e);
            }
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
        tbl.setCatalogVersion(newCatalogVersion);
        LOG.info(String.format("Refreshed table metadata: %s", tbl.getFullName()));
        return tbl.toTCatalogObject();
    } finally {
        Preconditions.checkState(!catalogLock_.isWriteLockedByCurrentThread());
        tbl.getLock().unlock();
    }
}
#method_after
public TCatalogObject reloadTable(Table tbl) throws CatalogException {
    LOG.info(String.format("Refreshing table metadata: %s", tbl.getFullName()));
    Preconditions.checkState(!(tbl instanceof IncompleteTable));
    String dbName = tbl.getDb().getName();
    String tblName = tbl.getName();
    if (!tryLockTable(tbl)) {
        throw new CatalogException(String.format("Error refreshing metadata for table " + "%s due to lock contention", tbl.getFullName()));
    }
    try {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = null;
            try {
                msTbl = msClient.getHiveClient().getTable(dbName, tblName);
            } catch (Exception e) {
                throw new TableLoadingException("Error loading metadata for table: " + dbName + "." + tblName, e);
            }
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
        tbl.setCatalogVersion(newCatalogVersion);
        LOG.info(String.format("Refreshed table metadata: %s", tbl.getFullName()));
        return tbl.toTCatalogObject();
    } finally {
        Preconditions.checkState(!catalogLock_.isWriteLockedByCurrentThread());
        tbl.getLock().unlock();
    }
}
#end_block

#method_before
private void handleTokenExchangeResponse(Channel chan, NegotiatePB response) {
    Preconditions.checkArgument(response.getStep() == NegotiateStep.TOKEN_EXCHANGE, "expected TOKEN_EXCHANGE, got step: {}", response.getStep());
    // The token response doesn't have any actual data in it, so we can just move on.
    handleSuccessResponse(chan, response);
}
#method_after
private void handleTokenExchangeResponse(Channel chan, NegotiatePB response) {
    Preconditions.checkArgument(response.getStep() == NegotiateStep.TOKEN_EXCHANGE, "expected TOKEN_EXCHANGE, got step: {}", response.getStep());
    // The token response doesn't have any actual data in it, so we can just move on.
    finish(chan);
}
#end_block

#method_before
private void handleSuccessResponse(Channel chan, NegotiatePB response) {
    Preconditions.checkState(saslClient.isComplete(), "server sent SASL_SUCCESS step, but SASL negotiation is not complete");
    if (peerCert != null && "GSSAPI".equals(chosenMech)) {
        verifyChannelBindings(response);
    }
    state = State.FINISHED;
    chan.getPipeline().remove(this);
    Channels.write(chan, makeConnectionContext());
    Channels.fireMessageReceived(chan, new Result(serverFeatures));
}
#method_after
private void handleSuccessResponse(Channel chan, NegotiatePB response) {
    Preconditions.checkState(saslClient.isComplete(), "server sent SASL_SUCCESS step, but SASL negotiation is not complete");
    if (peerCert != null && "GSSAPI".equals(chosenMech)) {
        verifyChannelBindings(response);
    }
    finish(chan);
}
#end_block

#method_before
public THdfsPartition toThrift(boolean includeFileDesc, boolean includeIncrementalStats) {
    List<TExpr> thriftExprs = Expr.treesToThrift(getPartitionValues());
    THdfsPartition thriftHdfsPart = new THdfsPartition(fileFormatDescriptor_.getLineDelim(), fileFormatDescriptor_.getFieldDelim(), fileFormatDescriptor_.getCollectionDelim(), fileFormatDescriptor_.getMapKeyDelim(), fileFormatDescriptor_.getEscapeChar(), fileFormatDescriptor_.getFileFormat().toThrift(), thriftExprs, fileFormatDescriptor_.getBlockSize());
    if (location_ != null)
        thriftHdfsPart.setLocation(location_.toThrift());
    thriftHdfsPart.setStats(new TTableStats(numRows_));
    thriftHdfsPart.setAccess_level(accessLevel_);
    thriftHdfsPart.setIs_marked_cached(isMarkedCached_);
    thriftHdfsPart.setId(getId());
    // IMPALA-XYZ: Shallow-clone the map to avoid concurrent modifications. One thread
    // may try to serialize the returned THdfsPartition after releasing the table's lock,
    // and another thread doing DDL may modify the map.
    thriftHdfsPart.setHms_parameters(Maps.newHashMap(includeIncrementalStats ? hmsParameters_ : getFilteredHmsParameters()));
    if (includeFileDesc) {
        // Add block location information
        for (FileDescriptor fd : fileDescriptors_) {
            thriftHdfsPart.addToFile_desc(fd.toThrift());
        }
    }
    return thriftHdfsPart;
}
#method_after
public THdfsPartition toThrift(boolean includeFileDesc, boolean includeIncrementalStats) {
    List<TExpr> thriftExprs = Expr.treesToThrift(getPartitionValues());
    THdfsPartition thriftHdfsPart = new THdfsPartition(fileFormatDescriptor_.getLineDelim(), fileFormatDescriptor_.getFieldDelim(), fileFormatDescriptor_.getCollectionDelim(), fileFormatDescriptor_.getMapKeyDelim(), fileFormatDescriptor_.getEscapeChar(), fileFormatDescriptor_.getFileFormat().toThrift(), thriftExprs, fileFormatDescriptor_.getBlockSize());
    if (location_ != null)
        thriftHdfsPart.setLocation(location_.toThrift());
    thriftHdfsPart.setStats(new TTableStats(numRows_));
    thriftHdfsPart.setAccess_level(accessLevel_);
    thriftHdfsPart.setIs_marked_cached(isMarkedCached_);
    thriftHdfsPart.setId(getId());
    // IMPALA-4902: Shallow-clone the map to avoid concurrent modifications. One thread
    // may try to serialize the returned THdfsPartition after releasing the table's lock,
    // and another thread doing DDL may modify the map.
    thriftHdfsPart.setHms_parameters(Maps.newHashMap(includeIncrementalStats ? hmsParameters_ : getFilteredHmsParameters()));
    if (includeFileDesc) {
        // Add block location information
        for (FileDescriptor fd : fileDescriptors_) {
            thriftHdfsPart.addToFile_desc(fd.toThrift());
        }
    }
    return thriftHdfsPart;
}
#end_block

#method_before
@Test
public void testGetAuthnToken() throws Exception {
    byte[] token = client.exportAuthenticationData().join();
    assertNotNull(token);
}
#method_after
@Test
public void testGetAuthnToken() throws Exception {
    byte[] token = client.exportAuthenticationCredentials().join();
    assertNotNull(token);
}
#end_block

#method_before
private Negotiator startNegotiation() {
    Negotiator negotiator = new Negotiator("127.0.0.1", secContext);
    embedder = new DecoderEmbedder<Object>(negotiator);
    negotiator.sendHello(embedder.getPipeline().getChannel());
    return negotiator;
}
#method_after
private void startNegotiation(boolean fakeLoopback) {
    Negotiator negotiator = new Negotiator("127.0.0.1", secContext);
    negotiator.overrideLoopbackForTests = fakeLoopback;
    embedder = new DecoderEmbedder<Object>(negotiator);
    negotiator.sendHello(embedder.getPipeline().getChannel());
}
#end_block

#method_before
@Test
public void testNegotiation() {
    startNegotiation();
    // Expect client->server: NEGOTIATE.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals(Negotiator.SASL_CALL_ID, msg.getHeaderBuilder().getCallId());
    assertEquals(NegotiateStep.NEGOTIATE, body.getStep());
    // Respond with NEGOTIATE.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSaslMechanisms(SaslMechanism.newBuilder().setMechanism("PLAIN")).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect client->server: SASL_INITIATE (PLAIN)
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals(Negotiator.SASL_CALL_ID, msg.getHeaderBuilder().getCallId());
    assertEquals(NegotiateStep.SASL_INITIATE, body.getStep());
    assertEquals(1, body.getSaslMechanismsCount());
    assertEquals("PLAIN", body.getSaslMechanisms(0).getMechanism());
    assertTrue(body.hasToken());
    // Respond with SASL_SUCCESS:
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().setStep(NegotiateStep.SASL_SUCCESS).build()));
    // Expect client->server: ConnectionContext
    assertComplete();
}
#method_after
@Test
public void testNegotiation() {
    startNegotiation(false);
    // Expect client->server: NEGOTIATE.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals(Negotiator.SASL_CALL_ID, msg.getHeaderBuilder().getCallId());
    assertEquals(NegotiateStep.NEGOTIATE, body.getStep());
    // Respond with NEGOTIATE.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSaslMechanisms(SaslMechanism.newBuilder().setMechanism("PLAIN")).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect client->server: SASL_INITIATE (PLAIN)
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals(Negotiator.SASL_CALL_ID, msg.getHeaderBuilder().getCallId());
    assertEquals(NegotiateStep.SASL_INITIATE, body.getStep());
    assertEquals(1, body.getSaslMechanismsCount());
    assertEquals("PLAIN", body.getSaslMechanisms(0).getMechanism());
    assertTrue(body.hasToken());
    // Respond with SASL_SUCCESS:
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().setStep(NegotiateStep.SASL_SUCCESS).build()));
    // Expect client->server: ConnectionContext
    assertComplete();
}
#end_block

#method_before
@Test
public void testTlsNegotiation() throws Exception {
    startNegotiation();
    // Expect client->server: NEGOTIATE, TLS included.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals(NegotiateStep.NEGOTIATE, body.getStep());
    assertTrue(body.getSupportedFeaturesList().contains(RpcFeatureFlag.TLS));
    // Fake a server response with TLS enabled.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSaslMechanisms(NegotiatePB.SaslMechanism.newBuilder().setMechanism("PLAIN")).addSupportedFeatures(RpcFeatureFlag.TLS).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect client->server: TLS_HANDSHAKE.
    runTlsHandshake();
    // The pipeline should now have an SSL handler as the first handler.
    assertTrue(embedder.getPipeline().getFirst() instanceof SslHandler);
    // The Negotiator should have sent the SASL_INITIATE at this point.
    // NOTE: in a non-mock environment, this message would now be encrypted
    // by the newly-added TLS handler. But, with the DecoderEmbedder that we're
    // using, we don't actually end up processing outbound events. Upgrading
    // to Netty 4 and using EmbeddedChannel instead would make this more realistic.
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals(NegotiateStep.SASL_INITIATE, body.getStep());
}
#method_after
@Test
public void testTlsNegotiation() throws Exception {
    startNegotiation(false);
    // Expect client->server: NEGOTIATE, TLS included.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals(NegotiateStep.NEGOTIATE, body.getStep());
    assertTrue(body.getSupportedFeaturesList().contains(RpcFeatureFlag.TLS));
    // Fake a server response with TLS enabled.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSaslMechanisms(NegotiatePB.SaslMechanism.newBuilder().setMechanism("PLAIN")).addSupportedFeatures(RpcFeatureFlag.TLS).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect client->server: TLS_HANDSHAKE.
    runTlsHandshake();
    // The pipeline should now have an SSL handler as the first handler.
    assertTrue(embedder.getPipeline().getFirst() instanceof SslHandler);
    // The Negotiator should have sent the SASL_INITIATE at this point.
    // NOTE: in a non-mock environment, this message would now be encrypted
    // by the newly-added TLS handler. But, with the DecoderEmbedder that we're
    // using, we don't actually end up processing outbound events. Upgrading
    // to Netty 4 and using EmbeddedChannel instead would make this more realistic.
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals(NegotiateStep.SASL_INITIATE, body.getStep());
}
#end_block

#method_before
@Test
public void testNoTokenAuthWhenNoTrustedCerts() throws Exception {
    secContext.setAuthenticationToken(SignedTokenPB.getDefaultInstance());
    startNegotiation();
    // Expect client->server: NEGOTIATE, TLS included, Token not included.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals("supported_features: APPLICATION_FEATURE_FLAGS " + "supported_features: TLS " + "step: NEGOTIATE " + "authn_types { sasl { } }", TextFormat.shortDebugString(body));
}
#method_after
@Test
public void testNoTokenAuthWhenNoTrustedCerts() throws Exception {
    secContext.setAuthenticationToken(SignedTokenPB.getDefaultInstance());
    startNegotiation(false);
    // Expect client->server: NEGOTIATE, TLS included, Token not included.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals("supported_features: APPLICATION_FEATURE_FLAGS " + "supported_features: TLS " + "step: NEGOTIATE " + "authn_types { sasl { } }", TextFormat.shortDebugString(body));
}
#end_block

#method_before
@Test
public void testTokenAuthWithTrustedCerts() throws Exception {
    secContext.trustCertificates(ImmutableList.of(ByteString.copyFromUtf8(CA_CERT_DER)));
    secContext.setAuthenticationToken(SignedTokenPB.getDefaultInstance());
    startNegotiation();
    // Expect client->server: NEGOTIATE, TLS included, Token included.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals("supported_features: APPLICATION_FEATURE_FLAGS " + "supported_features: TLS " + "step: NEGOTIATE " + "authn_types { sasl { } } " + "authn_types { token { } }", TextFormat.shortDebugString(body));
    // Fake a server response with TLS enabled and TOKEN chosen.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSupportedFeatures(RpcFeatureFlag.TLS).addAuthnTypes(AuthenticationTypePB.newBuilder().setToken(AuthenticationTypePB.Token.getDefaultInstance())).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect to now run the TLS handshake
    runTlsHandshake();
    // Expect the client to send the token.
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals("step: TOKEN_EXCHANGE authn_token { }", TextFormat.shortDebugString(body));
    // Fake a response indicating success.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().setStep(NegotiateStep.TOKEN_EXCHANGE).build()));
    // Should be complete now.
    assertComplete();
}
#method_after
@Test
public void testTokenAuthWithTrustedCerts() throws Exception {
    secContext.trustCertificates(ImmutableList.of(ByteString.copyFromUtf8(CA_CERT_DER)));
    secContext.setAuthenticationToken(SignedTokenPB.getDefaultInstance());
    startNegotiation(false);
    // Expect client->server: NEGOTIATE, TLS included, Token included.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals("supported_features: APPLICATION_FEATURE_FLAGS " + "supported_features: TLS " + "step: NEGOTIATE " + "authn_types { sasl { } } " + "authn_types { token { } }", TextFormat.shortDebugString(body));
    // Fake a server response with TLS enabled and TOKEN chosen.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSupportedFeatures(RpcFeatureFlag.TLS).addAuthnTypes(AuthenticationTypePB.newBuilder().setToken(AuthenticationTypePB.Token.getDefaultInstance())).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect to now run the TLS handshake
    runTlsHandshake();
    // Expect the client to send the token.
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals("step: TOKEN_EXCHANGE authn_token { }", TextFormat.shortDebugString(body));
    // Fake a response indicating success.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().setStep(NegotiateStep.TOKEN_EXCHANGE).build()));
    // Should be complete now.
    assertComplete();
}
#end_block

#method_before
@Test
public void testTokenAuthWithTrustedCerts() throws Exception {
    secContext.trustCertificates(ImmutableList.of(ByteString.copyFromUtf8(CA_CERT_DER)));
    secContext.setAuthenticationToken(SignedTokenPB.getDefaultInstance());
    startNegotiation(false);
    // Expect client->server: NEGOTIATE, TLS included, Token included.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals("supported_features: APPLICATION_FEATURE_FLAGS " + "supported_features: TLS " + "step: NEGOTIATE " + "authn_types { sasl { } } " + "authn_types { token { } }", TextFormat.shortDebugString(body));
    // Fake a server response with TLS enabled and TOKEN chosen.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSupportedFeatures(RpcFeatureFlag.TLS).addAuthnTypes(AuthenticationTypePB.newBuilder().setToken(AuthenticationTypePB.Token.getDefaultInstance())).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect to now run the TLS handshake
    runTlsHandshake();
    // Expect the client to send the token.
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals("step: TOKEN_EXCHANGE authn_token { }", TextFormat.shortDebugString(body));
    // Fake a response indicating success.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().setStep(NegotiateStep.TOKEN_EXCHANGE).build()));
    // Should be complete now.
    assertComplete();
}
#method_after
@Test
public void testTokenAuthWithTrustedCerts() throws Exception {
    secContext.trustCertificate(ByteString.copyFromUtf8(CA_CERT_DER));
    secContext.setAuthenticationToken(SignedTokenPB.getDefaultInstance());
    startNegotiation(false);
    // Expect client->server: NEGOTIATE, TLS included, Token included.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals("supported_features: APPLICATION_FEATURE_FLAGS " + "supported_features: TLS " + "step: NEGOTIATE " + "authn_types { sasl { } } " + "authn_types { token { } }", TextFormat.shortDebugString(body));
    // Fake a server response with TLS enabled and TOKEN chosen.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSupportedFeatures(RpcFeatureFlag.TLS).addAuthnTypes(AuthenticationTypePB.newBuilder().setToken(AuthenticationTypePB.Token.getDefaultInstance())).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect to now run the TLS handshake
    runTlsHandshake();
    // Expect the client to send the token.
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals("step: TOKEN_EXCHANGE authn_token { }", TextFormat.shortDebugString(body));
    // Fake a response indicating success.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().setStep(NegotiateStep.TOKEN_EXCHANGE).build()));
    // Should be complete now.
    assertComplete();
}
#end_block

#method_before
private Type resolveDecimalReturnType(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(type_.isWildcardDecimal());
    Preconditions.checkState(fn_.getBinaryType() == TFunctionBinaryType.BUILTIN);
    Preconditions.checkState(children_.size() > 0);
    // Find first decimal input (some functions, such as if(), begin with non-decimal
    // arguments).
    ScalarType childType = null;
    for (Expr child : children_) {
        if (child.type_.isDecimal()) {
            childType = (ScalarType) child.type_;
            break;
        }
    }
    Preconditions.checkState(childType != null && !childType.isWildcardDecimal());
    Type returnType = childType;
    if (fnName_.getFunction().equalsIgnoreCase("sum")) {
        return childType.getMaxResolutionType();
    }
    int digitsBefore = childType.decimalPrecision() - childType.decimalScale();
    int digitsAfter = childType.decimalScale();
    if (fnName_.getFunction().equalsIgnoreCase("avg") && analyzer.getQueryOptions().isDecimal_v2()) {
        int resultScale = Math.max(ScalarType.MIN_ADJUSTED_SCALE, digitsAfter);
        int resultPrecision = digitsBefore + resultScale;
        return ScalarType.createAdjustedDecimalType(resultPrecision, resultScale);
    } else if (fnName_.getFunction().equalsIgnoreCase("ceil") || fnName_.getFunction().equalsIgnoreCase("ceiling") || fnName_.getFunction().equals("floor") || fnName_.getFunction().equals("dfloor")) {
        // These functions just return with scale 0 but can trigger rounding. We need
        // to increase the precision by 1 to handle that.
        ++digitsBefore;
        digitsAfter = 0;
    } else if (fnName_.getFunction().equalsIgnoreCase("truncate") || fnName_.getFunction().equalsIgnoreCase("dtrunc") || fnName_.getFunction().equalsIgnoreCase("round") || fnName_.getFunction().equalsIgnoreCase("dround")) {
        if (children_.size() > 1) {
            // The second argument to these functions is the desired scale, otherwise
            // the default is 0.
            Preconditions.checkState(children_.size() == 2);
            if (children_.get(1).isNullLiteral()) {
                throw new AnalysisException(fnName_.getFunction() + "() cannot be called with a NULL second argument.");
            }
            if (!children_.get(1).isConstant()) {
                // a reasonable restriction.
                throw new AnalysisException(fnName_.getFunction() + "() must be called with a constant second argument.");
            }
            NumericLiteral scaleLiteral = (NumericLiteral) LiteralExpr.create(children_.get(1), analyzer.getQueryCtx());
            digitsAfter = (int) scaleLiteral.getLongValue();
            if (Math.abs(digitsAfter) > ScalarType.MAX_SCALE) {
                throw new AnalysisException("Cannot round/truncate to scales greater than " + ScalarType.MAX_SCALE + ".");
            }
            // Round/Truncate to a negative scale means to round to the digit before
            // the decimal e.g. round(1234.56, -2) would be 1200.
            // The resulting scale is always 0.
            digitsAfter = Math.max(digitsAfter, 0);
        } else {
            // Round()/Truncate() with no second argument.
            digitsAfter = 0;
        }
        if ((fnName_.getFunction().equalsIgnoreCase("round") || fnName_.getFunction().equalsIgnoreCase("dround")) && digitsAfter < childType.decimalScale()) {
            // If we are rounding to fewer decimal places, it's possible we need another
            // digit before the decimal.
            ++digitsBefore;
        }
    }
    Preconditions.checkState(returnType.isDecimal() && !returnType.isWildcardDecimal());
    return ScalarType.createClippedDecimalType(digitsBefore + digitsAfter, digitsAfter);
}
#method_after
private Type resolveDecimalReturnType(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(type_.isWildcardDecimal());
    Preconditions.checkState(fn_.getBinaryType() == TFunctionBinaryType.BUILTIN);
    Preconditions.checkState(children_.size() > 0);
    // Find first decimal input (some functions, such as if(), begin with non-decimal
    // arguments).
    ScalarType childType = null;
    for (Expr child : children_) {
        if (child.type_.isDecimal()) {
            childType = (ScalarType) child.type_;
            break;
        }
    }
    Preconditions.checkState(childType != null && !childType.isWildcardDecimal());
    Type returnType = childType;
    if (fnName_.getFunction().equalsIgnoreCase("sum")) {
        return childType.getMaxResolutionType();
    }
    int digitsBefore = childType.decimalPrecision() - childType.decimalScale();
    int digitsAfter = childType.decimalScale();
    if (fnName_.getFunction().equalsIgnoreCase("avg") && analyzer.getQueryOptions().isDecimal_v2()) {
        // AVG() always gets at least MIN_ADJUSTED_SCALE decimal places since it performs
        // an implicit divide. The output type isn't always the same as SUM()/COUNT().
        // Scale is set the same as MS SQL Server, which takes the max of the input scale
        // and MIN_ADJUST_SCALE. For precision, MS SQL always sets it to 38. We choose to
        // trim it down to the size that's needed because the absolute value of the result
        // is less than the absolute value of the largest input. Using a smaller precision
        // allows for better DECIMAL types to be chosen for the overall expression when
        // AVG() is a subexpression. For DECIMAL_V1, we set the output type to be the same
        // as the input type.
        int resultScale = Math.max(ScalarType.MIN_ADJUSTED_SCALE, digitsAfter);
        int resultPrecision = digitsBefore + resultScale;
        return ScalarType.createAdjustedDecimalType(resultPrecision, resultScale);
    } else if (fnName_.getFunction().equalsIgnoreCase("ceil") || fnName_.getFunction().equalsIgnoreCase("ceiling") || fnName_.getFunction().equals("floor") || fnName_.getFunction().equals("dfloor")) {
        // These functions just return with scale 0 but can trigger rounding. We need
        // to increase the precision by 1 to handle that.
        ++digitsBefore;
        digitsAfter = 0;
    } else if (fnName_.getFunction().equalsIgnoreCase("truncate") || fnName_.getFunction().equalsIgnoreCase("dtrunc") || fnName_.getFunction().equalsIgnoreCase("round") || fnName_.getFunction().equalsIgnoreCase("dround")) {
        if (children_.size() > 1) {
            // The second argument to these functions is the desired scale, otherwise
            // the default is 0.
            Preconditions.checkState(children_.size() == 2);
            if (children_.get(1).isNullLiteral()) {
                throw new AnalysisException(fnName_.getFunction() + "() cannot be called with a NULL second argument.");
            }
            if (!children_.get(1).isConstant()) {
                // a reasonable restriction.
                throw new AnalysisException(fnName_.getFunction() + "() must be called with a constant second argument.");
            }
            NumericLiteral scaleLiteral = (NumericLiteral) LiteralExpr.create(children_.get(1), analyzer.getQueryCtx());
            digitsAfter = (int) scaleLiteral.getLongValue();
            if (Math.abs(digitsAfter) > ScalarType.MAX_SCALE) {
                throw new AnalysisException("Cannot round/truncate to scales greater than " + ScalarType.MAX_SCALE + ".");
            }
            // Round/Truncate to a negative scale means to round to the digit before
            // the decimal e.g. round(1234.56, -2) would be 1200.
            // The resulting scale is always 0.
            digitsAfter = Math.max(digitsAfter, 0);
        } else {
            // Round()/Truncate() with no second argument.
            digitsAfter = 0;
        }
        if ((fnName_.getFunction().equalsIgnoreCase("round") || fnName_.getFunction().equalsIgnoreCase("dround")) && digitsAfter < childType.decimalScale()) {
            // If we are rounding to fewer decimal places, it's possible we need another
            // digit before the decimal.
            ++digitsBefore;
        }
    }
    Preconditions.checkState(returnType.isDecimal() && !returnType.isWildcardDecimal());
    return ScalarType.createClippedDecimalType(digitsBefore + digitsAfter, digitsAfter);
}
#end_block

#method_before
public void install() {
    if (installed)
        return;
    try {
        Field field = InetAddress.class.getDeclaredField("nameServices");
        field.setAccessible(true);
        @SuppressWarnings("unchecked")
        List<NameService> nameServices = (List<NameService>) field.get(null);
        nameServices.add(0, new NSImpl());
    } catch (Exception e) {
        throw Throwables.propagate(e);
    }
    installed = true;
}
#method_after
public synchronized void install() {
    if (installed)
        return;
    try {
        Field field = InetAddress.class.getDeclaredField("nameServices");
        field.setAccessible(true);
        @SuppressWarnings("unchecked")
        List<NameService> nameServices = (List<NameService>) field.get(null);
        nameServices.add(0, new NSImpl());
    } catch (Exception e) {
        throw Throwables.propagate(e);
    }
    installed = true;
}
#end_block

#method_before
@Override
public void channelConnected(final ChannelHandlerContext ctx, final ChannelStateEvent e) {
    assert chan != null;
    Channels.write(chan, ChannelBuffers.wrappedBuffer(CONNECTION_HEADER));
    Negotiator negotiator = new Negotiator(getSubject(), serverInfo.getHostname(), kuduClient.getSecurityContext());
    ctx.getPipeline().addBefore(ctx.getName(), "negotiation", negotiator);
    negotiator.sendHello(chan);
}
#method_after
@Override
public void channelConnected(final ChannelHandlerContext ctx, final ChannelStateEvent e) {
    assert chan != null;
    Channels.write(chan, ChannelBuffers.wrappedBuffer(CONNECTION_HEADER));
    Negotiator negotiator = new Negotiator(serverInfo.getHostname(), kuduClient.getSecurityContext());
    ctx.getPipeline().addBefore(ctx.getName(), "negotiation", negotiator);
    negotiator.sendHello(chan);
}
#end_block

#method_before
@Before
public void setup() {
    serverEngine = createServerEngine();
    serverEngine.setUseClientMode(false);
}
#method_after
@Before
public void setup() {
    serverEngine = createServerEngine();
    serverEngine.setUseClientMode(false);
    secContext = new SecurityContext(Subject.getSubject(AccessController.getContext()));
}
#end_block

#method_before
private Negotiator startNegotiation() {
    AccessControlContext context = AccessController.getContext();
    Subject subject = Subject.getSubject(context);
    Negotiator negotiator = new Negotiator(subject, "127.0.0.1", secContext);
    embedder = new DecoderEmbedder<Object>(negotiator);
    negotiator.sendHello(embedder.getPipeline().getChannel());
    return negotiator;
}
#method_after
private Negotiator startNegotiation() {
    Negotiator negotiator = new Negotiator("127.0.0.1", secContext);
    embedder = new DecoderEmbedder<Object>(negotiator);
    negotiator.sendHello(embedder.getPipeline().getChannel());
    return negotiator;
}
#end_block

#method_before
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException, SSLException {
    Preconditions.checkState(response.getStep() == NegotiateStep.NEGOTIATE, "Expected NEGOTIATE message, got {}", response.getStep());
    // Store the supported features advertised by the server.
    serverFeatures = getFeatureFlags(response);
    // TODO: is this always true?
    negotiatedTls = serverFeatures.contains(RpcFeatureFlag.TLS);
    // Check the negotiated authentication type sent by the server.
    chosenAuthnType = chooseAuthenticationType(response);
    if (chosenAuthnType == AuthenticationTypePB.TypeCase.SASL) {
        chooseAndInitializeSaslMech(response);
    }
    // we can move directly to the authentication phase.
    if (negotiatedTls) {
        startTlsHandshake(chan);
    } else {
        startAuthentication(chan);
    }
}
#method_after
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException, SSLException {
    Preconditions.checkState(response.getStep() == NegotiateStep.NEGOTIATE, "Expected NEGOTIATE message, got {}", response.getStep());
    // Store the supported features advertised by the server.
    serverFeatures = getFeatureFlags(response);
    // If the server supports TLS, we will always speak TLS to it.
    negotiatedTls = serverFeatures.contains(RpcFeatureFlag.TLS);
    // Check the negotiated authentication type sent by the server.
    chosenAuthnType = chooseAuthenticationType(response);
    if (chosenAuthnType == AuthenticationTypePB.TypeCase.SASL) {
        chooseAndInitializeSaslMech(response);
    }
    // we can move directly to the authentication phase.
    if (negotiatedTls) {
        startTlsHandshake(chan);
    } else {
        startAuthentication(chan);
    }
}
#end_block

#method_before
private byte[] evaluateChallenge(final byte[] challenge) throws SaslException {
    try {
        return Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {

            @Override
            public byte[] run() throws Exception {
                return saslClient.evaluateChallenge(challenge);
            }
        });
    } catch (Exception e) {
        Throwables.propagateIfInstanceOf(e, SaslException.class);
        throw Throwables.propagate(e);
    }
}
#method_after
private byte[] evaluateChallenge(final byte[] challenge) throws SaslException {
    try {
        return Subject.doAs(securityContext.getSubject(), new PrivilegedExceptionAction<byte[]>() {

            @Override
            public byte[] run() throws Exception {
                return saslClient.evaluateChallenge(challenge);
            }
        });
    } catch (Exception e) {
        Throwables.propagateIfInstanceOf(e, SaslException.class);
        throw Throwables.propagate(e);
    }
}
#end_block

#method_before
<R> Deferred<R> sendRpcToTablet(final KuduRpc<R> request) {
    if (cannotRetryRequest(request)) {
        return tooManyAttemptsOrTimeout(request, null);
    }
    request.attempt++;
    final String tableId = request.getTable().getTableId();
    byte[] partitionKey = request.partitionKey();
    TableLocationsCache.Entry entry = getTableLocationEntry(tableId, partitionKey);
    if (entry != null && entry.isNonCoveredRange()) {
        Exception e = new NonCoveredRangeException(entry.getLowerBoundPartitionKey(), entry.getUpperBoundPartitionKey());
        // Sending both as an errback and returning fromError because sendRpcToTablet might be
        // called via a callback that won't care about the returned Deferred.
        request.errback(e);
        return Deferred.fromError(e);
    }
    // Set the propagated timestamp so that the next time we send a message to
    // the server the message includes the last propagated timestamp.
    long lastPropagatedTs = getLastPropagatedTimestamp();
    if (request.getExternalConsistencyMode() == CLIENT_PROPAGATED && lastPropagatedTs != NO_TIMESTAMP) {
        request.setPropagatedTimestamp(lastPropagatedTs);
    }
    // If we found a tablet, we'll try to find the TS to talk to.
    if (entry != null) {
        RemoteTablet tablet = entry.getTablet();
        String uuid = tablet.getReplicaSelectedUUID(request.getReplicaSelection());
        if (uuid != null) {
            Deferred<R> d = request.getDeferred();
            request.setTablet(tablet);
            TabletClient client = connectionCache.getLiveClient(uuid);
            if (client != null) {
                client.sendRpc(request);
                return d;
            }
        }
    }
    request.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(request.method(), RpcTraceFrame.Action.QUERY_MASTER).build());
    // leader replica.
    if (tablesNotServed.contains(tableId)) {
        return delayedIsCreateTableDone(request.getTable(), request, new RetryRpcCB<R, Master.IsCreateTableDoneResponsePB>(request), getDelayedIsCreateTableDoneErrback(request));
    }
    Callback<Deferred<R>, Master.GetTableLocationsResponsePB> cb = new RetryRpcCB<>(request);
    Callback<Deferred<R>, Exception> eb = new RetryRpcErrback<>(request);
    Deferred<Master.GetTableLocationsResponsePB> returnedD = locateTablet(request.getTable(), partitionKey, FETCH_TABLETS_PER_POINT_LOOKUP, request);
    return AsyncUtil.addCallbacksDeferring(returnedD, cb, eb);
}
#method_after
<R> Deferred<R> sendRpcToTablet(final KuduRpc<R> request) {
    if (cannotRetryRequest(request)) {
        return tooManyAttemptsOrTimeout(request, null);
    }
    request.attempt++;
    final String tableId = request.getTable().getTableId();
    byte[] partitionKey = request.partitionKey();
    TableLocationsCache.Entry entry = getTableLocationEntry(tableId, partitionKey);
    if (entry != null && entry.isNonCoveredRange()) {
        Exception e = new NonCoveredRangeException(entry.getLowerBoundPartitionKey(), entry.getUpperBoundPartitionKey());
        // Sending both as an errback and returning fromError because sendRpcToTablet might be
        // called via a callback that won't care about the returned Deferred.
        Deferred<R> d = request.getDeferred();
        request.errback(e);
        return d;
    }
    // Set the propagated timestamp so that the next time we send a message to
    // the server the message includes the last propagated timestamp.
    long lastPropagatedTs = getLastPropagatedTimestamp();
    if (request.getExternalConsistencyMode() == CLIENT_PROPAGATED && lastPropagatedTs != NO_TIMESTAMP) {
        request.setPropagatedTimestamp(lastPropagatedTs);
    }
    // If we found a tablet, we'll try to find the TS to talk to.
    if (entry != null) {
        RemoteTablet tablet = entry.getTablet();
        String uuid = tablet.getReplicaSelectedUUID(request.getReplicaSelection());
        if (uuid != null) {
            Deferred<R> d = request.getDeferred();
            request.setTablet(tablet);
            TabletClient client = connectionCache.getLiveClient(uuid);
            if (client != null) {
                client.sendRpc(request);
                return d;
            }
        }
    }
    request.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(request.method(), RpcTraceFrame.Action.QUERY_MASTER).build());
    // leader replica.
    if (tablesNotServed.contains(tableId)) {
        return delayedIsCreateTableDone(request.getTable(), request, new RetryRpcCB<R, Master.IsCreateTableDoneResponsePB>(request), getDelayedIsCreateTableDoneErrback(request));
    }
    Callback<Deferred<R>, Master.GetTableLocationsResponsePB> cb = new RetryRpcCB<>(request);
    Callback<Deferred<R>, Exception> eb = new RetryRpcErrback<>(request);
    Deferred<Master.GetTableLocationsResponsePB> returnedD = locateTablet(request.getTable(), partitionKey, FETCH_TABLETS_PER_POINT_LOOKUP, request);
    return AsyncUtil.addCallbacksDeferring(returnedD, cb, eb);
}
#end_block

#method_before
static <R> Deferred<R> tooManyAttemptsOrTimeout(final KuduRpc<R> request, final KuduException cause) {
    String message;
    if (request.attempt > MAX_RPC_ATTEMPTS) {
        message = "Too many attempts: ";
    } else {
        message = "RPC can not complete before timeout: ";
    }
    Status statusTimedOut = Status.TimedOut(message + request);
    final Exception e = new NonRecoverableException(statusTimedOut, cause);
    LOG.debug("Cannot continue with this RPC: {} because of: {}", request, message, e);
    request.errback(e);
    return Deferred.fromError(e);
}
#method_after
static <R> Deferred<R> tooManyAttemptsOrTimeout(final KuduRpc<R> request, final KuduException cause) {
    String message;
    if (request.attempt > MAX_RPC_ATTEMPTS) {
        message = "Too many attempts: ";
    } else {
        message = "RPC can not complete before timeout: ";
    }
    Status statusTimedOut = Status.TimedOut(message + request);
    final Exception e = new NonRecoverableException(statusTimedOut, cause);
    LOG.debug("Cannot continue with this RPC: {} because of: {}", request, message, e);
    Deferred<R> d = request.getDeferred();
    request.errback(e);
    return d;
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt) throws Exception {
    Object m = evt.getMessage();
    if (m instanceof Negotiator.Result) {
        ArrayList<KuduRpc<?>> queuedRpcs;
        lock.lock();
        try {
            assert chan != null;
            this.negotiationResult = (Result) m;
            state = State.ALIVE;
            queuedRpcs = pendingRpcs;
            pendingRpcs = null;
        } finally {
            lock.unlock();
        }
        // Send the queued RPCs after dropping the lock, so we don't end up calling
        // their callbacks/errbacks with the lock held.
        sendQueuedRpcs(queuedRpcs);
        return;
    }
    if (!(m instanceof CallResponse)) {
        ctx.sendUpstream(evt);
        return;
    }
    CallResponse response = (CallResponse) m;
    final long start = System.nanoTime();
    RpcHeader.ResponseHeader header = response.getHeader();
    if (!header.hasCallId()) {
        final int size = response.getTotalResponseSize();
        final String msg = getPeerUuidLoggingString() + "RPC response (size: " + size + ") doesn't" + " have a call ID: " + header;
        LOG.error(msg);
        Status statusIncomplete = Status.Incomplete(msg);
        throw new NonRecoverableException(statusIncomplete);
    }
    final int rpcid = header.getCallId();
    KuduRpc<Object> rpc;
    lock.lock();
    try {
        rpc = (KuduRpc<Object>) rpcsInflight.remove(rpcid);
    } finally {
        lock.unlock();
    }
    if (rpc == null) {
        final String msg = getPeerUuidLoggingString() + "Invalid rpcid: " + rpcid;
        LOG.error(msg);
        Status statusIllegalState = Status.IllegalState(msg);
        // is likely the way to go.
        throw new NonRecoverableException(statusIllegalState);
    }
    // Start building the trace, we'll finish it as we parse the response.
    RpcTraceFrame.RpcTraceFrameBuilder traceBuilder = new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo);
    Pair<Object, Object> decoded = null;
    KuduException exception = null;
    Status retryableHeaderError = Status.OK();
    if (header.hasIsError() && header.getIsError()) {
        RpcHeader.ErrorStatusPB.Builder errorBuilder = RpcHeader.ErrorStatusPB.newBuilder();
        KuduRpc.readProtobuf(response.getPBMessage(), errorBuilder);
        RpcHeader.ErrorStatusPB error = errorBuilder.build();
        if (error.getCode().equals(RpcHeader.ErrorStatusPB.RpcErrorCodePB.ERROR_SERVER_TOO_BUSY)) {
            // We can't return right away, we still need to remove ourselves from 'rpcsInflight', so we
            // populate 'retryableHeaderError'.
            retryableHeaderError = Status.ServiceUnavailable(error.getMessage());
        } else {
            String message = getPeerUuidLoggingString() + "Tablet server sent error " + error.getMessage();
            Status status = Status.RemoteError(message);
            exception = new NonRecoverableException(status);
            // can be useful
            LOG.error(message);
        }
    } else {
        try {
            decoded = rpc.deserialize(response, this.serverInfo.getUuid());
        } catch (KuduException ex) {
            exception = ex;
        }
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace(getPeerUuidLoggingString() + " received RPC response: " + "rpcId=" + rpcid + ", response size=" + response.getTotalResponseSize() + ", rpc=" + rpc);
    }
    // This check is specifically for the ERROR_SERVER_TOO_BUSY case above.
    if (!retryableHeaderError.ok()) {
        rpc.addTrace(traceBuilder.callStatus(retryableHeaderError).build());
        kuduClient.handleRetryableError(rpc, new RecoverableException(retryableHeaderError));
        return;
    }
    // Have to do it for both TS and Master errors.
    if (decoded != null) {
        if (decoded.getSecond() instanceof Tserver.TabletServerErrorPB) {
            Tserver.TabletServerErrorPB error = (Tserver.TabletServerErrorPB) decoded.getSecond();
            exception = dispatchTSErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // It was taken care of.
                return;
            } else {
                // We're going to errback.
                decoded = null;
            }
        } else if (decoded.getSecond() instanceof Master.MasterErrorPB) {
            Master.MasterErrorPB error = (Master.MasterErrorPB) decoded.getSecond();
            exception = dispatchMasterErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // Exception was taken care of.
                return;
            } else {
                decoded = null;
            }
        }
    }
    try {
        if (decoded != null) {
            assert !(decoded.getFirst() instanceof Exception);
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), decoded.getFirst());
            }
            rpc.addTrace(traceBuilder.callStatus(Status.OK()).build());
            rpc.callback(decoded.getFirst());
        } else {
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), null);
            }
            rpc.addTrace(traceBuilder.callStatus(exception.getStatus()).build());
            rpc.errback(exception);
        }
    } catch (Exception e) {
        LOG.debug(getPeerUuidLoggingString() + "Unexpected exception while handling RPC #" + rpcid + ", rpc=" + rpc, e);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("------------------<< LEAVING  DECODE <<------------------" + " time elapsed: " + ((System.nanoTime() - start) / 1000) + "us");
    }
}
#method_after
@Override
@SuppressWarnings("unchecked")
public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt) throws Exception {
    Object m = evt.getMessage();
    if (m instanceof Negotiator.Result) {
        ArrayList<KuduRpc<?>> queuedRpcs;
        lock.lock();
        try {
            assert chan != null;
            this.negotiationResult = (Result) m;
            state = State.ALIVE;
            queuedRpcs = pendingRpcs;
            pendingRpcs = null;
        } finally {
            lock.unlock();
        }
        // Send the queued RPCs after dropping the lock, so we don't end up calling
        // their callbacks/errbacks with the lock held.
        sendQueuedRpcs(queuedRpcs);
        return;
    }
    if (!(m instanceof CallResponse)) {
        ctx.sendUpstream(evt);
        return;
    }
    CallResponse response = (CallResponse) m;
    final long start = System.nanoTime();
    RpcHeader.ResponseHeader header = response.getHeader();
    if (!header.hasCallId()) {
        final int size = response.getTotalResponseSize();
        final String msg = getPeerUuidLoggingString() + "RPC response (size: " + size + ") doesn't" + " have a call ID: " + header;
        LOG.error(msg);
        Status statusIncomplete = Status.Incomplete(msg);
        throw new NonRecoverableException(statusIncomplete);
    }
    final int rpcid = header.getCallId();
    KuduRpc<Object> rpc;
    lock.lock();
    try {
        rpc = (KuduRpc<Object>) rpcsInflight.remove(rpcid);
    } finally {
        lock.unlock();
    }
    if (rpc == null) {
        final String msg = getPeerUuidLoggingString() + "Invalid rpcid: " + rpcid;
        LOG.error(msg);
        // the server. So, we disconnect the connection.
        throw new NonRecoverableException(Status.IllegalState(msg));
    }
    // Start building the trace, we'll finish it as we parse the response.
    RpcTraceFrame.RpcTraceFrameBuilder traceBuilder = new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo);
    Pair<Object, Object> decoded = null;
    KuduException exception = null;
    Status retryableHeaderError = Status.OK();
    if (header.hasIsError() && header.getIsError()) {
        RpcHeader.ErrorStatusPB.Builder errorBuilder = RpcHeader.ErrorStatusPB.newBuilder();
        KuduRpc.readProtobuf(response.getPBMessage(), errorBuilder);
        RpcHeader.ErrorStatusPB error = errorBuilder.build();
        if (error.getCode().equals(RpcHeader.ErrorStatusPB.RpcErrorCodePB.ERROR_SERVER_TOO_BUSY)) {
            // We can't return right away, we still need to remove ourselves from 'rpcsInflight', so we
            // populate 'retryableHeaderError'.
            retryableHeaderError = Status.ServiceUnavailable(error.getMessage());
        } else {
            String message = getPeerUuidLoggingString() + "Tablet server sent error " + error.getMessage();
            Status status = Status.RemoteError(message);
            exception = new NonRecoverableException(status);
            // can be useful
            LOG.error(message);
        }
    } else {
        try {
            decoded = rpc.deserialize(response, this.serverInfo.getUuid());
        } catch (KuduException ex) {
            exception = ex;
        }
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace(getPeerUuidLoggingString() + " received RPC response: " + "rpcId=" + rpcid + ", response size=" + response.getTotalResponseSize() + ", rpc=" + rpc);
    }
    // This check is specifically for the ERROR_SERVER_TOO_BUSY case above.
    if (!retryableHeaderError.ok()) {
        rpc.addTrace(traceBuilder.callStatus(retryableHeaderError).build());
        kuduClient.handleRetryableError(rpc, new RecoverableException(retryableHeaderError));
        return;
    }
    // Have to do it for both TS and Master errors.
    if (decoded != null) {
        if (decoded.getSecond() instanceof Tserver.TabletServerErrorPB) {
            Tserver.TabletServerErrorPB error = (Tserver.TabletServerErrorPB) decoded.getSecond();
            exception = dispatchTSErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // It was taken care of.
                return;
            } else {
                // We're going to errback.
                decoded = null;
            }
        } else if (decoded.getSecond() instanceof Master.MasterErrorPB) {
            Master.MasterErrorPB error = (Master.MasterErrorPB) decoded.getSecond();
            exception = dispatchMasterErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // Exception was taken care of.
                return;
            } else {
                decoded = null;
            }
        }
    }
    try {
        if (decoded != null) {
            assert !(decoded.getFirst() instanceof Exception);
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), decoded.getFirst());
            }
            rpc.addTrace(traceBuilder.callStatus(Status.OK()).build());
            rpc.callback(decoded.getFirst());
        } else {
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), null);
            }
            rpc.addTrace(traceBuilder.callStatus(exception.getStatus()).build());
            rpc.errback(exception);
        }
    } catch (Exception e) {
        LOG.debug(getPeerUuidLoggingString() + "Unexpected exception while handling RPC #" + rpcid + ", rpc=" + rpc, e);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("------------------<< LEAVING  DECODE <<------------------" + " time elapsed: " + ((System.nanoTime() - start) / 1000) + "us");
    }
}
#end_block

#method_before
@Test
public void testScanTokensConcurrentAlterTable() throws Exception {
    Schema schema = new Schema(ImmutableList.of(new ColumnSchema.ColumnSchemaBuilder("key", Type.INT64).nullable(false).key(true).build(), new ColumnSchema.ColumnSchemaBuilder("a", Type.INT64).nullable(false).key(false).build()));
    CreateTableOptions createOptions = new CreateTableOptions();
    createOptions.setRangePartitionColumns(ImmutableList.<String>of());
    createOptions.setNumReplicas(1);
    syncClient.createTable(tableName, schema, createOptions);
    KuduTable table = syncClient.openTable(tableName);
    KuduScanToken.KuduScanTokenBuilder tokenBuilder = syncClient.newScanTokenBuilder(table);
    List<KuduScanToken> tokens = tokenBuilder.build();
    assertEquals(1, tokens.size());
    KuduScanToken token = tokens.get(0);
    // Drop a column
    syncClient.alterTable(tableName, new AlterTableOptions().dropColumn("a"));
    assertTrue(syncClient.isAlterTableDone(tableName));
    try {
        token.intoScanner(syncClient);
        throw new AssertionError();
    } catch (IllegalArgumentException e) {
        assertTrue(e.getMessage().contains("Unknown column"));
    }
    // Add back the column with the wrong type.
    syncClient.alterTable(tableName, new AlterTableOptions().addColumn(new ColumnSchema.ColumnSchemaBuilder("a", Type.STRING).nullable(true).build()));
    assertTrue(syncClient.isAlterTableDone(tableName));
    try {
        token.intoScanner(syncClient);
        throw new AssertionError();
    } catch (IllegalStateException e) {
        assertTrue(e.getMessage().contains("invalid type INT64 for column 'a' in scan token, expected: STRING"));
    }
    // Add the column with the wrong nullability.
    syncClient.alterTable(tableName, new AlterTableOptions().dropColumn("a").addColumn(new ColumnSchema.ColumnSchemaBuilder("a", Type.INT64).nullable(true).build()));
    assertTrue(syncClient.isAlterTableDone(tableName));
    try {
        token.intoScanner(syncClient);
        throw new AssertionError();
    } catch (IllegalStateException e) {
        assertTrue(e.getMessage().contains("invalid nullability for column 'a' in scan token, expected: NOT NULL"));
    }
    // Add the column with the correct type and nullability.
    syncClient.alterTable(tableName, new AlterTableOptions().dropColumn("a").addColumn(new ColumnSchema.ColumnSchemaBuilder("a", Type.INT64).nullable(false).defaultValue(0L).build()));
    assertTrue(syncClient.isAlterTableDone(tableName));
    token.intoScanner(syncClient);
}
#method_after
@Test
public void testScanTokensConcurrentAlterTable() throws Exception {
    Schema schema = new Schema(ImmutableList.of(new ColumnSchema.ColumnSchemaBuilder("key", Type.INT64).nullable(false).key(true).build(), new ColumnSchema.ColumnSchemaBuilder("a", Type.INT64).nullable(false).key(false).build()));
    CreateTableOptions createOptions = new CreateTableOptions();
    createOptions.setRangePartitionColumns(ImmutableList.<String>of());
    createOptions.setNumReplicas(1);
    syncClient.createTable(tableName, schema, createOptions);
    KuduTable table = syncClient.openTable(tableName);
    KuduScanToken.KuduScanTokenBuilder tokenBuilder = syncClient.newScanTokenBuilder(table);
    List<KuduScanToken> tokens = tokenBuilder.build();
    assertEquals(1, tokens.size());
    KuduScanToken token = tokens.get(0);
    // Drop a column
    syncClient.alterTable(tableName, new AlterTableOptions().dropColumn("a"));
    assertTrue(syncClient.isAlterTableDone(tableName));
    try {
        token.intoScanner(syncClient);
        fail();
    } catch (IllegalArgumentException e) {
        assertTrue(e.getMessage().contains("Unknown column"));
    }
    // Add back the column with the wrong type.
    syncClient.alterTable(tableName, new AlterTableOptions().addColumn(new ColumnSchema.ColumnSchemaBuilder("a", Type.STRING).nullable(true).build()));
    assertTrue(syncClient.isAlterTableDone(tableName));
    try {
        token.intoScanner(syncClient);
        fail();
    } catch (IllegalStateException e) {
        assertTrue(e.getMessage().contains("invalid type INT64 for column 'a' in scan token, expected: STRING"));
    }
    // Add the column with the wrong nullability.
    syncClient.alterTable(tableName, new AlterTableOptions().dropColumn("a").addColumn(new ColumnSchema.ColumnSchemaBuilder("a", Type.INT64).nullable(true).build()));
    assertTrue(syncClient.isAlterTableDone(tableName));
    try {
        token.intoScanner(syncClient);
        fail();
    } catch (IllegalStateException e) {
        assertTrue(e.getMessage().contains("invalid nullability for column 'a' in scan token, expected: NOT NULL"));
    }
    // Add the column with the correct type and nullability.
    syncClient.alterTable(tableName, new AlterTableOptions().dropColumn("a").addColumn(new ColumnSchema.ColumnSchemaBuilder("a", Type.INT64).nullable(false).defaultValue(0L).build()));
    assertTrue(syncClient.isAlterTableDone(tableName));
    token.intoScanner(syncClient);
}
#end_block

#method_before
<R> void sendRpc(KuduRpc<R> rpc) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.SEND_TO_SERVER).serverInfo(serverInfo).build());
    if (!rpc.deadlineTracker.hasDeadline()) {
        LOG.warn(getPeerUuidLoggingString() + " sending an rpc without a timeout " + rpc);
    }
    RpcOutboundMessage outbound = null;
    if (chan != null) {
        if (!rpc.getRequiredFeatures().isEmpty() && !negotiationResult.serverFeatures.contains(RpcHeader.RpcFeatureFlag.APPLICATION_FEATURE_FLAGS)) {
            Status statusNotSupported = Status.NotSupported("the server does not support the" + "APPLICATION_FEATURE_FLAGS RPC feature");
            rpc.errback(new NonRecoverableException(statusNotSupported));
        // TODO(todd): this should return here. We seem to lack test coverage!
        }
        outbound = encode(rpc);
        if (outbound == null) {
            // Stop here.  RPC has been failed already.
            return;
        }
        // Volatile read.
        final Channel chan = this.chan;
        if (chan != null) {
            // Double check if we disconnected during encode().
            Channels.write(chan, outbound);
            return;
        }
    }
    // True when we notice we are about to get connected to the TS.
    boolean tryAgain = false;
    // True when the connection was closed while encoding.
    boolean failRpc = false;
    synchronized (this) {
        // Check if we got connected while entering this synchronized block.
        if (chan != null) {
            tryAgain = true;
        } else if (dead) {
            // `outbound` is null iff `chan` is null.
            if (outbound == null || rpcsInflight.containsKey(outbound.getHeader().getCallId())) {
                failRpc = true;
            }
        } else {
            if (pendingRpcs == null) {
                pendingRpcs = new ArrayList<>();
            }
            pendingRpcs.add(rpc);
        }
    }
    if (failRpc) {
        Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset");
        failOrRetryRpc(rpc, new RecoverableException(statusNetworkError));
    } else if (tryAgain) {
        // This recursion will not lead to a loop because we only get here if we
        // connected while entering the synchronized block above. So when trying
        // a second time,  we will either succeed to send the RPC if we're still
        // connected, or fail through to the code below if we got disconnected
        // in the mean time.
        sendRpc(rpc);
    }
}
#method_after
<R> void sendRpc(KuduRpc<R> rpc) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.SEND_TO_SERVER).serverInfo(serverInfo).build());
    if (!rpc.deadlineTracker.hasDeadline()) {
        LOG.warn(getPeerUuidLoggingString() + " sending an rpc without a timeout " + rpc);
    }
    // Serialize the request outside the lock.
    Message req;
    try {
        req = rpc.createRequestPB();
    } catch (Exception e) {
        LOG.error("Uncaught exception while constructing RPC request: " + rpc, e);
        // Make the RPC fail with the exception.
        rpc.errback(e);
        return;
    }
    lock.lock();
    boolean needsUnlock = true;
    try {
        // If we are disconnected, immediately fail the RPC
        if (state == State.DISCONNECTED) {
            lock.unlock();
            needsUnlock = false;
            Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset");
            failOrRetryRpc(rpc, new RecoverableException(statusNetworkError));
            return;
        }
        // fails.
        if (state == State.NEGOTIATING) {
            pendingRpcs.add(rpc);
            return;
        }
        // We are alive, in which case we must have a channel.
        assert state == State.ALIVE;
        assert chan != null;
        // Check that the server supports feature flags, if our RPC uses them.
        if (!rpc.getRequiredFeatures().isEmpty() && !negotiationResult.serverFeatures.contains(RpcHeader.RpcFeatureFlag.APPLICATION_FEATURE_FLAGS)) {
            // We don't want to call out of this class while holding the lock.
            lock.unlock();
            needsUnlock = false;
            Status statusNotSupported = Status.NotSupported("the server does not support the" + "APPLICATION_FEATURE_FLAGS RPC feature");
            rpc.errback(new NonRecoverableException(statusNotSupported));
            return;
        }
        // Assign the call ID and write it to the wire.
        sendCallToWire(rpc, req);
    } finally {
        if (needsUnlock) {
            lock.unlock();
        }
    }
}
#end_block

#method_before
@VisibleForTesting
void disconnect() {
    Channel chancopy = chan;
    if (chancopy != null && chancopy.isConnected()) {
        Channels.disconnect(chancopy);
    }
}
#method_after
@VisibleForTesting
ChannelFuture disconnect() {
    // 'chan' should never be null, because as soon as this object is created, it's
    // added to a ChannelPipeline, which synchronously fires the channelOpen()
    // event.
    Preconditions.checkNotNull(chan);
    return Channels.disconnect(chan);
}
#end_block

#method_before
public Deferred<Void> shutdown() {
    Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Client is shutting down");
    NonRecoverableException exception = new NonRecoverableException(statusNetworkError);
    // First, check whether we have RPCs in flight and cancel them.
    for (Iterator<KuduRpc<?>> ite = rpcsInflight.values().iterator(); ite.hasNext(); ) {
        ite.next().errback(exception);
        ite.remove();
    }
    // Same for the pending RPCs.
    synchronized (this) {
        if (pendingRpcs != null) {
            for (Iterator<KuduRpc<?>> ite = pendingRpcs.iterator(); ite.hasNext(); ) {
                ite.next().errback(exception);
                ite.remove();
            }
        }
    }
    final Channel chancopy = chan;
    if (chancopy == null) {
        return Deferred.fromResult(null);
    }
    if (chancopy.isConnected()) {
        // ... this is going to set it to null.
        Channels.disconnect(chancopy);
    // At this point, all in-flight RPCs are going to be failed.
    }
    if (chancopy.isBound()) {
        Channels.unbind(chancopy);
    }
    // It's OK to call close() on a Channel if it's already closed.
    final ChannelFuture future = Channels.close(chancopy);
    // Now wrap the ChannelFuture in a Deferred.
    final Deferred<Void> d = new Deferred<Void>();
    // Opportunistically check if it's already completed successfully.
    if (future.isSuccess()) {
        d.callback(null);
    } else {
        // If we get here, either the future failed (yeah, that sounds weird)
        // or the future hasn't completed yet (heh).
        future.addListener(new ChannelFutureListener() {

            public void operationComplete(final ChannelFuture future) {
                if (future.isSuccess()) {
                    d.callback(null);
                    return;
                }
                final Throwable t = future.getCause();
                if (t instanceof Exception) {
                    d.callback(t);
                } else {
                    // Wrap the Throwable because Deferred doesn't handle Throwables,
                    // it only uses Exception.
                    Status statusIllegalState = Status.IllegalState("Failed to shutdown: " + TabletClient.this);
                    d.callback(new NonRecoverableException(statusIllegalState, t));
                }
            }
        });
    }
    return d;
}
#method_after
public Deferred<Void> shutdown() {
    ChannelFuture disconnectFuture = disconnect();
    final Deferred<Void> d = new Deferred<Void>();
    disconnectFuture.addListener(new ChannelFutureListener() {

        public void operationComplete(final ChannelFuture future) {
            if (future.isSuccess()) {
                d.callback(null);
                return;
            }
            final Throwable t = future.getCause();
            if (t instanceof Exception) {
                d.callback(t);
            } else {
                // Wrap the Throwable because Deferred doesn't handle Throwables,
                // it only uses Exception.
                Status statusIllegalState = Status.IllegalState("Failed to shutdown: " + TabletClient.this);
                d.callback(new NonRecoverableException(statusIllegalState, t));
            }
        }
    });
    return d;
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt) throws Exception {
    Object m = evt.getMessage();
    if (m instanceof Negotiator.Result) {
        this.negotiationResult = (Result) m;
        this.chan = ctx.getChannel();
        sendQueuedRpcs();
        return;
    }
    if (!(m instanceof CallResponse)) {
        ctx.sendUpstream(evt);
        return;
    }
    CallResponse response = (CallResponse) m;
    final long start = System.nanoTime();
    RpcHeader.ResponseHeader header = response.getHeader();
    if (!header.hasCallId()) {
        final int size = response.getTotalResponseSize();
        final String msg = getPeerUuidLoggingString() + "RPC response (size: " + size + ") doesn't" + " have a call ID: " + header;
        LOG.error(msg);
        Status statusIncomplete = Status.Incomplete(msg);
        throw new NonRecoverableException(statusIncomplete);
    }
    final int rpcid = header.getCallId();
    @SuppressWarnings("rawtypes")
    final KuduRpc rpc = rpcsInflight.get(rpcid);
    if (rpc == null) {
        final String msg = getPeerUuidLoggingString() + "Invalid rpcid: " + rpcid;
        LOG.error(msg);
        Status statusIllegalState = Status.IllegalState(msg);
        // all RPCs in flight to be failed.
        throw new NonRecoverableException(statusIllegalState);
    }
    // Start building the trace, we'll finish it as we parse the response.
    RpcTraceFrame.RpcTraceFrameBuilder traceBuilder = new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo);
    Pair<Object, Object> decoded = null;
    KuduException exception = null;
    Status retryableHeaderError = Status.OK();
    if (header.hasIsError() && header.getIsError()) {
        RpcHeader.ErrorStatusPB.Builder errorBuilder = RpcHeader.ErrorStatusPB.newBuilder();
        KuduRpc.readProtobuf(response.getPBMessage(), errorBuilder);
        RpcHeader.ErrorStatusPB error = errorBuilder.build();
        if (error.getCode().equals(RpcHeader.ErrorStatusPB.RpcErrorCodePB.ERROR_SERVER_TOO_BUSY)) {
            // We can't return right away, we still need to remove ourselves from 'rpcsInflight', so we
            // populate 'retryableHeaderError'.
            retryableHeaderError = Status.ServiceUnavailable(error.getMessage());
        } else {
            String message = getPeerUuidLoggingString() + "Tablet server sent error " + error.getMessage();
            Status status = Status.RemoteError(message);
            exception = new NonRecoverableException(status);
            // can be useful
            LOG.error(message);
        }
    } else {
        try {
            decoded = rpc.deserialize(response, this.serverInfo.getUuid());
        } catch (KuduException ex) {
            exception = ex;
        }
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace(getPeerUuidLoggingString() + " received RPC response: " + ", response size=" + response.getTotalResponseSize() + ", rpc=" + rpc);
    }
    {
        final KuduRpc<?> removed = rpcsInflight.remove(rpcid);
        if (removed == null) {
            // The RPC we were decoding was cleaned up already, give up.
            Status statusIllegalState = Status.IllegalState("RPC not found");
            throw new NonRecoverableException(statusIllegalState);
        }
    }
    // This check is specifically for the ERROR_SERVER_TOO_BUSY case above.
    if (!retryableHeaderError.ok()) {
        rpc.addTrace(traceBuilder.callStatus(retryableHeaderError).build());
        kuduClient.handleRetryableError(rpc, new RecoverableException(retryableHeaderError));
        return;
    }
    // Have to do it for both TS and Master errors.
    if (decoded != null) {
        if (decoded.getSecond() instanceof Tserver.TabletServerErrorPB) {
            Tserver.TabletServerErrorPB error = (Tserver.TabletServerErrorPB) decoded.getSecond();
            exception = dispatchTSErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // It was taken care of.
                return;
            } else {
                // We're going to errback.
                decoded = null;
            }
        } else if (decoded.getSecond() instanceof Master.MasterErrorPB) {
            Master.MasterErrorPB error = (Master.MasterErrorPB) decoded.getSecond();
            exception = dispatchMasterErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // Exception was taken care of.
                return;
            } else {
                decoded = null;
            }
        }
    }
    try {
        if (decoded != null) {
            assert !(decoded.getFirst() instanceof Exception);
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), decoded.getFirst());
            }
            rpc.addTrace(traceBuilder.callStatus(Status.OK()).build());
            rpc.callback(decoded.getFirst());
        } else {
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), null);
            }
            rpc.addTrace(traceBuilder.callStatus(exception.getStatus()).build());
            rpc.errback(exception);
        }
    } catch (Exception e) {
        LOG.debug(getPeerUuidLoggingString() + "Unexpected exception while handling RPC #" + rpcid + ", rpc=" + rpc, e);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("------------------<< LEAVING  DECODE <<------------------" + " time elapsed: " + ((System.nanoTime() - start) / 1000) + "us");
    }
}
#method_after
@Override
@SuppressWarnings("unchecked")
public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt) throws Exception {
    Object m = evt.getMessage();
    if (m instanceof Negotiator.Result) {
        ArrayList<KuduRpc<?>> queuedRpcs;
        lock.lock();
        try {
            assert chan != null;
            this.negotiationResult = (Result) m;
            state = State.ALIVE;
            queuedRpcs = pendingRpcs;
            pendingRpcs = null;
        } finally {
            lock.unlock();
        }
        // Send the queued RPCs after dropping the lock, so we don't end up calling
        // their callbacks/errbacks with the lock held.
        sendQueuedRpcs(queuedRpcs);
        return;
    }
    if (!(m instanceof CallResponse)) {
        ctx.sendUpstream(evt);
        return;
    }
    CallResponse response = (CallResponse) m;
    final long start = System.nanoTime();
    RpcHeader.ResponseHeader header = response.getHeader();
    if (!header.hasCallId()) {
        final int size = response.getTotalResponseSize();
        final String msg = getPeerUuidLoggingString() + "RPC response (size: " + size + ") doesn't" + " have a call ID: " + header;
        LOG.error(msg);
        Status statusIncomplete = Status.Incomplete(msg);
        throw new NonRecoverableException(statusIncomplete);
    }
    final int rpcid = header.getCallId();
    @SuppressWarnings("rawtypes")
    final KuduRpc rpc = rpcsInflight.get(rpcid);
    if (rpc == null) {
        final String msg = getPeerUuidLoggingString() + "Invalid rpcid: " + rpcid;
        LOG.error(msg);
        Status statusIllegalState = Status.IllegalState(msg);
        // all RPCs in flight to be failed.
        throw new NonRecoverableException(statusIllegalState);
    }
    // Start building the trace, we'll finish it as we parse the response.
    RpcTraceFrame.RpcTraceFrameBuilder traceBuilder = new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo);
    Pair<Object, Object> decoded = null;
    KuduException exception = null;
    Status retryableHeaderError = Status.OK();
    if (header.hasIsError() && header.getIsError()) {
        RpcHeader.ErrorStatusPB.Builder errorBuilder = RpcHeader.ErrorStatusPB.newBuilder();
        KuduRpc.readProtobuf(response.getPBMessage(), errorBuilder);
        RpcHeader.ErrorStatusPB error = errorBuilder.build();
        if (error.getCode().equals(RpcHeader.ErrorStatusPB.RpcErrorCodePB.ERROR_SERVER_TOO_BUSY)) {
            // We can't return right away, we still need to remove ourselves from 'rpcsInflight', so we
            // populate 'retryableHeaderError'.
            retryableHeaderError = Status.ServiceUnavailable(error.getMessage());
        } else {
            String message = getPeerUuidLoggingString() + "Tablet server sent error " + error.getMessage();
            Status status = Status.RemoteError(message);
            exception = new NonRecoverableException(status);
            // can be useful
            LOG.error(message);
        }
    } else {
        try {
            decoded = rpc.deserialize(response, this.serverInfo.getUuid());
        } catch (KuduException ex) {
            exception = ex;
        }
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace(getPeerUuidLoggingString() + " received RPC response: " + "rpcId=" + rpcid + ", response size=" + response.getTotalResponseSize() + ", rpc=" + rpc);
    }
    {
        final KuduRpc<?> removed = rpcsInflight.remove(rpcid);
        if (removed == null) {
            // The RPC we were decoding was cleaned up already, give up.
            Status statusIllegalState = Status.IllegalState("RPC not found");
            throw new NonRecoverableException(statusIllegalState);
        }
    }
    // This check is specifically for the ERROR_SERVER_TOO_BUSY case above.
    if (!retryableHeaderError.ok()) {
        rpc.addTrace(traceBuilder.callStatus(retryableHeaderError).build());
        kuduClient.handleRetryableError(rpc, new RecoverableException(retryableHeaderError));
        return;
    }
    // Have to do it for both TS and Master errors.
    if (decoded != null) {
        if (decoded.getSecond() instanceof Tserver.TabletServerErrorPB) {
            Tserver.TabletServerErrorPB error = (Tserver.TabletServerErrorPB) decoded.getSecond();
            exception = dispatchTSErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // It was taken care of.
                return;
            } else {
                // We're going to errback.
                decoded = null;
            }
        } else if (decoded.getSecond() instanceof Master.MasterErrorPB) {
            Master.MasterErrorPB error = (Master.MasterErrorPB) decoded.getSecond();
            exception = dispatchMasterErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // Exception was taken care of.
                return;
            } else {
                decoded = null;
            }
        }
    }
    try {
        if (decoded != null) {
            assert !(decoded.getFirst() instanceof Exception);
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), decoded.getFirst());
            }
            rpc.addTrace(traceBuilder.callStatus(Status.OK()).build());
            rpc.callback(decoded.getFirst());
        } else {
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), null);
            }
            rpc.addTrace(traceBuilder.callStatus(exception.getStatus()).build());
            rpc.errback(exception);
        }
    } catch (Exception e) {
        LOG.debug(getPeerUuidLoggingString() + "Unexpected exception while handling RPC #" + rpcid + ", rpc=" + rpc, e);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("------------------<< LEAVING  DECODE <<------------------" + " time elapsed: " + ((System.nanoTime() - start) / 1000) + "us");
    }
}
#end_block

#method_before
private KuduException dispatchTSErrorOrReturnException(KuduRpc rpc, Tserver.TabletServerErrorPB error, RpcTraceFrame.RpcTraceFrameBuilder traceBuilder) {
    WireProtocol.AppStatusPB.ErrorCode code = error.getStatus().getCode();
    Status status = Status.fromTabletServerErrorPB(error);
    if (error.getCode() == Tserver.TabletServerErrorPB.Code.TABLET_NOT_FOUND) {
        kuduClient.handleTabletNotFound(rpc, new RecoverableException(status), this);
    // we're not calling rpc.callback() so we rely on the client to retry that RPC
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.SERVICE_UNAVAILABLE) {
        kuduClient.handleRetryableError(rpc, new RecoverableException(status));
    // The following two error codes are an indication that the tablet isn't a leader.
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.ILLEGAL_STATE || code == WireProtocol.AppStatusPB.ErrorCode.ABORTED) {
        kuduClient.handleNotLeader(rpc, new RecoverableException(status), this);
    } else {
        return new NonRecoverableException(status);
    }
    rpc.addTrace(traceBuilder.callStatus(status).build());
    return null;
}
#method_after
private KuduException dispatchTSErrorOrReturnException(KuduRpc<?> rpc, Tserver.TabletServerErrorPB error, RpcTraceFrame.RpcTraceFrameBuilder traceBuilder) {
    WireProtocol.AppStatusPB.ErrorCode code = error.getStatus().getCode();
    Status status = Status.fromTabletServerErrorPB(error);
    if (error.getCode() == Tserver.TabletServerErrorPB.Code.TABLET_NOT_FOUND) {
        kuduClient.handleTabletNotFound(rpc, new RecoverableException(status), this);
    // we're not calling rpc.callback() so we rely on the client to retry that RPC
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.SERVICE_UNAVAILABLE) {
        kuduClient.handleRetryableError(rpc, new RecoverableException(status));
    // The following two error codes are an indication that the tablet isn't a leader.
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.ILLEGAL_STATE || code == WireProtocol.AppStatusPB.ErrorCode.ABORTED) {
        kuduClient.handleNotLeader(rpc, new RecoverableException(status), this);
    } else {
        return new NonRecoverableException(status);
    }
    rpc.addTrace(traceBuilder.callStatus(status).build());
    return null;
}
#end_block

#method_before
private KuduException dispatchMasterErrorOrReturnException(KuduRpc rpc, Master.MasterErrorPB error, RpcTraceFrame.RpcTraceFrameBuilder traceBuilder) {
    WireProtocol.AppStatusPB.ErrorCode code = error.getStatus().getCode();
    Status status = Status.fromMasterErrorPB(error);
    if (error.getCode() == Master.MasterErrorPB.Code.NOT_THE_LEADER) {
        kuduClient.handleNotLeader(rpc, new RecoverableException(status), this);
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.SERVICE_UNAVAILABLE) {
        if (rpc instanceof GetMasterRegistrationRequest) {
            // is. If the error is truly non recoverable, it'll be handled later.
            return new RecoverableException(status);
        } else {
            // TODO: This is a crutch until we either don't have to retry RPCs going to the
            // same server or use retry policies.
            kuduClient.handleRetryableError(rpc, new RecoverableException(status));
        }
    } else {
        return new NonRecoverableException(status);
    }
    rpc.addTrace(traceBuilder.callStatus(status).build());
    return null;
}
#method_after
private KuduException dispatchMasterErrorOrReturnException(KuduRpc<?> rpc, Master.MasterErrorPB error, RpcTraceFrame.RpcTraceFrameBuilder traceBuilder) {
    WireProtocol.AppStatusPB.ErrorCode code = error.getStatus().getCode();
    Status status = Status.fromMasterErrorPB(error);
    if (error.getCode() == Master.MasterErrorPB.Code.NOT_THE_LEADER) {
        kuduClient.handleNotLeader(rpc, new RecoverableException(status), this);
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.SERVICE_UNAVAILABLE) {
        if (rpc instanceof GetMasterRegistrationRequest) {
            // is. If the error is truly non recoverable, it'll be handled later.
            return new RecoverableException(status);
        } else {
            // TODO: This is a crutch until we either don't have to retry RPCs going to the
            // same server or use retry policies.
            kuduClient.handleRetryableError(rpc, new RecoverableException(status));
        }
    } else {
        return new NonRecoverableException(status);
    }
    rpc.addTrace(traceBuilder.callStatus(status).build());
    return null;
}
#end_block

#method_before
public boolean isAlive() {
    synchronized (this) {
        return !dead;
    }
}
#method_after
public boolean isAlive() {
    lock.lock();
    try {
        return state == State.ALIVE || state == State.NEGOTIATING;
    } finally {
        lock.unlock();
    }
}
#end_block

#method_before
@Override
public void channelConnected(final ChannelHandlerContext ctx, final ChannelStateEvent e) {
    final Channel chan = e.getChannel();
    Channels.write(chan, ChannelBuffers.wrappedBuffer(CONNECTION_HEADER));
    Negotiator secureRpcHelper = new Negotiator(getSubject(), serverInfo.getHostname());
    ctx.getPipeline().addBefore(ctx.getName(), "negotiation", secureRpcHelper);
    secureRpcHelper.sendHello(chan);
}
#method_after
@Override
public void channelConnected(final ChannelHandlerContext ctx, final ChannelStateEvent e) {
    assert chan != null;
    Channels.write(chan, ChannelBuffers.wrappedBuffer(CONNECTION_HEADER));
    Negotiator secureRpcHelper = new Negotiator(getSubject(), serverInfo.getHostname());
    ctx.getPipeline().addBefore(ctx.getName(), "negotiation", secureRpcHelper);
    secureRpcHelper.sendHello(chan);
}
#end_block

#method_before
@Override
public void handleUpstream(final ChannelHandlerContext ctx, final ChannelEvent e) throws Exception {
    if (LOG.isTraceEnabled()) {
        LOG.trace(getPeerUuidLoggingString() + e.toString());
    }
    super.handleUpstream(ctx, e);
}
#method_after
@Override
public void handleUpstream(final ChannelHandlerContext ctx, final ChannelEvent e) throws Exception {
    if (LOG.isTraceEnabled()) {
        LOG.trace(e.toString());
    }
    super.handleUpstream(ctx, e);
}
#end_block

#method_before
@Override
public void channelDisconnected(final ChannelHandlerContext ctx, final ChannelStateEvent e) throws Exception {
    chan = null;
    // Let the ReplayingDecoder cleanup.
    super.channelDisconnected(ctx, e);
    cleanup("Connection disconnected");
}
#method_after
@Override
public void channelDisconnected(final ChannelHandlerContext ctx, final ChannelStateEvent e) throws Exception {
    // Let the ReplayingDecoder cleanup.
    super.channelDisconnected(ctx, e);
    cleanup("Connection disconnected");
}
#end_block

#method_before
@Override
public void channelClosed(final ChannelHandlerContext ctx, final ChannelStateEvent e) {
    chan = null;
    // No need to call super.channelClosed() because we already called
    // super.channelDisconnected().  If we get here without getting a
    // DISCONNECTED event, then we were never connected in the first place so
    // the ReplayingDecoder has nothing to cleanup.
    cleanup("Connection closed");
}
#method_after
@Override
public void channelClosed(final ChannelHandlerContext ctx, final ChannelStateEvent e) throws Exception {
    super.channelClosed(ctx, e);
    cleanup("Connection closed");
}
#end_block

#method_before
private void cleanup(final String errorMessage) {
    final ArrayList<KuduRpc<?>> rpcs;
    // to be sent to failOrRetryRpc.
    synchronized (this) {
        // clear up rpcsInflight multiple times.
        if (dead) {
            return;
        }
        dead = true;
        rpcs = pendingRpcs == null ? new ArrayList<KuduRpc<?>>(rpcsInflight.size()) : pendingRpcs;
        for (Iterator<KuduRpc<?>> iterator = rpcsInflight.values().iterator(); iterator.hasNext(); ) {
            KuduRpc<?> rpc = iterator.next();
            rpcs.add(rpc);
            iterator.remove();
        }
        // After this, rpcsInflight might still have entries since they could have been added
        // concurrently, and those RPCs will be handled by their caller in sendRpc.
        pendingRpcs = null;
    }
    Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + (errorMessage == null ? "Connection reset" : errorMessage));
    RecoverableException exception = new RecoverableException(statusNetworkError);
    failOrRetryRpcs(rpcs, exception);
}
#method_after
private void cleanup(final String errorMessage) {
    final ArrayList<KuduRpc<?>> rpcsToFail = Lists.newArrayList();
    lock.lock();
    try {
        // Cleanup can be called multiple times, but we only want to run it once.
        if (state == State.DISCONNECTED) {
            assert pendingRpcs == null;
            return;
        }
        state = State.DISCONNECTED;
        // for negotiation to complete.
        if (pendingRpcs != null) {
            rpcsToFail.addAll(pendingRpcs);
        }
        pendingRpcs = null;
        // Similarly, we need to fail any that were already sent and in-flight.
        rpcsToFail.addAll(rpcsInflight.values());
        rpcsInflight = null;
    } finally {
        lock.unlock();
    }
    Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + (errorMessage == null ? "Connection reset" : errorMessage));
    RecoverableException exception = new RecoverableException(statusNetworkError);
    failOrRetryRpcs(rpcsToFail, exception);
}
#end_block

#method_before
private void failOrRetryRpc(final KuduRpc<?> rpc, final RecoverableException exception) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo).callStatus(exception.getStatus()).build());
    RemoteTablet tablet = rpc.getTablet();
    // tablet it's because we didn't set it properly before calling sendRpc().
    if (tablet == null) {
        // Can't retry, dunno where this RPC should go.
        rpc.errback(exception);
    } else {
        if (gotUncaughtException) {
            // This will remove this TabletClient from this RPC's cache since there's something wrong
            // about it.
            kuduClient.handleTabletNotFound(rpc, exception, this);
        } else {
            kuduClient.handleRetryableError(rpc, exception);
        }
    }
}
#method_after
private void failOrRetryRpc(final KuduRpc<?> rpc, final RecoverableException exception) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo).callStatus(exception.getStatus()).build());
    RemoteTablet tablet = rpc.getTablet();
    // tablet it's because we didn't set it properly before calling sendRpc().
    if (tablet == null) {
        // Can't retry, dunno where this RPC should go.
        rpc.errback(exception);
    } else {
        kuduClient.handleTabletNotFound(rpc, exception, this);
    }
}
#end_block

#method_before
@Override
public void exceptionCaught(final ChannelHandlerContext ctx, final ExceptionEvent event) {
    final Throwable e = event.getCause();
    final Channel c = event.getChannel();
    if (e instanceof RejectedExecutionException) {
        LOG.warn(getPeerUuidLoggingString() + "RPC rejected by the executor," + " ignore this if we're shutting down", e);
    } else if (e instanceof ReadTimeoutException) {
        LOG.debug(getPeerUuidLoggingString() + "Encountered a read timeout, will close the channel");
    } else {
        LOG.error(getPeerUuidLoggingString() + "Unexpected exception from downstream on " + c, e);
        // For any other exception, likely a connection error, we'll clear the tablet caches for the
        // RPCs we're going to retry.
        gotUncaughtException = true;
    }
    if (c.isOpen()) {
        // Will trigger channelClosed(), which will cleanup()
        Channels.close(c);
    } else {
        // else: presumably a connection timeout.
        // => need to cleanup() from here directly.
        cleanup(e.getMessage());
    }
}
#method_after
@Override
public void exceptionCaught(final ChannelHandlerContext ctx, final ExceptionEvent event) {
    final Throwable e = event.getCause();
    final Channel c = event.getChannel();
    if (e instanceof RejectedExecutionException) {
        LOG.warn(getPeerUuidLoggingString() + "RPC rejected by the executor," + " ignore this if we're shutting down", e);
    } else if (e instanceof ReadTimeoutException) {
        LOG.debug(getPeerUuidLoggingString() + "Encountered a read timeout, will close the channel");
    } else {
        LOG.error(getPeerUuidLoggingString() + "Unexpected exception from downstream on " + c, e);
    }
    if (c.isOpen()) {
        // Will trigger channelClosed(), which will cleanup()
        Channels.close(c);
    } else {
        // else: presumably a connection timeout.
        // => need to cleanup() from here directly.
        cleanup(e.getMessage());
    }
}
#end_block

#method_before
private void sendQueuedRpcs() {
    ArrayList<KuduRpc<?>> rpcs;
    synchronized (this) {
        rpcs = pendingRpcs;
        pendingRpcs = null;
    }
    if (rpcs != null) {
        for (final KuduRpc<?> rpc : rpcs) {
            LOG.debug(getPeerUuidLoggingString() + "Executing RPC queued: " + rpc);
            sendRpc(rpc);
        }
    }
}
#method_after
private void sendQueuedRpcs(List<KuduRpc<?>> rpcs) {
    assert !lock.isHeldByCurrentThread();
    for (final KuduRpc<?> rpc : rpcs) {
        LOG.debug(getPeerUuidLoggingString() + "Executing RPC queued: " + rpc);
        sendRpc(rpc);
    }
}
#end_block

#method_before
public String toString() {
    final StringBuilder buf = new StringBuilder(13 + 10 + 6 + 64 + 7 + 32 + 16 + 1 + 17 + 2 + 1);
    // =13
    buf.append("TabletClient@").append(// ~10
    hashCode()).append(// = 6
    "(chan=").append(// ~64 (up to 66 when using IPv4)
    chan).append(// = 7
    ", uuid=").append(// = 32
    serverInfo.getUuid()).append(// =16
    ", #pending_rpcs=");
    int npendingRpcs;
    synchronized (this) {
        npendingRpcs = pendingRpcs == null ? 0 : pendingRpcs.size();
    }
    // = 1
    buf.append(npendingRpcs);
    // =17
    buf.append(", #rpcs_inflight=").append(// ~ 2
    rpcsInflight.size()).append(// = 1
    ')');
    return buf.toString();
}
#method_after
public String toString() {
    final StringBuilder buf = new StringBuilder();
    buf.append("TabletClient@").append(hashCode()).append("(chan=").append(chan).append(", uuid=").append(serverInfo.getUuid()).append(", #pending_rpcs=");
    int npendingRpcs;
    int nInFlight;
    lock.lock();
    try {
        npendingRpcs = pendingRpcs == null ? 0 : pendingRpcs.size();
        nInFlight = rpcsInflight == null ? 0 : rpcsInflight.size();
    } finally {
        lock.unlock();
    }
    buf.append(npendingRpcs);
    buf.append(", #rpcs_inflight=").append(nInFlight).append(')');
    return buf.toString();
}
#end_block

#method_before
private void loadBlockMetadata(Path dirPath, HashMap<Path, List<HdfsPartition>> partsByPath) {
    try {
        FileSystem fs = dirPath.getFileSystem(CONF);
        // No need to load blocks for empty partitions list.
        if (partsByPath.size() == 0 || !fs.exists(dirPath))
            return;
        if (LOG.isTraceEnabled()) {
            LOG.trace("Loading block md for " + name_ + " directory " + dirPath.toString());
        }
        // Clear the state of partitions under dirPath since they are going to be updated
        // based on the current snapshot of files in the directory.
        List<HdfsPartition> dirPathPartitions = partsByPath.get(dirPath);
        if (dirPathPartitions != null) {
            // unpartitioned table, or the path of at least one partition.
            for (HdfsPartition partition : dirPathPartitions) {
                partition.setFileDescriptors(new ArrayList<FileDescriptor>());
            }
        } else {
            // a descendant of dirPath.
            for (Map.Entry<Path, List<HdfsPartition>> entry : partsByPath.entrySet()) {
                Path partDir = entry.getKey();
                if (!FileSystemUtil.isDescendantPath(partDir, dirPath))
                    continue;
                for (HdfsPartition partition : entry.getValue()) {
                    partition.setFileDescriptors(new ArrayList<FileDescriptor>());
                }
            }
        }
        // block location metadata based on file formats.
        if (!FileSystemUtil.supportsStorageIds(fs)) {
            synthesizeBlockMetadata(fs, dirPath, partsByPath);
            return;
        }
        int unknownDiskIdCount = 0;
        RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
        while (fileStatusIter.hasNext()) {
            LocatedFileStatus fileStatus = fileStatusIter.next();
            if (!FileSystemUtil.isValidDataFile(fileStatus))
                continue;
            // Find the partition that this file belongs (if any).
            Path partPathDir = fileStatus.getPath().getParent();
            Preconditions.checkNotNull(partPathDir);
            List<HdfsPartition> partitions = partsByPath.get(partPathDir);
            // Skip if this file does not belong to any known partition.
            if (partitions == null) {
                if (LOG.isTraceEnabled()) {
                    LOG.trace("File " + fileStatus.getPath().toString() + " doesn't correspond " + " to a known partition. Skipping metadata load for this file.");
                }
                continue;
            }
            String fileName = fileStatus.getPath().getName();
            FileDescriptor fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
            BlockLocation[] locations = fileStatus.getBlockLocations();
            unknownDiskIdCount += computeFdBlockMetadata(fd, locations);
            if (LOG.isTraceEnabled()) {
                LOG.trace("Adding file md dir: " + partPathDir.toString() + " file: " + fileName);
            }
            // Update the partitions' metadata that this file belongs to.
            for (HdfsPartition partition : partitions) {
                partition.getFileDescriptors().add(fd);
                numHdfsFiles_++;
                totalHdfsBytes_ += fd.getFileLength();
            }
        }
        if (unknownDiskIdCount > 0) {
            if (LOG.isWarnEnabled()) {
                LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException("Error loading block metadata for directory " + dirPath.toString() + ": " + e.getMessage(), e);
    }
}
#method_after
private void loadBlockMetadata(Path dirPath, HashMap<Path, List<HdfsPartition>> partsByPath) {
    try {
        FileSystem fs = dirPath.getFileSystem(CONF);
        // No need to load blocks for empty partitions list.
        if (partsByPath.size() == 0 || !fs.exists(dirPath))
            return;
        if (LOG.isTraceEnabled()) {
            LOG.trace("Loading block md for " + name_ + " directory " + dirPath.toString());
        }
        // Clear the state of partitions under dirPath since they are going to be updated
        // based on the current snapshot of files in the directory.
        List<HdfsPartition> dirPathPartitions = partsByPath.get(dirPath);
        if (dirPathPartitions != null) {
            // unpartitioned table, or the path of at least one partition.
            for (HdfsPartition partition : dirPathPartitions) {
                partition.setFileDescriptors(new ArrayList<FileDescriptor>());
            }
        } else {
            // a descendant of dirPath.
            for (Map.Entry<Path, List<HdfsPartition>> entry : partsByPath.entrySet()) {
                Path partDir = entry.getKey();
                if (!FileSystemUtil.isDescendantPath(partDir, dirPath))
                    continue;
                for (HdfsPartition partition : entry.getValue()) {
                    partition.setFileDescriptors(new ArrayList<FileDescriptor>());
                }
            }
        }
        // block location metadata based on file formats.
        if (!FileSystemUtil.supportsStorageIds(fs)) {
            synthesizeBlockMetadata(fs, dirPath, partsByPath);
            return;
        }
        int unknownDiskIdCount = 0;
        RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
        while (fileStatusIter.hasNext()) {
            LocatedFileStatus fileStatus = fileStatusIter.next();
            if (!FileSystemUtil.isValidDataFile(fileStatus))
                continue;
            // Find the partition that this file belongs (if any).
            Path partPathDir = fileStatus.getPath().getParent();
            Preconditions.checkNotNull(partPathDir);
            List<HdfsPartition> partitions = partsByPath.get(partPathDir);
            // Skip if this file does not belong to any known partition.
            if (partitions == null) {
                if (LOG.isTraceEnabled()) {
                    LOG.trace("File " + fileStatus.getPath().toString() + " doesn't correspond " + " to a known partition. Skipping metadata load for this file.");
                }
                continue;
            }
            String fileName = fileStatus.getPath().getName();
            FileDescriptor fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
            BlockLocation[] locations = fileStatus.getBlockLocations();
            unknownDiskIdCount += setFdBlockMetadata(fd, locations);
            if (LOG.isTraceEnabled()) {
                LOG.trace("Adding file md dir: " + partPathDir.toString() + " file: " + fileName);
            }
            // Update the partitions' metadata that this file belongs to.
            for (HdfsPartition partition : partitions) {
                partition.getFileDescriptors().add(fd);
                numHdfsFiles_++;
                totalHdfsBytes_ += fd.getFileLength();
            }
        }
        if (unknownDiskIdCount > 0) {
            if (LOG.isWarnEnabled()) {
                LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException("Error loading block metadata for directory " + dirPath.toString() + ": " + e.getMessage(), e);
    }
}
#end_block

#method_before
public HdfsPartition createAndLoadPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws CatalogException {
    HdfsPartition hdfsPartition = createPartition(storageDescriptor, msPartition);
    loadMetadataAndDiskIds(hdfsPartition);
    return hdfsPartition;
}
#method_after
public HdfsPartition createAndLoadPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws CatalogException {
    HdfsPartition hdfsPartition = createPartition(storageDescriptor, msPartition);
    refreshFileMetadata(hdfsPartition);
    return hdfsPartition;
}
#end_block

#method_before
private void updateUnpartitionedTableFileMd() throws CatalogException {
    if (LOG.isTraceEnabled()) {
        LOG.trace("update unpartitioned table: " + name_);
    }
    resetPartitions();
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    addDefaultPartition(msTbl.getSd());
    HdfsPartition part = createPartition(msTbl.getSd(), null);
    addPartition(part);
    if (isMarkedCached_)
        part.markCached();
    loadMetadataAndDiskIds(part);
}
#method_after
private void updateUnpartitionedTableFileMd() throws CatalogException {
    if (LOG.isTraceEnabled()) {
        LOG.trace("update unpartitioned table: " + name_);
    }
    resetPartitions();
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    addDefaultPartition(msTbl.getSd());
    HdfsPartition part = createPartition(msTbl.getSd(), null);
    addPartition(part);
    if (isMarkedCached_)
        part.markCached();
    refreshFileMetadata(part);
}
#end_block

#method_before
private void loadPartitionsFromMetastore(Set<String> partitionNames, IMetaStoreClient client) throws Exception {
    Preconditions.checkNotNull(partitionNames);
    if (partitionNames.isEmpty())
        return;
    // Load partition metadata from Hive Metastore.
    List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
    msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(partitionNames), db_.getName(), name_));
    for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
        HdfsPartition partition = createPartition(msPartition.getSd(), msPartition);
        addPartition(partition);
        // this table's partition list. Skip the partition.
        if (partition == null)
            continue;
        if (msPartition.getParameters() != null) {
            partition.setNumRows(getRowCount(msPartition.getParameters()));
        }
        if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
            // TODO: READ_ONLY isn't exactly correct because the it's possible the
            // partition does not have READ permissions either. When we start checking
            // whether we can READ from a table, this should be updated to set the
            // table's access level to the "lowest" effective level across all
            // partitions. That is, if one partition has READ_ONLY and another has
            // WRITE_ONLY the table's access level should be NONE.
            accessLevel_ = TAccessLevel.READ_ONLY;
        }
        loadMetadataAndDiskIds(partition);
    }
}
#method_after
private void loadPartitionsFromMetastore(Set<String> partitionNames, IMetaStoreClient client) throws Exception {
    Preconditions.checkNotNull(partitionNames);
    if (partitionNames.isEmpty())
        return;
    // Load partition metadata from Hive Metastore.
    List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
    msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(partitionNames), db_.getName(), name_));
    for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
        HdfsPartition partition = createPartition(msPartition.getSd(), msPartition);
        addPartition(partition);
        // this table's partition list. Skip the partition.
        if (partition == null)
            continue;
        if (msPartition.getParameters() != null) {
            partition.setNumRows(getRowCount(msPartition.getParameters()));
        }
        if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
            // TODO: READ_ONLY isn't exactly correct because the it's possible the
            // partition does not have READ permissions either. When we start checking
            // whether we can READ from a table, this should be updated to set the
            // table's access level to the "lowest" effective level across all
            // partitions. That is, if one partition has READ_ONLY and another has
            // WRITE_ONLY the table's access level should be NONE.
            accessLevel_ = TAccessLevel.READ_ONLY;
        }
        refreshFileMetadata(partition);
    }
}
#end_block

#method_before
private void loadPartitionFileMetadata(StorageDescriptor storageDescriptor, HdfsPartition partition) throws Exception {
    Preconditions.checkNotNull(storageDescriptor);
    Preconditions.checkNotNull(partition);
    org.apache.hadoop.hive.metastore.api.Partition msPart = partition.toHmsPartition();
    Path partDirPath = new Path(storageDescriptor.getLocation());
    FileSystem fs = partDirPath.getFileSystem(CONF);
    if (!fs.exists(partDirPath))
        return;
    numHdfsFiles_ -= partition.getNumFileDescriptors();
    totalHdfsBytes_ -= partition.getSize();
    Preconditions.checkState(numHdfsFiles_ >= 0 && totalHdfsBytes_ >= 0);
    loadMetadataAndDiskIds(partition);
}
#method_after
private void loadPartitionFileMetadata(StorageDescriptor storageDescriptor, HdfsPartition partition) throws Exception {
    Preconditions.checkNotNull(storageDescriptor);
    Preconditions.checkNotNull(partition);
    org.apache.hadoop.hive.metastore.api.Partition msPart = partition.toHmsPartition();
    Path partDirPath = new Path(storageDescriptor.getLocation());
    FileSystem fs = partDirPath.getFileSystem(CONF);
    if (!fs.exists(partDirPath))
        return;
    numHdfsFiles_ -= partition.getNumFileDescriptors();
    totalHdfsBytes_ -= partition.getSize();
    Preconditions.checkState(numHdfsFiles_ >= 0 && totalHdfsBytes_ >= 0);
    refreshFileMetadata(partition);
}
#end_block

#method_before
public ReferenceHashSet<Set<T>> getSets() {
    return uniqueSets_;
}
#method_after
public Set<Set<T>> getSets() {
    return uniqueSets_.keySet();
}
#end_block

#method_before
public Set<T> makeSet(T item) {
    if (itemSets_.containsKey(item)) {
        throw new IllegalStateException("Item set for item already exists: " + item.toString());
    }
    Set<T> s = Sets.newHashSet(item);
    itemSets_.put(item, s);
    uniqueSets_.add(s);
    return s;
}
#method_after
public Set<T> makeSet(T item) {
    if (itemSets_.containsKey(item)) {
        throw new IllegalStateException("Item set for item already exists: " + item.toString());
    }
    Set<T> s = Sets.newHashSet(item);
    itemSets_.put(item, s);
    uniqueSets_.put(s, DUMMY_VALUE);
    return s;
}
#end_block

#method_before
public boolean union(T a, T b) {
    Set<T> aItems = itemSets_.get(a);
    Set<T> bItems = itemSets_.get(b);
    // check if the sets are already identical
    if (aItems != null && bItems != null && aItems == bItems)
        return false;
    // union(x, x) is equivalent to makeSet(x)
    if (a.equals(b) && aItems == null) {
        makeSet(a);
        return true;
    }
    // create sets for a or b if not present already
    if (aItems == null)
        aItems = makeSet(a);
    if (bItems == null)
        bItems = makeSet(b);
    // will contain the union of aItems and bItems
    Set<T> mergedItems = aItems;
    // always the smaller of the two sets to be merged
    Set<T> updateItems = bItems;
    if (bItems.size() > aItems.size()) {
        mergedItems = bItems;
        updateItems = aItems;
    }
    for (T item : updateItems) {
        mergedItems.add(item);
        itemSets_.put(item, mergedItems);
    }
    boolean removed = uniqueSets_.remove(updateItems);
    Preconditions.checkState(removed);
    return true;
}
#method_after
public boolean union(T a, T b) {
    Set<T> aItems = itemSets_.get(a);
    Set<T> bItems = itemSets_.get(b);
    // check if the sets are already identical
    if (aItems != null && bItems != null && aItems == bItems)
        return false;
    // union(x, x) is equivalent to makeSet(x)
    if (a.equals(b) && aItems == null) {
        makeSet(a);
        return true;
    }
    // create sets for a or b if not present already
    if (aItems == null)
        aItems = makeSet(a);
    if (bItems == null)
        bItems = makeSet(b);
    // will contain the union of aItems and bItems
    Set<T> mergedItems = aItems;
    // always the smaller of the two sets to be merged
    Set<T> updateItems = bItems;
    if (bItems.size() > aItems.size()) {
        mergedItems = bItems;
        updateItems = aItems;
    }
    for (T item : updateItems) {
        mergedItems.add(item);
        itemSets_.put(item, mergedItems);
    }
    Object removedValue = uniqueSets_.remove(updateItems);
    Preconditions.checkState(removedValue == DUMMY_VALUE);
    return true;
}
#end_block

#method_before
public void checkConsistency() {
    // Validate map from item to item set.
    Set<Set<T>> validatedSets = Sets.newHashSet();
    for (Set<T> itemSet : itemSets_.values()) {
        // Avoid checking the same item set multiple times.
        if (validatedSets.contains(itemSet))
            continue;
        // the set itself.
        for (T item : itemSet) {
            if (itemSet != itemSets_.get(item)) {
                throw new IllegalStateException("DisjointSet is in an inconsistent state. Failed item set validation.");
            }
        }
        validatedSets.add(itemSet);
    }
    // Validate set of item sets. Every element should appear in exactly one item set.
    Set<T> seenItems = Sets.newHashSet();
    for (Set<T> itemSet : uniqueSets_) {
        for (T item : itemSet) {
            if (!seenItems.add(item)) {
                throw new IllegalStateException("DisjointSet is in an inconsistent state. Failed unique set validation.");
            }
        }
    }
}
#method_after
public void checkConsistency() {
    // Validate map from item to item set.
    Set<Set<T>> validatedSets = Sets.newHashSet();
    for (Set<T> itemSet : itemSets_.values()) {
        // Avoid checking the same item set multiple times.
        if (validatedSets.contains(itemSet))
            continue;
        // the set itself.
        for (T item : itemSet) {
            if (itemSet != itemSets_.get(item)) {
                throw new IllegalStateException("DisjointSet is in an inconsistent state. Failed item set validation.");
            }
        }
        validatedSets.add(itemSet);
    }
    // Validate set of item sets. Every element should appear in exactly one item set.
    Set<T> seenItems = Sets.newHashSet();
    for (Set<T> itemSet : uniqueSets_.keySet()) {
        for (T item : itemSet) {
            if (!seenItems.add(item)) {
                throw new IllegalStateException("DisjointSet is in an inconsistent state. Failed unique set validation.");
            }
        }
    }
}
#end_block

#method_before
@Override
protected Object encode(ChannelHandlerContext ctx, Channel chan, Object obj) throws Exception {
    if (!(obj instanceof RpcOutboundMessage)) {
        return obj;
    }
    RpcOutboundMessage msg = (RpcOutboundMessage) obj;
    if (LOG.isTraceEnabled()) {
        LOG.trace(chan + " Sending RPC " + msg);
    }
    // callers.
    return KuduRpc.toChannelBuffer(msg.getHeader(), msg.getBody());
}
#method_after
@Override
protected Object encode(ChannelHandlerContext ctx, Channel chan, Object obj) throws Exception {
    if (!(obj instanceof RpcOutboundMessage)) {
        return obj;
    }
    RpcOutboundMessage msg = (RpcOutboundMessage) obj;
    if (LOG.isTraceEnabled()) {
        LOG.trace("{}: sending RPC {}", chan, msg);
    }
    // callers.
    return KuduRpc.toChannelBuffer(msg.getHeader(), msg.getBody());
}
#end_block

#method_before
@Test
public void testMessageTooLong() {
    char[] chars = new char[Status.MAX_MESSAGE_LENGTH * 2];
    Arrays.fill(chars, 'a');
    Status s = Status.Corruption(new String(chars));
    assertEquals(Status.MAX_MESSAGE_LENGTH, s.getMessage().length());
}
#method_after
@Test
public void testMessageTooLong() {
    // Test string that will not get abbreviated.
    char[] chars = new char[Status.MAX_MESSAGE_LENGTH];
    Arrays.fill(chars, 'a');
    Status s = Status.Corruption(new String(chars));
    assertEquals(Status.MAX_MESSAGE_LENGTH, s.getMessage().length());
    assertEquals(s.getMessage().substring(Status.MAX_MESSAGE_LENGTH - Status.ABBREVIATION_CHARS_LENGTH), "aaa");
    // Test string just over the limit that will get abbreviated.
    chars = new char[Status.MAX_MESSAGE_LENGTH + 1];
    Arrays.fill(chars, 'a');
    s = Status.Corruption(new String(chars));
    assertEquals(Status.MAX_MESSAGE_LENGTH, s.getMessage().length());
    assertEquals(s.getMessage().substring(Status.MAX_MESSAGE_LENGTH - Status.ABBREVIATION_CHARS_LENGTH), Status.ABBREVIATION_CHARS);
    // Test string that's way too big that will get abbreviated.
    chars = new char[Status.MAX_MESSAGE_LENGTH * 2];
    Arrays.fill(chars, 'a');
    s = Status.Corruption(new String(chars));
    assertEquals(Status.MAX_MESSAGE_LENGTH, s.getMessage().length());
    assertEquals(s.getMessage().substring(Status.MAX_MESSAGE_LENGTH - Status.ABBREVIATION_CHARS_LENGTH), Status.ABBREVIATION_CHARS);
}
#end_block

#method_before
private void handleResponse(Channel chan, CallResponse callResponse) throws SaslException, SSLException {
    // TODO(todd): this needs to handle error responses, not just success responses.
    RpcHeader.NegotiatePB response = parseSaslMsgResponse(callResponse);
    // TODO: check that the message type m
    switch(state) {
        case AWAIT_NEGOTIATE:
            handleNegotiateResponse(chan, response);
            break;
        case AWAIT_SASL:
            handleSaslMessage(chan, response);
            break;
        case AWAIT_TLS_HANDSHAKE:
            handleTlsMessage(chan, response);
            break;
        default:
            throw new IllegalStateException("received a message in unexpected state: " + state.toString());
    }
}
#method_after
private void handleResponse(Channel chan, CallResponse callResponse) throws SaslException, SSLException {
    // TODO(todd): this needs to handle error responses, not just success responses.
    RpcHeader.NegotiatePB response = parseSaslMsgResponse(callResponse);
    // of the below implementations.
    switch(state) {
        case AWAIT_NEGOTIATE:
            handleNegotiateResponse(chan, response);
            break;
        case AWAIT_SASL:
            handleSaslMessage(chan, response);
            break;
        case AWAIT_TLS_HANDSHAKE:
            handleTlsMessage(chan, response);
            break;
        default:
            throw new IllegalStateException("received a message in unexpected state: " + state.toString());
    }
}
#end_block

#method_before
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException, SSLException {
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    // Gather the set of server-supported mechanisms.
    Set<String> serverMechs = Sets.newHashSet();
    for (RpcHeader.NegotiatePB.SaslMechanism mech : response.getSaslMechanismsList()) {
        serverMechs.add(mech.getMechanism());
    }
    // For each of our own mechanisms, in descending priority, check if
    // the server also supports them. If so, try to initialize saslClient.
    // If we find a common mechanism that also can be successfully initialized,
    // choose that mech.
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (String clientMech : PRIORITIZED_MECHS) {
        if (!serverMechs.contains(clientMech)) {
            errorsByMech.put(clientMech, "not advertised by server");
            continue;
        }
        try {
            Map<String, String> props = Maps.newHashMap();
            // to securely transmit the channel bindings.
            if ("GSSAPI".equals(clientMech)) {
                props.put(Sasl.QOP, "auth-int");
            }
            saslClient = Sasl.createSaslClient(new String[] { clientMech }, null, "kudu", remoteHostname, props, SASL_CALLBACK);
            chosenMech = clientMech;
            break;
        } catch (SaslException e) {
            errorsByMech.put(clientMech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenMech == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    // If we negotiated TLS, then we want to start the TLS handshake.
    if (serverFeatures.contains(RpcFeatureFlag.TLS)) {
        startTlsHandshake(chan);
    } else {
        sendSaslInitiate(chan);
    }
}
#method_after
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException, SSLException {
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    boolean willUseTls = serverFeatures.contains(RpcFeatureFlag.TLS);
    // Gather the set of server-supported mechanisms.
    Set<String> serverMechs = Sets.newHashSet();
    for (RpcHeader.NegotiatePB.SaslMechanism mech : response.getSaslMechanismsList()) {
        serverMechs.add(mech.getMechanism());
    }
    // For each of our own mechanisms, in descending priority, check if
    // the server also supports them. If so, try to initialize saslClient.
    // If we find a common mechanism that also can be successfully initialized,
    // choose that mech.
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (String clientMech : PRIORITIZED_MECHS) {
        if (!serverMechs.contains(clientMech)) {
            errorsByMech.put(clientMech, "not advertised by server");
            continue;
        }
        Map<String, String> props = Maps.newHashMap();
        // to securely transmit the channel bindings.
        if ("GSSAPI".equals(clientMech) && willUseTls) {
            props.put(Sasl.QOP, "auth-int");
        }
        try {
            saslClient = Sasl.createSaslClient(new String[] { clientMech }, null, "kudu", remoteHostname, props, SASL_CALLBACK);
            chosenMech = clientMech;
            break;
        } catch (SaslException e) {
            errorsByMech.put(clientMech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenMech == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    // If we negotiated TLS, then we want to start the TLS handshake.
    if (willUseTls) {
        startTlsHandshake(chan);
    } else {
        sendSaslInitiate(chan);
    }
}
#end_block

#method_before
private void handleTlsMessage(Channel chan, NegotiatePB response) throws SaslException {
    Preconditions.checkState(response.getStep() == NegotiateStep.TLS_HANDSHAKE);
    Preconditions.checkArgument(!response.getTlsHandshake().isEmpty(), "empty TLS message from server");
    // Pass the TLS message into our embedded SslHandler.
    sslEmbedder.offer(ChannelBuffers.copiedBuffer(response.getTlsHandshake().asReadOnlyByteBuffer()));
    if (sendPendingOutboundTls(chan)) {
        // Data was sent -- we must continue the handshake process.
        return;
    }
    // The handshake completed.
    // Insert the SSL handler into the pipeline so that all following traffic
    // gets encrypted, and then move on to the SASL portion of negotiation.
    // 
    // NOTE: this takes effect immediately (i.e. the following SASL initiation
    // sequence is encrypted).
    SslHandler handler = (SslHandler) sslEmbedder.getPipeline().getFirst();
    try {
        Certificate[] certs = handler.getEngine().getSession().getPeerCertificates();
        if (certs.length > 0) {
            peerCert = certs[0];
        }
    } catch (SSLPeerUnverifiedException e) {
        throw Throwables.propagate(e);
    }
    chan.getPipeline().addFirst("tls", handler);
    sendSaslInitiate(chan);
}
#method_after
private void handleTlsMessage(Channel chan, NegotiatePB response) throws SaslException {
    Preconditions.checkState(response.getStep() == NegotiateStep.TLS_HANDSHAKE);
    Preconditions.checkArgument(!response.getTlsHandshake().isEmpty(), "empty TLS message from server");
    // Pass the TLS message into our embedded SslHandler.
    sslEmbedder.offer(ChannelBuffers.copiedBuffer(response.getTlsHandshake().asReadOnlyByteBuffer()));
    if (sendPendingOutboundTls(chan)) {
        // Data was sent -- we must continue the handshake process.
        return;
    }
    // The handshake completed.
    // Insert the SSL handler into the pipeline so that all following traffic
    // gets encrypted, and then move on to the SASL portion of negotiation.
    // 
    // NOTE: this takes effect immediately (i.e. the following SASL initiation
    // sequence is encrypted).
    SslHandler handler = (SslHandler) sslEmbedder.getPipeline().getFirst();
    try {
        Certificate[] certs = handler.getEngine().getSession().getPeerCertificates();
        if (certs.length == 0) {
            throw new SSLPeerUnverifiedException("no peer cert found");
        }
    } catch (SSLPeerUnverifiedException e) {
        throw Throwables.propagate(e);
    }
    chan.getPipeline().addFirst("tls", handler);
    sendSaslInitiate(chan);
}
#end_block

#method_before
private void handleResponse(Channel chan, CallResponse callResponse) throws SaslException, SSLException {
    // TODO(todd): this needs to handle error responses, not just success responses.
    RpcHeader.NegotiatePB response = parseSaslMsgResponse(callResponse);
    // TODO: check that the message type m
    switch(state) {
        case AWAIT_NEGOTIATE:
            handleNegotiateResponse(chan, response);
            break;
        case AWAIT_SASL:
            handleSaslMessage(chan, response);
            break;
        case AWAIT_TLS_HANDSHAKE:
            handleTlsMessage(chan, response);
            break;
        default:
            throw new IllegalStateException("received a message in unexpected state: " + state.toString());
    }
}
#method_after
private void handleResponse(Channel chan, CallResponse callResponse) throws SaslException, SSLException {
    // TODO(todd): this needs to handle error responses, not just success responses.
    RpcHeader.NegotiatePB response = parseSaslMsgResponse(callResponse);
    // of the below implementations.
    switch(state) {
        case AWAIT_NEGOTIATE:
            handleNegotiateResponse(chan, response);
            break;
        case AWAIT_SASL:
            handleSaslMessage(chan, response);
            break;
        case AWAIT_TLS_HANDSHAKE:
            handleTlsMessage(chan, response);
            break;
        default:
            throw new IllegalStateException("received a message in unexpected state: " + state.toString());
    }
}
#end_block

#method_before
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException, SSLException {
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    // Gather the set of server-supported mechanisms.
    Set<String> serverMechs = Sets.newHashSet();
    for (RpcHeader.NegotiatePB.SaslMechanism mech : response.getSaslMechanismsList()) {
        serverMechs.add(mech.getMechanism());
    }
    // For each of our own mechanisms, in descending priority, check if
    // the server also supports them. If so, try to initialize saslClient.
    // If we find a common mechanism that also can be successfully initialized,
    // choose that mech.
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (String clientMech : PRIORITIZED_MECHS) {
        if (!serverMechs.contains(clientMech)) {
            errorsByMech.put(clientMech, "not advertised by server");
            continue;
        }
        try {
            saslClient = Sasl.createSaslClient(new String[] { clientMech }, null, "kudu", remoteHostname, SASL_PROPS, SASL_CALLBACK);
            chosenMech = clientMech;
            break;
        } catch (SaslException e) {
            errorsByMech.put(clientMech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenMech == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    // If we negotiated TLS, then we want to start the TLS handshake.
    if (serverFeatures.contains(RpcFeatureFlag.TLS)) {
        startTlsHandshake(chan);
    } else {
        sendSaslInitiate(chan);
    }
}
#method_after
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException, SSLException {
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    boolean willUseTls = serverFeatures.contains(RpcFeatureFlag.TLS);
    // Gather the set of server-supported mechanisms.
    Set<String> serverMechs = Sets.newHashSet();
    for (RpcHeader.NegotiatePB.SaslMechanism mech : response.getSaslMechanismsList()) {
        serverMechs.add(mech.getMechanism());
    }
    // For each of our own mechanisms, in descending priority, check if
    // the server also supports them. If so, try to initialize saslClient.
    // If we find a common mechanism that also can be successfully initialized,
    // choose that mech.
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (String clientMech : PRIORITIZED_MECHS) {
        if (!serverMechs.contains(clientMech)) {
            errorsByMech.put(clientMech, "not advertised by server");
            continue;
        }
        Map<String, String> props = Maps.newHashMap();
        // won't talk to us.
        if ("GSSAPI".equals(clientMech) && willUseTls) {
            props.put(Sasl.QOP, "auth-int");
        }
        try {
            saslClient = Sasl.createSaslClient(new String[] { clientMech }, null, "kudu", remoteHostname, props, SASL_CALLBACK);
            chosenMech = clientMech;
            break;
        } catch (SaslException e) {
            errorsByMech.put(clientMech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenMech == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    // If we negotiated TLS, then we want to start the TLS handshake.
    if (willUseTls) {
        startTlsHandshake(chan);
    } else {
        sendSaslInitiate(chan);
    }
}
#end_block

#method_before
private void startTlsHandshake(Channel chan) throws SSLException {
    SSLEngine engine = SecurityUtil.createSslEngine();
    // TODO(PKI): we allow the anonymous cipher suite for now, but
    // this needs to be more context-dependent. We also only use this
    // in the Java unit tests, and not in the C++ code. Should we enable
    // it in C++ rather than using self-signed certs?
    engine.setEnabledCipherSuites(ObjectArrays.concat(engine.getEnabledCipherSuites(), "TLS_DH_anon_WITH_AES_128_CBC_SHA"));
    engine.setUseClientMode(true);
    SslHandler handler = new SslHandler(engine);
    handler.setEnableRenegotiation(false);
    sslEmbedder = new DecoderEmbedder<>(handler);
    sslHandshakeFuture = handler.handshake();
    state = State.AWAIT_TLS_HANDSHAKE;
    boolean sent = sendPendingOutboundTls(chan);
    assert sent;
}
#method_after
private void startTlsHandshake(Channel chan) throws SSLException {
    SSLEngine engine = SecurityUtil.createSslEngine();
    // TODO(todd): remove usage of this anonymous cipher suite.
    // It's replaced in the next patch in this patch series by
    // a self-signed cert used for tests.
    engine.setEnabledCipherSuites(ObjectArrays.concat(engine.getEnabledCipherSuites(), "TLS_DH_anon_WITH_AES_128_CBC_SHA"));
    engine.setUseClientMode(true);
    SslHandler handler = new SslHandler(engine);
    handler.setEnableRenegotiation(false);
    sslEmbedder = new DecoderEmbedder<>(handler);
    sslHandshakeFuture = handler.handshake();
    state = State.AWAIT_TLS_HANDSHAKE;
    boolean sent = sendPendingOutboundTls(chan);
    assert sent;
}
#end_block

#method_before
@Test
public void testNegotiation() {
    negotiator.sendHello(embedder.getPipeline().getChannel());
    // Expect client->server: NEGOTIATE.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals(-33, msg.getHeader().getCallId());
    assertEquals(NegotiateStep.NEGOTIATE, ((NegotiatePB) msg.getBody()).getStep());
    // Respond with NEGOTIATE.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(-33).build(), NegotiatePB.newBuilder().addSaslMechanisms(NegotiatePB.SaslMechanism.newBuilder().setMechanism("PLAIN")).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect client->server: SASL_INITIATE (PLAIN)
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals(-33, msg.getHeader().getCallId());
    assertEquals(NegotiateStep.SASL_INITIATE, body.getStep());
    assertEquals(1, body.getSaslMechanismsCount());
    assertEquals("PLAIN", body.getSaslMechanisms(0).getMechanism());
    assertTrue(body.hasToken());
    // Respond with SASL_SUCCESS:
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(-33).build(), NegotiatePB.newBuilder().setStep(NegotiateStep.SASL_SUCCESS).build()));
    // Expect client->server: ConnectionContext
    msg = (RpcOutboundMessage) embedder.poll();
    ConnectionContextPB connCtx = (ConnectionContextPB) msg.getBody();
    assertEquals(-3, msg.getHeader().getCallId());
    assertEquals("java_client", connCtx.getDEPRECATEDUserInfo().getRealUser());
    // Expect the client to also emit a negotiation Result.
    Result result = (Result) embedder.poll();
    assertNotNull(result);
}
#method_after
@Test
public void testNegotiation() {
    negotiator.sendHello(embedder.getPipeline().getChannel());
    // Expect client->server: NEGOTIATE.
    RpcOutboundMessage msg = (RpcOutboundMessage) embedder.poll();
    NegotiatePB body = (NegotiatePB) msg.getBody();
    assertEquals(Negotiator.SASL_CALL_ID, msg.getHeader().getCallId());
    assertEquals(NegotiateStep.NEGOTIATE, ((NegotiatePB) msg.getBody()).getStep());
    // Respond with NEGOTIATE.
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().addSaslMechanisms(SaslMechanism.newBuilder().setMechanism("PLAIN")).setStep(NegotiateStep.NEGOTIATE).build()));
    // Expect client->server: SASL_INITIATE (PLAIN)
    msg = (RpcOutboundMessage) embedder.poll();
    body = (NegotiatePB) msg.getBody();
    assertEquals(Negotiator.SASL_CALL_ID, msg.getHeader().getCallId());
    assertEquals(NegotiateStep.SASL_INITIATE, body.getStep());
    assertEquals(1, body.getSaslMechanismsCount());
    assertEquals("PLAIN", body.getSaslMechanisms(0).getMechanism());
    assertTrue(body.hasToken());
    // Respond with SASL_SUCCESS:
    embedder.offer(fakeResponse(ResponseHeader.newBuilder().setCallId(Negotiator.SASL_CALL_ID).build(), NegotiatePB.newBuilder().setStep(NegotiateStep.SASL_SUCCESS).build()));
    // Expect client->server: ConnectionContext
    msg = (RpcOutboundMessage) embedder.poll();
    ConnectionContextPB connCtx = (ConnectionContextPB) msg.getBody();
    assertEquals(Negotiator.CONNECTION_CTX_CALL_ID, msg.getHeader().getCallId());
    assertEquals("java_client", connCtx.getDEPRECATEDUserInfo().getRealUser());
    // Expect the client to also emit a negotiation Result.
    Result result = (Result) embedder.poll();
    assertNotNull(result);
}
#end_block

#method_before
@Override
protected Object encode(ChannelHandlerContext ctx, Channel chan, Object obj) throws Exception {
    if (!(obj instanceof RpcOutboundMessage))
        return obj;
    RpcOutboundMessage msg = (RpcOutboundMessage) obj;
    // callers.
    return KuduRpc.toChannelBuffer(msg.getHeader(), msg.getBody());
}
#method_after
@Override
protected Object encode(ChannelHandlerContext ctx, Channel chan, Object obj) throws Exception {
    if (!(obj instanceof RpcOutboundMessage)) {
        return obj;
    }
    RpcOutboundMessage msg = (RpcOutboundMessage) obj;
    // callers.
    return KuduRpc.toChannelBuffer(msg.getHeader(), msg.getBody());
}
#end_block

#method_before
<R> void sendRpc(KuduRpc<R> rpc) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.SEND_TO_SERVER).serverInfo(serverInfo).build());
    if (!rpc.deadlineTracker.hasDeadline()) {
        LOG.warn(getPeerUuidLoggingString() + " sending an rpc without a timeout " + rpc);
    }
    Pair<ChannelBuffer, Integer> encodedRpcAndId = null;
    if (chan != null) {
        if (!rpc.getRequiredFeatures().isEmpty() && !negotiationResult.serverFeatures.contains(RpcHeader.RpcFeatureFlag.APPLICATION_FEATURE_FLAGS)) {
            Status statusNotSupported = Status.NotSupported("the server does not support the" + "APPLICATION_FEATURE_FLAGS RPC feature");
            rpc.errback(new NonRecoverableException(statusNotSupported));
        // TODO(todd): shouldn't this return here?
        }
        encodedRpcAndId = encode(rpc);
        if (encodedRpcAndId == null) {
            // Stop here.  RPC has been failed already.
            return;
        }
        // Volatile read.
        final Channel chan = this.chan;
        if (chan != null) {
            // Double check if we disconnected during encode().
            Channels.write(chan, encodedRpcAndId.getFirst());
            return;
        }
    }
    // True when we notice we are about to get connected to the TS.
    boolean tryAgain = false;
    // True when the connection was closed while encoding.
    boolean failRpc = false;
    synchronized (this) {
        // Check if we got connected while entering this synchronized block.
        if (chan != null) {
            tryAgain = true;
        } else if (dead) {
            // `encodedRpcAndId` is null iff `chan` is null.
            if (encodedRpcAndId == null || rpcsInflight.containsKey(encodedRpcAndId.getSecond())) {
                failRpc = true;
            }
        } else {
            if (pendingRpcs == null) {
                pendingRpcs = new ArrayList<>();
            }
            pendingRpcs.add(rpc);
        }
    }
    if (failRpc) {
        Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset");
        failOrRetryRpc(rpc, new RecoverableException(statusNetworkError));
    } else if (tryAgain) {
        // This recursion will not lead to a loop because we only get here if we
        // connected while entering the synchronized block above. So when trying
        // a second time,  we will either succeed to send the RPC if we're still
        // connected, or fail through to the code below if we got disconnected
        // in the mean time.
        sendRpc(rpc);
    }
}
#method_after
<R> void sendRpc(KuduRpc<R> rpc) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.SEND_TO_SERVER).serverInfo(serverInfo).build());
    if (!rpc.deadlineTracker.hasDeadline()) {
        LOG.warn(getPeerUuidLoggingString() + " sending an rpc without a timeout " + rpc);
    }
    Pair<ChannelBuffer, Integer> encodedRpcAndId = null;
    if (chan != null) {
        if (!rpc.getRequiredFeatures().isEmpty() && !negotiationResult.serverFeatures.contains(RpcHeader.RpcFeatureFlag.APPLICATION_FEATURE_FLAGS)) {
            Status statusNotSupported = Status.NotSupported("the server does not support the" + "APPLICATION_FEATURE_FLAGS RPC feature");
            rpc.errback(new NonRecoverableException(statusNotSupported));
        // TODO(todd): this should return here. We seem to lack test coverage!
        }
        encodedRpcAndId = encode(rpc);
        if (encodedRpcAndId == null) {
            // Stop here.  RPC has been failed already.
            return;
        }
        // Volatile read.
        final Channel chan = this.chan;
        if (chan != null) {
            // Double check if we disconnected during encode().
            Channels.write(chan, encodedRpcAndId.getFirst());
            return;
        }
    }
    // True when we notice we are about to get connected to the TS.
    boolean tryAgain = false;
    // True when the connection was closed while encoding.
    boolean failRpc = false;
    synchronized (this) {
        // Check if we got connected while entering this synchronized block.
        if (chan != null) {
            tryAgain = true;
        } else if (dead) {
            // `encodedRpcAndId` is null iff `chan` is null.
            if (encodedRpcAndId == null || rpcsInflight.containsKey(encodedRpcAndId.getSecond())) {
                failRpc = true;
            }
        } else {
            if (pendingRpcs == null) {
                pendingRpcs = new ArrayList<>();
            }
            pendingRpcs.add(rpc);
        }
    }
    if (failRpc) {
        Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset");
        failOrRetryRpc(rpc, new RecoverableException(statusNetworkError));
    } else if (tryAgain) {
        // This recursion will not lead to a loop because we only get here if we
        // connected while entering the synchronized block above. So when trying
        // a second time,  we will either succeed to send the RPC if we're still
        // connected, or fail through to the code below if we got disconnected
        // in the mean time.
        sendRpc(rpc);
    }
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt) throws Exception {
    Object m = evt.getMessage();
    if (m instanceof SecureRpcHelper.NegotiationResult) {
        this.negotiationResult = (NegotiationResult) m;
        this.chan = ctx.getChannel();
        sendQueuedRpcs();
        return;
    }
    if (!(m instanceof ChannelBuffer)) {
        ctx.sendUpstream(evt);
        return;
    }
    ChannelBuffer buf = (ChannelBuffer) m;
    final long start = System.nanoTime();
    final int rdx = buf.readerIndex();
    LOG.debug("------------------>> ENTERING DECODE >>------------------");
    CallResponse response = new CallResponse(buf);
    RpcHeader.ResponseHeader header = response.getHeader();
    if (!header.hasCallId()) {
        final int size = response.getTotalResponseSize();
        final String msg = getPeerUuidLoggingString() + "RPC response (size: " + size + ") doesn't" + " have a call ID: " + header;
        LOG.error(msg);
        Status statusIncomplete = Status.Incomplete(msg);
        throw new NonRecoverableException(statusIncomplete);
    }
    final int rpcid = header.getCallId();
    @SuppressWarnings("rawtypes")
    final KuduRpc rpc = rpcsInflight.get(rpcid);
    if (rpc == null) {
        final String msg = getPeerUuidLoggingString() + "Invalid rpcid: " + rpcid;
        LOG.error(msg);
        Status statusIllegalState = Status.IllegalState(msg);
        // all RPCs in flight to be failed.
        throw new NonRecoverableException(statusIllegalState);
    }
    // Start building the trace, we'll finish it as we parse the response.
    RpcTraceFrame.RpcTraceFrameBuilder traceBuilder = new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo);
    Pair<Object, Object> decoded = null;
    KuduException exception = null;
    Status retryableHeaderError = Status.OK();
    if (header.hasIsError() && header.getIsError()) {
        RpcHeader.ErrorStatusPB.Builder errorBuilder = RpcHeader.ErrorStatusPB.newBuilder();
        KuduRpc.readProtobuf(response.getPBMessage(), errorBuilder);
        RpcHeader.ErrorStatusPB error = errorBuilder.build();
        if (error.getCode().equals(RpcHeader.ErrorStatusPB.RpcErrorCodePB.ERROR_SERVER_TOO_BUSY)) {
            // We can't return right away, we still need to remove ourselves from 'rpcsInflight', so we
            // populate 'retryableHeaderError'.
            retryableHeaderError = Status.ServiceUnavailable(error.getMessage());
        } else {
            String message = getPeerUuidLoggingString() + "Tablet server sent error " + error.getMessage();
            Status status = Status.RemoteError(message);
            exception = new NonRecoverableException(status);
            // can be useful
            LOG.error(message);
        }
    } else {
        try {
            decoded = rpc.deserialize(response, this.serverInfo.getUuid());
        } catch (KuduException ex) {
            exception = ex;
        }
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug(getPeerUuidLoggingString() + "rpcid=" + rpcid + ", response size=" + (buf.readerIndex() - rdx) + " bytes" + ", rpc=" + rpc);
    }
    {
        final KuduRpc<?> removed = rpcsInflight.remove(rpcid);
        if (removed == null) {
            // The RPC we were decoding was cleaned up already, give up.
            Status statusIllegalState = Status.IllegalState("RPC not found");
            throw new NonRecoverableException(statusIllegalState);
        }
    }
    // This check is specifically for the ERROR_SERVER_TOO_BUSY case above.
    if (!retryableHeaderError.ok()) {
        rpc.addTrace(traceBuilder.callStatus(retryableHeaderError).build());
        kuduClient.handleRetryableError(rpc, new RecoverableException(retryableHeaderError));
        return;
    }
    // Have to do it for both TS and Master errors.
    if (decoded != null) {
        if (decoded.getSecond() instanceof Tserver.TabletServerErrorPB) {
            Tserver.TabletServerErrorPB error = (Tserver.TabletServerErrorPB) decoded.getSecond();
            exception = dispatchTSErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // It was taken care of.
                return;
            } else {
                // We're going to errback.
                decoded = null;
            }
        } else if (decoded.getSecond() instanceof Master.MasterErrorPB) {
            Master.MasterErrorPB error = (Master.MasterErrorPB) decoded.getSecond();
            exception = dispatchMasterErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // Exception was taken care of.
                return;
            } else {
                decoded = null;
            }
        }
    }
    try {
        if (decoded != null) {
            assert !(decoded.getFirst() instanceof Exception);
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), decoded.getFirst());
            }
            rpc.addTrace(traceBuilder.callStatus(Status.OK()).build());
            rpc.callback(decoded.getFirst());
        } else {
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), null);
            }
            rpc.addTrace(traceBuilder.callStatus(exception.getStatus()).build());
            rpc.errback(exception);
        }
    } catch (Exception e) {
        LOG.debug(getPeerUuidLoggingString() + "Unexpected exception while handling RPC #" + rpcid + ", rpc=" + rpc, e);
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug("------------------<< LEAVING  DECODE <<------------------" + " time elapsed: " + ((System.nanoTime() - start) / 1000) + "us");
    }
}
#method_after
@Override
@SuppressWarnings("unchecked")
public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt) throws Exception {
    Object m = evt.getMessage();
    if (m instanceof SecureRpcHelper.NegotiationResult) {
        this.negotiationResult = (NegotiationResult) m;
        this.chan = ctx.getChannel();
        sendQueuedRpcs();
        return;
    }
    if (!(m instanceof CallResponse)) {
        ctx.sendUpstream(evt);
        return;
    }
    CallResponse response = (CallResponse) m;
    final long start = System.nanoTime();
    RpcHeader.ResponseHeader header = response.getHeader();
    if (!header.hasCallId()) {
        final int size = response.getTotalResponseSize();
        final String msg = getPeerUuidLoggingString() + "RPC response (size: " + size + ") doesn't" + " have a call ID: " + header;
        LOG.error(msg);
        Status statusIncomplete = Status.Incomplete(msg);
        throw new NonRecoverableException(statusIncomplete);
    }
    final int rpcid = header.getCallId();
    @SuppressWarnings("rawtypes")
    final KuduRpc rpc = rpcsInflight.get(rpcid);
    if (rpc == null) {
        final String msg = getPeerUuidLoggingString() + "Invalid rpcid: " + rpcid;
        LOG.error(msg);
        Status statusIllegalState = Status.IllegalState(msg);
        // all RPCs in flight to be failed.
        throw new NonRecoverableException(statusIllegalState);
    }
    // Start building the trace, we'll finish it as we parse the response.
    RpcTraceFrame.RpcTraceFrameBuilder traceBuilder = new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo);
    Pair<Object, Object> decoded = null;
    KuduException exception = null;
    Status retryableHeaderError = Status.OK();
    if (header.hasIsError() && header.getIsError()) {
        RpcHeader.ErrorStatusPB.Builder errorBuilder = RpcHeader.ErrorStatusPB.newBuilder();
        KuduRpc.readProtobuf(response.getPBMessage(), errorBuilder);
        RpcHeader.ErrorStatusPB error = errorBuilder.build();
        if (error.getCode().equals(RpcHeader.ErrorStatusPB.RpcErrorCodePB.ERROR_SERVER_TOO_BUSY)) {
            // We can't return right away, we still need to remove ourselves from 'rpcsInflight', so we
            // populate 'retryableHeaderError'.
            retryableHeaderError = Status.ServiceUnavailable(error.getMessage());
        } else {
            String message = getPeerUuidLoggingString() + "Tablet server sent error " + error.getMessage();
            Status status = Status.RemoteError(message);
            exception = new NonRecoverableException(status);
            // can be useful
            LOG.error(message);
        }
    } else {
        try {
            decoded = rpc.deserialize(response, this.serverInfo.getUuid());
        } catch (KuduException ex) {
            exception = ex;
        }
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug(getPeerUuidLoggingString() + "rpcid=" + rpcid + ", response size=" + response.getTotalResponseSize() + ", rpc=" + rpc);
    }
    {
        final KuduRpc<?> removed = rpcsInflight.remove(rpcid);
        if (removed == null) {
            // The RPC we were decoding was cleaned up already, give up.
            Status statusIllegalState = Status.IllegalState("RPC not found");
            throw new NonRecoverableException(statusIllegalState);
        }
    }
    // This check is specifically for the ERROR_SERVER_TOO_BUSY case above.
    if (!retryableHeaderError.ok()) {
        rpc.addTrace(traceBuilder.callStatus(retryableHeaderError).build());
        kuduClient.handleRetryableError(rpc, new RecoverableException(retryableHeaderError));
        return;
    }
    // Have to do it for both TS and Master errors.
    if (decoded != null) {
        if (decoded.getSecond() instanceof Tserver.TabletServerErrorPB) {
            Tserver.TabletServerErrorPB error = (Tserver.TabletServerErrorPB) decoded.getSecond();
            exception = dispatchTSErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // It was taken care of.
                return;
            } else {
                // We're going to errback.
                decoded = null;
            }
        } else if (decoded.getSecond() instanceof Master.MasterErrorPB) {
            Master.MasterErrorPB error = (Master.MasterErrorPB) decoded.getSecond();
            exception = dispatchMasterErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // Exception was taken care of.
                return;
            } else {
                decoded = null;
            }
        }
    }
    try {
        if (decoded != null) {
            assert !(decoded.getFirst() instanceof Exception);
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), decoded.getFirst());
            }
            rpc.addTrace(traceBuilder.callStatus(Status.OK()).build());
            rpc.callback(decoded.getFirst());
        } else {
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), null);
            }
            rpc.addTrace(traceBuilder.callStatus(exception.getStatus()).build());
            rpc.errback(exception);
        }
    } catch (Exception e) {
        LOG.debug(getPeerUuidLoggingString() + "Unexpected exception while handling RPC #" + rpcid + ", rpc=" + rpc, e);
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug("------------------<< LEAVING  DECODE <<------------------" + " time elapsed: " + ((System.nanoTime() - start) / 1000) + "us");
    }
}
#end_block

#method_before
@Override
public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt) throws Exception {
    Object m = evt.getMessage();
    if (!(m instanceof ChannelBuffer)) {
        ctx.sendUpstream(evt);
        return;
    }
    handleResponse(ctx.getChannel(), (ChannelBuffer) m);
}
#method_after
@Override
public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt) throws Exception {
    Object m = evt.getMessage();
    if (!(m instanceof CallResponse)) {
        ctx.sendUpstream(evt);
        return;
    }
    handleResponse(ctx.getChannel(), (CallResponse) m);
}
#end_block

#method_before
private void handleResponse(Channel chan, ChannelBuffer buf) throws SaslException {
    Preconditions.checkState(!finished, "received a response after negotiation was complete");
    RpcHeader.NegotiatePB response = parseSaslMsgResponse(buf);
    switch(response.getStep()) {
        case NEGOTIATE:
            handleNegotiateResponse(chan, response);
            break;
        case SASL_CHALLENGE:
            handleChallengeResponse(chan, response);
            break;
        case SASL_SUCCESS:
            handleSuccessResponse(chan);
            break;
        default:
            LOG.error(String.format("Wrong negotiation step: %s", response.getStep()));
    }
}
#method_after
private void handleResponse(Channel chan, CallResponse callResponse) throws SaslException {
    // TODO(todd): this needs to handle error responses, not just success responses.
    Preconditions.checkState(!finished, "received a response after negotiation was complete");
    RpcHeader.NegotiatePB response = parseSaslMsgResponse(callResponse);
    switch(response.getStep()) {
        case NEGOTIATE:
            handleNegotiateResponse(chan, response);
            break;
        case SASL_CHALLENGE:
            handleChallengeResponse(chan, response);
            break;
        case SASL_SUCCESS:
            handleSuccessResponse(chan);
            break;
        default:
            LOG.error(String.format("Wrong negotiation step: %s", response.getStep()));
    }
}
#end_block

#method_before
private RpcHeader.NegotiatePB parseSaslMsgResponse(ChannelBuffer buf) {
    CallResponse response = new CallResponse(buf);
    RpcHeader.ResponseHeader responseHeader = response.getHeader();
    int id = responseHeader.getCallId();
    if (id != SASL_CALL_ID) {
        throw new IllegalStateException("Received a call that wasn't for SASL");
    }
    RpcHeader.NegotiatePB.Builder saslBuilder = RpcHeader.NegotiatePB.newBuilder();
    KuduRpc.readProtobuf(response.getPBMessage(), saslBuilder);
    return saslBuilder.build();
}
#method_after
private RpcHeader.NegotiatePB parseSaslMsgResponse(CallResponse response) {
    RpcHeader.ResponseHeader responseHeader = response.getHeader();
    int id = responseHeader.getCallId();
    if (id != SASL_CALL_ID) {
        throw new IllegalStateException("Received a call that wasn't for SASL");
    }
    RpcHeader.NegotiatePB.Builder saslBuilder = RpcHeader.NegotiatePB.newBuilder();
    KuduRpc.readProtobuf(response.getPBMessage(), saslBuilder);
    return saslBuilder.build();
}
#end_block

#method_before
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException {
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    byte[] initialResponse = null;
    RpcHeader.NegotiatePB.SaslAuth chosenAuth = null;
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (RpcHeader.NegotiatePB.SaslAuth auth : response.getAuthsList()) {
        // TODO(todd): should we skip trying to initialize GSSAPI if we see we have no krb5
        // credentials in the subject?
        String mech = auth.getMechanism();
        try {
            saslClient = Sasl.createSaslClient(new String[] { mech }, null, "kudu", remoteHostname, SASL_PROPS, SASL_CALLBACK);
            if (saslClient.hasInitialResponse()) {
                initialResponse = evaluateChallenge(new byte[0]);
            }
            chosenAuth = auth;
            break;
        } catch (SaslException e) {
            errorsByMech.put(mech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenAuth == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    RpcHeader.NegotiatePB.Builder builder = RpcHeader.NegotiatePB.newBuilder();
    if (initialResponse != null) {
        builder.setToken(ZeroCopyLiteralByteString.wrap(initialResponse));
    }
    builder.setStep(RpcHeader.NegotiatePB.NegotiateStep.SASL_INITIATE);
    builder.addAuths(chosenAuth);
    sendSaslMessage(chan, builder.build());
}
#method_after
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException {
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    // Gather the set of server-supported mechanisms.
    Set<String> serverMechs = Sets.newHashSet();
    for (RpcHeader.NegotiatePB.SaslMechanism mech : response.getSaslMechanismsList()) {
        serverMechs.add(mech.getMechanism());
    }
    // For each of our own mechanisms, in descending priority, check if
    // the server also supports them. If so, try to initialize saslClient.
    // If we find a common mechanism that also can be successfully initialized,
    // choose that mech.
    byte[] initialResponse = null;
    String chosenMech = null;
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (String clientMech : PRIORITIZED_MECHS) {
        if (!serverMechs.contains(clientMech)) {
            errorsByMech.put(clientMech, "not advertised by server");
            continue;
        }
        try {
            saslClient = Sasl.createSaslClient(new String[] { clientMech }, null, "kudu", remoteHostname, SASL_PROPS, SASL_CALLBACK);
            if (saslClient.hasInitialResponse()) {
                initialResponse = evaluateChallenge(new byte[0]);
            }
            chosenMech = clientMech;
            break;
        } catch (SaslException e) {
            errorsByMech.put(clientMech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenMech == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    RpcHeader.NegotiatePB.Builder builder = RpcHeader.NegotiatePB.newBuilder();
    if (initialResponse != null) {
        builder.setToken(ZeroCopyLiteralByteString.wrap(initialResponse));
    }
    builder.setStep(RpcHeader.NegotiatePB.NegotiateStep.SASL_INITIATE);
    builder.addSaslMechanismsBuilder().setMechanism(chosenMech);
    sendSaslMessage(chan, builder.build());
}
#end_block

#method_before
private byte[] evaluateChallenge(final byte[] challenge) throws SaslException {
    try {
        return Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {

            @Override
            public byte[] run() throws Exception {
                return saslClient.evaluateChallenge(challenge);
            }
        });
    } catch (Exception e) {
        if (e instanceof SaslException) {
            throw (SaslException) e;
        }
        throw new RuntimeException(e);
    }
}
#method_after
private byte[] evaluateChallenge(final byte[] challenge) throws SaslException {
    try {
        return Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {

            @Override
            public byte[] run() throws Exception {
                return saslClient.evaluateChallenge(challenge);
            }
        });
    } catch (Exception e) {
        Throwables.propagateIfInstanceOf(e, SaslException.class);
        throw Throwables.propagate(e);
    }
}
#end_block

#method_before
public ChannelBuffer handleResponse(ChannelBuffer buf, Channel chan) throws SaslException {
    if (saslClient == null || !saslClient.isComplete() || negoUnderway) {
        RpcHeader.NegotiatePB response = parseSaslMsgResponse(buf);
        switch(response.getStep()) {
            case NEGOTIATE:
                handleNegotiateResponse(chan, response);
                break;
            case SASL_CHALLENGE:
                handleChallengeResponse(chan, response);
                break;
            case SASL_SUCCESS:
                handleSuccessResponse(chan);
                break;
            default:
                LOG.error(String.format("Wrong negotiation step: %s", response.getStep()));
        }
        return null;
    }
    return buf;
}
#method_after
public ChannelBuffer handleResponse(ChannelBuffer buf, Channel chan) throws SaslException {
    if (negoUnderway) {
        RpcHeader.NegotiatePB response = parseSaslMsgResponse(buf);
        switch(response.getStep()) {
            case NEGOTIATE:
                handleNegotiateResponse(chan, response);
                break;
            case SASL_CHALLENGE:
                handleChallengeResponse(chan, response);
                break;
            case SASL_SUCCESS:
                handleSuccessResponse(chan);
                break;
            default:
                LOG.error(String.format("Wrong negotiation step: %s", response.getStep()));
        }
        return null;
    }
    return buf;
}
#end_block

#method_before
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException {
    Preconditions.checkNotNull(chan);
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    byte[] initialResponse = null;
    RpcHeader.NegotiatePB.SaslAuth chosenAuth = null;
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (RpcHeader.NegotiatePB.SaslAuth auth : response.getAuthsList()) {
        // TODO(todd): should we skip trying to initialize GSSAPI if we see we have no krb5
        // credentials in the subject?
        String mech = auth.getMechanism();
        try {
            saslClient = Sasl.createSaslClient(new String[] { mech }, null, "kudu", client.getServerInfo().getHostname(), SASL_PROPS, SASL_CALLBACK);
            if (saslClient.hasInitialResponse()) {
                initialResponse = evaluateChallenge(new byte[0]);
            }
            chosenAuth = auth;
            break;
        } catch (SaslException e) {
            errorsByMech.put(mech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenAuth == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    RpcHeader.NegotiatePB.Builder builder = RpcHeader.NegotiatePB.newBuilder();
    if (initialResponse != null) {
        builder.setToken(ZeroCopyLiteralByteString.wrap(initialResponse));
    }
    builder.setStep(RpcHeader.NegotiatePB.NegotiateStep.SASL_INITIATE);
    builder.addAuths(chosenAuth);
    sendSaslMessage(chan, builder.build());
}
#method_after
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException {
    Preconditions.checkNotNull(chan);
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    // Gather the set of server-supported mechanisms.
    Set<String> serverMechs = Sets.newHashSet();
    for (RpcHeader.NegotiatePB.SaslMechanism mech : response.getSaslMechanismsList()) {
        serverMechs.add(mech.getMechanism());
    }
    // For each of our own mechanisms, in descending priority, check if
    // the server also supports them. If so, try to initialize saslClient.
    // If we find a common mechanism that also can be successfully initialized,
    // choose that mech.
    byte[] initialResponse = null;
    String chosenMech = null;
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (String clientMech : PRIORITIZED_MECHS) {
        if (!serverMechs.contains(clientMech)) {
            errorsByMech.put(clientMech, "not advertised by server");
            continue;
        }
        try {
            saslClient = Sasl.createSaslClient(new String[] { clientMech }, null, "kudu", client.getServerInfo().getHostname(), SASL_PROPS, SASL_CALLBACK);
            if (saslClient.hasInitialResponse()) {
                initialResponse = evaluateChallenge(new byte[0]);
            }
            chosenMech = clientMech;
            break;
        } catch (SaslException e) {
            errorsByMech.put(clientMech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenMech == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    RpcHeader.NegotiatePB.Builder builder = RpcHeader.NegotiatePB.newBuilder();
    if (initialResponse != null) {
        builder.setToken(ZeroCopyLiteralByteString.wrap(initialResponse));
    }
    builder.setStep(RpcHeader.NegotiatePB.NegotiateStep.SASL_INITIATE);
    builder.addSaslMechanismsBuilder().setMechanism(chosenMech);
    sendSaslMessage(chan, builder.build());
}
#end_block

#method_before
private byte[] evaluateChallenge(final byte[] challenge) throws SaslException {
    final Subject subject = client.getSubject();
    try {
        return Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {

            @Override
            public byte[] run() throws Exception {
                return saslClient.evaluateChallenge(challenge);
            }
        });
    } catch (Exception e) {
        if (e instanceof SaslException) {
            throw (SaslException) e;
        }
        throw new RuntimeException(e);
    }
}
#method_after
private byte[] evaluateChallenge(final byte[] challenge) throws SaslException {
    final Subject subject = client.getSubject();
    try {
        return Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {

            @Override
            public byte[] run() throws Exception {
                return saslClient.evaluateChallenge(challenge);
            }
        });
    } catch (Exception e) {
        Throwables.propagateIfInstanceOf(e, SaslException.class);
        throw Throwables.propagate(e);
    }
}
#end_block

#method_before
<R> void sendRpc(KuduRpc<R> rpc) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.SEND_TO_SERVER).serverInfo(serverInfo).build());
    if (!rpc.deadlineTracker.hasDeadline()) {
        LOG.warn(getPeerUuidLoggingString() + " sending an rpc without a timeout " + rpc);
    }
    Pair<ChannelBuffer, Integer> encodedRpcAndId = null;
    if (chan != null) {
        if (!rpc.getRequiredFeatures().isEmpty() && !secureRpcHelper.getServerFeatures().contains(RpcHeader.RpcFeatureFlag.APPLICATION_FEATURE_FLAGS)) {
            Status statusNotSupported = Status.NotSupported("the server does not support the" + "APPLICATION_FEATURE_FLAGS RPC feature");
            rpc.errback(new NonRecoverableException(statusNotSupported));
        // TODO(todd): shouldn't this return here?
        }
        encodedRpcAndId = encode(rpc);
        if (encodedRpcAndId == null) {
            // Stop here.  RPC has been failed already.
            return;
        }
        // Volatile read.
        final Channel chan = this.chan;
        if (chan != null) {
            // Double check if we disconnected during encode().
            Channels.write(chan, encodedRpcAndId.getFirst());
            return;
        }
    }
    // True when we notice we are about to get connected to the TS.
    boolean tryAgain = false;
    // True when the connection was closed while encoding.
    boolean failRpc = false;
    synchronized (this) {
        // Check if we got connected while entering this synchronized block.
        if (chan != null) {
            tryAgain = true;
        } else if (dead) {
            // `encodedRpcAndId` is null iff `chan` is null.
            if (encodedRpcAndId == null || rpcsInflight.containsKey(encodedRpcAndId.getSecond())) {
                failRpc = true;
            }
        } else {
            if (pendingRpcs == null) {
                pendingRpcs = new ArrayList<>();
            }
            pendingRpcs.add(rpc);
        }
    }
    if (failRpc) {
        Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset");
        failOrRetryRpc(rpc, new RecoverableException(statusNetworkError));
    } else if (tryAgain) {
        // This recursion will not lead to a loop because we only get here if we
        // connected while entering the synchronized block above. So when trying
        // a second time,  we will either succeed to send the RPC if we're still
        // connected, or fail through to the code below if we got disconnected
        // in the mean time.
        sendRpc(rpc);
    }
}
#method_after
<R> void sendRpc(KuduRpc<R> rpc) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.SEND_TO_SERVER).serverInfo(serverInfo).build());
    if (!rpc.deadlineTracker.hasDeadline()) {
        LOG.warn(getPeerUuidLoggingString() + " sending an rpc without a timeout " + rpc);
    }
    Pair<ChannelBuffer, Integer> encodedRpcAndId = null;
    if (chan != null) {
        if (!rpc.getRequiredFeatures().isEmpty() && !secureRpcHelper.getServerFeatures().contains(RpcHeader.RpcFeatureFlag.APPLICATION_FEATURE_FLAGS)) {
            Status statusNotSupported = Status.NotSupported("the server does not support the" + "APPLICATION_FEATURE_FLAGS RPC feature");
            rpc.errback(new NonRecoverableException(statusNotSupported));
        // TODO(todd): this should return here. We seem to lack test coverage!
        }
        encodedRpcAndId = encode(rpc);
        if (encodedRpcAndId == null) {
            // Stop here.  RPC has been failed already.
            return;
        }
        // Volatile read.
        final Channel chan = this.chan;
        if (chan != null) {
            // Double check if we disconnected during encode().
            Channels.write(chan, encodedRpcAndId.getFirst());
            return;
        }
    }
    // True when we notice we are about to get connected to the TS.
    boolean tryAgain = false;
    // True when the connection was closed while encoding.
    boolean failRpc = false;
    synchronized (this) {
        // Check if we got connected while entering this synchronized block.
        if (chan != null) {
            tryAgain = true;
        } else if (dead) {
            // `encodedRpcAndId` is null iff `chan` is null.
            if (encodedRpcAndId == null || rpcsInflight.containsKey(encodedRpcAndId.getSecond())) {
                failRpc = true;
            }
        } else {
            if (pendingRpcs == null) {
                pendingRpcs = new ArrayList<>();
            }
            pendingRpcs.add(rpc);
        }
    }
    if (failRpc) {
        Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset");
        failOrRetryRpc(rpc, new RecoverableException(statusNetworkError));
    } else if (tryAgain) {
        // This recursion will not lead to a loop because we only get here if we
        // connected while entering the synchronized block above. So when trying
        // a second time,  we will either succeed to send the RPC if we're still
        // connected, or fail through to the code below if we got disconnected
        // in the mean time.
        sendRpc(rpc);
    }
}
#end_block

#method_before
public ChannelBuffer handleResponse(ChannelBuffer buf, Channel chan) throws SaslException {
    if (saslClient == null || !saslClient.isComplete() || negoUnderway) {
        RpcHeader.NegotiatePB response = parseSaslMsgResponse(buf);
        switch(response.getStep()) {
            case NEGOTIATE:
                handleNegotiateResponse(chan, response);
                break;
            case SASL_CHALLENGE:
                handleChallengeResponse(chan, response);
                break;
            case SASL_SUCCESS:
                handleSuccessResponse(chan);
                break;
            default:
                LOG.error(String.format("Wrong negotiation step: %s", response.getStep()));
        }
        return null;
    }
    return buf;
}
#method_after
public ChannelBuffer handleResponse(ChannelBuffer buf, Channel chan) throws SaslException {
    if (negoUnderway) {
        RpcHeader.NegotiatePB response = parseSaslMsgResponse(buf);
        switch(response.getStep()) {
            case NEGOTIATE:
                handleNegotiateResponse(chan, response);
                break;
            case SASL_CHALLENGE:
                handleChallengeResponse(chan, response);
                break;
            case SASL_SUCCESS:
                handleSuccessResponse(chan);
                break;
            default:
                LOG.error(String.format("Wrong negotiation step: %s", response.getStep()));
        }
        return null;
    }
    return buf;
}
#end_block

#method_before
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException {
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    byte[] initialResponse = null;
    RpcHeader.NegotiatePB.SaslAuth chosenAuth = null;
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (RpcHeader.NegotiatePB.SaslAuth auth : response.getAuthsList()) {
        // TODO(todd): should we skip trying to initialize GSSAPI if we see we have no krb5
        // credentials in the subject?
        String mech = auth.getMechanism();
        try {
            saslClient = Sasl.createSaslClient(new String[] { mech }, null, "kudu", client.getServerInfo().getHostname(), SASL_PROPS, SASL_CALLBACK);
            if (saslClient.hasInitialResponse()) {
                initialResponse = evaluateChallenge(new byte[0]);
            }
            chosenAuth = auth;
            break;
        } catch (SaslException e) {
            errorsByMech.put(mech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenAuth == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    RpcHeader.NegotiatePB.Builder builder = RpcHeader.NegotiatePB.newBuilder();
    if (initialResponse != null) {
        builder.setToken(ZeroCopyLiteralByteString.wrap(initialResponse));
    }
    builder.setStep(RpcHeader.NegotiatePB.NegotiateStep.SASL_INITIATE);
    builder.addAuths(chosenAuth);
    sendSaslMessage(chan, builder.build());
}
#method_after
private void handleNegotiateResponse(Channel chan, RpcHeader.NegotiatePB response) throws SaslException {
    // Store the supported features advertised by the server.
    ImmutableSet.Builder<RpcHeader.RpcFeatureFlag> features = ImmutableSet.builder();
    for (RpcHeader.RpcFeatureFlag feature : response.getSupportedFeaturesList()) {
        if (SUPPORTED_RPC_FEATURES.contains(feature)) {
            features.add(feature);
        }
    }
    serverFeatures = features.build();
    // Gather the set of server-supported mechanisms.
    Set<String> serverMechs = Sets.newHashSet();
    for (RpcHeader.NegotiatePB.SaslMechanism mech : response.getSaslMechanismsList()) {
        serverMechs.add(mech.getMechanism());
    }
    // For each of our own mechanisms, in descending priority, check if
    // the server also supports them. If so, try to initialize saslClient.
    // If we find a common mechanism that also can be successfully initialized,
    // choose that mech.
    byte[] initialResponse = null;
    String chosenMech = null;
    Map<String, String> errorsByMech = Maps.newHashMap();
    for (String clientMech : PRIORITIZED_MECHS) {
        if (!serverMechs.contains(clientMech)) {
            errorsByMech.put(clientMech, "not advertised by server");
            continue;
        }
        try {
            saslClient = Sasl.createSaslClient(new String[] { clientMech }, null, "kudu", client.getServerInfo().getHostname(), SASL_PROPS, SASL_CALLBACK);
            if (saslClient.hasInitialResponse()) {
                initialResponse = evaluateChallenge(new byte[0]);
            }
            chosenMech = clientMech;
            break;
        } catch (SaslException e) {
            errorsByMech.put(clientMech, e.getMessage());
            saslClient = null;
        }
    }
    if (chosenMech == null) {
        throw new SaslException("unable to negotiate a matching mechanism. Errors: [" + Joiner.on(",").withKeyValueSeparator(": ").join(errorsByMech) + "]");
    }
    RpcHeader.NegotiatePB.Builder builder = RpcHeader.NegotiatePB.newBuilder();
    if (initialResponse != null) {
        builder.setToken(ZeroCopyLiteralByteString.wrap(initialResponse));
    }
    builder.setStep(RpcHeader.NegotiatePB.NegotiateStep.SASL_INITIATE);
    builder.addSaslMechanismsBuilder().setMechanism(chosenMech);
    sendSaslMessage(chan, builder.build());
}
#end_block

#method_before
private byte[] evaluateChallenge(final byte[] challenge) throws SaslException {
    final Subject subject = client.getSubject();
    try {
        return Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {

            @Override
            public byte[] run() throws Exception {
                return saslClient.evaluateChallenge(challenge);
            }
        });
    } catch (Exception e) {
        if (e instanceof SaslException) {
            throw (SaslException) e;
        }
        throw new RuntimeException(e);
    }
}
#method_after
private byte[] evaluateChallenge(final byte[] challenge) throws SaslException {
    final Subject subject = client.getSubject();
    try {
        return Subject.doAs(subject, new PrivilegedExceptionAction<byte[]>() {

            @Override
            public byte[] run() throws Exception {
                return saslClient.evaluateChallenge(challenge);
            }
        });
    } catch (Exception e) {
        Throwables.propagateIfInstanceOf(e, SaslException.class);
        throw Throwables.propagate(e);
    }
}
#end_block

#method_before
public static PartitionPruner create(AbstractKuduScannerBuilder<?, ?> scanner) {
    Schema schema = scanner.table.getSchema();
    PartitionSchema partitionSchema = scanner.table.getPartitionSchema();
    PartitionSchema.RangeSchema rangeSchema = partitionSchema.getRangeSchema();
    Map<String, KuduPredicate> predicates = scanner.predicates;
    // bound PK.
    if (scanner.upperBoundPrimaryKey.length > 0 && Bytes.memcmp(scanner.lowerBoundPrimaryKey, scanner.upperBoundPrimaryKey) >= 0) {
        return PartitionPruner.empty();
    }
    for (KuduPredicate predicate : predicates.values()) {
        if (predicate.getType() == KuduPredicate.PredicateType.NONE) {
            return PartitionPruner.empty();
        }
    }
    // Build a set of partition key ranges which cover the tablets necessary for
    // the scan.
    // 
    // Example predicate sets and resulting partition key ranges, based on the
    // following tablet schema:
    // 
    // CREATE TABLE t (a INT32, b INT32, c INT32) PRIMARY KEY (a, b, c)
    // DISTRIBUTE BY RANGE (c)
    // HASH (a) INTO 2 BUCKETS
    // HASH (b) INTO 3 BUCKETS;
    // 
    // Assume that hash(0) = 0 and hash(2) = 2.
    // 
    // | Predicates | Partition Key Ranges                                   |
    // +------------+--------------------------------------------------------+
    // | a = 0      | [(bucket=0, bucket=2, c=0), (bucket=0, bucket=2, c=1)) |
    // | b = 2      |                                                        |
    // | c = 0      |                                                        |
    // +------------+--------------------------------------------------------+
    // | a = 0      | [(bucket=0, bucket=2), (bucket=0, bucket=3))           |
    // | b = 2      |                                                        |
    // +------------+--------------------------------------------------------+
    // | a = 0      | [(bucket=0, bucket=0, c=0), (bucket=0, bucket=0, c=1)) |
    // | c = 0      | [(bucket=0, bucket=1, c=0), (bucket=0, bucket=1, c=1)) |
    // |            | [(bucket=0, bucket=2, c=0), (bucket=0, bucket=2, c=1)) |
    // +------------+--------------------------------------------------------+
    // | b = 2      | [(bucket=0, bucket=2, c=0), (bucket=0, bucket=2, c=1)) |
    // | c = 0      | [(bucket=1, bucket=2, c=0), (bucket=1, bucket=2, c=1)) |
    // +------------+--------------------------------------------------------+
    // | a = 0      | [(bucket=0), (bucket=1))                               |
    // +------------+--------------------------------------------------------+
    // | b = 2      | [(bucket=0, bucket=2), (bucket=0, bucket=3))           |
    // |            | [(bucket=1, bucket=2), (bucket=1, bucket=3))           |
    // +------------+--------------------------------------------------------+
    // | c = 0      | [(bucket=0, bucket=0, c=0), (bucket=0, bucket=0, c=1)) |
    // |            | [(bucket=0, bucket=1, c=0), (bucket=0, bucket=1, c=1)) |
    // |            | [(bucket=0, bucket=2, c=0), (bucket=0, bucket=2, c=1)) |
    // |            | [(bucket=1, bucket=0, c=0), (bucket=1, bucket=0, c=1)) |
    // |            | [(bucket=1, bucket=1, c=0), (bucket=1, bucket=1, c=1)) |
    // |            | [(bucket=1, bucket=2, c=0), (bucket=1, bucket=2, c=1)) |
    // +------------+--------------------------------------------------------+
    // | None       | [(), ())                                               |
    // 
    // If the partition key is considered as a sequence of the hash bucket
    // components and a range component, then a few patterns emerge from the
    // examples above:
    // 
    // 1) The partition keys are truncated after the final constrained component
    // (hash bucket components are constrained when the scan is limited to some
    // bucket via equality or in-list predicates on that component, while range
    // components are constrained if they have an upper or lower bound via
    // range or equality predicates on that component).
    // 
    // 2) If the final constrained component is a hash bucket, then the
    // corresponding bucket in the upper bound is incremented in order to make
    // it an exclusive key.
    // 
    // 3) The number of partition key ranges in the result is equal to the product
    // of the number of buckets of each unconstrained hash component which come
    // before a final constrained component. If there are no unconstrained hash
    // components and no in-list predicates, then the number of
    // partition key ranges is one.
    // Step 1: Build the range portion of the partition key. If the range partition
    // columns match the primary key columns, then we can substitute the primary
    // key bounds, if they are tighter.
    byte[] rangeLowerBound = pushPredsIntoLowerBoundRangeKey(schema, rangeSchema, predicates);
    byte[] rangeUpperBound = pushPredsIntoUpperBoundRangeKey(schema, rangeSchema, predicates);
    if (partitionSchema.isSimpleRangePartitioning()) {
        if (Bytes.memcmp(rangeLowerBound, scanner.lowerBoundPrimaryKey) < 0) {
            rangeLowerBound = scanner.lowerBoundPrimaryKey;
        }
        if (scanner.upperBoundPrimaryKey.length > 0 && (rangeUpperBound.length == 0 || Bytes.memcmp(rangeUpperBound, scanner.upperBoundPrimaryKey) > 0)) {
            rangeUpperBound = scanner.upperBoundPrimaryKey;
        }
    }
    // Step 2: Create the hash bucket portion of the partition key.
    // The list of hash buckets bitset per hash component
    List<List<Boolean>> hashBucketBitSet = new ArrayList<>(partitionSchema.getHashBucketSchemas().size());
    for (PartitionSchema.HashBucketSchema hashSchema : partitionSchema.getHashBucketSchemas()) {
        hashBucketBitSet.add(pruneHashComponent(schema, hashSchema, predicates));
    }
    // The index of the final constrained component in the partition key.
    int constrainedIndex = 0;
    if (rangeLowerBound.length > 0 || rangeUpperBound.length > 0) {
        // The range component is constrained if either of the range bounds are
        // specified (non-empty).
        constrainedIndex = hashBucketBitSet.size();
    } else {
        // first constrained component.
        for (int i = hashBucketBitSet.size(); i > 0; i--) {
            if (hashBucketBitSet.get(i - 1).contains(true)) {
                constrainedIndex = i;
                break;
            }
        }
    }
    // Build up a set of partition key ranges out of the hash components.
    // 
    // Each hash component simply appends its bucket number to the
    // partition key ranges (possibly incrementing the upper bound by one bucket
    // number if this is the final constraint, see note 2 in the example above).
    List<Pair<ByteVec, ByteVec>> partitionKeyRanges = new ArrayList<>();
    partitionKeyRanges.add(new Pair<>(ByteVec.create(), ByteVec.create()));
    ByteBuffer bucketBuf = ByteBuffer.allocate(4);
    bucketBuf.order(ByteOrder.BIG_ENDIAN);
    for (int hashIdx = 0; hashIdx < constrainedIndex; hashIdx++) {
        // This is the final partition key component if this is the final constrained
        // bucket, and the range upper bound is empty. In this case we need to
        // increment the bucket on the upper bound to convert from inclusive to
        // exclusive.
        boolean isLast = hashIdx + 1 == constrainedIndex && rangeUpperBound.length == 0;
        PartitionSchema.HashBucketSchema hashSchema = partitionSchema.getHashBucketSchemas().get(hashIdx);
        List<Pair<ByteVec, ByteVec>> newPartitionKeyRanges = new ArrayList<>(partitionKeyRanges.size() * hashSchema.getNumBuckets());
        for (Pair<ByteVec, ByteVec> partitionKeyRange : partitionKeyRanges) {
            List<Boolean> bucketsBitSet = hashBucketBitSet.get(hashIdx);
            for (int bucket = 0; bucket < bucketsBitSet.size(); bucket++) {
                if (!bucketsBitSet.get(bucket)) {
                    continue;
                }
                int bucketUpper = isLast ? bucket + 1 : bucket;
                ByteVec lower = partitionKeyRange.getFirst().clone();
                ByteVec upper = partitionKeyRange.getFirst().clone();
                KeyEncoder.encodeHashBucket(bucket, lower);
                KeyEncoder.encodeHashBucket(bucketUpper, upper);
                newPartitionKeyRanges.add(new Pair<>(lower, upper));
            }
        }
        partitionKeyRanges = newPartitionKeyRanges;
    }
    // Step 3: append the (possibly empty) range bounds to the partition key ranges.
    for (Pair<ByteVec, ByteVec> range : partitionKeyRanges) {
        range.getFirst().append(rangeLowerBound);
        range.getSecond().append(rangeUpperBound);
    }
    // Step 4: Filter ranges that fall outside the scan's upper and lower bound partition keys.
    Deque<Pair<byte[], byte[]>> partitionKeyRangeBytes = new ArrayDeque<>(partitionKeyRanges.size());
    for (Pair<ByteVec, ByteVec> range : partitionKeyRanges) {
        byte[] lower = range.getFirst().toArray();
        byte[] upper = range.getSecond().toArray();
        // Sanity check that the lower bound is less than the upper bound.
        assert upper.length == 0 || Bytes.memcmp(lower, upper) < 0;
        // Find the intersection of the ranges.
        if (scanner.lowerBoundPartitionKey.length > 0 && (lower.length == 0 || Bytes.memcmp(lower, scanner.lowerBoundPartitionKey) < 0)) {
            lower = scanner.lowerBoundPartitionKey;
        }
        if (scanner.upperBoundPartitionKey.length > 0 && (upper.length == 0 || Bytes.memcmp(upper, scanner.upperBoundPartitionKey) > 0)) {
            upper = scanner.upperBoundPartitionKey;
        }
        // If the intersection is valid, then add it as a range partition.
        if (upper.length == 0 || Bytes.memcmp(lower, upper) < 0) {
            partitionKeyRangeBytes.add(new Pair<>(lower, upper));
        }
    }
    return new PartitionPruner(partitionKeyRangeBytes);
}
#method_after
public static PartitionPruner create(AbstractKuduScannerBuilder<?, ?> scanner) {
    Schema schema = scanner.table.getSchema();
    PartitionSchema partitionSchema = scanner.table.getPartitionSchema();
    PartitionSchema.RangeSchema rangeSchema = partitionSchema.getRangeSchema();
    Map<String, KuduPredicate> predicates = scanner.predicates;
    // bound PK.
    if (scanner.upperBoundPrimaryKey.length > 0 && Bytes.memcmp(scanner.lowerBoundPrimaryKey, scanner.upperBoundPrimaryKey) >= 0) {
        return PartitionPruner.empty();
    }
    for (KuduPredicate predicate : predicates.values()) {
        if (predicate.getType() == KuduPredicate.PredicateType.NONE) {
            return PartitionPruner.empty();
        }
    }
    // Build a set of partition key ranges which cover the tablets necessary for
    // the scan.
    // 
    // Example predicate sets and resulting partition key ranges, based on the
    // following tablet schema:
    // 
    // CREATE TABLE t (a INT32, b INT32, c INT32) PRIMARY KEY (a, b, c)
    // DISTRIBUTE BY RANGE (c)
    // HASH (a) INTO 2 BUCKETS
    // HASH (b) INTO 3 BUCKETS;
    // 
    // Assume that hash(0) = 0 and hash(2) = 2.
    // 
    // | Predicates | Partition Key Ranges                                   |
    // +------------+--------------------------------------------------------+
    // | a = 0      | [(bucket=0, bucket=2, c=0), (bucket=0, bucket=2, c=1)) |
    // | b = 2      |                                                        |
    // | c = 0      |                                                        |
    // +------------+--------------------------------------------------------+
    // | a = 0      | [(bucket=0, bucket=2), (bucket=0, bucket=3))           |
    // | b = 2      |                                                        |
    // +------------+--------------------------------------------------------+
    // | a = 0      | [(bucket=0, bucket=0, c=0), (bucket=0, bucket=0, c=1)) |
    // | c = 0      | [(bucket=0, bucket=1, c=0), (bucket=0, bucket=1, c=1)) |
    // |            | [(bucket=0, bucket=2, c=0), (bucket=0, bucket=2, c=1)) |
    // +------------+--------------------------------------------------------+
    // | b = 2      | [(bucket=0, bucket=2, c=0), (bucket=0, bucket=2, c=1)) |
    // | c = 0      | [(bucket=1, bucket=2, c=0), (bucket=1, bucket=2, c=1)) |
    // +------------+--------------------------------------------------------+
    // | a = 0      | [(bucket=0), (bucket=1))                               |
    // +------------+--------------------------------------------------------+
    // | b = 2      | [(bucket=0, bucket=2), (bucket=0, bucket=3))           |
    // |            | [(bucket=1, bucket=2), (bucket=1, bucket=3))           |
    // +------------+--------------------------------------------------------+
    // | c = 0      | [(bucket=0, bucket=0, c=0), (bucket=0, bucket=0, c=1)) |
    // |            | [(bucket=0, bucket=1, c=0), (bucket=0, bucket=1, c=1)) |
    // |            | [(bucket=0, bucket=2, c=0), (bucket=0, bucket=2, c=1)) |
    // |            | [(bucket=1, bucket=0, c=0), (bucket=1, bucket=0, c=1)) |
    // |            | [(bucket=1, bucket=1, c=0), (bucket=1, bucket=1, c=1)) |
    // |            | [(bucket=1, bucket=2, c=0), (bucket=1, bucket=2, c=1)) |
    // +------------+--------------------------------------------------------+
    // | None       | [(), ())                                               |
    // 
    // If the partition key is considered as a sequence of the hash bucket
    // components and a range component, then a few patterns emerge from the
    // examples above:
    // 
    // 1) The partition keys are truncated after the final constrained component
    // Hash bucket components are constrained when the scan is limited to a
    // subset of buckets via equality or in-list predicates on that component.
    // Range components are constrained if they have an upper or lower bound
    // via range or equality predicates on that component.
    // 
    // 2) If the final constrained component is a hash bucket, then the
    // corresponding bucket in the upper bound is incremented in order to make
    // it an exclusive key.
    // 
    // 3) The number of partition key ranges in the result is equal to the product
    // of the number of buckets of each unconstrained hash component which come
    // before a final constrained component. If there are no unconstrained hash
    // components, then the number of resulting partition key ranges is one. Note
    // that this can be a lot of ranges, and we may find we need to limit the
    // algorithm to give up on pruning if the number of ranges exceeds a limit.
    // Until this becomes a problem in practice, we'll continue always pruning,
    // since it is precisely these highly-hash-partitioned tables which get the
    // most benefit from pruning.
    // Step 1: Build the range portion of the partition key. If the range partition
    // columns match the primary key columns, then we can substitute the primary
    // key bounds, if they are tighter.
    byte[] rangeLowerBound = pushPredsIntoLowerBoundRangeKey(schema, rangeSchema, predicates);
    byte[] rangeUpperBound = pushPredsIntoUpperBoundRangeKey(schema, rangeSchema, predicates);
    if (partitionSchema.isSimpleRangePartitioning()) {
        if (Bytes.memcmp(rangeLowerBound, scanner.lowerBoundPrimaryKey) < 0) {
            rangeLowerBound = scanner.lowerBoundPrimaryKey;
        }
        if (scanner.upperBoundPrimaryKey.length > 0 && (rangeUpperBound.length == 0 || Bytes.memcmp(rangeUpperBound, scanner.upperBoundPrimaryKey) > 0)) {
            rangeUpperBound = scanner.upperBoundPrimaryKey;
        }
    }
    // Step 2: Create the hash bucket portion of the partition key.
    // List of pruned hash buckets per hash component.
    List<BitSet> hashComponents = new ArrayList<>(partitionSchema.getHashBucketSchemas().size());
    for (PartitionSchema.HashBucketSchema hashSchema : partitionSchema.getHashBucketSchemas()) {
        hashComponents.add(pruneHashComponent(schema, hashSchema, predicates));
    }
    // The index of the final constrained component in the partition key.
    int constrainedIndex = 0;
    if (rangeLowerBound.length > 0 || rangeUpperBound.length > 0) {
        // The range component is constrained if either of the range bounds are
        // specified (non-empty).
        constrainedIndex = partitionSchema.getHashBucketSchemas().size();
    } else {
        // first constrained component.
        for (int i = hashComponents.size(); i > 0; i--) {
            int numBuckets = partitionSchema.getHashBucketSchemas().get(i - 1).getNumBuckets();
            BitSet hashBuckets = hashComponents.get(i - 1);
            if (hashBuckets.nextClearBit(0) < numBuckets) {
                constrainedIndex = i;
                break;
            }
        }
    }
    // Build up a set of partition key ranges out of the hash components.
    // 
    // Each hash component simply appends its bucket number to the
    // partition key ranges (possibly incrementing the upper bound by one bucket
    // number if this is the final constraint, see note 2 in the example above).
    List<Pair<ByteVec, ByteVec>> partitionKeyRanges = new ArrayList<>();
    partitionKeyRanges.add(new Pair<>(ByteVec.create(), ByteVec.create()));
    for (int hashIdx = 0; hashIdx < constrainedIndex; hashIdx++) {
        // This is the final partition key component if this is the final constrained
        // bucket, and the range upper bound is empty. In this case we need to
        // increment the bucket on the upper bound to convert from inclusive to
        // exclusive.
        boolean isLast = hashIdx + 1 == constrainedIndex && rangeUpperBound.length == 0;
        BitSet hashBuckets = hashComponents.get(hashIdx);
        List<Pair<ByteVec, ByteVec>> newPartitionKeyRanges = new ArrayList<>(partitionKeyRanges.size() * hashBuckets.cardinality());
        for (Pair<ByteVec, ByteVec> partitionKeyRange : partitionKeyRanges) {
            for (int bucket = hashBuckets.nextSetBit(0); bucket != -1; bucket = hashBuckets.nextSetBit(bucket + 1)) {
                int bucketUpper = isLast ? bucket + 1 : bucket;
                ByteVec lower = partitionKeyRange.getFirst().clone();
                ByteVec upper = partitionKeyRange.getFirst().clone();
                KeyEncoder.encodeHashBucket(bucket, lower);
                KeyEncoder.encodeHashBucket(bucketUpper, upper);
                newPartitionKeyRanges.add(new Pair<>(lower, upper));
            }
        }
        partitionKeyRanges = newPartitionKeyRanges;
    }
    // Step 3: append the (possibly empty) range bounds to the partition key ranges.
    for (Pair<ByteVec, ByteVec> range : partitionKeyRanges) {
        range.getFirst().append(rangeLowerBound);
        range.getSecond().append(rangeUpperBound);
    }
    // Step 4: Filter ranges that fall outside the scan's upper and lower bound partition keys.
    Deque<Pair<byte[], byte[]>> partitionKeyRangeBytes = new ArrayDeque<>(partitionKeyRanges.size());
    for (Pair<ByteVec, ByteVec> range : partitionKeyRanges) {
        byte[] lower = range.getFirst().toArray();
        byte[] upper = range.getSecond().toArray();
        // Sanity check that the lower bound is less than the upper bound.
        assert upper.length == 0 || Bytes.memcmp(lower, upper) < 0;
        // Find the intersection of the ranges.
        if (scanner.lowerBoundPartitionKey.length > 0 && (lower.length == 0 || Bytes.memcmp(lower, scanner.lowerBoundPartitionKey) < 0)) {
            lower = scanner.lowerBoundPartitionKey;
        }
        if (scanner.upperBoundPartitionKey.length > 0 && (upper.length == 0 || Bytes.memcmp(upper, scanner.upperBoundPartitionKey) > 0)) {
            upper = scanner.upperBoundPartitionKey;
        }
        // If the intersection is valid, then add it as a range partition.
        if (upper.length == 0 || Bytes.memcmp(lower, upper) < 0) {
            partitionKeyRangeBytes.add(new Pair<>(lower, upper));
        }
    }
    return new PartitionPruner(partitionKeyRangeBytes);
}
#end_block

#method_before
private static List<Boolean> pruneHashComponent(Schema schema, PartitionSchema.HashBucketSchema hashSchema, Map<String, KuduPredicate> predicates) {
    List<Integer> columnIdxs = idsToIndexes(schema, hashSchema.getColumnIds());
    for (int idx : columnIdxs) {
        ColumnSchema column = schema.getColumnByIndex(idx);
        KuduPredicate predicate = predicates.get(column.getName());
        if (predicate == null || (predicate.getType() != KuduPredicate.PredicateType.EQUALITY && predicate.getType() != KuduPredicate.PredicateType.IN_LIST)) {
            return new ArrayList<>(Collections.nCopies(hashSchema.getNumBuckets(), true));
        }
    }
    List<Boolean> hashBucketBitSet = new ArrayList<>(Collections.nCopies(hashSchema.getNumBuckets(), false));
    List<PartialRow> rows = Arrays.asList(schema.newPartialRow());
    for (int idx : columnIdxs) {
        List<PartialRow> newRows = new ArrayList<>();
        ColumnSchema column = schema.getColumnByIndex(idx);
        KuduPredicate predicate = predicates.get(column.getName());
        List<byte[]> predicateValues;
        if (predicate.getType() == KuduPredicate.PredicateType.EQUALITY) {
            predicateValues = Arrays.asList(predicate.getLower());
        } else {
            predicateValues = Arrays.asList(predicate.getInListValues());
        }
        // equality and in-list predicate.
        for (PartialRow row : rows) {
            for (byte[] predicateValue : predicateValues) {
                PartialRow newRow = new PartialRow(row);
                newRow.setRaw(idx, predicateValue);
                newRows.add(newRow);
            }
        }
        rows = newRows;
    }
    for (PartialRow row : rows) {
        int hash = KeyEncoder.getHashBucket(row, hashSchema);
        hashBucketBitSet.set(hash, true);
    }
    return hashBucketBitSet;
}
#method_after
private static BitSet pruneHashComponent(Schema schema, PartitionSchema.HashBucketSchema hashSchema, Map<String, KuduPredicate> predicates) {
    BitSet hashBuckets = new BitSet(hashSchema.getNumBuckets());
    List<Integer> columnIdxs = idsToIndexes(schema, hashSchema.getColumnIds());
    for (int idx : columnIdxs) {
        ColumnSchema column = schema.getColumnByIndex(idx);
        KuduPredicate predicate = predicates.get(column.getName());
        if (predicate == null || (predicate.getType() != KuduPredicate.PredicateType.EQUALITY && predicate.getType() != KuduPredicate.PredicateType.IN_LIST)) {
            hashBuckets.set(0, hashSchema.getNumBuckets());
            return hashBuckets;
        }
    }
    List<PartialRow> rows = Arrays.asList(schema.newPartialRow());
    for (int idx : columnIdxs) {
        List<PartialRow> newRows = new ArrayList<>();
        ColumnSchema column = schema.getColumnByIndex(idx);
        KuduPredicate predicate = predicates.get(column.getName());
        List<byte[]> predicateValues;
        if (predicate.getType() == KuduPredicate.PredicateType.EQUALITY) {
            predicateValues = Arrays.asList(predicate.getLower());
        } else {
            predicateValues = Arrays.asList(predicate.getInListValues());
        }
        // equality and in-list predicate.
        for (PartialRow row : rows) {
            for (byte[] predicateValue : predicateValues) {
                PartialRow newRow = new PartialRow(row);
                newRow.setRaw(idx, predicateValue);
                newRows.add(newRow);
            }
        }
        rows = newRows;
    }
    for (PartialRow row : rows) {
        int hash = KeyEncoder.getHashBucket(row, hashSchema);
        hashBuckets.set(hash);
    }
    return hashBuckets;
}
#end_block

#method_before
@Test
public void testPrimaryKeyRangePruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c))
    // PARTITION BY RANGE (a, b, c)
    // (PARTITION                 VALUES < (0, 0, 0),
    // PARTITION    (0, 0, 0) <= VALUES < (10, 10, 10)
    // PARTITION (10, 10, 10) <= VALUES);
    ArrayList<ColumnSchema> columns = new ArrayList<>(3);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build());
    Schema schema = new Schema(columns);
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("a", "b", "c"));
    PartialRow split = schema.newPartialRow();
    split.addByte("a", (byte) 0);
    split.addByte("b", (byte) 0);
    split.addByte("c", (byte) 0);
    tableBuilder.addSplitRow(split);
    split.addByte("a", (byte) 10);
    split.addByte("b", (byte) 10);
    split.addByte("c", (byte) 10);
    tableBuilder.addSplitRow(split);
    String tableName = "testPrimaryKeyRangePruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    byte min = Byte.MIN_VALUE;
    // No bounds
    assertEquals(3, countPartitionsPrimaryKey(table, partitions, null, null));
    // PK < (-1, min, min)
    assertEquals(1, countPartitionsPrimaryKey(table, partitions, null, new byte[] { -1, min, min }));
    // PK < (10, 10, 10)
    assertEquals(2, countPartitionsPrimaryKey(table, partitions, null, new byte[] { 10, 10, 10 }));
    // PK < (100, min, min)
    assertEquals(3, countPartitionsPrimaryKey(table, partitions, null, new byte[] { 100, min, min }));
    // PK >= (-10, -10, -10)
    assertEquals(3, countPartitionsPrimaryKey(table, partitions, new byte[] { -10, -10, -10 }, null));
    // PK >= (0, 0, 0)
    assertEquals(2, countPartitionsPrimaryKey(table, partitions, new byte[] { 0, 0, 0 }, null));
    // PK >= (100, 0, 0)
    assertEquals(1, countPartitionsPrimaryKey(table, partitions, new byte[] { 100, 0, 0 }, null));
    // PK >= (-10, 0, 0)
    // PK  < (100, 0, 0)
    assertEquals(3, countPartitionsPrimaryKey(table, partitions, new byte[] { -10, 0, 0 }, new byte[] { 100, 0, 0 }));
    // PK >= (0, 0, 0)
    // PK  < (10, 10, 10)
    assertEquals(1, countPartitionsPrimaryKey(table, partitions, new byte[] { 0, 0, 0 }, new byte[] { 10, 0, 0 }));
    // PK >= (0, 0, 0)
    // PK  < (10, 10, 11)
    assertEquals(1, countPartitionsPrimaryKey(table, partitions, new byte[] { 0, 0, 0 }, new byte[] { 10, 0, 0 }));
    // PK < (0, 0, 0)
    // PK >= (10, 10, 11)
    assertEquals(0, countPartitionsPrimaryKey(table, partitions, new byte[] { 10, 0, 0 }, new byte[] { 0, 0, 0 }));
}
#method_after
@Test
public void testPrimaryKeyRangePruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c))
    // PARTITION BY RANGE (a, b, c)
    // (PARTITION                 VALUES < (0, 0, 0),
    // PARTITION    (0, 0, 0) <= VALUES < (10, 10, 10)
    // PARTITION (10, 10, 10) <= VALUES);
    ArrayList<ColumnSchema> columns = new ArrayList<>(3);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build());
    Schema schema = new Schema(columns);
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("a", "b", "c"));
    PartialRow split = schema.newPartialRow();
    split.addByte("a", (byte) 0);
    split.addByte("b", (byte) 0);
    split.addByte("c", (byte) 0);
    tableBuilder.addSplitRow(split);
    split.addByte("a", (byte) 10);
    split.addByte("b", (byte) 10);
    split.addByte("c", (byte) 10);
    tableBuilder.addSplitRow(split);
    String tableName = "testPrimaryKeyRangePruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    byte min = Byte.MIN_VALUE;
    // No bounds
    checkPartitionsPrimaryKey(3, table, partitions, null, null);
    // PK < (-1, min, min)
    checkPartitionsPrimaryKey(1, table, partitions, null, new byte[] { -1, min, min });
    // PK < (10, 10, 10)
    checkPartitionsPrimaryKey(2, table, partitions, null, new byte[] { 10, 10, 10 });
    // PK < (100, min, min)
    checkPartitionsPrimaryKey(3, table, partitions, null, new byte[] { 100, min, min });
    // PK >= (-10, -10, -10)
    checkPartitionsPrimaryKey(3, table, partitions, new byte[] { -10, -10, -10 }, null);
    // PK >= (0, 0, 0)
    checkPartitionsPrimaryKey(2, table, partitions, new byte[] { 0, 0, 0 }, null);
    // PK >= (100, 0, 0)
    checkPartitionsPrimaryKey(1, table, partitions, new byte[] { 100, 0, 0 }, null);
    // PK >= (-10, 0, 0)
    // PK  < (100, 0, 0)
    checkPartitionsPrimaryKey(3, table, partitions, new byte[] { -10, 0, 0 }, new byte[] { 100, 0, 0 });
    // PK >= (0, 0, 0)
    // PK  < (10, 10, 10)
    checkPartitionsPrimaryKey(1, table, partitions, new byte[] { 0, 0, 0 }, new byte[] { 10, 0, 0 });
    // PK >= (0, 0, 0)
    // PK  < (10, 10, 11)
    checkPartitionsPrimaryKey(1, table, partitions, new byte[] { 0, 0, 0 }, new byte[] { 10, 0, 0 });
    // PK < (0, 0, 0)
    // PK >= (10, 10, 11)
    checkPartitionsPrimaryKey(0, table, partitions, new byte[] { 10, 0, 0 }, new byte[] { 0, 0, 0 });
}
#end_block

#method_before
@Test
public void testRangePartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b STRING, c INT8)
    // PRIMARY KEY (a, b, c))
    // PARTITION BY RANGE (c, b)
    // (PARTITION              VALUES < (0, "m"),
    // PARTITION  (0, "m") <= VALUES < (10, "r")
    // PARTITION (10, "r") <= VALUES);
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.STRING).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("c", "b"));
    PartialRow split = schema.newPartialRow();
    split.addByte("c", (byte) 0);
    split.addString("b", "m");
    tableBuilder.addSplitRow(split);
    split.addByte("c", (byte) 10);
    split.addString("b", "r");
    tableBuilder.addSplitRow(split);
    String tableName = "testRangePartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(3, countPartitions(table, partitions));
    // c < -10
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, -10)));
    // c = -10
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, -10)));
    // c < 10
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 10)));
    // c < 100
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 100)));
    // c < MIN
    assertEquals(0, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, Byte.MIN_VALUE)));
    // c < MAX
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, Byte.MAX_VALUE)));
    // c >= -10
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10)));
    // c >= 0
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10)));
    // c >= 5
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 5)));
    // c >= 10
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10)));
    // c >= 100
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 100)));
    // c >= MIN
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, Byte.MIN_VALUE)));
    // c >= MAX
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, Byte.MAX_VALUE)));
    // c >= -10
    // c < 0
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10), KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 0)));
    // c >= 5
    // c < 100
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 100)));
    // b = ""
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, "")));
    // b >= "z"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "z")));
    // b < "a"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "a")));
    // b >= "m"
    // b < "z"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "m"), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "z")));
    // c >= 10
    // b >= "r"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "r")));
    // c >= 10
    // b < "r"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "r")));
    // c = 10
    // b < "r"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "r")));
    // c < 0
    // b < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // c < 0
    // b < "z"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "z")));
    // c = 0
    // b = "m\0"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, "m\0")));
    // c = 0
    // b < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // c = 0
    // b < "m\0"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m\0")));
    // c = 0
    // c = 2
    assertEquals(0, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // c = MIN
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, Byte.MIN_VALUE)));
    // c = MAX
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, Byte.MAX_VALUE)));
    // a IN (1, 2)
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 1, (byte) 2))));
    // a IN (0, 1, 2)
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1, (byte) 2))));
    // a IN (-10, 0)
    // B < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) -10, (byte) 0)), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // a IN (-10, 0)
    // B < "m\0"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) -10, (byte) 0)), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m\0")));
}
#method_after
@Test
public void testRangePartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b STRING, c INT8)
    // PRIMARY KEY (a, b, c))
    // PARTITION BY RANGE (c, b)
    // (PARTITION              VALUES < (0, "m"),
    // PARTITION  (0, "m") <= VALUES < (10, "r")
    // PARTITION (10, "r") <= VALUES);
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.STRING).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("c", "b"));
    PartialRow split = schema.newPartialRow();
    split.addByte("c", (byte) 0);
    split.addString("b", "m");
    tableBuilder.addSplitRow(split);
    split.addByte("c", (byte) 10);
    split.addString("b", "r");
    tableBuilder.addSplitRow(split);
    String tableName = "testRangePartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    checkPartitions(3, 1, table, partitions);
    // c < -10
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, -10));
    // c = -10
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, -10));
    // c < 10
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 10));
    // c < 100
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 100));
    // c < MIN
    checkPartitions(0, 0, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, Byte.MIN_VALUE));
    // c < MAX
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, Byte.MAX_VALUE));
    // c >= -10
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10));
    // c >= 0
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10));
    // c >= 5
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 5));
    // c >= 10
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10));
    // c >= 100
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 100));
    // c >= MIN
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, Byte.MIN_VALUE));
    // c >= MAX
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, Byte.MAX_VALUE));
    // c >= -10
    // c < 0
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10), KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 0));
    // c >= 5
    // c < 100
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 100));
    // b = ""
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, ""));
    // b >= "z"
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "z"));
    // b < "a"
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "a"));
    // b >= "m"
    // b < "z"
    checkPartitions(3, 1, table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "m"), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "z"));
    // c >= 10
    // b >= "r"
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "r"));
    // c >= 10
    // b < "r"
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "r"));
    // c = 10
    // b < "r"
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "r"));
    // c < 0
    // b < "m"
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m"));
    // c < 0
    // b < "z"
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "z"));
    // c = 0
    // b = "m\0"
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, "m\0"));
    // c = 0
    // b < "m"
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m"));
    // c = 0
    // b < "m\0"
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m\0"));
    // c = 0
    // c = 2
    checkPartitions(0, 0, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2));
    // c = MIN
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, Byte.MIN_VALUE));
    // c = MAX
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, Byte.MAX_VALUE));
    // c IN (1, 2)
    checkPartitions(1, 1, table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 1, (byte) 2)));
    // c IN (0, 1, 2)
    checkPartitions(2, 1, table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1, (byte) 2)));
    // c IN (-10, 0)
    // b < "m"
    checkPartitions(1, 1, table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) -10, (byte) 0)), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m"));
    // c IN (-10, 0)
    // b < "m\0"
    checkPartitions(2, 1, table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) -10, (byte) 0)), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m\0"));
}
#end_block

#method_before
@Test
public void testHashPartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c)
    // PARTITION BY HASH (a) PARTITIONS 2,
    // HASH (b, c) PARTITIONS 2;
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(new ArrayList<String>());
    tableBuilder.addHashPartitions(ImmutableList.of("a"), 2);
    tableBuilder.addHashPartitions(ImmutableList.of("b", "c"), 2);
    String tableName = "testHashPartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(4, countPartitions(table, partitions));
    // a = 0;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.EQUAL, 0)));
    // a >= 0;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0)));
    // a >= 0;
    // a < 1;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(a, ComparisonOp.LESS, 1)));
    // a >= 0;
    // a < 2;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(a, ComparisonOp.LESS, 2)));
    // b = 1;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1)));
    // b = 1;
    // c = 2;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // a = 0;
    // b = 1;
    // c = 2;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // a IN (0, 10)
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 10))));
}
#method_after
@Test
public void testHashPartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c)
    // PARTITION BY HASH (a) PARTITIONS 2,
    // HASH (b, c) PARTITIONS 2;
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(new ArrayList<String>());
    tableBuilder.addHashPartitions(ImmutableList.of("a"), 2);
    tableBuilder.addHashPartitions(ImmutableList.of("b", "c"), 2);
    String tableName = "testHashPartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    checkPartitions(4, 1, table, partitions);
    // a = 0;
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.EQUAL, 0));
    // a >= 0;
    checkPartitions(4, 1, table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0));
    // a >= 0;
    // a < 1;
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(a, ComparisonOp.LESS, 1));
    // a >= 0;
    // a < 2;
    checkPartitions(4, 1, table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(a, ComparisonOp.LESS, 2));
    // b = 1;
    checkPartitions(4, 1, table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1));
    // b = 1;
    // c = 2;
    checkPartitions(2, 2, table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2));
    // a = 0;
    // b = 1;
    // c = 2;
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2));
    // a IN (0, 10)
    checkPartitions(4, 1, table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 10)));
}
#end_block

#method_before
@Test
public void testInListHashPartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c)
    // PARTITION BY HASH (a) PARTITIONS 3,
    // HASH (b) PARTITIONS 3,
    // HASH (c) PARTITIONS 3;
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(new ArrayList<String>());
    tableBuilder.addHashPartitions(ImmutableList.of("a"), 3);
    tableBuilder.addHashPartitions(ImmutableList.of("b"), 3);
    tableBuilder.addHashPartitions(ImmutableList.of("c"), 3);
    String tableName = "testInListHashPartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // a in [0, 1];
    assertEquals(18, countPartitions(table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1))));
    // a in [0, 1, 8];
    assertEquals(27, countPartitions(table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1, (byte) 8))));
    // b in [0, 1];
    assertEquals(18, countPartitions(table, partitions, KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1))));
    // c in [0, 1];
    assertEquals(18, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1))));
    // b in [0, 1], c in [0, 1];
    assertEquals(12, countPartitions(table, partitions, KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1))));
    // a in [0, 1], b in [0, 1], c in [0, 1];
    assertEquals(8, countPartitions(table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1))));
}
#method_after
@Test
public void testInListHashPartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c)
    // PARTITION BY HASH (a) PARTITIONS 3,
    // HASH (b) PARTITIONS 3,
    // HASH (c) PARTITIONS 3;
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(new ArrayList<String>());
    tableBuilder.addHashPartitions(ImmutableList.of("a"), 3);
    tableBuilder.addHashPartitions(ImmutableList.of("b"), 3);
    tableBuilder.addHashPartitions(ImmutableList.of("c"), 3);
    String tableName = "testInListHashPartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // a in [0, 1];
    checkPartitions(18, 2, table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1)));
    // a in [0, 1, 8];
    checkPartitions(27, 1, table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1, (byte) 8)));
    // b in [0, 1];
    checkPartitions(18, 6, table, partitions, KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)));
    // c in [0, 1];
    checkPartitions(18, 18, table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1)));
    // b in [0, 1], c in [0, 1];
    checkPartitions(12, 12, table, partitions, KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1)));
    // a in [0, 1], b in [0, 1], c in [0, 1];
    checkPartitions(8, 8, table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1)));
}
#end_block

#method_before
@Test
public void TestMultiColumnInListHashPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c)
    // PARTITION BY HASH (a) PARTITIONS 3,
    // HASH (b, c) PARTITIONS 3;
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(new ArrayList<String>());
    tableBuilder.addHashPartitions(ImmutableList.of("a"), 3);
    tableBuilder.addHashPartitions(ImmutableList.of("b", "c"), 3);
    String tableName = "testMultiColumnInListHashPartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // a in [0, 1];
    assertEquals(6, countPartitions(table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1))));
    // a in [0, 1, 8];
    assertEquals(9, countPartitions(table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1, (byte) 8))));
    // b in [0, 1];
    assertEquals(9, countPartitions(table, partitions, KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1))));
    // c in [0, 1];
    assertEquals(9, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1))));
    // b in [0, 1], c in [0, 1]
    // (0, 0) in bucket 2
    // (0, 1) in bucket 2
    // (1, 0) in bucket 1
    // (1, 1) in bucket 0
    assertEquals(9, countPartitions(table, partitions, KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1))));
    // b = 0, c in [0, 1]
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 0), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1))));
    // b = 1, c in [0, 1]
    assertEquals(6, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1))));
    // a in [0, 1], b in [0, 1], c in [0, 1];
    assertEquals(6, countPartitions(table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1))));
}
#method_after
@Test
public void TestMultiColumnInListHashPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c)
    // PARTITION BY HASH (a) PARTITIONS 3,
    // HASH (b, c) PARTITIONS 3;
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(new ArrayList<String>());
    tableBuilder.addHashPartitions(ImmutableList.of("a"), 3);
    tableBuilder.addHashPartitions(ImmutableList.of("b", "c"), 3);
    String tableName = "testMultiColumnInListHashPartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // a in [0, 1];
    checkPartitions(6, 2, table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1)));
    // a in [0, 1, 8];
    checkPartitions(9, 1, table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1, (byte) 8)));
    // b in [0, 1];
    checkPartitions(9, 1, table, partitions, KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)));
    // c in [0, 1];
    checkPartitions(9, 1, table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1)));
    // b in [0, 1], c in [0, 1]
    // (0, 0) in bucket 2
    // (0, 1) in bucket 2
    // (1, 0) in bucket 1
    // (1, 1) in bucket 0
    checkPartitions(9, 1, table, partitions, KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1)));
    // b = 0, c in [0, 1]
    checkPartitions(3, 3, table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 0), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1)));
    // b = 1, c in [0, 1]
    checkPartitions(6, 6, table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1)));
    // a in [0, 1], b in [0, 1], c in [0, 1];
    checkPartitions(6, 2, table, partitions, KuduPredicate.newInListPredicate(a, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(b, ImmutableList.of((byte) 0, (byte) 1)), KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1)));
}
#end_block

#method_before
@Test
public void testPruning() throws Exception {
    // CREATE TABLE timeseries
    // (host STRING, metric STRING, timestamp UNIXTIME_MICROS, value DOUBLE)
    // PRIMARY KEY (host, metric, time)
    // DISTRIBUTE BY
    // RANGE(time)
    // (PARTITION VALUES < 10,
    // PARTITION VALUES >= 10);
    // HASH (host, metric) 2 PARTITIONS;
    ColumnSchema host = new ColumnSchema.ColumnSchemaBuilder("host", Type.STRING).key(true).build();
    ColumnSchema metric = new ColumnSchema.ColumnSchemaBuilder("metric", Type.STRING).key(true).build();
    ColumnSchema timestamp = new ColumnSchema.ColumnSchemaBuilder("timestamp", Type.UNIXTIME_MICROS).key(true).build();
    ColumnSchema value = new ColumnSchema.ColumnSchemaBuilder("value", Type.DOUBLE).build();
    Schema schema = new Schema(ImmutableList.of(host, metric, timestamp, value));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("timestamp"));
    PartialRow split = schema.newPartialRow();
    split.addLong("timestamp", 10);
    tableBuilder.addSplitRow(split);
    tableBuilder.addHashPartitions(ImmutableList.of("host", "metric"), 2);
    String tableName = "testPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(4, countPartitions(table, partitions));
    // host = "a"
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    // timestamp >= 9;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 9)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    // timestamp < 20;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 20)));
    // host = "a"
    // metric = "a"
    // timestamp < 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 10)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10)));
    // host = "a"
    // metric = "a"
    // timestamp = 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // partition key < (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }));
    // partition key >= (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}));
    // timestamp = 10
    // partition key < (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp = 10
    // partition key >= (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp IN (0, 9)
    // host = "a"
    // metric IN ("foo", "bar")
    // 
    // We do not prune hash partitions based on IN list predicates (yet),
    // so the IN list on the hash columns is really just testing that it doesn't fail.
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(0L, 9L)), KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newInListPredicate(metric, ImmutableList.of("foo", "bar"))));
    // timestamp IN (10, 100)
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(10L, 100L))));
    // timestamp IN (9, 10)
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(9L, 10L))));
    // timestamp IS NOT NULL
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.isNotNull(timestamp)));
}
#method_after
@Test
public void testPruning() throws Exception {
    // CREATE TABLE timeseries
    // (host STRING, metric STRING, timestamp UNIXTIME_MICROS, value DOUBLE)
    // PRIMARY KEY (host, metric, time)
    // DISTRIBUTE BY
    // RANGE(time)
    // (PARTITION VALUES < 10,
    // PARTITION VALUES >= 10);
    // HASH (host, metric) 2 PARTITIONS;
    ColumnSchema host = new ColumnSchema.ColumnSchemaBuilder("host", Type.STRING).key(true).build();
    ColumnSchema metric = new ColumnSchema.ColumnSchemaBuilder("metric", Type.STRING).key(true).build();
    ColumnSchema timestamp = new ColumnSchema.ColumnSchemaBuilder("timestamp", Type.UNIXTIME_MICROS).key(true).build();
    ColumnSchema value = new ColumnSchema.ColumnSchemaBuilder("value", Type.DOUBLE).build();
    Schema schema = new Schema(ImmutableList.of(host, metric, timestamp, value));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("timestamp"));
    PartialRow split = schema.newPartialRow();
    split.addLong("timestamp", 10);
    tableBuilder.addSplitRow(split);
    tableBuilder.addHashPartitions(ImmutableList.of("host", "metric"), 2);
    String tableName = "testPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    checkPartitions(4, 1, table, partitions);
    // host = "a"
    checkPartitions(4, 1, table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"));
    // host = "a"
    // metric = "a"
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"));
    // host = "a"
    // metric = "a"
    // timestamp >= 9;
    checkPartitions(2, 1, table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 9));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    // timestamp < 20;
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 20));
    // host = "a"
    // metric = "a"
    // timestamp < 10;
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 10));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10));
    // host = "a"
    // metric = "a"
    // timestamp = 10;
    checkPartitions(1, 1, table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10));
    byte[] hash1 = new byte[] { 0, 0, 0, 1 };
    // partition key < (hash=1)
    checkPartitions(2, 1, table, partitions, null, hash1);
    // partition key >= (hash=1)
    checkPartitions(2, 1, table, partitions, hash1, null);
    // timestamp = 10
    // partition key < (hash=1)
    checkPartitions(1, 1, table, partitions, null, hash1, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10));
    // timestamp = 10
    // partition key >= (hash=1)
    checkPartitions(1, 1, table, partitions, hash1, null, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10));
    // timestamp IN (0, 9)
    // host = "a"
    // metric IN ("foo", "baz")
    checkPartitions(1, 1, table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(0L, 9L)), KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newInListPredicate(metric, ImmutableList.of("foo", "baz")));
    // timestamp IN (10, 100)
    checkPartitions(2, 2, table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(10L, 100L)));
    // timestamp IN (9, 10)
    checkPartitions(4, 2, table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(9L, 10L)));
    // timestamp IS NOT NULL
    checkPartitions(4, 1, table, partitions, KuduPredicate.newIsNotNullPredicate(timestamp));
    // timestamp IS NULL
    checkPartitions(0, 0, table, partitions, KuduPredicate.newIsNullPredicate(timestamp));
}
#end_block

#method_before
public static KuduPredicate newIsNullPredicate(ColumnSchema column) {
    return new KuduPredicate(PredicateType.IS_NULL, column, null, null);
}
#method_after
public static KuduPredicate newIsNullPredicate(ColumnSchema column) {
    if (!column.isNullable()) {
        return none(column);
    }
    return new KuduPredicate(PredicateType.IS_NULL, column, null, null);
}
#end_block

#method_before
@Test
public void testScanWithPredicates() throws Exception {
    Schema schema = createManyStringsSchema();
    syncClient.createTable(tableName, schema, createTableOptions());
    KuduSession session = syncClient.newSession();
    session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_BACKGROUND);
    KuduTable table = syncClient.openTable(tableName);
    for (int i = 0; i < 100; i++) {
        Insert insert = table.newInsert();
        PartialRow row = insert.getRow();
        row.addString("key", String.format("key_%02d", i));
        row.addString("c1", "c1_" + i);
        row.addString("c2", "c2_" + i);
        session.apply(insert);
    }
    session.flush();
    assertEquals(100, scanTableToStrings(table).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER_EQUAL, "key_50")).size());
    assertEquals(25, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_74")).size());
    assertEquals(25, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_24"), KuduPredicate.newComparisonPredicate(schema.getColumn("c1"), LESS_EQUAL, "c1_49")).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_24"), KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER_EQUAL, "key_50")).size());
    assertEquals(0, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("c1"), GREATER, "c1_30"), KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), LESS, "c2_20")).size());
    assertEquals(0, scanTableToStrings(table, // Short circuit scan
    KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), GREATER, "c2_30"), KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), LESS, "c2_20")).size());
    // IS NOT NULL
    assertEquals(100, scanTableToStrings(table, KuduPredicate.newIsNotNullPredicate(schema.getColumn("c2")), KuduPredicate.newIsNotNullPredicate(schema.getColumn("key"))).size());
    // IN list
    assertEquals(3, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("key"), ImmutableList.of("key_30", "key_01", "invalid", "key_99"))).size());
    assertEquals(3, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("c2"), ImmutableList.of("c2_30", "c2_1", "invalid", "c2_99"))).size());
    assertEquals(2, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("c2"), ImmutableList.of("c2_30", "c2_1", "invalid", "c2_99")), KuduPredicate.newIsNotNullPredicate(schema.getColumn("c2")), KuduPredicate.newInListPredicate(schema.getColumn("key"), ImmutableList.of("key_30", "key_45", "invalid", "key_99"))).size());
}
#method_after
@Test
public void testScanWithPredicates() throws Exception {
    Schema schema = createManyStringsSchema();
    syncClient.createTable(tableName, schema, createTableOptions());
    KuduSession session = syncClient.newSession();
    session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_BACKGROUND);
    KuduTable table = syncClient.openTable(tableName);
    for (int i = 0; i < 100; i++) {
        Insert insert = table.newInsert();
        PartialRow row = insert.getRow();
        row.addString("key", String.format("key_%02d", i));
        row.addString("c1", "c1_" + i);
        row.addString("c2", "c2_" + i);
        if (i % 2 == 0) {
            row.addString("c3", "c3_" + i);
        }
        session.apply(insert);
    }
    session.flush();
    assertEquals(100, scanTableToStrings(table).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER_EQUAL, "key_50")).size());
    assertEquals(25, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_74")).size());
    assertEquals(25, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_24"), KuduPredicate.newComparisonPredicate(schema.getColumn("c1"), LESS_EQUAL, "c1_49")).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_24"), KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER_EQUAL, "key_50")).size());
    assertEquals(0, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("c1"), GREATER, "c1_30"), KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), LESS, "c2_20")).size());
    assertEquals(0, scanTableToStrings(table, // Short circuit scan
    KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), GREATER, "c2_30"), KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), LESS, "c2_20")).size());
    // IS NOT NULL
    assertEquals(100, scanTableToStrings(table, KuduPredicate.newIsNotNullPredicate(schema.getColumn("c1")), KuduPredicate.newIsNotNullPredicate(schema.getColumn("key"))).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newIsNotNullPredicate(schema.getColumn("c3"))).size());
    // IS NULL
    assertEquals(0, scanTableToStrings(table, KuduPredicate.newIsNullPredicate(schema.getColumn("c2")), KuduPredicate.newIsNullPredicate(schema.getColumn("key"))).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newIsNullPredicate(schema.getColumn("c3"))).size());
    // IN list
    assertEquals(3, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("key"), ImmutableList.of("key_30", "key_01", "invalid", "key_99"))).size());
    assertEquals(3, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("c2"), ImmutableList.of("c2_30", "c2_1", "invalid", "c2_99"))).size());
    assertEquals(2, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("c2"), ImmutableList.of("c2_30", "c2_1", "invalid", "c2_99")), KuduPredicate.newIsNotNullPredicate(schema.getColumn("c2")), KuduPredicate.newInListPredicate(schema.getColumn("key"), ImmutableList.of("key_30", "key_45", "invalid", "key_99"))).size());
}
#end_block

#method_before
@Test
public void testScanTokens() throws Exception {
    Schema schema = createManyStringsSchema();
    CreateTableOptions createOptions = new CreateTableOptions();
    createOptions.addHashPartitions(ImmutableList.of("key"), 8);
    PartialRow splitRow = schema.newPartialRow();
    splitRow.addString("key", "key_50");
    createOptions.addSplitRow(splitRow);
    syncClient.createTable(tableName, schema, createOptions);
    KuduSession session = syncClient.newSession();
    session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_BACKGROUND);
    KuduTable table = syncClient.openTable(tableName);
    for (int i = 0; i < 100; i++) {
        Insert insert = table.newInsert();
        PartialRow row = insert.getRow();
        row.addString("key", String.format("key_%02d", i));
        row.addString("c1", "c1_" + i);
        row.addString("c2", "c2_" + i);
        session.apply(insert);
    }
    session.flush();
    KuduScanToken.KuduScanTokenBuilder tokenBuilder = syncClient.newScanTokenBuilder(table);
    tokenBuilder.setProjectedColumnIndexes(ImmutableList.<Integer>of());
    List<KuduScanToken> tokens = tokenBuilder.build();
    assertEquals(16, tokens.size());
    for (KuduScanToken token : tokens) {
        // Sanity check to make sure the debug printing does not throw.
        LOG.debug(KuduScanToken.stringifySerializedToken(token.serialize(), syncClient));
    }
}
#method_after
@Test
public void testScanTokens() throws Exception {
    int saveFetchTablets = AsyncKuduClient.FETCH_TABLETS_PER_RANGE_LOOKUP;
    try {
        // For this test, make sure that we cover the case that not all tablets
        // are returned in a single batch.
        AsyncKuduClient.FETCH_TABLETS_PER_RANGE_LOOKUP = 4;
        Schema schema = createManyStringsSchema();
        CreateTableOptions createOptions = new CreateTableOptions();
        createOptions.addHashPartitions(ImmutableList.of("key"), 8);
        PartialRow splitRow = schema.newPartialRow();
        splitRow.addString("key", "key_50");
        createOptions.addSplitRow(splitRow);
        syncClient.createTable(tableName, schema, createOptions);
        KuduSession session = syncClient.newSession();
        session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_BACKGROUND);
        KuduTable table = syncClient.openTable(tableName);
        for (int i = 0; i < 100; i++) {
            Insert insert = table.newInsert();
            PartialRow row = insert.getRow();
            row.addString("key", String.format("key_%02d", i));
            row.addString("c1", "c1_" + i);
            row.addString("c2", "c2_" + i);
            session.apply(insert);
        }
        session.flush();
        KuduScanToken.KuduScanTokenBuilder tokenBuilder = syncClient.newScanTokenBuilder(table);
        tokenBuilder.setProjectedColumnIndexes(ImmutableList.<Integer>of());
        List<KuduScanToken> tokens = tokenBuilder.build();
        assertEquals(16, tokens.size());
        for (KuduScanToken token : tokens) {
            // Sanity check to make sure the debug printing does not throw.
            LOG.debug(KuduScanToken.stringifySerializedToken(token.serialize(), syncClient));
        }
    } finally {
        AsyncKuduClient.FETCH_TABLETS_PER_RANGE_LOOKUP = saveFetchTablets;
    }
}
#end_block

#method_before
@Test
public void testMergeInt() {
    // Equality + Equality
    // |
    // |
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0));
    // |
    // |
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 1), KuduPredicate.none(intCol));
    // Range + Equality
    // [-------->
    // |
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10));
    // [-------->
    // |
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.none(intCol));
    // <--------)
    // |
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5));
    // <--------)
    // |
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10), KuduPredicate.none(intCol));
    // Unbounded Range + Unbounded Range
    // [--------> AND
    // [-------->
    // =
    // [-------->
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0));
    // [--------> AND
    // [----->
    // =
    // [----->
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5));
    // <--------) AND
    // <--------)
    // =
    // <--------)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.newComparisonPredicate(intCol, LESS, 0));
    // <--------) AND
    // <----)
    // =
    // <----)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.newComparisonPredicate(intCol, LESS, -10), KuduPredicate.newComparisonPredicate(intCol, LESS, -10));
    // [--------> AND
    // <-------)
    // =
    // [----)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, LESS, 10), intRange(0, 10));
    // [-----> AND
    // <----)
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, LESS, 6), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5));
    // [-----> AND
    // <---)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, LESS, 5), KuduPredicate.none(intCol));
    // [-----> AND
    // <---)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, LESS, 3), KuduPredicate.none(intCol));
    // Range + Range
    // [--------) AND
    // [--------)
    // =
    // [--------)
    testMerge(intRange(0, 10), intRange(0, 10), intRange(0, 10));
    // [--------) AND
    // [----)
    // =
    // [----)
    testMerge(intRange(0, 10), intRange(0, 5), intRange(0, 5));
    // [--------) AND
    // [----)
    // =
    // [----)
    testMerge(intRange(0, 10), intRange(3, 8), intRange(3, 8));
    // [-----) AND
    // [------)
    // =
    // [---)
    testMerge(intRange(0, 8), intRange(3, 10), intRange(3, 8));
    // [--) AND
    // [---)
    // =
    // None
    testMerge(intRange(0, 5), intRange(5, 10), KuduPredicate.none(intCol));
    // [--) AND
    // [---)
    // =
    // None
    testMerge(intRange(0, 3), intRange(5, 10), KuduPredicate.none(intCol));
    // Lower Bound + Range
    // [------------>
    // [---)
    // =
    // [---)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), intRange(5, 10), intRange(5, 10));
    // [------------>
    // [--------)
    // =
    // [--------)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), intRange(5, 10), intRange(5, 10));
    // [------------>
    // [--------)
    // =
    // [---)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), intRange(0, 10), intRange(5, 10));
    // [------->
    // [-----)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 10), intRange(0, 5), KuduPredicate.none(intCol));
    // Upper Bound + Range
    // <------------)
    // [---)
    // =
    // [---)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 10), intRange(3, 8), intRange(3, 8));
    // <------------)
    // [--------)
    // =
    // [--------)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 10), intRange(5, 10), intRange(5, 10));
    // <------------)
    // [--------)
    // =
    // [----)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 5), intRange(0, 10), intRange(0, 5));
    // Range + Equality
    // [---) AND
    // |
    // =
    // None
    testMerge(intRange(3, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 1), KuduPredicate.none(intCol));
    // [---) AND
    // |
    // =
    // |
    testMerge(intRange(0, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0));
    // [---) AND
    // |
    // =
    // |
    testMerge(intRange(0, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 3), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 3));
    // [---) AND
    // |
    // =
    // None
    testMerge(intRange(0, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.none(intCol));
    // [---) AND
    // |
    // =
    // None
    testMerge(intRange(0, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 7), KuduPredicate.none(intCol));
    // IN list + IN list
    // | | |
    // | | |
    testMerge(intInList(0, 10, 20), intInList(20, 10, 20, 30), intInList(10, 20));
    // |   |
    // | |
    testMerge(intInList(0, 20), intInList(15, 30), KuduPredicate.none(intCol));
    // IN list + NOT NULL
    testMerge(intInList(10), KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10));
    testMerge(intInList(10, -100), KuduPredicate.newIsNotNullPredicate(intCol), intInList(-100, 10));
    // IN list + Equality
    // | | |
    // |
    // =
    // |
    testMerge(intInList(0, 10, 20), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10));
    // | | |
    // |
    // =
    // none
    testMerge(intInList(0, 10, 20), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 30), KuduPredicate.none(intCol));
    // IN list + Range
    // | | | | |
    // [---)
    // =
    // | |
    testMerge(intInList(0, 10, 20, 30, 40), intRange(10, 30), intInList(10, 20));
    // | |   | |
    // [--)
    // =
    // none
    testMerge(intInList(0, 10, 20, 30), intRange(25, 30), KuduPredicate.none(intCol));
    // | | | |
    // [------>
    // =
    // | |
    testMerge(intInList(0, 10, 20, 30), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 15), intInList(20, 30));
    // | | |
    // [------>
    // =
    // |
    testMerge(intInList(0, 10, 20), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 15), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 20));
    // | |
    // [------>
    // =
    // none
    testMerge(intInList(0, 10), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 15), KuduPredicate.none(intCol));
    // | | | |
    // <--)
    // =
    // | |
    testMerge(intInList(0, 10, 20, 30), KuduPredicate.newComparisonPredicate(intCol, LESS, 15), intInList(0, 10));
    // |  | |
    // <--)
    // =
    // |
    testMerge(intInList(0, 10, 20), KuduPredicate.newComparisonPredicate(intCol, LESS, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0));
    // | |
    // <--)
    // =
    // none
    testMerge(intInList(10, 20), KuduPredicate.newComparisonPredicate(intCol, LESS, 5), KuduPredicate.none(intCol));
    // None
    // None AND
    // [---->
    // =
    // None
    testMerge(KuduPredicate.none(intCol), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.none(intCol));
    // None AND
    // <----)
    // =
    // None
    testMerge(KuduPredicate.none(intCol), KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.none(intCol));
    // None AND
    // [----)
    // =
    // None
    testMerge(KuduPredicate.none(intCol), intRange(3, 7), KuduPredicate.none(intCol));
    // None AND
    // |
    // =
    // None
    testMerge(KuduPredicate.none(intCol), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.none(intCol));
    // None AND
    // None
    // =
    // None
    testMerge(KuduPredicate.none(intCol), KuduPredicate.none(intCol), KuduPredicate.none(intCol));
    // IS NOT NULL
    // IS NOT NULL AND
    // NONE
    // =
    // NONE
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.none(intCol), KuduPredicate.none(intCol));
    // IS NOT NULL AND
    // IS NULL
    // =
    // NONE
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.none(intCol));
    // IS NOT NULL AND
    // IS NOT NULL
    // =
    // IS NOT NULL
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newIsNotNullPredicate(intCol));
    // IS NOT NULL AND
    // |
    // =
    // |
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5));
    // IS NOT NULL AND
    // [------->
    // =
    // [------->
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5));
    // IS NOT NULL AND
    // |   |   |
    // =
    // |   |   |
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), intInList(0, 10, 20), intInList(0, 10, 20));
    // IS NULL
    // IS NULL AND
    // NONE
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.none(intCol), KuduPredicate.none(intCol));
    // IS NULL AND
    // IS NULL
    // =
    // IS_NULL
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newIsNullPredicate(intCol));
    // IS NULL AND
    // IS NOT NULL
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.none(intCol));
    // IS NULL AND
    // |
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.none(intCol));
    // IS NULL AND
    // [------->
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.none(intCol));
    // IS NULL AND
    // |   |   |
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), intInList(0, 10, 20), KuduPredicate.none(intCol));
}
#method_after
@Test
public void testMergeInt() {
    // Equality + Equality
    // --------------------
    // |
    // |
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0));
    // |
    // |
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 1), KuduPredicate.none(intCol));
    // Range + Equality
    // --------------------
    // [-------->
    // |
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10));
    // [-------->
    // |
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.none(intCol));
    // <--------)
    // |
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5));
    // <--------)
    // |
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10), KuduPredicate.none(intCol));
    // Unbounded Range + Unbounded Range
    // --------------------
    // [--------> AND
    // [-------->
    // =
    // [-------->
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0));
    // [--------> AND
    // [----->
    // =
    // [----->
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5));
    // <--------) AND
    // <--------)
    // =
    // <--------)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.newComparisonPredicate(intCol, LESS, 0));
    // <--------) AND
    // <----)
    // =
    // <----)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.newComparisonPredicate(intCol, LESS, -10), KuduPredicate.newComparisonPredicate(intCol, LESS, -10));
    // [--------> AND
    // <-------)
    // =
    // [----)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, LESS, 10), intRange(0, 10));
    // [-----> AND
    // <----)
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, LESS, 6), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5));
    // [-----> AND
    // <---)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, LESS, 5), KuduPredicate.none(intCol));
    // [-----> AND
    // <---)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, LESS, 3), KuduPredicate.none(intCol));
    // Range + Range
    // --------------------
    // [--------) AND
    // [--------)
    // =
    // [--------)
    testMerge(intRange(0, 10), intRange(0, 10), intRange(0, 10));
    // [--------) AND
    // [----)
    // =
    // [----)
    testMerge(intRange(0, 10), intRange(0, 5), intRange(0, 5));
    // [--------) AND
    // [----)
    // =
    // [----)
    testMerge(intRange(0, 10), intRange(3, 8), intRange(3, 8));
    // [-----) AND
    // [------)
    // =
    // [---)
    testMerge(intRange(0, 8), intRange(3, 10), intRange(3, 8));
    // [--) AND
    // [---)
    // =
    // None
    testMerge(intRange(0, 5), intRange(5, 10), KuduPredicate.none(intCol));
    // [--) AND
    // [---)
    // =
    // None
    testMerge(intRange(0, 3), intRange(5, 10), KuduPredicate.none(intCol));
    // Lower Bound + Range
    // --------------------
    // [------------>
    // [---)
    // =
    // [---)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), intRange(5, 10), intRange(5, 10));
    // [------------>
    // [--------)
    // =
    // [--------)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), intRange(5, 10), intRange(5, 10));
    // [------------>
    // [--------)
    // =
    // [---)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), intRange(0, 10), intRange(5, 10));
    // [------->
    // [-----)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 10), intRange(0, 5), KuduPredicate.none(intCol));
    // Upper Bound + Range
    // --------------------
    // <------------)
    // [---)
    // =
    // [---)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 10), intRange(3, 8), intRange(3, 8));
    // <------------)
    // [--------)
    // =
    // [--------)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 10), intRange(5, 10), intRange(5, 10));
    // <------------)
    // [--------)
    // =
    // [----)
    testMerge(KuduPredicate.newComparisonPredicate(intCol, LESS, 5), intRange(0, 10), intRange(0, 5));
    // Range + Equality
    // --------------------
    // [---) AND
    // |
    // =
    // None
    testMerge(intRange(3, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 1), KuduPredicate.none(intCol));
    // [---) AND
    // |
    // =
    // |
    testMerge(intRange(0, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0));
    // [---) AND
    // |
    // =
    // |
    testMerge(intRange(0, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 3), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 3));
    // [---) AND
    // |
    // =
    // None
    testMerge(intRange(0, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.none(intCol));
    // [---) AND
    // |
    // =
    // None
    testMerge(intRange(0, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 7), KuduPredicate.none(intCol));
    // IN list + IN list
    // --------------------
    // | | |
    // | | |
    testMerge(intInList(0, 10, 20), intInList(20, 10, 20, 30), intInList(10, 20));
    // |   |
    // | |
    testMerge(intInList(0, 20), intInList(15, 30), KuduPredicate.none(intCol));
    // IN list + NOT NULL
    // --------------------
    testMerge(intInList(10), KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10));
    testMerge(intInList(10, -100), KuduPredicate.newIsNotNullPredicate(intCol), intInList(-100, 10));
    // IN list + Equality
    // --------------------
    // | | |
    // |
    // =
    // |
    testMerge(intInList(0, 10, 20), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 10));
    // | | |
    // |
    // =
    // none
    testMerge(intInList(0, 10, 20), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 30), KuduPredicate.none(intCol));
    // IN list + Range
    // --------------------
    // | | | | |
    // [---)
    // =
    // | |
    testMerge(intInList(0, 10, 20, 30, 40), intRange(10, 30), intInList(10, 20));
    // | |   | |
    // [--)
    // =
    // none
    testMerge(intInList(0, 10, 20, 30), intRange(25, 30), KuduPredicate.none(intCol));
    // | | | |
    // [------>
    // =
    // | |
    testMerge(intInList(0, 10, 20, 30), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 15), intInList(20, 30));
    // | | |
    // [------>
    // =
    // |
    testMerge(intInList(0, 10, 20), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 15), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 20));
    // | |
    // [------>
    // =
    // none
    testMerge(intInList(0, 10), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 15), KuduPredicate.none(intCol));
    // | | | |
    // <--)
    // =
    // | |
    testMerge(intInList(0, 10, 20, 30), KuduPredicate.newComparisonPredicate(intCol, LESS, 15), intInList(0, 10));
    // |  | |
    // <--)
    // =
    // |
    testMerge(intInList(0, 10, 20), KuduPredicate.newComparisonPredicate(intCol, LESS, 10), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 0));
    // | |
    // <--)
    // =
    // none
    testMerge(intInList(10, 20), KuduPredicate.newComparisonPredicate(intCol, LESS, 5), KuduPredicate.none(intCol));
    // None
    // --------------------
    // None AND
    // [---->
    // =
    // None
    testMerge(KuduPredicate.none(intCol), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.none(intCol));
    // None AND
    // <----)
    // =
    // None
    testMerge(KuduPredicate.none(intCol), KuduPredicate.newComparisonPredicate(intCol, LESS, 0), KuduPredicate.none(intCol));
    // None AND
    // [----)
    // =
    // None
    testMerge(KuduPredicate.none(intCol), intRange(3, 7), KuduPredicate.none(intCol));
    // None AND
    // |
    // =
    // None
    testMerge(KuduPredicate.none(intCol), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.none(intCol));
    // None AND
    // None
    // =
    // None
    testMerge(KuduPredicate.none(intCol), KuduPredicate.none(intCol), KuduPredicate.none(intCol));
    // IS NOT NULL
    // --------------------
    // IS NOT NULL AND
    // NONE
    // =
    // NONE
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.none(intCol), KuduPredicate.none(intCol));
    // IS NOT NULL AND
    // IS NULL
    // =
    // NONE
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.none(intCol));
    // IS NOT NULL AND
    // IS NOT NULL
    // =
    // IS NOT NULL
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newIsNotNullPredicate(intCol));
    // IS NOT NULL AND
    // |
    // =
    // |
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5));
    // IS NOT NULL AND
    // [------->
    // =
    // [------->
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 5));
    // IS NOT NULL AND
    // <---------)
    // =
    // <---------)
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, LESS, 5), KuduPredicate.newComparisonPredicate(intCol, LESS, 5));
    // IS NOT NULL AND
    // [-------)
    // =
    // [-------)
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), intRange(0, 12), intRange(0, 12));
    // IS NOT NULL AND
    // |   |   |
    // =
    // |   |   |
    testMerge(KuduPredicate.newIsNotNullPredicate(intCol), intInList(0, 10, 20), intInList(0, 10, 20));
    // IS NULL
    // --------------------
    // IS NULL AND
    // NONE
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.none(intCol), KuduPredicate.none(intCol));
    // IS NULL AND
    // IS NULL
    // =
    // IS_NULL
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newIsNullPredicate(intCol));
    // IS NULL AND
    // IS NOT NULL
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newIsNotNullPredicate(intCol), KuduPredicate.none(intCol));
    // IS NULL AND
    // |
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, EQUAL, 5), KuduPredicate.none(intCol));
    // IS NULL AND
    // [------->
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, GREATER_EQUAL, 0), KuduPredicate.none(intCol));
    // IS NULL AND
    // <---------)
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), KuduPredicate.newComparisonPredicate(intCol, LESS, 5), KuduPredicate.none(intCol));
    // IS NULL AND
    // [-------)
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), intRange(0, 12), KuduPredicate.none(intCol));
    // IS NULL AND
    // |   |   |
    // =
    // NONE
    testMerge(KuduPredicate.newIsNullPredicate(intCol), intInList(0, 10, 20), KuduPredicate.none(intCol));
}
#end_block

#method_before
@Test
public void testMergeString() {
    // [----->
    // <-----)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "b\0"), KuduPredicate.newComparisonPredicate(stringCol, LESS, "b"), KuduPredicate.none(stringCol));
    // [----->
    // <-----)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "b"), KuduPredicate.newComparisonPredicate(stringCol, LESS, "b"), KuduPredicate.none(stringCol));
    // [----->
    // <----)
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "b"), KuduPredicate.newComparisonPredicate(stringCol, LESS, "b\0"), KuduPredicate.newComparisonPredicate(stringCol, EQUAL, "b"));
    // [----->
    // <-----)
    // =
    // [--)
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "a"), KuduPredicate.newComparisonPredicate(stringCol, LESS, "a\0\0"), new KuduPredicate(RANGE, stringCol, Bytes.fromString("a"), Bytes.fromString("a\0\0")));
    // [----->
    // | | | |
    // =
    // [--)
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "a"), stringInList("a", "c", "b", ""), stringInList("a", "b", "c"));
    // IS NOT NULL
    // | | | |
    // =
    // [--)
    testMerge(KuduPredicate.newIsNotNullPredicate(stringCol), stringInList("a", "c", "b", ""), stringInList("", "a", "b", "c"));
}
#method_after
@Test
public void testMergeString() {
    // [----->
    // <-----)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "b\0"), KuduPredicate.newComparisonPredicate(stringCol, LESS, "b"), KuduPredicate.none(stringCol));
    // [----->
    // <-----)
    // =
    // None
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "b"), KuduPredicate.newComparisonPredicate(stringCol, LESS, "b"), KuduPredicate.none(stringCol));
    // [----->
    // <----)
    // =
    // |
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "b"), KuduPredicate.newComparisonPredicate(stringCol, LESS, "b\0"), KuduPredicate.newComparisonPredicate(stringCol, EQUAL, "b"));
    // [----->
    // <-----)
    // =
    // [--)
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "a"), KuduPredicate.newComparisonPredicate(stringCol, LESS, "a\0\0"), new KuduPredicate(RANGE, stringCol, Bytes.fromString("a"), Bytes.fromString("a\0\0")));
    // [----->
    // | | | |
    // =
    // | | |
    testMerge(KuduPredicate.newComparisonPredicate(stringCol, GREATER_EQUAL, "a"), stringInList("a", "c", "b", ""), stringInList("a", "b", "c"));
    // IS NOT NULL
    // | | | |
    // =
    // | | | |
    testMerge(KuduPredicate.newIsNotNullPredicate(stringCol), stringInList("a", "c", "b", ""), stringInList("", "a", "b", "c"));
}
#end_block

#method_before
@Test
public void testToString() {
    Assert.assertEquals("`bool` = true", KuduPredicate.newComparisonPredicate(boolCol, EQUAL, true).toString());
    Assert.assertEquals("`byte` = 11", KuduPredicate.newComparisonPredicate(byteCol, EQUAL, 11).toString());
    Assert.assertEquals("`short` = 11", KuduPredicate.newComparisonPredicate(shortCol, EQUAL, 11).toString());
    Assert.assertEquals("`int` = -123", KuduPredicate.newComparisonPredicate(intCol, EQUAL, -123).toString());
    Assert.assertEquals("`long` = 5454", KuduPredicate.newComparisonPredicate(longCol, EQUAL, 5454).toString());
    Assert.assertEquals("`float` = 123.456", KuduPredicate.newComparisonPredicate(floatCol, EQUAL, 123.456f).toString());
    Assert.assertEquals("`double` = 123.456", KuduPredicate.newComparisonPredicate(doubleCol, EQUAL, 123.456).toString());
    Assert.assertEquals("`string` = \"my string\"", KuduPredicate.newComparisonPredicate(stringCol, EQUAL, "my string").toString());
    Assert.assertEquals("`binary` = 0xAB01CD", KuduPredicate.newComparisonPredicate(binaryCol, EQUAL, new byte[] { (byte) 0xAB, (byte) 0x01, (byte) 0xCD }).toString());
    Assert.assertEquals("`int` IN (-10, 0, 10)", intInList(10, 0, -10).toString());
    Assert.assertEquals("`string` IS NOT NULL", KuduPredicate.newIsNotNullPredicate(stringCol).toString());
    Assert.assertEquals("`bool` = true", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(true)).toString());
    Assert.assertEquals("`bool` = false", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(false)).toString());
    Assert.assertEquals("`bool` IS NOT NULL", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(false, true, true)).toString());
    Assert.assertEquals("`byte` IN (1, 10, 100)", KuduPredicate.newInListPredicate(byteCol, ImmutableList.of((byte) 1, (byte) 10, (byte) 100)).toString());
    Assert.assertEquals("`short` IN (1, 10, 100)", KuduPredicate.newInListPredicate(shortCol, ImmutableList.of((short) 1, (short) 100, (short) 10)).toString());
    Assert.assertEquals("`int` IN (1, 10, 100)", KuduPredicate.newInListPredicate(intCol, ImmutableList.of(1, 100, 10)).toString());
    Assert.assertEquals("`long` IN (1, 10, 100)", KuduPredicate.newInListPredicate(longCol, ImmutableList.of(1L, 100L, 10L)).toString());
    Assert.assertEquals("`float` IN (78.9, 123.456)", KuduPredicate.newInListPredicate(floatCol, ImmutableList.of(123.456f, 78.9f)).toString());
    Assert.assertEquals("`double` IN (78.9, 123.456)", KuduPredicate.newInListPredicate(doubleCol, ImmutableList.of(123.456d, 78.9d)).toString());
    Assert.assertEquals("`string` IN (\"a\", \"my string\")", KuduPredicate.newInListPredicate(stringCol, ImmutableList.of("my string", "a")).toString());
    Assert.assertEquals("`binary` IN (0x00, 0xAB01CD)", KuduPredicate.newInListPredicate(binaryCol, ImmutableList.of(new byte[] { (byte) 0xAB, (byte) 0x01, (byte) 0xCD }, new byte[] { (byte) 0x00 })).toString());
}
#method_after
@Test
public void testToString() {
    Assert.assertEquals("`bool` = true", KuduPredicate.newComparisonPredicate(boolCol, EQUAL, true).toString());
    Assert.assertEquals("`byte` = 11", KuduPredicate.newComparisonPredicate(byteCol, EQUAL, 11).toString());
    Assert.assertEquals("`short` = 11", KuduPredicate.newComparisonPredicate(shortCol, EQUAL, 11).toString());
    Assert.assertEquals("`int` = -123", KuduPredicate.newComparisonPredicate(intCol, EQUAL, -123).toString());
    Assert.assertEquals("`long` = 5454", KuduPredicate.newComparisonPredicate(longCol, EQUAL, 5454).toString());
    Assert.assertEquals("`float` = 123.456", KuduPredicate.newComparisonPredicate(floatCol, EQUAL, 123.456f).toString());
    Assert.assertEquals("`double` = 123.456", KuduPredicate.newComparisonPredicate(doubleCol, EQUAL, 123.456).toString());
    Assert.assertEquals("`string` = \"my string\"", KuduPredicate.newComparisonPredicate(stringCol, EQUAL, "my string").toString());
    Assert.assertEquals("`binary` = 0xAB01CD", KuduPredicate.newComparisonPredicate(binaryCol, EQUAL, new byte[] { (byte) 0xAB, (byte) 0x01, (byte) 0xCD }).toString());
    Assert.assertEquals("`int` IN (-10, 0, 10)", intInList(10, 0, -10).toString());
    Assert.assertEquals("`string` IS NOT NULL", KuduPredicate.newIsNotNullPredicate(stringCol).toString());
    Assert.assertEquals("`string` IS NULL", KuduPredicate.newIsNullPredicate(stringCol).toString());
    // IS NULL predicate on non-nullable column = NONE predicate
    Assert.assertEquals("`int` NONE", KuduPredicate.newIsNullPredicate(intCol).toString());
    Assert.assertEquals("`bool` = true", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(true)).toString());
    Assert.assertEquals("`bool` = false", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(false)).toString());
    Assert.assertEquals("`bool` IS NOT NULL", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(false, true, true)).toString());
    Assert.assertEquals("`byte` IN (1, 10, 100)", KuduPredicate.newInListPredicate(byteCol, ImmutableList.of((byte) 1, (byte) 10, (byte) 100)).toString());
    Assert.assertEquals("`short` IN (1, 10, 100)", KuduPredicate.newInListPredicate(shortCol, ImmutableList.of((short) 1, (short) 100, (short) 10)).toString());
    Assert.assertEquals("`int` IN (1, 10, 100)", KuduPredicate.newInListPredicate(intCol, ImmutableList.of(1, 100, 10)).toString());
    Assert.assertEquals("`long` IN (1, 10, 100)", KuduPredicate.newInListPredicate(longCol, ImmutableList.of(1L, 100L, 10L)).toString());
    Assert.assertEquals("`float` IN (78.9, 123.456)", KuduPredicate.newInListPredicate(floatCol, ImmutableList.of(123.456f, 78.9f)).toString());
    Assert.assertEquals("`double` IN (78.9, 123.456)", KuduPredicate.newInListPredicate(doubleCol, ImmutableList.of(123.456d, 78.9d)).toString());
    Assert.assertEquals("`string` IN (\"a\", \"my string\")", KuduPredicate.newInListPredicate(stringCol, ImmutableList.of("my string", "a")).toString());
    Assert.assertEquals("`binary` IN (0x00, 0xAB01CD)", KuduPredicate.newInListPredicate(binaryCol, ImmutableList.of(new byte[] { (byte) 0xAB, (byte) 0x01, (byte) 0xCD }, new byte[] { (byte) 0x00 })).toString());
}
#end_block

#method_before
@Test
public void testPruning() throws Exception {
    // CREATE TABLE timeseries
    // (host STRING, metric STRING, timestamp UNIXTIME_MICROS, value DOUBLE)
    // PRIMARY KEY (host, metric, time)
    // DISTRIBUTE BY
    // RANGE(time)
    // (PARTITION VALUES < 10,
    // PARTITION VALUES >= 10);
    // HASH (host, metric) 2 PARTITIONS;
    ColumnSchema host = new ColumnSchema.ColumnSchemaBuilder("host", Type.STRING).key(true).build();
    ColumnSchema metric = new ColumnSchema.ColumnSchemaBuilder("metric", Type.STRING).key(true).build();
    ColumnSchema timestamp = new ColumnSchema.ColumnSchemaBuilder("timestamp", Type.UNIXTIME_MICROS).key(true).build();
    ColumnSchema value = new ColumnSchema.ColumnSchemaBuilder("value", Type.DOUBLE).build();
    Schema schema = new Schema(ImmutableList.of(host, metric, timestamp, value));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("timestamp"));
    PartialRow split = schema.newPartialRow();
    split.addLong("timestamp", 10);
    tableBuilder.addSplitRow(split);
    tableBuilder.addHashPartitions(ImmutableList.of("host", "metric"), 2);
    String tableName = "testPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(4, countPartitions(table, partitions));
    // host = "a"
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    // timestamp >= 9;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 9)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    // timestamp < 20;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 20)));
    // host = "a"
    // metric = "a"
    // timestamp < 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 10)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10)));
    // host = "a"
    // metric = "a"
    // timestamp = 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // partition key < (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }));
    // partition key >= (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}));
    // timestamp = 10
    // partition key < (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp = 10
    // partition key >= (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp IN (0, 9)
    // host = "a"
    // metric IN ("foo", "bar")
    // 
    // We do not prune hash partitions based on IN list predicates (yet),
    // so the IN list on the hash columns is really just testing that it doesn't fail.
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(0L, 9L)), KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newInListPredicate(metric, ImmutableList.of("foo", "bar"))));
    // timestamp IN (10, 100)
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(10L, 100L))));
    // timestamp IN (9, 10)
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(9L, 10L))));
    // timestamp IS NOT NULL
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newIsNotNullPredicate(timestamp)));
}
#method_after
@Test
public void testPruning() throws Exception {
    // CREATE TABLE timeseries
    // (host STRING, metric STRING, timestamp UNIXTIME_MICROS, value DOUBLE)
    // PRIMARY KEY (host, metric, time)
    // DISTRIBUTE BY
    // RANGE(time)
    // (PARTITION VALUES < 10,
    // PARTITION VALUES >= 10);
    // HASH (host, metric) 2 PARTITIONS;
    ColumnSchema host = new ColumnSchema.ColumnSchemaBuilder("host", Type.STRING).key(true).build();
    ColumnSchema metric = new ColumnSchema.ColumnSchemaBuilder("metric", Type.STRING).key(true).build();
    ColumnSchema timestamp = new ColumnSchema.ColumnSchemaBuilder("timestamp", Type.UNIXTIME_MICROS).key(true).build();
    ColumnSchema value = new ColumnSchema.ColumnSchemaBuilder("value", Type.DOUBLE).build();
    Schema schema = new Schema(ImmutableList.of(host, metric, timestamp, value));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("timestamp"));
    PartialRow split = schema.newPartialRow();
    split.addLong("timestamp", 10);
    tableBuilder.addSplitRow(split);
    tableBuilder.addHashPartitions(ImmutableList.of("host", "metric"), 2);
    String tableName = "testPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(4, countPartitions(table, partitions));
    // host = "a"
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    // timestamp >= 9;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 9)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    // timestamp < 20;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 20)));
    // host = "a"
    // metric = "a"
    // timestamp < 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 10)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10)));
    // host = "a"
    // metric = "a"
    // timestamp = 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // partition key < (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }));
    // partition key >= (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}));
    // timestamp = 10
    // partition key < (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp = 10
    // partition key >= (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp IN (0, 9)
    // host = "a"
    // metric IN ("foo", "bar")
    // 
    // We do not prune hash partitions based on IN list predicates (yet),
    // so the IN list on the hash columns is really just testing that it doesn't fail.
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(0L, 9L)), KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newInListPredicate(metric, ImmutableList.of("foo", "bar"))));
    // timestamp IN (10, 100)
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(10L, 100L))));
    // timestamp IN (9, 10)
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(9L, 10L))));
    // timestamp IS NOT NULL
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newIsNotNullPredicate(timestamp)));
    // timestamp IS NULL
    assertEquals(0, countPartitions(table, partitions, KuduPredicate.newIsNullPredicate(timestamp)));
}
#end_block

#method_before
public static boolean isDescendantPath(Path p, Path parent) {
    if (p == null || parent == null)
        return false;
    while (!p.isRoot() && p.depth() != parent.depth()) p = p.getParent();
    if (p.isRoot())
        return false;
    return p.equals(parent);
}
#method_after
public static boolean isDescendantPath(Path p, Path parent) {
    if (p == null || parent == null)
        return false;
    while (!p.isRoot() && p.depth() != parent.depth()) p = p.getParent();
    if (p.isRoot())
        return false;
    boolean result = p.equals(parent);
    if (!result && LOG.isTraceEnabled()) {
        // Add a message to the log if 'p' and 'parent' have inconsistent qualification.
        URI pUri = p.toUri();
        URI parentUri = parent.toUri();
        boolean sameScheme = Objects.equal(pUri.getScheme(), parentUri.getScheme());
        boolean sameAuthority = Objects.equal(pUri.getAuthority(), parentUri.getAuthority());
        if (!sameScheme || !sameAuthority) {
            LOG.trace("Inconsistent schema or authority for paths: " + p.toString() + " " + parent.toString());
        }
    }
    return result;
}
#end_block

#method_before
private void loadBlockMetadata(Path dirPath, HashMap<Path, List<HdfsPartition>> partsByPath) {
    try {
        FileSystem fs = dirPath.getFileSystem(CONF);
        // No need to load blocks for empty partitions list.
        if (partsByPath.size() == 0 || !fs.exists(dirPath))
            return;
        if (LOG.isTraceEnabled()) {
            LOG.trace("Loading block md for " + name_ + " directory " + dirPath.toString());
        }
        // Clear the state of partitions under dirPath since they are now updated based
        // on the current snapshot of files in the directory.
        List<HdfsPartition> dirPathPartitions = partsByPath.get(dirPath);
        if (dirPathPartitions != null) {
            // unpartitioned table, or the path of a partition with a custom location.
            for (HdfsPartition partition : dirPathPartitions) {
                partition.setFileDescriptors(new ArrayList<FileDescriptor>());
            }
        } else {
            // a descendant of dirPath.
            for (Map.Entry<Path, List<HdfsPartition>> entry : partsByPath.entrySet()) {
                Path partDir = entry.getKey();
                if (!FileSystemUtil.isDescendantPath(partDir, dirPath))
                    continue;
                for (HdfsPartition partition : entry.getValue()) {
                    partition.setFileDescriptors(new ArrayList<FileDescriptor>());
                }
            }
        }
        // block location metadata based on file formats.
        if (!FileSystemUtil.supportsStorageIds(fs)) {
            synthesizeBlockMetadata(fs, dirPath, partsByPath);
            return;
        }
        int unknownDiskIdCount = 0;
        RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
        while (fileStatusIter.hasNext()) {
            LocatedFileStatus fileStatus = fileStatusIter.next();
            if (!FileSystemUtil.isValidDataFile(fileStatus))
                continue;
            // Find the partition that this file belongs (if any).
            Path partPathDir = fileStatus.getPath().getParent();
            Preconditions.checkNotNull(partPathDir);
            List<HdfsPartition> partitions = partsByPath.get(partPathDir);
            // Skip if this file does not belong to any known partition.
            if (partitions == null) {
                if (LOG.isTraceEnabled()) {
                    LOG.trace("File " + fileStatus.getPath().toString() + " doesn't correspond " + " to a known partition. Skipping metadata load for this file.");
                }
                continue;
            }
            String fileName = fileStatus.getPath().getName();
            FileDescriptor fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
            BlockLocation[] locations = fileStatus.getBlockLocations();
            String partPathDirName = partPathDir.toString();
            for (BlockLocation loc : locations) {
                Set<String> cachedHosts = Sets.newHashSet(loc.getCachedHosts());
                // Enumerate all replicas of the block, adding any unknown hosts
                // to hostIndex_. We pick the network address from getNames() and
                // map it to the corresponding hostname from getHosts().
                List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(loc.getNames().length);
                for (int i = 0; i < loc.getNames().length; ++i) {
                    TNetworkAddress networkAddress = BlockReplica.parseLocation(loc.getNames()[i]);
                    replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(loc.getHosts()[i])));
                }
                FileBlock currentBlock = new FileBlock(loc.getOffset(), loc.getLength(), replicas);
                THdfsFileBlock tHdfsFileBlock = currentBlock.toThrift();
                fd.addThriftFileBlock(tHdfsFileBlock);
                unknownDiskIdCount += loadDiskIds(loc, tHdfsFileBlock);
            }
            if (LOG.isTraceEnabled()) {
                LOG.trace("Adding file md dir: " + partPathDirName + " file: " + fileName);
            }
            // Update the partitions' metadata that this file belongs to.
            for (HdfsPartition partition : partitions) {
                partition.getFileDescriptors().add(fd);
                numHdfsFiles_++;
                totalHdfsBytes_ += fd.getFileLength();
            }
        }
        if (unknownDiskIdCount > 0) {
            if (LOG.isWarnEnabled()) {
                LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException("Error loading block metadata for directory " + dirPath.toString() + ": " + e.getMessage(), e);
    }
}
#method_after
private void loadBlockMetadata(Path dirPath, HashMap<Path, List<HdfsPartition>> partsByPath) {
    try {
        FileSystem fs = dirPath.getFileSystem(CONF);
        // No need to load blocks for empty partitions list.
        if (partsByPath.size() == 0 || !fs.exists(dirPath))
            return;
        if (LOG.isTraceEnabled()) {
            LOG.trace("Loading block md for " + name_ + " directory " + dirPath.toString());
        }
        // Clear the state of partitions under dirPath since they are going to be updated
        // based on the current snapshot of files in the directory.
        List<HdfsPartition> dirPathPartitions = partsByPath.get(dirPath);
        if (dirPathPartitions != null) {
            // unpartitioned table, or the path of at least one partition.
            for (HdfsPartition partition : dirPathPartitions) {
                partition.setFileDescriptors(new ArrayList<FileDescriptor>());
            }
        } else {
            // a descendant of dirPath.
            for (Map.Entry<Path, List<HdfsPartition>> entry : partsByPath.entrySet()) {
                Path partDir = entry.getKey();
                if (!FileSystemUtil.isDescendantPath(partDir, dirPath))
                    continue;
                for (HdfsPartition partition : entry.getValue()) {
                    partition.setFileDescriptors(new ArrayList<FileDescriptor>());
                }
            }
        }
        // block location metadata based on file formats.
        if (!FileSystemUtil.supportsStorageIds(fs)) {
            synthesizeBlockMetadata(fs, dirPath, partsByPath);
            return;
        }
        int unknownDiskIdCount = 0;
        RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
        while (fileStatusIter.hasNext()) {
            LocatedFileStatus fileStatus = fileStatusIter.next();
            if (!FileSystemUtil.isValidDataFile(fileStatus))
                continue;
            // Find the partition that this file belongs (if any).
            Path partPathDir = fileStatus.getPath().getParent();
            Preconditions.checkNotNull(partPathDir);
            List<HdfsPartition> partitions = partsByPath.get(partPathDir);
            // Skip if this file does not belong to any known partition.
            if (partitions == null) {
                if (LOG.isTraceEnabled()) {
                    LOG.trace("File " + fileStatus.getPath().toString() + " doesn't correspond " + " to a known partition. Skipping metadata load for this file.");
                }
                continue;
            }
            String fileName = fileStatus.getPath().getName();
            FileDescriptor fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
            BlockLocation[] locations = fileStatus.getBlockLocations();
            String partPathDirName = partPathDir.toString();
            for (BlockLocation loc : locations) {
                Set<String> cachedHosts = Sets.newHashSet(loc.getCachedHosts());
                // Enumerate all replicas of the block, adding any unknown hosts
                // to hostIndex_. We pick the network address from getNames() and
                // map it to the corresponding hostname from getHosts().
                List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(loc.getNames().length);
                for (int i = 0; i < loc.getNames().length; ++i) {
                    TNetworkAddress networkAddress = BlockReplica.parseLocation(loc.getNames()[i]);
                    replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(loc.getHosts()[i])));
                }
                FileBlock currentBlock = new FileBlock(loc.getOffset(), loc.getLength(), replicas);
                THdfsFileBlock tHdfsFileBlock = currentBlock.toThrift();
                fd.addThriftFileBlock(tHdfsFileBlock);
                unknownDiskIdCount += loadDiskIds(loc, tHdfsFileBlock);
            }
            if (LOG.isTraceEnabled()) {
                LOG.trace("Adding file md dir: " + partPathDirName + " file: " + fileName);
            }
            // Update the partitions' metadata that this file belongs to.
            for (HdfsPartition partition : partitions) {
                partition.getFileDescriptors().add(fd);
                numHdfsFiles_++;
                totalHdfsBytes_ += fd.getFileLength();
            }
        }
        if (unknownDiskIdCount > 0) {
            if (LOG.isWarnEnabled()) {
                LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException("Error loading block metadata for directory " + dirPath.toString() + ": " + e.getMessage(), e);
    }
}
#end_block

#method_before
private void loadAllPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl) throws IOException, CatalogException {
    Preconditions.checkNotNull(msTbl);
    initializePartitionMetadata(msTbl);
    // Map of partition paths to their corresponding HdfsPartition objects. Populated
    // using createPartition() calls. A single partition path can correspond to multiple
    // partitions.
    HashMap<Path, List<HdfsPartition>> partsByPath = Maps.newHashMap();
    Path tblLocation = getHdfsBaseDirPath();
    FileSystem fs = tblLocation.getFileSystem(CONF);
    // Qualify to ensure isDescendantPath() works correctly.
    tblLocation = tblLocation.makeQualified(fs.getUri(), tblLocation);
    // List of directories that we scan for block locations. We optimize the block metadata
    // loading to reduce the number of RPCs to the NN by separately loading partitions
    // with default directory paths (under the base table directory) and non-default
    // directory paths. For the former we issue a single RPC to the NN to load all the
    // blocks from hdfsBaseDir_ and for the latter we load each of the partition directory
    // separately.
    // TODO: We can still do some advanced optimization by grouping all the partition
    // directories under the same ancestor path up the tree.
    List<Path> dirsToLoad = Lists.newArrayList(tblLocation);
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null);
        partsByPath.put(tblLocation, Lists.newArrayList(part));
        if (isMarkedCached_)
            part.markCached();
        addPartition(part);
        if (fs.exists(tblLocation)) {
            accessLevel_ = getAvailableAccessLevel(fs, tblLocation);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition);
            addPartition(partition);
            // to this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null) {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
            Path partDir = new Path(msPartition.getSd().getLocation());
            FileSystem partFs = partDir.getFileSystem(CONF);
            // Qualify to ensure isDescendantPath() works correctly.
            partDir = partDir.makeQualified(partFs.getUri(), partDir);
            List<HdfsPartition> parts = partsByPath.get(partDir);
            if (parts == null) {
                partsByPath.put(partDir, Lists.newArrayList(partition));
            } else {
                parts.add(partition);
            }
            if (!dirsToLoad.contains(partDir) && !FileSystemUtil.isDescendantPath(partDir, tblLocation)) {
                // This partition has a custom filesystem location. Load its file/block
                // metadata separately by adding it to the list of dirs to load.
                dirsToLoad.add(partDir);
            }
        }
    }
    loadMetadataAndDiskIds(dirsToLoad, partsByPath);
}
#method_after
private void loadAllPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl) throws IOException, CatalogException {
    Preconditions.checkNotNull(msTbl);
    initializePartitionMetadata(msTbl);
    // Map of partition paths to their corresponding HdfsPartition objects. Populated
    // using createPartition() calls. A single partition path can correspond to multiple
    // partitions.
    HashMap<Path, List<HdfsPartition>> partsByPath = Maps.newHashMap();
    // Qualify to ensure isDescendantPath() works correctly.
    Path tblLocation = FileSystemUtil.createFullyQualifiedPath(getHdfsBaseDirPath());
    // List of directories that we scan for block locations. We optimize the block metadata
    // loading to reduce the number of RPCs to the NN by separately loading partitions
    // with default directory paths (under the base table directory) and non-default
    // directory paths. For the former we issue a single RPC to the NN to load all the
    // blocks from hdfsBaseDir_ and for the latter we load each of the partition directory
    // separately.
    // TODO: We can still do some advanced optimization by grouping all the partition
    // directories under the same ancestor path up the tree.
    List<Path> dirsToLoad = Lists.newArrayList(tblLocation);
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null);
        partsByPath.put(tblLocation, Lists.newArrayList(part));
        if (isMarkedCached_)
            part.markCached();
        addPartition(part);
        FileSystem fs = tblLocation.getFileSystem(CONF);
        if (fs.exists(tblLocation)) {
            accessLevel_ = getAvailableAccessLevel(fs, tblLocation);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition);
            addPartition(partition);
            // to this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null) {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
            // Qualify to ensure isDescendantPath() works correctly.
            Path partDir = FileSystemUtil.createFullyQualifiedPath(new Path(msPartition.getSd().getLocation()));
            List<HdfsPartition> parts = partsByPath.get(partDir);
            if (parts == null) {
                partsByPath.put(partDir, Lists.newArrayList(partition));
            } else {
                parts.add(partition);
            }
            if (!dirsToLoad.contains(partDir) && !FileSystemUtil.isDescendantPath(partDir, tblLocation)) {
                // This partition has a custom filesystem location. Load its file/block
                // metadata separately by adding it to the list of dirs to load.
                dirsToLoad.add(partDir);
            }
        }
    }
    loadMetadataAndDiskIds(dirsToLoad, partsByPath);
}
#end_block

#method_before
public List<List<String>> getPathsWithoutPartitions() throws CatalogException {
    List<List<LiteralExpr>> existingPartitions = new ArrayList<List<LiteralExpr>>();
    // Get the list of partition values of existing partitions in Hive Metastore.
    for (HdfsPartition partition : partitionMap_.values()) {
        if (partition.isDefaultPartition())
            continue;
        existingPartitions.add(partition.getPartitionValues());
    }
    List<String> partitionKeys = Lists.newArrayList();
    for (int i = 0; i < numClusteringCols_; ++i) {
        partitionKeys.add(getColumns().get(i).getName());
    }
    Path basePath = new Path(hdfsBaseDir_);
    List<List<String>> partitionsNotInHms = new ArrayList<List<String>>();
    try {
        getAllPartitionsNotInHms(basePath, partitionKeys, existingPartitions, partitionsNotInHms);
    } catch (Exception e) {
        throw new CatalogException(String.format("Failed to recover partitions for %s " + "with exception:%s.", getFullName(), e));
    }
    return partitionsNotInHms;
}
#method_after
public List<List<String>> getPathsWithoutPartitions() throws CatalogException {
    HashSet<List<LiteralExpr>> existingPartitions = new HashSet<List<LiteralExpr>>();
    // Get the list of partition values of existing partitions in Hive Metastore.
    for (HdfsPartition partition : partitionMap_.values()) {
        if (partition.isDefaultPartition())
            continue;
        existingPartitions.add(partition.getPartitionValues());
    }
    List<String> partitionKeys = Lists.newArrayList();
    for (int i = 0; i < numClusteringCols_; ++i) {
        partitionKeys.add(getColumns().get(i).getName());
    }
    Path basePath = new Path(hdfsBaseDir_);
    List<List<String>> partitionsNotInHms = new ArrayList<List<String>>();
    try {
        getAllPartitionsNotInHms(basePath, partitionKeys, existingPartitions, partitionsNotInHms);
    } catch (Exception e) {
        throw new CatalogException(String.format("Failed to recover partitions for %s " + "with exception:%s.", getFullName(), e));
    }
    return partitionsNotInHms;
}
#end_block

#method_before
private void getAllPartitionsNotInHms(Path path, List<String> partitionKeys, int depth, FileSystem fs, List<String> partitionValues, List<LiteralExpr> partitionExprs, List<List<LiteralExpr>> existingPartitions, List<List<String>> partitionsNotInHms) throws IOException {
    if (depth == partitionKeys.size()) {
        if (existingPartitions.contains(partitionExprs)) {
            if (LOG.isTraceEnabled()) {
                LOG.trace(String.format("Skip recovery of path '%s' because it already " + "exists in metastore", path.toString()));
            }
        } else {
            partitionsNotInHms.add(partitionValues);
            existingPartitions.add(partitionExprs);
        }
        return;
    }
    FileStatus[] statuses = fs.listStatus(path);
    for (FileStatus status : statuses) {
        if (!status.isDirectory())
            continue;
        Pair<String, LiteralExpr> keyValues = getTypeCompatibleValue(status.getPath(), partitionKeys.get(depth));
        if (keyValues == null)
            continue;
        List<String> currentPartitionValues = Lists.newArrayList(partitionValues);
        List<LiteralExpr> currentPartitionExprs = Lists.newArrayList(partitionExprs);
        currentPartitionValues.add(keyValues.first);
        currentPartitionExprs.add(keyValues.second);
        getAllPartitionsNotInHms(status.getPath(), partitionKeys, depth + 1, fs, currentPartitionValues, currentPartitionExprs, existingPartitions, partitionsNotInHms);
    }
}
#method_after
private void getAllPartitionsNotInHms(Path path, List<String> partitionKeys, HashSet<List<LiteralExpr>> existingPartitions, List<List<String>> partitionsNotInHms) throws IOException {
    FileSystem fs = path.getFileSystem(CONF);
    // Check whether the base directory exists.
    if (!fs.exists(path))
        return;
    List<String> partitionValues = Lists.newArrayList();
    List<LiteralExpr> partitionExprs = Lists.newArrayList();
    getAllPartitionsNotInHms(path, partitionKeys, 0, fs, partitionValues, partitionExprs, existingPartitions, partitionsNotInHms);
}
#end_block

#method_before
@Test(timeout = 100000)
public void testCreateTableOutOfOrderPrimaryKeys() throws Exception {
    ArrayList<ColumnSchema> columns = new ArrayList<ColumnSchema>(6);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key_1", Type.INT8).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("column1_i", Type.INT32).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key_2", Type.INT16).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("column2_i", Type.INT32).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("column3_s", Type.STRING).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("column4_b", Type.BOOL).build());
    Schema schema = new Schema(columns);
    try {
        client.createTable("testCreateTableOutOfOrderPrimaryKeys-" + System.currentTimeMillis(), schema, getBasicCreateTableOptions()).join();
    } catch (IllegalArgumentException iae) {
        assertTrue(iae.getMessage().startsWith("Got out-of-order key column"));
    }
}
#method_after
@Test(timeout = 100000)
public void testCreateTableOutOfOrderPrimaryKeys() throws Exception {
    ArrayList<ColumnSchema> columns = new ArrayList<ColumnSchema>(6);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key_1", Type.INT8).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("column1_i", Type.INT32).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key_2", Type.INT16).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("column2_i", Type.INT32).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("column3_s", Type.STRING).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("column4_b", Type.BOOL).build());
    Schema schema = new Schema(columns);
    try {
        client.createTable("testCreateTableOutOfOrderPrimaryKeys-" + System.currentTimeMillis(), schema, getBasicCreateTableOptions()).join();
    } catch (NonRecoverableException nre) {
        assertTrue(nre.getMessage().startsWith("Got out-of-order key column"));
    }
}
#end_block

#method_before
public Deferred<KuduTable> createTable(final String name, Schema schema, CreateTableOptions builder) {
    checkIsClosed();
    checkSchemaIsValid(schema);
    if (builder == null) {
        throw new IllegalArgumentException("CreateTableOptions may not be null");
    }
    if (!builder.getBuilder().getPartitionSchema().hasRangeSchema() && builder.getBuilder().getPartitionSchema().getHashBucketSchemasCount() == 0) {
        throw new IllegalArgumentException("Table partitioning must be specified using " + "setRangePartitionColumns or addHashPartitions");
    }
    CreateTableRequest create = new CreateTableRequest(this.masterTable, name, schema, builder);
    create.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    return sendRpcToTablet(create).addCallbackDeferring(new Callback<Deferred<KuduTable>, CreateTableResponse>() {

        @Override
        public Deferred<KuduTable> call(CreateTableResponse createTableResponse) throws Exception {
            return openTable(name);
        }
    });
}
#method_after
public Deferred<KuduTable> createTable(final String name, Schema schema, CreateTableOptions builder) {
    checkIsClosed();
    if (builder == null) {
        throw new IllegalArgumentException("CreateTableOptions may not be null");
    }
    if (!builder.getBuilder().getPartitionSchema().hasRangeSchema() && builder.getBuilder().getPartitionSchema().getHashBucketSchemasCount() == 0) {
        throw new IllegalArgumentException("Table partitioning must be specified using " + "setRangePartitionColumns or addHashPartitions");
    }
    CreateTableRequest create = new CreateTableRequest(this.masterTable, name, schema, builder);
    create.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    return sendRpcToTablet(create).addCallbackDeferring(new Callback<Deferred<KuduTable>, CreateTableResponse>() {

        @Override
        public Deferred<KuduTable> call(CreateTableResponse createTableResponse) throws Exception {
            return openTable(name);
        }
    });
}
#end_block

#method_before
public void analyze(String stmt, Analyzer analyzer) throws AnalysisException {
    SqlScanner input = new SqlScanner(new StringReader(stmt));
    SqlParser parser = new SqlParser(input);
    try {
        analysisResult_ = new AnalysisResult();
        analysisResult_.analyzer_ = analyzer;
        if (analysisResult_.analyzer_ == null) {
            analysisResult_.analyzer_ = new Analyzer(catalog_, queryCtx_, authzConfig_);
        }
        analysisResult_.stmt_ = (StatementBase) parser.parse().value;
        if (analysisResult_.stmt_ == null)
            return;
        analysisResult_.stmt_.analyze(analysisResult_.analyzer_);
        boolean isExplain = analysisResult_.isExplainStmt();
        // Apply expr and subquery rewrites.
        boolean reAnalyze = false;
        if (analysisResult_.requiresExprRewrite()) {
            rewriter_.reset();
            analysisResult_.stmt_.rewriteExprs(rewriter_);
            reAnalyze = rewriter_.changed();
        }
        if (analysisResult_.requiresSubqueryRewrite()) {
            StmtRewriter.rewrite(analysisResult_);
            reAnalyze = true;
        }
        if (reAnalyze) {
            // The rewrites should have no user-visible effect. Remember the original result
            // types and column labels to restore them after the rewritten stmt has been
            // reset() and re-analyzed.
            List<Type> origResultTypes = Lists.newArrayList();
            for (Expr e : analysisResult_.stmt_.getResultExprs()) {
                origResultTypes.add(e.getType());
            }
            List<String> origColLabels = Lists.newArrayList(analysisResult_.stmt_.getColLabels());
            // Re-analyze the stmt with a new analyzer.
            analysisResult_.analyzer_ = new Analyzer(catalog_, queryCtx_, authzConfig_);
            analysisResult_.stmt_.reset();
            analysisResult_.stmt_.analyze(analysisResult_.analyzer_);
            // Restore the original result types and column labels.
            analysisResult_.stmt_.castResultExprs(origResultTypes);
            analysisResult_.stmt_.setColLabels(origColLabels);
            if (LOG.isTraceEnabled()) {
                LOG.trace("rewrittenStmt: " + analysisResult_.stmt_.toSql());
            }
            if (isExplain)
                analysisResult_.stmt_.setIsExplain();
            Preconditions.checkState(!analysisResult_.requiresSubqueryRewrite());
        }
    } catch (AnalysisException e) {
        // Don't wrap AnalysisExceptions in another AnalysisException
        throw e;
    } catch (Exception e) {
        throw new AnalysisException(parser.getErrorMsg(stmt), e);
    }
}
#method_after
public void analyze(String stmt, Analyzer analyzer) throws AnalysisException {
    SqlScanner input = new SqlScanner(new StringReader(stmt));
    SqlParser parser = new SqlParser(input);
    try {
        analysisResult_ = new AnalysisResult();
        analysisResult_.analyzer_ = analyzer;
        if (analysisResult_.analyzer_ == null) {
            analysisResult_.analyzer_ = new Analyzer(catalog_, queryCtx_, authzConfig_);
        }
        analysisResult_.timeline_ = timeline_;
        analysisResult_.stmt_ = (StatementBase) parser.parse().value;
        if (analysisResult_.stmt_ == null)
            return;
        analysisResult_.stmt_.analyze(analysisResult_.analyzer_);
        boolean isExplain = analysisResult_.isExplainStmt();
        // Apply expr and subquery rewrites.
        boolean reAnalyze = false;
        if (analysisResult_.requiresExprRewrite()) {
            rewriter_.reset();
            analysisResult_.stmt_.rewriteExprs(rewriter_);
            reAnalyze = rewriter_.changed();
        }
        if (analysisResult_.requiresSubqueryRewrite()) {
            StmtRewriter.rewrite(analysisResult_);
            reAnalyze = true;
        }
        if (reAnalyze) {
            // The rewrites should have no user-visible effect. Remember the original result
            // types and column labels to restore them after the rewritten stmt has been
            // reset() and re-analyzed.
            List<Type> origResultTypes = Lists.newArrayList();
            for (Expr e : analysisResult_.stmt_.getResultExprs()) {
                origResultTypes.add(e.getType());
            }
            List<String> origColLabels = Lists.newArrayList(analysisResult_.stmt_.getColLabels());
            // Re-analyze the stmt with a new analyzer.
            analysisResult_.analyzer_ = new Analyzer(catalog_, queryCtx_, authzConfig_);
            analysisResult_.stmt_.reset();
            analysisResult_.stmt_.analyze(analysisResult_.analyzer_);
            // Restore the original result types and column labels.
            analysisResult_.stmt_.castResultExprs(origResultTypes);
            analysisResult_.stmt_.setColLabels(origColLabels);
            if (LOG.isTraceEnabled()) {
                LOG.trace("rewrittenStmt: " + analysisResult_.stmt_.toSql());
            }
            if (isExplain)
                analysisResult_.stmt_.setIsExplain();
            Preconditions.checkState(!analysisResult_.requiresSubqueryRewrite());
        }
    } catch (AnalysisException e) {
        // Don't wrap AnalysisExceptions in another AnalysisException
        throw e;
    } catch (Exception e) {
        throw new AnalysisException(parser.getErrorMsg(stmt), e);
    }
}
#end_block

#method_before
public Expr RewritesOk(String expr, ExprRewriteRule rule, String expectedExpr) throws AnalysisException {
    // Create a rewriter with only a single rule.
    List<ExprRewriteRule> rules = Lists.newArrayList();
    rules.add(rule);
    return RewritesOk(expr, rules, expectedExpr);
}
#method_after
public Expr RewritesOk(String expr, ExprRewriteRule rule, String expectedExpr) throws AnalysisException {
    return RewritesOk(expr, Lists.newArrayList(rule), expectedExpr);
}
#end_block

#method_before
public void run() {
    if (LOG.isTraceEnabled()) {
        LOG.trace("Reloading cache pool names from HDFS");
    }
    // Map of cache pool name to CachePoolInfo. Stored in a map to allow Set operations
    // to be performed on the keys.
    Map<String, CachePoolInfo> currentCachePools = Maps.newHashMap();
    try {
        DistributedFileSystem dfs = FileSystemUtil.getDistributedFileSystem();
        RemoteIterator<CachePoolEntry> itr = dfs.listCachePools();
        while (itr.hasNext()) {
            CachePoolInfo cachePoolInfo = itr.next().getInfo();
            currentCachePools.put(cachePoolInfo.getPoolName(), cachePoolInfo);
        }
    } catch (Exception e) {
        LOG.error("Error loading cache pools: ", e);
        return;
    }
    catalogLock_.writeLock().lock();
    try {
        // Determine what has changed relative to what we have cached.
        Set<String> droppedCachePoolNames = Sets.difference(hdfsCachePools_.keySet(), currentCachePools.keySet());
        Set<String> createdCachePoolNames = Sets.difference(currentCachePools.keySet(), hdfsCachePools_.keySet());
        // Add all new cache pools.
        for (String createdCachePool : createdCachePoolNames) {
            HdfsCachePool cachePool = new HdfsCachePool(currentCachePools.get(createdCachePool));
            cachePool.setCatalogVersion(CatalogServiceCatalog.this.incrementAndGetCatalogVersion());
            hdfsCachePools_.add(cachePool);
        }
        // Remove dropped cache pools.
        for (String cachePoolName : droppedCachePoolNames) {
            hdfsCachePools_.remove(cachePoolName);
            CatalogServiceCatalog.this.incrementAndGetCatalogVersion();
        }
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#method_after
public void run() {
    if (LOG.isTraceEnabled())
        LOG.trace("Reloading cache pool names from HDFS");
    // Map of cache pool name to CachePoolInfo. Stored in a map to allow Set operations
    // to be performed on the keys.
    Map<String, CachePoolInfo> currentCachePools = Maps.newHashMap();
    try {
        DistributedFileSystem dfs = FileSystemUtil.getDistributedFileSystem();
        RemoteIterator<CachePoolEntry> itr = dfs.listCachePools();
        while (itr.hasNext()) {
            CachePoolInfo cachePoolInfo = itr.next().getInfo();
            currentCachePools.put(cachePoolInfo.getPoolName(), cachePoolInfo);
        }
    } catch (Exception e) {
        LOG.error("Error loading cache pools: ", e);
        return;
    }
    catalogLock_.writeLock().lock();
    try {
        // Determine what has changed relative to what we have cached.
        Set<String> droppedCachePoolNames = Sets.difference(hdfsCachePools_.keySet(), currentCachePools.keySet());
        Set<String> createdCachePoolNames = Sets.difference(currentCachePools.keySet(), hdfsCachePools_.keySet());
        // Add all new cache pools.
        for (String createdCachePool : createdCachePoolNames) {
            HdfsCachePool cachePool = new HdfsCachePool(currentCachePools.get(createdCachePool));
            cachePool.setCatalogVersion(CatalogServiceCatalog.this.incrementAndGetCatalogVersion());
            hdfsCachePools_.add(cachePool);
        }
        // Remove dropped cache pools.
        for (String cachePoolName : droppedCachePoolNames) {
            hdfsCachePools_.remove(cachePoolName);
            CatalogServiceCatalog.this.incrementAndGetCatalogVersion();
        }
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#end_block

#method_before
public TGetAllCatalogObjectsResponse getCatalogObjects(long fromVersion) {
    TGetAllCatalogObjectsResponse resp = new TGetAllCatalogObjectsResponse();
    resp.setObjects(new ArrayList<TCatalogObject>());
    resp.setMax_catalog_version(Catalog.INITIAL_CATALOG_VERSION);
    catalogLock_.readLock().lock();
    try {
        for (Db db : getDbs(PatternMatcher.MATCHER_MATCH_ALL)) {
            TCatalogObject catalogDb = new TCatalogObject(TCatalogObjectType.DATABASE, db.getCatalogVersion());
            catalogDb.setDb(db.toThrift());
            resp.addToObjects(catalogDb);
            for (String tblName : db.getAllTableNames()) {
                TCatalogObject catalogTbl = new TCatalogObject(TCatalogObjectType.TABLE, Catalog.INITIAL_CATALOG_VERSION);
                Table tbl = db.getTable(tblName);
                if (tbl == null) {
                    LOG.error("Table: " + tblName + " was expected to be in the catalog " + "cache. Skipping table for this update.");
                    continue;
                }
                // Protect the table from concurrent modifications.
                tbl.getLock().writeLock().lock();
                try {
                    // the fromVersion.
                    if (tbl.getCatalogVersion() >= fromVersion) {
                        try {
                            catalogTbl.setTable(tbl.toThrift());
                        } catch (Exception e) {
                            if (LOG.isTraceEnabled()) {
                                LOG.trace(String.format("Error calling toThrift() on table %s.%s: %s", db.getName(), tblName, e.getMessage()), e);
                            }
                            continue;
                        }
                        catalogTbl.setCatalog_version(tbl.getCatalogVersion());
                    } else {
                        catalogTbl.setTable(new TTable(db.getName(), tblName));
                    }
                } finally {
                    tbl.getLock().writeLock().unlock();
                }
                resp.addToObjects(catalogTbl);
            }
            for (Function fn : db.getFunctions(null, new PatternMatcher())) {
                TCatalogObject function = new TCatalogObject(TCatalogObjectType.FUNCTION, fn.getCatalogVersion());
                function.setFn(fn.toThrift());
                resp.addToObjects(function);
            }
        }
        for (DataSource dataSource : getDataSources()) {
            TCatalogObject catalogObj = new TCatalogObject(TCatalogObjectType.DATA_SOURCE, dataSource.getCatalogVersion());
            catalogObj.setData_source(dataSource.toThrift());
            resp.addToObjects(catalogObj);
        }
        for (HdfsCachePool cachePool : hdfsCachePools_) {
            TCatalogObject pool = new TCatalogObject(TCatalogObjectType.HDFS_CACHE_POOL, cachePool.getCatalogVersion());
            pool.setCache_pool(cachePool.toThrift());
            resp.addToObjects(pool);
        }
        // Get all roles
        for (Role role : authPolicy_.getAllRoles()) {
            TCatalogObject thriftRole = new TCatalogObject();
            thriftRole.setRole(role.toThrift());
            thriftRole.setCatalog_version(role.getCatalogVersion());
            thriftRole.setType(role.getCatalogObjectType());
            resp.addToObjects(thriftRole);
            for (RolePrivilege p : role.getPrivileges()) {
                TCatalogObject privilege = new TCatalogObject();
                privilege.setPrivilege(p.toThrift());
                privilege.setCatalog_version(p.getCatalogVersion());
                privilege.setType(p.getCatalogObjectType());
                resp.addToObjects(privilege);
            }
        }
        // Each update should contain a single "TCatalog" object which is used to
        // pass overall state on the catalog, such as the current version and the
        // catalog service id.
        TCatalogObject catalog = new TCatalogObject();
        catalog.setType(TCatalogObjectType.CATALOG);
        // By setting the catalog version to the latest catalog version at this point,
        // it ensure impalads will always bump their versions, even in the case where
        // an object has been dropped.
        catalog.setCatalog_version(getCatalogVersion());
        catalog.setCatalog(new TCatalog(catalogServiceId_));
        resp.addToObjects(catalog);
        // The max version is the max catalog version of all items in the update.
        resp.setMax_catalog_version(getCatalogVersion());
        return resp;
    } finally {
        catalogLock_.readLock().unlock();
    }
}
#method_after
public TGetAllCatalogObjectsResponse getCatalogObjects(long fromVersion) {
    TGetAllCatalogObjectsResponse resp = new TGetAllCatalogObjectsResponse();
    resp.setObjects(new ArrayList<TCatalogObject>());
    resp.setMax_catalog_version(Catalog.INITIAL_CATALOG_VERSION);
    catalogLock_.readLock().lock();
    try {
        for (Db db : getDbs(PatternMatcher.MATCHER_MATCH_ALL)) {
            TCatalogObject catalogDb = new TCatalogObject(TCatalogObjectType.DATABASE, db.getCatalogVersion());
            catalogDb.setDb(db.toThrift());
            resp.addToObjects(catalogDb);
            for (String tblName : db.getAllTableNames()) {
                TCatalogObject catalogTbl = new TCatalogObject(TCatalogObjectType.TABLE, Catalog.INITIAL_CATALOG_VERSION);
                Table tbl = db.getTable(tblName);
                if (tbl == null) {
                    LOG.error("Table: " + tblName + " was expected to be in the catalog " + "cache. Skipping table for this update.");
                    continue;
                }
                // Protect the table from concurrent modifications.
                tbl.getLock().lock();
                try {
                    // the fromVersion.
                    if (tbl.getCatalogVersion() >= fromVersion) {
                        try {
                            catalogTbl.setTable(tbl.toThrift());
                        } catch (Exception e) {
                            if (LOG.isTraceEnabled()) {
                                LOG.trace(String.format("Error calling toThrift() on table %s.%s: %s", db.getName(), tblName, e.getMessage()), e);
                            }
                            continue;
                        }
                        catalogTbl.setCatalog_version(tbl.getCatalogVersion());
                    } else {
                        catalogTbl.setTable(new TTable(db.getName(), tblName));
                    }
                } finally {
                    tbl.getLock().unlock();
                }
                resp.addToObjects(catalogTbl);
            }
            for (Function fn : db.getFunctions(null, new PatternMatcher())) {
                TCatalogObject function = new TCatalogObject(TCatalogObjectType.FUNCTION, fn.getCatalogVersion());
                function.setFn(fn.toThrift());
                resp.addToObjects(function);
            }
        }
        for (DataSource dataSource : getDataSources()) {
            TCatalogObject catalogObj = new TCatalogObject(TCatalogObjectType.DATA_SOURCE, dataSource.getCatalogVersion());
            catalogObj.setData_source(dataSource.toThrift());
            resp.addToObjects(catalogObj);
        }
        for (HdfsCachePool cachePool : hdfsCachePools_) {
            TCatalogObject pool = new TCatalogObject(TCatalogObjectType.HDFS_CACHE_POOL, cachePool.getCatalogVersion());
            pool.setCache_pool(cachePool.toThrift());
            resp.addToObjects(pool);
        }
        // Get all roles
        for (Role role : authPolicy_.getAllRoles()) {
            TCatalogObject thriftRole = new TCatalogObject();
            thriftRole.setRole(role.toThrift());
            thriftRole.setCatalog_version(role.getCatalogVersion());
            thriftRole.setType(role.getCatalogObjectType());
            resp.addToObjects(thriftRole);
            for (RolePrivilege p : role.getPrivileges()) {
                TCatalogObject privilege = new TCatalogObject();
                privilege.setPrivilege(p.toThrift());
                privilege.setCatalog_version(p.getCatalogVersion());
                privilege.setType(p.getCatalogObjectType());
                resp.addToObjects(privilege);
            }
        }
        // Each update should contain a single "TCatalog" object which is used to
        // pass overall state on the catalog, such as the current version and the
        // catalog service id.
        TCatalogObject catalog = new TCatalogObject();
        catalog.setType(TCatalogObjectType.CATALOG);
        // By setting the catalog version to the latest catalog version at this point,
        // it ensure impalads will always bump their versions, even in the case where
        // an object has been dropped.
        catalog.setCatalog_version(getCatalogVersion());
        catalog.setCatalog(new TCatalog(catalogServiceId_));
        resp.addToObjects(catalog);
        // The max version is the max catalog version of all items in the update.
        resp.setMax_catalog_version(getCatalogVersion());
        return resp;
    } finally {
        catalogLock_.readLock().unlock();
    }
}
#end_block

#method_before
private void loadFunctionsFromDbParams(Db db, org.apache.hadoop.hive.metastore.api.Database msDb) {
    if (msDb == null || msDb.getParameters() == null)
        return;
    if (LOG.isTraceEnabled()) {
        LOG.trace("Loading native functions for database: " + db.getName());
    }
    TCompactProtocol.Factory protocolFactory = new TCompactProtocol.Factory();
    for (String key : msDb.getParameters().keySet()) {
        if (!key.startsWith(Db.FUNCTION_INDEX_PREFIX))
            continue;
        try {
            TFunction fn = new TFunction();
            JniUtil.deserializeThrift(protocolFactory, fn, Base64.decodeBase64(msDb.getParameters().get(key)));
            Function addFn = Function.fromThrift(fn);
            db.addFunction(addFn, false);
            addFn.setCatalogVersion(incrementAndGetCatalogVersion());
        } catch (ImpalaException e) {
            LOG.error("Encountered an error during function load: key=" + key + ",continuing", e);
        }
    }
}
#method_after
private void loadFunctionsFromDbParams(Db db, org.apache.hadoop.hive.metastore.api.Database msDb) {
    if (msDb == null || msDb.getParameters() == null)
        return;
    LOG.info("Loading native functions for database: " + db.getName());
    TCompactProtocol.Factory protocolFactory = new TCompactProtocol.Factory();
    for (String key : msDb.getParameters().keySet()) {
        if (!key.startsWith(Db.FUNCTION_INDEX_PREFIX))
            continue;
        try {
            TFunction fn = new TFunction();
            JniUtil.deserializeThrift(protocolFactory, fn, Base64.decodeBase64(msDb.getParameters().get(key)));
            Function addFn = Function.fromThrift(fn);
            db.addFunction(addFn, false);
            addFn.setCatalogVersion(incrementAndGetCatalogVersion());
        } catch (ImpalaException e) {
            LOG.error("Encountered an error during function load: key=" + key + ",continuing", e);
        }
    }
    LOG.info("Loaded native functions for database: " + db.getName());
}
#end_block

#method_before
private void loadJavaFunctions(Db db, List<org.apache.hadoop.hive.metastore.api.Function> functions) {
    Preconditions.checkNotNull(functions);
    if (LOG.isTraceEnabled()) {
        LOG.trace("Loading Java functions for database: " + db.getName());
    }
    for (org.apache.hadoop.hive.metastore.api.Function function : functions) {
        try {
            for (Function fn : extractFunctions(db.getName(), function)) {
                db.addFunction(fn);
                fn.setCatalogVersion(incrementAndGetCatalogVersion());
            }
        } catch (Exception e) {
            LOG.error("Skipping function load: " + function.getFunctionName(), e);
        }
    }
}
#method_after
private void loadJavaFunctions(Db db, List<org.apache.hadoop.hive.metastore.api.Function> functions) {
    Preconditions.checkNotNull(functions);
    LOG.info("Loading Java functions for database: " + db.getName());
    for (org.apache.hadoop.hive.metastore.api.Function function : functions) {
        try {
            for (Function fn : extractFunctions(db.getName(), function)) {
                db.addFunction(fn);
                fn.setCatalogVersion(incrementAndGetCatalogVersion());
            }
        } catch (Exception e) {
            LOG.error("Skipping function load: " + function.getFunctionName(), e);
        }
    }
    LOG.info("Loaded Java functions for database: " + db.getName());
}
#end_block

#method_before
public void reset() throws CatalogException {
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                dbName = dbName.toLowerCase();
                Db oldDb = oldDbCache.get(dbName);
                Pair<Db, List<TTableName>> invalidatedDb = invalidateDb(msClient, dbName, oldDb);
                if (invalidatedDb == null)
                    continue;
                newDbCache.put(dbName, invalidatedDb.first);
                tblsToBackgroundLoad.addAll(invalidatedDb.second);
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#method_after
public void reset() throws CatalogException {
    LOG.info("Invalidating all metadata.");
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                dbName = dbName.toLowerCase();
                Db oldDb = oldDbCache.get(dbName);
                Pair<Db, List<TTableName>> invalidatedDb = invalidateDb(msClient, dbName, oldDb);
                if (invalidatedDb == null)
                    continue;
                newDbCache.put(dbName, invalidatedDb.first);
                tblsToBackgroundLoad.addAll(invalidatedDb.second);
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
    LOG.info("Invalidated all metadata.");
}
#end_block

#method_before
public Table reloadTable(Table tbl) throws CatalogException {
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Refreshing table metadata: %s", tbl.getFullName()));
    }
    TTableName tblName = new TTableName(tbl.getDb().getName().toLowerCase(), tbl.getName().toLowerCase());
    Db db = tbl.getDb();
    if (tbl instanceof IncompleteTable) {
        TableLoadingMgr.LoadRequest loadReq;
        long previousCatalogVersion;
        // Return the table if it is already loaded or submit a new load request.
        catalogLock_.readLock().lock();
        try {
            previousCatalogVersion = tbl.getCatalogVersion();
            loadReq = tableLoadingMgr_.loadAsync(tblName);
        } finally {
            catalogLock_.readLock().unlock();
        }
        Preconditions.checkNotNull(loadReq);
        try {
            // only apply the update if the existing table hasn't changed.
            return replaceTableIfUnchanged(loadReq.get(), previousCatalogVersion);
        } finally {
            loadReq.close();
        }
    }
    while (true) {
        catalogLock_.writeLock().lock();
        if (tbl.getLock().writeLock().tryLock()) {
            try {
                long newCatalogVersion = incrementAndGetCatalogVersion();
                catalogLock_.writeLock().unlock();
                try (MetaStoreClient msClient = getMetaStoreClient()) {
                    org.apache.hadoop.hive.metastore.api.Table msTbl = null;
                    try {
                        msTbl = msClient.getHiveClient().getTable(db.getName(), tblName.getTable_name());
                    } catch (Exception e) {
                        throw new TableLoadingException("Error loading metadata for table: " + db.getName() + "." + tblName.getTable_name(), e);
                    }
                    tbl.load(false, msClient.getHiveClient(), msTbl);
                }
                tbl.setCatalogVersion(newCatalogVersion);
                return tbl;
            } finally {
                tbl.getLock().writeLock().unlock();
            }
        } else {
            catalogLock_.writeLock().unlock();
            continue;
        }
    }
}
#method_after
public Table reloadTable(Table tbl) throws CatalogException {
    LOG.info(String.format("Refreshing table metadata: %s", tbl.getFullName()));
    TTableName tblName = new TTableName(tbl.getDb().getName().toLowerCase(), tbl.getName().toLowerCase());
    Db db = tbl.getDb();
    if (tbl instanceof IncompleteTable) {
        TableLoadingMgr.LoadRequest loadReq;
        long previousCatalogVersion;
        // Return the table if it is already loaded or submit a new load request.
        catalogLock_.readLock().lock();
        try {
            previousCatalogVersion = tbl.getCatalogVersion();
            loadReq = tableLoadingMgr_.loadAsync(tblName);
        } finally {
            catalogLock_.readLock().unlock();
        }
        Preconditions.checkNotNull(loadReq);
        try {
            // only apply the update if the existing table hasn't changed.
            return replaceTableIfUnchanged(loadReq.get(), previousCatalogVersion);
        } finally {
            loadReq.close();
            LOG.info(String.format("Refreshed table metadata: %s", tbl.getFullName()));
        }
    }
    if (!tryLockTable(tbl)) {
        throw new CatalogException(String.format("Error refreshing metadata for table " + "%s due to lock contention", tbl.getFullName()));
    }
    try {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = null;
            try {
                msTbl = msClient.getHiveClient().getTable(db.getName(), tblName.getTable_name());
            } catch (Exception e) {
                throw new TableLoadingException("Error loading metadata for table: " + db.getName() + "." + tblName.getTable_name(), e);
            }
            tbl.load(false, msClient.getHiveClient(), msTbl);
        }
        tbl.setCatalogVersion(newCatalogVersion);
        LOG.info(String.format("Refreshed table metadata: %s", tbl.getFullName()));
        return tbl;
    } finally {
        Preconditions.checkState(!catalogLock_.isWriteLockedByCurrentThread());
        tbl.getLock().unlock();
    }
}
#end_block

#method_before
public Table dropPartitions(Table tbl, List<List<TPartitionKeyValue>> partitionSet) throws CatalogException {
    Preconditions.checkNotNull(tbl);
    Preconditions.checkNotNull(partitionSet);
    Preconditions.checkArgument(tbl.getLock().isWriteLockedByCurrentThread());
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an Hdfs table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    List<HdfsPartition> partitions = hdfsTable.getPartitionsFromPartitionSet(partitionSet);
    hdfsTable.dropPartitions(partitions);
    return hdfsTable;
}
#method_after
public Table dropPartitions(Table tbl, List<List<TPartitionKeyValue>> partitionSet) throws CatalogException {
    Preconditions.checkNotNull(tbl);
    Preconditions.checkNotNull(partitionSet);
    Preconditions.checkArgument(tbl.getLock().isHeldByCurrentThread());
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an Hdfs table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    List<HdfsPartition> partitions = hdfsTable.getPartitionsFromPartitionSet(partitionSet);
    hdfsTable.dropPartitions(partitions);
    return hdfsTable;
}
#end_block

#method_before
public Table dropPartition(Table tbl, List<TPartitionKeyValue> partitionSpec) throws CatalogException {
    Preconditions.checkNotNull(tbl);
    Preconditions.checkNotNull(partitionSpec);
    Preconditions.checkArgument(tbl.getLock().isWriteLockedByCurrentThread());
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an Hdfs table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    if (hdfsTable.dropPartition(partitionSpec) == null)
        return null;
    return hdfsTable;
}
#method_after
public Table dropPartition(Table tbl, List<TPartitionKeyValue> partitionSpec) throws CatalogException {
    Preconditions.checkNotNull(tbl);
    Preconditions.checkNotNull(partitionSpec);
    Preconditions.checkArgument(tbl.getLock().isHeldByCurrentThread());
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an Hdfs table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    if (hdfsTable.dropPartition(partitionSpec) == null)
        return null;
    return hdfsTable;
}
#end_block

#method_before
public boolean invalidateTable(TTableName tableName, Pair<Db, Table> updatedObjects) {
    Preconditions.checkNotNull(updatedObjects);
    updatedObjects.first = null;
    updatedObjects.second = null;
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Invalidating table metadata: %s.%s", tableName.getDb_name(), tableName.getTable_name()));
    }
    String dbName = tableName.getDb_name();
    String tblName = tableName.getTable_name();
    // Stores whether the table exists in the metastore. Can have three states:
    // 1) true - Table exists in metastore.
    // 2) false - Table does not exist in metastore.
    // 3) unknown (null) - There was exception thrown by the metastore client.
    Boolean tableExistsInMetaStore;
    Db db = null;
    try (MetaStoreClient msClient = getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Database msDb = null;
        try {
            tableExistsInMetaStore = msClient.getHiveClient().tableExists(dbName, tblName);
        } catch (UnknownDBException e) {
            // The parent database does not exist in the metastore. Treat this the same
            // as if the table does not exist.
            tableExistsInMetaStore = false;
        } catch (TException e) {
            LOG.error("Error executing tableExists() metastore call: " + tblName, e);
            tableExistsInMetaStore = null;
        }
        if (tableExistsInMetaStore != null && !tableExistsInMetaStore) {
            updatedObjects.second = removeTable(dbName, tblName);
            return true;
        }
        db = getDb(dbName);
        if ((db == null || !db.containsTable(tblName)) && tableExistsInMetaStore == null) {
            // table exists in the metastore. Do nothing.
            return false;
        } else if (db == null && tableExistsInMetaStore) {
            // must be valid since tableExistsInMetaStore is true.
            try {
                msDb = msClient.getHiveClient().getDatabase(dbName);
                Preconditions.checkNotNull(msDb);
                db = new Db(dbName, this, msDb);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                addDb(db);
                updatedObjects.first = db;
            } catch (TException e) {
                // The metastore database cannot be get. Log the error and return.
                LOG.error("Error executing getDatabase() metastore call: " + dbName, e);
                return false;
            }
        }
    }
    // Add a new uninitialized table to the table cache, effectively invalidating
    // any existing entry. The metadata for the table will be loaded lazily, on the
    // on the next access to the table.
    Table newTable = IncompleteTable.createUninitializedTable(db, tblName);
    newTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(newTable);
    if (loadInBackground_) {
        tableLoadingMgr_.backgroundLoad(new TTableName(dbName.toLowerCase(), tblName.toLowerCase()));
    }
    updatedObjects.second = newTable;
    return false;
}
#method_after
public boolean invalidateTable(TTableName tableName, Pair<Db, Table> updatedObjects) {
    Preconditions.checkNotNull(updatedObjects);
    updatedObjects.first = null;
    updatedObjects.second = null;
    String dbName = tableName.getDb_name();
    String tblName = tableName.getTable_name();
    LOG.info(String.format("Invalidating table metadata: %s.%s", dbName, tblName));
    // Stores whether the table exists in the metastore. Can have three states:
    // 1) true - Table exists in metastore.
    // 2) false - Table does not exist in metastore.
    // 3) unknown (null) - There was exception thrown by the metastore client.
    Boolean tableExistsInMetaStore;
    Db db = null;
    try (MetaStoreClient msClient = getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Database msDb = null;
        try {
            tableExistsInMetaStore = msClient.getHiveClient().tableExists(dbName, tblName);
        } catch (UnknownDBException e) {
            // The parent database does not exist in the metastore. Treat this the same
            // as if the table does not exist.
            tableExistsInMetaStore = false;
        } catch (TException e) {
            LOG.error("Error executing tableExists() metastore call: " + tblName, e);
            tableExistsInMetaStore = null;
        }
        if (tableExistsInMetaStore != null && !tableExistsInMetaStore) {
            updatedObjects.second = removeTable(dbName, tblName);
            return true;
        }
        db = getDb(dbName);
        if ((db == null || !db.containsTable(tblName)) && tableExistsInMetaStore == null) {
            // table exists in the metastore. Do nothing.
            return false;
        } else if (db == null && tableExistsInMetaStore) {
            // must be valid since tableExistsInMetaStore is true.
            try {
                msDb = msClient.getHiveClient().getDatabase(dbName);
                Preconditions.checkNotNull(msDb);
                db = new Db(dbName, this, msDb);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                addDb(db);
                updatedObjects.first = db;
            } catch (TException e) {
                // The metastore database cannot be get. Log the error and return.
                LOG.error("Error executing getDatabase() metastore call: " + dbName, e);
                return false;
            }
        }
    }
    // Add a new uninitialized table to the table cache, effectively invalidating
    // any existing entry. The metadata for the table will be loaded lazily, on the
    // on the next access to the table.
    Table newTable = IncompleteTable.createUninitializedTable(db, tblName);
    newTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(newTable);
    if (loadInBackground_) {
        tableLoadingMgr_.backgroundLoad(new TTableName(dbName.toLowerCase(), tblName.toLowerCase()));
    }
    updatedObjects.second = newTable;
    return false;
}
#end_block

#method_before
public Table reloadPartition(Table tbl, List<TPartitionKeyValue> partitionSpec) throws CatalogException {
    while (true) {
        catalogLock_.writeLock().lock();
        if (tbl.getLock().writeLock().tryLock()) {
            try {
                long newCatalogVersion = incrementAndGetCatalogVersion();
                catalogLock_.writeLock().unlock();
                HdfsTable hdfsTable = (HdfsTable) tbl;
                HdfsPartition hdfsPartition = hdfsTable.getPartitionFromThriftPartitionSpec(partitionSpec);
                // Retrieve partition name from existing partition or construct it from
                // the partition spec
                String partitionName = hdfsPartition == null ? HdfsTable.constructPartitionName(partitionSpec) : hdfsPartition.getPartitionName();
                if (LOG.isTraceEnabled()) {
                    LOG.trace(String.format("Refreshing Partition metadata: %s %s", hdfsTable.getFullName(), partitionName));
                }
                try (MetaStoreClient msClient = getMetaStoreClient()) {
                    org.apache.hadoop.hive.metastore.api.Partition hmsPartition = null;
                    try {
                        hmsPartition = msClient.getHiveClient().getPartition(hdfsTable.getDb().getName(), hdfsTable.getName(), partitionName);
                    } catch (NoSuchObjectException e) {
                        // catalog
                        if (hdfsPartition != null) {
                            hdfsTable.dropPartition(partitionSpec);
                            hdfsTable.setCatalogVersion(newCatalogVersion);
                        }
                        return hdfsTable;
                    } catch (Exception e) {
                        throw new CatalogException("Error loading metadata for partition: " + hdfsTable.getFullName() + " " + partitionName, e);
                    }
                    hdfsTable.reloadPartition(hdfsPartition, hmsPartition);
                }
                hdfsTable.setCatalogVersion(newCatalogVersion);
                return hdfsTable;
            } finally {
                tbl.getLock().writeLock().unlock();
            }
        } else {
            catalogLock_.writeLock().unlock();
            continue;
        }
    }
}
#method_after
public Table reloadPartition(Table tbl, List<TPartitionKeyValue> partitionSpec) throws CatalogException {
    if (!tryLockTable(tbl)) {
        throw new CatalogException(String.format("Error reloading partition of table %s " + "due to lock contention", tbl.getFullName()));
    }
    try {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        HdfsTable hdfsTable = (HdfsTable) tbl;
        HdfsPartition hdfsPartition = hdfsTable.getPartitionFromThriftPartitionSpec(partitionSpec);
        // Retrieve partition name from existing partition or construct it from
        // the partition spec
        String partitionName = hdfsPartition == null ? HdfsTable.constructPartitionName(partitionSpec) : hdfsPartition.getPartitionName();
        LOG.info(String.format("Refreshing partition metadata: %s %s", hdfsTable.getFullName(), partitionName));
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Partition hmsPartition = null;
            try {
                hmsPartition = msClient.getHiveClient().getPartition(hdfsTable.getDb().getName(), hdfsTable.getName(), partitionName);
            } catch (NoSuchObjectException e) {
                // catalog
                if (hdfsPartition != null) {
                    hdfsTable.dropPartition(partitionSpec);
                    hdfsTable.setCatalogVersion(newCatalogVersion);
                }
                return hdfsTable;
            } catch (Exception e) {
                throw new CatalogException("Error loading metadata for partition: " + hdfsTable.getFullName() + " " + partitionName, e);
            }
            hdfsTable.reloadPartition(hdfsPartition, hmsPartition);
        }
        hdfsTable.setCatalogVersion(newCatalogVersion);
        LOG.info(String.format("Refreshed partition metadata: %s %s", hdfsTable.getFullName(), partitionName));
        return hdfsTable;
    } finally {
        Preconditions.checkState(!catalogLock_.isWriteLockedByCurrentThread());
        tbl.getLock().unlock();
    }
}
#end_block

#method_before
public ReentrantReadWriteLock getLock() {
    return tableLock_;
}
#method_after
public ReentrantLock getLock() {
    return tableLock_;
}
#end_block

#method_before
private void alterTable(TAlterTableParams params, TDdlExecResponse response) throws ImpalaException {
    // When true, loads the file/block metadata.
    boolean reloadFileMetadata = false;
    // When true, loads the table schema and the column stats from the Hive Metastore.
    boolean reloadTableSchema = false;
    // When true, sets the result to be reported to the client.
    boolean setResultSet = false;
    TColumnValue resultColVal = new TColumnValue();
    Reference<Long> numUpdatedPartitions = new Reference<>(0L);
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Table tbl = getExistingTable(tableName.getDb(), tableName.getTbl());
    while (true) {
        catalog_.getLock().writeLock().lock();
        if (tbl.getLock().writeLock().tryLock()) {
            try {
                if (params.getAlter_type() == TAlterTableType.RENAME_VIEW || params.getAlter_type() == TAlterTableType.RENAME_TABLE) {
                    // the catalog lock.
                    try {
                        alterTableOrViewRename(tbl, TableName.fromThrift(params.getRename_params().getNew_table_name()), response);
                        return;
                    } finally {
                        catalog_.getLock().writeLock().unlock();
                    }
                }
                // Get a new catalog version to assign to the table being altered.
                long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
                boolean reloadMetadata = true;
                catalog_.getLock().writeLock().unlock();
                if (tbl instanceof KuduTable && altersKuduTable(params.getAlter_type())) {
                    alterKuduTable(params, response, (KuduTable) tbl, newCatalogVersion);
                    return;
                }
                switch(params.getAlter_type()) {
                    case ADD_REPLACE_COLUMNS:
                        TAlterTableAddReplaceColsParams addReplaceColParams = params.getAdd_replace_cols_params();
                        alterTableAddReplaceCols(tbl, addReplaceColParams.getColumns(), addReplaceColParams.isReplace_existing_cols());
                        reloadTableSchema = true;
                        break;
                    case ADD_PARTITION:
                        TAlterTableAddPartitionParams addPartParams = params.getAdd_partition_params();
                        // Create and add HdfsPartition object to the corresponding HdfsTable and
                        // load its block metadata. Get the new table object with an updated catalog
                        // version. If the partition already exists in Hive and "IfNotExists" is
                        // true, then return without populating the response object.
                        Table refreshedTable = alterTableAddPartition(tbl, addPartParams.getPartition_spec(), addPartParams.isIf_not_exists(), addPartParams.getLocation(), addPartParams.getCache_op());
                        if (refreshedTable != null) {
                            refreshedTable.setCatalogVersion(newCatalogVersion);
                            addTableToCatalogUpdate(refreshedTable, response.result);
                        }
                        reloadMetadata = false;
                        break;
                    case DROP_COLUMN:
                        TAlterTableDropColParams dropColParams = params.getDrop_col_params();
                        alterTableDropCol(tbl, dropColParams.getCol_name());
                        reloadTableSchema = true;
                        break;
                    case CHANGE_COLUMN:
                        TAlterTableChangeColParams changeColParams = params.getChange_col_params();
                        alterTableChangeCol(tbl, changeColParams.getCol_name(), changeColParams.getNew_col_def());
                        reloadTableSchema = true;
                        break;
                    case DROP_PARTITION:
                        TAlterTableDropPartitionParams dropPartParams = params.getDrop_partition_params();
                        // Drop the partition from the corresponding table. Get the table object
                        // with an updated catalog version. If the partition does not exist and
                        // "IfExists" is true, null is returned. If "purge" option is specified
                        // partition data is purged by skipping Trash, if configured.
                        refreshedTable = alterTableDropPartition(tbl, dropPartParams.getPartition_set(), dropPartParams.isIf_exists(), dropPartParams.isPurge(), numUpdatedPartitions);
                        if (refreshedTable != null) {
                            refreshedTable.setCatalogVersion(newCatalogVersion);
                            addTableToCatalogUpdate(refreshedTable, response.result);
                        }
                        resultColVal.setString_val("Dropped " + numUpdatedPartitions.getRef() + " partition(s).");
                        setResultSet = true;
                        reloadMetadata = false;
                        break;
                    case RENAME_TABLE:
                    case RENAME_VIEW:
                        Preconditions.checkState(false, "RENAME TABLE/VIEW operation has been processed");
                        break;
                    case SET_FILE_FORMAT:
                        TAlterTableSetFileFormatParams fileFormatParams = params.getSet_file_format_params();
                        reloadFileMetadata = alterTableSetFileFormat(tbl, fileFormatParams.getPartition_set(), fileFormatParams.getFile_format(), numUpdatedPartitions);
                        if (fileFormatParams.isSetPartition_set()) {
                            resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s).");
                        } else {
                            resultColVal.setString_val("Updated table.");
                        }
                        setResultSet = true;
                        break;
                    case SET_LOCATION:
                        TAlterTableSetLocationParams setLocationParams = params.getSet_location_params();
                        reloadFileMetadata = alterTableSetLocation(tbl, setLocationParams.getPartition_spec(), setLocationParams.getLocation());
                        break;
                    case SET_TBL_PROPERTIES:
                        alterTableSetTblProperties(tbl, params.getSet_tbl_properties_params(), numUpdatedPartitions);
                        if (params.getSet_tbl_properties_params().isSetPartition_set()) {
                            resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s).");
                        } else {
                            resultColVal.setString_val("Updated table.");
                        }
                        setResultSet = true;
                        break;
                    case UPDATE_STATS:
                        Preconditions.checkState(params.isSetUpdate_stats_params());
                        Reference<Long> numUpdatedColumns = new Reference<>(0L);
                        alterTableUpdateStats(tbl, params.getUpdate_stats_params(), response, numUpdatedPartitions, numUpdatedColumns);
                        reloadTableSchema = true;
                        resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s) and " + numUpdatedColumns.getRef() + " column(s).");
                        setResultSet = true;
                        break;
                    case SET_CACHED:
                        Preconditions.checkState(params.isSetSet_cached_params());
                        String op = params.getSet_cached_params().getCache_op().isSet_cached() ? "Cached " : "Uncached ";
                        if (params.getSet_cached_params().getPartition_set() == null) {
                            reloadFileMetadata = alterTableSetCached(tbl, params.getSet_cached_params());
                            resultColVal.setString_val(op + "table.");
                        } else {
                            alterPartitionSetCached(tbl, params.getSet_cached_params(), numUpdatedPartitions);
                            resultColVal.setString_val(op + numUpdatedPartitions.getRef() + " partition(s).");
                        }
                        setResultSet = true;
                        break;
                    case RECOVER_PARTITIONS:
                        alterTableRecoverPartitions(tbl);
                        break;
                    default:
                        throw new UnsupportedOperationException("Unknown ALTER TABLE operation type: " + params.getAlter_type());
                }
                if (reloadMetadata) {
                    loadTableMetadata(tbl, newCatalogVersion, reloadFileMetadata, reloadTableSchema, null);
                    addTableToCatalogUpdate(tbl, response.result);
                }
                if (setResultSet) {
                    TResultSet resultSet = new TResultSet();
                    resultSet.setSchema(new TResultSetMetadata(Lists.newArrayList(new TColumn("summary", Type.STRING.toThrift()))));
                    TResultRow resultRow = new TResultRow();
                    resultRow.setColVals(Lists.newArrayList(resultColVal));
                    resultSet.setRows(Lists.newArrayList(resultRow));
                    response.setResult_set(resultSet);
                }
            } finally {
                tbl.getLock().writeLock().unlock();
            }
        } else {
            catalog_.getLock().writeLock().unlock();
            continue;
        }
        break;
    }
}
#method_after
private void alterTable(TAlterTableParams params, TDdlExecResponse response) throws ImpalaException {
    // When true, loads the file/block metadata.
    boolean reloadFileMetadata = false;
    // When true, loads the table schema and the column stats from the Hive Metastore.
    boolean reloadTableSchema = false;
    // When true, sets the result to be reported to the client.
    boolean setResultSet = false;
    TColumnValue resultColVal = new TColumnValue();
    Reference<Long> numUpdatedPartitions = new Reference<>(0L);
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Table tbl = getExistingTable(tableName.getDb(), tableName.getTbl());
    if (!catalog_.tryLockTable(tbl)) {
        throw new InternalException(String.format("Error altering table %s due to lock " + "contention.", tbl.getFullName()));
    }
    try {
        if (params.getAlter_type() == TAlterTableType.RENAME_VIEW || params.getAlter_type() == TAlterTableType.RENAME_TABLE) {
            // the catalog lock.
            try {
                alterTableOrViewRename(tbl, TableName.fromThrift(params.getRename_params().getNew_table_name()), response);
                return;
            } finally {
                catalog_.getLock().writeLock().unlock();
            }
        }
        // Get a new catalog version to assign to the table being altered.
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        boolean reloadMetadata = true;
        catalog_.getLock().writeLock().unlock();
        if (tbl instanceof KuduTable && altersKuduTable(params.getAlter_type())) {
            alterKuduTable(params, response, (KuduTable) tbl, newCatalogVersion);
            return;
        }
        switch(params.getAlter_type()) {
            case ADD_REPLACE_COLUMNS:
                TAlterTableAddReplaceColsParams addReplaceColParams = params.getAdd_replace_cols_params();
                alterTableAddReplaceCols(tbl, addReplaceColParams.getColumns(), addReplaceColParams.isReplace_existing_cols());
                reloadTableSchema = true;
                break;
            case ADD_PARTITION:
                TAlterTableAddPartitionParams addPartParams = params.getAdd_partition_params();
                // Create and add HdfsPartition object to the corresponding HdfsTable and
                // load its block metadata. Get the new table object with an updated catalog
                // version. If the partition already exists in Hive and "IfNotExists" is
                // true, then return without populating the response object.
                Table refreshedTable = alterTableAddPartition(tbl, addPartParams.getPartition_spec(), addPartParams.isIf_not_exists(), addPartParams.getLocation(), addPartParams.getCache_op());
                if (refreshedTable != null) {
                    refreshedTable.setCatalogVersion(newCatalogVersion);
                    addTableToCatalogUpdate(refreshedTable, response.result);
                }
                reloadMetadata = false;
                break;
            case DROP_COLUMN:
                TAlterTableDropColParams dropColParams = params.getDrop_col_params();
                alterTableDropCol(tbl, dropColParams.getCol_name());
                reloadTableSchema = true;
                break;
            case CHANGE_COLUMN:
                TAlterTableChangeColParams changeColParams = params.getChange_col_params();
                alterTableChangeCol(tbl, changeColParams.getCol_name(), changeColParams.getNew_col_def());
                reloadTableSchema = true;
                break;
            case DROP_PARTITION:
                TAlterTableDropPartitionParams dropPartParams = params.getDrop_partition_params();
                // Drop the partition from the corresponding table. Get the table object
                // with an updated catalog version. If the partition does not exist and
                // "IfExists" is true, null is returned. If "purge" option is specified
                // partition data is purged by skipping Trash, if configured.
                refreshedTable = alterTableDropPartition(tbl, dropPartParams.getPartition_set(), dropPartParams.isIf_exists(), dropPartParams.isPurge(), numUpdatedPartitions);
                if (refreshedTable != null) {
                    refreshedTable.setCatalogVersion(newCatalogVersion);
                    addTableToCatalogUpdate(refreshedTable, response.result);
                }
                resultColVal.setString_val("Dropped " + numUpdatedPartitions.getRef() + " partition(s).");
                setResultSet = true;
                reloadMetadata = false;
                break;
            case RENAME_TABLE:
            case RENAME_VIEW:
                Preconditions.checkState(false, "RENAME TABLE/VIEW operation has been processed");
                break;
            case SET_FILE_FORMAT:
                TAlterTableSetFileFormatParams fileFormatParams = params.getSet_file_format_params();
                reloadFileMetadata = alterTableSetFileFormat(tbl, fileFormatParams.getPartition_set(), fileFormatParams.getFile_format(), numUpdatedPartitions);
                if (fileFormatParams.isSetPartition_set()) {
                    resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s).");
                } else {
                    resultColVal.setString_val("Updated table.");
                }
                setResultSet = true;
                break;
            case SET_LOCATION:
                TAlterTableSetLocationParams setLocationParams = params.getSet_location_params();
                reloadFileMetadata = alterTableSetLocation(tbl, setLocationParams.getPartition_spec(), setLocationParams.getLocation());
                break;
            case SET_TBL_PROPERTIES:
                alterTableSetTblProperties(tbl, params.getSet_tbl_properties_params(), numUpdatedPartitions);
                if (params.getSet_tbl_properties_params().isSetPartition_set()) {
                    resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s).");
                } else {
                    resultColVal.setString_val("Updated table.");
                }
                setResultSet = true;
                break;
            case UPDATE_STATS:
                Preconditions.checkState(params.isSetUpdate_stats_params());
                Reference<Long> numUpdatedColumns = new Reference<>(0L);
                alterTableUpdateStats(tbl, params.getUpdate_stats_params(), response, numUpdatedPartitions, numUpdatedColumns);
                reloadTableSchema = true;
                resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s) and " + numUpdatedColumns.getRef() + " column(s).");
                setResultSet = true;
                break;
            case SET_CACHED:
                Preconditions.checkState(params.isSetSet_cached_params());
                String op = params.getSet_cached_params().getCache_op().isSet_cached() ? "Cached " : "Uncached ";
                if (params.getSet_cached_params().getPartition_set() == null) {
                    reloadFileMetadata = alterTableSetCached(tbl, params.getSet_cached_params());
                    resultColVal.setString_val(op + "table.");
                } else {
                    alterPartitionSetCached(tbl, params.getSet_cached_params(), numUpdatedPartitions);
                    resultColVal.setString_val(op + numUpdatedPartitions.getRef() + " partition(s).");
                }
                setResultSet = true;
                break;
            case RECOVER_PARTITIONS:
                alterTableRecoverPartitions(tbl);
                break;
            default:
                throw new UnsupportedOperationException("Unknown ALTER TABLE operation type: " + params.getAlter_type());
        }
        if (reloadMetadata) {
            loadTableMetadata(tbl, newCatalogVersion, reloadFileMetadata, reloadTableSchema, null);
            addTableToCatalogUpdate(tbl, response.result);
        }
        if (setResultSet) {
            TResultSet resultSet = new TResultSet();
            resultSet.setSchema(new TResultSetMetadata(Lists.newArrayList(new TColumn("summary", Type.STRING.toThrift()))));
            TResultRow resultRow = new TResultRow();
            resultRow.setColVals(Lists.newArrayList(resultColVal));
            resultSet.setRows(Lists.newArrayList(resultRow));
            response.setResult_set(resultSet);
        }
    } finally {
        Preconditions.checkState(!catalog_.getLock().isWriteLockedByCurrentThread());
        tbl.getLock().unlock();
    }
}
#end_block

#method_before
private void alterKuduTable(TAlterTableParams params, TDdlExecResponse response, KuduTable tbl, long newCatalogVersion) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    switch(params.getAlter_type()) {
        case ADD_REPLACE_COLUMNS:
            TAlterTableAddReplaceColsParams addReplaceColParams = params.getAdd_replace_cols_params();
            KuduCatalogOpExecutor.addColumn((KuduTable) tbl, addReplaceColParams.getColumns());
            break;
        case DROP_COLUMN:
            TAlterTableDropColParams dropColParams = params.getDrop_col_params();
            KuduCatalogOpExecutor.dropColumn((KuduTable) tbl, dropColParams.getCol_name());
            break;
        case CHANGE_COLUMN:
            TAlterTableChangeColParams changeColParams = params.getChange_col_params();
            KuduCatalogOpExecutor.renameColumn((KuduTable) tbl, changeColParams.getCol_name(), changeColParams.getNew_col_def());
            break;
        case ADD_DROP_RANGE_PARTITION:
            TAlterTableAddDropRangePartitionParams partParams = params.getAdd_drop_range_partition_params();
            KuduCatalogOpExecutor.addDropRangePartition((KuduTable) tbl, partParams);
            break;
        default:
            throw new UnsupportedOperationException("Unsupported ALTER TABLE operation for Kudu tables: " + params.getAlter_type());
    }
    loadTableMetadata(tbl, newCatalogVersion, true, true, null);
    addTableToCatalogUpdate(tbl, response.result);
}
#method_after
private void alterKuduTable(TAlterTableParams params, TDdlExecResponse response, KuduTable tbl, long newCatalogVersion) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    switch(params.getAlter_type()) {
        case ADD_REPLACE_COLUMNS:
            TAlterTableAddReplaceColsParams addReplaceColParams = params.getAdd_replace_cols_params();
            KuduCatalogOpExecutor.addColumn((KuduTable) tbl, addReplaceColParams.getColumns());
            break;
        case DROP_COLUMN:
            TAlterTableDropColParams dropColParams = params.getDrop_col_params();
            KuduCatalogOpExecutor.dropColumn((KuduTable) tbl, dropColParams.getCol_name());
            break;
        case CHANGE_COLUMN:
            TAlterTableChangeColParams changeColParams = params.getChange_col_params();
            KuduCatalogOpExecutor.renameColumn((KuduTable) tbl, changeColParams.getCol_name(), changeColParams.getNew_col_def());
            break;
        case ADD_DROP_RANGE_PARTITION:
            TAlterTableAddDropRangePartitionParams partParams = params.getAdd_drop_range_partition_params();
            KuduCatalogOpExecutor.addDropRangePartition((KuduTable) tbl, partParams);
            break;
        default:
            throw new UnsupportedOperationException("Unsupported ALTER TABLE operation for Kudu tables: " + params.getAlter_type());
    }
    loadTableMetadata(tbl, newCatalogVersion, true, true, null);
    addTableToCatalogUpdate(tbl, response.result);
}
#end_block

#method_before
private void loadTableMetadata(Table tbl, long newCatalogVersion, boolean reloadFileMetadata, boolean reloadTableSchema, Set<String> partitionsToUpdate) throws CatalogException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(msClient, tbl);
        if (tbl instanceof HdfsTable) {
            ((HdfsTable) tbl).load(true, msClient.getHiveClient(), msTbl, reloadFileMetadata, reloadTableSchema, partitionsToUpdate);
        } else {
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
    }
    tbl.setCatalogVersion(newCatalogVersion);
}
#method_after
private void loadTableMetadata(Table tbl, long newCatalogVersion, boolean reloadFileMetadata, boolean reloadTableSchema, Set<String> partitionsToUpdate) throws CatalogException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(msClient, tbl);
        if (tbl instanceof HdfsTable) {
            ((HdfsTable) tbl).load(true, msClient.getHiveClient(), msTbl, reloadFileMetadata, reloadTableSchema, partitionsToUpdate);
        } else {
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
    }
    tbl.setCatalogVersion(newCatalogVersion);
}
#end_block

#method_before
private void alterView(TCreateOrAlterViewParams params, TDdlExecResponse resp) throws ImpalaException {
    TableName tableName = TableName.fromThrift(params.getView_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    Preconditions.checkState(params.getColumns() != null && params.getColumns().size() > 0, "Null or empty column list given as argument to DdlExecutor.alterView");
    Table tbl = catalog_.getTable(tableName.getDb(), tableName.getTbl());
    Preconditions.checkState(tbl instanceof View);
    while (true) {
        catalog_.getLock().writeLock().lock();
        if (tbl.getLock().writeLock().tryLock()) {
            try {
                long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
                catalog_.getLock().writeLock().unlock();
                // Operate on a copy of the metastore table to avoid prematurely applying the
                // alteration to our cached table in case the actual alteration fails.
                org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
                if (!msTbl.getTableType().equalsIgnoreCase((TableType.VIRTUAL_VIEW.toString()))) {
                    throw new ImpalaRuntimeException(String.format("ALTER VIEW not allowed on a table: %s", tableName.toString()));
                }
                // Set the altered view attributes and update the metastore.
                setViewAttributes(params, msTbl);
                if (LOG.isTraceEnabled()) {
                    LOG.trace(String.format("Altering view %s", tableName));
                }
                applyAlterTable(msTbl);
                try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
                    tbl.load(true, msClient.getHiveClient(), msTbl);
                }
                tbl.setCatalogVersion(newCatalogVersion);
                addTableToCatalogUpdate(tbl, resp.result);
            } finally {
                tbl.getLock().writeLock().unlock();
            }
        } else {
            catalog_.getLock().writeLock().unlock();
            continue;
        }
        break;
    }
}
#method_after
private void alterView(TCreateOrAlterViewParams params, TDdlExecResponse resp) throws ImpalaException {
    TableName tableName = TableName.fromThrift(params.getView_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    Preconditions.checkState(params.getColumns() != null && params.getColumns().size() > 0, "Null or empty column list given as argument to DdlExecutor.alterView");
    Table tbl = catalog_.getTable(tableName.getDb(), tableName.getTbl());
    Preconditions.checkState(tbl instanceof View);
    if (!catalog_.tryLockTable(tbl)) {
        throw new InternalException(String.format("Error altering view %s due to lock " + "contention", tbl.getFullName()));
    }
    try {
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        catalog_.getLock().writeLock().unlock();
        // Operate on a copy of the metastore table to avoid prematurely applying the
        // alteration to our cached table in case the actual alteration fails.
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        if (!msTbl.getTableType().equalsIgnoreCase((TableType.VIRTUAL_VIEW.toString()))) {
            throw new ImpalaRuntimeException(String.format("ALTER VIEW not allowed on a table: %s", tableName.toString()));
        }
        // Set the altered view attributes and update the metastore.
        setViewAttributes(params, msTbl);
        if (LOG.isTraceEnabled()) {
            LOG.trace(String.format("Altering view %s", tableName));
        }
        applyAlterTable(msTbl);
        try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
        tbl.setCatalogVersion(newCatalogVersion);
        addTableToCatalogUpdate(tbl, resp.result);
    } finally {
        Preconditions.checkState(!catalog_.getLock().isWriteLockedByCurrentThread());
        tbl.getLock().unlock();
    }
}
#end_block

#method_before
private void alterTableUpdateStats(Table table, TAlterTableUpdateStatsParams params, TDdlExecResponse resp, Reference<Long> numUpdatedPartitions, Reference<Long> numUpdatedColumns) throws ImpalaException {
    Preconditions.checkState(table.getLock().isWriteLockedByCurrentThread());
    if (params.isSetTable_stats()) {
        // Updating table and column stats via COMPUTE STATS.
        Preconditions.checkState(params.isSetPartition_stats() && params.isSetTable_stats());
    } else {
        // Only changing column stats via ALTER TABLE SET COLUMN STATS.
        Preconditions.checkState(params.isSetColumn_stats());
    }
    TableName tableName = table.getTableName();
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Updating table stats for: %s", tableName));
    }
    // Deep copy the msTbl to avoid updating our cache before successfully persisting
    // the results to the metastore.
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
    List<HdfsPartition> partitions = Lists.newArrayList();
    if (table instanceof HdfsTable) {
        // Build a list of non-default partitions to update.
        HdfsTable hdfsTable = (HdfsTable) table;
        for (HdfsPartition p : hdfsTable.getPartitions()) {
            if (!p.isDefaultPartition())
                partitions.add(p);
        }
    }
    long numTargetedPartitions = 0L;
    long numTargetedColumns = 0L;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Update the table and partition row counts based on the query results.
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        if (params.isSetTable_stats()) {
            numTargetedPartitions = updateTableStats(table, params, msTbl, partitions, modifiedParts);
        }
        ColumnStatistics colStats = null;
        if (params.isSetColumn_stats()) {
            // Create Hive column stats from the query results.
            colStats = createHiveColStats(params.getColumn_stats(), table);
            numTargetedColumns = colStats.getStatsObjSize();
        }
        // Update all partitions.
        bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
        if (numTargetedColumns > 0) {
            Preconditions.checkNotNull(colStats);
            // Update column stats.
            try {
                msClient.getHiveClient().updateTableColumnStatistics(colStats);
            } catch (Exception e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "updateTableColumnStatistics"), e);
            }
        }
        // Update the table stats. Apply the table alteration last to ensure the
        // lastDdlTime is as accurate as possible.
        applyAlterTable(msTbl);
    }
    numUpdatedPartitions.setRef(numTargetedPartitions);
    numUpdatedColumns.setRef(numTargetedColumns);
}
#method_after
private void alterTableUpdateStats(Table table, TAlterTableUpdateStatsParams params, TDdlExecResponse resp, Reference<Long> numUpdatedPartitions, Reference<Long> numUpdatedColumns) throws ImpalaException {
    Preconditions.checkState(table.getLock().isHeldByCurrentThread());
    if (params.isSetTable_stats()) {
        // Updating table and column stats via COMPUTE STATS.
        Preconditions.checkState(params.isSetPartition_stats() && params.isSetTable_stats());
    } else {
        // Only changing column stats via ALTER TABLE SET COLUMN STATS.
        Preconditions.checkState(params.isSetColumn_stats());
    }
    TableName tableName = table.getTableName();
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Updating table stats for: %s", tableName));
    }
    // Deep copy the msTbl to avoid updating our cache before successfully persisting
    // the results to the metastore.
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
    List<HdfsPartition> partitions = Lists.newArrayList();
    if (table instanceof HdfsTable) {
        // Build a list of non-default partitions to update.
        HdfsTable hdfsTable = (HdfsTable) table;
        for (HdfsPartition p : hdfsTable.getPartitions()) {
            if (!p.isDefaultPartition())
                partitions.add(p);
        }
    }
    long numTargetedPartitions = 0L;
    long numTargetedColumns = 0L;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Update the table and partition row counts based on the query results.
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        if (params.isSetTable_stats()) {
            numTargetedPartitions = updateTableStats(table, params, msTbl, partitions, modifiedParts);
        }
        ColumnStatistics colStats = null;
        if (params.isSetColumn_stats()) {
            // Create Hive column stats from the query results.
            colStats = createHiveColStats(params.getColumn_stats(), table);
            numTargetedColumns = colStats.getStatsObjSize();
        }
        // Update all partitions.
        bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
        if (numTargetedColumns > 0) {
            Preconditions.checkNotNull(colStats);
            // Update column stats.
            try {
                msClient.getHiveClient().updateTableColumnStatistics(colStats);
            } catch (Exception e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "updateTableColumnStatistics"), e);
            }
        }
        // Update the table stats. Apply the table alteration last to ensure the
        // lastDdlTime is as accurate as possible.
        applyAlterTable(msTbl);
    }
    numUpdatedPartitions.setRef(numTargetedPartitions);
    numUpdatedColumns.setRef(numTargetedColumns);
}
#end_block

#method_before
private void dropStats(TDropStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Table table = getExistingTable(params.getTable_name().getDb_name(), params.getTable_name().getTable_name());
    Preconditions.checkNotNull(table);
    while (true) {
        catalog_.getLock().writeLock().lock();
        if (table.getLock().writeLock().tryLock()) {
            try {
                long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
                catalog_.getLock().writeLock().unlock();
                if (params.getPartition_set() == null) {
                    // TODO: Report the number of updated partitions/columns to the user?
                    // TODO: bulk alter the partitions.
                    dropColumnStats(table);
                    dropTableStats(table);
                } else {
                    HdfsTable hdfsTbl = (HdfsTable) table;
                    List<HdfsPartition> partitions = hdfsTbl.getPartitionsFromPartitionSet(params.getPartition_set());
                    if (partitions.isEmpty())
                        return;
                    for (HdfsPartition partition : partitions) {
                        if (partition.getPartitionStats() != null) {
                            PartitionStatsUtil.deletePartStats(partition);
                            try {
                                applyAlterPartition(table, partition);
                            } finally {
                                partition.markDirty();
                            }
                        }
                    }
                }
                loadTableMetadata(table, newCatalogVersion, false, true, null);
                addTableToCatalogUpdate(table, resp.result);
            } finally {
                table.getLock().writeLock().unlock();
            }
        } else {
            catalog_.getLock().writeLock().unlock();
            continue;
        }
        break;
    }
}
#method_after
private void dropStats(TDropStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Table table = getExistingTable(params.getTable_name().getDb_name(), params.getTable_name().getTable_name());
    Preconditions.checkNotNull(table);
    if (!catalog_.tryLockTable(table)) {
        throw new InternalException(String.format("Error dropping stats for table %s " + "due to lock contention", table.getFullName()));
    }
    try {
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        catalog_.getLock().writeLock().unlock();
        if (params.getPartition_set() == null) {
            // TODO: Report the number of updated partitions/columns to the user?
            // TODO: bulk alter the partitions.
            dropColumnStats(table);
            dropTableStats(table);
        } else {
            HdfsTable hdfsTbl = (HdfsTable) table;
            List<HdfsPartition> partitions = hdfsTbl.getPartitionsFromPartitionSet(params.getPartition_set());
            if (partitions.isEmpty())
                return;
            for (HdfsPartition partition : partitions) {
                if (partition.getPartitionStats() != null) {
                    PartitionStatsUtil.deletePartStats(partition);
                    try {
                        applyAlterPartition(table, partition);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        }
        loadTableMetadata(table, newCatalogVersion, false, true, null);
        addTableToCatalogUpdate(table, resp.result);
    } finally {
        Preconditions.checkState(!catalog_.getLock().isWriteLockedByCurrentThread());
        table.getLock().unlock();
    }
}
#end_block

#method_before
private int dropColumnStats(Table table) throws ImpalaRuntimeException {
    Preconditions.checkState(table.getLock().isWriteLockedByCurrentThread());
    int numColsUpdated = 0;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        for (Column col : table.getColumns()) {
            // Skip columns that don't have stats.
            if (!col.getStats().hasStats())
                continue;
            try {
                msClient.getHiveClient().deleteTableColumnStatistics(table.getDb().getName(), table.getName(), col.getName());
                ++numColsUpdated;
            } catch (NoSuchObjectException e) {
            // We don't care if the column stats do not exist, just ignore the exception.
            // We would only expect to make it here if the Impala and HMS metadata
            // diverged.
            } catch (TException e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "delete_table_column_statistics"), e);
            }
        }
    }
    return numColsUpdated;
}
#method_after
private int dropColumnStats(Table table) throws ImpalaRuntimeException {
    Preconditions.checkState(table.getLock().isHeldByCurrentThread());
    int numColsUpdated = 0;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        for (Column col : table.getColumns()) {
            // Skip columns that don't have stats.
            if (!col.getStats().hasStats())
                continue;
            try {
                msClient.getHiveClient().deleteTableColumnStatistics(table.getDb().getName(), table.getName(), col.getName());
                ++numColsUpdated;
            } catch (NoSuchObjectException e) {
            // We don't care if the column stats do not exist, just ignore the exception.
            // We would only expect to make it here if the Impala and HMS metadata
            // diverged.
            } catch (TException e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "delete_table_column_statistics"), e);
            }
        }
    }
    return numColsUpdated;
}
#end_block

#method_before
private int dropTableStats(Table table) throws ImpalaException {
    Preconditions.checkState(table.getLock().isWriteLockedByCurrentThread());
    // Delete the ROW_COUNT from the table (if it was set).
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable();
    int numTargetedPartitions = 0;
    if (msTbl.getParameters().remove(StatsSetupConst.ROW_COUNT) != null) {
        applyAlterTable(msTbl);
        ++numTargetedPartitions;
    }
    if (!(table instanceof HdfsTable) || table.getNumClusteringCols() == 0) {
        // is no more work to be done so just return.
        return numTargetedPartitions;
    }
    // Now clear the stats for all partitions in the table.
    HdfsTable hdfsTable = (HdfsTable) table;
    Preconditions.checkNotNull(hdfsTable);
    // List of partitions that were modified as part of this operation.
    List<HdfsPartition> modifiedParts = Lists.newArrayList();
    for (HdfsPartition part : hdfsTable.getPartitions()) {
        boolean isModified = false;
        // represented in the Hive Metastore.
        if (part.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
            continue;
        }
        if (part.getPartitionStats() != null) {
            PartitionStatsUtil.deletePartStats(part);
            isModified = true;
        }
        // Remove the ROW_COUNT parameter if it has been set.
        if (part.getParameters().remove(StatsSetupConst.ROW_COUNT) != null) {
            isModified = true;
        }
        if (isModified)
            modifiedParts.add(part);
    }
    bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
    return modifiedParts.size();
}
#method_after
private int dropTableStats(Table table) throws ImpalaException {
    Preconditions.checkState(table.getLock().isHeldByCurrentThread());
    // Delete the ROW_COUNT from the table (if it was set).
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable();
    int numTargetedPartitions = 0;
    if (msTbl.getParameters().remove(StatsSetupConst.ROW_COUNT) != null) {
        applyAlterTable(msTbl);
        ++numTargetedPartitions;
    }
    if (!(table instanceof HdfsTable) || table.getNumClusteringCols() == 0) {
        // is no more work to be done so just return.
        return numTargetedPartitions;
    }
    // Now clear the stats for all partitions in the table.
    HdfsTable hdfsTable = (HdfsTable) table;
    Preconditions.checkNotNull(hdfsTable);
    // List of partitions that were modified as part of this operation.
    List<HdfsPartition> modifiedParts = Lists.newArrayList();
    for (HdfsPartition part : hdfsTable.getPartitions()) {
        boolean isModified = false;
        // represented in the Hive Metastore.
        if (part.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
            continue;
        }
        if (part.getPartitionStats() != null) {
            PartitionStatsUtil.deletePartStats(part);
            isModified = true;
        }
        // Remove the ROW_COUNT parameter if it has been set.
        if (part.getParameters().remove(StatsSetupConst.ROW_COUNT) != null) {
            isModified = true;
        }
        if (isModified)
            modifiedParts.add(part);
    }
    bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
    return modifiedParts.size();
}
#end_block

#method_before
private void truncateTable(TTruncateParams params, TDdlExecResponse resp) throws ImpalaException {
    TTableName tblName = params.getTable_name();
    Table table = null;
    try {
        table = getExistingTable(tblName.getDb_name(), tblName.getTable_name());
    } catch (TableNotFoundException e) {
        if (params.if_exists)
            return;
        throw e;
    }
    Preconditions.checkNotNull(table);
    if (!(table instanceof HdfsTable)) {
        throw new CatalogException(String.format("TRUNCATE TABLE not supported on non-HDFS table: %s", table.getFullName()));
    }
    while (true) {
        catalog_.getLock().writeLock().lock();
        if (table.getLock().writeLock().tryLock()) {
            try {
                long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
                catalog_.getLock().writeLock().unlock();
                try {
                    HdfsTable hdfsTable = (HdfsTable) table;
                    for (HdfsPartition part : hdfsTable.getPartitions()) {
                        if (part.isDefaultPartition())
                            continue;
                        FileSystemUtil.deleteAllVisibleFiles(new Path(part.getLocation()));
                    }
                    dropColumnStats(table);
                    dropTableStats(table);
                } catch (Exception e) {
                    String fqName = tblName.db_name + "." + tblName.table_name;
                    throw new CatalogException(String.format("Failed to truncate table: %s.\n" + "Table may be in a partially truncated state.", fqName), e);
                }
                loadTableMetadata(table, newCatalogVersion, true, true, null);
                addTableToCatalogUpdate(table, resp.result);
            } finally {
                table.getLock().writeLock().unlock();
            }
        } else {
            catalog_.getLock().writeLock().unlock();
            continue;
        }
        break;
    }
}
#method_after
private void truncateTable(TTruncateParams params, TDdlExecResponse resp) throws ImpalaException {
    TTableName tblName = params.getTable_name();
    Table table = null;
    try {
        table = getExistingTable(tblName.getDb_name(), tblName.getTable_name());
    } catch (TableNotFoundException e) {
        if (params.if_exists)
            return;
        throw e;
    }
    Preconditions.checkNotNull(table);
    if (!(table instanceof HdfsTable)) {
        throw new CatalogException(String.format("TRUNCATE TABLE not supported on non-HDFS table: %s", table.getFullName()));
    }
    if (!catalog_.tryLockTable(table)) {
        throw new InternalException(String.format("Error truncating table %s due to lock " + "contention", table.getFullName()));
    }
    try {
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        catalog_.getLock().writeLock().unlock();
        try {
            HdfsTable hdfsTable = (HdfsTable) table;
            for (HdfsPartition part : hdfsTable.getPartitions()) {
                if (part.isDefaultPartition())
                    continue;
                FileSystemUtil.deleteAllVisibleFiles(new Path(part.getLocation()));
            }
            dropColumnStats(table);
            dropTableStats(table);
        } catch (Exception e) {
            String fqName = tblName.db_name + "." + tblName.table_name;
            throw new CatalogException(String.format("Failed to truncate table: %s.\n" + "Table may be in a partially truncated state.", fqName), e);
        }
        loadTableMetadata(table, newCatalogVersion, true, true, null);
        addTableToCatalogUpdate(table, resp.result);
    } finally {
        Preconditions.checkState(!catalog_.getLock().isWriteLockedByCurrentThread());
        table.getLock().unlock();
    }
}
#end_block

#method_before
private void alterTableAddReplaceCols(Table tbl, List<TColumn> columns, boolean replaceExistingCols) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    List<FieldSchema> newColumns = buildFieldSchemaList(columns);
    if (replaceExistingCols) {
        msTbl.getSd().setCols(newColumns);
    } else {
        // Append the new column to the existing list of columns.
        for (FieldSchema fs : buildFieldSchemaList(columns)) {
            msTbl.getSd().addToCols(fs);
        }
    }
    applyAlterTable(msTbl);
}
#method_after
private void alterTableAddReplaceCols(Table tbl, List<TColumn> columns, boolean replaceExistingCols) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    List<FieldSchema> newColumns = buildFieldSchemaList(columns);
    if (replaceExistingCols) {
        msTbl.getSd().setCols(newColumns);
    } else {
        // Append the new column to the existing list of columns.
        for (FieldSchema fs : buildFieldSchemaList(columns)) {
            msTbl.getSd().addToCols(fs);
        }
    }
    applyAlterTable(msTbl);
}
#end_block

#method_before
private void alterTableChangeCol(Table tbl, String colName, TColumn newCol) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    // Find the matching column name and change it.
    Iterator<FieldSchema> iterator = msTbl.getSd().getColsIterator();
    while (iterator.hasNext()) {
        FieldSchema fs = iterator.next();
        if (fs.getName().toLowerCase().equals(colName.toLowerCase())) {
            fs.setName(newCol.getColumnName());
            Type type = Type.fromThrift(newCol.getColumnType());
            fs.setType(type.toSql().toLowerCase());
            // Don't overwrite the existing comment unless a new comment is given
            if (newCol.getComment() != null) {
                fs.setComment(newCol.getComment());
            }
            break;
        }
        if (!iterator.hasNext()) {
            throw new ColumnNotFoundException(String.format("Column name %s not found in table %s.", colName, tbl.getFullName()));
        }
    }
    applyAlterTable(msTbl);
}
#method_after
private void alterTableChangeCol(Table tbl, String colName, TColumn newCol) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    // Find the matching column name and change it.
    Iterator<FieldSchema> iterator = msTbl.getSd().getColsIterator();
    while (iterator.hasNext()) {
        FieldSchema fs = iterator.next();
        if (fs.getName().toLowerCase().equals(colName.toLowerCase())) {
            fs.setName(newCol.getColumnName());
            Type type = Type.fromThrift(newCol.getColumnType());
            fs.setType(type.toSql().toLowerCase());
            // Don't overwrite the existing comment unless a new comment is given
            if (newCol.getComment() != null) {
                fs.setComment(newCol.getComment());
            }
            break;
        }
        if (!iterator.hasNext()) {
            throw new ColumnNotFoundException(String.format("Column name %s not found in table %s.", colName, tbl.getFullName()));
        }
    }
    applyAlterTable(msTbl);
}
#end_block

#method_before
private Table alterTableAddPartition(Table tbl, List<TPartitionKeyValue> partitionSpec, boolean ifNotExists, String location, THdfsCachingOp cacheOp) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    TableName tableName = tbl.getTableName();
    if (ifNotExists && catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.trace(String.format("Skipping partition creation because (%s) already exists" + " and ifNotExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    org.apache.hadoop.hive.metastore.api.Partition partition = null;
    Table result = null;
    List<Long> cacheIds = null;
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    partition = createHmsPartition(partitionSpec, msTbl, tableName, location);
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Add the new partition.
        partition = msClient.getHiveClient().add_partition(partition);
        String cachePoolName = null;
        Short replication = null;
        if (cacheOp == null && parentTblCacheDirId != null) {
            // The user didn't specify an explicit caching operation, inherit the value
            // from the parent table.
            cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
            Preconditions.checkNotNull(cachePoolName);
            replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            Preconditions.checkNotNull(replication);
        } else if (cacheOp != null && cacheOp.isSet_cached()) {
            // The user explicitly stated that this partition should be cached.
            cachePoolName = cacheOp.getCache_pool_name();
            // explicitly set, use the default value.
            if (!cacheOp.isSetReplication() && parentTblCacheDirId != null) {
                replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            } else {
                replication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
            }
        }
        // If cache pool name is not null, it indicates this partition should be cached.
        if (cachePoolName != null) {
            long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
            cacheIds = Lists.<Long>newArrayList(id);
            // Update the partition metadata to include the cache directive id.
            msClient.getHiveClient().alter_partition(partition.getDbName(), partition.getTableName(), partition);
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (AlreadyExistsException e) {
        if (!ifNotExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
        }
        LOG.trace(String.format("Ignoring '%s' when adding partition to %s because" + " ifNotExists is true.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    }
    if (cacheIds != null)
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    // Return the table object with an updated catalog version after creating the
    // partition.
    result = addHdfsPartition(tbl, partition);
    return result;
}
#method_after
private Table alterTableAddPartition(Table tbl, List<TPartitionKeyValue> partitionSpec, boolean ifNotExists, String location, THdfsCachingOp cacheOp) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    TableName tableName = tbl.getTableName();
    if (ifNotExists && catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.trace(String.format("Skipping partition creation because (%s) already exists" + " and ifNotExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    org.apache.hadoop.hive.metastore.api.Partition partition = null;
    Table result = null;
    List<Long> cacheIds = null;
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    partition = createHmsPartition(partitionSpec, msTbl, tableName, location);
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Add the new partition.
        partition = msClient.getHiveClient().add_partition(partition);
        String cachePoolName = null;
        Short replication = null;
        if (cacheOp == null && parentTblCacheDirId != null) {
            // The user didn't specify an explicit caching operation, inherit the value
            // from the parent table.
            cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
            Preconditions.checkNotNull(cachePoolName);
            replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            Preconditions.checkNotNull(replication);
        } else if (cacheOp != null && cacheOp.isSet_cached()) {
            // The user explicitly stated that this partition should be cached.
            cachePoolName = cacheOp.getCache_pool_name();
            // explicitly set, use the default value.
            if (!cacheOp.isSetReplication() && parentTblCacheDirId != null) {
                replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            } else {
                replication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
            }
        }
        // If cache pool name is not null, it indicates this partition should be cached.
        if (cachePoolName != null) {
            long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
            cacheIds = Lists.<Long>newArrayList(id);
            // Update the partition metadata to include the cache directive id.
            msClient.getHiveClient().alter_partition(partition.getDbName(), partition.getTableName(), partition);
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (AlreadyExistsException e) {
        if (!ifNotExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
        }
        LOG.trace(String.format("Ignoring '%s' when adding partition to %s because" + " ifNotExists is true.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    }
    if (cacheIds != null)
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    // Return the table object with an updated catalog version after creating the
    // partition.
    result = addHdfsPartition(tbl, partition);
    return result;
}
#end_block

#method_before
private Table alterTableDropPartition(Table tbl, List<List<TPartitionKeyValue>> partitionSet, boolean ifExists, boolean purge, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    Preconditions.checkNotNull(partitionSet);
    TableName tableName = tbl.getTableName();
    if (!ifExists) {
        Preconditions.checkState(!partitionSet.isEmpty());
    } else {
        if (partitionSet.isEmpty()) {
            LOG.trace(String.format("Ignoring empty partition list when dropping " + "partitions from %s because ifExists is true.", tableName));
            return tbl;
        }
    }
    Preconditions.checkArgument(tbl instanceof HdfsTable);
    List<HdfsPartition> parts = ((HdfsTable) tbl).getPartitionsFromPartitionSet(partitionSet);
    if (!ifExists && parts.isEmpty()) {
        throw new PartitionNotFoundException("The partitions being dropped don't exist any more");
    }
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    PartitionDropOptions dropOptions = PartitionDropOptions.instance();
    dropOptions.purgeData(purge);
    long numTargetedPartitions = 0L;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        for (HdfsPartition part : parts) {
            try {
                msClient.getHiveClient().dropPartition(tableName.getDb(), tableName.getTbl(), part.getPartitionValuesAsStrings(true), dropOptions);
                ++numTargetedPartitions;
                if (part.isMarkedCached()) {
                    HdfsCachingUtil.uncachePartition(part);
                }
            } catch (NoSuchObjectException e) {
                if (!ifExists) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
                }
                LOG.trace(String.format("Ignoring '%s' when dropping partitions from %s because" + " ifExists is true.", e, tableName));
            }
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
    }
    numUpdatedPartitions.setRef(numTargetedPartitions);
    return catalog_.dropPartitions(tbl, partitionSet);
}
#method_after
private Table alterTableDropPartition(Table tbl, List<List<TPartitionKeyValue>> partitionSet, boolean ifExists, boolean purge, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    Preconditions.checkNotNull(partitionSet);
    TableName tableName = tbl.getTableName();
    if (!ifExists) {
        Preconditions.checkState(!partitionSet.isEmpty());
    } else {
        if (partitionSet.isEmpty()) {
            LOG.trace(String.format("Ignoring empty partition list when dropping " + "partitions from %s because ifExists is true.", tableName));
            return tbl;
        }
    }
    Preconditions.checkArgument(tbl instanceof HdfsTable);
    List<HdfsPartition> parts = ((HdfsTable) tbl).getPartitionsFromPartitionSet(partitionSet);
    if (!ifExists && parts.isEmpty()) {
        throw new PartitionNotFoundException("The partitions being dropped don't exist any more");
    }
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    PartitionDropOptions dropOptions = PartitionDropOptions.instance();
    dropOptions.purgeData(purge);
    long numTargetedPartitions = 0L;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        for (HdfsPartition part : parts) {
            try {
                msClient.getHiveClient().dropPartition(tableName.getDb(), tableName.getTbl(), part.getPartitionValuesAsStrings(true), dropOptions);
                ++numTargetedPartitions;
                if (part.isMarkedCached()) {
                    HdfsCachingUtil.uncachePartition(part);
                }
            } catch (NoSuchObjectException e) {
                if (!ifExists) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
                }
                LOG.trace(String.format("Ignoring '%s' when dropping partitions from %s because" + " ifExists is true.", e, tableName));
            }
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
    }
    numUpdatedPartitions.setRef(numTargetedPartitions);
    return catalog_.dropPartitions(tbl, partitionSet);
}
#end_block

#method_before
private void alterTableDropCol(Table tbl, String colName) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    // Find the matching column name and remove it.
    Iterator<FieldSchema> iterator = msTbl.getSd().getColsIterator();
    while (iterator.hasNext()) {
        FieldSchema fs = iterator.next();
        if (fs.getName().toLowerCase().equals(colName.toLowerCase())) {
            iterator.remove();
            break;
        }
        if (!iterator.hasNext()) {
            throw new ColumnNotFoundException(String.format("Column name %s not found in table %s.", colName, tbl.getFullName()));
        }
    }
    applyAlterTable(msTbl);
}
#method_after
private void alterTableDropCol(Table tbl, String colName) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    // Find the matching column name and remove it.
    Iterator<FieldSchema> iterator = msTbl.getSd().getColsIterator();
    while (iterator.hasNext()) {
        FieldSchema fs = iterator.next();
        if (fs.getName().toLowerCase().equals(colName.toLowerCase())) {
            iterator.remove();
            break;
        }
        if (!iterator.hasNext()) {
            throw new ColumnNotFoundException(String.format("Column name %s not found in table %s.", colName, tbl.getFullName()));
        }
    }
    applyAlterTable(msTbl);
}
#end_block

#method_before
private void alterTableOrViewRename(Table oldTbl, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    Preconditions.checkState(oldTbl.getLock().isWriteLockedByCurrentThread() && catalog_.getLock().isWriteLockedByCurrentThread());
    TableName tableName = oldTbl.getTableName();
    org.apache.hadoop.hive.metastore.api.Table msTbl = oldTbl.getMetaStoreTable().deepCopy();
    msTbl.setDbName(newTableName.getDb());
    msTbl.setTableName(newTableName.getTbl());
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column
        // stats across databases, we save, drop and restore the column stats because
        // the HMS does not properly move them to the new table via alteration.
        ColumnStatistics hmsColStats = null;
        if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
            Map<String, TColumnStats> colStats = Maps.newHashMap();
            for (Column c : oldTbl.getColumns()) {
                colStats.put(c.getName(), c.getStats().toThrift());
            }
            hmsColStats = createHiveColStats(colStats, oldTbl);
            // Set the new db/table.
            hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
            LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            // Delete all column stats of the original table from the HMS.
            msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
        }
        // Perform the table rename in any case.
        msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
        if (hmsColStats != null) {
            LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
        }
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    Table newTable = catalog_.renameTable(tableName.toThrift(), newTableName.toThrift());
    if (newTable == null) {
        // an inconsistent state, but can likely be fixed by running "invalidate metadata".
        throw new ImpalaRuntimeException(String.format("Table/view rename succeeded in the Hive Metastore, but failed in Impala's " + "Catalog Server. Running 'invalidate metadata <tbl>' on the old table name " + "'%s' and the new table name '%s' may fix the problem.", tableName.toString(), newTableName.toString()));
    }
    TCatalogObject addedObject = newTable.toTCatalogObject();
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable(tableName.getDb(), tableName.getTbl()));
    removedObject.setCatalog_version(addedObject.getCatalog_version());
    response.result.setRemoved_catalog_object_DEPRECATED(removedObject);
    response.result.setUpdated_catalog_object_DEPRECATED(addedObject);
    response.result.setVersion(addedObject.getCatalog_version());
}
#method_after
private void alterTableOrViewRename(Table oldTbl, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    Preconditions.checkState(oldTbl.getLock().isHeldByCurrentThread() && catalog_.getLock().isWriteLockedByCurrentThread());
    TableName tableName = oldTbl.getTableName();
    org.apache.hadoop.hive.metastore.api.Table msTbl = oldTbl.getMetaStoreTable().deepCopy();
    msTbl.setDbName(newTableName.getDb());
    msTbl.setTableName(newTableName.getTbl());
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column
        // stats across databases, we save, drop and restore the column stats because
        // the HMS does not properly move them to the new table via alteration.
        ColumnStatistics hmsColStats = null;
        if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
            Map<String, TColumnStats> colStats = Maps.newHashMap();
            for (Column c : oldTbl.getColumns()) {
                colStats.put(c.getName(), c.getStats().toThrift());
            }
            hmsColStats = createHiveColStats(colStats, oldTbl);
            // Set the new db/table.
            hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
            LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            // Delete all column stats of the original table from the HMS.
            msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
        }
        // Perform the table rename in any case.
        msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
        if (hmsColStats != null) {
            LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
        }
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    Table newTable = catalog_.renameTable(tableName.toThrift(), newTableName.toThrift());
    if (newTable == null) {
        // an inconsistent state, but can likely be fixed by running "invalidate metadata".
        throw new ImpalaRuntimeException(String.format("Table/view rename succeeded in the Hive Metastore, but failed in Impala's " + "Catalog Server. Running 'invalidate metadata <tbl>' on the old table name " + "'%s' and the new table name '%s' may fix the problem.", tableName.toString(), newTableName.toString()));
    }
    TCatalogObject addedObject = newTable.toTCatalogObject();
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable(tableName.getDb(), tableName.getTbl()));
    removedObject.setCatalog_version(addedObject.getCatalog_version());
    response.result.setRemoved_catalog_object_DEPRECATED(removedObject);
    response.result.setUpdated_catalog_object_DEPRECATED(addedObject);
    response.result.setVersion(addedObject.getCatalog_version());
}
#end_block

#method_before
private boolean alterTableSetFileFormat(Table tbl, List<List<TPartitionKeyValue>> partitionSet, THdfsFileFormat fileFormat, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    Preconditions.checkState(partitionSet == null || !partitionSet.isEmpty());
    boolean reloadFileMetadata = false;
    if (partitionSet == null) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        setStorageDescriptorFileFormat(msTbl.getSd(), fileFormat);
        // partitions are created with the new file format.
        if (tbl instanceof HdfsTable)
            ((HdfsTable) tbl).addDefaultPartition(msTbl.getSd());
        applyAlterTable(msTbl);
        reloadFileMetadata = true;
    } else {
        Preconditions.checkArgument(tbl instanceof HdfsTable);
        List<HdfsPartition> partitions = ((HdfsTable) tbl).getPartitionsFromPartitionSet(partitionSet);
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        for (HdfsPartition partition : partitions) {
            partition.setFileFormat(HdfsFileFormat.fromThrift(fileFormat));
            modifiedParts.add(partition);
        }
        TableName tableName = tbl.getTableName();
        bulkAlterPartitions(tableName.getDb(), tableName.getTbl(), modifiedParts);
        numUpdatedPartitions.setRef((long) modifiedParts.size());
    }
    return reloadFileMetadata;
}
#method_after
private boolean alterTableSetFileFormat(Table tbl, List<List<TPartitionKeyValue>> partitionSet, THdfsFileFormat fileFormat, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    Preconditions.checkState(partitionSet == null || !partitionSet.isEmpty());
    boolean reloadFileMetadata = false;
    if (partitionSet == null) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        setStorageDescriptorFileFormat(msTbl.getSd(), fileFormat);
        // partitions are created with the new file format.
        if (tbl instanceof HdfsTable)
            ((HdfsTable) tbl).addDefaultPartition(msTbl.getSd());
        applyAlterTable(msTbl);
        reloadFileMetadata = true;
    } else {
        Preconditions.checkArgument(tbl instanceof HdfsTable);
        List<HdfsPartition> partitions = ((HdfsTable) tbl).getPartitionsFromPartitionSet(partitionSet);
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        for (HdfsPartition partition : partitions) {
            partition.setFileFormat(HdfsFileFormat.fromThrift(fileFormat));
            modifiedParts.add(partition);
        }
        TableName tableName = tbl.getTableName();
        bulkAlterPartitions(tableName.getDb(), tableName.getTbl(), modifiedParts);
        numUpdatedPartitions.setRef((long) modifiedParts.size());
    }
    return reloadFileMetadata;
}
#end_block

#method_before
private boolean alterTableSetLocation(Table tbl, List<TPartitionKeyValue> partitionSpec, String location) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    boolean reloadFileMetadata = false;
    if (partitionSpec == null) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        if (msTbl.getPartitionKeysSize() == 0)
            reloadFileMetadata = true;
        msTbl.getSd().setLocation(location);
        applyAlterTable(msTbl);
    } else {
        TableName tableName = tbl.getTableName();
        HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
        partition.setLocation(location);
        try {
            applyAlterPartition(tbl, partition);
        } finally {
            partition.markDirty();
        }
    }
    return reloadFileMetadata;
}
#method_after
private boolean alterTableSetLocation(Table tbl, List<TPartitionKeyValue> partitionSpec, String location) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    boolean reloadFileMetadata = false;
    if (partitionSpec == null) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        if (msTbl.getPartitionKeysSize() == 0)
            reloadFileMetadata = true;
        msTbl.getSd().setLocation(location);
        applyAlterTable(msTbl);
    } else {
        TableName tableName = tbl.getTableName();
        HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
        partition.setLocation(location);
        try {
            applyAlterPartition(tbl, partition);
        } finally {
            partition.markDirty();
        }
    }
    return reloadFileMetadata;
}
#end_block

#method_before
private void alterTableSetTblProperties(Table tbl, TAlterTableSetTblPropertiesParams params, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isWriteLockedByCurrentThread());
    Map<String, String> properties = params.getProperties();
    Preconditions.checkNotNull(properties);
    if (params.isSetPartition_set()) {
        Preconditions.checkArgument(tbl instanceof HdfsTable);
        List<HdfsPartition> partitions = ((HdfsTable) tbl).getPartitionsFromPartitionSet(params.getPartition_set());
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        for (HdfsPartition partition : partitions) {
            switch(params.getTarget()) {
                case TBL_PROPERTY:
                    partition.getParameters().putAll(properties);
                    break;
                case SERDE_PROPERTY:
                    partition.getSerdeInfo().getParameters().putAll(properties);
                    break;
                default:
                    throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
            }
            modifiedParts.add(partition);
        }
        TableName tableName = tbl.getTableName();
        try {
            bulkAlterPartitions(tableName.getDb(), tableName.getTbl(), modifiedParts);
        } finally {
            for (HdfsPartition modifiedPart : modifiedParts) {
                modifiedPart.markDirty();
            }
        }
        numUpdatedPartitions.setRef((long) modifiedParts.size());
    } else {
        // Alter table params.
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        switch(params.getTarget()) {
            case TBL_PROPERTY:
                if (KuduTable.isKuduTable(msTbl)) {
                    // the underlying Kudu table.
                    if (properties.containsKey(KuduTable.KEY_TABLE_NAME) && !properties.get(KuduTable.KEY_TABLE_NAME).equals(msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME)) && !Table.isExternalTable(msTbl)) {
                        KuduCatalogOpExecutor.renameTable((KuduTable) tbl, properties.get(KuduTable.KEY_TABLE_NAME));
                    }
                    msTbl.getParameters().putAll(properties);
                    // Validate that the new table properties are valid and that
                    // the Kudu table is accessible.
                    KuduCatalogOpExecutor.validateKuduTblExists(msTbl);
                } else {
                    msTbl.getParameters().putAll(properties);
                }
                break;
            case SERDE_PROPERTY:
                msTbl.getSd().getSerdeInfo().getParameters().putAll(properties);
                break;
            default:
                throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
        }
        applyAlterTable(msTbl);
    }
}
#method_after
private void alterTableSetTblProperties(Table tbl, TAlterTableSetTblPropertiesParams params, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkState(tbl.getLock().isHeldByCurrentThread());
    Map<String, String> properties = params.getProperties();
    Preconditions.checkNotNull(properties);
    if (params.isSetPartition_set()) {
        Preconditions.checkArgument(tbl instanceof HdfsTable);
        List<HdfsPartition> partitions = ((HdfsTable) tbl).getPartitionsFromPartitionSet(params.getPartition_set());
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        for (HdfsPartition partition : partitions) {
            switch(params.getTarget()) {
                case TBL_PROPERTY:
                    partition.getParameters().putAll(properties);
                    break;
                case SERDE_PROPERTY:
                    partition.getSerdeInfo().getParameters().putAll(properties);
                    break;
                default:
                    throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
            }
            modifiedParts.add(partition);
        }
        TableName tableName = tbl.getTableName();
        try {
            bulkAlterPartitions(tableName.getDb(), tableName.getTbl(), modifiedParts);
        } finally {
            for (HdfsPartition modifiedPart : modifiedParts) {
                modifiedPart.markDirty();
            }
        }
        numUpdatedPartitions.setRef((long) modifiedParts.size());
    } else {
        // Alter table params.
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        switch(params.getTarget()) {
            case TBL_PROPERTY:
                if (KuduTable.isKuduTable(msTbl)) {
                    // the underlying Kudu table.
                    if (properties.containsKey(KuduTable.KEY_TABLE_NAME) && !properties.get(KuduTable.KEY_TABLE_NAME).equals(msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME)) && !Table.isExternalTable(msTbl)) {
                        KuduCatalogOpExecutor.renameTable((KuduTable) tbl, properties.get(KuduTable.KEY_TABLE_NAME));
                    }
                    msTbl.getParameters().putAll(properties);
                    // Validate that the new table properties are valid and that
                    // the Kudu table is accessible.
                    KuduCatalogOpExecutor.validateKuduTblExists(msTbl);
                } else {
                    msTbl.getParameters().putAll(properties);
                }
                break;
            case SERDE_PROPERTY:
                msTbl.getSd().getSerdeInfo().getParameters().putAll(properties);
                break;
            default:
                throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
        }
        applyAlterTable(msTbl);
    }
}
#end_block

#method_before
private boolean alterTableSetCached(Table tbl, TAlterTableSetCachedParams params) throws ImpalaException {
    Preconditions.checkArgument(tbl.getLock().isWriteLockedByCurrentThread());
    THdfsCachingOp cacheOp = params.getCache_op();
    Preconditions.checkNotNull(cacheOp);
    // Alter table params.
    if (!(tbl instanceof HdfsTable)) {
        throw new ImpalaRuntimeException("ALTER TABLE SET CACHED/UNCACHED must target " + "an HDFS table.");
    }
    boolean loadFileMetadata = false;
    TableName tableName = tbl.getTableName();
    HdfsTable hdfsTable = (HdfsTable) tbl;
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (cacheOp.isSet_cached()) {
        // List of cache directive IDs that were submitted as part of this
        // ALTER TABLE operation.
        List<Long> cacheDirIds = Lists.newArrayList();
        short cacheReplication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
        // the pool name and update the cache replication factor if necessary
        if (cacheDirId == null) {
            cacheDirIds.add(HdfsCachingUtil.submitCacheTblDirective(msTbl, cacheOp.getCache_pool_name(), cacheReplication));
        } else {
            // Check if the cache directive needs to be changed
            if (HdfsCachingUtil.isUpdateOp(cacheOp, msTbl.getParameters())) {
                HdfsCachingUtil.validateCachePool(cacheOp, cacheDirId, tableName);
                cacheDirIds.add(HdfsCachingUtil.modifyCacheDirective(cacheDirId, msTbl, cacheOp.getCache_pool_name(), cacheReplication));
            }
        }
        if (tbl.getNumClusteringCols() > 0) {
            // partitions.
            for (HdfsPartition partition : hdfsTable.getPartitions()) {
                // not referred to by scan nodes.
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                // needs to be updated
                if (!partition.isMarkedCached() || HdfsCachingUtil.isUpdateOp(cacheOp, partition.getParameters())) {
                    try {
                        // issue new cache directive
                        if (!partition.isMarkedCached()) {
                            cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(partition, cacheOp.getCache_pool_name(), cacheReplication));
                        } else {
                            Long directiveId = HdfsCachingUtil.getCacheDirectiveId(partition.getParameters());
                            cacheDirIds.add(HdfsCachingUtil.modifyCacheDirective(directiveId, partition, cacheOp.getCache_pool_name(), cacheReplication));
                        }
                    } catch (ImpalaRuntimeException e) {
                        if (partition.isMarkedCached()) {
                            LOG.error("Unable to modify cache partition: " + partition.getPartitionName(), e);
                        } else {
                            LOG.error("Unable to cache partition: " + partition.getPartitionName(), e);
                        }
                    }
                    // Update the partition metadata.
                    try {
                        applyAlterPartition(tbl, partition);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        } else {
            loadFileMetadata = true;
        }
        // Nothing to do.
        if (cacheDirIds.isEmpty())
            return loadFileMetadata;
        // Submit a request to watch these cache directives. The TableLoadingMgr will
        // asynchronously refresh the table metadata once the directives complete.
        catalog_.watchCacheDirs(cacheDirIds, tableName.toThrift());
    } else {
        // Uncache the table.
        if (cacheDirId != null)
            HdfsCachingUtil.uncacheTbl(msTbl);
        // Uncache all table partitions.
        if (tbl.getNumClusteringCols() > 0) {
            for (HdfsPartition partition : hdfsTable.getPartitions()) {
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                if (partition.isMarkedCached()) {
                    HdfsCachingUtil.uncachePartition(partition);
                    try {
                        applyAlterPartition(tbl, partition);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        } else {
            loadFileMetadata = true;
        }
    }
    // Update the table metadata.
    applyAlterTable(msTbl);
    return loadFileMetadata;
}
#method_after
private boolean alterTableSetCached(Table tbl, TAlterTableSetCachedParams params) throws ImpalaException {
    Preconditions.checkArgument(tbl.getLock().isHeldByCurrentThread());
    THdfsCachingOp cacheOp = params.getCache_op();
    Preconditions.checkNotNull(cacheOp);
    // Alter table params.
    if (!(tbl instanceof HdfsTable)) {
        throw new ImpalaRuntimeException("ALTER TABLE SET CACHED/UNCACHED must target " + "an HDFS table.");
    }
    boolean loadFileMetadata = false;
    TableName tableName = tbl.getTableName();
    HdfsTable hdfsTable = (HdfsTable) tbl;
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    Long cacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (cacheOp.isSet_cached()) {
        // List of cache directive IDs that were submitted as part of this
        // ALTER TABLE operation.
        List<Long> cacheDirIds = Lists.newArrayList();
        short cacheReplication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
        // the pool name and update the cache replication factor if necessary
        if (cacheDirId == null) {
            cacheDirIds.add(HdfsCachingUtil.submitCacheTblDirective(msTbl, cacheOp.getCache_pool_name(), cacheReplication));
        } else {
            // Check if the cache directive needs to be changed
            if (HdfsCachingUtil.isUpdateOp(cacheOp, msTbl.getParameters())) {
                HdfsCachingUtil.validateCachePool(cacheOp, cacheDirId, tableName);
                cacheDirIds.add(HdfsCachingUtil.modifyCacheDirective(cacheDirId, msTbl, cacheOp.getCache_pool_name(), cacheReplication));
            }
        }
        if (tbl.getNumClusteringCols() > 0) {
            // partitions.
            for (HdfsPartition partition : hdfsTable.getPartitions()) {
                // not referred to by scan nodes.
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                // needs to be updated
                if (!partition.isMarkedCached() || HdfsCachingUtil.isUpdateOp(cacheOp, partition.getParameters())) {
                    try {
                        // issue new cache directive
                        if (!partition.isMarkedCached()) {
                            cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(partition, cacheOp.getCache_pool_name(), cacheReplication));
                        } else {
                            Long directiveId = HdfsCachingUtil.getCacheDirectiveId(partition.getParameters());
                            cacheDirIds.add(HdfsCachingUtil.modifyCacheDirective(directiveId, partition, cacheOp.getCache_pool_name(), cacheReplication));
                        }
                    } catch (ImpalaRuntimeException e) {
                        if (partition.isMarkedCached()) {
                            LOG.error("Unable to modify cache partition: " + partition.getPartitionName(), e);
                        } else {
                            LOG.error("Unable to cache partition: " + partition.getPartitionName(), e);
                        }
                    }
                    // Update the partition metadata.
                    try {
                        applyAlterPartition(tbl, partition);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        } else {
            loadFileMetadata = true;
        }
        // Nothing to do.
        if (cacheDirIds.isEmpty())
            return loadFileMetadata;
        // Submit a request to watch these cache directives. The TableLoadingMgr will
        // asynchronously refresh the table metadata once the directives complete.
        catalog_.watchCacheDirs(cacheDirIds, tableName.toThrift());
    } else {
        // Uncache the table.
        if (cacheDirId != null)
            HdfsCachingUtil.uncacheTbl(msTbl);
        // Uncache all table partitions.
        if (tbl.getNumClusteringCols() > 0) {
            for (HdfsPartition partition : hdfsTable.getPartitions()) {
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                if (partition.isMarkedCached()) {
                    HdfsCachingUtil.uncachePartition(partition);
                    try {
                        applyAlterPartition(tbl, partition);
                    } finally {
                        partition.markDirty();
                    }
                }
            }
        } else {
            loadFileMetadata = true;
        }
    }
    // Update the table metadata.
    applyAlterTable(msTbl);
    return loadFileMetadata;
}
#end_block

#method_before
private void alterPartitionSetCached(Table tbl, TAlterTableSetCachedParams params, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkArgument(tbl.getLock().isWriteLockedByCurrentThread());
    THdfsCachingOp cacheOp = params.getCache_op();
    Preconditions.checkNotNull(cacheOp);
    Preconditions.checkNotNull(params.getPartition_set());
    TableName tableName = tbl.getTableName();
    Preconditions.checkArgument(tbl instanceof HdfsTable);
    List<HdfsPartition> partitions = ((HdfsTable) tbl).getPartitionsFromPartitionSet(params.getPartition_set());
    List<HdfsPartition> modifiedParts = Lists.newArrayList();
    if (cacheOp.isSet_cached()) {
        for (HdfsPartition partition : partitions) {
            // The directive is null if the partition is not cached
            Long directiveId = HdfsCachingUtil.getCacheDirectiveId(partition.getParameters());
            short replication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
            List<Long> cacheDirs = Lists.newArrayList();
            if (directiveId == null) {
                cacheDirs.add(HdfsCachingUtil.submitCachePartitionDirective(partition, cacheOp.getCache_pool_name(), replication));
            } else {
                if (HdfsCachingUtil.isUpdateOp(cacheOp, partition.getParameters())) {
                    HdfsCachingUtil.validateCachePool(cacheOp, directiveId, tableName, partition);
                    cacheDirs.add(HdfsCachingUtil.modifyCacheDirective(directiveId, partition, cacheOp.getCache_pool_name(), replication));
                }
            }
            // until no more progress is made -- either fully cached or out of cache memory
            if (!cacheDirs.isEmpty()) {
                catalog_.watchCacheDirs(cacheDirs, tableName.toThrift());
            }
            if (!partition.isMarkedCached()) {
                modifiedParts.add(partition);
            }
        }
    } else {
        for (HdfsPartition partition : partitions) {
            if (partition.isMarkedCached()) {
                HdfsCachingUtil.uncachePartition(partition);
                modifiedParts.add(partition);
            }
        }
    }
    try {
        bulkAlterPartitions(tableName.getDb(), tableName.getTbl(), modifiedParts);
    } finally {
        for (HdfsPartition modifiedPart : modifiedParts) {
            modifiedPart.markDirty();
        }
    }
    numUpdatedPartitions.setRef((long) modifiedParts.size());
}
#method_after
private void alterPartitionSetCached(Table tbl, TAlterTableSetCachedParams params, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkArgument(tbl.getLock().isHeldByCurrentThread());
    THdfsCachingOp cacheOp = params.getCache_op();
    Preconditions.checkNotNull(cacheOp);
    Preconditions.checkNotNull(params.getPartition_set());
    TableName tableName = tbl.getTableName();
    Preconditions.checkArgument(tbl instanceof HdfsTable);
    List<HdfsPartition> partitions = ((HdfsTable) tbl).getPartitionsFromPartitionSet(params.getPartition_set());
    List<HdfsPartition> modifiedParts = Lists.newArrayList();
    if (cacheOp.isSet_cached()) {
        for (HdfsPartition partition : partitions) {
            // The directive is null if the partition is not cached
            Long directiveId = HdfsCachingUtil.getCacheDirectiveId(partition.getParameters());
            short replication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
            List<Long> cacheDirs = Lists.newArrayList();
            if (directiveId == null) {
                cacheDirs.add(HdfsCachingUtil.submitCachePartitionDirective(partition, cacheOp.getCache_pool_name(), replication));
            } else {
                if (HdfsCachingUtil.isUpdateOp(cacheOp, partition.getParameters())) {
                    HdfsCachingUtil.validateCachePool(cacheOp, directiveId, tableName, partition);
                    cacheDirs.add(HdfsCachingUtil.modifyCacheDirective(directiveId, partition, cacheOp.getCache_pool_name(), replication));
                }
            }
            // until no more progress is made -- either fully cached or out of cache memory
            if (!cacheDirs.isEmpty()) {
                catalog_.watchCacheDirs(cacheDirs, tableName.toThrift());
            }
            if (!partition.isMarkedCached()) {
                modifiedParts.add(partition);
            }
        }
    } else {
        for (HdfsPartition partition : partitions) {
            if (partition.isMarkedCached()) {
                HdfsCachingUtil.uncachePartition(partition);
                modifiedParts.add(partition);
            }
        }
    }
    try {
        bulkAlterPartitions(tableName.getDb(), tableName.getTbl(), modifiedParts);
    } finally {
        for (HdfsPartition modifiedPart : modifiedParts) {
            modifiedPart.markDirty();
        }
    }
    numUpdatedPartitions.setRef((long) modifiedParts.size());
}
#end_block

#method_before
private void alterTableRecoverPartitions(Table tbl) throws ImpalaException {
    Preconditions.checkArgument(tbl.getLock().isWriteLockedByCurrentThread());
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an HDFS table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    List<List<String>> partitionsNotInHms = hdfsTable.getPathsWithoutPartitions();
    if (partitionsNotInHms.isEmpty())
        return;
    List<Partition> hmsPartitions = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    TableName tableName = tbl.getTableName();
    for (List<String> partitionSpecValues : partitionsNotInHms) {
        hmsPartitions.add(createHmsPartitionFromValues(partitionSpecValues, msTbl, tableName, null));
    }
    String cachePoolName = null;
    Short replication = null;
    List<Long> cacheIds = Lists.newArrayList();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (parentTblCacheDirId != null) {
        // Inherit the HDFS cache value from the parent table.
        cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
        Preconditions.checkNotNull(cachePoolName);
        replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
        Preconditions.checkNotNull(replication);
    }
    // Add partitions to metastore.
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // ifNotExists and needResults are true.
        hmsPartitions = msClient.getHiveClient().add_partitions(hmsPartitions, true, true);
        for (Partition partition : hmsPartitions) {
            // Create and add the HdfsPartition. Return the table object with an updated
            // catalog version.
            addHdfsPartition(tbl, partition);
        }
        // Handle HDFS cache.
        if (cachePoolName != null) {
            for (Partition partition : hmsPartitions) {
                long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
                cacheIds.add(id);
            }
            // Update the partition metadata to include the cache directive id.
            msClient.getHiveClient().alter_partitions(tableName.getDb(), tableName.getTbl(), hmsPartitions);
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (AlreadyExistsException e) {
        // This may happen when another client of HMS has added the partitions.
        LOG.trace(String.format("Ignoring '%s' when adding partition to %s.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    }
    if (!cacheIds.isEmpty()) {
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    }
}
#method_after
private void alterTableRecoverPartitions(Table tbl) throws ImpalaException {
    Preconditions.checkArgument(tbl.getLock().isHeldByCurrentThread());
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an HDFS table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    List<List<String>> partitionsNotInHms = hdfsTable.getPathsWithoutPartitions();
    if (partitionsNotInHms.isEmpty())
        return;
    List<Partition> hmsPartitions = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    TableName tableName = tbl.getTableName();
    for (List<String> partitionSpecValues : partitionsNotInHms) {
        hmsPartitions.add(createHmsPartitionFromValues(partitionSpecValues, msTbl, tableName, null));
    }
    String cachePoolName = null;
    Short replication = null;
    List<Long> cacheIds = Lists.newArrayList();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (parentTblCacheDirId != null) {
        // Inherit the HDFS cache value from the parent table.
        cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
        Preconditions.checkNotNull(cachePoolName);
        replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
        Preconditions.checkNotNull(replication);
    }
    // Add partitions to metastore.
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // ifNotExists and needResults are true.
        hmsPartitions = msClient.getHiveClient().add_partitions(hmsPartitions, true, true);
        for (Partition partition : hmsPartitions) {
            // Create and add the HdfsPartition. Return the table object with an updated
            // catalog version.
            addHdfsPartition(tbl, partition);
        }
        // Handle HDFS cache.
        if (cachePoolName != null) {
            for (Partition partition : hmsPartitions) {
                long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
                cacheIds.add(id);
            }
            // Update the partition metadata to include the cache directive id.
            msClient.getHiveClient().alter_partitions(tableName.getDb(), tableName.getTbl(), hmsPartitions);
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (AlreadyExistsException e) {
        // This may happen when another client of HMS has added the partitions.
        LOG.trace(String.format("Ignoring '%s' when adding partition to %s.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    }
    if (!cacheIds.isEmpty()) {
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    }
}
#end_block

#method_before
private void applyAlterTable(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    long lastDdlTime = -1;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        lastDdlTime = calculateDdlTime(msTbl);
        msTbl.putToParameters("transient_lastDdlTime", Long.toString(lastDdlTime));
        msClient.getHiveClient().alter_table(msTbl.getDbName(), msTbl.getTableName(), msTbl);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    } finally {
        catalog_.updateLastDdlTime(new TTableName(msTbl.getDbName(), msTbl.getTableName()), lastDdlTime);
    }
}
#method_after
private void applyAlterTable(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    long lastDdlTime = -1;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        lastDdlTime = calculateDdlTime(msTbl);
        msTbl.putToParameters("transient_lastDdlTime", Long.toString(lastDdlTime));
        // TODO: Remove this workaround for HIVE-15653 to preserve table stats
        // during table alterations.
        msTbl.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
        msClient.getHiveClient().alter_table(msTbl.getDbName(), msTbl.getTableName(), msTbl);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    } finally {
        catalog_.updateLastDdlTime(new TTableName(msTbl.getDbName(), msTbl.getTableName()), lastDdlTime);
    }
}
#end_block

#method_before
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    while (true) {
        catalog_.getLock().writeLock().lock();
        if (table.getLock().writeLock().tryLock()) {
            try {
                long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
                catalog_.getLock().writeLock().unlock();
                // Collects the cache directive IDs of any cached table/partitions that were
                // targeted. A watch on these cache directives is submitted to the
                // TableLoadingMgr and the table will be refreshed asynchronously after all
                // cache directives complete.
                List<Long> cacheDirIds = Lists.<Long>newArrayList();
                // If the table is cached, get its cache pool name and replication factor. New
                // partitions will inherit this property.
                Pair<String, Short> cacheInfo = table.getTableCacheInfo(cacheDirIds);
                String cachePoolName = cacheInfo.first;
                Short cacheReplication = cacheInfo.second;
                TableName tblName = new TableName(table.getDb().getName(), table.getName());
                List<String> errorMessages = Lists.newArrayList();
                HashSet<String> partsToLoadMetadata = null;
                if (table.getNumClusteringCols() > 0) {
                    // Set of all partition names targeted by the insert that need to be created
                    // in the Metastore (partitions that do not currently exist in the catalog).
                    // In the BE, we don't currently distinguish between which targeted partitions
                    // are new and which already exist, so initialize the set with all targeted
                    // partition names and remove the ones that are found to exist.
                    HashSet<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
                    partsToLoadMetadata = Sets.newHashSet(partsToCreate);
                    for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
                        // Skip dummy default partition.
                        long partitionId = partition.getId();
                        if (partitionId == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                            continue;
                        }
                        // TODO: In the BE we build partition names without a trailing char. In FE
                        // we build partition name with a trailing char. We should make this
                        // consistent.
                        String partName = partition.getPartitionName() + "/";
                        // returns true, it indicates the partition already exists.
                        if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                            // The partition was targeted by the insert and is also a cached. Since
                            // data was written to the partition, a watch needs to be placed on the
                            // cache cache directive so the TableLoadingMgr can perform an async
                            // refresh once all data becomes cached.
                            cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
                        }
                        if (partsToCreate.size() == 0)
                            break;
                    }
                    if (!partsToCreate.isEmpty()) {
                        try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
                            org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
                            List<org.apache.hadoop.hive.metastore.api.Partition> hmsParts = Lists.newArrayList();
                            HiveConf hiveConf = new HiveConf(this.getClass());
                            Warehouse warehouse = new Warehouse(hiveConf);
                            for (String partName : partsToCreate) {
                                org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
                                hmsParts.add(partition);
                                partition.setDbName(tblName.getDb());
                                partition.setTableName(tblName.getTbl());
                                partition.setValues(getPartValsFromName(msTbl, partName));
                                partition.setParameters(new HashMap<String, String>());
                                partition.setSd(msTbl.getSd().deepCopy());
                                partition.getSd().setSerdeInfo(msTbl.getSd().getSerdeInfo().deepCopy());
                                partition.getSd().setLocation(msTbl.getSd().getLocation() + "/" + partName.substring(0, partName.length() - 1));
                                MetaStoreUtils.updatePartitionStatsFast(partition, warehouse);
                            }
                            // First add_partitions and then alter_partitions the successful ones with
                            // caching directives. The reason is that some partitions could have been
                            // added concurrently, and we want to avoid caching a partition twice and
                            // leaking a caching directive.
                            List<org.apache.hadoop.hive.metastore.api.Partition> addedHmsParts = msClient.getHiveClient().add_partitions(hmsParts, true, true);
                            if (addedHmsParts.size() > 0) {
                                if (cachePoolName != null) {
                                    List<org.apache.hadoop.hive.metastore.api.Partition> cachedHmsParts = Lists.newArrayList();
                                    // the directive id.
                                    for (org.apache.hadoop.hive.metastore.api.Partition part : addedHmsParts) {
                                        try {
                                            cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(part, cachePoolName, cacheReplication));
                                            cachedHmsParts.add(part);
                                        } catch (ImpalaRuntimeException e) {
                                            String msg = String.format("Partition %s.%s(%s): State: Not " + "cached. Action: Cache manully via 'ALTER TABLE'.", part.getDbName(), part.getTableName(), part.getValues());
                                            LOG.error(msg, e);
                                            errorMessages.add(msg);
                                        }
                                    }
                                    try {
                                        msClient.getHiveClient().alter_partitions(tblName.getDb(), tblName.getTbl(), cachedHmsParts);
                                    } catch (Exception e) {
                                        LOG.error("Failed in alter_partitions: ", e);
                                        // failed.
                                        for (org.apache.hadoop.hive.metastore.api.Partition part : cachedHmsParts) {
                                            try {
                                                HdfsCachingUtil.uncachePartition(part);
                                            } catch (ImpalaException e1) {
                                                String msg = String.format("Partition %s.%s(%s): State: Leaked caching directive. " + "Action: Manually uncache directory %s via hdfs " + "cacheAdmin.", part.getDbName(), part.getTableName(), part.getValues(), part.getSd().getLocation());
                                                LOG.error(msg, e);
                                                errorMessages.add(msg);
                                            }
                                        }
                                    }
                                }
                                updateLastDdlTime(msTbl, msClient);
                            }
                        } catch (AlreadyExistsException e) {
                            throw new InternalException("AlreadyExistsException thrown although ifNotExists given", e);
                        } catch (Exception e) {
                            throw new InternalException("Error adding partitions", e);
                        }
                    }
                }
                // Submit the watch request for the given cache directives.
                if (!cacheDirIds.isEmpty()) {
                    catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
                }
                response.setResult(new TCatalogUpdateResult());
                response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
                if (errorMessages.size() > 0) {
                    errorMessages.add("Please refer to the catalogd error log for details " + "regarding the failed un/caching operations.");
                    response.getResult().setStatus(new TStatus(TErrorCode.INTERNAL_ERROR, errorMessages));
                } else {
                    response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
                }
                loadTableMetadata(table, newCatalogVersion, true, false, partsToLoadMetadata);
                addTableToCatalogUpdate(table, response.result);
                return response;
            } finally {
                table.getLock().writeLock().unlock();
            }
        } else {
            catalog_.getLock().writeLock().unlock();
            continue;
        }
    }
}
#method_after
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    if (!catalog_.tryLockTable(table)) {
        throw new InternalException("Error updating the catalog due to lock contention.");
    }
    try {
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        catalog_.getLock().writeLock().unlock();
        // Collects the cache directive IDs of any cached table/partitions that were
        // targeted. A watch on these cache directives is submitted to the
        // TableLoadingMgr and the table will be refreshed asynchronously after all
        // cache directives complete.
        List<Long> cacheDirIds = Lists.<Long>newArrayList();
        // If the table is cached, get its cache pool name and replication factor. New
        // partitions will inherit this property.
        Pair<String, Short> cacheInfo = table.getTableCacheInfo(cacheDirIds);
        String cachePoolName = cacheInfo.first;
        Short cacheReplication = cacheInfo.second;
        TableName tblName = new TableName(table.getDb().getName(), table.getName());
        List<String> errorMessages = Lists.newArrayList();
        HashSet<String> partsToLoadMetadata = null;
        if (table.getNumClusteringCols() > 0) {
            // Set of all partition names targeted by the insert that need to be created
            // in the Metastore (partitions that do not currently exist in the catalog).
            // In the BE, we don't currently distinguish between which targeted partitions
            // are new and which already exist, so initialize the set with all targeted
            // partition names and remove the ones that are found to exist.
            HashSet<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
            partsToLoadMetadata = Sets.newHashSet(partsToCreate);
            for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
                // Skip dummy default partition.
                long partitionId = partition.getId();
                if (partitionId == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                // TODO: In the BE we build partition names without a trailing char. In FE
                // we build partition name with a trailing char. We should make this
                // consistent.
                String partName = partition.getPartitionName() + "/";
                // returns true, it indicates the partition already exists.
                if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                    // The partition was targeted by the insert and is also a cached. Since
                    // data was written to the partition, a watch needs to be placed on the
                    // cache cache directive so the TableLoadingMgr can perform an async
                    // refresh once all data becomes cached.
                    cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
                }
                if (partsToCreate.size() == 0)
                    break;
            }
            if (!partsToCreate.isEmpty()) {
                try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
                    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
                    List<org.apache.hadoop.hive.metastore.api.Partition> hmsParts = Lists.newArrayList();
                    HiveConf hiveConf = new HiveConf(this.getClass());
                    Warehouse warehouse = new Warehouse(hiveConf);
                    for (String partName : partsToCreate) {
                        org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
                        hmsParts.add(partition);
                        partition.setDbName(tblName.getDb());
                        partition.setTableName(tblName.getTbl());
                        partition.setValues(getPartValsFromName(msTbl, partName));
                        partition.setParameters(new HashMap<String, String>());
                        partition.setSd(msTbl.getSd().deepCopy());
                        partition.getSd().setSerdeInfo(msTbl.getSd().getSerdeInfo().deepCopy());
                        partition.getSd().setLocation(msTbl.getSd().getLocation() + "/" + partName.substring(0, partName.length() - 1));
                        MetaStoreUtils.updatePartitionStatsFast(partition, warehouse);
                    }
                    // First add_partitions and then alter_partitions the successful ones with
                    // caching directives. The reason is that some partitions could have been
                    // added concurrently, and we want to avoid caching a partition twice and
                    // leaking a caching directive.
                    List<org.apache.hadoop.hive.metastore.api.Partition> addedHmsParts = msClient.getHiveClient().add_partitions(hmsParts, true, true);
                    if (addedHmsParts.size() > 0) {
                        if (cachePoolName != null) {
                            List<org.apache.hadoop.hive.metastore.api.Partition> cachedHmsParts = Lists.newArrayList();
                            // the directive id.
                            for (org.apache.hadoop.hive.metastore.api.Partition part : addedHmsParts) {
                                try {
                                    cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(part, cachePoolName, cacheReplication));
                                    cachedHmsParts.add(part);
                                } catch (ImpalaRuntimeException e) {
                                    String msg = String.format("Partition %s.%s(%s): State: Not " + "cached. Action: Cache manully via 'ALTER TABLE'.", part.getDbName(), part.getTableName(), part.getValues());
                                    LOG.error(msg, e);
                                    errorMessages.add(msg);
                                }
                            }
                            try {
                                msClient.getHiveClient().alter_partitions(tblName.getDb(), tblName.getTbl(), cachedHmsParts);
                            } catch (Exception e) {
                                LOG.error("Failed in alter_partitions: ", e);
                                // failed.
                                for (org.apache.hadoop.hive.metastore.api.Partition part : cachedHmsParts) {
                                    try {
                                        HdfsCachingUtil.uncachePartition(part);
                                    } catch (ImpalaException e1) {
                                        String msg = String.format("Partition %s.%s(%s): State: Leaked caching directive. " + "Action: Manually uncache directory %s via hdfs " + "cacheAdmin.", part.getDbName(), part.getTableName(), part.getValues(), part.getSd().getLocation());
                                        LOG.error(msg, e);
                                        errorMessages.add(msg);
                                    }
                                }
                            }
                        }
                        updateLastDdlTime(msTbl, msClient);
                    }
                } catch (AlreadyExistsException e) {
                    throw new InternalException("AlreadyExistsException thrown although ifNotExists given", e);
                } catch (Exception e) {
                    throw new InternalException("Error adding partitions", e);
                }
            }
        }
        // Submit the watch request for the given cache directives.
        if (!cacheDirIds.isEmpty()) {
            catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
        }
        response.setResult(new TCatalogUpdateResult());
        response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
        if (errorMessages.size() > 0) {
            errorMessages.add("Please refer to the catalogd error log for details " + "regarding the failed un/caching operations.");
            response.getResult().setStatus(new TStatus(TErrorCode.INTERNAL_ERROR, errorMessages));
        } else {
            response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
        }
        loadTableMetadata(table, newCatalogVersion, true, false, partsToLoadMetadata);
        addTableToCatalogUpdate(table, response.result);
        return response;
    } finally {
        Preconditions.checkState(!catalog_.getLock().isWriteLockedByCurrentThread());
        table.getLock().unlock();
    }
}
#end_block

#method_before
private void applyAlterTable(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    long lastDdlTime = -1;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        lastDdlTime = calculateDdlTime(msTbl);
        msTbl.putToParameters("transient_lastDdlTime", Long.toString(lastDdlTime));
        // Workaround for HIVE-15653: Alterations should preserve table stats.
        msTbl.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
        msClient.getHiveClient().alter_table(msTbl.getDbName(), msTbl.getTableName(), msTbl);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    } finally {
        catalog_.updateLastDdlTime(new TTableName(msTbl.getDbName(), msTbl.getTableName()), lastDdlTime);
    }
}
#method_after
private void applyAlterTable(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    long lastDdlTime = -1;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        lastDdlTime = calculateDdlTime(msTbl);
        msTbl.putToParameters("transient_lastDdlTime", Long.toString(lastDdlTime));
        // TODO: Remove this workaround for HIVE-15653 to preserve table stats
        // during table alterations.
        msTbl.putToParameters(StatsSetupConst.STATS_GENERATED_VIA_STATS_TASK, StatsSetupConst.TRUE);
        msClient.getHiveClient().alter_table(msTbl.getDbName(), msTbl.getTableName(), msTbl);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    } finally {
        catalog_.updateLastDdlTime(new TTableName(msTbl.getDbName(), msTbl.getTableName()), lastDdlTime);
    }
}
#end_block

#method_before
private void loadTableMetadata(Table tbl, long newCatalogVersion, boolean reloadFileMetadata, boolean reloadTableSchema, Set<String> partitionsToUpdate) throws CatalogException {
    if (LOG.isDebugEnabled()) {
        LOG.debug("Loading table metadata: " + tbl.getFullName());
    }
    Preconditions.checkState(Thread.holdsLock(tbl));
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(msClient, tbl);
        if (tbl instanceof HdfsTable) {
            ((HdfsTable) tbl).load(true, msClient.getHiveClient(), msTbl, reloadFileMetadata, reloadTableSchema, partitionsToUpdate);
        } else {
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
    }
    tbl.setCatalogVersion(newCatalogVersion);
    if (LOG.isDebugEnabled()) {
        LOG.debug("Loaded table metadata: " + tbl.getFullName());
    }
}
#method_after
private void loadTableMetadata(Table tbl, long newCatalogVersion, boolean reloadFileMetadata, boolean reloadTableSchema, Set<String> partitionsToUpdate) throws CatalogException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(msClient, tbl);
        if (tbl instanceof HdfsTable) {
            ((HdfsTable) tbl).load(true, msClient.getHiveClient(), msTbl, reloadFileMetadata, reloadTableSchema, partitionsToUpdate);
        } else {
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
    }
    tbl.setCatalogVersion(newCatalogVersion);
}
#end_block

#method_before
private void loadBlockMetadata(Path dirPath, HashMap<Path, List<HdfsPartition>> partsByPath, BlockMetadataLoadStats stats) {
    Preconditions.checkNotNull(stats);
    try {
        FileSystem fs = dirPath.getFileSystem(CONF);
        // No need to load blocks for empty partitions list.
        if (partsByPath.size() == 0 || !fs.exists(dirPath))
            return;
        if (LOG.isTraceEnabled()) {
            LOG.trace("Loading block md for " + name_ + " directory " + dirPath.toString());
        }
        // on the current snapshot of files in the directory.
        for (Map.Entry<Path, List<HdfsPartition>> entry : partsByPath.entrySet()) {
            Path partDir = entry.getKey();
            if (!FileSystemUtil.isDescendantPath(partDir, dirPath))
                continue;
            for (HdfsPartition partition : entry.getValue()) {
                partition.setFileDescriptors(new ArrayList<FileDescriptor>());
            }
        }
        // block location metadata based on file formats.
        if (!FileSystemUtil.supportsStorageIds(fs)) {
            synthesizeBlockMetadata(fs, dirPath, partsByPath);
            return;
        }
        RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
        while (fileStatusIter.hasNext()) {
            LocatedFileStatus fileStatus = fileStatusIter.next();
            ++stats.fileCount;
            if (!FileSystemUtil.isValidDataFile(fileStatus))
                continue;
            // Find the partition that this file belongs (if any).
            Path partPathDir = fileStatus.getPath().getParent();
            Preconditions.checkNotNull(partPathDir);
            List<HdfsPartition> partitions = partsByPath.get(partPathDir);
            // Skip if this file does not belong to any known partition.
            if (partitions == null) {
                if (LOG.isTraceEnabled()) {
                    LOG.trace("File " + fileStatus.getPath().toString() + " doesn't correspond " + " to a known partition. Skipping metadata load for this file.");
                }
                continue;
            }
            String fileName = fileStatus.getPath().getName();
            FileDescriptor fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
            BlockLocation[] locations = fileStatus.getBlockLocations();
            String partPathDirName = partPathDir.toString();
            for (BlockLocation loc : locations) {
                Set<String> cachedHosts = Sets.newHashSet(loc.getCachedHosts());
                // Enumerate all replicas of the block, adding any unknown hosts
                // to hostIndex_. We pick the network address from getNames() and
                // map it to the corresponding hostname from getHosts().
                List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(loc.getNames().length);
                for (int i = 0; i < loc.getNames().length; ++i) {
                    TNetworkAddress networkAddress = BlockReplica.parseLocation(loc.getNames()[i]);
                    replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(loc.getHosts()[i])));
                }
                FileBlock currentBlock = new FileBlock(loc.getOffset(), loc.getLength(), replicas);
                THdfsFileBlock tHdfsFileBlock = currentBlock.toThrift();
                fd.addThriftFileBlock(tHdfsFileBlock);
                stats.unknownDiskIdCount += loadDiskIds(loc, tHdfsFileBlock);
            }
            if (LOG.isTraceEnabled()) {
                LOG.trace("Adding file md dir: " + partPathDirName + " file: " + fileName);
            }
            // Update the partitions' metadata that this file belongs to.
            for (HdfsPartition partition : partitions) {
                partition.getFileDescriptors().add(fd);
                numHdfsFiles_++;
                totalHdfsBytes_ += fd.getFileLength();
            }
            ++stats.tableFileCount;
        }
    } catch (IOException e) {
        throw new RuntimeException("Error loading block metadata for directory " + dirPath.toString() + ": " + e.getMessage(), e);
    }
}
#method_after
private void loadBlockMetadata(Path dirPath, HashMap<Path, List<HdfsPartition>> partsByPath) {
    try {
        FileSystem fs = dirPath.getFileSystem(CONF);
        // No need to load blocks for empty partitions list.
        if (partsByPath.size() == 0 || !fs.exists(dirPath))
            return;
        if (LOG.isTraceEnabled()) {
            LOG.trace("Loading block md for " + name_ + " directory " + dirPath.toString());
        }
        // on the current snapshot of files in the directory.
        for (Map.Entry<Path, List<HdfsPartition>> entry : partsByPath.entrySet()) {
            Path partDir = entry.getKey();
            if (!FileSystemUtil.isDescendantPath(partDir, dirPath))
                continue;
            for (HdfsPartition partition : entry.getValue()) {
                partition.setFileDescriptors(new ArrayList<FileDescriptor>());
            }
        }
        // block location metadata based on file formats.
        if (!FileSystemUtil.supportsStorageIds(fs)) {
            synthesizeBlockMetadata(fs, dirPath, partsByPath);
            return;
        }
        int unknownDiskIdCount = 0;
        RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
        while (fileStatusIter.hasNext()) {
            LocatedFileStatus fileStatus = fileStatusIter.next();
            if (!FileSystemUtil.isValidDataFile(fileStatus))
                continue;
            // Find the partition that this file belongs (if any).
            Path partPathDir = fileStatus.getPath().getParent();
            Preconditions.checkNotNull(partPathDir);
            List<HdfsPartition> partitions = partsByPath.get(partPathDir);
            // Skip if this file does not belong to any known partition.
            if (partitions == null) {
                if (LOG.isTraceEnabled()) {
                    LOG.trace("File " + fileStatus.getPath().toString() + " doesn't correspond " + " to a known partition. Skipping metadata load for this file.");
                }
                continue;
            }
            String fileName = fileStatus.getPath().getName();
            FileDescriptor fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
            BlockLocation[] locations = fileStatus.getBlockLocations();
            String partPathDirName = partPathDir.toString();
            for (BlockLocation loc : locations) {
                Set<String> cachedHosts = Sets.newHashSet(loc.getCachedHosts());
                // Enumerate all replicas of the block, adding any unknown hosts
                // to hostIndex_. We pick the network address from getNames() and
                // map it to the corresponding hostname from getHosts().
                List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(loc.getNames().length);
                for (int i = 0; i < loc.getNames().length; ++i) {
                    TNetworkAddress networkAddress = BlockReplica.parseLocation(loc.getNames()[i]);
                    replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(loc.getHosts()[i])));
                }
                FileBlock currentBlock = new FileBlock(loc.getOffset(), loc.getLength(), replicas);
                THdfsFileBlock tHdfsFileBlock = currentBlock.toThrift();
                fd.addThriftFileBlock(tHdfsFileBlock);
                unknownDiskIdCount += loadDiskIds(loc, tHdfsFileBlock);
            }
            if (LOG.isTraceEnabled()) {
                LOG.trace("Adding file md dir: " + partPathDirName + " file: " + fileName);
            }
            // Update the partitions' metadata that this file belongs to.
            for (HdfsPartition partition : partitions) {
                partition.getFileDescriptors().add(fd);
                numHdfsFiles_++;
                totalHdfsBytes_ += fd.getFileLength();
            }
        }
        if (unknownDiskIdCount > 0) {
            if (LOG.isWarnEnabled()) {
                LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException("Error loading block metadata for directory " + dirPath.toString() + ": " + e.getMessage(), e);
    }
}
#end_block

#method_before
private void loadAllPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl) throws IOException, CatalogException {
    Preconditions.checkNotNull(msTbl);
    if (LOG.isDebugEnabled()) {
        LOG.debug("Creating empty partition objects: " + getFullName());
    }
    initializePartitionMetadata(msTbl);
    // Map of partition paths to their corresponding HdfsPartition objects. Populated
    // using createPartition() calls. A single partition path can correspond to multiple
    // partitions.
    HashMap<Path, List<HdfsPartition>> partsByPath = Maps.newHashMap();
    Path tblLocation = getHdfsBaseDirPath();
    // List of directories that we scan for block locations. We optimize the block metadata
    // loading to reduce the number of RPCs to the NN by separately loading partitions
    // with default directory paths (under the base table directory) and non-default
    // directory paths. For the former we issue a single RPC to the NN to load all the
    // blocks from hdfsBaseDir_ and for the latter we load each of the partition directory
    // separately.
    // TODO: We can still do some advanced optimization by grouping all the partition
    // directories under the same ancestor path up the tree.
    List<Path> dirsToLoad = Lists.newArrayList(tblLocation);
    FileSystem fs = tblLocation.getFileSystem(CONF);
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null);
        partsByPath.put(tblLocation, Lists.newArrayList(part));
        if (isMarkedCached_)
            part.markCached();
        addPartition(part);
        if (fs.exists(tblLocation)) {
            accessLevel_ = getAvailableAccessLevel(fs, tblLocation);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition);
            addPartition(partition);
            // to this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null) {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
            Path partDir = new Path(msPartition.getSd().getLocation());
            List<HdfsPartition> parts = partsByPath.get(partDir);
            if (parts == null) {
                partsByPath.put(partDir, Lists.newArrayList(partition));
            } else {
                parts.add(partition);
            }
            if (!dirsToLoad.contains(partDir) && !FileSystemUtil.isDescendantPath(partDir, tblLocation)) {
                // This partition has a custom filesystem location. Load its file/block
                // metadata separately by adding it to the list of dirs to load.
                dirsToLoad.add(partDir);
            }
        }
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Created %s empty partition objects: %s", partsByPath.size(), getFullName()));
    }
    loadMetadataAndDiskIds(dirsToLoad, partsByPath);
}
#method_after
private void loadAllPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl) throws IOException, CatalogException {
    Preconditions.checkNotNull(msTbl);
    initializePartitionMetadata(msTbl);
    // Map of partition paths to their corresponding HdfsPartition objects. Populated
    // using createPartition() calls. A single partition path can correspond to multiple
    // partitions.
    HashMap<Path, List<HdfsPartition>> partsByPath = Maps.newHashMap();
    Path tblLocation = getHdfsBaseDirPath();
    // List of directories that we scan for block locations. We optimize the block metadata
    // loading to reduce the number of RPCs to the NN by separately loading partitions
    // with default directory paths (under the base table directory) and non-default
    // directory paths. For the former we issue a single RPC to the NN to load all the
    // blocks from hdfsBaseDir_ and for the latter we load each of the partition directory
    // separately.
    // TODO: We can still do some advanced optimization by grouping all the partition
    // directories under the same ancestor path up the tree.
    List<Path> dirsToLoad = Lists.newArrayList(tblLocation);
    FileSystem fs = tblLocation.getFileSystem(CONF);
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null);
        partsByPath.put(tblLocation, Lists.newArrayList(part));
        if (isMarkedCached_)
            part.markCached();
        addPartition(part);
        if (fs.exists(tblLocation)) {
            accessLevel_ = getAvailableAccessLevel(fs, tblLocation);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition);
            addPartition(partition);
            // to this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null) {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
            Path partDir = new Path(msPartition.getSd().getLocation());
            List<HdfsPartition> parts = partsByPath.get(partDir);
            if (parts == null) {
                partsByPath.put(partDir, Lists.newArrayList(partition));
            } else {
                parts.add(partition);
            }
            if (!dirsToLoad.contains(partDir) && !FileSystemUtil.isDescendantPath(partDir, tblLocation)) {
                // This partition has a custom filesystem location. Load its file/block
                // metadata separately by adding it to the list of dirs to load.
                dirsToLoad.add(partDir);
            }
        }
    }
    loadMetadataAndDiskIds(dirsToLoad, partsByPath);
}
#end_block

#method_before
private void loadMetadataAndDiskIds(List<Path> locations, HashMap<Path, List<HdfsPartition>> partsByPath) {
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Loading file and block metadata for %s partitions: %s", partsByPath.size(), getFullName()));
    }
    BlockMetadataLoadStats stats = new BlockMetadataLoadStats();
    for (Path location : locations) {
        loadBlockMetadata(location, partsByPath, stats);
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Loaded file and block metadata for %s partitions: %s", partsByPath.size(), getFullName()));
        LOG.debug(String.format("Loading stats: %s %s", getFullName(), stats.toString()));
    }
    if (stats.unknownDiskIdCount > 0) {
        if (LOG.isWarnEnabled()) {
            LOG.warn(String.format("Unknown disk id count for table: %s %s", getFullName(), stats.unknownDiskIdCount));
        }
    }
}
#method_after
private void loadMetadataAndDiskIds(List<Path> locations, HashMap<Path, List<HdfsPartition>> partsByPath) {
    LOG.info(String.format("Loading file and block metadata for %s partitions: %s", partsByPath.size(), getFullName()));
    for (Path location : locations) {
        loadBlockMetadata(location, partsByPath);
    }
    LOG.info(String.format("Loaded file and block metadata for %s partitions: %s", partsByPath.size(), getFullName()));
}
#end_block

#method_before
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl, boolean loadFileMetadata, boolean loadTableSchema, Set<String> partitionsToUpdate) throws TableLoadingException {
    // turn all exceptions into TableLoadingException
    msTable_ = msTbl;
    String tableName = db_.getName() + "." + name_;
    try {
        if (loadTableSchema)
            loadSchema(client, msTbl);
        if (reuseMetadata && getCatalogVersion() == Catalog.INITIAL_CATALOG_VERSION) {
            // This is the special case of CTAS that creates a 'temp' table that does not
            // actually exist in the Hive Metastore.
            initializePartitionMetadata(msTbl);
            updateStatsFromHmsTable(msTbl);
            return;
        }
        // Load partition and file metadata
        if (reuseMetadata) {
            // Incrementally update this table's partitions and file metadata
            if (LOG.isDebugEnabled()) {
                LOG.debug("Incrementally loading metadata for: " + tableName);
            }
            Preconditions.checkState(partitionsToUpdate == null || loadFileMetadata);
            updateMdFromHmsTable(msTbl);
            if (msTbl.getPartitionKeysSize() == 0) {
                if (loadFileMetadata)
                    updateUnpartitionedTableFileMd();
            } else {
                updatePartitionsFromHms(client, partitionsToUpdate, loadFileMetadata);
            }
            if (LOG.isDebugEnabled()) {
                LOG.debug("Incrementally loaded metadata for: " + tableName);
            }
        } else {
            // Load all partitions from Hive Metastore, including file metadata.
            if (LOG.isDebugEnabled()) {
                LOG.debug("Fetching partition metadata from the Metastore: " + tableName);
            }
            List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES);
            if (LOG.isDebugEnabled()) {
                LOG.debug("Fetched partition metadata from the Metastore: " + tableName);
            }
            loadAllPartitions(msPartitions, msTbl);
        }
        if (loadTableSchema)
            setAvroSchema(client, msTbl);
        updateStatsFromHmsTable(msTbl);
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#method_after
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl, boolean loadFileMetadata, boolean loadTableSchema, Set<String> partitionsToUpdate) throws TableLoadingException {
    // turn all exceptions into TableLoadingException
    msTable_ = msTbl;
    try {
        if (loadTableSchema)
            loadSchema(client, msTbl);
        if (reuseMetadata && getCatalogVersion() == Catalog.INITIAL_CATALOG_VERSION) {
            // This is the special case of CTAS that creates a 'temp' table that does not
            // actually exist in the Hive Metastore.
            initializePartitionMetadata(msTbl);
            updateStatsFromHmsTable(msTbl);
            return;
        }
        // Load partition and file metadata
        if (reuseMetadata) {
            // Incrementally update this table's partitions and file metadata
            LOG.info("Incrementally loading table metadata for: " + getFullName());
            Preconditions.checkState(partitionsToUpdate == null || loadFileMetadata);
            updateMdFromHmsTable(msTbl);
            if (msTbl.getPartitionKeysSize() == 0) {
                if (loadFileMetadata)
                    updateUnpartitionedTableFileMd();
            } else {
                updatePartitionsFromHms(client, partitionsToUpdate, loadFileMetadata);
            }
            LOG.info("Incrementally loaded table metadata for: " + getFullName());
        } else {
            // Load all partitions from Hive Metastore, including file metadata.
            LOG.info("Fetching partition metadata from the Metastore: " + getFullName());
            List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES);
            LOG.info("Fetched partition metadata from the Metastore: " + getFullName());
            loadAllPartitions(msPartitions, msTbl);
        }
        if (loadTableSchema)
            setAvroSchema(client, msTbl);
        updateStatsFromHmsTable(msTbl);
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#end_block

#method_before
private void updatePartitionsFromHms(IMetaStoreClient client, Set<String> partitionsToUpdate, boolean loadFileMetadata) throws Exception {
    if (LOG.isTraceEnabled()) {
        LOG.trace("sync table partitions: " + name_);
    }
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    Preconditions.checkState(msTbl.getPartitionKeysSize() != 0);
    Preconditions.checkState(loadFileMetadata || partitionsToUpdate == null);
    // Retrieve all the partition names from the Hive Metastore. We need this to
    // identify the delta between partitions of the local HdfsTable and the table entry
    // in the Hive Metastore. Note: This is a relatively "cheap" operation
    // (~.3 secs for 30K partitions).
    Set<String> msPartitionNames = Sets.newHashSet();
    msPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
    // Names of loaded partitions in this table
    Set<String> partitionNames = Sets.newHashSet();
    // Partitions for which file metadata must be loaded
    List<HdfsPartition> partitionsToUpdateFileMd = Lists.newArrayList();
    // Partitions that need to be dropped and recreated from scratch
    List<HdfsPartition> dirtyPartitions = Lists.newArrayList();
    // Partitions that need to be removed from this table. That includes dirty
    // partitions as well as partitions that were removed from the Hive Metastore.
    List<HdfsPartition> partitionsToRemove = Lists.newArrayList();
    // partitions that no longer exist in the Hive Metastore.
    for (HdfsPartition partition : partitionMap_.values()) {
        // Ignore the default partition
        if (partition.isDefaultPartition())
            continue;
        // that were removed from HMS using some external process, e.g. Hive.
        if (!msPartitionNames.contains(partition.getPartitionName())) {
            partitionsToRemove.add(partition);
        }
        if (partition.isDirty()) {
            // Dirty partitions are updated by removing them from table's partition
            // list and loading them from the Hive Metastore.
            dirtyPartitions.add(partition);
        } else {
            if (partitionsToUpdate == null && loadFileMetadata) {
                partitionsToUpdateFileMd.add(partition);
            }
        }
        Preconditions.checkNotNull(partition.getCachedMsPartitionDescriptor());
        partitionNames.add(partition.getPartitionName());
    }
    partitionsToRemove.addAll(dirtyPartitions);
    dropPartitions(partitionsToRemove);
    // Load dirty partitions from Hive Metastore
    loadPartitionsFromMetastore(dirtyPartitions, client);
    // Identify and load partitions that were added in the Hive Metastore but don't
    // exist in this table.
    Set<String> newPartitionsInHms = Sets.difference(msPartitionNames, partitionNames);
    loadPartitionsFromMetastore(newPartitionsInHms, client);
    // reloaded by loadPartitionsFromMetastore().
    if (partitionsToUpdate != null) {
        partitionsToUpdate.removeAll(newPartitionsInHms);
    }
    // descriptors and block metadata of a table (e.g. REFRESH statement).
    if (loadFileMetadata) {
        if (partitionsToUpdate != null) {
            // Only reload file metadata of partitions specified in 'partitionsToUpdate'
            Preconditions.checkState(partitionsToUpdateFileMd.isEmpty());
            partitionsToUpdateFileMd = getPartitionsByName(partitionsToUpdate);
        }
        loadPartitionFileMetadata(partitionsToUpdateFileMd);
    }
}
#method_after
private void updatePartitionsFromHms(IMetaStoreClient client, Set<String> partitionsToUpdate, boolean loadFileMetadata) throws Exception {
    if (LOG.isTraceEnabled())
        LOG.trace("Sync table partitions: " + name_);
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    Preconditions.checkState(msTbl.getPartitionKeysSize() != 0);
    Preconditions.checkState(loadFileMetadata || partitionsToUpdate == null);
    // Retrieve all the partition names from the Hive Metastore. We need this to
    // identify the delta between partitions of the local HdfsTable and the table entry
    // in the Hive Metastore. Note: This is a relatively "cheap" operation
    // (~.3 secs for 30K partitions).
    Set<String> msPartitionNames = Sets.newHashSet();
    msPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
    // Names of loaded partitions in this table
    Set<String> partitionNames = Sets.newHashSet();
    // Partitions for which file metadata must be loaded
    List<HdfsPartition> partitionsToUpdateFileMd = Lists.newArrayList();
    // Partitions that need to be dropped and recreated from scratch
    List<HdfsPartition> dirtyPartitions = Lists.newArrayList();
    // Partitions that need to be removed from this table. That includes dirty
    // partitions as well as partitions that were removed from the Hive Metastore.
    List<HdfsPartition> partitionsToRemove = Lists.newArrayList();
    // partitions that no longer exist in the Hive Metastore.
    for (HdfsPartition partition : partitionMap_.values()) {
        // Ignore the default partition
        if (partition.isDefaultPartition())
            continue;
        // that were removed from HMS using some external process, e.g. Hive.
        if (!msPartitionNames.contains(partition.getPartitionName())) {
            partitionsToRemove.add(partition);
        }
        if (partition.isDirty()) {
            // Dirty partitions are updated by removing them from table's partition
            // list and loading them from the Hive Metastore.
            dirtyPartitions.add(partition);
        } else {
            if (partitionsToUpdate == null && loadFileMetadata) {
                partitionsToUpdateFileMd.add(partition);
            }
        }
        Preconditions.checkNotNull(partition.getCachedMsPartitionDescriptor());
        partitionNames.add(partition.getPartitionName());
    }
    partitionsToRemove.addAll(dirtyPartitions);
    dropPartitions(partitionsToRemove);
    // Load dirty partitions from Hive Metastore
    loadPartitionsFromMetastore(dirtyPartitions, client);
    // Identify and load partitions that were added in the Hive Metastore but don't
    // exist in this table.
    Set<String> newPartitionsInHms = Sets.difference(msPartitionNames, partitionNames);
    loadPartitionsFromMetastore(newPartitionsInHms, client);
    // reloaded by loadPartitionsFromMetastore().
    if (partitionsToUpdate != null) {
        partitionsToUpdate.removeAll(newPartitionsInHms);
    }
    // descriptors and block metadata of a table (e.g. REFRESH statement).
    if (loadFileMetadata) {
        if (partitionsToUpdate != null) {
            // Only reload file metadata of partitions specified in 'partitionsToUpdate'
            Preconditions.checkState(partitionsToUpdateFileMd.isEmpty());
            partitionsToUpdateFileMd = getPartitionsByName(partitionsToUpdate);
        }
        loadPartitionFileMetadata(partitionsToUpdateFileMd);
    }
}
#end_block

#method_before
private AnalysisContext.AnalysisResult analyzeStmt(TQueryCtx queryCtx) throws AnalysisException, InternalException, AuthorizationException {
    if (!impaladCatalog_.isReady()) {
        throw new AnalysisException("This Impala daemon is not ready to accept user " + "requests. Status: Waiting for catalog update from the StateStore.");
    }
    AnalysisContext analysisCtx = new AnalysisContext(impaladCatalog_, queryCtx, authzConfig_);
    if (LOG.isDebugEnabled()) {
        LOG.debug("Compiling query: " + queryCtx.client_request.stmt);
    }
    // 3) Analysis fails with an AuthorizationException.
    try {
        while (true) {
            try {
                analysisCtx.analyze(queryCtx.client_request.stmt);
                Preconditions.checkState(analysisCtx.getAnalyzer().getMissingTbls().isEmpty());
                return analysisCtx.getAnalysisResult();
            } catch (AnalysisException e) {
                Set<TableName> missingTbls = analysisCtx.getAnalyzer().getMissingTbls();
                // Only re-throw the AnalysisException if there were no missing tables.
                if (missingTbls.isEmpty())
                    throw e;
                // Record that analysis needs table metadata
                analysisCtx.getTimeline().markEvent("Metadata load started");
                // Some tables/views were missing, request and wait for them to load.
                if (!requestTblLoadAndWait(missingTbls, MISSING_TBL_LOAD_WAIT_TIMEOUT_MS)) {
                    if (LOG.isTraceEnabled()) {
                        LOG.trace(String.format("Missing tables were not received in %dms. Load " + "request will be retried.", MISSING_TBL_LOAD_WAIT_TIMEOUT_MS));
                    }
                    analysisCtx.getTimeline().markEvent("Metadata load timeout");
                } else {
                    analysisCtx.getTimeline().markEvent("Metadata load finished");
                }
            }
        }
    } finally {
        // Authorize all accesses.
        // AuthorizationExceptions must take precedence over any AnalysisException
        // that has been thrown, so perform the authorization first.
        analysisCtx.authorize(getAuthzChecker());
    }
}
#method_after
private AnalysisContext.AnalysisResult analyzeStmt(TQueryCtx queryCtx) throws AnalysisException, InternalException, AuthorizationException {
    if (!impaladCatalog_.isReady()) {
        throw new AnalysisException("This Impala daemon is not ready to accept user " + "requests. Status: Waiting for catalog update from the StateStore.");
    }
    AnalysisContext analysisCtx = new AnalysisContext(impaladCatalog_, queryCtx, authzConfig_);
    LOG.info("Compiling query: " + queryCtx.client_request.stmt);
    // 3) Analysis fails with an AuthorizationException.
    try {
        while (true) {
            try {
                analysisCtx.analyze(queryCtx.client_request.stmt);
                Preconditions.checkState(analysisCtx.getAnalyzer().getMissingTbls().isEmpty());
                return analysisCtx.getAnalysisResult();
            } catch (AnalysisException e) {
                Set<TableName> missingTbls = analysisCtx.getAnalyzer().getMissingTbls();
                // Only re-throw the AnalysisException if there were no missing tables.
                if (missingTbls.isEmpty())
                    throw e;
                // Record that analysis needs table metadata
                analysisCtx.getTimeline().markEvent("Metadata load started");
                // Some tables/views were missing, request and wait for them to load.
                if (!requestTblLoadAndWait(missingTbls, MISSING_TBL_LOAD_WAIT_TIMEOUT_MS)) {
                    if (LOG.isWarnEnabled()) {
                        LOG.warn(String.format("Missing tables were not received in %dms. Load " + "request will be retried.", MISSING_TBL_LOAD_WAIT_TIMEOUT_MS));
                    }
                    analysisCtx.getTimeline().markEvent("Metadata load timeout");
                } else {
                    analysisCtx.getTimeline().markEvent("Metadata load finished");
                }
            }
        }
    } finally {
        // Authorize all accesses.
        // AuthorizationExceptions must take precedence over any AnalysisException
        // that has been thrown, so perform the authorization first.
        analysisCtx.authorize(getAuthzChecker());
        LOG.info("Compiled query.");
    }
}
#end_block

#method_before
public void run() {
    if (LOG.isTraceEnabled()) {
        LOG.trace("Reloading cache pool names from HDFS");
    }
    // Map of cache pool name to CachePoolInfo. Stored in a map to allow Set operations
    // to be performed on the keys.
    Map<String, CachePoolInfo> currentCachePools = Maps.newHashMap();
    try {
        DistributedFileSystem dfs = FileSystemUtil.getDistributedFileSystem();
        RemoteIterator<CachePoolEntry> itr = dfs.listCachePools();
        while (itr.hasNext()) {
            CachePoolInfo cachePoolInfo = itr.next().getInfo();
            currentCachePools.put(cachePoolInfo.getPoolName(), cachePoolInfo);
        }
    } catch (Exception e) {
        LOG.error("Error loading cache pools: ", e);
        return;
    }
    catalogLock_.writeLock().lock();
    try {
        // Determine what has changed relative to what we have cached.
        Set<String> droppedCachePoolNames = Sets.difference(hdfsCachePools_.keySet(), currentCachePools.keySet());
        Set<String> createdCachePoolNames = Sets.difference(currentCachePools.keySet(), hdfsCachePools_.keySet());
        // Add all new cache pools.
        for (String createdCachePool : createdCachePoolNames) {
            HdfsCachePool cachePool = new HdfsCachePool(currentCachePools.get(createdCachePool));
            cachePool.setCatalogVersion(CatalogServiceCatalog.this.incrementAndGetCatalogVersion());
            hdfsCachePools_.add(cachePool);
        }
        // Remove dropped cache pools.
        for (String cachePoolName : droppedCachePoolNames) {
            hdfsCachePools_.remove(cachePoolName);
            CatalogServiceCatalog.this.incrementAndGetCatalogVersion();
        }
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#method_after
public void run() {
    if (LOG.isTraceEnabled())
        LOG.trace("Reloading cache pool names from HDFS");
    // Map of cache pool name to CachePoolInfo. Stored in a map to allow Set operations
    // to be performed on the keys.
    Map<String, CachePoolInfo> currentCachePools = Maps.newHashMap();
    try {
        DistributedFileSystem dfs = FileSystemUtil.getDistributedFileSystem();
        RemoteIterator<CachePoolEntry> itr = dfs.listCachePools();
        while (itr.hasNext()) {
            CachePoolInfo cachePoolInfo = itr.next().getInfo();
            currentCachePools.put(cachePoolInfo.getPoolName(), cachePoolInfo);
        }
    } catch (Exception e) {
        LOG.error("Error loading cache pools: ", e);
        return;
    }
    catalogLock_.writeLock().lock();
    try {
        // Determine what has changed relative to what we have cached.
        Set<String> droppedCachePoolNames = Sets.difference(hdfsCachePools_.keySet(), currentCachePools.keySet());
        Set<String> createdCachePoolNames = Sets.difference(currentCachePools.keySet(), hdfsCachePools_.keySet());
        // Add all new cache pools.
        for (String createdCachePool : createdCachePoolNames) {
            HdfsCachePool cachePool = new HdfsCachePool(currentCachePools.get(createdCachePool));
            cachePool.setCatalogVersion(CatalogServiceCatalog.this.incrementAndGetCatalogVersion());
            hdfsCachePools_.add(cachePool);
        }
        // Remove dropped cache pools.
        for (String cachePoolName : droppedCachePoolNames) {
            hdfsCachePools_.remove(cachePoolName);
            CatalogServiceCatalog.this.incrementAndGetCatalogVersion();
        }
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#end_block

#method_before
private void loadFunctionsFromDbParams(Db db, org.apache.hadoop.hive.metastore.api.Database msDb) {
    if (msDb == null || msDb.getParameters() == null)
        return;
    if (LOG.isTraceEnabled()) {
        LOG.trace("Loading native functions for database: " + db.getName());
    }
    TCompactProtocol.Factory protocolFactory = new TCompactProtocol.Factory();
    for (String key : msDb.getParameters().keySet()) {
        if (!key.startsWith(Db.FUNCTION_INDEX_PREFIX))
            continue;
        try {
            TFunction fn = new TFunction();
            JniUtil.deserializeThrift(protocolFactory, fn, Base64.decodeBase64(msDb.getParameters().get(key)));
            Function addFn = Function.fromThrift(fn);
            db.addFunction(addFn, false);
            addFn.setCatalogVersion(incrementAndGetCatalogVersion());
        } catch (ImpalaException e) {
            LOG.error("Encountered an error during function load: key=" + key + ",continuing", e);
        }
    }
}
#method_after
private void loadFunctionsFromDbParams(Db db, org.apache.hadoop.hive.metastore.api.Database msDb) {
    if (msDb == null || msDb.getParameters() == null)
        return;
    LOG.info("Loading native functions for database: " + db.getName());
    TCompactProtocol.Factory protocolFactory = new TCompactProtocol.Factory();
    for (String key : msDb.getParameters().keySet()) {
        if (!key.startsWith(Db.FUNCTION_INDEX_PREFIX))
            continue;
        try {
            TFunction fn = new TFunction();
            JniUtil.deserializeThrift(protocolFactory, fn, Base64.decodeBase64(msDb.getParameters().get(key)));
            Function addFn = Function.fromThrift(fn);
            db.addFunction(addFn, false);
            addFn.setCatalogVersion(incrementAndGetCatalogVersion());
        } catch (ImpalaException e) {
            LOG.error("Encountered an error during function load: key=" + key + ",continuing", e);
        }
    }
    LOG.info("Loaded native functions for database: " + db.getName());
}
#end_block

#method_before
private void loadJavaFunctions(Db db, List<org.apache.hadoop.hive.metastore.api.Function> functions) {
    Preconditions.checkNotNull(functions);
    if (LOG.isTraceEnabled()) {
        LOG.trace("Loading Java functions for database: " + db.getName());
    }
    for (org.apache.hadoop.hive.metastore.api.Function function : functions) {
        try {
            for (Function fn : extractFunctions(db.getName(), function)) {
                db.addFunction(fn);
                fn.setCatalogVersion(incrementAndGetCatalogVersion());
            }
        } catch (Exception e) {
            LOG.error("Skipping function load: " + function.getFunctionName(), e);
        }
    }
}
#method_after
private void loadJavaFunctions(Db db, List<org.apache.hadoop.hive.metastore.api.Function> functions) {
    Preconditions.checkNotNull(functions);
    LOG.info("Loading Java functions for database: " + db.getName());
    for (org.apache.hadoop.hive.metastore.api.Function function : functions) {
        try {
            for (Function fn : extractFunctions(db.getName(), function)) {
                db.addFunction(fn);
                fn.setCatalogVersion(incrementAndGetCatalogVersion());
            }
        } catch (Exception e) {
            LOG.error("Skipping function load: " + function.getFunctionName(), e);
        }
    }
    LOG.info("Loaded Java functions for database: " + db.getName());
}
#end_block

#method_before
public void reset() throws CatalogException {
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                dbName = dbName.toLowerCase();
                Db oldDb = oldDbCache.get(dbName);
                Pair<Db, List<TTableName>> invalidatedDb = invalidateDb(msClient, dbName, oldDb);
                if (invalidatedDb == null)
                    continue;
                newDbCache.put(dbName, invalidatedDb.first);
                tblsToBackgroundLoad.addAll(invalidatedDb.second);
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#method_after
public void reset() throws CatalogException {
    LOG.info("Invalidating all metadata.");
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                dbName = dbName.toLowerCase();
                Db oldDb = oldDbCache.get(dbName);
                Pair<Db, List<TTableName>> invalidatedDb = invalidateDb(msClient, dbName, oldDb);
                if (invalidatedDb == null)
                    continue;
                newDbCache.put(dbName, invalidatedDb.first);
                tblsToBackgroundLoad.addAll(invalidatedDb.second);
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
    LOG.info("Invalidated all metadata.");
}
#end_block

#method_before
public Table reloadTable(Table tbl) throws CatalogException {
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Refreshing metadata: %s", tbl.getFullName()));
    }
    TTableName tblName = new TTableName(tbl.getDb().getName().toLowerCase(), tbl.getName().toLowerCase());
    Db db = tbl.getDb();
    if (tbl instanceof IncompleteTable) {
        TableLoadingMgr.LoadRequest loadReq;
        long previousCatalogVersion;
        // Return the table if it is already loaded or submit a new load request.
        catalogLock_.readLock().lock();
        try {
            previousCatalogVersion = tbl.getCatalogVersion();
            loadReq = tableLoadingMgr_.loadAsync(tblName);
        } finally {
            catalogLock_.readLock().unlock();
        }
        Preconditions.checkNotNull(loadReq);
        try {
            // only apply the update if the existing table hasn't changed.
            return replaceTableIfUnchanged(loadReq.get(), previousCatalogVersion);
        } finally {
            loadReq.close();
            if (LOG.isDebugEnabled()) {
                LOG.debug(String.format("Refreshed metadata: %s", tbl.getFullName()));
            }
        }
    }
    catalogLock_.writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = null;
            try {
                msTbl = msClient.getHiveClient().getTable(db.getName(), tblName.getTable_name());
            } catch (Exception e) {
                throw new TableLoadingException("Error loading metadata for table: " + db.getName() + "." + tblName.getTable_name(), e);
            }
            tbl.load(false, msClient.getHiveClient(), msTbl);
        }
        tbl.setCatalogVersion(newCatalogVersion);
        if (LOG.isDebugEnabled()) {
            LOG.debug(String.format("Refreshed metadata: %s", tbl.getFullName()));
        }
        return tbl;
    }
}
#method_after
public Table reloadTable(Table tbl) throws CatalogException {
    LOG.info(String.format("Refreshing table metadata: %s", tbl.getFullName()));
    TTableName tblName = new TTableName(tbl.getDb().getName().toLowerCase(), tbl.getName().toLowerCase());
    Db db = tbl.getDb();
    if (tbl instanceof IncompleteTable) {
        TableLoadingMgr.LoadRequest loadReq;
        long previousCatalogVersion;
        // Return the table if it is already loaded or submit a new load request.
        catalogLock_.readLock().lock();
        try {
            previousCatalogVersion = tbl.getCatalogVersion();
            loadReq = tableLoadingMgr_.loadAsync(tblName);
        } finally {
            catalogLock_.readLock().unlock();
        }
        Preconditions.checkNotNull(loadReq);
        try {
            // only apply the update if the existing table hasn't changed.
            return replaceTableIfUnchanged(loadReq.get(), previousCatalogVersion);
        } finally {
            loadReq.close();
            LOG.info(String.format("Refreshed table metadata: %s", tbl.getFullName()));
        }
    }
    catalogLock_.writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = null;
            try {
                msTbl = msClient.getHiveClient().getTable(db.getName(), tblName.getTable_name());
            } catch (Exception e) {
                throw new TableLoadingException("Error loading metadata for table: " + db.getName() + "." + tblName.getTable_name(), e);
            }
            tbl.load(false, msClient.getHiveClient(), msTbl);
        }
        tbl.setCatalogVersion(newCatalogVersion);
        LOG.info(String.format("Refreshed table metadata: %s", tbl.getFullName()));
        return tbl;
    }
}
#end_block

#method_before
public boolean invalidateTable(TTableName tableName, Pair<Db, Table> updatedObjects) {
    Preconditions.checkNotNull(updatedObjects);
    updatedObjects.first = null;
    updatedObjects.second = null;
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Invalidating table metadata: %s.%s", tableName.getDb_name(), tableName.getTable_name()));
    }
    String dbName = tableName.getDb_name();
    String tblName = tableName.getTable_name();
    // Stores whether the table exists in the metastore. Can have three states:
    // 1) true - Table exists in metastore.
    // 2) false - Table does not exist in metastore.
    // 3) unknown (null) - There was exception thrown by the metastore client.
    Boolean tableExistsInMetaStore;
    Db db = null;
    try (MetaStoreClient msClient = getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Database msDb = null;
        try {
            tableExistsInMetaStore = msClient.getHiveClient().tableExists(dbName, tblName);
        } catch (UnknownDBException e) {
            // The parent database does not exist in the metastore. Treat this the same
            // as if the table does not exist.
            tableExistsInMetaStore = false;
        } catch (TException e) {
            LOG.error("Error executing tableExists() metastore call: " + tblName, e);
            tableExistsInMetaStore = null;
        }
        if (tableExistsInMetaStore != null && !tableExistsInMetaStore) {
            updatedObjects.second = removeTable(dbName, tblName);
            return true;
        }
        db = getDb(dbName);
        if ((db == null || !db.containsTable(tblName)) && tableExistsInMetaStore == null) {
            // table exists in the metastore. Do nothing.
            return false;
        } else if (db == null && tableExistsInMetaStore) {
            // must be valid since tableExistsInMetaStore is true.
            try {
                msDb = msClient.getHiveClient().getDatabase(dbName);
                Preconditions.checkNotNull(msDb);
                db = new Db(dbName, this, msDb);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                addDb(db);
                updatedObjects.first = db;
            } catch (TException e) {
                // The metastore database cannot be get. Log the error and return.
                LOG.error("Error executing getDatabase() metastore call: " + dbName, e);
                return false;
            }
        }
    }
    // Add a new uninitialized table to the table cache, effectively invalidating
    // any existing entry. The metadata for the table will be loaded lazily, on the
    // on the next access to the table.
    Table newTable = IncompleteTable.createUninitializedTable(db, tblName);
    newTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(newTable);
    if (loadInBackground_) {
        tableLoadingMgr_.backgroundLoad(new TTableName(dbName.toLowerCase(), tblName.toLowerCase()));
    }
    updatedObjects.second = newTable;
    return false;
}
#method_after
public boolean invalidateTable(TTableName tableName, Pair<Db, Table> updatedObjects) {
    Preconditions.checkNotNull(updatedObjects);
    updatedObjects.first = null;
    updatedObjects.second = null;
    String dbName = tableName.getDb_name();
    String tblName = tableName.getTable_name();
    LOG.info(String.format("Invalidating table metadata: %s.%s", dbName, tblName));
    // Stores whether the table exists in the metastore. Can have three states:
    // 1) true - Table exists in metastore.
    // 2) false - Table does not exist in metastore.
    // 3) unknown (null) - There was exception thrown by the metastore client.
    Boolean tableExistsInMetaStore;
    Db db = null;
    try (MetaStoreClient msClient = getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Database msDb = null;
        try {
            tableExistsInMetaStore = msClient.getHiveClient().tableExists(dbName, tblName);
        } catch (UnknownDBException e) {
            // The parent database does not exist in the metastore. Treat this the same
            // as if the table does not exist.
            tableExistsInMetaStore = false;
        } catch (TException e) {
            LOG.error("Error executing tableExists() metastore call: " + tblName, e);
            tableExistsInMetaStore = null;
        }
        if (tableExistsInMetaStore != null && !tableExistsInMetaStore) {
            updatedObjects.second = removeTable(dbName, tblName);
            return true;
        }
        db = getDb(dbName);
        if ((db == null || !db.containsTable(tblName)) && tableExistsInMetaStore == null) {
            // table exists in the metastore. Do nothing.
            return false;
        } else if (db == null && tableExistsInMetaStore) {
            // must be valid since tableExistsInMetaStore is true.
            try {
                msDb = msClient.getHiveClient().getDatabase(dbName);
                Preconditions.checkNotNull(msDb);
                db = new Db(dbName, this, msDb);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                addDb(db);
                updatedObjects.first = db;
            } catch (TException e) {
                // The metastore database cannot be get. Log the error and return.
                LOG.error("Error executing getDatabase() metastore call: " + dbName, e);
                return false;
            }
        }
    }
    // Add a new uninitialized table to the table cache, effectively invalidating
    // any existing entry. The metadata for the table will be loaded lazily, on the
    // on the next access to the table.
    Table newTable = IncompleteTable.createUninitializedTable(db, tblName);
    newTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(newTable);
    if (loadInBackground_) {
        tableLoadingMgr_.backgroundLoad(new TTableName(dbName.toLowerCase(), tblName.toLowerCase()));
    }
    updatedObjects.second = newTable;
    return false;
}
#end_block

#method_before
public Table reloadPartition(Table tbl, List<TPartitionKeyValue> partitionSpec) throws CatalogException {
    catalogLock_.writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        HdfsTable hdfsTable = (HdfsTable) tbl;
        HdfsPartition hdfsPartition = hdfsTable.getPartitionFromThriftPartitionSpec(partitionSpec);
        // Retrieve partition name from existing partition or construct it from
        // the partition spec
        String partitionName = hdfsPartition == null ? HdfsTable.constructPartitionName(partitionSpec) : hdfsPartition.getPartitionName();
        if (LOG.isTraceEnabled()) {
            LOG.trace(String.format("Refreshing Partition metadata: %s %s", hdfsTable.getFullName(), partitionName));
        }
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Partition hmsPartition = null;
            try {
                hmsPartition = msClient.getHiveClient().getPartition(hdfsTable.getDb().getName(), hdfsTable.getName(), partitionName);
            } catch (NoSuchObjectException e) {
                // catalog
                if (hdfsPartition != null) {
                    hdfsTable.dropPartition(partitionSpec);
                    hdfsTable.setCatalogVersion(newCatalogVersion);
                }
                return hdfsTable;
            } catch (Exception e) {
                throw new CatalogException("Error loading metadata for partition: " + hdfsTable.getFullName() + " " + partitionName, e);
            }
            hdfsTable.reloadPartition(hdfsPartition, hmsPartition);
        }
        hdfsTable.setCatalogVersion(newCatalogVersion);
        return hdfsTable;
    }
}
#method_after
public Table reloadPartition(Table tbl, List<TPartitionKeyValue> partitionSpec) throws CatalogException {
    catalogLock_.writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        HdfsTable hdfsTable = (HdfsTable) tbl;
        HdfsPartition hdfsPartition = hdfsTable.getPartitionFromThriftPartitionSpec(partitionSpec);
        // Retrieve partition name from existing partition or construct it from
        // the partition spec
        String partitionName = hdfsPartition == null ? HdfsTable.constructPartitionName(partitionSpec) : hdfsPartition.getPartitionName();
        LOG.info(String.format("Refreshing partition metadata: %s %s", hdfsTable.getFullName(), partitionName));
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Partition hmsPartition = null;
            try {
                hmsPartition = msClient.getHiveClient().getPartition(hdfsTable.getDb().getName(), hdfsTable.getName(), partitionName);
            } catch (NoSuchObjectException e) {
                // catalog
                if (hdfsPartition != null) {
                    hdfsTable.dropPartition(partitionSpec);
                    hdfsTable.setCatalogVersion(newCatalogVersion);
                }
                return hdfsTable;
            } catch (Exception e) {
                throw new CatalogException("Error loading metadata for partition: " + hdfsTable.getFullName() + " " + partitionName, e);
            }
            hdfsTable.reloadPartition(hdfsPartition, hmsPartition);
        }
        hdfsTable.setCatalogVersion(newCatalogVersion);
        LOG.info(String.format("Refreshed partition metadata: %s %s", hdfsTable.getFullName(), partitionName));
        return hdfsTable;
    }
}
#end_block

#method_before
public Table load(Db db, String tblName) {
    String fullTblName = db.getName() + "." + tblName;
    if (LOG.isDebugEnabled())
        LOG.debug("Loading metadata for: " + fullTblName);
    Table table;
    // turn all exceptions into TableLoadingException
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = null;
        // All calls to getTable() need to be serialized due to HIVE-5457.
        synchronized (metastoreAccessLock_) {
            msTbl = msClient.getHiveClient().getTable(db.getName(), tblName);
        }
        // Check that the Hive TableType is supported
        TableType tableType = TableType.valueOf(msTbl.getTableType());
        if (!SUPPORTED_TABLE_TYPES.contains(tableType)) {
            throw new TableLoadingException(String.format("Unsupported table type '%s' for: %s", tableType, fullTblName));
        }
        // Create a table of appropriate type and have it load itself
        table = Table.fromMetastoreTable(db, msTbl);
        if (table == null) {
            throw new TableLoadingException("Unrecognized table type for table: " + fullTblName);
        }
        table.load(false, msClient.getHiveClient(), msTbl);
        table.validate();
    } catch (TableLoadingException e) {
        table = IncompleteTable.createFailedMetadataLoadTable(db, tblName, e);
    } catch (NoSuchObjectException e) {
        TableLoadingException tableDoesNotExist = new TableLoadingException("Table " + fullTblName + " no longer exists in the Hive MetaStore. " + "Run 'invalidate metadata " + fullTblName + "' to update the Impala " + "catalog.");
        table = IncompleteTable.createFailedMetadataLoadTable(db, tblName, tableDoesNotExist);
    } catch (Throwable e) {
        table = IncompleteTable.createFailedMetadataLoadTable(db, tblName, new TableLoadingException("Failed to load metadata for table: " + fullTblName + ". Running " + "'invalidate metadata " + fullTblName + "' may resolve this problem.", e));
    }
    if (LOG.isDebugEnabled())
        LOG.debug("Loaded metadata for: " + fullTblName);
    return table;
}
#method_after
public Table load(Db db, String tblName) {
    String fullTblName = db.getName() + "." + tblName;
    LOG.info("Loading metadata for: " + fullTblName);
    Table table;
    // turn all exceptions into TableLoadingException
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = null;
        // All calls to getTable() need to be serialized due to HIVE-5457.
        synchronized (metastoreAccessLock_) {
            msTbl = msClient.getHiveClient().getTable(db.getName(), tblName);
        }
        // Check that the Hive TableType is supported
        TableType tableType = TableType.valueOf(msTbl.getTableType());
        if (!SUPPORTED_TABLE_TYPES.contains(tableType)) {
            throw new TableLoadingException(String.format("Unsupported table type '%s' for: %s", tableType, fullTblName));
        }
        // Create a table of appropriate type and have it load itself
        table = Table.fromMetastoreTable(db, msTbl);
        if (table == null) {
            throw new TableLoadingException("Unrecognized table type for table: " + fullTblName);
        }
        table.load(false, msClient.getHiveClient(), msTbl);
        table.validate();
    } catch (TableLoadingException e) {
        table = IncompleteTable.createFailedMetadataLoadTable(db, tblName, e);
    } catch (NoSuchObjectException e) {
        TableLoadingException tableDoesNotExist = new TableLoadingException("Table " + fullTblName + " no longer exists in the Hive MetaStore. " + "Run 'invalidate metadata " + fullTblName + "' to update the Impala " + "catalog.");
        table = IncompleteTable.createFailedMetadataLoadTable(db, tblName, tableDoesNotExist);
    } catch (Throwable e) {
        table = IncompleteTable.createFailedMetadataLoadTable(db, tblName, new TableLoadingException("Failed to load metadata for table: " + fullTblName + ". Running " + "'invalidate metadata " + fullTblName + "' may resolve this problem.", e));
    }
    LOG.info("Loaded metadata for: " + fullTblName);
    return table;
}
#end_block

#method_before
public void prioritizeLoad(TTableName tblName) {
    tableLoadingSet_.add(tblName);
    tableLoadingDeque_.offerFirst(tblName);
}
#method_after
public void prioritizeLoad(TTableName tblName) {
    AtomicBoolean isLoading = tableLoadingBarrier_.putIfAbsent(tblName, new AtomicBoolean(false));
    // Only queue the table if a load is not already in progress.
    if (isLoading != null && isLoading.get())
        return;
    tableLoadingDeque_.offerFirst(tblName);
}
#end_block

#method_before
public void backgroundLoad(TTableName tblName) {
    // in the table loading set.
    if (tableLoadingSet_.add(tblName)) {
        tableLoadingDeque_.offerLast(tblName);
    }
}
#method_after
public void backgroundLoad(TTableName tblName) {
    // currently being loaded.
    if (tableLoadingBarrier_.putIfAbsent(tblName, new AtomicBoolean(false)) == null) {
        tableLoadingDeque_.offerLast(tblName);
    }
}
#end_block

#method_before
private void loadNextTable() throws InterruptedException {
    // Always get the next table from the head of the deque.
    final TTableName tblName = tableLoadingDeque_.takeFirst();
    tableLoadingSet_.remove(tblName);
    if (LOG.isDebugEnabled()) {
        LOG.debug("Loading next table from queue: " + tblName);
        LOG.debug(String.format("Remaining items in queue: %s. Loads in progress: %s", tableLoadingDeque_.size(), loadingTables_.size()));
    }
    // work on the remaining items in the table loading queue.
    if (loadingTables_.containsKey(tblName))
        return;
    try {
        // TODO: Instead of calling "getOrLoad" here we could call "loadAsync". We would
        // just need to add a mechanism for moving loaded tables into the Catalog.
        catalog_.getOrLoadTable(tblName.getDb_name(), tblName.getTable_name());
    } catch (CatalogException e) {
    // Ignore.
    }
}
#method_after
private void loadNextTable() throws InterruptedException {
    // Always get the next table from the head of the deque.
    final TTableName tblName = tableLoadingDeque_.takeFirst();
    LOG.info("Loading next table from queue: " + tblName.db_name + "." + tblName.table_name);
    LOG.info(String.format("Remaining items in queue: %s. Loads in progress: %s", tableLoadingDeque_.size(), loadingTables_.size()));
    AtomicBoolean isLoading = tableLoadingBarrier_.get(tblName);
    if (isLoading == null || !isLoading.compareAndSet(false, true)) {
        // Return so this thread can work on another table in the queue.
        return;
    }
    try {
        // TODO: Instead of calling "getOrLoad" here we could call "loadAsync". We would
        // just need to add a mechanism for moving loaded tables into the Catalog.
        catalog_.getOrLoadTable(tblName.getDb_name(), tblName.getTable_name());
    } catch (CatalogException e) {
    // Ignore.
    } finally {
        tableLoadingBarrier_.remove(tblName);
    }
}
#end_block

#method_before
@Test
public void TestCreateTable() throws AnalysisException {
    testToSql("create table p (a int)", "default", "CREATE TABLE default.p ( a INT ) STORED AS TEXTFILE", true);
}
#method_after
@Test
public void TestCreateTable() throws AnalysisException {
    testToSql("create table p (a int) partitioned by (day string) " + "comment 'This is a test'", "default", "CREATE TABLE default.p ( a INT ) PARTITIONED BY ( day STRING ) " + "COMMENT 'This is a test' STORED AS TEXTFILE", true);
}
#end_block

#method_before
private AnalysisContext.AnalysisResult analyzeStmt(TQueryCtx queryCtx) throws AnalysisException, InternalException, AuthorizationException {
    if (!impaladCatalog_.isReady()) {
        throw new AnalysisException("This Impala daemon is not ready to accept user " + "requests. Status: Waiting for catalog update from the StateStore.");
    }
    AnalysisContext analysisCtx = new AnalysisContext(impaladCatalog_, queryCtx, authzConfig_);
    if (LOG.isTraceEnabled())
        LOG.trace("analyze query " + queryCtx.client_request.stmt);
    // 3) Analysis fails with an AuthorizationException.
    try {
        while (true) {
            try {
                analysisCtx.analyze(queryCtx.client_request.stmt);
                Preconditions.checkState(analysisCtx.getAnalyzer().getMissingTbls().isEmpty());
                return analysisCtx.getAnalysisResult();
            } catch (AnalysisException e) {
                Set<TableName> missingTbls = analysisCtx.getAnalyzer().getMissingTbls();
                // Only re-throw the AnalysisException if there were no missing tables.
                if (missingTbls.isEmpty())
                    throw e;
                // Record that analysis needs table metadata
                analysisCtx.getTimeline().markEvent("Analysis Requires Metadata Load");
                // Some tables/views were missing, request and wait for them to load.
                if (!requestTblLoadAndWait(missingTbls, MISSING_TBL_LOAD_WAIT_TIMEOUT_MS)) {
                    if (LOG.isTraceEnabled()) {
                        LOG.trace(String.format("Missing tables were not received in %dms. Load " + "request will be retried.", MISSING_TBL_LOAD_WAIT_TIMEOUT_MS));
                    }
                    analysisCtx.getTimeline().markEvent("Metadata Load timeout");
                } else {
                    analysisCtx.getTimeline().markEvent("Metadata Load finished");
                }
            }
        }
    } finally {
        // Authorize all accesses.
        // AuthorizationExceptions must take precedence over any AnalysisException
        // that has been thrown, so perform the authorization first.
        analysisCtx.authorize(getAuthzChecker());
    }
}
#method_after
private AnalysisContext.AnalysisResult analyzeStmt(TQueryCtx queryCtx) throws AnalysisException, InternalException, AuthorizationException {
    if (!impaladCatalog_.isReady()) {
        throw new AnalysisException("This Impala daemon is not ready to accept user " + "requests. Status: Waiting for catalog update from the StateStore.");
    }
    AnalysisContext analysisCtx = new AnalysisContext(impaladCatalog_, queryCtx, authzConfig_);
    if (LOG.isTraceEnabled())
        LOG.trace("analyze query " + queryCtx.client_request.stmt);
    // 3) Analysis fails with an AuthorizationException.
    try {
        while (true) {
            try {
                analysisCtx.analyze(queryCtx.client_request.stmt);
                Preconditions.checkState(analysisCtx.getAnalyzer().getMissingTbls().isEmpty());
                return analysisCtx.getAnalysisResult();
            } catch (AnalysisException e) {
                Set<TableName> missingTbls = analysisCtx.getAnalyzer().getMissingTbls();
                // Only re-throw the AnalysisException if there were no missing tables.
                if (missingTbls.isEmpty())
                    throw e;
                // Record that analysis needs table metadata
                analysisCtx.getTimeline().markEvent("Metadata load started");
                // Some tables/views were missing, request and wait for them to load.
                if (!requestTblLoadAndWait(missingTbls, MISSING_TBL_LOAD_WAIT_TIMEOUT_MS)) {
                    if (LOG.isTraceEnabled()) {
                        LOG.trace(String.format("Missing tables were not received in %dms. Load " + "request will be retried.", MISSING_TBL_LOAD_WAIT_TIMEOUT_MS));
                    }
                    analysisCtx.getTimeline().markEvent("Metadata load timeout");
                } else {
                    analysisCtx.getTimeline().markEvent("Metadata load finished");
                }
            }
        }
    } finally {
        // Authorize all accesses.
        // AuthorizationExceptions must take precedence over any AnalysisException
        // that has been thrown, so perform the authorization first.
        analysisCtx.authorize(getAuthzChecker());
    }
}
#end_block

#method_before
public void analyze(String stmt, Analyzer analyzer) throws AnalysisException {
    SqlScanner input = new SqlScanner(new StringReader(stmt));
    SqlParser parser = new SqlParser(input);
    try {
        analysisResult_ = new AnalysisResult();
        analysisResult_.analyzer_ = analyzer;
        analysisResult_.timeline_ = timeline_;
        if (analysisResult_.analyzer_ == null) {
            analysisResult_.analyzer_ = new Analyzer(catalog_, queryCtx_, authzConfig_);
        }
        analysisResult_.stmt_ = (StatementBase) parser.parse().value;
        if (analysisResult_.stmt_ == null)
            return;
        analysisResult_.stmt_.analyze(analysisResult_.analyzer_);
        boolean isExplain = analysisResult_.isExplainStmt();
        // Apply expr and subquery rewrites.
        boolean reAnalyze = false;
        if (analysisResult_.requiresExprRewrite()) {
            rewriter_.reset();
            analysisResult_.stmt_.rewriteExprs(rewriter_);
            reAnalyze = rewriter_.changed();
        }
        if (analysisResult_.requiresSubqueryRewrite()) {
            StmtRewriter.rewrite(analysisResult_);
            reAnalyze = true;
        }
        if (reAnalyze) {
            // The rewrites should have no user-visible effect. Remember the original result
            // types and column labels to restore them after the rewritten stmt has been
            // reset() and re-analyzed.
            List<Type> origResultTypes = Lists.newArrayList();
            for (Expr e : analysisResult_.stmt_.getResultExprs()) {
                origResultTypes.add(e.getType());
            }
            List<String> origColLabels = Lists.newArrayList(analysisResult_.stmt_.getColLabels());
            // Re-analyze the stmt with a new analyzer.
            analysisResult_.analyzer_ = new Analyzer(catalog_, queryCtx_, authzConfig_);
            analysisResult_.stmt_.reset();
            analysisResult_.stmt_.analyze(analysisResult_.analyzer_);
            // Restore the original result types and column labels.
            analysisResult_.stmt_.castResultExprs(origResultTypes);
            analysisResult_.stmt_.setColLabels(origColLabels);
            if (LOG.isTraceEnabled()) {
                LOG.trace("rewrittenStmt: " + analysisResult_.stmt_.toSql());
            }
            if (isExplain)
                analysisResult_.stmt_.setIsExplain();
            Preconditions.checkState(!analysisResult_.requiresSubqueryRewrite());
        }
    } catch (AnalysisException e) {
        // Don't wrap AnalysisExceptions in another AnalysisException
        throw e;
    } catch (Exception e) {
        throw new AnalysisException(parser.getErrorMsg(stmt), e);
    }
}
#method_after
public void analyze(String stmt, Analyzer analyzer) throws AnalysisException {
    SqlScanner input = new SqlScanner(new StringReader(stmt));
    SqlParser parser = new SqlParser(input);
    try {
        analysisResult_ = new AnalysisResult();
        analysisResult_.analyzer_ = analyzer;
        if (analysisResult_.analyzer_ == null) {
            analysisResult_.analyzer_ = new Analyzer(catalog_, queryCtx_, authzConfig_);
        }
        analysisResult_.timeline_ = timeline_;
        analysisResult_.stmt_ = (StatementBase) parser.parse().value;
        if (analysisResult_.stmt_ == null)
            return;
        analysisResult_.stmt_.analyze(analysisResult_.analyzer_);
        boolean isExplain = analysisResult_.isExplainStmt();
        // Apply expr and subquery rewrites.
        boolean reAnalyze = false;
        if (analysisResult_.requiresExprRewrite()) {
            rewriter_.reset();
            analysisResult_.stmt_.rewriteExprs(rewriter_);
            reAnalyze = rewriter_.changed();
        }
        if (analysisResult_.requiresSubqueryRewrite()) {
            StmtRewriter.rewrite(analysisResult_);
            reAnalyze = true;
        }
        if (reAnalyze) {
            // The rewrites should have no user-visible effect. Remember the original result
            // types and column labels to restore them after the rewritten stmt has been
            // reset() and re-analyzed.
            List<Type> origResultTypes = Lists.newArrayList();
            for (Expr e : analysisResult_.stmt_.getResultExprs()) {
                origResultTypes.add(e.getType());
            }
            List<String> origColLabels = Lists.newArrayList(analysisResult_.stmt_.getColLabels());
            // Re-analyze the stmt with a new analyzer.
            analysisResult_.analyzer_ = new Analyzer(catalog_, queryCtx_, authzConfig_);
            analysisResult_.stmt_.reset();
            analysisResult_.stmt_.analyze(analysisResult_.analyzer_);
            // Restore the original result types and column labels.
            analysisResult_.stmt_.castResultExprs(origResultTypes);
            analysisResult_.stmt_.setColLabels(origColLabels);
            if (LOG.isTraceEnabled()) {
                LOG.trace("rewrittenStmt: " + analysisResult_.stmt_.toSql());
            }
            if (isExplain)
                analysisResult_.stmt_.setIsExplain();
            Preconditions.checkState(!analysisResult_.requiresSubqueryRewrite());
        }
    } catch (AnalysisException e) {
        // Don't wrap AnalysisExceptions in another AnalysisException
        throw e;
    } catch (Exception e) {
        throw new AnalysisException(parser.getErrorMsg(stmt), e);
    }
}
#end_block

#method_before
private void markConstantConjunct(Expr conjunct, boolean fromHavingClause) throws AnalysisException {
    if (!conjunct.isConstant() || isOjConjunct(conjunct))
        return;
    markConjunctAssigned(conjunct);
    if ((!fromHavingClause && !hasEmptySpjResultSet_) || (fromHavingClause && !hasEmptyResultSet_)) {
        try {
            if (conjunct instanceof BetweenPredicate) {
                // Rewrite the BetweenPredicate into a CompoundPredicate so we can evaluate it
                // below (BetweenPredicates are not executable). We might be in the first
                // analysis pass, so the conjunct may not have been rewritten yet.
                ExprRewriter rewriter = new ExprRewriter(BetweenToCompoundRule.INSTANCE);
                conjunct = rewriter.rewrite(conjunct, this);
            }
            if (!FeSupport.EvalPredicate(conjunct, globalState_.queryCtx)) {
                if (fromHavingClause) {
                    hasEmptyResultSet_ = true;
                } else {
                    hasEmptySpjResultSet_ = true;
                }
            }
        } catch (InternalException ex) {
            throw new AnalysisException("Error evaluating \"" + conjunct.toSql() + "\"", ex);
        }
    }
}
#method_after
private void markConstantConjunct(Expr conjunct, boolean fromHavingClause) throws AnalysisException {
    if (!conjunct.isConstant() || isOjConjunct(conjunct))
        return;
    markConjunctAssigned(conjunct);
    if ((!fromHavingClause && !hasEmptySpjResultSet_) || (fromHavingClause && !hasEmptyResultSet_)) {
        try {
            if (conjunct instanceof BetweenPredicate) {
                // Rewrite the BetweenPredicate into a CompoundPredicate so we can evaluate it
                // below (BetweenPredicates are not executable). We might be in the first
                // analysis pass, so the conjunct may not have been rewritten yet.
                ExprRewriter rewriter = new ExprRewriter(BetweenToCompoundRule.INSTANCE);
                conjunct = rewriter.rewrite(conjunct, this);
                // analyze this conjunct here: we know it can't contain references to select list
                // aliases and having it analyzed is needed for the following EvalPredicate() call
                conjunct.analyze(this);
                ;
            }
            if (!FeSupport.EvalPredicate(conjunct, globalState_.queryCtx)) {
                if (fromHavingClause) {
                    hasEmptyResultSet_ = true;
                } else {
                    hasEmptySpjResultSet_ = true;
                }
            }
        } catch (InternalException ex) {
            throw new AnalysisException("Error evaluating \"" + conjunct.toSql() + "\"", ex);
        }
    }
}
#end_block

#method_before
public HdfsPartition getPartitionFromThriftPartitionSpec(List<TPartitionKeyValue> partitionSpec) {
    // First, build a list of the partition values to search for in the same order they
    // are defined in the table.
    List<String> targetValues = Lists.newArrayList();
    Set<String> keys = Sets.newHashSet();
    for (FieldSchema fs : getMetaStoreTable().getPartitionKeys()) {
        for (TPartitionKeyValue kv : partitionSpec) {
            if (fs.getName().toLowerCase().equals(kv.getName().toLowerCase())) {
                targetValues.add(kv.getValue().toLowerCase());
                // Same key was specified twice
                if (!keys.add(kv.getName().toLowerCase())) {
                    return null;
                }
            }
        }
    }
    // Make sure the number of values match up and that some values were found.
    if (targetValues.size() == 0 || (targetValues.size() != getMetaStoreTable().getPartitionKeysSize())) {
        return null;
    }
    // match the values being searched for.
    for (HdfsPartition partition : partitionMap_.values()) {
        if (partition.isDefaultPartition())
            continue;
        List<LiteralExpr> partitionValues = partition.getPartitionValues();
        Preconditions.checkState(partitionValues.size() == targetValues.size());
        boolean matchFound = true;
        for (int i = 0; i < targetValues.size(); ++i) {
            String value;
            if (partitionValues.get(i) instanceof NullLiteral) {
                value = getNullPartitionKeyValue();
            } else {
                value = partitionValues.get(i).getStringValue();
                Preconditions.checkNotNull(value);
                // backwards compatibility with Hive, and is clearly broken.
                if (value.isEmpty())
                    value = getNullPartitionKeyValue();
            }
            if (!targetValues.get(i).equals(value.toLowerCase())) {
                matchFound = false;
                break;
            }
        }
        if (matchFound) {
            return partition;
        }
    }
    return null;
}
#method_after
public HdfsPartition getPartitionFromThriftPartitionSpec(List<TPartitionKeyValue> partitionSpec) {
    // First, build a list of the partition values to search for in the same order they
    // are defined in the table.
    List<String> targetValues = Lists.newArrayList();
    Set<String> keys = Sets.newHashSet();
    for (FieldSchema fs : getMetaStoreTable().getPartitionKeys()) {
        for (TPartitionKeyValue kv : partitionSpec) {
            if (fs.getName().toLowerCase().equals(kv.getName().toLowerCase())) {
                targetValues.add(kv.getValue());
                // Same key was specified twice
                if (!keys.add(kv.getName().toLowerCase())) {
                    return null;
                }
            }
        }
    }
    // Make sure the number of values match up and that some values were found.
    if (targetValues.size() == 0 || (targetValues.size() != getMetaStoreTable().getPartitionKeysSize())) {
        return null;
    }
    // match the values being searched for.
    for (HdfsPartition partition : partitionMap_.values()) {
        if (partition.isDefaultPartition())
            continue;
        List<LiteralExpr> partitionValues = partition.getPartitionValues();
        Preconditions.checkState(partitionValues.size() == targetValues.size());
        boolean matchFound = true;
        for (int i = 0; i < targetValues.size(); ++i) {
            String value;
            if (partitionValues.get(i) instanceof NullLiteral) {
                value = getNullPartitionKeyValue();
            } else {
                value = partitionValues.get(i).getStringValue();
                Preconditions.checkNotNull(value);
                // backwards compatibility with Hive, and is clearly broken.
                if (value.isEmpty())
                    value = getNullPartitionKeyValue();
            }
            if (!targetValues.get(i).equals(value)) {
                matchFound = false;
                break;
            }
        }
        if (matchFound) {
            return partition;
        }
    }
    return null;
}
#end_block

#method_before
public String checkConfiguration() {
    StringBuilder output = new StringBuilder();
    output.append(checkLogFilePermission());
    output.append(checkFileSystem(CONF));
    output.append(checkShortCircuitRead(CONF));
    output.append(checkBlockLocationTracking(CONF));
    return output.toString();
}
#method_after
public String checkConfiguration() {
    StringBuilder output = new StringBuilder();
    output.append(checkLogFilePermission());
    output.append(checkFileSystem(CONF));
    output.append(checkShortCircuitRead(CONF));
    return output.toString();
}
#end_block

#method_before
private static void addTableToCatalogUpdate(Table tbl, TCatalogUpdateResult result) {
    TCatalogObject updatedCatalogObject = TableToTCatalogObject(tbl);
    result.setUpdated_catalog_object_DEPRECATED(TableToTCatalogObject(tbl));
    result.setVersion(updatedCatalogObject.getCatalog_version());
}
#method_after
private static void addTableToCatalogUpdate(Table tbl, TCatalogUpdateResult result) {
    Preconditions.checkNotNull(tbl);
    TCatalogObject updatedCatalogObject = tbl.toTCatalogObject();
    result.setUpdated_catalog_object_DEPRECATED(updatedCatalogObject);
    result.setVersion(updatedCatalogObject.getCatalog_version());
}
#end_block

#method_before
private void dropDataSource(TDropDataSourceParams params, TDdlExecResponse resp) throws ImpalaException {
    if (LOG.isTraceEnabled())
        LOG.trace("Drop DATA SOURCE: " + params.toString());
    DataSource dataSource = catalog_.getDataSource(params.getData_source());
    if (dataSource == null) {
        if (!params.if_exists) {
            throw new ImpalaRuntimeException("Data source " + params.getData_source() + " does not exists.");
        }
        // The user specified IF EXISTS and the data source didn't exist, just
        // return the current catalog version.
        resp.result.setVersion(catalog_.getCatalogVersion());
        return;
    }
    catalog_.removeDataSource(params.getData_source());
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.DATA_SOURCE);
    removedObject.setData_source(dataSource.toThrift());
    removedObject.setCatalog_version(dataSource.getCatalogVersion());
    resp.result.setRemoved_catalog_object_DEPRECATED(removedObject);
    resp.result.setVersion(dataSource.getCatalogVersion());
}
#method_after
private void dropDataSource(TDropDataSourceParams params, TDdlExecResponse resp) throws ImpalaException {
    if (LOG.isTraceEnabled())
        LOG.trace("Drop DATA SOURCE: " + params.toString());
    DataSource dataSource = catalog_.removeDataSource(params.getData_source());
    if (dataSource == null) {
        if (!params.if_exists) {
            throw new ImpalaRuntimeException("Data source " + params.getData_source() + " does not exists.");
        }
        // No data source was removed.
        resp.result.setVersion(catalog_.getCatalogVersion());
        return;
    }
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.DATA_SOURCE);
    removedObject.setData_source(dataSource.toThrift());
    removedObject.setCatalog_version(dataSource.getCatalogVersion());
    resp.result.setRemoved_catalog_object_DEPRECATED(removedObject);
    resp.result.setVersion(dataSource.getCatalogVersion());
}
#end_block

#method_before
private void alterTableOrViewRename(Table oldTbl, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(oldTbl) && catalog_.getLock().isWriteLockedByCurrentThread());
    TableName tableName = oldTbl.getTableName();
    org.apache.hadoop.hive.metastore.api.Table msTbl = oldTbl.getMetaStoreTable().deepCopy();
    msTbl.setDbName(newTableName.getDb());
    msTbl.setTableName(newTableName.getTbl());
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column
        // stats across databases, we save, drop and restore the column stats because
        // the HMS does not properly move them to the new table via alteration.
        ColumnStatistics hmsColStats = null;
        if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
            Map<String, TColumnStats> colStats = Maps.newHashMap();
            for (Column c : oldTbl.getColumns()) {
                colStats.put(c.getName(), c.getStats().toThrift());
            }
            hmsColStats = createHiveColStats(colStats, oldTbl);
            // Set the new db/table.
            hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
            LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            // Delete all column stats of the original table from the HMS.
            msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
        }
        // Perform the table rename in any case.
        msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
        if (hmsColStats != null) {
            LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
        }
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object_DEPRECATED(removedObject);
    response.result.setUpdated_catalog_object_DEPRECATED(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#method_after
private void alterTableOrViewRename(Table oldTbl, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(oldTbl) && catalog_.getLock().isWriteLockedByCurrentThread());
    TableName tableName = oldTbl.getTableName();
    org.apache.hadoop.hive.metastore.api.Table msTbl = oldTbl.getMetaStoreTable().deepCopy();
    msTbl.setDbName(newTableName.getDb());
    msTbl.setTableName(newTableName.getTbl());
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column
        // stats across databases, we save, drop and restore the column stats because
        // the HMS does not properly move them to the new table via alteration.
        ColumnStatistics hmsColStats = null;
        if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
            Map<String, TColumnStats> colStats = Maps.newHashMap();
            for (Column c : oldTbl.getColumns()) {
                colStats.put(c.getName(), c.getStats().toThrift());
            }
            hmsColStats = createHiveColStats(colStats, oldTbl);
            // Set the new db/table.
            hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
            LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            // Delete all column stats of the original table from the HMS.
            msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
        }
        // Perform the table rename in any case.
        msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
        if (hmsColStats != null) {
            LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
        }
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    Table newTable = catalog_.renameTable(tableName.toThrift(), newTableName.toThrift());
    if (newTable == null) {
        // an inconsistent state, but can likely be fixed by running "invalidate metadata".
        throw new ImpalaRuntimeException(String.format("Table/view rename succeeded in the Hive Metastore, but failed in Impala's " + "Catalog Server. Running 'invalidate metadata <tbl>' on the old table name " + "'%s' and the new table name '%s' may fix the problem.", tableName.toString(), newTableName.toString()));
    }
    TCatalogObject addedObject = newTable.toTCatalogObject();
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable(tableName.getDb(), tableName.getTbl()));
    removedObject.setCatalog_version(addedObject.getCatalog_version());
    response.result.setRemoved_catalog_object_DEPRECATED(removedObject);
    response.result.setUpdated_catalog_object_DEPRECATED(addedObject);
    response.result.setVersion(addedObject.getCatalog_version());
}
#end_block

#method_before
private void createDropRole(User requestingUser, TCreateDropRoleParams createDropRoleParams, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(requestingUser);
    verifySentryServiceEnabled();
    Role role;
    if (createDropRoleParams.isIs_drop()) {
        role = catalog_.getSentryProxy().dropRole(requestingUser, createDropRoleParams.getRole_name());
        if (role == null) {
            role = new Role(createDropRoleParams.getRole_name(), Sets.<String>newHashSet());
            role.setCatalogVersion(catalog_.getCatalogVersion());
        }
    } else {
        role = catalog_.getSentryProxy().createRole(requestingUser, createDropRoleParams.getRole_name());
    }
    Preconditions.checkNotNull(role);
    TCatalogObject catalogObject = new TCatalogObject();
    catalogObject.setType(role.getCatalogObjectType());
    catalogObject.setRole(role.toThrift());
    catalogObject.setCatalog_version(role.getCatalogVersion());
    if (createDropRoleParams.isIs_drop()) {
        resp.result.setRemoved_catalog_object_DEPRECATED(catalogObject);
    } else {
        resp.result.setUpdated_catalog_object_DEPRECATED(catalogObject);
    }
    resp.result.setVersion(role.getCatalogVersion());
}
#method_after
private void createDropRole(User requestingUser, TCreateDropRoleParams createDropRoleParams, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(requestingUser);
    verifySentryServiceEnabled();
    Role role;
    if (createDropRoleParams.isIs_drop()) {
        role = catalog_.getSentryProxy().dropRole(requestingUser, createDropRoleParams.getRole_name());
        if (role == null) {
            // Nothing was removed from the catalogd's cache.
            resp.result.setVersion(catalog_.getCatalogVersion());
            return;
        }
    } else {
        role = catalog_.getSentryProxy().createRole(requestingUser, createDropRoleParams.getRole_name());
    }
    Preconditions.checkNotNull(role);
    TCatalogObject catalogObject = new TCatalogObject();
    catalogObject.setType(role.getCatalogObjectType());
    catalogObject.setRole(role.toThrift());
    catalogObject.setCatalog_version(role.getCatalogVersion());
    if (createDropRoleParams.isIs_drop()) {
        resp.result.setRemoved_catalog_object_DEPRECATED(catalogObject);
    } else {
        resp.result.setUpdated_catalog_object_DEPRECATED(catalogObject);
    }
    resp.result.setVersion(role.getCatalogVersion());
}
#end_block

#method_before
public TResetMetadataResponse execResetMetadata(TResetMetadataRequest req) throws CatalogException {
    TResetMetadataResponse resp = new TResetMetadataResponse();
    resp.setResult(new TCatalogUpdateResult());
    resp.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (req.isSetTable_name()) {
        // Tracks any CatalogObjects updated/added/removed as a result of
        // the invalidate metadata or refresh call. For refresh() it is only expected
        // that a table be modified, but for invalidateTable() the table's parent database
        // may have also been added if it did not previously exist in the catalog.
        Pair<Db, Table> modifiedObjects = new Pair<Db, Table>(null, null);
        boolean wasRemoved = false;
        if (req.isIs_refresh()) {
            TableName tblName = TableName.fromThrift(req.getTable_name());
            Table tbl = getExistingTable(tblName.getDb(), tblName.getTbl());
            if (tbl == null) {
                modifiedObjects.second = null;
            } else {
                if (req.isSetPartition_spec()) {
                    modifiedObjects.second = catalog_.reloadPartition(tbl, req.getPartition_spec());
                } else {
                    modifiedObjects.second = catalog_.reloadTable(tbl);
                }
            }
        } else {
            wasRemoved = catalog_.invalidateTable(req.getTable_name(), modifiedObjects);
        }
        if (modifiedObjects.first == null) {
            TCatalogObject thriftTable = TableToTCatalogObject(modifiedObjects.second);
            if (modifiedObjects.second != null) {
                // processed as a direct DDL operation.
                if (wasRemoved) {
                    resp.getResult().setRemoved_catalog_object_DEPRECATED(thriftTable);
                } else {
                    resp.getResult().setUpdated_catalog_object_DEPRECATED(thriftTable);
                }
            } else {
                // Table does not exist in the meta store and Impala catalog, throw error.
                throw new TableNotFoundException("Table not found: " + req.getTable_name().getDb_name() + "." + req.getTable_name().getTable_name());
            }
            resp.getResult().setVersion(thriftTable.getCatalog_version());
        } else {
            // If there were two catalog objects modified it indicates there was an
            // "invalidateTable()" call that added a new table AND database to the catalog.
            Preconditions.checkState(!req.isIs_refresh());
            Preconditions.checkNotNull(modifiedObjects.first);
            Preconditions.checkNotNull(modifiedObjects.second);
            // The database should always have a lower catalog version than the table because
            // it needs to be created before the table can be added.
            Preconditions.checkState(modifiedObjects.first.getCatalogVersion() < modifiedObjects.second.getCatalogVersion());
            // Since multiple catalog objects were modified, don't treat this as a direct DDL
            // operation. Just set the overall catalog version and the impalad will wait for
            // a statestore heartbeat that contains the update.
            resp.getResult().setVersion(modifiedObjects.second.getCatalogVersion());
        }
    } else {
        // Invalidate the entire catalog if no table name is provided.
        Preconditions.checkArgument(!req.isIs_refresh());
        catalog_.reset();
        resp.result.setVersion(catalog_.getCatalogVersion());
    }
    resp.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    return resp;
}
#method_after
public TResetMetadataResponse execResetMetadata(TResetMetadataRequest req) throws CatalogException {
    TResetMetadataResponse resp = new TResetMetadataResponse();
    resp.setResult(new TCatalogUpdateResult());
    resp.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (req.isSetTable_name()) {
        // Tracks any CatalogObjects updated/added/removed as a result of
        // the invalidate metadata or refresh call. For refresh() it is only expected
        // that a table be modified, but for invalidateTable() the table's parent database
        // may have also been added if it did not previously exist in the catalog.
        Pair<Db, Table> modifiedObjects = new Pair<Db, Table>(null, null);
        boolean wasRemoved = false;
        if (req.isIs_refresh()) {
            TableName tblName = TableName.fromThrift(req.getTable_name());
            Table tbl = getExistingTable(tblName.getDb(), tblName.getTbl());
            if (tbl == null) {
                modifiedObjects.second = null;
            } else {
                if (req.isSetPartition_spec()) {
                    modifiedObjects.second = catalog_.reloadPartition(tbl, req.getPartition_spec());
                } else {
                    modifiedObjects.second = catalog_.reloadTable(tbl);
                }
            }
        } else {
            wasRemoved = catalog_.invalidateTable(req.getTable_name(), modifiedObjects);
        }
        if (modifiedObjects.first == null) {
            if (modifiedObjects.second == null) {
                // Table does not exist in the meta store and Impala catalog, throw error.
                throw new TableNotFoundException("Table not found: " + req.getTable_name().getDb_name() + "." + req.getTable_name().getTable_name());
            }
            TCatalogObject thriftTable = modifiedObjects.second.toTCatalogObject();
            // processed as a direct DDL operation.
            if (wasRemoved) {
                resp.getResult().setRemoved_catalog_object_DEPRECATED(thriftTable);
            } else {
                resp.getResult().setUpdated_catalog_object_DEPRECATED(thriftTable);
            }
            resp.getResult().setVersion(thriftTable.getCatalog_version());
        } else {
            // If there were two catalog objects modified it indicates there was an
            // "invalidateTable()" call that added a new table AND database to the catalog.
            Preconditions.checkState(!req.isIs_refresh());
            Preconditions.checkNotNull(modifiedObjects.first);
            Preconditions.checkNotNull(modifiedObjects.second);
            // The database should always have a lower catalog version than the table because
            // it needs to be created before the table can be added.
            Preconditions.checkState(modifiedObjects.first.getCatalogVersion() < modifiedObjects.second.getCatalogVersion());
            // Since multiple catalog objects were modified, don't treat this as a direct DDL
            // operation. Just set the overall catalog version and the impalad will wait for
            // a statestore heartbeat that contains the update.
            resp.getResult().setVersion(modifiedObjects.second.getCatalogVersion());
        }
    } else {
        // Invalidate the entire catalog if no table name is provided.
        Preconditions.checkArgument(!req.isIs_refresh());
        catalog_.reset();
        resp.result.setVersion(catalog_.getCatalogVersion());
    }
    resp.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    return resp;
}
#end_block

#method_before
@Override
protected Object decodeLast(final ChannelHandlerContext ctx, final Channel chan, final ChannelBuffer buf, final VoidEnum unused) throws NonRecoverableException {
    // doesn't contain enough data, which unnecessarily pollutes the logs.
    if (buf.readable()) {
        try {
            return decode(ctx, chan, buf, unused);
        } finally {
            if (buf.readable()) {
                LOG.error(getPeerUuidLoggingString() + "After decoding the last message on " + chan + ", there was still some undecoded bytes in the channel's" + " buffer (which are going to be lost): " + buf);
            }
        }
    } else {
        return null;
    }
}
#method_after
@Override
protected Object decodeLast(final ChannelHandlerContext ctx, final Channel chan, final ChannelBuffer buf, final VoidEnum unused) throws NonRecoverableException {
    // doesn't contain enough data, which unnecessarily pollutes the logs.
    if (buf.readable()) {
        try {
            return decode(ctx, chan, buf, unused);
        } finally {
            if (buf.readable()) {
                LOG.error(getPeerUuidLoggingString() + "After decoding the last message on " + chan + ", there was still some undecoded bytes in the channel's" + " buffer (which are going to be lost)");
            }
        }
    } else {
        return null;
    }
}
#end_block

#method_before
public void setJoinHints(List<PlanHint> hints) {
    this.joinHints_ = hints;
}
#method_after
public void setJoinHints(List<PlanHint> hints) {
    Preconditions.checkNotNull(hints);
    joinHints_ = hints;
}
#end_block

#method_before
public void setTableHints(List<PlanHint> hints) {
    this.tableHints_ = hints;
}
#method_after
public void setTableHints(List<PlanHint> hints) {
    Preconditions.checkNotNull(hints);
    tableHints_ = hints;
}
#end_block

#method_before
private void analyzeTableHints(Analyzer analyzer) {
    if (tableHints_ == null)
        return;
    if (!(this instanceof BaseTableRef)) {
        analyzer.addWarning("Table hints not supported for inline view and collections");
        return;
    }
    // BaseTableRef will always have their path resolved at this point.
    Preconditions.checkState(getResolvedPath() != null);
    if (getResolvedPath().destTable() != null && !(getResolvedPath().destTable() instanceof HdfsTable)) {
        analyzer.addWarning("Table hints only supported for Hdfs tables");
    }
    for (PlanHint hint : tableHints_) {
        if (hint.is("SCHEDULE_CACHE_LOCAL")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.CACHE_LOCAL;
        } else if (hint.is("SCHEDULE_DISK_LOCAL")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.DISK_LOCAL;
        } else if (hint.is("SCHEDULE_REMOTE")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.REMOTE;
        } else if (hint.is("SCHEDULE_RANDOM_REPLICA")) {
            analyzer.setHasPlanHints();
            randomReplica_ = true;
        } else {
            Preconditions.checkState(getAliases() != null && getAliases().length > 0);
            analyzer.addWarning("Table hint not recognized for table " + getUniqueAlias() + ": " + hint);
        }
    }
}
#method_after
private void analyzeTableHints(Analyzer analyzer) {
    if (tableHints_.isEmpty())
        return;
    if (!(this instanceof BaseTableRef)) {
        analyzer.addWarning("Table hints not supported for inline view and collections");
        return;
    }
    // BaseTableRef will always have their path resolved at this point.
    Preconditions.checkState(getResolvedPath() != null);
    if (getResolvedPath().destTable() != null && !(getResolvedPath().destTable() instanceof HdfsTable)) {
        analyzer.addWarning("Table hints only supported for Hdfs tables");
    }
    for (PlanHint hint : tableHints_) {
        if (hint.is("SCHEDULE_CACHE_LOCAL")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.CACHE_LOCAL;
        } else if (hint.is("SCHEDULE_DISK_LOCAL")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.DISK_LOCAL;
        } else if (hint.is("SCHEDULE_REMOTE")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.REMOTE;
        } else if (hint.is("SCHEDULE_RANDOM_REPLICA")) {
            analyzer.setHasPlanHints();
            randomReplica_ = true;
        } else {
            Preconditions.checkState(getAliases() != null && getAliases().length > 0);
            analyzer.addWarning("Table hint not recognized for table " + getUniqueAlias() + ": " + hint);
        }
    }
}
#end_block

#method_before
private void analyzeJoinHints(Analyzer analyzer) throws AnalysisException {
    if (joinHints_ == null)
        return;
    for (PlanHint hint : joinHints_) {
        if (hint.is("BROADCAST")) {
            if (joinOp_ == JoinOperator.RIGHT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN || joinOp_ == JoinOperator.RIGHT_SEMI_JOIN || joinOp_ == JoinOperator.RIGHT_ANTI_JOIN) {
                throw new AnalysisException(joinOp_.toString() + " does not support BROADCAST.");
            }
            if (isPartitionedJoin()) {
                throw new AnalysisException("Conflicting JOIN hint: " + hint);
            }
            distrMode_ = DistributionMode.BROADCAST;
            analyzer.setHasPlanHints();
        } else if (hint.is("SHUFFLE")) {
            if (joinOp_ == JoinOperator.CROSS_JOIN) {
                throw new AnalysisException("CROSS JOIN does not support SHUFFLE.");
            }
            if (isBroadcastJoin()) {
                throw new AnalysisException("Conflicting JOIN hint: " + hint);
            }
            distrMode_ = DistributionMode.PARTITIONED;
            analyzer.setHasPlanHints();
        } else {
            analyzer.addWarning("JOIN hint not recognized: " + hint);
        }
    }
}
#method_after
private void analyzeJoinHints(Analyzer analyzer) throws AnalysisException {
    if (joinHints_.isEmpty())
        return;
    for (PlanHint hint : joinHints_) {
        if (hint.is("BROADCAST")) {
            if (joinOp_ == JoinOperator.RIGHT_OUTER_JOIN || joinOp_ == JoinOperator.FULL_OUTER_JOIN || joinOp_ == JoinOperator.RIGHT_SEMI_JOIN || joinOp_ == JoinOperator.RIGHT_ANTI_JOIN) {
                throw new AnalysisException(joinOp_.toString() + " does not support BROADCAST.");
            }
            if (isPartitionedJoin()) {
                throw new AnalysisException("Conflicting JOIN hint: " + hint);
            }
            distrMode_ = DistributionMode.BROADCAST;
            analyzer.setHasPlanHints();
        } else if (hint.is("SHUFFLE")) {
            if (joinOp_ == JoinOperator.CROSS_JOIN) {
                throw new AnalysisException("CROSS JOIN does not support SHUFFLE.");
            }
            if (isBroadcastJoin()) {
                throw new AnalysisException("Conflicting JOIN hint: " + hint);
            }
            distrMode_ = DistributionMode.PARTITIONED;
            analyzer.setHasPlanHints();
        } else {
            analyzer.addWarning("JOIN hint not recognized: " + hint);
        }
    }
}
#end_block

#method_before
@Override
public String toSql() {
    if (joinOp_ == null) {
        // explicit JOIN clause
        return (leftTblRef_ != null ? ", " : "") + tableRefToSql();
    }
    StringBuilder output = new StringBuilder(" " + joinOp_.toString() + " ");
    if (joinHints_ != null)
        output.append(ToSqlUtils.getPlanHintsSql(joinHints_) + " ");
    output.append(tableRefToSql());
    if (usingColNames_ != null) {
        output.append(" USING (").append(Joiner.on(", ").join(usingColNames_)).append(")");
    } else if (onClause_ != null) {
        output.append(" ON ").append(onClause_.toSql());
    }
    return output.toString();
}
#method_after
@Override
public String toSql() {
    if (joinOp_ == null) {
        // explicit JOIN clause
        return (leftTblRef_ != null ? ", " : "") + tableRefToSql();
    }
    StringBuilder output = new StringBuilder(" " + joinOp_.toString() + " ");
    if (!joinHints_.isEmpty())
        output.append(ToSqlUtils.getPlanHintsSql(joinHints_) + " ");
    output.append(tableRefToSql());
    if (usingColNames_ != null) {
        output.append(" USING (").append(Joiner.on(", ").join(usingColNames_)).append(")");
    } else if (onClause_ != null) {
        output.append(" ON ").append(onClause_.toSql());
    }
    return output.toString();
}
#end_block

#method_before
@Test
public void TestMultilineComment() {
    ParserError("/**/");
    ParserError("/*****/");
    ParserError("/* select 1 */");
    ParserError("/*/ select 1");
    ParserError("select 1 /*/");
    ParsesOk("/**/select 1");
    ParsesOk("select/* */1");
    ParsesOk("/** */ select 1");
    ParsesOk("select 1/* **/");
    ParsesOk("/*/*/select 1");
    ParsesOk("/*//*/select 1");
    ParsesOk("select 1/***/");
    ParsesOk("/*****/select 1");
    ParsesOk("/**//**/select 1");
    ParserError("/**/**/select 1");
    ParsesOk("\nselect 1/**/");
    ParsesOk("/*\n*/select 1");
    ParsesOk("/*\r*/select 1");
    ParsesOk("/*\r\n*/select 1");
    ParsesOk("/**\n* Doc style\n*/select 1");
    ParsesOk("/************\n*\n* Header style\n*\n***********/select 1");
    ParsesOk("/* 1 */ select 1 /* 2 */");
    ParsesOk("select\n/**/\n1");
    ParserError("/**// select 1");
    ParserError("/**/*/ select 1");
    ParserError("/ **/ select 1");
    ParserError("/** / select 1");
    ParserError("/\n**/ select 1");
    ParserError("/**\n/ select 1");
    ParsesOk("/*--*/ select 1");
    ParsesOk("/* --foo */ select 1");
    ParsesOk("/*\n--foo */ select 1");
    ParsesOk("/*\n--foo\n*/ select 1");
    ParserError("select 1 /* --bar");
    ParserError("select 1 /*--");
    ParsesOk("/* select 1; */ select 1");
    ParsesOk("/** select 1; */ select 1");
    ParsesOk("/* select */ select 1 /* 1 */");
    ParsesOk("select 1 /* sortby(() */");
    ParserError("select 1 /*+ sortby() */");
    ParserError("select 1 /*+ sortby(() */");
}
#method_after
@Test
public void TestMultilineComment() {
    ParserError("/**/");
    ParserError("/*****/");
    ParserError("/* select 1 */");
    ParserError("/*/ select 1");
    ParserError("select 1 /*/");
    ParsesOk("/**/select 1");
    ParsesOk("select/* */1");
    ParsesOk("/** */ select 1");
    ParsesOk("select 1/* **/");
    ParsesOk("/*/*/select 1");
    ParsesOk("/*//*/select 1");
    ParsesOk("select 1/***/");
    ParsesOk("/*****/select 1");
    ParsesOk("/**//**/select 1");
    ParserError("/**/**/select 1");
    ParsesOk("\nselect 1/**/");
    ParsesOk("/*\n*/select 1");
    ParsesOk("/*\r*/select 1");
    ParsesOk("/*\r\n*/select 1");
    ParsesOk("/**\n* Doc style\n*/select 1");
    ParsesOk("/************\n*\n* Header style\n*\n***********/select 1");
    ParsesOk("/* 1 */ select 1 /* 2 */");
    ParsesOk("select\n/**/\n1");
    ParserError("/**// select 1");
    ParserError("/**/*/ select 1");
    ParserError("/ **/ select 1");
    ParserError("/** / select 1");
    ParserError("/\n**/ select 1");
    ParserError("/**\n/ select 1");
    ParsesOk("/*--*/ select 1");
    ParsesOk("/* --foo */ select 1");
    ParsesOk("/*\n--foo */ select 1");
    ParsesOk("/*\n--foo\n*/ select 1");
    ParserError("select 1 /* --bar");
    ParserError("select 1 /*--");
    ParsesOk("/* select 1; */ select 1");
    ParsesOk("/** select 1; */ select 1");
    ParsesOk("/* select */ select 1 /* 1 */");
    ParsesOk("select 1 /* sortby(() */");
    // Empty columns list in sortby hint
    ParserError("select 1 /*+ sortby() */");
    // Mismatching parentheses
    ParserError("select 1 /*+ sortby(() */");
    ParserError("select 1 /*+ sortby(a) \n");
    ParserError("select 1 --+ sortby(a) */\n from t");
}
#end_block

#method_before
@Test
public void TestSinglelineComment() {
    ParserError("--");
    ParserError("--select 1");
    ParsesOk("select 1--");
    ParsesOk("select 1 --foo");
    ParsesOk("select 1 --\ncol_name");
    ParsesOk("--foo's \nselect 1 --bar");
    ParsesOk("--foo\nselect 1 --bar");
    ParsesOk("--foo\r\nselect 1 --bar");
    ParsesOk("--/* foo */\n select 1");
    ParsesOk("select 1 --/**/");
    ParsesOk("-- foo /*\nselect 1");
    ParserError("-- baz /*\nselect 1*/");
    ParsesOk("select -- blah\n 1");
    ParsesOk("select -- select 1\n 1");
    ParsesOk("select 1 -- sortby(()");
    ParserError("select 1 -- +sortby(()\n");
}
#method_after
@Test
public void TestSinglelineComment() {
    ParserError("--");
    ParserError("--select 1");
    ParsesOk("select 1--");
    ParsesOk("select 1 --foo");
    ParsesOk("select 1 --\ncol_name");
    ParsesOk("--foo's \nselect 1 --bar");
    ParsesOk("--foo\nselect 1 --bar");
    ParsesOk("--foo\r\nselect 1 --bar");
    ParsesOk("--/* foo */\n select 1");
    ParsesOk("select 1 --/**/");
    ParsesOk("-- foo /*\nselect 1");
    ParserError("-- baz /*\nselect 1*/");
    ParsesOk("select -- blah\n 1");
    ParsesOk("select -- select 1\n 1");
    ParsesOk("select 1 -- sortby(()");
    // Mismatching parentheses
    ParserError("select 1 -- +sortby(()\n");
}
#end_block

#method_before
private void TestJoinHints(String stmt, String... expectedHints) {
    SelectStmt selectStmt = (SelectStmt) ParsesOk(stmt);
    Preconditions.checkState(selectStmt.getTableRefs().size() > 1);
    List<PlanHint> actualHints = Lists.newArrayList();
    assertEquals(null, selectStmt.getTableRefs().get(0).getJoinHints());
    for (int i = 1; i < selectStmt.getTableRefs().size(); ++i) {
        List<PlanHint> hints = selectStmt.getTableRefs().get(i).getJoinHints();
        if (hints != null)
            actualHints.addAll(hints);
    }
    if (actualHints.isEmpty())
        actualHints = Lists.newArrayList((PlanHint) null);
    assertEquals(PlanHint.stringsToHints(Lists.newArrayList(expectedHints)), actualHints);
}
#method_after
private void TestJoinHints(String stmt, String... expectedHints) {
    SelectStmt selectStmt = (SelectStmt) ParsesOk(stmt);
    Preconditions.checkState(selectStmt.getTableRefs().size() > 1);
    List<String> actualHints = Lists.newArrayList();
    assertTrue(selectStmt.getTableRefs().get(0).getJoinHints().isEmpty());
    for (int i = 1; i < selectStmt.getTableRefs().size(); ++i) {
        List<PlanHint> hints = selectStmt.getTableRefs().get(i).getJoinHints();
        for (PlanHint hint : hints) actualHints.add(hint.toString());
    }
    if (actualHints.isEmpty())
        actualHints = Lists.newArrayList((String) null);
    assertEquals(Lists.newArrayList(expectedHints), actualHints);
}
#end_block

#method_before
private void TestTableHints(String stmt, String... expectedHints) {
    SelectStmt selectStmt = (SelectStmt) ParsesOk(stmt);
    Preconditions.checkState(selectStmt.getTableRefs().size() > 0);
    List<PlanHint> actualHints = Lists.newArrayList();
    for (int i = 0; i < selectStmt.getTableRefs().size(); ++i) {
        List<PlanHint> hints = selectStmt.getTableRefs().get(i).getTableHints();
        if (hints != null)
            actualHints.addAll(hints);
    }
    if (actualHints.isEmpty())
        actualHints = Lists.newArrayList((PlanHint) null);
    assertEquals(PlanHint.stringsToHints(Lists.newArrayList(expectedHints)), actualHints);
}
#method_after
private void TestTableHints(String stmt, String... expectedHints) {
    SelectStmt selectStmt = (SelectStmt) ParsesOk(stmt);
    Preconditions.checkState(selectStmt.getTableRefs().size() > 0);
    List<String> actualHints = Lists.newArrayList();
    for (int i = 0; i < selectStmt.getTableRefs().size(); ++i) {
        List<PlanHint> hints = selectStmt.getTableRefs().get(i).getTableHints();
        for (PlanHint hint : hints) actualHints.add(hint.toString());
    }
    if (actualHints.isEmpty())
        actualHints = Lists.newArrayList((String) null);
    assertEquals(Lists.newArrayList(expectedHints), actualHints);
}
#end_block

#method_before
private void TestTableAndJoinHints(String stmt, String... expectedHints) {
    SelectStmt selectStmt = (SelectStmt) ParsesOk(stmt);
    Preconditions.checkState(selectStmt.getTableRefs().size() > 0);
    List<PlanHint> actualHints = Lists.newArrayList();
    for (int i = 0; i < selectStmt.getTableRefs().size(); ++i) {
        List<PlanHint> joinHints = selectStmt.getTableRefs().get(i).getJoinHints();
        if (joinHints != null)
            actualHints.addAll(joinHints);
        List<PlanHint> tableHints = selectStmt.getTableRefs().get(i).getTableHints();
        if (tableHints != null)
            actualHints.addAll(tableHints);
    }
    if (actualHints.isEmpty())
        actualHints = Lists.newArrayList((PlanHint) null);
    assertEquals(PlanHint.stringsToHints(Lists.newArrayList(expectedHints)), actualHints);
}
#method_after
private void TestTableAndJoinHints(String stmt, String... expectedHints) {
    SelectStmt selectStmt = (SelectStmt) ParsesOk(stmt);
    Preconditions.checkState(selectStmt.getTableRefs().size() > 0);
    List<String> actualHints = Lists.newArrayList();
    for (int i = 0; i < selectStmt.getTableRefs().size(); ++i) {
        List<PlanHint> joinHints = selectStmt.getTableRefs().get(i).getJoinHints();
        for (PlanHint hint : joinHints) actualHints.add(hint.toString());
        List<PlanHint> tableHints = selectStmt.getTableRefs().get(i).getTableHints();
        for (PlanHint hint : tableHints) actualHints.add(hint.toString());
    }
    if (actualHints.isEmpty())
        actualHints = Lists.newArrayList((String) null);
    assertEquals(Lists.newArrayList(expectedHints), actualHints);
}
#end_block

#method_before
private void TestSelectListHints(String stmt, String... expectedHints) {
    SelectStmt selectStmt = (SelectStmt) ParsesOk(stmt);
    List<PlanHint> actualHints = selectStmt.getSelectList().getPlanHints();
    if (actualHints == null)
        actualHints = Lists.newArrayList((PlanHint) null);
    assertEquals(PlanHint.stringsToHints(Lists.newArrayList(expectedHints)), actualHints);
}
#method_after
private void TestSelectListHints(String stmt, String... expectedHints) {
    SelectStmt selectStmt = (SelectStmt) ParsesOk(stmt);
    List<String> actualHints = Lists.newArrayList();
    List<PlanHint> hints = selectStmt.getSelectList().getPlanHints();
    for (PlanHint hint : hints) actualHints.add(hint.toString());
    if (actualHints.isEmpty())
        actualHints = Lists.newArrayList((String) null);
    assertEquals(Lists.newArrayList(expectedHints), actualHints);
}
#end_block

#method_before
private void TestInsertHints(String stmt, String... expectedHints) {
    InsertStmt insertStmt = (InsertStmt) ParsesOk(stmt);
    List<PlanHint> actualHints = insertStmt.getPlanHints();
    if (actualHints == null)
        actualHints = Lists.newArrayList((PlanHint) null);
    assertEquals(PlanHint.stringsToHints(Lists.newArrayList(expectedHints)), actualHints);
}
#method_after
private void TestInsertHints(String stmt, String... expectedHints) {
    InsertStmt insertStmt = (InsertStmt) ParsesOk(stmt);
    List<String> actualHints = Lists.newArrayList();
    List<PlanHint> hints = insertStmt.getPlanHints();
    for (PlanHint hint : hints) actualHints.add(hint.toString());
    if (actualHints.isEmpty())
        actualHints = Lists.newArrayList((String) null);
    assertEquals(Lists.newArrayList(expectedHints), actualHints);
}
#end_block

#method_before
@Test
public void TestPlanHints() {
    // All plan-hint styles embed a comma-separated list of hints.
    String[][] hintStyles = new String[][] { // traditional commented hint
    new String[] { "/* +", "*/" }, // eol commented hint
    new String[] { "-- +", "\n" }, // eol commented hint
    new String[] { "\n-- +", "\n" }, // legacy style
    new String[] { "[", "]" } };
    String[][] commentStyles = new String[][] { // traditional comment
    new String[] { "/*", "*/" }, // eol comment
    new String[] { "--", "\n" } };
    for (String[] hintStyle : hintStyles) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test join hints.
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b on(a.id = b.id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a cross join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        // Multiple comma-separated hints.
        TestJoinHints(String.format("select * from functional.alltypes a join " + "%sbroadcast,shuffle,foo,bar%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast", "shuffle", "foo", "bar");
        // Test hints in a multi-way join.
        TestJoinHints(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, prefix, suffix, prefix, suffix, prefix, suffix), "broadcast", "shuffle", "broadcast", "shuffle");
        // Test hints in a multi-way join (flipped prefix/suffix -> bad hint start/ends).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, suffix, prefix, prefix, suffix, suffix, prefix));
        // Test hints in a multi-way join (missing prefixes/suffixes).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", suffix, suffix, suffix, suffix, prefix, "", "", ""));
        // Test insert hints.
        TestInsertHints(String.format("insert into t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert overwrite t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t partition(x, y) %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t(a, b) partition(x, y) %sshuffle%s select * from t", prefix, suffix), "shuffle");
        TestInsertHints(String.format("insert overwrite t(a, b) partition(x, y) %sfoo,bar,baz%s select * from t", prefix, suffix), "foo", "bar", "baz");
        // Test upsert hints.
        ParsesOk(String.format("upsert into t %sshuffle%s select * from t", prefix, suffix));
        ParsesOk(String.format("upsert into t (x, y) %sshuffle%s select * from t", prefix, suffix));
        // Test TableRef hints.
        TestTableHints(String.format("select * from functional.alltypes %sschedule_disk_local%s", prefix, suffix), "schedule_disk_local");
        TestTableHints(String.format("select * from functional.alltypes %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s" + ", functional.alltypes b %sschedule_remote%s", prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "schedule_remote");
        // Test both TableRef and join hints.
        TestTableAndJoinHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s join %sbroadcast%s functional.alltypes b " + "%sschedule_remote%s using(id)", prefix, suffix, prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "broadcast", "schedule_remote");
        TestSelectListHints(String.format("select %sfoo,bar,baz%s * from functional.alltypes a", prefix, suffix), "foo", "bar", "baz");
        // Test select-list hints (e.g., straight_join). The legacy-style hint has no
        // prefix and suffix.
        {
            String localPrefix = prefix;
            String localSuffix = suffix;
            if (prefix == "[") {
                localPrefix = "";
                localSuffix = "";
            }
            TestSelectListHints(String.format("select %sstraight_join%s * from functional.alltypes a", localPrefix, localSuffix), "straight_join");
        }
        // Below are tests for hints that are not supported by the legacy syntax.
        if (prefix == "[")
            continue;
        // Test mixing commented hints and comments.
        for (String[] commentStyle : commentStyles) {
            String commentPrefix = commentStyle[0];
            String commentSuffix = commentStyle[1];
            String queryTemplate = "$1comment$2 select $1comment$2 $3straight_join$4 $1comment$2 * " + "from $1comment$2 functional.alltypes a join $1comment$2 $3shuffle$4 " + "$1comment$2 functional.alltypes b $1comment$2 on $1comment$2 " + "(a.id = b.id)";
            String query = queryTemplate.replaceAll("\\$1", commentPrefix).replaceAll("\\$2", commentSuffix).replaceAll("\\$3", prefix).replaceAll("\\$4", suffix);
            TestSelectListHints(query, "straight_join");
            TestJoinHints(query, "shuffle");
        }
        // Tests for hints with arguments.
        TestInsertHints(String.format("insert into t %ssortby(a)%s select * from t", prefix, suffix), "sortby(a)");
        TestInsertHints(String.format("insert into t %sclustered,shuffle,sortby(a)%s select * from t", prefix, suffix), "sortby(a)");
        TestInsertHints(String.format("insert into t %ssortby(a,b)%s select * from t", prefix, suffix), "sortby(a,b)");
        TestInsertHints(String.format("insert into t %ssortby(a  , b)%s select * from t", prefix, suffix), "sortby(a,b)");
        TestInsertHints(String.format("insert into t %sSoRtBy(  a  ,  , ,,, b  )%s select * from t", prefix, suffix), "sortby(a,b)");
    }
    // No "+" at the beginning so the comment is not recognized as a hint.
    TestJoinHints("select * from functional.alltypes a join /* comment */" + "functional.alltypes b using (int_col)", (String) null);
    TestSelectListHints("select /* comment */ * from functional.alltypes", (String) null);
    TestInsertHints("insert into t(a, b) partition(x, y) /* comment */ select 1", (String) null);
    TestSelectListHints("select /* -- +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* abcdef +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- abcdef +straight_join\n * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- /*+straight_join\n * from functional.alltypes", (String) null);
    // Commented hints cannot span lines (recognized as comments instead).
    TestSelectListHints("select /*\n +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_join \n*/ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_\njoin */ * from functional.alltypes", (String) null);
    ParserError("select -- +straight_join * from functional.alltypes");
    ParserError("select \n-- +straight_join * from functional.alltypes");
    // Missing "/*" or "/*"
    ParserError("select * from functional.alltypes a join + */" + "functional.alltypes b using (int_col)");
    ParserError("select * from functional.alltypes a join /* + " + "functional.alltypes b using (int_col)");
    // Test empty hint tokens.
    TestSelectListHints("select /* +straight_join, ,, */ * from functional.alltypes", "straight_join");
    // Traditional commented hints are not parsed inside a comment.
    ParserError("select /* /* +straight_join */ */ * from functional.alltypes");
}
#method_after
@Test
public void TestPlanHints() {
    // All plan-hint styles embed a comma-separated list of hints.
    String[][] hintStyles = new String[][] { // traditional commented hint
    new String[] { "/* +", "*/" }, // eol commented hint
    new String[] { "-- +", "\n" }, // eol commented hint
    new String[] { "\n-- +", "\n" }, // legacy style
    new String[] { "[", "]" } };
    String[][] commentStyles = new String[][] { // traditional comment
    new String[] { "/*", "*/" }, // eol comment
    new String[] { "--", "\n" } };
    for (String[] hintStyle : hintStyles) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test join hints.
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b on(a.id = b.id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a cross join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        // Multiple comma-separated hints.
        TestJoinHints(String.format("select * from functional.alltypes a join " + "%sbroadcast,shuffle,foo,bar%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast", "shuffle", "foo", "bar");
        // Test hints in a multi-way join.
        TestJoinHints(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, prefix, suffix, prefix, suffix, prefix, suffix), "broadcast", "shuffle", "broadcast", "shuffle");
        // Test hints in a multi-way join (flipped prefix/suffix -> bad hint start/ends).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, suffix, prefix, prefix, suffix, suffix, prefix));
        // Test hints in a multi-way join (missing prefixes/suffixes).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", suffix, suffix, suffix, suffix, prefix, "", "", ""));
        // Test insert hints.
        TestInsertHints(String.format("insert into t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert overwrite t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t partition(x, y) %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t(a, b) partition(x, y) %sshuffle%s select * from t", prefix, suffix), "shuffle");
        TestInsertHints(String.format("insert overwrite t(a, b) partition(x, y) %sfoo,bar,baz%s select * from t", prefix, suffix), "foo", "bar", "baz");
        // Test upsert hints.
        ParsesOk(String.format("upsert into t %sshuffle%s select * from t", prefix, suffix));
        ParsesOk(String.format("upsert into t (x, y) %sshuffle%s select * from t", prefix, suffix));
        // Test TableRef hints.
        TestTableHints(String.format("select * from functional.alltypes %sschedule_disk_local%s", prefix, suffix), "schedule_disk_local");
        TestTableHints(String.format("select * from functional.alltypes %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s" + ", functional.alltypes b %sschedule_remote%s", prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "schedule_remote");
        // Test both TableRef and join hints.
        TestTableAndJoinHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s join %sbroadcast%s functional.alltypes b " + "%sschedule_remote%s using(id)", prefix, suffix, prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "broadcast", "schedule_remote");
        TestSelectListHints(String.format("select %sfoo,bar,baz%s * from functional.alltypes a", prefix, suffix), "foo", "bar", "baz");
        // Test select-list hints (e.g., straight_join). The legacy-style hint has no
        // prefix and suffix.
        {
            String localPrefix = prefix;
            String localSuffix = suffix;
            if (prefix == "[") {
                localPrefix = "";
                localSuffix = "";
            }
            TestSelectListHints(String.format("select %sstraight_join%s * from functional.alltypes a", localPrefix, localSuffix), "straight_join");
        }
        // Below are tests for hints that are not supported by the legacy syntax.
        if (prefix == "[")
            continue;
        // Test mixing commented hints and comments.
        for (String[] commentStyle : commentStyles) {
            String commentPrefix = commentStyle[0];
            String commentSuffix = commentStyle[1];
            String queryTemplate = "$1comment$2 select $1comment$2 $3straight_join$4 $1comment$2 * " + "from $1comment$2 functional.alltypes a join $1comment$2 $3shuffle$4 " + "$1comment$2 functional.alltypes b $1comment$2 on $1comment$2 " + "(a.id = b.id)";
            String query = queryTemplate.replaceAll("\\$1", commentPrefix).replaceAll("\\$2", commentSuffix).replaceAll("\\$3", prefix).replaceAll("\\$4", suffix);
            TestSelectListHints(query, "straight_join");
            TestJoinHints(query, "shuffle");
        }
        // Tests for hints with arguments.
        TestInsertHints(String.format("insert into t %ssortby(a)%s select * from t", prefix, suffix), "sortby(a)");
        TestInsertHints(String.format("insert into t %sclustered,shuffle,sortby(a)%s select * from t", prefix, suffix), "clustered", "shuffle", "sortby(a)");
        TestInsertHints(String.format("insert into t %ssortby(a,b)%s select * from t", prefix, suffix), "sortby(a,b)");
        TestInsertHints(String.format("insert into t %ssortby(a  , b)%s select * from t", prefix, suffix), "sortby(a,b)");
        ParserError(String.format("insert into t %ssortby(  a  ,  , ,,, b  )%s select * from t", prefix, suffix));
    }
    // No "+" at the beginning so the comment is not recognized as a hint.
    TestJoinHints("select * from functional.alltypes a join /* comment */" + "functional.alltypes b using (int_col)", (String) null);
    TestSelectListHints("select /* comment */ * from functional.alltypes", (String) null);
    TestInsertHints("insert into t(a, b) partition(x, y) /* comment */ select 1", (String) null);
    TestSelectListHints("select /* -- +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* abcdef +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- abcdef +straight_join\n * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- /*+straight_join\n * from functional.alltypes", (String) null);
    // Commented hints cannot span lines (recognized as comments instead).
    TestSelectListHints("select /*\n +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_join \n*/ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_\njoin */ * from functional.alltypes", (String) null);
    ParserError("select -- +straight_join * from functional.alltypes");
    ParserError("select \n-- +straight_join * from functional.alltypes");
    // Missing "/*" or "/*"
    ParserError("select * from functional.alltypes a join + */" + "functional.alltypes b using (int_col)");
    ParserError("select * from functional.alltypes a join /* + " + "functional.alltypes b using (int_col)");
    // Test empty hint tokens.
    TestSelectListHints("select /* +straight_join, ,, */ * from functional.alltypes", "straight_join");
    // Traditional commented hints are not parsed inside a comment.
    ParserError("select /* /* +straight_join */ */ * from functional.alltypes");
}
#end_block

#method_before
@Test
public void TestIdentQuoting() {
    ParsesOk("select a from `t`");
    ParsesOk("select a from `default`.`t`");
    ParsesOk("select a from `default`.t");
    ParsesOk("select a from default.`t`");
    ParsesOk("select 01a from default.`01_t`");
    ParsesOk("select `a` from default.t");
    ParsesOk("select `tbl`.`a` from default.t");
    ParsesOk("select `db`.`tbl`.`a` from default.t");
    ParsesOk("select `12db`.`tbl`.`12_a` from default.t");
    // Make sure quoted float literals are identifiers.
    ParsesOk("select `8e6`", SlotRef.class);
    ParsesOk("select `4.5e2`", SlotRef.class);
    ParsesOk("select `.7e9`", SlotRef.class);
    // Mixed quoting
    ParsesOk("select `db`.tbl.`a` from default.t");
    ParsesOk("select `db.table.a` from default.t");
    // Identifiers consisting of only whitespace not allowed.
    ParserError("select a from ` `");
    ParserError("select a from `    `");
    // Empty quoted identifier doesn't parse.
    ParserError("select a from ``");
    // Whitespace can be interspersed with other characters.
    // Whitespace is trimmed from the beginning and end of an identifier.
    ParsesOk("select a from `a a a    `");
    ParsesOk("select a from `    a a a`");
    ParsesOk("select a from `    a a a    `");
    // Quoted identifiers can contain any characters except "`".
    ParsesOk("select a from `all types`");
    ParsesOk("select a from `default`.`all types`");
    ParsesOk("select a from `~!@#$%^&*()-_=+|;:'\",<.>/?`");
    // Quoted identifiers do not unescape escape sequences.
    ParsesOk("select a from `ab\rabc`");
    ParsesOk("select a from `ab\tabc`");
    ParsesOk("select a from `ab\fabc`");
    ParsesOk("select a from `ab\babc`");
    ParsesOk("select a from `ab\nabc`");
    // Test non-printable control characters inside quoted identifiers.
    ParsesOk("select a from `abc\u0000abc`");
    ParsesOk("select a from `abc\u0019abc`");
    ParsesOk("select a from `abc\u007fabc`");
    // Quoted identifiers can contain keywords.
    ParsesOk("select `select`, `insert`, `upsert` from `table` where `where` = 10");
    // Quoted identifiers cannot contain "`"
    ParserError("select a from `abcde`abcde`");
    ParserError("select a from `abc\u0060abc`");
    // Wrong quotes
    ParserError("select a from 'default'.'t'");
    // Lots of quoting
    ParsesOk("select `db`.`tbl`.`a` from `default`.`t` `alias` where `alias`.`col` = 'string'" + " group by `alias`.`col`");
}
#method_after
@Test
public void TestIdentQuoting() {
    ParsesOk("select a from `t`");
    ParsesOk("select a from default.`t`");
    ParsesOk("select a from default.t");
    ParsesOk("select a from default.`t`");
    ParsesOk("select 01a from default.`01_t`");
    ParsesOk("select `a` from default.t");
    ParsesOk("select `tbl`.`a` from default.t");
    ParsesOk("select `db`.`tbl`.`a` from default.t");
    ParsesOk("select `12db`.`tbl`.`12_a` from default.t");
    // Make sure quoted float literals are identifiers.
    ParsesOk("select `8e6`", SlotRef.class);
    ParsesOk("select `4.5e2`", SlotRef.class);
    ParsesOk("select `.7e9`", SlotRef.class);
    // Mixed quoting
    ParsesOk("select `db`.tbl.`a` from default.t");
    ParsesOk("select `db.table.a` from default.t");
    // Identifiers consisting of only whitespace not allowed.
    ParserError("select a from ` `");
    ParserError("select a from `    `");
    // Empty quoted identifier doesn't parse.
    ParserError("select a from ``");
    // Whitespace can be interspersed with other characters.
    // Whitespace is trimmed from the beginning and end of an identifier.
    ParsesOk("select a from `a a a    `");
    ParsesOk("select a from `    a a a`");
    ParsesOk("select a from `    a a a    `");
    // Quoted identifiers can contain any characters except "`".
    ParsesOk("select a from `all types`");
    ParsesOk("select a from default.`all types`");
    ParsesOk("select a from `~!@#$%^&*()-_=+|;:'\",<.>/?`");
    // Quoted identifiers do not unescape escape sequences.
    ParsesOk("select a from `ab\rabc`");
    ParsesOk("select a from `ab\tabc`");
    ParsesOk("select a from `ab\fabc`");
    ParsesOk("select a from `ab\babc`");
    ParsesOk("select a from `ab\nabc`");
    // Test non-printable control characters inside quoted identifiers.
    ParsesOk("select a from `abc\u0000abc`");
    ParsesOk("select a from `abc\u0019abc`");
    ParsesOk("select a from `abc\u007fabc`");
    // Quoted identifiers can contain keywords.
    ParsesOk("select `select`, `insert`, `upsert` from `table` where `where` = 10");
    // Quoted identifiers cannot contain "`"
    ParserError("select a from `abcde`abcde`");
    ParserError("select a from `abc\u0060abc`");
    // Wrong quotes
    ParserError("select a from 'default'.'t'");
    // Lots of quoting
    ParsesOk("select `db`.`tbl`.`a` from `default`.`t` `alias` where `alias`.`col` = 'string'" + " group by `alias`.`col`");
}
#end_block

#method_before
@Test
public void TestKuduUpdate() {
    TestUtils.assumeKuduIsSupported();
    ParserError("update (select * from functional_kudu.testtbl) a set name = '10'");
}
#method_after
@Test
public void TestKuduUpdate() {
    // TestUtils.assumeKuduIsSupported();
    ParserError("update (select * from functional_kudu.testtbl) a set name = '10'");
}
#end_block

#method_before
@Test
public void TestShow() {
    // Short form ok
    ParsesOk("SHOW TABLES");
    // Well-formed pattern
    ParsesOk("SHOW TABLES 'tablename|othername'");
    // Empty pattern ok
    ParsesOk("SHOW TABLES ''");
    // Databases
    ParsesOk("SHOW DATABASES");
    ParsesOk("SHOW SCHEMAS");
    ParsesOk("SHOW DATABASES LIKE 'pattern'");
    ParsesOk("SHOW SCHEMAS LIKE 'p*ttern'");
    // Data sources
    ParsesOk("SHOW DATA SOURCES");
    ParsesOk("SHOW DATA SOURCES 'pattern'");
    ParsesOk("SHOW DATA SOURCES LIKE 'pattern'");
    ParsesOk("SHOW DATA SOURCES LIKE 'p*ttern'");
    // Functions
    for (String fnType : new String[] { "", "AGGREGATE", "ANALYTIC" }) {
        ParsesOk(String.format("SHOW %s FUNCTIONS", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS LIKE 'pattern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS LIKE 'p*ttern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS in DB LIKE 'pattern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS in DB", fnType));
    }
    // Show table/column stats.
    ParsesOk("SHOW TABLE STATS tbl");
    ParsesOk("SHOW TABLE STATS db.tbl");
    ParsesOk("SHOW TABLE STATS `db`.`tbl`");
    ParsesOk("SHOW COLUMN STATS tbl");
    ParsesOk("SHOW COLUMN STATS db.tbl");
    ParsesOk("SHOW COLUMN STATS `db`.`tbl`");
    // Show partitions
    ParsesOk("SHOW PARTITIONS tbl");
    ParsesOk("SHOW PARTITIONS db.tbl");
    ParsesOk("SHOW PARTITIONS `db`.`tbl`");
    // Show files of table
    ParsesOk("SHOW FILES IN tbl");
    ParsesOk("SHOW FILES IN db.tbl");
    ParsesOk("SHOW FILES IN `db`.`tbl`");
    ParsesOk("SHOW FILES IN db.tbl PARTITION(x='a',y='b')");
    // Missing arguments
    ParserError("SHOW");
    // Malformed pattern (no quotes)
    ParserError("SHOW TABLES tablename");
    // Invalid SHOW DATA SOURCE statements
    ParserError("SHOW DATA");
    ParserError("SHOW SOURCE");
    ParserError("SHOW DATA SOURCE LIKE NotStrLiteral");
    ParserError("SHOW UNKNOWN FUNCTIONS");
    // Missing table/column qualifier.
    ParserError("SHOW STATS tbl");
    // Missing table.
    ParserError("SHOW TABLE STATS");
    ParserError("SHOW COLUMN STATS");
    // String literal not accepted.
    ParserError("SHOW TABLE STATS 'strlit'");
    // Missing table.
    ParserError("SHOW FILES IN");
    // Invalid partition.
    ParserError("SHOW FILES IN db.tbl PARTITION(p)");
}
#method_after
@Test
public void TestShow() {
    // Short form ok
    ParsesOk("SHOW TABLES");
    // Well-formed pattern
    ParsesOk("SHOW TABLES 'tablename|othername'");
    // Empty pattern ok
    ParsesOk("SHOW TABLES ''");
    // Databases
    ParsesOk("SHOW DATABASES");
    ParsesOk("SHOW SCHEMAS");
    ParsesOk("SHOW DATABASES LIKE 'pattern'");
    ParsesOk("SHOW SCHEMAS LIKE 'p*ttern'");
    // Data sources
    ParsesOk("SHOW DATA SOURCES");
    ParsesOk("SHOW DATA SOURCES 'pattern'");
    ParsesOk("SHOW DATA SOURCES LIKE 'pattern'");
    ParsesOk("SHOW DATA SOURCES LIKE 'p*ttern'");
    // Functions
    for (String fnType : new String[] { "", "AGGREGATE", "ANALYTIC" }) {
        ParsesOk(String.format("SHOW %s FUNCTIONS", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS LIKE 'pattern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS LIKE 'p*ttern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS in DB LIKE 'pattern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS in DB", fnType));
    }
    // Show table/column stats.
    ParsesOk("SHOW TABLE STATS tbl");
    ParsesOk("SHOW TABLE STATS db.tbl");
    ParsesOk("SHOW TABLE STATS `db`.`tbl`");
    ParsesOk("SHOW COLUMN STATS tbl");
    ParsesOk("SHOW COLUMN STATS db.tbl");
    ParsesOk("SHOW COLUMN STATS `db`.`tbl`");
    // Show partitions
    ParsesOk("SHOW PARTITIONS tbl");
    ParsesOk("SHOW PARTITIONS db.tbl");
    ParsesOk("SHOW PARTITIONS `db`.`tbl`");
    // Show range partitions
    ParsesOk("SHOW RANGE PARTITIONS tbl");
    ParsesOk("SHOW RANGE PARTITIONS db.tbl");
    ParsesOk("SHOW RANGE PARTITIONS `db`.`tbl`");
    // Show files of table
    ParsesOk("SHOW FILES IN tbl");
    ParsesOk("SHOW FILES IN db.tbl");
    ParsesOk("SHOW FILES IN `db`.`tbl`");
    ParsesOk("SHOW FILES IN db.tbl PARTITION(x='a',y='b')");
    // Missing arguments
    ParserError("SHOW");
    // Malformed pattern (no quotes)
    ParserError("SHOW TABLES tablename");
    // Invalid SHOW DATA SOURCE statements
    ParserError("SHOW DATA");
    ParserError("SHOW SOURCE");
    ParserError("SHOW DATA SOURCE LIKE NotStrLiteral");
    ParserError("SHOW UNKNOWN FUNCTIONS");
    // Missing table/column qualifier.
    ParserError("SHOW STATS tbl");
    // Missing table.
    ParserError("SHOW TABLE STATS");
    ParserError("SHOW COLUMN STATS");
    // String literal not accepted.
    ParserError("SHOW TABLE STATS 'strlit'");
    // Missing table.
    ParserError("SHOW FILES IN");
    ParsesOk("SHOW FILES IN db.tbl PARTITION(p)");
}
#end_block

#method_before
@Test
public void TestAlterTableAddReplaceColumns() {
    String[] addReplaceKw = { "ADD", "REPLACE" };
    for (String addReplace : addReplaceKw) {
        ParsesOk(String.format("ALTER TABLE Foo %s COLUMNS (i int, s string)", addReplace));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (i int, s string)", addReplace));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (i int)", addReplace));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (i int comment 'hi')", addReplace));
        // Negative syntax tests
        ParserError(String.format("ALTER TABLE TestDb.Foo %s COLUMNS i int", addReplace));
        ParserError(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (int i)", addReplace));
        ParserError(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (i int COMMENT)", addReplace));
        ParserError(String.format("ALTER TestDb.Foo %s COLUMNS (i int)", addReplace));
        ParserError(String.format("ALTER TestDb.Foo %s COLUMNS", addReplace));
        ParserError(String.format("ALTER TestDb.Foo %s COLUMNS ()", addReplace));
        ParserError(String.format("ALTER Foo %s COLUMNS (i int, s string)", addReplace));
        ParserError(String.format("ALTER TABLE %s COLUMNS (i int, s string)", addReplace));
        // Don't yet support ALTER TABLE ADD COLUMN syntax
        ParserError(String.format("ALTER TABLE Foo %s COLUMN i int", addReplace));
        ParserError(String.format("ALTER TABLE Foo %s COLUMN (i int)", addReplace));
    }
}
#method_after
@Test
public void TestAlterTableAddReplaceColumns() {
    String[] addReplaceKw = { "ADD", "REPLACE" };
    for (String addReplace : addReplaceKw) {
        ParsesOk(String.format("ALTER TABLE Foo %s COLUMNS (i int, s string)", addReplace));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (i int, s string)", addReplace));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (i int)", addReplace));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (i int comment 'hi')", addReplace));
        // Kudu column options
        ParsesOk(String.format("ALTER TABLE Foo %s COLUMNS (i int PRIMARY KEY NOT NULL " + "ENCODING RLE COMPRESSION SNAPPY BLOCK_SIZE 1024 DEFAULT 10, " + "j string NULL ENCODING PLAIN_ENCODING COMPRESSION LZ4 BLOCK_SIZE 10 " + "DEFAULT 'test')", addReplace));
        // Negative syntax tests
        ParserError(String.format("ALTER TABLE TestDb.Foo %s COLUMNS i int", addReplace));
        ParserError(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (int i)", addReplace));
        ParserError(String.format("ALTER TABLE TestDb.Foo %s COLUMNS (i int COMMENT)", addReplace));
        ParserError(String.format("ALTER TestDb.Foo %s COLUMNS (i int)", addReplace));
        ParserError(String.format("ALTER TestDb.Foo %s COLUMNS", addReplace));
        ParserError(String.format("ALTER TestDb.Foo %s COLUMNS ()", addReplace));
        ParserError(String.format("ALTER Foo %s COLUMNS (i int, s string)", addReplace));
        ParserError(String.format("ALTER TABLE %s COLUMNS (i int, s string)", addReplace));
        // Don't yet support ALTER TABLE ADD COLUMN syntax
        ParserError(String.format("ALTER TABLE Foo %s COLUMN i int", addReplace));
        ParserError(String.format("ALTER TABLE Foo %s COLUMN (i int)", addReplace));
    }
}
#end_block

#method_before
@Test
public void TestAlterTableAddPartition() {
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=1)");
    ParsesOk("ALTER TABLE TestDb.Foo ADD IF NOT EXISTS PARTITION (i=1, s='Hello')");
    ParsesOk("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, s='Hello') LOCATION '/a/b'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=NULL)");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=NULL, j=2, k=NULL)");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=abc, j=(5*8+10), k=!true and false)");
    // Cannot use dynamic partition syntax
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION (partcol)");
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, partcol)");
    // Location needs to be a string literal
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, s='Hello') LOCATION a/b");
    // Caching ops
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED 'pool'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' WITH replication = 3");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' " + "with replication = -1");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) UNCACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' UNCACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' CACHED IN 'pool'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' CACHED IN 'pool' " + "with replication = 3");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' LOCATION 'a/b'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) UNCACHED LOCATION 'a/b'");
    ParserError("ALTER TABLE Foo ADD IF EXISTS PARTITION (i=1, s='Hello')");
    ParserError("ALTER TABLE TestDb.Foo ADD (i=1, s='Hello')");
    ParserError("ALTER TABLE TestDb.Foo ADD (i=1)");
    ParserError("ALTER TABLE Foo (i=1)");
    ParserError("ALTER TABLE TestDb.Foo PARTITION (i=1)");
    ParserError("ALTER TABLE Foo ADD PARTITION");
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION ()");
    ParserError("ALTER Foo ADD PARTITION (i=1)");
    ParserError("ALTER TABLE ADD PARTITION (i=1)");
    ParserError("ALTER TABLE ADD");
    ParserError("ALTER TABLE DROP");
}
#method_after
@Test
public void TestAlterTableAddPartition() {
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=1)");
    ParsesOk("ALTER TABLE TestDb.Foo ADD IF NOT EXISTS PARTITION (i=1, s='Hello')");
    ParsesOk("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, s='Hello') LOCATION '/a/b'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=NULL)");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=NULL, j=2, k=NULL)");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=abc, j=(5*8+10), k=!true and false)");
    // Location needs to be a string literal
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, s='Hello') LOCATION a/b");
    // Caching ops
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED 'pool'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' WITH replication = 3");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' " + "with replication = -1");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) UNCACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' UNCACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' CACHED IN 'pool'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' CACHED IN 'pool' " + "with replication = 3");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' LOCATION 'a/b'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) UNCACHED LOCATION 'a/b'");
    ParserError("ALTER TABLE Foo ADD IF EXISTS PARTITION (i=1, s='Hello')");
    ParserError("ALTER TABLE TestDb.Foo ADD (i=1, s='Hello')");
    ParserError("ALTER TABLE TestDb.Foo ADD (i=1)");
    ParserError("ALTER TABLE Foo (i=1)");
    ParserError("ALTER TABLE TestDb.Foo PARTITION (i=1)");
    ParserError("ALTER TABLE Foo ADD PARTITION");
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION ()");
    ParserError("ALTER Foo ADD PARTITION (i=1)");
    ParserError("ALTER TABLE ADD PARTITION (i=1)");
    ParserError("ALTER TABLE ADD");
    ParserError("ALTER TABLE DROP");
    // Kudu range partitions
    String[] ifNotExistsOption = { "IF NOT EXISTS", "" };
    for (String option : ifNotExistsOption) {
        ParsesOk(String.format("ALTER TABLE Foo ADD %s RANGE PARTITION 10 < VALUES < 20", option));
        ParsesOk(String.format("ALTER TABLE Foo ADD %s RANGE PARTITION VALUE = 100", option));
        ParserError(String.format("ALTER TABLE Foo ADD %s RANGE PARTITION 10 < VALUES " + "<= 20, PARTITION 20 < VALUES <= 30", option));
        ParserError(String.format("ALTER TABLE Foo ADD %s (RANGE PARTITION 10 < VALUES " + "<= 20)", option));
    }
}
#end_block

#method_before
@Test
public void TestAlterTableDropPartition() {
    // PURGE is optional
    String[] purgeKw = { "PURGE", "" };
    for (String kw : purgeKw) {
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=1) %s", kw));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo DROP IF EXISTS " + "PARTITION (i=1, s='Hello') %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=NULL) %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=NULL, " + "j=2, k=NULL) %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=abc, " + "j=(5*8+10), k=!true and false) %s", kw));
        // Cannot use dynamic partition syntax
        ParserError(String.format("ALTER TABLE Foo DROP PARTITION (partcol) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP PARTITION (i=1, j) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP IF NOT EXISTS " + "PARTITION (i=1, s='Hello') %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP (i=1, s='Hello') %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE Foo (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo PARTITION (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP PARTITION %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP PARTITION () %s", kw));
        ParserError(String.format("ALTER Foo DROP PARTITION (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE DROP PARTITION (i=1) %s", kw));
    }
}
#method_after
@Test
public void TestAlterTableDropPartition() {
    // PURGE is optional
    String[] purgeKw = { "PURGE", "" };
    for (String kw : purgeKw) {
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=1) %s", kw));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo DROP IF EXISTS " + "PARTITION (i=1, s='Hello') %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=NULL) %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=NULL, " + "j=2, k=NULL) %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=abc, " + "j=(5*8+10), k=!true and false) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP IF NOT EXISTS " + "PARTITION (i=1, s='Hello') %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP (i=1, s='Hello') %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE Foo (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo PARTITION (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP PARTITION %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP PARTITION () %s", kw));
        ParserError(String.format("ALTER Foo DROP PARTITION (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE DROP PARTITION (i=1) %s", kw));
    }
    // Kudu range partitions
    String[] ifExistsOption = { "IF EXISTS", "" };
    for (String option : ifExistsOption) {
        ParsesOk(String.format("ALTER TABLE Foo DROP %s RANGE PARTITION 10 < VALUES < 20", option));
        ParsesOk(String.format("ALTER TABLE Foo DROP %s RANGE PARTITION VALUE = 100", option));
        ParserError(String.format("ALTER TABLE Foo DROP %s RANGE PARTITION 10 < VALUES " + "<= 20, PARTITION 20 < VALUES <= 30", option));
        ParserError(String.format("ALTER TABLE Foo DROP %s (RANGE PARTITION 10 < VALUES " + "<= 20)", option));
        ParserError(String.format("ALTER TABLE Foo DROP %s RANGE PARTITION VALUE = 100 " + "PURGE", option));
    }
}
#end_block

#method_before
@Test
public void TestAlterTableChangeColumn() {
    // KW_COLUMN is optional
    String[] columnKw = { "COLUMN", "" };
    for (String kw : columnKw) {
        ParsesOk(String.format("ALTER TABLE Foo.Bar CHANGE %s c1 c2 int", kw));
        ParsesOk(String.format("ALTER TABLE Foo CHANGE %s c1 c2 int comment 'hi'", kw));
        // Negative syntax tests
        ParserError(String.format("ALTER TABLE Foo CHANGE %s c1 int c2", kw));
        ParserError(String.format("ALTER TABLE Foo CHANGE %s col1 int", kw));
        ParserError(String.format("ALTER TABLE Foo CHANGE %s col1", kw));
        ParserError(String.format("ALTER TABLE Foo CHANGE %s", kw));
        ParserError(String.format("ALTER TABLE CHANGE %s c1 c2 int", kw));
    }
}
#method_after
@Test
public void TestAlterTableChangeColumn() {
    // KW_COLUMN is optional
    String[] columnKw = { "COLUMN", "" };
    for (String kw : columnKw) {
        ParsesOk(String.format("ALTER TABLE Foo.Bar CHANGE %s c1 c2 int", kw));
        ParsesOk(String.format("ALTER TABLE Foo CHANGE %s c1 c2 int comment 'hi'", kw));
        // Kudu column options
        ParsesOk(String.format("ALTER TABLE Foo CHANGE %s c1 c2 int comment 'hi' " + "NULL ENCODING PLAIN_ENCODING COMPRESSION LZ4 DEFAULT 10 BLOCK_SIZE 1024", kw));
        // Negative syntax tests
        ParserError(String.format("ALTER TABLE Foo CHANGE %s c1 int c2", kw));
        ParserError(String.format("ALTER TABLE Foo CHANGE %s col1 int", kw));
        ParserError(String.format("ALTER TABLE Foo CHANGE %s col1", kw));
        ParserError(String.format("ALTER TABLE Foo CHANGE %s", kw));
        ParserError(String.format("ALTER TABLE CHANGE %s c1 c2 int", kw));
    }
}
#end_block

#method_before
@Test
public void TestAlterTableSet() {
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("ALTER TABLE Foo SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE TestDb.Foo SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE TestDb.Foo PARTITION (a=1) SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE Foo PARTITION (s='str') SET FILEFORMAT " + format);
        ParserError("ALTER TABLE TestDb.Foo PARTITION (i=5) SET " + format);
        ParserError("ALTER TABLE TestDb.Foo SET " + format);
        ParserError("ALTER TABLE TestDb.Foo " + format);
    }
    ParserError("ALTER TABLE TestDb.Foo SET FILEFORMAT");
    ParsesOk("ALTER TABLE Foo SET LOCATION '/a/b/c'");
    ParsesOk("ALTER TABLE TestDb.Foo SET LOCATION '/a/b/c'");
    ParsesOk("ALTER TABLE Foo PARTITION (i=1,s='str') SET LOCATION '/a/i=1/s=str'");
    ParsesOk("ALTER TABLE Foo PARTITION (s='str') SET LOCATION '/a/i=1/s=str'");
    ParserError("ALTER TABLE Foo PARTITION (s) SET LOCATION '/a'");
    ParserError("ALTER TABLE Foo PARTITION () SET LOCATION '/a'");
    ParserError("ALTER TABLE Foo PARTITION ('str') SET FILEFORMAT TEXTFILE");
    ParserError("ALTER TABLE Foo PARTITION (a=1, 5) SET FILEFORMAT TEXTFILE");
    ParserError("ALTER TABLE Foo PARTITION () SET FILEFORMAT PARQUETFILE");
    ParserError("ALTER TABLE Foo PARTITION (,) SET FILEFORMAT PARQUET");
    ParserError("ALTER TABLE Foo PARTITION (a=1) SET FILEFORMAT");
    ParserError("ALTER TABLE Foo PARTITION (a=1) SET LOCATION");
    ParserError("ALTER TABLE TestDb.Foo SET LOCATION abc");
    ParserError("ALTER TABLE TestDb.Foo SET LOCATION");
    ParserError("ALTER TABLE TestDb.Foo SET");
    String[] tblPropTypes = { "TBLPROPERTIES", "SERDEPROPERTIES" };
    String[] partClauses = { "", "PARTITION(k1=10, k2=20)" };
    for (String propType : tblPropTypes) {
        for (String part : partClauses) {
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('a'='b')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('abc'='123')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('abc'='123', 'a'='1')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('a'='1', 'b'='2', 'c'='3')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ( )", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a', 'b')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a'='b',)", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a'=b)", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s (a='b')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s (a=b)", part, propType));
        }
    }
    // Test SET COLUMN STATS.
    ParsesOk("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'='10')");
    ParsesOk("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'='10','maxSize'='20')");
    ParsesOk("ALTER TABLE TestDb.Foo SET COLUMN STATS col ('avgSize'='20')");
    ParserError("ALTER TABLE SET COLUMN STATS col ('numDVs'='10'");
    ParserError("ALTER TABLE Foo SET COLUMN STATS ('numDVs'='10'");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col ()");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col (numDVs='10')");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'=10)");
    ParserError("ALTER TABLE Foo PARTITION (p=1) SET COLUMN STATS col ('avgSize'='20')");
    for (String cacheClause : Lists.newArrayList("UNCACHED", "CACHED in 'pool'", "CACHED in 'pool' with replication = 4")) {
        ParsesOk("ALTER TABLE Foo SET " + cacheClause);
        ParsesOk("ALTER TABLE Foo PARTITION(j=0) SET " + cacheClause);
        ParserError("ALTER TABLE Foo PARTITION(j=0) " + cacheClause);
    }
}
#method_after
@Test
public void TestAlterTableSet() {
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("ALTER TABLE Foo SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE TestDb.Foo SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE TestDb.Foo PARTITION (a=1) SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE Foo PARTITION (s='str') SET FILEFORMAT " + format);
        ParserError("ALTER TABLE TestDb.Foo PARTITION (i=5) SET " + format);
        ParserError("ALTER TABLE TestDb.Foo SET " + format);
        ParserError("ALTER TABLE TestDb.Foo " + format);
    }
    ParserError("ALTER TABLE TestDb.Foo SET FILEFORMAT");
    ParsesOk("ALTER TABLE Foo SET LOCATION '/a/b/c'");
    ParsesOk("ALTER TABLE TestDb.Foo SET LOCATION '/a/b/c'");
    ParsesOk("ALTER TABLE Foo PARTITION (i=1,s='str') SET LOCATION '/a/i=1/s=str'");
    ParsesOk("ALTER TABLE Foo PARTITION (s='str') SET LOCATION '/a/i=1/s=str'");
    ParserError("ALTER TABLE Foo PARTITION () SET LOCATION '/a'");
    ParserError("ALTER TABLE Foo PARTITION () SET FILEFORMAT PARQUETFILE");
    ParserError("ALTER TABLE Foo PARTITION (,) SET FILEFORMAT PARQUET");
    ParserError("ALTER TABLE Foo PARTITION (a=1) SET FILEFORMAT");
    ParserError("ALTER TABLE Foo PARTITION (a=1) SET LOCATION");
    ParserError("ALTER TABLE TestDb.Foo SET LOCATION abc");
    ParserError("ALTER TABLE TestDb.Foo SET LOCATION");
    ParserError("ALTER TABLE TestDb.Foo SET");
    String[] tblPropTypes = { "TBLPROPERTIES", "SERDEPROPERTIES" };
    String[] partClauses = { "", "PARTITION(k1=10, k2=20)" };
    for (String propType : tblPropTypes) {
        for (String part : partClauses) {
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('a'='b')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('abc'='123')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('abc'='123', 'a'='1')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('a'='1', 'b'='2', 'c'='3')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ( )", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a', 'b')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a'='b',)", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a'=b)", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s (a='b')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s (a=b)", part, propType));
        }
    }
    // Test SET COLUMN STATS.
    ParsesOk("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'='10')");
    ParsesOk("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'='10','maxSize'='20')");
    ParsesOk("ALTER TABLE TestDb.Foo SET COLUMN STATS col ('avgSize'='20')");
    ParserError("ALTER TABLE SET COLUMN STATS col ('numDVs'='10'");
    ParserError("ALTER TABLE Foo SET COLUMN STATS ('numDVs'='10'");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col ()");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col (numDVs='10')");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'=10)");
    ParserError("ALTER TABLE Foo PARTITION (p=1) SET COLUMN STATS col ('avgSize'='20')");
    for (String cacheClause : Lists.newArrayList("UNCACHED", "CACHED in 'pool'", "CACHED in 'pool' with replication = 4")) {
        ParsesOk("ALTER TABLE Foo SET " + cacheClause);
        ParsesOk("ALTER TABLE Foo PARTITION(j=0) SET " + cacheClause);
        ParserError("ALTER TABLE Foo PARTITION(j=0) " + cacheClause);
    }
}
#end_block

#method_before
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo (i int,)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, " + "HASH(a) INTO 2 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int, k int) DISTRIBUTE BY HASH INTO 4 BUCKETS," + " HASH(k) INTO 4 BUCKETS");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo DISTRIBUTE BY HASH INTO 4 BUCKETS");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) DISTRIBUTE BY " + "HASH (a) INTO 3 BUCKETS, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 1 + 1 < VALUES) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY HASH (a) INTO 4 BUCKETS, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) INTO 3 BUCKETS");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUES = 10) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 10 < VALUE < 20) STORED AS KUDU");
}
#method_after
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo (i int,)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY HASH(i) PARTITIONS 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY HASH(i) PARTITIONS 4, " + "HASH(a) PARTITIONS 2");
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY HASH PARTITIONS 4");
    ParsesOk("CREATE TABLE Foo (i int, k int) PARTITION BY HASH PARTITIONS 4," + " HASH(k) PARTITIONS 4");
    ParserError("CREATE TABLE Foo (i int) PARTITION BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo PARTITION BY HASH PARTITIONS 4");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) PARTITION BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) PARTITION BY " + "HASH (a) PARTITIONS 3, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION 1 + 1 < VALUES) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int, b int) PARTITION BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY HASH (a) PARTITIONS 4, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) PARTITIONS 3");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION VALUES = 10) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION 10 < VALUE < 20) STORED AS KUDU");
    // Column options for Kudu tables
    String[] encodings = { "encoding auto_encoding", "encoding plain_encoding", "encoding prefix_encoding", "encoding group_varint", "encoding rle", "encoding dict_encoding", "encoding bit_shuffle", "encoding unknown", "" };
    String[] compression = { "compression default_compression", "compression no_compression", "compression snappy", "compression lz4", "compression zlib", "compression unknown", "" };
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (String enc : encodings) {
        for (String comp : compression) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", nul, enc, comp, def, block));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", block, nul, enc, comp, def));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", def, block, nul, enc, comp));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", comp, def, block, nul, enc));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, def, block, nul));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, block, def, nul));
                    }
                }
            }
        }
    }
    // Column option is specified multiple times for the same column
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY ENCODING RLE ENCODING PLAIN) " + "STORED AS KUDU");
    // Constant expr used in DEFAULT
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b int DEFAULT 1+1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b float DEFAULT cast(1.1 as float)) " + "STORED AS KUDU");
    // Non-literal value used in BLOCK_SIZE
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY, b int BLOCK_SIZE 1+1) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY BLOCK_SIZE -1) STORED AS KUDU");
}
#end_block

#method_before
@Test
public void TestCreateTableAsSelect() {
    ParsesOk("CREATE TABLE Foo AS SELECT 1, 2, 3");
    ParsesOk("CREATE TABLE Foo AS SELECT * from foo.bar");
    ParsesOk("CREATE TABLE Foo.Bar AS SELECT int_col, bool_col from tbl limit 10");
    ParsesOk("CREATE TABLE Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE Foo STORED AS PARQUET AS SELECT 1");
    ParsesOk("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH INTO 2 BUCKETS " + "AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH (b) INTO 2 " + "BUCKETS AS SELECT * from bar");
    // With clause works
    ParsesOk("CREATE TABLE Foo AS with t1 as (select 1) select * from t1");
    // Incomplete AS SELECT statement
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS SELECT");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS WITH");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS");
    // INSERT/UPSERT statements are not allowed
    ParserError("CREATE TABLE Foo AS INSERT INTO Foo SELECT 1");
    ParserError("CREATE TABLE Foo AS UPSERT INTO Foo SELECT 1");
    // Column and partition definitions not allowed
    ParserError("CREATE TABLE Foo(i int) AS SELECT 1");
    ParserError("CREATE TABLE Foo PARTITIONED BY(i int) AS SELECT 1");
    // Partitioned by syntax following insert into syntax
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) ROW FORMAT DELIMITED STORED AS " + "PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1, 2");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT * from Bar");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a=2, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a, b=2) AS SELECT * from Bar");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (i) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS " + "SELECT 1");
    ParserError("CREATE TABLE Foo DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY HASH(a) INTO 4 BUCKETS " + "TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY RANGE(a) " + "(PARTITION 1 < VALUES < 10, PARTITION 10 <= VALUES < 20, PARTITION VALUE = 30) " + "STORED AS KUDU AS SELECT * FROM Bar");
}
#method_after
@Test
public void TestCreateTableAsSelect() {
    ParsesOk("CREATE TABLE Foo AS SELECT 1, 2, 3");
    ParsesOk("CREATE TABLE Foo AS SELECT * from foo.bar");
    ParsesOk("CREATE TABLE Foo.Bar AS SELECT int_col, bool_col from tbl limit 10");
    ParsesOk("CREATE TABLE Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE Foo STORED AS PARQUET AS SELECT 1");
    ParsesOk("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) PARTITION BY HASH PARTITIONS 2 " + "AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) PARTITION BY HASH (b) PARTITIONS 2 " + "AS SELECT * from bar");
    // With clause works
    ParsesOk("CREATE TABLE Foo AS with t1 as (select 1) select * from t1");
    // Incomplete AS SELECT statement
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS SELECT");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS WITH");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS");
    // INSERT/UPSERT statements are not allowed
    ParserError("CREATE TABLE Foo AS INSERT INTO Foo SELECT 1");
    ParserError("CREATE TABLE Foo AS UPSERT INTO Foo SELECT 1");
    // Column and partition definitions not allowed
    ParserError("CREATE TABLE Foo(i int) AS SELECT 1");
    ParserError("CREATE TABLE Foo PARTITIONED BY(i int) AS SELECT 1");
    // Partitioned by syntax following insert into syntax
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) ROW FORMAT DELIMITED STORED AS " + "PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1, 2");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT * from Bar");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a=2, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a, b=2) AS SELECT * from Bar");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (i) PARTITION BY HASH(i) PARTITIONS 4 AS " + "SELECT 1");
    ParserError("CREATE TABLE Foo PARTITION BY HASH(i) PARTITIONS 4 AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) PARTITION BY HASH(a) PARTITIONS 4 " + "TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) PARTITION BY RANGE(a) " + "(PARTITION 1 < VALUES < 10, PARTITION 10 <= VALUES < 20, PARTITION VALUE = 30) " + "STORED AS KUDU AS SELECT * FROM Bar");
}
#end_block

#method_before
@Test
public void TestGetErrorMsg() {
    // missing select
    ParserError("c, b, c from t", "Syntax error in line 1:\n" + "c, b, c from t\n" + "^\n" + "Encountered: IDENTIFIER\n" + "Expected: ALTER, COMPUTE, CREATE, DELETE, DESCRIBE, DROP, EXPLAIN, GRANT, " + "INSERT, INVALIDATE, LOAD, REFRESH, REVOKE, SELECT, SET, SHOW, TRUNCATE, " + "UPDATE, UPSERT, USE, VALUES, WITH\n");
    // missing select list
    ParserError("select from t", "Syntax error in line 1:\n" + "select from t\n" + "       ^\n" + "Encountered: FROM\n" + "Expected: ALL, CASE, CAST, DISTINCT, EXISTS, " + "FALSE, IF, INTERVAL, NOT, NULL, " + "STRAIGHT_JOIN, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing from
    ParserError("select c, b, c where a = 5", "Syntax error in line 1:\n" + "select c, b, c where a = 5\n" + "               ^\n" + "Encountered: WHERE\n" + "Expected: AND, AS, BETWEEN, DIV, FROM, ILIKE, IN, IREGEXP, IS, LIKE, LIMIT, NOT, OR, " + "ORDER, REGEXP, RLIKE, UNION, COMMA, IDENTIFIER\n");
    // missing table list
    ParserError("select c, b, c from where a = 5", "Syntax error in line 1:\n" + "select c, b, c from where a = 5\n" + "                    ^\n" + "Encountered: WHERE\n" + "Expected: IDENTIFIER\n");
    // missing predicate in where clause (no group by)
    ParserError("select c, b, c from t where", "Syntax error in line 1:\n" + "select c, b, c from t where\n" + "                           ^\n" + "Encountered: EOF\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing predicate in where clause (group by)
    ParserError("select c, b, c from t where group by a, b", "Syntax error in line 1:\n" + "select c, b, c from t where group by a, b\n" + "                            ^\n" + "Encountered: GROUP\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // unmatched string literal starting with "
    ParserError("select c, \"b, c from t", "Unmatched string literal in line 1:\n" + "select c, \"b, c from t\n" + "           ^\n");
    // unmatched string literal starting with '
    ParserError("select c, 'b, c from t", "Unmatched string literal in line 1:\n" + "select c, 'b, c from t\n" + "           ^\n");
    // test placement of error indicator ^ on queries with multiple lines
    ParserError("select (i + 5)(1 - i) from t", "Syntax error in line 1:\n" + "select (i + 5)(1 - i) from t\n" + "              ^\n" + "Encountered: (\n" + "Expected:");
    ParserError("select (i + 5)\n(1 - i) from t", "Syntax error in line 2:\n" + "(1 - i) from t\n" + "^\n" + "Encountered: (\n" + "Expected");
    ParserError("select (i + 5)\n(1 - i)\nfrom t", "Syntax error in line 2:\n" + "(1 - i)\n" + "^\n" + "Encountered: (\n" + "Expected");
    // Long line: error in the middle
    ParserError("select c, b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "... b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c,...\n" + "                             ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the start
    ParserError("select a a a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "select a a a, b, c,c,c,c,c,c,c,c,c,c,c,...\n" + "           ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the end
    ParserError("select a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t", "Syntax error in line 1:\n" + "...c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t\n" + "                             ^\n" + "Encountered: COMMA\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // Parsing identifiers that have different names printed as EXPECTED
    ParserError("DROP DATA SRC foo", "Syntax error in line 1:\n" + "DROP DATA SRC foo\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCE\n");
    ParserError("SHOW DATA SRCS", "Syntax error in line 1:\n" + "SHOW DATA SRCS\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCES\n");
    ParserError("USE ` `", "Syntax error in line 1:\n" + "USE ` `\n" + "    ^\n" + "Encountered: EMPTY IDENTIFIER\n" + "Expected: IDENTIFIER\n");
    // Expecting = token
    ParserError("SET foo", "Syntax error in line 1:\n" + "SET foo\n" + "       ^\n" + "Encountered: EOF\n" + "Expected: =\n");
}
#method_after
@Test
public void TestGetErrorMsg() {
    // missing select
    ParserError("c, b, c from t", "Syntax error in line 1:\n" + "c, b, c from t\n" + "^\n" + "Encountered: IDENTIFIER\n" + "Expected: ALTER, COMPUTE, CREATE, DELETE, DESCRIBE, DROP, EXPLAIN, GRANT, " + "INSERT, INVALIDATE, LOAD, REFRESH, REVOKE, SELECT, SET, SHOW, TRUNCATE, " + "UPDATE, UPSERT, USE, VALUES, WITH\n");
    // missing select list
    ParserError("select from t", "Syntax error in line 1:\n" + "select from t\n" + "       ^\n" + "Encountered: FROM\n" + "Expected: ALL, CASE, CAST, DEFAULT, DISTINCT, EXISTS, " + "FALSE, IF, INTERVAL, NOT, NULL, " + "STRAIGHT_JOIN, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing from
    ParserError("select c, b, c where a = 5", "Syntax error in line 1:\n" + "select c, b, c where a = 5\n" + "               ^\n" + "Encountered: WHERE\n" + "Expected: AND, AS, BETWEEN, DEFAULT, DIV, FROM, ILIKE, IN, IREGEXP, IS, LIKE, " + "LIMIT, NOT, OR, ORDER, REGEXP, RLIKE, UNION, COMMA, IDENTIFIER\n");
    // missing table list
    ParserError("select c, b, c from where a = 5", "Syntax error in line 1:\n" + "select c, b, c from where a = 5\n" + "                    ^\n" + "Encountered: WHERE\n" + "Expected: DEFAULT, IDENTIFIER\n");
    // missing predicate in where clause (no group by)
    ParserError("select c, b, c from t where", "Syntax error in line 1:\n" + "select c, b, c from t where\n" + "                           ^\n" + "Encountered: EOF\n" + "Expected: CASE, CAST, DEFAULT, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing predicate in where clause (group by)
    ParserError("select c, b, c from t where group by a, b", "Syntax error in line 1:\n" + "select c, b, c from t where group by a, b\n" + "                            ^\n" + "Encountered: GROUP\n" + "Expected: CASE, CAST, DEFAULT, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // unmatched string literal starting with "
    ParserError("select c, \"b, c from t", "Unmatched string literal in line 1:\n" + "select c, \"b, c from t\n" + "           ^\n");
    // unmatched string literal starting with '
    ParserError("select c, 'b, c from t", "Unmatched string literal in line 1:\n" + "select c, 'b, c from t\n" + "           ^\n");
    // test placement of error indicator ^ on queries with multiple lines
    ParserError("select (i + 5)(1 - i) from t", "Syntax error in line 1:\n" + "select (i + 5)(1 - i) from t\n" + "              ^\n" + "Encountered: (\n" + "Expected:");
    ParserError("select (i + 5)\n(1 - i) from t", "Syntax error in line 2:\n" + "(1 - i) from t\n" + "^\n" + "Encountered: (\n" + "Expected");
    ParserError("select (i + 5)\n(1 - i)\nfrom t", "Syntax error in line 2:\n" + "(1 - i)\n" + "^\n" + "Encountered: (\n" + "Expected");
    // Long line: error in the middle
    ParserError("select c, b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "... b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c,...\n" + "                             ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the start
    ParserError("select a a a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "select a a a, b, c,c,c,c,c,c,c,c,c,c,c,...\n" + "           ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the end
    ParserError("select a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t", "Syntax error in line 1:\n" + "...c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t\n" + "                             ^\n" + "Encountered: COMMA\n" + "Expected: CASE, CAST, DEFAULT, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // Parsing identifiers that have different names printed as EXPECTED
    ParserError("DROP DATA SRC foo", "Syntax error in line 1:\n" + "DROP DATA SRC foo\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCE\n");
    ParserError("SHOW DATA SRCS", "Syntax error in line 1:\n" + "SHOW DATA SRCS\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCES\n");
    ParserError("USE ` `", "Syntax error in line 1:\n" + "USE ` `\n" + "    ^\n" + "Encountered: EMPTY IDENTIFIER\n" + "Expected: DEFAULT, IDENTIFIER\n");
    // Expecting = token
    ParserError("SET foo", "Syntax error in line 1:\n" + "SET foo\n" + "       ^\n" + "Encountered: EOF\n" + "Expected: =\n");
}
#end_block

#method_before
@Test
public void TestComputeStatsIncremental() {
    ParsesOk("COMPUTE INCREMENTAL STATS functional.alltypes");
    ParserError("COMPUTE INCREMENTAL functional.alltypes");
    ParsesOk("COMPUTE INCREMENTAL STATS functional.alltypes PARTITION(month=10, year=2010)");
    // No dynamic partition specs
    ParserError("COMPUTE INCREMENTAL STATS functional.alltypes PARTITION(month, year)");
    ParserError("COMPUTE INCREMENTAL STATS");
    ParsesOk("DROP INCREMENTAL STATS functional.alltypes PARTITION(month=10, year=2010)");
    ParserError("DROP INCREMENTAL STATS functional.alltypes PARTITION(month, year)");
    ParserError("DROP INCREMENTAL STATS functional.alltypes");
}
#method_after
@Test
public void TestComputeStatsIncremental() {
    ParsesOk("COMPUTE INCREMENTAL STATS functional.alltypes");
    ParserError("COMPUTE INCREMENTAL functional.alltypes");
    ParsesOk("COMPUTE INCREMENTAL STATS functional.alltypes PARTITION(month=10, year=2010)");
    ParserError("COMPUTE INCREMENTAL STATS");
    ParsesOk("DROP INCREMENTAL STATS functional.alltypes PARTITION(month=10, year=2010)");
    ParserError("DROP INCREMENTAL STATS functional.alltypes");
}
#end_block

#method_before
public static String getIdentSql(String ident) {
    boolean hiveNeedsQuotes = true;
    HiveLexer hiveLexer = new HiveLexer(new ANTLRStringStream(ident));
    try {
        Token t = hiveLexer.nextToken();
        // Check that the lexer recognizes an identifier and then EOF.
        boolean identFound = t.getType() == HiveLexer.Identifier;
        t = hiveLexer.nextToken();
        // No enclosing quotes are necessary for Hive.
        hiveNeedsQuotes = !(identFound && t.getType() == HiveLexer.EOF);
    } catch (Exception e) {
    // Ignore exception and just quote the identifier to be safe.
    }
    boolean isImpalaKeyword = SqlScanner.isKeyword(ident.toUpperCase());
    // Impala's scanner recognizes the ".123" portion of "db.123_tbl" as a decimal,
    // so while the quoting is not necessary for the given identifier itself, the quotes
    // are needed if this identifier will be preceded by a ".".
    boolean startsWithNumber = false;
    if (!hiveNeedsQuotes && !isImpalaKeyword) {
        try {
            Integer.parseInt(ident.substring(0, 1));
            startsWithNumber = true;
        } catch (NumberFormatException e) {
        // Ignore exception, identifier does not start with number.
        }
    }
    if (hiveNeedsQuotes || isImpalaKeyword || startsWithNumber)
        return "`" + ident + "`";
    return ident;
}
#method_after
public static String getIdentSql(String ident) {
    boolean hiveNeedsQuotes = true;
    HiveLexer hiveLexer = new HiveLexer(new ANTLRStringStream(ident));
    try {
        Token t = hiveLexer.nextToken();
        // Check that the lexer recognizes an identifier and then EOF.
        boolean identFound = t.getType() == HiveLexer.Identifier;
        t = hiveLexer.nextToken();
        // No enclosing quotes are necessary for Hive.
        hiveNeedsQuotes = !(identFound && t.getType() == HiveLexer.EOF);
    } catch (Exception e) {
    // Ignore exception and just quote the identifier to be safe.
    }
    boolean isImpalaKeyword = SqlScanner.isKeyword(ident.toUpperCase());
    // Impala's scanner recognizes the ".123" portion of "db.123_tbl" as a decimal,
    // so while the quoting is not necessary for the given identifier itself, the quotes
    // are needed if this identifier will be preceded by a ".".
    boolean startsWithNumber = false;
    if (!hiveNeedsQuotes && !isImpalaKeyword) {
        startsWithNumber = Character.isDigit(ident.charAt(0));
    }
    if (hiveNeedsQuotes || isImpalaKeyword || startsWithNumber)
        return "`" + ident + "`";
    return ident;
}
#end_block

#method_before
public static String getCreateTableSql(Table table) throws CatalogException {
    Preconditions.checkNotNull(table);
    if (table instanceof View)
        return getCreateViewSql((View) table);
    org.apache.hadoop.hive.metastore.api.Table msTable = table.getMetaStoreTable();
    HashMap<String, String> properties = Maps.newHashMap(msTable.getParameters());
    if (properties.containsKey("transient_lastDdlTime")) {
        properties.remove("transient_lastDdlTime");
    }
    boolean isExternal = msTable.getTableType() != null && msTable.getTableType().equals(TableType.EXTERNAL_TABLE.toString());
    String comment = properties.get("comment");
    for (String hiddenProperty : HIDDEN_TABLE_PROPERTIES) {
        properties.remove(hiddenProperty);
    }
    ArrayList<String> colsSql = Lists.newArrayList();
    ArrayList<String> partitionColsSql = Lists.newArrayList();
    boolean isHbaseTable = table instanceof HBaseTable;
    for (int i = 0; i < table.getColumns().size(); i++) {
        if (!isHbaseTable && i < table.getNumClusteringCols()) {
            partitionColsSql.add(columnToSql(table.getColumns().get(i)));
        } else {
            colsSql.add(columnToSql(table.getColumns().get(i)));
        }
    }
    RowFormat rowFormat = RowFormat.fromStorageDescriptor(msTable.getSd());
    HdfsFileFormat format = HdfsFileFormat.fromHdfsInputFormatClass(msTable.getSd().getInputFormat());
    HdfsCompression compression = HdfsCompression.fromHdfsInputFormatClass(msTable.getSd().getInputFormat());
    String location = isHbaseTable ? null : msTable.getSd().getLocation();
    Map<String, String> serdeParameters = msTable.getSd().getSerdeInfo().getParameters();
    String storageHandlerClassName = table.getStorageHandlerClassName();
    List<String> primaryKeySql = Lists.newArrayList();
    String kuduDistributeByParams = null;
    if (table instanceof KuduTable) {
        KuduTable kuduTable = (KuduTable) table;
        // Kudu tables don't use LOCATION syntax
        location = null;
        format = HdfsFileFormat.KUDU;
        // Kudu tables cannot use the Hive DDL syntax for the storage handler
        storageHandlerClassName = null;
        properties.remove(KuduTable.KEY_STORAGE_HANDLER);
        String kuduTableName = properties.get(KuduTable.KEY_TABLE_NAME);
        Preconditions.checkNotNull(kuduTableName);
        if (kuduTableName.equals(KuduUtil.getDefaultCreateKuduTableName(table.getDb().getName(), table.getName()))) {
            properties.remove(KuduTable.KEY_TABLE_NAME);
        }
        // Internal property, should not be exposed to the user.
        properties.remove(StatsSetupConst.DO_NOT_UPDATE_STATS);
        if (!isExternal) {
            primaryKeySql.addAll(kuduTable.getPrimaryKeyColumnNames());
            List<String> paramsSql = Lists.newArrayList();
            for (DistributeParam param : kuduTable.getDistributeBy()) {
                paramsSql.add(param.toSql());
            }
            kuduDistributeByParams = Joiner.on(", ").join(paramsSql);
        }
    }
    HdfsUri tableLocation = location == null ? null : new HdfsUri(location);
    return getCreateTableSql(table.getDb().getName(), table.getName(), comment, colsSql, partitionColsSql, primaryKeySql, kuduDistributeByParams, properties, serdeParameters, isExternal, false, rowFormat, format, compression, storageHandlerClassName, tableLocation);
}
#method_after
public static String getCreateTableSql(Table table) throws CatalogException {
    Preconditions.checkNotNull(table);
    if (table instanceof View)
        return getCreateViewSql((View) table);
    org.apache.hadoop.hive.metastore.api.Table msTable = table.getMetaStoreTable();
    HashMap<String, String> properties = Maps.newHashMap(msTable.getParameters());
    if (properties.containsKey("transient_lastDdlTime")) {
        properties.remove("transient_lastDdlTime");
    }
    boolean isExternal = msTable.getTableType() != null && msTable.getTableType().equals(TableType.EXTERNAL_TABLE.toString());
    String comment = properties.get("comment");
    for (String hiddenProperty : HIDDEN_TABLE_PROPERTIES) {
        properties.remove(hiddenProperty);
    }
    ArrayList<String> colsSql = Lists.newArrayList();
    ArrayList<String> partitionColsSql = Lists.newArrayList();
    boolean isHbaseTable = table instanceof HBaseTable;
    for (int i = 0; i < table.getColumns().size(); i++) {
        if (!isHbaseTable && i < table.getNumClusteringCols()) {
            partitionColsSql.add(columnToSql(table.getColumns().get(i)));
        } else {
            colsSql.add(columnToSql(table.getColumns().get(i)));
        }
    }
    RowFormat rowFormat = RowFormat.fromStorageDescriptor(msTable.getSd());
    HdfsFileFormat format = HdfsFileFormat.fromHdfsInputFormatClass(msTable.getSd().getInputFormat());
    HdfsCompression compression = HdfsCompression.fromHdfsInputFormatClass(msTable.getSd().getInputFormat());
    String location = isHbaseTable ? null : msTable.getSd().getLocation();
    Map<String, String> serdeParameters = msTable.getSd().getSerdeInfo().getParameters();
    String storageHandlerClassName = table.getStorageHandlerClassName();
    List<String> primaryKeySql = Lists.newArrayList();
    String kuduPartitionByParams = null;
    if (table instanceof KuduTable) {
        KuduTable kuduTable = (KuduTable) table;
        // Kudu tables don't use LOCATION syntax
        location = null;
        format = HdfsFileFormat.KUDU;
        // Kudu tables cannot use the Hive DDL syntax for the storage handler
        storageHandlerClassName = null;
        properties.remove(KuduTable.KEY_STORAGE_HANDLER);
        String kuduTableName = properties.get(KuduTable.KEY_TABLE_NAME);
        Preconditions.checkNotNull(kuduTableName);
        if (kuduTableName.equals(KuduUtil.getDefaultCreateKuduTableName(table.getDb().getName(), table.getName()))) {
            properties.remove(KuduTable.KEY_TABLE_NAME);
        }
        // Internal property, should not be exposed to the user.
        properties.remove(StatsSetupConst.DO_NOT_UPDATE_STATS);
        if (!isExternal) {
            primaryKeySql.addAll(kuduTable.getPrimaryKeyColumnNames());
            List<String> paramsSql = Lists.newArrayList();
            for (KuduPartitionParam param : kuduTable.getPartitionBy()) {
                paramsSql.add(param.toSql());
            }
            kuduPartitionByParams = Joiner.on(", ").join(paramsSql);
        } else {
            // We shouldn't output the columns for external tables
            colsSql = null;
        }
    }
    HdfsUri tableLocation = location == null ? null : new HdfsUri(location);
    return getCreateTableSql(table.getDb().getName(), table.getName(), comment, colsSql, partitionColsSql, primaryKeySql, kuduPartitionByParams, properties, serdeParameters, isExternal, false, rowFormat, format, compression, storageHandlerClassName, tableLocation);
}
#end_block

#method_before
public static String getCreateTableSql(String dbName, String tableName, String tableComment, List<String> columnsSql, List<String> partitionColumnsSql, List<String> primaryKeysSql, String kuduDistributeByParams, Map<String, String> tblProperties, Map<String, String> serdeParameters, boolean isExternal, boolean ifNotExists, RowFormat rowFormat, HdfsFileFormat fileFormat, HdfsCompression compression, String storageHandlerClass, HdfsUri location) {
    Preconditions.checkNotNull(tableName);
    StringBuilder sb = new StringBuilder("CREATE ");
    if (isExternal)
        sb.append("EXTERNAL ");
    sb.append("TABLE ");
    if (ifNotExists)
        sb.append("IF NOT EXISTS ");
    if (dbName != null)
        sb.append(dbName + ".");
    sb.append(tableName);
    if (columnsSql != null) {
        sb.append(" (\n  ");
        sb.append(Joiner.on(",\n  ").join(columnsSql));
        if (!primaryKeysSql.isEmpty()) {
            sb.append(",\n  PRIMARY KEY (");
            Joiner.on(", ").appendTo(sb, primaryKeysSql).append(")");
        }
        sb.append("\n)");
    }
    sb.append("\n");
    if (tableComment != null)
        sb.append(" COMMENT '" + tableComment + "'\n");
    if (partitionColumnsSql != null && partitionColumnsSql.size() > 0) {
        sb.append(String.format("PARTITIONED BY (\n  %s\n)\n", Joiner.on(", \n  ").join(partitionColumnsSql)));
    }
    if (kuduDistributeByParams != null) {
        sb.append("DISTRIBUTE BY " + kuduDistributeByParams + "\n");
    }
    if (rowFormat != null && !rowFormat.isDefault()) {
        sb.append("ROW FORMAT DELIMITED");
        if (rowFormat.getFieldDelimiter() != null) {
            String fieldDelim = StringEscapeUtils.escapeJava(rowFormat.getFieldDelimiter());
            sb.append(" FIELDS TERMINATED BY '" + fieldDelim + "'");
        }
        if (rowFormat.getEscapeChar() != null) {
            String escapeChar = StringEscapeUtils.escapeJava(rowFormat.getEscapeChar());
            sb.append(" ESCAPED BY '" + escapeChar + "'");
        }
        if (rowFormat.getLineDelimiter() != null) {
            String lineDelim = StringEscapeUtils.escapeJava(rowFormat.getLineDelimiter());
            sb.append(" LINES TERMINATED BY '" + lineDelim + "'");
        }
        sb.append("\n");
    }
    if (storageHandlerClass == null) {
        // supported by Impala.
        if (compression != HdfsCompression.LZO && compression != HdfsCompression.LZO_INDEX && serdeParameters != null && !serdeParameters.isEmpty()) {
            sb.append("WITH SERDEPROPERTIES " + propertyMapToSql(serdeParameters) + "\n");
        }
        if (fileFormat != null) {
            sb.append("STORED AS " + fileFormat.toSql(compression) + "\n");
        }
    } else {
        // If the storageHandlerClass is set, then we will generate the proper Hive DDL
        // because we do not yet support creating HBase tables via Impala.
        sb.append("STORED BY '" + storageHandlerClass + "'\n");
        if (serdeParameters != null && !serdeParameters.isEmpty()) {
            sb.append("WITH SERDEPROPERTIES " + propertyMapToSql(serdeParameters) + "\n");
        }
    }
    if (location != null) {
        sb.append("LOCATION '" + location.toString() + "'\n");
    }
    if (tblProperties != null && !tblProperties.isEmpty()) {
        sb.append("TBLPROPERTIES " + propertyMapToSql(tblProperties));
    }
    return sb.toString();
}
#method_after
public static String getCreateTableSql(String dbName, String tableName, String tableComment, List<String> columnsSql, List<String> partitionColumnsSql, List<String> primaryKeysSql, String kuduPartitionByParams, Map<String, String> tblProperties, Map<String, String> serdeParameters, boolean isExternal, boolean ifNotExists, RowFormat rowFormat, HdfsFileFormat fileFormat, HdfsCompression compression, String storageHandlerClass, HdfsUri location) {
    Preconditions.checkNotNull(tableName);
    StringBuilder sb = new StringBuilder("CREATE ");
    if (isExternal)
        sb.append("EXTERNAL ");
    sb.append("TABLE ");
    if (ifNotExists)
        sb.append("IF NOT EXISTS ");
    if (dbName != null)
        sb.append(dbName + ".");
    sb.append(tableName);
    if (columnsSql != null) {
        sb.append(" (\n  ");
        sb.append(Joiner.on(",\n  ").join(columnsSql));
        if (!primaryKeysSql.isEmpty()) {
            sb.append(",\n  PRIMARY KEY (");
            Joiner.on(", ").appendTo(sb, primaryKeysSql).append(")");
        }
        sb.append("\n)");
    }
    sb.append("\n");
    if (tableComment != null)
        sb.append(" COMMENT '" + tableComment + "'\n");
    if (partitionColumnsSql != null && partitionColumnsSql.size() > 0) {
        sb.append(String.format("PARTITIONED BY (\n  %s\n)\n", Joiner.on(", \n  ").join(partitionColumnsSql)));
    }
    if (kuduPartitionByParams != null) {
        sb.append("PARTITION BY " + kuduPartitionByParams + "\n");
    }
    if (rowFormat != null && !rowFormat.isDefault()) {
        sb.append("ROW FORMAT DELIMITED");
        if (rowFormat.getFieldDelimiter() != null) {
            String fieldDelim = StringEscapeUtils.escapeJava(rowFormat.getFieldDelimiter());
            sb.append(" FIELDS TERMINATED BY '" + fieldDelim + "'");
        }
        if (rowFormat.getEscapeChar() != null) {
            String escapeChar = StringEscapeUtils.escapeJava(rowFormat.getEscapeChar());
            sb.append(" ESCAPED BY '" + escapeChar + "'");
        }
        if (rowFormat.getLineDelimiter() != null) {
            String lineDelim = StringEscapeUtils.escapeJava(rowFormat.getLineDelimiter());
            sb.append(" LINES TERMINATED BY '" + lineDelim + "'");
        }
        sb.append("\n");
    }
    if (storageHandlerClass == null) {
        // supported by Impala.
        if (compression != HdfsCompression.LZO && compression != HdfsCompression.LZO_INDEX && serdeParameters != null && !serdeParameters.isEmpty()) {
            sb.append("WITH SERDEPROPERTIES " + propertyMapToSql(serdeParameters) + "\n");
        }
        if (fileFormat != null) {
            sb.append("STORED AS " + fileFormat.toSql(compression) + "\n");
        }
    } else {
        // If the storageHandlerClass is set, then we will generate the proper Hive DDL
        // because we do not yet support creating HBase tables via Impala.
        sb.append("STORED BY '" + storageHandlerClass + "'\n");
        if (serdeParameters != null && !serdeParameters.isEmpty()) {
            sb.append("WITH SERDEPROPERTIES " + propertyMapToSql(serdeParameters) + "\n");
        }
    }
    if (location != null) {
        sb.append("LOCATION '" + location.toString() + "'\n");
    }
    if (tblProperties != null && !tblProperties.isEmpty()) {
        sb.append("TBLPROPERTIES " + propertyMapToSql(tblProperties));
    }
    return sb.toString();
}
#end_block

#method_before
private static String columnToSql(Column col) {
    StringBuilder sb = new StringBuilder(col.getName());
    if (col.getType() != null)
        sb.append(" " + col.getType().toSql());
    if (!Strings.isNullOrEmpty(col.getComment())) {
        sb.append(String.format(" COMMENT '%s'", col.getComment()));
    }
    return sb.toString();
}
#method_after
private static String columnToSql(Column col) {
    StringBuilder sb = new StringBuilder(col.getName());
    if (col.getType() != null)
        sb.append(" " + col.getType().toSql());
    if (col instanceof KuduColumn) {
        KuduColumn kuduCol = (KuduColumn) col;
        Boolean isNullable = kuduCol.isNullable();
        if (isNullable != null)
            sb.append(isNullable ? " NULL" : " NOT NULL");
        if (kuduCol.getEncoding() != null)
            sb.append(" ENCODING " + kuduCol.getEncoding());
        if (kuduCol.getCompression() != null) {
            sb.append(" COMPRESSION " + kuduCol.getCompression());
        }
        if (kuduCol.getDefaultValue() != null) {
            sb.append(" DEFAULT " + kuduCol.getDefaultValue().toSql());
        }
        if (kuduCol.getBlockSize() != 0) {
            sb.append(String.format(" BLOCK_SIZE %d", kuduCol.getBlockSize()));
        }
    }
    if (!Strings.isNullOrEmpty(col.getComment())) {
        sb.append(String.format(" COMMENT '%s'", col.getComment()));
    }
    return sb.toString();
}
#end_block

#method_before
public static String getPlanHintsSql(List<PlanHint> hints) {
    if (hints == null || hints.isEmpty())
        return "";
    StringBuilder sb = new StringBuilder();
    sb.append("\n-- +");
    sb.append(Joiner.on(",").join(hints));
    sb.append("\n");
    return sb.toString();
}
#method_after
public static String getPlanHintsSql(List<PlanHint> hints) {
    Preconditions.checkNotNull(hints);
    if (hints.isEmpty())
        return "";
    StringBuilder sb = new StringBuilder();
    sb.append("\n-- +");
    sb.append(Joiner.on(",").join(hints));
    sb.append("\n");
    return sb.toString();
}
#end_block

#method_before
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createPreInsertSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (RuntimeEnv.INSTANCE.computeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#method_after
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryCtx().disable_codegen_hint = true;
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createPreInsertSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    if (LOG.isTraceEnabled()) {
        LOG.trace("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
        LOG.trace("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
        LOG.trace("finalize plan fragments");
    }
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (BackendConfig.INSTANCE.getComputeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Lineage is disabled for UPDATE AND DELETE statements
        if (ctx_.isUpdateOrDelete())
            return fragments;
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
            List<Expr> exprs = Lists.newArrayList();
            Table targetTable = insertStmt.getTargetTable();
            Preconditions.checkNotNull(targetTable);
            if (targetTable instanceof KuduTable) {
                if (ctx_.isInsert()) {
                    // For insert statements on Kudu tables, we only need to consider
                    // the labels of columns mentioned in the column list.
                    List<String> mentionedColumns = insertStmt.getMentionedColumns();
                    Preconditions.checkState(!mentionedColumns.isEmpty());
                    List<String> targetColLabels = Lists.newArrayList();
                    String tblFullName = targetTable.getFullName();
                    for (String column : mentionedColumns) {
                        targetColLabels.add(tblFullName + "." + column);
                    }
                    graph.addTargetColumnLabels(targetColLabels);
                } else {
                    graph.addTargetColumnLabels(targetTable);
                }
                exprs.addAll(resultExprs);
            } else if (targetTable instanceof HBaseTable) {
                graph.addTargetColumnLabels(targetTable);
                exprs.addAll(resultExprs);
            } else {
                graph.addTargetColumnLabels(targetTable);
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        if (LOG.isTraceEnabled())
            LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#end_block

#method_before
public void computeResourceReqs(List<PlanFragment> fragments, boolean excludeUnpartitionedFragments, TQueryExecRequest request) {
    Preconditions.checkState(!fragments.isEmpty());
    Preconditions.checkNotNull(request);
    // Compute pipelined plan node sets.
    ArrayList<PipelinedPlanNodeSet> planNodeSets = PipelinedPlanNodeSet.computePlanNodeSets(fragments.get(0).getPlanRoot());
    // Compute the max of the per-host mem and vcores requirement.
    // Note that the max mem and vcores may come from different plan node sets.
    long maxPerHostMem = Long.MIN_VALUE;
    int maxPerHostVcores = Integer.MIN_VALUE;
    for (PipelinedPlanNodeSet planNodeSet : planNodeSets) {
        if (!planNodeSet.computeResourceEstimates(excludeUnpartitionedFragments, ctx_.getQueryOptions())) {
            continue;
        }
        long perHostMem = planNodeSet.getPerHostMem();
        int perHostVcores = planNodeSet.getPerHostVcores();
        if (perHostMem > maxPerHostMem)
            maxPerHostMem = perHostMem;
        if (perHostVcores > maxPerHostVcores)
            maxPerHostVcores = perHostVcores;
    }
    // Do not ask for more cores than are in the RuntimeEnv.
    maxPerHostVcores = Math.min(maxPerHostVcores, RuntimeEnv.INSTANCE.getNumCores());
    // Special case for some trivial coordinator-only queries (IMPALA-3053, IMPALA-1092).
    if (isTrivialCoordOnlyPlan(fragments)) {
        maxPerHostMem = 1024;
        maxPerHostVcores = 1;
    }
    // TODO: handle this case with a better indication for unknown, e.g. -1 or not set.
    if (maxPerHostMem == Long.MIN_VALUE || maxPerHostVcores == Integer.MIN_VALUE) {
        boolean allUnpartitioned = true;
        for (PlanFragment fragment : fragments) {
            if (fragment.isPartitioned()) {
                allUnpartitioned = false;
                break;
            }
        }
        if (allUnpartitioned && excludeUnpartitionedFragments) {
            maxPerHostMem = 0;
            maxPerHostVcores = 0;
        }
    }
    if (maxPerHostMem < 0 || maxPerHostMem == Long.MIN_VALUE) {
        LOG.warn("Invalid per-host memory requirement: " + maxPerHostMem);
    }
    if (maxPerHostVcores < 0 || maxPerHostVcores == Integer.MIN_VALUE) {
        LOG.warn("Invalid per-host virtual cores requirement: " + maxPerHostVcores);
    }
    request.setPer_host_mem_req(maxPerHostMem);
    request.setPer_host_vcores((short) maxPerHostVcores);
    LOG.debug("Estimated per-host peak memory requirement: " + maxPerHostMem);
    LOG.debug("Estimated per-host virtual cores requirement: " + maxPerHostVcores);
}
#method_after
public void computeResourceReqs(List<PlanFragment> fragments, boolean excludeUnpartitionedFragments, TQueryExecRequest request) {
    Preconditions.checkState(!fragments.isEmpty());
    Preconditions.checkNotNull(request);
    // Compute pipelined plan node sets.
    ArrayList<PipelinedPlanNodeSet> planNodeSets = PipelinedPlanNodeSet.computePlanNodeSets(fragments.get(0).getPlanRoot());
    // Compute the max of the per-host mem and vcores requirement.
    // Note that the max mem and vcores may come from different plan node sets.
    long maxPerHostMem = Long.MIN_VALUE;
    int maxPerHostVcores = Integer.MIN_VALUE;
    for (PipelinedPlanNodeSet planNodeSet : planNodeSets) {
        if (!planNodeSet.computeResourceEstimates(excludeUnpartitionedFragments, ctx_.getQueryOptions())) {
            continue;
        }
        long perHostMem = planNodeSet.getPerHostMem();
        int perHostVcores = planNodeSet.getPerHostVcores();
        if (perHostMem > maxPerHostMem)
            maxPerHostMem = perHostMem;
        if (perHostVcores > maxPerHostVcores)
            maxPerHostVcores = perHostVcores;
    }
    // Do not ask for more cores than are in the RuntimeEnv.
    maxPerHostVcores = Math.min(maxPerHostVcores, RuntimeEnv.INSTANCE.getNumCores());
    // Special case for some trivial coordinator-only queries (IMPALA-3053, IMPALA-1092).
    if (isTrivialCoordOnlyPlan(fragments)) {
        maxPerHostMem = 1024;
        maxPerHostVcores = 1;
    }
    // TODO: handle this case with a better indication for unknown, e.g. -1 or not set.
    if (maxPerHostMem == Long.MIN_VALUE || maxPerHostVcores == Integer.MIN_VALUE) {
        boolean allUnpartitioned = true;
        for (PlanFragment fragment : fragments) {
            if (fragment.isPartitioned()) {
                allUnpartitioned = false;
                break;
            }
        }
        if (allUnpartitioned && excludeUnpartitionedFragments) {
            maxPerHostMem = 0;
            maxPerHostVcores = 0;
        }
    }
    if (maxPerHostMem < 0 || maxPerHostMem == Long.MIN_VALUE) {
        LOG.warn("Invalid per-host memory requirement: " + maxPerHostMem);
    }
    if (maxPerHostVcores < 0 || maxPerHostVcores == Integer.MIN_VALUE) {
        LOG.warn("Invalid per-host virtual cores requirement: " + maxPerHostVcores);
    }
    request.setPer_host_mem_req(maxPerHostMem);
    request.setPer_host_vcores((short) maxPerHostVcores);
    if (LOG.isTraceEnabled()) {
        LOG.trace("Estimated per-host peak memory requirement: " + maxPerHostMem);
        LOG.trace("Estimated per-host virtual cores requirement: " + maxPerHostVcores);
    }
}
#end_block

#method_before
public void createPreInsertSort(InsertStmt insertStmt, PlanFragment inputFragment, Analyzer analyzer) throws ImpalaException {
    if (!insertStmt.hasClusteredHint())
        return;
    List<Expr> orderingExprs;
    if (insertStmt.getTargetTable() instanceof KuduTable) {
        orderingExprs = Lists.newArrayList(insertStmt.getPrimaryKeyExprs());
    } else {
        orderingExprs = Lists.newArrayList(insertStmt.getPartitionKeyExprs());
    }
    orderingExprs.addAll(insertStmt.getSortByExprs());
    // Ignore constants for the sake of clustering.
    Expr.removeConstants(orderingExprs);
    if (orderingExprs.isEmpty())
        return;
    // Build sortinfo to sort by the ordering exprs.
    List<Boolean> isAscOrder = Collections.nCopies(orderingExprs.size(), false);
    List<Boolean> nullsFirstParams = Collections.nCopies(orderingExprs.size(), false);
    SortInfo sortInfo = new SortInfo(orderingExprs, isAscOrder, nullsFirstParams);
    ExprSubstitutionMap smap = sortInfo.createSortTupleInfo(insertStmt.getResultExprs(), analyzer);
    sortInfo.getSortTupleDescriptor().materializeSlots();
    insertStmt.substituteResultExprs(smap, analyzer);
    SortNode sortNode = new SortNode(ctx_.getNextNodeId(), inputFragment.getPlanRoot(), sortInfo, false, 0);
    sortNode.init(analyzer);
    inputFragment.setPlanRoot(sortNode);
}
#method_after
public void createPreInsertSort(InsertStmt insertStmt, PlanFragment inputFragment, Analyzer analyzer) throws ImpalaException {
    List<Expr> orderingExprs = Lists.newArrayList();
    if (insertStmt.hasClusteredHint()) {
        if (insertStmt.getTargetTable() instanceof KuduTable) {
            orderingExprs.addAll(insertStmt.getPrimaryKeyExprs());
        } else {
            orderingExprs.addAll(insertStmt.getPartitionKeyExprs());
        }
    }
    orderingExprs.addAll(insertStmt.getSortByExprs());
    // Ignore constants for the sake of clustering.
    Expr.removeConstants(orderingExprs);
    if (orderingExprs.isEmpty())
        return;
    // Build sortinfo to sort by the ordering exprs.
    List<Boolean> isAscOrder = Collections.nCopies(orderingExprs.size(), false);
    List<Boolean> nullsFirstParams = Collections.nCopies(orderingExprs.size(), false);
    SortInfo sortInfo = new SortInfo(orderingExprs, isAscOrder, nullsFirstParams);
    ExprSubstitutionMap smap = sortInfo.createSortTupleInfo(insertStmt.getResultExprs(), analyzer);
    sortInfo.getSortTupleDescriptor().materializeSlots();
    insertStmt.substituteResultExprs(smap, analyzer);
    SortNode sortNode = new SortNode(ctx_.getNextNodeId(), inputFragment.getPlanRoot(), sortInfo, false, 0);
    sortNode.init(analyzer);
    inputFragment.setPlanRoot(sortNode);
}
#end_block

#method_before
@Test
public void TestJoinHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        AnalyzesOk(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix));
        AnalyzesOk(String.format("select * from functional.alltypes a join %sshuffle%s " + "functional.alltypes b using (int_col)", prefix, suffix));
        AnalyzesOk(String.format("select * from functional.alltypes a cross join %sbroadcast%s " + "functional.alltypes b", prefix, suffix));
        // Only warn on unrecognized hints for view-compatibility with Hive.
        AnalyzesOk(String.format("select * from functional.alltypes a join %sbadhint%s " + "functional.alltypes b using (int_col)", prefix, suffix), "JOIN hint not recognized: badhint");
        // of space-separated identifiers.
        if (!prefix.contains("[")) {
            AnalyzesOk(String.format("select * from functional.alltypes a join %sbroadcast broadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "JOIN hint not recognized: broadcast broadcast");
        }
        AnalysisError(String.format("select * from functional.alltypes a cross join %sshuffle%s " + "functional.alltypes b", prefix, suffix), "CROSS JOIN does not support SHUFFLE.");
        AnalysisError(String.format("select * from functional.alltypes a right outer join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "RIGHT OUTER JOIN does not support BROADCAST.");
        AnalysisError(String.format("select * from functional.alltypes a full outer join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "FULL OUTER JOIN does not support BROADCAST.");
        AnalysisError(String.format("select * from functional.alltypes a right semi join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "RIGHT SEMI JOIN does not support BROADCAST.");
        AnalysisError(String.format("select * from functional.alltypes a right anti join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "RIGHT ANTI JOIN does not support BROADCAST.");
        // Conflicting join hints.
        AnalysisError(String.format("select * from functional.alltypes a join %sbroadcast,shuffle%s " + "functional.alltypes b using (int_col)", prefix, suffix), "Conflicting JOIN hint: shuffle");
    }
}
#method_after
@Test
public void TestJoinHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        AnalyzesOk(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix));
        AnalyzesOk(String.format("select * from functional.alltypes a join %sshuffle%s " + "functional.alltypes b using (int_col)", prefix, suffix));
        AnalyzesOk(String.format("select * from functional.alltypes a cross join %sbroadcast%s " + "functional.alltypes b", prefix, suffix));
        // Only warn on unrecognized hints for view-compatibility with Hive.
        AnalyzesOk(String.format("select * from functional.alltypes a join %sbadhint%s " + "functional.alltypes b using (int_col)", prefix, suffix), "JOIN hint not recognized: badhint");
        AnalysisError(String.format("select * from functional.alltypes a cross join %sshuffle%s " + "functional.alltypes b", prefix, suffix), "CROSS JOIN does not support SHUFFLE.");
        AnalysisError(String.format("select * from functional.alltypes a right outer join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "RIGHT OUTER JOIN does not support BROADCAST.");
        AnalysisError(String.format("select * from functional.alltypes a full outer join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "FULL OUTER JOIN does not support BROADCAST.");
        AnalysisError(String.format("select * from functional.alltypes a right semi join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "RIGHT SEMI JOIN does not support BROADCAST.");
        AnalysisError(String.format("select * from functional.alltypes a right anti join %sbroadcast%s " + "functional.alltypes b using (int_col)", prefix, suffix), "RIGHT ANTI JOIN does not support BROADCAST.");
        // Conflicting join hints.
        AnalysisError(String.format("select * from functional.alltypes a join %sbroadcast,shuffle%s " + "functional.alltypes b using (int_col)", prefix, suffix), "Conflicting JOIN hint: shuffle");
    }
}
#end_block

#method_before
@Test
public void TestTableHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        for (String alias : new String[] { "", "a" }) {
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_cache_local%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_disk_local%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_remote%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_remote," + "schedule_random_replica%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %s" + "schedule_random_replica,schedule_remote%s", alias, prefix, suffix));
            String name = alias.isEmpty() ? "functional.alltypes" : alias;
            AnalyzesOk(String.format("select * from functional.alltypes %s %sFOO%s", alias, prefix, suffix), String.format("Table hint not recognized for table %s: " + "foo", name));
            // Table hints not supported for HBase tables
            AnalyzesOk(String.format("select * from functional_hbase.alltypes %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints only supported for Hdfs tables");
            // Table hints not supported for catalog views
            AnalyzesOk(String.format("select * from functional.alltypes_view %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints not supported for inline view and collections");
            // Table hints not supported for with clauses
            AnalyzesOk(String.format("with t as (select 1) select * from t %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints not supported for inline view and collections");
        }
        // Table hints not supported for inline views
        AnalyzesOk(String.format("select * from (select tinyint_col * 2 as c1 from " + "functional.alltypes) as v1 %sschedule_random_replica%s", prefix, suffix), "Table hints not supported for inline view and collections");
        // Table hints not supported for collection tables
        AnalyzesOk(String.format("select item from functional.allcomplextypes, " + "allcomplextypes.int_array_col %sschedule_random_replica%s", prefix, suffix), "Table hints not supported for inline view and collections");
    }
}
#method_after
@Test
public void TestTableHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        for (String alias : new String[] { "", "a" }) {
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_cache_local%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_disk_local%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_remote%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_remote," + "schedule_random_replica%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %s" + "schedule_random_replica,schedule_remote%s", alias, prefix, suffix));
            String name = alias.isEmpty() ? "functional.alltypes" : alias;
            AnalyzesOk(String.format("select * from functional.alltypes %s %sFOO%s", alias, prefix, suffix), String.format("Table hint not recognized for table %s: " + "FOO", name));
            // Table hints not supported for HBase tables
            AnalyzesOk(String.format("select * from functional_hbase.alltypes %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints only supported for Hdfs tables");
            // Table hints not supported for catalog views
            AnalyzesOk(String.format("select * from functional.alltypes_view %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints not supported for inline view and collections");
            // Table hints not supported for with clauses
            AnalyzesOk(String.format("with t as (select 1) select * from t %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints not supported for inline view and collections");
        }
        // Table hints not supported for inline views
        AnalyzesOk(String.format("select * from (select tinyint_col * 2 as c1 from " + "functional.alltypes) as v1 %sschedule_random_replica%s", prefix, suffix), "Table hints not supported for inline view and collections");
        // Table hints not supported for collection tables
        AnalyzesOk(String.format("select item from functional.allcomplextypes, " + "allcomplextypes.int_array_col %sschedule_random_replica%s", prefix, suffix), "Table hints not supported for inline view and collections");
    }
}
#end_block

#method_before
@Test
public void TestInsertHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test plan hints for partitioned Hdfs tables.
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle%s select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into table functional.alltypessmall " + "partition (year, month) %snoshuffle%s select * from functional.alltypes", prefix, suffix));
        // Only warn on unrecognized hints.
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sbadhint%s select * from functional.alltypes", prefix, suffix), "INSERT hint not recognized: badhint");
        // Insert hints are ok for unpartitioned tables.
        AnalyzesOk(String.format("insert into table functional.alltypesnopart %sshuffle%s " + "select * from functional.alltypesnopart", prefix, suffix));
        // Plan hints do not make sense for inserting into HBase tables.
        AnalysisError(String.format("insert into table functional_hbase.alltypes %sshuffle%s " + "select * from functional_hbase.alltypes", prefix, suffix), "INSERT hints are only supported for inserting into Hdfs and Kudu tables.");
        // Conflicting plan hints.
        AnalysisError("insert into table functional.alltypessmall " + "partition (year, month) /* +shuffle,noshuffle */ " + "select * from functional.alltypes", "Conflicting INSERT hints: shuffle and noshuffle");
        // Below are tests for hints that are not supported by the legacy syntax.
        if (prefix == "[")
            continue;
        // Tests for sortby hint
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(int_col)%s select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle,clustered,sortby(int_col)%s select * from " + "functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(int_col, bool_col)%s select * from " + "functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle,clustered,sortby(int_col,bool_col)%s " + "select * from functional.alltypes", prefix, suffix));
        // Column list in sortby hint must not be empty
        AnalysisError(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby()%s select * from functional.alltypes", prefix, suffix), "No columns specified for SORTBY hint.");
        // Non-existant column must not occur in sortby hint
        AnalysisError(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(foo)%s select * from functional.alltypes", prefix, suffix), "Could not find SORTBY hint column foo in table.");
        // Hdfs partition column must not occur in sortby hint
        AnalysisError(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(year)%s select * from " + "functional.alltypes", prefix, suffix), "SORTBY hint column list must not contain Hdfs partition columns.");
        // Kudu primary key column must not occur in sortby hint
        AnalysisError(String.format("insert into functional_kudu.alltypessmall " + "%ssortby(id)%s select * from functional_kudu.alltypes", prefix, suffix), "SORTBY hint column list must not contain Kudu primary key columns.");
        // sortby is not supported in UPSERT queries
        AnalysisError(String.format("upsert into functional_kudu.alltypessmall " + "%ssortby()%s select * from functional_kudu.alltypes", prefix, suffix), "SORTBY hint is not supported for in UPSERT ");
        AnalysisError(String.format("upsert into functional_kudu.alltypessmall " + "%ssortby(id)%s select * from functional_kudu.alltypes", prefix, suffix), "SORTBY hint is not supported for in UPSERT ");
    }
    // Multiple non-conflicting hints and case insensitivity of hints.
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) /* +shuffle, ShUfFlE */ " + "select * from functional.alltypes");
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) [shuffle, ShUfFlE] " + "select * from functional.alltypes");
}
#method_after
@Test
public void TestInsertHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test plan hints for partitioned Hdfs tables.
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle%s select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into table functional.alltypessmall " + "partition (year, month) %snoshuffle%s select * from functional.alltypes", prefix, suffix));
        // Only warn on unrecognized hints.
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sbadhint%s select * from functional.alltypes", prefix, suffix), "INSERT hint not recognized: badhint");
        // Insert hints are ok for unpartitioned tables.
        AnalyzesOk(String.format("insert into table functional.alltypesnopart %sshuffle%s " + "select * from functional.alltypesnopart", prefix, suffix));
        // Plan hints do not make sense for inserting into HBase tables.
        AnalysisError(String.format("insert into table functional_hbase.alltypes %sshuffle%s " + "select * from functional_hbase.alltypes", prefix, suffix), "INSERT hints are only supported for inserting into Hdfs and Kudu tables: " + "functional_hbase.alltypes");
        // Conflicting plan hints.
        AnalysisError("insert into table functional.alltypessmall " + "partition (year, month) /* +shuffle,noshuffle */ " + "select * from functional.alltypes", "Conflicting INSERT hints: shuffle and noshuffle");
        // Test clustered hint.
        AnalyzesOk(String.format("insert into functional.alltypessmall partition (year, month) %sclustered%s " + "select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into table functional.alltypesnopart %sclustered%s " + "select * from functional.alltypesnopart", prefix, suffix));
        AnalyzesOk(String.format("insert into table functional.alltypesnopart %snoclustered%s " + "select * from functional.alltypesnopart", prefix, suffix));
        // Conflicting clustered hints.
        AnalysisError(String.format("insert into table functional.alltypessmall partition (year, month) " + "/* +clustered,noclustered */ select * from functional.alltypes", prefix, suffix), "Conflicting INSERT hints: clustered and noclustered");
        // Below are tests for hints that are not supported by the legacy syntax.
        if (prefix == "[")
            continue;
        // Tests for sortby hint
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(int_col)%s select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle,clustered,sortby(int_col)%s select * from " + "functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(int_col, bool_col)%s select * from " + "functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle,clustered,sortby(int_col,bool_col)%s " + "select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle,sortby(int_col,bool_col),clustered%s " + "select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(int_col,bool_col),shuffle,clustered%s " + "select * from functional.alltypes", prefix, suffix));
        // Column in sortby hint must exist.
        AnalysisError(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(foo)%s select * from functional.alltypes", prefix, suffix), "Could not find SORTBY hint column 'foo' in table.");
        // Column in sortby hint must not be a Hdfs partition column.
        AnalysisError(String.format("insert into functional.alltypessmall " + "partition (year, month) %ssortby(year)%s select * from " + "functional.alltypes", prefix, suffix), "SORTBY hint column list must not contain Hdfs partition column: 'year'");
        // Column in sortby hint must not be a Kudu primary key column.
        AnalysisError(String.format("insert into functional_kudu.alltypessmall " + "%ssortby(id)%s select * from functional_kudu.alltypes", prefix, suffix), "SORTBY hint column list must not contain Kudu primary key column: 'id'");
        // sortby() hint is not supported in UPSERT queries
        AnalysisError(String.format("upsert into functional_kudu.alltypessmall " + "%ssortby(id)%s select * from functional_kudu.alltypes", prefix, suffix), "SORTBY hint is not supported in UPSERT statements.");
    }
    // Multiple non-conflicting hints and case insensitivity of hints.
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) /* +shuffle, ShUfFlE */ " + "select * from functional.alltypes");
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) [shuffle, ShUfFlE] " + "select * from functional.alltypes");
}
#end_block

#method_before
@Test
public void TestClone() {
    testNumberOfMembers(QueryStmt.class, 9);
    testNumberOfMembers(UnionStmt.class, 8);
    testNumberOfMembers(ValuesStmt.class, 0);
    // Also check TableRefs.
    testNumberOfMembers(TableRef.class, 18);
    testNumberOfMembers(BaseTableRef.class, 0);
    testNumberOfMembers(InlineViewRef.class, 8);
}
#method_after
@Test
public void TestClone() {
    testNumberOfMembers(QueryStmt.class, 9);
    testNumberOfMembers(UnionStmt.class, 8);
    testNumberOfMembers(ValuesStmt.class, 0);
    // Also check TableRefs.
    testNumberOfMembers(TableRef.class, 19);
    testNumberOfMembers(BaseTableRef.class, 0);
    testNumberOfMembers(InlineViewRef.class, 8);
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    hasClusteredHint_ = false;
    sortByExprs_.clear();
    resultExprs_.clear();
    mentionedUpsertColumns_.clear();
    primaryKeyExprs_.clear();
}
#method_after
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    hasClusteredHint_ = false;
    sortByExprs_.clear();
    resultExprs_.clear();
    mentionedColumns_.clear();
    primaryKeyExprs_.clear();
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Subqueries need to be rewritten by the StmtRewriter first.
            if (analyzer.containsSubquery())
                return;
            // Use getResultExprs() and not getBaseTblResultExprs() here because the final
            // substitution with TupleIsNullPredicate() wrapping happens in planning.
            selectListExprs = Expr.cloneList(queryStmt_.getResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    analyzeTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions, unless this is an
    // UPSERT, in which case we don't want to overwrite unmentioned columns with NULL.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Use getResultExprs() and not getBaseTblResultExprs() here because the final
            // substitution with TupleIsNullPredicate() wrapping happens in planning.
            selectListExprs = Expr.cloneList(queryStmt_.getResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    analyzeTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions, unless the target is
    // a Kudu table, in which case we don't want to overwrite unmentioned columns with
    // NULL.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#end_block

#method_before
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    // Finally, 'undo' the permutation so that the selectListExprs are in Hive column
    // order, and add NULL expressions to all missing columns, unless this is an UPSERT.
    ArrayList<Column> columns = table_.getColumnsInHiveOrder();
    for (int col = 0; col < columns.size(); ++col) {
        Column tblColumn = columns.get(col);
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                if (isUpsert_)
                    mentionedUpsertColumns_.add(col);
                matchFound = true;
                break;
            }
        }
        // expression if this is an INSERT.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols && !isUpsert_) {
                // Unmentioned non-clustering columns get NULL literals with the appropriate
                // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                resultExprs_.add(NullLiteral.create(tblColumn.getType()));
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && table_ instanceof KuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#method_after
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getLiteralValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    boolean isKuduTable = table_ instanceof KuduTable;
    // Finally, 'undo' the permutation so that the selectListExprs are in Hive column
    // order, and add NULL expressions to all missing columns, unless this is an UPSERT.
    ArrayList<Column> columns = table_.getColumnsInHiveOrder();
    for (int col = 0; col < columns.size(); ++col) {
        Column tblColumn = columns.get(col);
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                if (isKuduTable)
                    mentionedColumns_.add(col);
                matchFound = true;
                break;
            }
        }
        // expression if this is an INSERT and the target is not a Kudu table.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                if (isKuduTable) {
                    Preconditions.checkState(tblColumn instanceof KuduColumn);
                    KuduColumn kuduCol = (KuduColumn) tblColumn;
                    if (!kuduCol.hasDefaultValue() && !kuduCol.isNullable()) {
                        throw new AnalysisException("Missing values for column that is not " + "nullable and has no default value " + kuduCol.getName());
                    }
                } else {
                    // Unmentioned non-clustering columns get NULL literals with the appropriate
                    // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                    resultExprs_.add(NullLiteral.create(tblColumn.getType()));
                }
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && isKuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#end_block

#method_before
private void analyzePlanHints(Analyzer analyzer) throws AnalysisException {
    if (planHints_ == null)
        return;
    if (!planHints_.isEmpty() && table_ instanceof HBaseTable) {
        throw new AnalysisException("INSERT hints are only supported for inserting into " + "Hdfs and Kudu tables.");
    }
    boolean hasNoClusteredHint = false;
    for (PlanHint hint : planHints_) {
        if (hint.is("SHUFFLE")) {
            hasShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.is("NOSHUFFLE")) {
            hasNoShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.is("CLUSTERED")) {
            hasClusteredHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.is("NOCLUSTERED")) {
            hasNoClusteredHint = true;
            analyzer.setHasPlanHints();
        } else if (hint.is("SORTBY")) {
            analyzeSortByColumns(hint.getHintArgs());
            analyzer.setHasPlanHints();
        } else {
            analyzer.addWarning("INSERT hint not recognized: " + hint);
        }
    }
    // Both flags may be false or one of them may be true, but not both.
    if (hasShuffleHint_ && hasNoShuffleHint_) {
        throw new AnalysisException("Conflicting INSERT hints: shuffle and noshuffle");
    }
    if (hasClusteredHint_ && hasNoClusteredHint) {
        throw new AnalysisException("Conflicting INSERT hints: clustered and noclustered");
    }
}
#method_after
private void analyzePlanHints(Analyzer analyzer) throws AnalysisException {
    if (!planHints_.isEmpty() && table_ instanceof HBaseTable) {
        throw new AnalysisException(String.format("INSERT hints are only supported for " + "inserting into Hdfs and Kudu tables: %s", getTargetTableName()));
    }
    boolean hasNoClusteredHint = false;
    for (PlanHint hint : planHints_) {
        if (hint.is("SHUFFLE")) {
            hasShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.is("NOSHUFFLE")) {
            hasNoShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.is("CLUSTERED")) {
            hasClusteredHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.is("NOCLUSTERED")) {
            hasNoClusteredHint = true;
            analyzer.setHasPlanHints();
        } else if (hint.is("SORTBY")) {
            analyzeSortByHint(hint);
            analyzer.setHasPlanHints();
        } else {
            analyzer.addWarning("INSERT hint not recognized: " + hint);
        }
    }
    // Both flags may be false or one of them may be true, but not both.
    if (hasShuffleHint_ && hasNoShuffleHint_) {
        throw new AnalysisException("Conflicting INSERT hints: shuffle and noshuffle");
    }
    if (hasClusteredHint_ && hasNoClusteredHint) {
        throw new AnalysisException("Conflicting INSERT hints: clustered and noclustered");
    }
}
#end_block

#method_before
public ArrayList<Expr> getResultExprs() {
    return resultExprs_;
}
#method_after
@Override
public ArrayList<Expr> getResultExprs() {
    return resultExprs_;
}
#end_block

#method_before
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    Preconditions.checkState(isUpsert_ || mentionedUpsertColumns_.isEmpty());
    return TableSink.create(table_, isUpsert_ ? TableSink.Op.UPSERT : TableSink.Op.INSERT, partitionKeyExprs_, mentionedUpsertColumns_, overwrite_, hasClusteredHint_);
}
#method_after
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    return TableSink.create(table_, isUpsert_ ? TableSink.Op.UPSERT : TableSink.Op.INSERT, partitionKeyExprs_, mentionedColumns_, overwrite_, hasClusteredHint_);
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    if (withClause_ != null)
        strBuilder.append(withClause_.toSql() + " ");
    strBuilder.append(getOpName() + " ");
    if (overwrite_) {
        strBuilder.append("OVERWRITE ");
    } else {
        strBuilder.append("INTO ");
    }
    strBuilder.append("TABLE " + originalTableName_);
    if (columnPermutation_ != null) {
        strBuilder.append("(");
        strBuilder.append(Joiner.on(", ").join(columnPermutation_));
        strBuilder.append(")");
    }
    if (partitionKeyValues_ != null) {
        List<String> values = Lists.newArrayList();
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            values.add(pkv.getColName() + (pkv.getValue() != null ? ("=" + pkv.getValue().toSql()) : ""));
        }
        strBuilder.append(" PARTITION (" + Joiner.on(", ").join(values) + ")");
    }
    if (planHints_ != null) {
        strBuilder.append(" " + ToSqlUtils.getPlanHintsSql(getPlanHints()));
    }
    if (!needsGeneratedQueryStatement_) {
        strBuilder.append(" " + queryStmt_.toSql());
    }
    return strBuilder.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    if (withClause_ != null)
        strBuilder.append(withClause_.toSql() + " ");
    strBuilder.append(getOpName() + " ");
    if (overwrite_) {
        strBuilder.append("OVERWRITE ");
    } else {
        strBuilder.append("INTO ");
    }
    strBuilder.append("TABLE " + originalTableName_);
    if (columnPermutation_ != null) {
        strBuilder.append("(");
        strBuilder.append(Joiner.on(", ").join(columnPermutation_));
        strBuilder.append(")");
    }
    if (partitionKeyValues_ != null) {
        List<String> values = Lists.newArrayList();
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            values.add(pkv.getColName() + (pkv.getValue() != null ? ("=" + pkv.getValue().toSql()) : ""));
        }
        strBuilder.append(" PARTITION (" + Joiner.on(", ").join(values) + ")");
    }
    if (!planHints_.isEmpty()) {
        strBuilder.append(" " + ToSqlUtils.getPlanHintsSql(getPlanHints()));
    }
    if (!needsGeneratedQueryStatement_) {
        strBuilder.append(" " + queryStmt_.toSql());
    }
    return strBuilder.toString();
}
#end_block

#method_before
public void setPlanHints(List<PlanHint> planHints) {
    planHints_ = planHints;
}
#method_after
public void setPlanHints(List<PlanHint> planHints) {
    Preconditions.checkNotNull(planHints);
    planHints_ = planHints;
}
#end_block

#method_before
public boolean hasPlanHints() {
    return planHints_ != null;
}
#method_after
public boolean hasPlanHints() {
    return !planHints_.isEmpty();
}
#end_block

#method_before
public void analyzePlanHints(Analyzer analyzer) {
    if (planHints_ == null)
        return;
    for (PlanHint hint : planHints_) {
        if (!hint.is("straight_join")) {
            analyzer.addWarning("PLAN hint not recognized: " + hint);
        }
        analyzer.setIsStraightJoin();
    }
}
#method_after
public void analyzePlanHints(Analyzer analyzer) {
    for (PlanHint hint : planHints_) {
        if (!hint.is("straight_join")) {
            analyzer.addWarning("PLAN hint not recognized: " + hint);
        }
        analyzer.setIsStraightJoin();
    }
}
#end_block

#method_before
public boolean is(String s) {
    return hintName_.equals(s.toLowerCase());
}
#method_after
public boolean is(String s) {
    return name_.equalsIgnoreCase(s);
}
#end_block

#method_before
@Override
public boolean equals(Object o) {
    if (this == o)
        return true;
    if (o == null)
        return false;
    if (getClass() != o.getClass())
        return false;
    PlanHint oh = (PlanHint) o;
    return hintName_.equals(oh.hintName_) && hintArgs_.equals(oh.hintArgs_);
}
#method_after
@Override
public boolean equals(Object o) {
    if (this == o)
        return true;
    if (o == null)
        return false;
    if (getClass() != o.getClass())
        return false;
    PlanHint oh = (PlanHint) o;
    return name_.equals(oh.name_) && args_.equals(oh.args_);
}
#end_block

#method_before
@Override
public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append(hintName_);
    if (!hintArgs_.isEmpty()) {
        sb.append("(");
        sb.append(Joiner.on(",").join(hintArgs_));
        sb.append(")");
    }
    return sb.toString();
}
#method_after
@Override
public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append(name_);
    if (!args_.isEmpty()) {
        sb.append("(");
        sb.append(Joiner.on(",").join(args_));
        sb.append(")");
    }
    return sb.toString();
}
#end_block

#method_before
private boolean tryConvertBinaryKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof BinaryPredicate))
        return false;
    BinaryPredicate predicate = (BinaryPredicate) expr;
    // TODO KUDU-931 look into handling implicit/explicit casts on the SlotRef.
    predicate = normalizeSlotRefComparison(predicate, analyzer);
    if (predicate == null)
        return false;
    ComparisonOp op = getKuduOperator(predicate.getOp());
    if (op == null)
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    LiteralExpr literal = (LiteralExpr) predicate.getChild(1);
    // Cannot push prediates with null literal values (KUDU-1595).
    if (literal instanceof NullLiteral)
        return false;
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    KuduPredicate kuduPredicate = null;
    switch(literal.getType().getPrimitiveType()) {
        case BOOLEAN:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((BoolLiteral) literal).getValue());
                break;
            }
        case TINYINT:
        case SMALLINT:
        case INT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getIntValue());
                break;
            }
        case BIGINT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getLongValue());
                break;
            }
        case FLOAT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, (float) ((NumericLiteral) literal).getDoubleValue());
                break;
            }
        case DOUBLE:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getDoubleValue());
                break;
            }
        case STRING:
        case VARCHAR:
        case CHAR:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((StringLiteral) literal).getStringValue());
                break;
            }
        default:
            break;
    }
    if (kuduPredicate == null)
        return false;
    kuduConjuncts_.add(predicate);
    kuduPredicates_.add(kuduPredicate);
    return true;
}
#method_after
private boolean tryConvertBinaryKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof BinaryPredicate))
        return false;
    BinaryPredicate predicate = (BinaryPredicate) expr;
    // TODO KUDU-931 look into handling implicit/explicit casts on the SlotRef.
    predicate = normalizeSlotRefComparison(predicate, analyzer);
    if (predicate == null)
        return false;
    ComparisonOp op = getKuduOperator(predicate.getOp());
    if (op == null)
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    LiteralExpr literal = (LiteralExpr) predicate.getChild(1);
    // Cannot push predicates with null literal values (KUDU-1595).
    if (literal instanceof NullLiteral)
        return false;
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    KuduPredicate kuduPredicate = null;
    switch(literal.getType().getPrimitiveType()) {
        case BOOLEAN:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((BoolLiteral) literal).getValue());
                break;
            }
        case TINYINT:
        case SMALLINT:
        case INT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getIntValue());
                break;
            }
        case BIGINT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getLongValue());
                break;
            }
        case FLOAT:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, (float) ((NumericLiteral) literal).getDoubleValue());
                break;
            }
        case DOUBLE:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((NumericLiteral) literal).getDoubleValue());
                break;
            }
        case STRING:
        case VARCHAR:
        case CHAR:
            {
                kuduPredicate = KuduPredicate.newComparisonPredicate(column, op, ((StringLiteral) literal).getStringValue());
                break;
            }
        default:
            break;
    }
    if (kuduPredicate == null)
        return false;
    kuduConjuncts_.add(predicate);
    kuduPredicates_.add(kuduPredicate);
    return true;
}
#end_block

#method_before
private boolean tryConvertInListKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof InPredicate))
        return false;
    InPredicate predicate = (InPredicate) expr;
    // Only convert IN predicates, i.e. cannot convert NOT IN.
    if (predicate.isNotIn())
        return false;
    // Do not convert if there is an implicit cast.
    if (!(predicate.getChild(0) instanceof SlotRef))
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    // KuduPredicate takes a list of values as Objects.
    List<Object> values = Lists.newArrayList();
    for (int i = 1; i < predicate.getChildren().size(); ++i) {
        if (!(predicate.getChild(i).isLiteral()))
            return false;
        LiteralExpr literal = (LiteralExpr) predicate.getChild(i);
        // Cannot push prediates with null literal values (KUDU-1595).
        if (literal instanceof NullLiteral)
            return false;
        Object value = getKuduInListValue(literal);
        Preconditions.checkNotNull(value == null);
        values.add(value);
    }
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    kuduPredicates_.add(KuduPredicate.newInListPredicate(column, values));
    kuduConjuncts_.add(predicate);
    return true;
}
#method_after
private boolean tryConvertInListKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof InPredicate))
        return false;
    InPredicate predicate = (InPredicate) expr;
    // Only convert IN predicates, i.e. cannot convert NOT IN.
    if (predicate.isNotIn())
        return false;
    // Do not convert if there is an implicit cast.
    if (!(predicate.getChild(0) instanceof SlotRef))
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    // KuduPredicate takes a list of values as Objects.
    List<Object> values = Lists.newArrayList();
    for (int i = 1; i < predicate.getChildren().size(); ++i) {
        if (!(predicate.getChild(i).isLiteral()))
            return false;
        LiteralExpr literal = (LiteralExpr) predicate.getChild(i);
        // Cannot push predicates with null literal values (KUDU-1595).
        if (literal instanceof NullLiteral)
            return false;
        Object value = getKuduInListValue(literal);
        Preconditions.checkNotNull(value);
        values.add(value);
    }
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    kuduPredicates_.add(KuduPredicate.newInListPredicate(column, values));
    kuduConjuncts_.add(predicate);
    return true;
}
#end_block

#method_before
private TAccessLevel getAvailableAccessLevel(FileSystem fs, Path location) throws IOException {
    // permissions.
    if (FileSystemUtil.isS3AFileSystem(fs))
        return TAccessLevel.READ_WRITE;
    FsPermissionChecker permissionChecker = FsPermissionChecker.getInstance();
    while (location != null) {
        if (fs.exists(location)) {
            FsPermissionChecker.Permissions perms = permissionChecker.getPermissions(fs, location);
            if (perms.canReadAndWrite()) {
                return TAccessLevel.READ_WRITE;
            } else if (perms.canRead()) {
                return TAccessLevel.READ_ONLY;
            } else if (perms.canWrite()) {
                return TAccessLevel.WRITE_ONLY;
            }
            return TAccessLevel.NONE;
        }
        location = location.getParent();
    }
    // Should never get here.
    Preconditions.checkNotNull(location, "Error: no path ancestor exists");
    return TAccessLevel.NONE;
}
#method_after
private TAccessLevel getAvailableAccessLevel(FileSystem fs, Path location) throws IOException {
    // permissions. (see HADOOP-13892)
    if (FileSystemUtil.isS3AFileSystem(fs))
        return TAccessLevel.READ_WRITE;
    FsPermissionChecker permissionChecker = FsPermissionChecker.getInstance();
    while (location != null) {
        if (fs.exists(location)) {
            FsPermissionChecker.Permissions perms = permissionChecker.getPermissions(fs, location);
            if (perms.canReadAndWrite()) {
                return TAccessLevel.READ_WRITE;
            } else if (perms.canRead()) {
                return TAccessLevel.READ_ONLY;
            } else if (perms.canWrite()) {
                return TAccessLevel.WRITE_ONLY;
            }
            return TAccessLevel.NONE;
        }
        location = location.getParent();
    }
    // Should never get here.
    Preconditions.checkNotNull(location, "Error: no path ancestor exists");
    return TAccessLevel.NONE;
}
#end_block

#method_before
public byte[] getStats(byte[] thriftShowStatsParams) throws ImpalaException {
    TShowStatsParams params = new TShowStatsParams();
    JniUtil.deserializeThrift(protocolFactory_, params, thriftShowStatsParams);
    Preconditions.checkState(params.isSetTable_name());
    TResultSet result;
    if (params.isIs_show_col_stats()) {
        result = frontend_.getColumnStats(params.getTable_name().getDb_name(), params.getTable_name().getTable_name());
    } else {
        result = frontend_.getTableStats(params.getTable_name().getDb_name(), params.getTable_name().getTable_name(), params.isIs_show_range_partitions());
    }
    TSerializer serializer = new TSerializer(protocolFactory_);
    try {
        return serializer.serialize(result);
    } catch (TException e) {
        throw new InternalException(e.getMessage());
    }
}
#method_after
public byte[] getStats(byte[] thriftShowStatsParams) throws ImpalaException {
    TShowStatsParams params = new TShowStatsParams();
    JniUtil.deserializeThrift(protocolFactory_, params, thriftShowStatsParams);
    Preconditions.checkState(params.isSetTable_name());
    TResultSet result;
    if (params.op == TShowStatsOp.COLUMN_STATS) {
        result = frontend_.getColumnStats(params.getTable_name().getDb_name(), params.getTable_name().getTable_name());
    } else {
        result = frontend_.getTableStats(params.getTable_name().getDb_name(), params.getTable_name().getTable_name(), params.op);
    }
    TSerializer serializer = new TSerializer(protocolFactory_);
    try {
        return serializer.serialize(result);
    } catch (TException e) {
        throw new InternalException(e.getMessage());
    }
}
#end_block

#method_before
public TResultSet getTableStats(String dbName, String tableName, boolean isShowRangePartitions) throws ImpalaException {
    Table table = impaladCatalog_.getTable(dbName, tableName);
    if (table instanceof HdfsTable) {
        return ((HdfsTable) table).getTableStats();
    } else if (table instanceof HBaseTable) {
        return ((HBaseTable) table).getTableStats();
    } else if (table instanceof DataSourceTable) {
        return ((DataSourceTable) table).getTableStats();
    } else if (table instanceof KuduTable) {
        if (isShowRangePartitions) {
            return ((KuduTable) table).getRangePartitions();
        } else {
            return ((KuduTable) table).getTableStats();
        }
    } else {
        throw new InternalException("Invalid table class: " + table.getClass());
    }
}
#method_after
public TResultSet getTableStats(String dbName, String tableName, TShowStatsOp op) throws ImpalaException {
    Table table = impaladCatalog_.getTable(dbName, tableName);
    if (table instanceof HdfsTable) {
        return ((HdfsTable) table).getTableStats();
    } else if (table instanceof HBaseTable) {
        return ((HBaseTable) table).getTableStats();
    } else if (table instanceof DataSourceTable) {
        return ((DataSourceTable) table).getTableStats();
    } else if (table instanceof KuduTable) {
        if (op == TShowStatsOp.RANGE_PARTITIONS) {
            return ((KuduTable) table).getRangePartitions();
        } else {
            return ((KuduTable) table).getTableStats();
        }
    } else {
        throw new InternalException("Invalid table class: " + table.getClass());
    }
}
#end_block

#method_before
@Override
public void load(boolean dummy, /* not used */
IMetaStoreClient msClient, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    msTable_ = msTbl;
    // This is set to 0 for Kudu tables.
    // TODO: Change this to reflect the number of pk columns and modify all the
    // places (e.g. insert stmt) that currently make use of this parameter.
    numClusteringCols_ = 0;
    kuduTableName_ = msTable_.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkNotNull(kuduTableName_);
    kuduMasters_ = msTable_.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    Preconditions.checkNotNull(kuduMasters_);
    org.apache.kudu.client.KuduTable kuduTable = null;
    numRows_ = getRowCount(msTable_.getParameters());
    // Connect to Kudu to retrieve table metadata
    try (KuduClient kuduClient = KuduUtil.createKuduClient(getKuduMasterHosts())) {
        kuduTable = kuduClient.openTable(kuduTableName_);
    } catch (KuduException e) {
        throw new TableLoadingException(String.format("Error opening Kudu table '%s', Kudu error: %s", kuduTableName_, e.getMessage()));
    }
    Preconditions.checkNotNull(kuduTable);
    // Load metadata from Kudu and HMS
    try {
        loadSchema(kuduTable);
        loadDistributeByParams(kuduTable);
        loadAllColumnStats(msClient);
    } catch (ImpalaRuntimeException e) {
        throw new TableLoadingException("Error loading metadata for Kudu table " + kuduTableName_, e);
    }
    // Update the table schema in HMS.
    try {
        long lastDdlTime = CatalogOpExecutor.calculateDdlTime(msTable_);
        msTable_.putToParameters("transient_lastDdlTime", Long.toString(lastDdlTime));
        msTable_.putToParameters(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);
        msClient.alter_table(msTable_.getDbName(), msTable_.getTableName(), msTable_);
    } catch (TException e) {
        throw new TableLoadingException(e.getMessage());
    }
}
#method_after
@Override
public void load(boolean dummy, /* not used */
IMetaStoreClient msClient, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    msTable_ = msTbl;
    // This is set to 0 for Kudu tables.
    // TODO: Change this to reflect the number of pk columns and modify all the
    // places (e.g. insert stmt) that currently make use of this parameter.
    numClusteringCols_ = 0;
    kuduTableName_ = msTable_.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkNotNull(kuduTableName_);
    kuduMasters_ = msTable_.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    Preconditions.checkNotNull(kuduMasters_);
    org.apache.kudu.client.KuduTable kuduTable = null;
    numRows_ = getRowCount(msTable_.getParameters());
    // Connect to Kudu to retrieve table metadata
    try (KuduClient kuduClient = KuduUtil.createKuduClient(getKuduMasterHosts())) {
        kuduTable = kuduClient.openTable(kuduTableName_);
    } catch (KuduException e) {
        throw new TableLoadingException(String.format("Error opening Kudu table '%s', Kudu error: %s", kuduTableName_, e.getMessage()));
    }
    Preconditions.checkNotNull(kuduTable);
    // Load metadata from Kudu and HMS
    try {
        loadSchema(kuduTable);
        loadPartitionByParams(kuduTable);
        loadAllColumnStats(msClient);
    } catch (ImpalaRuntimeException e) {
        throw new TableLoadingException("Error loading metadata for Kudu table " + kuduTableName_, e);
    }
    // Update the table schema in HMS.
    try {
        long lastDdlTime = CatalogOpExecutor.calculateDdlTime(msTable_);
        msTable_.putToParameters("transient_lastDdlTime", Long.toString(lastDdlTime));
        msTable_.putToParameters(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);
        msClient.alter_table(msTable_.getDbName(), msTable_.getTableName(), msTable_);
    } catch (TException e) {
        throw new TableLoadingException(e.getMessage());
    }
}
#end_block

#method_before
public static KuduTable createCtasTarget(Db db, org.apache.hadoop.hive.metastore.api.Table msTbl, List<ColumnDef> columnDefs, List<String> primaryKeyColumnNames, List<DistributeParam> distributeParams) {
    KuduTable tmpTable = new KuduTable(msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    int pos = 0;
    for (ColumnDef colDef : columnDefs) {
        tmpTable.addColumn(new Column(colDef.getColName(), colDef.getType(), pos++));
    }
    tmpTable.primaryKeyColumnNames_.addAll(primaryKeyColumnNames);
    tmpTable.distributeBy_.addAll(distributeParams);
    return tmpTable;
}
#method_after
public static KuduTable createCtasTarget(Db db, org.apache.hadoop.hive.metastore.api.Table msTbl, List<ColumnDef> columnDefs, List<String> primaryKeyColumnNames, List<KuduPartitionParam> partitionParams) {
    KuduTable tmpTable = new KuduTable(msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    int pos = 0;
    for (ColumnDef colDef : columnDefs) {
        tmpTable.addColumn(new Column(colDef.getColName(), colDef.getType(), pos++));
    }
    tmpTable.primaryKeyColumnNames_.addAll(primaryKeyColumnNames);
    tmpTable.partitionBy_.addAll(partitionParams);
    return tmpTable;
}
#end_block

#method_before
@Override
protected void loadFromThrift(TTable thriftTable) throws TableLoadingException {
    super.loadFromThrift(thriftTable);
    TKuduTable tkudu = thriftTable.getKudu_table();
    kuduTableName_ = tkudu.getTable_name();
    kuduMasters_ = Joiner.on(',').join(tkudu.getMaster_addresses());
    primaryKeyColumnNames_.clear();
    primaryKeyColumnNames_.addAll(tkudu.getKey_columns());
    loadDistributeByParamsFromThrift(tkudu.getDistribute_by());
}
#method_after
@Override
protected void loadFromThrift(TTable thriftTable) throws TableLoadingException {
    super.loadFromThrift(thriftTable);
    TKuduTable tkudu = thriftTable.getKudu_table();
    kuduTableName_ = tkudu.getTable_name();
    kuduMasters_ = Joiner.on(',').join(tkudu.getMaster_addresses());
    primaryKeyColumnNames_.clear();
    primaryKeyColumnNames_.addAll(tkudu.getKey_columns());
    loadPartitionByParamsFromThrift(tkudu.getPartition_by());
}
#end_block

#method_before
private TKuduTable getTKuduTable() {
    TKuduTable tbl = new TKuduTable();
    tbl.setKey_columns(Preconditions.checkNotNull(primaryKeyColumnNames_));
    tbl.setMaster_addresses(Lists.newArrayList(kuduMasters_.split(",")));
    tbl.setTable_name(kuduTableName_);
    Preconditions.checkNotNull(distributeBy_);
    for (DistributeParam distributeParam : distributeBy_) {
        tbl.addToDistribute_by(distributeParam.toThrift());
    }
    return tbl;
}
#method_after
private TKuduTable getTKuduTable() {
    TKuduTable tbl = new TKuduTable();
    tbl.setKey_columns(Preconditions.checkNotNull(primaryKeyColumnNames_));
    tbl.setMaster_addresses(Lists.newArrayList(kuduMasters_.split(",")));
    tbl.setTable_name(kuduTableName_);
    Preconditions.checkNotNull(partitionBy_);
    for (KuduPartitionParam partitionParam : partitionBy_) {
        tbl.addToPartition_by(partitionParam.toThrift());
    }
    return tbl;
}
#end_block

#method_before
public TResultSet getRangePartitions() throws ImpalaRuntimeException {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    resultSchema.addToColumns(new TColumn("Partition Specifier", Type.STRING.toThrift()));
    try (KuduClient client = KuduUtil.createKuduClient(getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable kuduTable = client.openTable(kuduTableName_);
        List<String> partitions = kuduTable.getFormattedRangePartitions(BackendConfig.INSTANCE.getKuduClientTimeoutMs());
        if (partitions.isEmpty()) {
            TResultRowBuilder builder = new TResultRowBuilder();
            result.addToRows(builder.add("").get());
            return result;
        }
        for (String partition : partitions) {
            TResultRowBuilder builder = new TResultRowBuilder();
            builder.add(partition);
            result.addToRows(builder.get());
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Error accessing Kudu for table partitions.", e);
    }
    return result;
}
#method_after
public TResultSet getRangePartitions() throws ImpalaRuntimeException {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    // Build column header
    String header = "RANGE (" + Joiner.on(',').join(getRangePartitioningColNames()) + ")";
    resultSchema.addToColumns(new TColumn(header, Type.STRING.toThrift()));
    try (KuduClient client = KuduUtil.createKuduClient(getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable kuduTable = client.openTable(kuduTableName_);
        // The Kudu table API will return the partitions in sorted order by value.
        List<String> partitions = kuduTable.getFormattedRangePartitions(BackendConfig.INSTANCE.getKuduClientTimeoutMs());
        if (partitions.isEmpty()) {
            TResultRowBuilder builder = new TResultRowBuilder();
            result.addToRows(builder.add("").get());
            return result;
        }
        for (String partition : partitions) {
            TResultRowBuilder builder = new TResultRowBuilder();
            builder.add(partition);
            result.addToRows(builder.get());
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Error accessing Kudu for table partitions.", e);
    }
    return result;
}
#end_block

#method_before
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo (i int,)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, " + "HASH(a) INTO 2 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int, k int) DISTRIBUTE BY HASH INTO 4 BUCKETS," + " HASH(k) INTO 4 BUCKETS");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo DISTRIBUTE BY HASH INTO 4 BUCKETS");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) DISTRIBUTE BY " + "HASH (a) INTO 3 BUCKETS, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 1 + 1 < VALUES) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY HASH (a) INTO 4 BUCKETS, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) INTO 3 BUCKETS");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUES = 10) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 10 < VALUE < 20) STORED AS KUDU");
    // Column options for Kudu tables
    String[] encodings = { "encoding auto_encoding", "encoding plain_encoding", "encoding prefix_encoding", "encoding group_varint", "encoding rle", "encoding dict_encoding", "encoding bit_shuffle", "encoding unknown", "" };
    String[] compression = { "compression default_compression", "compression no_compression", "compression snappy", "compression lz4", "compression zlib", "compression unknown", "" };
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (String enc : encodings) {
        for (String comp : compression) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", nul, enc, comp, def, block));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", block, nul, enc, comp, def));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", def, block, nul, enc, comp));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", comp, def, block, nul, enc));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, def, block, nul));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, block, def, nul));
                    }
                }
            }
        }
    }
    // Column option is specified multiple times for the same column
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY ENCODING RLE ENCODING PLAIN) " + "STORED AS KUDU");
    // Constant expr used in DEFAULT
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b int DEFAULT 1+1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b float DEFAULT cast(1.1 as float)) " + "STORED AS KUDU");
    // Non-literal value used in BLOCK_SIZE
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY, b int BLOCK_SIZE 1+1) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY BLOCK_SIZE -1) STORED AS KUDU");
}
#method_after
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo (i int,)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY HASH(i) PARTITIONS 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY HASH(i) PARTITIONS 4, " + "HASH(a) PARTITIONS 2");
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY HASH PARTITIONS 4");
    ParsesOk("CREATE TABLE Foo (i int, k int) PARTITION BY HASH PARTITIONS 4," + " HASH(k) PARTITIONS 4");
    ParserError("CREATE TABLE Foo (i int) PARTITION BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo PARTITION BY HASH PARTITIONS 4");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITION BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) PARTITION BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) PARTITION BY " + "HASH (a) PARTITIONS 3, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION 1 + 1 < VALUES) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int, b int) PARTITION BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY HASH (a) PARTITIONS 4, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) PARTITIONS 3");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION VALUES = 10) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) PARTITION BY RANGE (a) " + "(PARTITION 10 < VALUE < 20) STORED AS KUDU");
    // Column options for Kudu tables
    String[] encodings = { "encoding auto_encoding", "encoding plain_encoding", "encoding prefix_encoding", "encoding group_varint", "encoding rle", "encoding dict_encoding", "encoding bit_shuffle", "encoding unknown", "" };
    String[] compression = { "compression default_compression", "compression no_compression", "compression snappy", "compression lz4", "compression zlib", "compression unknown", "" };
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (String enc : encodings) {
        for (String comp : compression) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", nul, enc, comp, def, block));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", block, nul, enc, comp, def));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", def, block, nul, enc, comp));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", comp, def, block, nul, enc));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, def, block, nul));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, block, def, nul));
                    }
                }
            }
        }
    }
    // Column option is specified multiple times for the same column
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY ENCODING RLE ENCODING PLAIN) " + "STORED AS KUDU");
    // Constant expr used in DEFAULT
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b int DEFAULT 1+1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b float DEFAULT cast(1.1 as float)) " + "STORED AS KUDU");
    // Non-literal value used in BLOCK_SIZE
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY, b int BLOCK_SIZE 1+1) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY BLOCK_SIZE -1) STORED AS KUDU");
}
#end_block

#method_before
@Test
public void TestCreateTableAsSelect() {
    ParsesOk("CREATE TABLE Foo AS SELECT 1, 2, 3");
    ParsesOk("CREATE TABLE Foo AS SELECT * from foo.bar");
    ParsesOk("CREATE TABLE Foo.Bar AS SELECT int_col, bool_col from tbl limit 10");
    ParsesOk("CREATE TABLE Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE Foo STORED AS PARQUET AS SELECT 1");
    ParsesOk("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH INTO 2 BUCKETS " + "AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH (b) INTO 2 " + "BUCKETS AS SELECT * from bar");
    // With clause works
    ParsesOk("CREATE TABLE Foo AS with t1 as (select 1) select * from t1");
    // Incomplete AS SELECT statement
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS SELECT");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS WITH");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS");
    // INSERT/UPSERT statements are not allowed
    ParserError("CREATE TABLE Foo AS INSERT INTO Foo SELECT 1");
    ParserError("CREATE TABLE Foo AS UPSERT INTO Foo SELECT 1");
    // Column and partition definitions not allowed
    ParserError("CREATE TABLE Foo(i int) AS SELECT 1");
    ParserError("CREATE TABLE Foo PARTITIONED BY(i int) AS SELECT 1");
    // Partitioned by syntax following insert into syntax
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) ROW FORMAT DELIMITED STORED AS " + "PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1, 2");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT * from Bar");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a=2, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a, b=2) AS SELECT * from Bar");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (i) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS " + "SELECT 1");
    ParserError("CREATE TABLE Foo DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY HASH(a) INTO 4 BUCKETS " + "TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY RANGE(a) " + "(PARTITION 1 < VALUES < 10, PARTITION 10 <= VALUES < 20, PARTITION VALUE = 30) " + "STORED AS KUDU AS SELECT * FROM Bar");
}
#method_after
@Test
public void TestCreateTableAsSelect() {
    ParsesOk("CREATE TABLE Foo AS SELECT 1, 2, 3");
    ParsesOk("CREATE TABLE Foo AS SELECT * from foo.bar");
    ParsesOk("CREATE TABLE Foo.Bar AS SELECT int_col, bool_col from tbl limit 10");
    ParsesOk("CREATE TABLE Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE Foo STORED AS PARQUET AS SELECT 1");
    ParsesOk("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) PARTITION BY HASH PARTITIONS 2 " + "AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) PARTITION BY HASH (b) PARTITIONS 2 " + "AS SELECT * from bar");
    // With clause works
    ParsesOk("CREATE TABLE Foo AS with t1 as (select 1) select * from t1");
    // Incomplete AS SELECT statement
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS SELECT");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS WITH");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS");
    // INSERT/UPSERT statements are not allowed
    ParserError("CREATE TABLE Foo AS INSERT INTO Foo SELECT 1");
    ParserError("CREATE TABLE Foo AS UPSERT INTO Foo SELECT 1");
    // Column and partition definitions not allowed
    ParserError("CREATE TABLE Foo(i int) AS SELECT 1");
    ParserError("CREATE TABLE Foo PARTITIONED BY(i int) AS SELECT 1");
    // Partitioned by syntax following insert into syntax
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) ROW FORMAT DELIMITED STORED AS " + "PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1, 2");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT * from Bar");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a=2, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a, b=2) AS SELECT * from Bar");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (i) PARTITION BY HASH(i) PARTITIONS 4 AS " + "SELECT 1");
    ParserError("CREATE TABLE Foo PARTITION BY HASH(i) PARTITIONS 4 AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) PARTITION BY HASH(a) PARTITIONS 4 " + "TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) PARTITION BY RANGE(a) " + "(PARTITION 1 < VALUES < 10, PARTITION 10 <= VALUES < 20, PARTITION VALUE = 30) " + "STORED AS KUDU AS SELECT * FROM Bar");
}
#end_block

#method_before
protected String getSqlPrefix() {
    return "SHOW " + ((isShowColStats_) ? "COLUMN" : "TABLE") + " STATS";
}
#method_after
protected String getSqlPrefix() {
    if (op_ == TShowStatsOp.TABLE_STATS) {
        return "SHOW TABLE STATS";
    } else if (op_ == TShowStatsOp.COLUMN_STATS) {
        return "SHOW COLUMN STATS";
    } else if (op_ == TShowStatsOp.PARTITIONS) {
        return "SHOW PARTITIONS";
    } else if (op_ == TShowStatsOp.RANGE_PARTITIONS) {
        return "SHOW RANGE PARTITIONS";
    } else {
        Preconditions.checkState(false);
        return "";
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    table_ = analyzer.getTable(tableName_, Privilege.VIEW_METADATA);
    if (table_ instanceof View) {
        throw new AnalysisException(String.format("%s not applicable to a view: %s", getSqlPrefix(), table_.getFullName()));
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    table_ = analyzer.getTable(tableName_, Privilege.VIEW_METADATA);
    Preconditions.checkNotNull(table_);
    if (table_ instanceof View) {
        throw new AnalysisException(String.format("%s not applicable to a view: %s", getSqlPrefix(), table_.getFullName()));
    }
    if (table_ instanceof HdfsTable) {
        if (table_.getNumClusteringCols() == 0 && op_ == TShowStatsOp.PARTITIONS) {
            throw new AnalysisException("Table is not partitioned: " + table_.getFullName());
        }
        if (op_ == TShowStatsOp.RANGE_PARTITIONS) {
            throw new AnalysisException(getSqlPrefix() + " must target a Kudu table: " + table_.getFullName());
        }
    } else if (table_ instanceof KuduTable) {
        KuduTable kuduTable = (KuduTable) table_;
        if (op_ == TShowStatsOp.RANGE_PARTITIONS && kuduTable.getRangePartitioningColNames().isEmpty()) {
            throw new AnalysisException(getSqlPrefix() + " requested but table does not " + "have range partitions: " + table_.getFullName());
        }
    } else {
        if (op_ == TShowStatsOp.RANGE_PARTITIONS) {
            throw new AnalysisException(getSqlPrefix() + " must target a Kudu table: " + table_.getFullName());
        } else if (op_ == TShowStatsOp.PARTITIONS) {
            throw new AnalysisException(getSqlPrefix() + " must target an HDFS table: " + table_.getFullName());
        }
    }
}
#end_block

#method_before
public TShowStatsParams toThrift() {
    // Ensure the DB is set in the table_name field by using table and not tableName.
    return new TShowStatsParams(isShowColStats_, isShowRangePartitions_, new TableName(table_.getDb().getName(), table_.getName()).toThrift());
}
#method_after
public TShowStatsParams toThrift() {
    // Ensure the DB is set in the table_name field by using table and not tableName.
    return new TShowStatsParams(op_, new TableName(table_.getDb().getName(), table_.getName()).toThrift());
}
#end_block

#method_before
private static Type getTypeInfo(Schema schema, String colName) throws AnalysisException {
    // and NULL.  This is annoying and we're going to hide it from the user.
    if (isNullableType(schema)) {
        return getTypeInfo(getColumnType(schema), colName);
    }
    Schema.Type type = schema.getType();
    if (avroToImpalaPrimitiveTypeMap_.containsKey(type)) {
        return avroToImpalaPrimitiveTypeMap_.get(type);
    }
    switch(type) {
        case ARRAY:
            Type itemType = getTypeInfo(schema.getElementType(), colName);
            return new ArrayType(itemType);
        case MAP:
            Type valueType = getTypeInfo(schema.getValueType(), colName);
            return new MapType(Type.STRING, valueType);
        case RECORD:
            StructType structType = new StructType();
            for (Schema.Field field : schema.getFields()) {
                Type fieldType = getTypeInfo(field.schema(), colName);
                structType.addField(new StructField(field.name(), fieldType, field.doc()));
            }
            return structType;
        case BYTES:
            // Decimal is stored in Avro as a BYTE.
            Type decimalType = getDecimalType(schema);
            if (decimalType != null)
                return decimalType;
            String logicalType = schema.getProp("logicalType");
            if (logicalType == null) {
                throw new AnalysisException(String.format("logicalType for column '%s' specified at wrong level or was not specified", colName));
            }
        // TODO: Add support for stored Avro UNIONs by exposing them as STRUCTs in Impala.
        case UNION:
        case ENUM:
        case FIXED:
        case NULL:
        default:
            {
                throw new AnalysisException(String.format("Unsupported type '%s' of column '%s'", type.getName(), colName));
            }
    }
}
#method_after
private static Type getTypeInfo(Schema schema, String colName) throws AnalysisException {
    // and NULL.  This is annoying and we're going to hide it from the user.
    if (isNullableType(schema)) {
        return getTypeInfo(getColumnType(schema), colName);
    }
    Schema.Type type = schema.getType();
    if (avroToImpalaPrimitiveTypeMap_.containsKey(type)) {
        return avroToImpalaPrimitiveTypeMap_.get(type);
    }
    switch(type) {
        case ARRAY:
            Type itemType = getTypeInfo(schema.getElementType(), colName);
            return new ArrayType(itemType);
        case MAP:
            Type valueType = getTypeInfo(schema.getValueType(), colName);
            return new MapType(Type.STRING, valueType);
        case RECORD:
            StructType structType = new StructType();
            for (Schema.Field field : schema.getFields()) {
                Type fieldType = getTypeInfo(field.schema(), colName);
                structType.addField(new StructField(field.name(), fieldType, field.doc()));
            }
            return structType;
        case BYTES:
            String logicalType = schema.getProp("logicalType");
            if (logicalType == null) {
                throw new AnalysisException(String.format("logicalType for column '%s' specified at wrong level or was not specified", colName));
            }
            // Decimal is stored in Avro as a BYTE.
            if (logicalType.equalsIgnoreCase("decimal")) {
                return getDecimalType(schema);
            } else {
                throw new AnalysisException(String.format("Unsupported logicalType: '%s' for column '%s' with type BYTES", logicalType, colName));
            }
        // TODO: Add support for stored Avro UNIONs by exposing them as STRUCTs in Impala.
        case UNION:
        case ENUM:
        case FIXED:
        case NULL:
        default:
            {
                throw new AnalysisException(String.format("Unsupported type '%s' of column '%s'", type.getName(), colName));
            }
    }
}
#end_block

#method_before
private static Type getDecimalType(Schema schema) {
    Preconditions.checkState(schema.getType() == Schema.Type.BYTES);
    String logicalType = schema.getProp("logicalType");
    if (logicalType != null && logicalType.equalsIgnoreCase("decimal")) {
        // Parse the scale/precision of the decimal type.
        Integer scale = getDecimalProp(schema, "scale");
        // The Avro spec states that scale should default to zero if not set.
        if (scale == null)
            scale = 0;
        // Precision is a required property according to the Avro spec.
        Integer precision = getDecimalProp(schema, "precision");
        if (precision == null) {
            throw new SchemaParseException("No 'precision' property specified for 'decimal' logicalType");
        }
        return ScalarType.createDecimalType(precision, scale);
    }
    return null;
}
#method_after
private static Type getDecimalType(Schema schema) {
    Preconditions.checkState(schema.getType() == Schema.Type.BYTES);
    // Parse the scale/precision of the decimal type.
    Integer scale = getDecimalProp(schema, "scale");
    // The Avro spec states that scale should default to zero if not set.
    if (scale == null)
        scale = 0;
    // Precision is a required property according to the Avro spec.
    Integer precision = getDecimalProp(schema, "precision");
    if (precision == null) {
        throw new SchemaParseException("No 'precision' property specified for 'decimal' logicalType");
    }
    return ScalarType.createDecimalType(precision, scale);
}
#end_block

#method_before
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    List<String> rawPath = tableRef.getPath();
    Path resolvedPath = null;
    try {
        resolvedPath = resolvePath(tableRef.getPath(), PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath.size() > 1) {
                registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath.get(0), rawPath.get(1)).allOf(Privilege.SELECT).toRequest());
            }
            registerPrivReq(new PrivilegeRequestBuilder().onTable(getDefaultDb(), rawPath.get(0)).allOf(Privilege.SELECT).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath)), e);
    }
    Preconditions.checkNotNull(resolvedPath);
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof KuduTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef, resolvedPath);
    } else {
        return new CollectionTableRef(tableRef, resolvedPath);
    }
}
#method_after
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    List<String> rawPath = tableRef.getPath();
    Path resolvedPath = null;
    try {
        resolvedPath = resolvePath(tableRef.getPath(), PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath.size() > 1) {
                registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath.get(0), rawPath.get(1)).allOf(tableRef.getPrivilege()).toRequest());
            }
            registerPrivReq(new PrivilegeRequestBuilder().onTable(getDefaultDb(), rawPath.get(0)).allOf(tableRef.getPrivilege()).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath)), e);
    }
    Preconditions.checkNotNull(resolvedPath);
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof KuduTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef, resolvedPath);
    } else {
        return new CollectionTableRef(tableRef, resolvedPath);
    }
}
#end_block

#method_before
public void registerFullOuterJoinedConjunct(Expr e) {
    Preconditions.checkState(!globalState_.fullOuterJoinedConjuncts.containsKey(e.getId()));
    List<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    for (TupleId tid : tids) {
        if (!globalState_.fullOuterJoinedTupleIds.containsKey(tid))
            continue;
        TableRef currentOuterJoin = globalState_.fullOuterJoinedTupleIds.get(tid);
        globalState_.fullOuterJoinedConjuncts.put(e.getId(), currentOuterJoin);
        break;
    }
    LOG.trace("registerFullOuterJoinedConjunct: " + globalState_.fullOuterJoinedConjuncts.toString());
}
#method_after
public void registerFullOuterJoinedConjunct(Expr e) {
    Preconditions.checkState(!globalState_.fullOuterJoinedConjuncts.containsKey(e.getId()));
    List<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    for (TupleId tid : tids) {
        if (!globalState_.fullOuterJoinedTupleIds.containsKey(tid))
            continue;
        TableRef currentOuterJoin = globalState_.fullOuterJoinedTupleIds.get(tid);
        globalState_.fullOuterJoinedConjuncts.put(e.getId(), currentOuterJoin);
        break;
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerFullOuterJoinedConjunct: " + globalState_.fullOuterJoinedConjuncts.toString());
    }
}
#end_block

#method_before
public void registerFullOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.fullOuterJoinedTupleIds.put(tid, rhsRef);
    }
    LOG.trace("registerFullOuterJoinedTids: " + globalState_.fullOuterJoinedTupleIds.toString());
}
#method_after
public void registerFullOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.fullOuterJoinedTupleIds.put(tid, rhsRef);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerFullOuterJoinedTids: " + globalState_.fullOuterJoinedTupleIds.toString());
    }
}
#end_block

#method_before
public void registerOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.outerJoinedTupleIds.put(tid, rhsRef);
    }
    LOG.trace("registerOuterJoinedTids: " + globalState_.outerJoinedTupleIds.toString());
}
#method_after
public void registerOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.outerJoinedTupleIds.put(tid, rhsRef);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerOuterJoinedTids: " + globalState_.outerJoinedTupleIds.toString());
    }
}
#end_block

#method_before
private void registerConjunct(Expr e) {
    // always generate a new expr id; this might be a cloned conjunct that already
    // has the id of its origin set
    e.setId(globalState_.conjunctIdGenerator.getNextId());
    globalState_.conjuncts.put(e.getId(), e);
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    ArrayList<SlotId> slotIds = Lists.newArrayList();
    e.getIds(tupleIds, slotIds);
    registerFullOuterJoinedConjunct(e);
    // register single tid conjuncts
    if (tupleIds.size() == 1)
        globalState_.singleTidConjuncts.add(e.getId());
    LOG.trace("register tuple/slotConjunct: " + Integer.toString(e.getId().asInt()) + " " + e.toSql() + " " + e.debugString());
    if (!(e instanceof BinaryPredicate))
        return;
    BinaryPredicate binaryPred = (BinaryPredicate) e;
    // exactly one tuple id
    if (binaryPred.getOp() != BinaryPredicate.Operator.EQ && binaryPred.getOp() != BinaryPredicate.Operator.NULL_MATCHING_EQ && binaryPred.getOp() != BinaryPredicate.Operator.NOT_DISTINCT) {
        return;
    }
    // the binary predicate must refer to at least two tuples to be an eqJoinConjunct
    if (tupleIds.size() < 2)
        return;
    // examine children and update eqJoinConjuncts
    for (int i = 0; i < 2; ++i) {
        tupleIds = Lists.newArrayList();
        binaryPred.getChild(i).getIds(tupleIds, null);
        if (tupleIds.size() == 1) {
            if (!globalState_.eqJoinConjuncts.containsKey(tupleIds.get(0))) {
                List<ExprId> conjunctIds = Lists.newArrayList();
                conjunctIds.add(e.getId());
                globalState_.eqJoinConjuncts.put(tupleIds.get(0), conjunctIds);
            } else {
                globalState_.eqJoinConjuncts.get(tupleIds.get(0)).add(e.getId());
            }
            binaryPred.setIsEqJoinConjunct(true);
            LOG.trace("register eqJoinConjunct: " + Integer.toString(e.getId().asInt()));
        }
    }
}
#method_after
private void registerConjunct(Expr e) {
    // always generate a new expr id; this might be a cloned conjunct that already
    // has the id of its origin set
    e.setId(globalState_.conjunctIdGenerator.getNextId());
    globalState_.conjuncts.put(e.getId(), e);
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    ArrayList<SlotId> slotIds = Lists.newArrayList();
    e.getIds(tupleIds, slotIds);
    registerFullOuterJoinedConjunct(e);
    // register single tid conjuncts
    if (tupleIds.size() == 1 && !e.isAuxExpr()) {
        globalState_.singleTidConjuncts.add(e.getId());
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("register tuple/slotConjunct: " + Integer.toString(e.getId().asInt()) + " " + e.toSql() + " " + e.debugString());
    }
    if (!(e instanceof BinaryPredicate))
        return;
    BinaryPredicate binaryPred = (BinaryPredicate) e;
    // exactly one tuple id
    if (binaryPred.getOp() != BinaryPredicate.Operator.EQ && binaryPred.getOp() != BinaryPredicate.Operator.NULL_MATCHING_EQ && binaryPred.getOp() != BinaryPredicate.Operator.NOT_DISTINCT) {
        return;
    }
    // the binary predicate must refer to at least two tuples to be an eqJoinConjunct
    if (tupleIds.size() < 2)
        return;
    // examine children and update eqJoinConjuncts
    for (int i = 0; i < 2; ++i) {
        tupleIds = Lists.newArrayList();
        binaryPred.getChild(i).getIds(tupleIds, null);
        if (tupleIds.size() == 1) {
            if (!globalState_.eqJoinConjuncts.containsKey(tupleIds.get(0))) {
                List<ExprId> conjunctIds = Lists.newArrayList();
                conjunctIds.add(e.getId());
                globalState_.eqJoinConjuncts.put(tupleIds.get(0), conjunctIds);
            } else {
                globalState_.eqJoinConjuncts.get(tupleIds.get(0)).add(e.getId());
            }
            binaryPred.setIsEqJoinConjunct(true);
            LOG.trace("register eqJoinConjunct: " + Integer.toString(e.getId().asInt()));
        }
    }
}
#end_block

#method_before
public void createAuxEquivPredicate(Expr lhs, Expr rhs) {
    // implicitly cast to a type different than NULL.
    if (lhs instanceof NullLiteral || rhs instanceof NullLiteral || lhs.getType().isNull() || rhs.getType().isNull()) {
        return;
    }
    // create an eq predicate between lhs and rhs
    BinaryPredicate p = new BinaryPredicate(BinaryPredicate.Operator.EQ, lhs, rhs);
    p.setIsAuxExpr();
    LOG.trace("register equiv predicate: " + p.toSql() + " " + p.debugString());
    registerConjunct(p);
}
#method_after
public void createAuxEquivPredicate(Expr lhs, Expr rhs) {
    // implicitly cast to a type different than NULL.
    if (lhs instanceof NullLiteral || rhs instanceof NullLiteral || lhs.getType().isNull() || rhs.getType().isNull()) {
        return;
    }
    // create an eq predicate between lhs and rhs
    BinaryPredicate p = new BinaryPredicate(BinaryPredicate.Operator.EQ, lhs, rhs);
    p.setIsAuxExpr();
    if (LOG.isTraceEnabled()) {
        LOG.trace("register equiv predicate: " + p.toSql() + " " + p.debugString());
    }
    registerConjunct(p);
}
#end_block

#method_before
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds, boolean inclOjConjuncts) {
    LOG.trace("getUnassignedConjuncts for " + Id.printIds(tupleIds));
    List<Expr> result = Lists.newArrayList();
    for (Expr e : globalState_.conjuncts.values()) {
        if (e.isBoundByTupleIds(tupleIds) && !e.isAuxExpr() && !globalState_.assignedConjuncts.contains(e.getId()) && ((inclOjConjuncts && !e.isConstant()) || !globalState_.ojClauseByConjunct.containsKey(e.getId()))) {
            result.add(e);
            LOG.trace("getUnassignedConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds, boolean inclOjConjuncts) {
    List<Expr> result = Lists.newArrayList();
    for (Expr e : globalState_.conjuncts.values()) {
        if (e.isBoundByTupleIds(tupleIds) && !e.isAuxExpr() && !globalState_.assignedConjuncts.contains(e.getId()) && ((inclOjConjuncts && !e.isConstant()) || !globalState_.ojClauseByConjunct.containsKey(e.getId()))) {
            result.add(e);
        }
    }
    return result;
}
#end_block

#method_before
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds) {
    LOG.trace("getUnassignedConjuncts for node with " + Id.printIds(tupleIds));
    List<Expr> result = Lists.newArrayList();
    for (Expr e : getUnassignedConjuncts(tupleIds, true)) {
        if (canEvalPredicate(tupleIds, e)) {
            result.add(e);
            LOG.trace("getUnassignedConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds) {
    List<Expr> result = Lists.newArrayList();
    for (Expr e : getUnassignedConjuncts(tupleIds, true)) {
        if (canEvalPredicate(tupleIds, e))
            result.add(e);
    }
    return result;
}
#end_block

#method_before
public List<Expr> getUnassignedOjConjuncts(TableRef ref) {
    Preconditions.checkState(ref.getJoinOp().isOuterJoin());
    List<Expr> result = Lists.newArrayList();
    List<ExprId> candidates = globalState_.conjunctsByOjClause.get(ref.getId());
    if (candidates == null)
        return result;
    for (ExprId conjunctId : candidates) {
        if (!globalState_.assignedConjuncts.contains(conjunctId)) {
            Expr e = globalState_.conjuncts.get(conjunctId);
            Preconditions.checkNotNull(e);
            result.add(e);
            LOG.trace("getUnassignedOjConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedOjConjuncts(TableRef ref) {
    Preconditions.checkState(ref.getJoinOp().isOuterJoin());
    List<Expr> result = Lists.newArrayList();
    List<ExprId> candidates = globalState_.conjunctsByOjClause.get(ref.getId());
    if (candidates == null)
        return result;
    for (ExprId conjunctId : candidates) {
        if (!globalState_.assignedConjuncts.contains(conjunctId)) {
            Expr e = globalState_.conjuncts.get(conjunctId);
            Preconditions.checkNotNull(e);
            result.add(e);
        }
    }
    return result;
}
#end_block

#method_before
public List<Expr> getEqJoinConjuncts(List<TupleId> lhsTblRefIds, List<TupleId> rhsTblRefIds) {
    // Contains all equi-join conjuncts that have one child fully bound by one of the
    // rhs table ref ids (the other child is not bound by that rhs table ref id).
    List<ExprId> conjunctIds = Lists.newArrayList();
    for (TupleId rhsId : rhsTblRefIds) {
        List<ExprId> cids = globalState_.eqJoinConjuncts.get(rhsId);
        if (cids == null)
            continue;
        for (ExprId eid : cids) {
            if (!conjunctIds.contains(eid))
                conjunctIds.add(eid);
        }
    }
    // Since we currently prevent join re-reordering across outer joins, we can never
    // have a bushy outer join with multiple rhs table ref ids. A busy outer join can
    // only be constructed with an inline view (which has a single table ref id).
    List<ExprId> ojClauseConjuncts = null;
    if (rhsTblRefIds.size() == 1) {
        ojClauseConjuncts = globalState_.conjunctsByOjClause.get(rhsTblRefIds.get(0));
    }
    // List of table ref ids that the join node will 'materialize'.
    List<TupleId> nodeTblRefIds = Lists.newArrayList(lhsTblRefIds);
    nodeTblRefIds.addAll(rhsTblRefIds);
    List<Expr> result = Lists.newArrayList();
    for (ExprId conjunctId : conjunctIds) {
        Expr e = globalState_.conjuncts.get(conjunctId);
        Preconditions.checkState(e != null);
        if (!canEvalFullOuterJoinedConjunct(e, nodeTblRefIds) || !canEvalAntiJoinedConjunct(e, nodeTblRefIds)) {
            continue;
        }
        if (ojClauseConjuncts != null && !ojClauseConjuncts.contains(conjunctId))
            continue;
        result.add(e);
    }
    return result;
}
#method_after
public List<Expr> getEqJoinConjuncts(List<TupleId> lhsTblRefIds, List<TupleId> rhsTblRefIds) {
    // Contains all equi-join conjuncts that have one child fully bound by one of the
    // rhs table ref ids (the other child is not bound by that rhs table ref id).
    List<ExprId> conjunctIds = Lists.newArrayList();
    for (TupleId rhsId : rhsTblRefIds) {
        List<ExprId> cids = globalState_.eqJoinConjuncts.get(rhsId);
        if (cids == null)
            continue;
        for (ExprId eid : cids) {
            if (!conjunctIds.contains(eid))
                conjunctIds.add(eid);
        }
    }
    // Since we currently prevent join re-reordering across outer joins, we can never
    // have a bushy outer join with multiple rhs table ref ids. A busy outer join can
    // only be constructed with an inline view (which has a single table ref id).
    List<ExprId> ojClauseConjuncts = null;
    if (rhsTblRefIds.size() == 1) {
        ojClauseConjuncts = globalState_.conjunctsByOjClause.get(rhsTblRefIds.get(0));
    }
    // List of table ref ids that the join node will 'materialize'.
    List<TupleId> nodeTblRefIds = Lists.newArrayList(lhsTblRefIds);
    nodeTblRefIds.addAll(rhsTblRefIds);
    List<Expr> result = Lists.newArrayList();
    for (ExprId conjunctId : conjunctIds) {
        Expr e = globalState_.conjuncts.get(conjunctId);
        Preconditions.checkState(e != null);
        if (!canEvalFullOuterJoinedConjunct(e, nodeTblRefIds) || !canEvalAntiJoinedConjunct(e, nodeTblRefIds) || !canEvalOuterJoinedConjunct(e, nodeTblRefIds)) {
            continue;
        }
        if (ojClauseConjuncts != null && !ojClauseConjuncts.contains(conjunctId))
            continue;
        result.add(e);
    }
    return result;
}
#end_block

#method_before
public boolean canEvalPredicate(List<TupleId> tupleIds, Expr e) {
    if (!e.isBoundByTupleIds(tupleIds))
        return false;
    ArrayList<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    if (tids.isEmpty())
        return true;
    if (e.isOnClauseConjunct()) {
        if (isAntiJoinedConjunct(e))
            return canEvalAntiJoinedConjunct(e, tupleIds);
        if (isFullOuterJoined(e))
            return canEvalFullOuterJoinedConjunct(e, tupleIds);
        if (isOjConjunct(e)) {
            // The join node will pick up the predicate later via getUnassignedOjConjuncts().
            if (tids.size() > 1)
                return false;
            // Optimization for single-tid predicates: Legal to assign below the outer join
            // if the predicate is from the same On-clause that makes tid nullable
            // (otherwise e needn't be true when that tuple is set).
            TupleId tid = tids.get(0);
            return globalState_.ojClauseByConjunct.get(e.getId()) == getLastOjClause(tid);
        }
        if (isIjConjunct(e) || isSjConjunct(e)) {
            if (!containsOuterJoinedTid(tids))
                return true;
            // If the predicate references an outer-joined tuple, then conservatively
            // evaluate it the join that the On-clause belongs to.
            TableRef onClauseTableRef = null;
            if (isIjConjunct(e)) {
                onClauseTableRef = globalState_.ijClauseByConjunct.get(e.getId());
            } else {
                onClauseTableRef = globalState_.sjClauseByConjunct.get(e.getId());
            }
            Preconditions.checkNotNull(onClauseTableRef);
            return tupleIds.containsAll(onClauseTableRef.getAllTableRefIds());
        }
        // Should have returned in one of the cases above.
        Preconditions.checkState(false);
    }
    for (TupleId tid : tids) {
        TableRef rhsRef = getLastOjClause(tid);
        // Ignore 'tid' because it is not outer-joined.
        if (rhsRef == null)
            continue;
        // Check whether the last join to outer-join 'tid' is materialized by tupleIds.
        if (!tupleIds.containsAll(rhsRef.getAllTableRefIds()))
            return false;
    }
    return true;
}
#method_after
public boolean canEvalPredicate(List<TupleId> tupleIds, Expr e) {
    if (!e.isBoundByTupleIds(tupleIds))
        return false;
    ArrayList<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    if (tids.isEmpty())
        return true;
    if (e.isOnClauseConjunct()) {
        if (isAntiJoinedConjunct(e))
            return canEvalAntiJoinedConjunct(e, tupleIds);
        if (isIjConjunct(e) || isSjConjunct(e)) {
            if (!containsOuterJoinedTid(tids))
                return true;
            // If the predicate references an outer-joined tuple, then evaluate it at
            // the join that the On-clause belongs to.
            TableRef onClauseTableRef = null;
            if (isIjConjunct(e)) {
                onClauseTableRef = globalState_.ijClauseByConjunct.get(e.getId());
            } else {
                onClauseTableRef = globalState_.sjClauseByConjunct.get(e.getId());
            }
            Preconditions.checkNotNull(onClauseTableRef);
            return tupleIds.containsAll(onClauseTableRef.getAllTableRefIds());
        }
        if (isFullOuterJoined(e))
            return canEvalFullOuterJoinedConjunct(e, tupleIds);
        if (isOjConjunct(e)) {
            // The join node will pick up the predicate later via getUnassignedOjConjuncts().
            if (tids.size() > 1)
                return false;
            // Optimization for single-tid predicates: Legal to assign below the outer join
            // if the predicate is from the same On-clause that makes tid nullable
            // (otherwise e needn't be true when that tuple is set).
            TupleId tid = tids.get(0);
            return globalState_.ojClauseByConjunct.get(e.getId()) == getLastOjClause(tid);
        }
        // Should have returned in one of the cases above.
        Preconditions.checkState(false);
    }
    for (TupleId tid : tids) {
        TableRef rhsRef = getLastOjClause(tid);
        // Ignore 'tid' because it is not outer-joined.
        if (rhsRef == null)
            continue;
        // Check whether the last join to outer-join 'tid' is materialized by tupleIds.
        if (!tupleIds.containsAll(rhsRef.getAllTableRefIds()))
            return false;
    }
    return true;
}
#end_block

#method_before
public ArrayList<Expr> getBoundPredicates(TupleId destTid, Set<SlotId> ignoreSlots, boolean markAssigned) {
    ArrayList<Expr> result = Lists.newArrayList();
    for (ExprId srcConjunctId : globalState_.singleTidConjuncts) {
        Expr srcConjunct = globalState_.conjuncts.get(srcConjunctId);
        if (srcConjunct instanceof SlotRef)
            continue;
        Preconditions.checkNotNull(srcConjunct);
        List<TupleId> srcTids = Lists.newArrayList();
        List<SlotId> srcSids = Lists.newArrayList();
        srcConjunct.getIds(srcTids, srcSids);
        Preconditions.checkState(srcTids.size() == 1);
        // Generate slot-mappings to bind srcConjunct to destTid.
        TupleId srcTid = srcTids.get(0);
        List<List<SlotId>> allDestSids = getEquivDestSlotIds(srcTid, srcSids, destTid, ignoreSlots);
        if (allDestSids.isEmpty())
            continue;
        // Indicates whether the source slots have equivalent slots that belong
        // to an outer-joined tuple.
        boolean hasOuterJoinedTuple = false;
        for (SlotId srcSid : srcSids) {
            if (hasOuterJoinedTuple(globalState_.equivClassBySlotId.get(srcSid))) {
                hasOuterJoinedTuple = true;
                break;
            }
        }
        // relative to 'srcConjunct'.
        if (hasOuterJoinedTuple && isTrueWithNullSlots(srcConjunct))
            continue;
        // (otherwise srcConjunct needn't be true when destTid is set)
        if (globalState_.ojClauseByConjunct.containsKey(srcConjunct.getId())) {
            if (!globalState_.outerJoinedTupleIds.containsKey(destTid))
                continue;
            if (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(destTid)) {
                continue;
            }
            // Do not propagate conjuncts from the on-clause of full-outer or anti-joins.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(srcConjunct.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                continue;
        }
        // join node.
        if (isAntiJoinedConjunct(srcConjunct))
            continue;
        // Generate predicates for all src-to-dest slot mappings.
        for (List<SlotId> destSids : allDestSids) {
            Preconditions.checkState(destSids.size() == srcSids.size());
            Expr p;
            if (srcSids.containsAll(destSids)) {
                p = srcConjunct;
            } else {
                ExprSubstitutionMap smap = new ExprSubstitutionMap();
                for (int i = 0; i < srcSids.size(); ++i) {
                    smap.put(new SlotRef(globalState_.descTbl.getSlotDesc(srcSids.get(i))), new SlotRef(globalState_.descTbl.getSlotDesc(destSids.get(i))));
                }
                try {
                    p = srcConjunct.trySubstitute(smap, this, false);
                } catch (ImpalaException exc) {
                    // not an executable predicate; ignore
                    continue;
                }
                // Unset the id because this bound predicate itself is not registered, and
                // to prevent callers from inadvertently marking the srcConjunct as assigned.
                p.setId(null);
                if (p instanceof BinaryPredicate)
                    ((BinaryPredicate) p).setIsInferred();
                LOG.trace("new pred: " + p.toSql() + " " + p.debugString());
            }
            if (markAssigned) {
                // predicate assignment doesn't hold if:
                // - the application against slotId doesn't transfer the value back to its
                // originating slot
                // - the original predicate is on an OJ'd table but doesn't originate from
                // that table's OJ clause's ON clause (if it comes from anywhere but that
                // ON clause, it needs to be evaluated directly by the join node that
                // materializes the OJ'd table)
                boolean reverseValueTransfer = true;
                for (int i = 0; i < srcSids.size(); ++i) {
                    if (!hasValueTransfer(destSids.get(i), srcSids.get(i))) {
                        reverseValueTransfer = false;
                        break;
                    }
                }
                // Check if either srcConjunct or the generated predicate needs to be evaluated
                // at a join node (IMPALA-2018).
                boolean evalByJoin = (evalByJoin(srcConjunct) && (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(srcTid))) || (evalByJoin(p) && (globalState_.ojClauseByConjunct.get(p.getId()) != globalState_.outerJoinedTupleIds.get(destTid)));
                // mark all bound predicates including duplicate ones
                if (reverseValueTransfer && !evalByJoin)
                    markConjunctAssigned(srcConjunct);
            }
            // check if we already created this predicate
            if (!result.contains(p))
                result.add(p);
        }
    }
    return result;
}
#method_after
public ArrayList<Expr> getBoundPredicates(TupleId destTid, Set<SlotId> ignoreSlots, boolean markAssigned) {
    ArrayList<Expr> result = Lists.newArrayList();
    for (ExprId srcConjunctId : globalState_.singleTidConjuncts) {
        Expr srcConjunct = globalState_.conjuncts.get(srcConjunctId);
        if (srcConjunct instanceof SlotRef)
            continue;
        Preconditions.checkNotNull(srcConjunct);
        List<TupleId> srcTids = Lists.newArrayList();
        List<SlotId> srcSids = Lists.newArrayList();
        srcConjunct.getIds(srcTids, srcSids);
        Preconditions.checkState(srcTids.size() == 1);
        // Generate slot-mappings to bind srcConjunct to destTid.
        TupleId srcTid = srcTids.get(0);
        List<List<SlotId>> allDestSids = getEquivDestSlotIds(srcTid, srcSids, destTid, ignoreSlots);
        if (allDestSids.isEmpty())
            continue;
        // Indicates whether the source slots have equivalent slots that belong
        // to an outer-joined tuple.
        boolean hasOuterJoinedTuple = false;
        for (SlotId srcSid : srcSids) {
            if (hasOuterJoinedTuple(globalState_.equivClassBySlotId.get(srcSid))) {
                hasOuterJoinedTuple = true;
                break;
            }
        }
        // relative to 'srcConjunct'.
        if (hasOuterJoinedTuple && isTrueWithNullSlots(srcConjunct))
            continue;
        // (otherwise srcConjunct needn't be true when destTid is set)
        if (globalState_.ojClauseByConjunct.containsKey(srcConjunct.getId())) {
            if (!globalState_.outerJoinedTupleIds.containsKey(destTid))
                continue;
            if (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(destTid)) {
                continue;
            }
            // Do not propagate conjuncts from the on-clause of full-outer or anti-joins.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(srcConjunct.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                continue;
        }
        // join node.
        if (isAntiJoinedConjunct(srcConjunct))
            continue;
        // Generate predicates for all src-to-dest slot mappings.
        for (List<SlotId> destSids : allDestSids) {
            Preconditions.checkState(destSids.size() == srcSids.size());
            Expr p;
            if (srcSids.containsAll(destSids)) {
                p = srcConjunct;
            } else {
                ExprSubstitutionMap smap = new ExprSubstitutionMap();
                for (int i = 0; i < srcSids.size(); ++i) {
                    smap.put(new SlotRef(globalState_.descTbl.getSlotDesc(srcSids.get(i))), new SlotRef(globalState_.descTbl.getSlotDesc(destSids.get(i))));
                }
                try {
                    p = srcConjunct.trySubstitute(smap, this, false);
                } catch (ImpalaException exc) {
                    // not an executable predicate; ignore
                    continue;
                }
                // Unset the id because this bound predicate itself is not registered, and
                // to prevent callers from inadvertently marking the srcConjunct as assigned.
                p.setId(null);
                if (p instanceof BinaryPredicate)
                    ((BinaryPredicate) p).setIsInferred();
                if (LOG.isTraceEnabled()) {
                    LOG.trace("new pred: " + p.toSql() + " " + p.debugString());
                }
            }
            if (markAssigned) {
                // predicate assignment doesn't hold if:
                // - the application against slotId doesn't transfer the value back to its
                // originating slot
                // - the original predicate is on an OJ'd table but doesn't originate from
                // that table's OJ clause's ON clause (if it comes from anywhere but that
                // ON clause, it needs to be evaluated directly by the join node that
                // materializes the OJ'd table)
                boolean reverseValueTransfer = true;
                for (int i = 0; i < srcSids.size(); ++i) {
                    if (!hasValueTransfer(destSids.get(i), srcSids.get(i))) {
                        reverseValueTransfer = false;
                        break;
                    }
                }
                // IMPALA-2018/4379: Check if srcConjunct or the generated predicate need to
                // be evaluated again at a later point in the plan, e.g., by a join that makes
                // referenced tuples nullable. The first condition is conservative but takes
                // into account that On-clause conjuncts can sometimes be legitimately assigned
                // below their originating join.
                boolean evalAfterJoin = (hasOuterJoinedTuple && !srcConjunct.isOnClauseConjunct_) || (evalAfterJoin(srcConjunct) && (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(srcTid))) || (evalAfterJoin(p) && (globalState_.ojClauseByConjunct.get(p.getId()) != globalState_.outerJoinedTupleIds.get(destTid)));
                // mark all bound predicates including duplicate ones
                if (reverseValueTransfer && !evalAfterJoin)
                    markConjunctAssigned(srcConjunct);
            }
            // check if we already created this predicate
            if (!result.contains(p))
                result.add(p);
        }
    }
    return result;
}
#end_block

#method_before
public List<SlotId> getEquivSlots(SlotId slotId, List<TupleId> tupleIds) {
    List<SlotId> result = Lists.newArrayList();
    LOG.trace("getequivslots: slotid=" + Integer.toString(slotId.asInt()));
    EquivalenceClassId classId = globalState_.equivClassBySlotId.get(slotId);
    for (SlotId memberId : globalState_.equivClassMembers.get(classId)) {
        if (tupleIds.contains(globalState_.descTbl.getSlotDesc(memberId).getParent().getId())) {
            result.add(memberId);
        }
    }
    return result;
}
#method_after
public List<SlotId> getEquivSlots(SlotId slotId, List<TupleId> tupleIds) {
    List<SlotId> result = Lists.newArrayList();
    EquivalenceClassId classId = globalState_.equivClassBySlotId.get(slotId);
    for (SlotId memberId : globalState_.equivClassMembers.get(classId)) {
        if (tupleIds.contains(globalState_.descTbl.getSlotDesc(memberId).getParent().getId())) {
            result.add(memberId);
        }
    }
    return result;
}
#end_block

#method_before
public void markConjunctsAssigned(List<Expr> conjuncts) {
    if (conjuncts == null)
        return;
    for (Expr p : conjuncts) {
        globalState_.assignedConjuncts.add(p.getId());
        LOG.trace("markAssigned " + p.toSql() + " " + p.debugString());
    }
}
#method_after
public void markConjunctsAssigned(List<Expr> conjuncts) {
    if (conjuncts == null)
        return;
    for (Expr p : conjuncts) {
        globalState_.assignedConjuncts.add(p.getId());
    }
}
#end_block

#method_before
public void markConjunctAssigned(Expr conjunct) {
    LOG.trace("markAssigned " + conjunct.toSql() + " " + conjunct.debugString());
    globalState_.assignedConjuncts.add(conjunct.getId());
}
#method_after
public void markConjunctAssigned(Expr conjunct) {
    globalState_.assignedConjuncts.add(conjunct.getId());
}
#end_block

#method_before
public boolean hasUnassignedConjuncts() {
    for (ExprId id : globalState_.conjuncts.keySet()) {
        if (globalState_.assignedConjuncts.contains(id))
            continue;
        Expr e = globalState_.conjuncts.get(id);
        if (e.isAuxExpr())
            continue;
        LOG.trace("unassigned: " + e.toSql() + " " + e.debugString());
        return true;
    }
    return false;
}
#method_after
public boolean hasUnassignedConjuncts() {
    for (ExprId id : globalState_.conjuncts.keySet()) {
        if (globalState_.assignedConjuncts.contains(id))
            continue;
        Expr e = globalState_.conjuncts.get(id);
        if (e.isAuxExpr())
            continue;
        return true;
    }
    return false;
}
#end_block

#method_before
public Table getTable(TableName tableName, Privilege privilege, boolean addAccessEvent) throws AnalysisException {
    Preconditions.checkNotNull(tableName);
    Preconditions.checkNotNull(privilege);
    Table table = null;
    tableName = new TableName(getTargetDbName(tableName), tableName.getTbl());
    if (privilege == Privilege.ANY) {
        registerPrivReq(new PrivilegeRequestBuilder().any().onAnyColumn(tableName.getDb(), tableName.getTbl()).toRequest());
    } else {
        registerPrivReq(new PrivilegeRequestBuilder().allOf(privilege).onTable(tableName.getDb(), tableName.getTbl()).toRequest());
    }
    // AnalysisExceptions.
    try {
        table = getTable(tableName.getDb(), tableName.getTbl());
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    Preconditions.checkNotNull(table);
    if (addAccessEvent) {
        // Add an audit event for this access
        TCatalogObjectType objectType = TCatalogObjectType.TABLE;
        if (table instanceof View)
            objectType = TCatalogObjectType.VIEW;
        globalState_.accessEvents.add(new TAccessEvent(tableName.toString(), objectType, privilege.toString()));
    }
    return table;
}
#method_after
public Table getTable(TableName tableName, Privilege privilege, boolean addAccessEvent) throws AnalysisException, TableLoadingException {
    Preconditions.checkNotNull(tableName);
    Preconditions.checkNotNull(privilege);
    Table table = null;
    tableName = new TableName(getTargetDbName(tableName), tableName.getTbl());
    if (privilege == Privilege.ANY) {
        registerPrivReq(new PrivilegeRequestBuilder().any().onAnyColumn(tableName.getDb(), tableName.getTbl()).toRequest());
    } else {
        registerPrivReq(new PrivilegeRequestBuilder().allOf(privilege).onTable(tableName.getDb(), tableName.getTbl()).toRequest());
    }
    table = getTable(tableName.getDb(), tableName.getTbl());
    Preconditions.checkNotNull(table);
    if (addAccessEvent) {
        // Add an audit event for this access
        TCatalogObjectType objectType = TCatalogObjectType.TABLE;
        if (table instanceof View)
            objectType = TCatalogObjectType.VIEW;
        globalState_.accessEvents.add(new TAccessEvent(tableName.toString(), objectType, privilege.toString()));
    }
    return table;
}
#end_block

#method_before
public Table getTable(TableName tableName, Privilege privilege) throws AnalysisException {
    return getTable(tableName, privilege, true);
}
#method_after
public Table getTable(TableName tableName, Privilege privilege) throws AnalysisException {
    // AnalysisExceptions.
    try {
        return getTable(tableName, privilege, true);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
}
#end_block

#method_before
public void registerAuthAndAuditEvent(Table table, Analyzer analyzer) {
    // Add access event for auditing.
    if (table instanceof View) {
        View view = (View) table;
        Preconditions.checkState(!view.isLocalView());
        analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, Privilege.SELECT.toString()));
    } else {
        analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, Privilege.SELECT.toString()));
    }
    // Add privilege request.
    TableName tableName = table.getTableName();
    analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(Privilege.SELECT).toRequest());
}
#method_after
public void registerAuthAndAuditEvent(Table table, Privilege priv) {
    // Add access event for auditing.
    if (table instanceof View) {
        View view = (View) table;
        Preconditions.checkState(!view.isLocalView());
        addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, priv.toString()));
    } else {
        addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, priv.toString()));
    }
    // Add privilege request.
    TableName tableName = table.getTableName();
    registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(priv).toRequest());
}
#end_block

#method_before
public void computeValueTransfers() {
    long start = System.currentTimeMillis();
    // Step1: Compute complete subgraphs and get uni-directional value transfers.
    List<Pair<SlotId, SlotId>> origValueTransfers = Lists.newArrayList();
    partitionValueTransfers(completeSubGraphs_, origValueTransfers);
    // Coalesce complete subgraphs into a single slot and assign new slot ids.
    coalescedSlots_ = new int[numSlots_];
    Arrays.fill(coalescedSlots_, -1);
    for (Set<SlotId> equivClass : completeSubGraphs_.getSets()) {
        int representative = nextCoalescedSlotId_;
        for (SlotId slotId : equivClass) {
            coalescedSlots_[slotId.asInt()] = representative;
        }
        ++nextCoalescedSlotId_;
    }
    // Step 2: Map uni-directional value transfers onto the new slot domain, and
    // store the connected components in graphPartitions.
    List<Pair<Integer, Integer>> coalescedValueTransfers = Lists.newArrayList();
    // A graph partition is a set of slot ids that are connected by uni-directional
    // value transfers. The graph corresponding to a graph partition is a DAG.
    DisjointSet<Integer> graphPartitions = new DisjointSet<Integer>();
    mapSlots(origValueTransfers, coalescedValueTransfers, graphPartitions);
    mapSlots(globalState_.registeredValueTransfers, coalescedValueTransfers, graphPartitions);
    // Step 3: Group the coalesced value transfers by the graph partition they
    // belong to. Maps from the graph partition to its list of value transfers.
    // TODO: Implement a specialized DisjointSet data structure to avoid this step.
    Map<Set<Integer>, List<Pair<Integer, Integer>>> partitionedValueTransfers = Maps.newHashMap();
    for (Pair<Integer, Integer> vt : coalescedValueTransfers) {
        Set<Integer> partition = graphPartitions.get(vt.first.intValue());
        List<Pair<Integer, Integer>> l = partitionedValueTransfers.get(partition);
        if (l == null) {
            l = Lists.newArrayList();
            partitionedValueTransfers.put(partition, l);
        }
        l.add(vt);
    }
    // Initialize the value transfer graph.
    int numCoalescedSlots = nextCoalescedSlotId_ + 1;
    valueTransfer_ = new boolean[numCoalescedSlots][numCoalescedSlots];
    for (int i = 0; i < numCoalescedSlots; ++i) {
        valueTransfer_[i][i] = true;
    }
    // Step 4: Compute the transitive closure for each graph partition.
    for (Map.Entry<Set<Integer>, List<Pair<Integer, Integer>>> graphPartition : partitionedValueTransfers.entrySet()) {
        // Set value transfers of this partition.
        for (Pair<Integer, Integer> vt : graphPartition.getValue()) {
            valueTransfer_[vt.first][vt.second] = true;
        }
        Set<Integer> partitionSlotIds = graphPartition.getKey();
        // No transitive value transfers.
        if (partitionSlotIds.size() <= 2)
            continue;
        // Indirection vector into valueTransfer_. Contains one entry for each distinct
        // slot id referenced in a value transfer of this partition.
        int[] p = new int[partitionSlotIds.size()];
        int numPartitionSlots = 0;
        for (Integer slotId : partitionSlotIds) {
            p[numPartitionSlots++] = slotId;
        }
        // Compute the transitive closure of this graph partition.
        // TODO: Since we are operating on a DAG the performance can be improved if
        // necessary (e.g., topological sort + backwards propagation of the transitive
        // closure).
        boolean changed = false;
        do {
            changed = false;
            for (int i = 0; i < numPartitionSlots; ++i) {
                for (int j = 0; j < numPartitionSlots; ++j) {
                    for (int k = 0; k < numPartitionSlots; ++k) {
                        if (valueTransfer_[p[i]][p[j]] && valueTransfer_[p[j]][p[k]] && !valueTransfer_[p[i]][p[k]]) {
                            valueTransfer_[p[i]][p[k]] = true;
                            changed = true;
                        }
                    }
                }
            }
        } while (changed);
    }
    long end = System.currentTimeMillis();
    LOG.trace("Time taken in computeValueTransfers(): " + (end - start) + "ms");
}
#method_after
public void computeValueTransfers() {
    long start = System.currentTimeMillis();
    // Step1: Compute complete subgraphs and get uni-directional value transfers.
    List<Pair<SlotId, SlotId>> origValueTransfers = Lists.newArrayList();
    partitionValueTransfers(completeSubGraphs_, origValueTransfers);
    // Coalesce complete subgraphs into a single slot and assign new slot ids.
    coalescedSlots_ = new int[numSlots_];
    Arrays.fill(coalescedSlots_, -1);
    for (Set<SlotId> equivClass : completeSubGraphs_.getSets()) {
        int representative = nextCoalescedSlotId_;
        for (SlotId slotId : equivClass) {
            coalescedSlots_[slotId.asInt()] = representative;
        }
        ++nextCoalescedSlotId_;
    }
    // Step 2: Map uni-directional value transfers onto the new slot domain, and
    // store the connected components in graphPartitions.
    List<Pair<Integer, Integer>> coalescedValueTransfers = Lists.newArrayList();
    // A graph partition is a set of slot ids that are connected by uni-directional
    // value transfers. The graph corresponding to a graph partition is a DAG.
    DisjointSet<Integer> graphPartitions = new DisjointSet<Integer>();
    mapSlots(origValueTransfers, coalescedValueTransfers, graphPartitions);
    mapSlots(globalState_.registeredValueTransfers, coalescedValueTransfers, graphPartitions);
    // Step 3: Group the coalesced value transfers by the graph partition they
    // belong to. Maps from the graph partition to its list of value transfers.
    // TODO: Implement a specialized DisjointSet data structure to avoid this step.
    Map<Set<Integer>, List<Pair<Integer, Integer>>> partitionedValueTransfers = Maps.newHashMap();
    for (Pair<Integer, Integer> vt : coalescedValueTransfers) {
        Set<Integer> partition = graphPartitions.get(vt.first.intValue());
        List<Pair<Integer, Integer>> l = partitionedValueTransfers.get(partition);
        if (l == null) {
            l = Lists.newArrayList();
            partitionedValueTransfers.put(partition, l);
        }
        l.add(vt);
    }
    // Initialize the value transfer graph.
    int numCoalescedSlots = nextCoalescedSlotId_ + 1;
    valueTransfer_ = new boolean[numCoalescedSlots][numCoalescedSlots];
    for (int i = 0; i < numCoalescedSlots; ++i) {
        valueTransfer_[i][i] = true;
    }
    // Step 4: Compute the transitive closure for each graph partition.
    for (Map.Entry<Set<Integer>, List<Pair<Integer, Integer>>> graphPartition : partitionedValueTransfers.entrySet()) {
        // Set value transfers of this partition.
        for (Pair<Integer, Integer> vt : graphPartition.getValue()) {
            valueTransfer_[vt.first][vt.second] = true;
        }
        Set<Integer> partitionSlotIds = graphPartition.getKey();
        // No transitive value transfers.
        if (partitionSlotIds.size() <= 2)
            continue;
        // Indirection vector into valueTransfer_. Contains one entry for each distinct
        // slot id referenced in a value transfer of this partition.
        int[] p = new int[partitionSlotIds.size()];
        int numPartitionSlots = 0;
        for (Integer slotId : partitionSlotIds) {
            p[numPartitionSlots++] = slotId;
        }
        // Compute the transitive closure of this graph partition.
        // TODO: Since we are operating on a DAG the performance can be improved if
        // necessary (e.g., topological sort + backwards propagation of the transitive
        // closure).
        boolean changed = false;
        do {
            changed = false;
            for (int i = 0; i < numPartitionSlots; ++i) {
                for (int j = 0; j < numPartitionSlots; ++j) {
                    for (int k = 0; k < numPartitionSlots; ++k) {
                        if (valueTransfer_[p[i]][p[j]] && valueTransfer_[p[j]][p[k]] && !valueTransfer_[p[i]][p[k]]) {
                            valueTransfer_[p[i]][p[k]] = true;
                            changed = true;
                        }
                    }
                }
            }
        } while (changed);
    }
    long end = System.currentTimeMillis();
    if (LOG.isDebugEnabled()) {
        LOG.trace("Time taken in computeValueTransfers(): " + (end - start) + "ms");
    }
}
#end_block

#method_before
private void partitionValueTransfers(DisjointSet<SlotId> completeSubGraphs, List<Pair<SlotId, SlotId>> valueTransfers) {
    // transform equality predicates into a transfer graph
    for (ExprId id : globalState_.conjuncts.keySet()) {
        Expr e = globalState_.conjuncts.get(id);
        Pair<SlotId, SlotId> slotIds = BinaryPredicate.getEqSlots(e);
        if (slotIds == null)
            continue;
        boolean isAntiJoin = false;
        TableRef sjTblRef = globalState_.sjClauseByConjunct.get(id);
        Preconditions.checkState(sjTblRef == null || sjTblRef.getJoinOp().isSemiJoin());
        isAntiJoin = sjTblRef != null && sjTblRef.getJoinOp().isAntiJoin();
        TableRef ojTblRef = globalState_.ojClauseByConjunct.get(id);
        Preconditions.checkState(ojTblRef == null || ojTblRef.getJoinOp().isOuterJoin());
        if (ojTblRef == null && !isAntiJoin) {
            // this eq predicate doesn't involve any outer or anti join, ie, it is true for
            // each result row;
            // value transfer is not legal if the receiving slot is in an enclosed
            // scope of the source slot and the receiving slot's block has a limit
            Analyzer firstBlock = globalState_.blockBySlot.get(slotIds.first);
            Analyzer secondBlock = globalState_.blockBySlot.get(slotIds.second);
            LOG.trace("value transfer: from " + slotIds.first.toString());
            Pair<SlotId, SlotId> firstToSecond = null;
            Pair<SlotId, SlotId> secondToFirst = null;
            if (!(secondBlock.hasLimitOffsetClause_ && secondBlock.ancestors_.contains(firstBlock))) {
                firstToSecond = new Pair<SlotId, SlotId>(slotIds.first, slotIds.second);
            }
            if (!(firstBlock.hasLimitOffsetClause_ && firstBlock.ancestors_.contains(secondBlock))) {
                secondToFirst = new Pair<SlotId, SlotId>(slotIds.second, slotIds.first);
            }
            // uni-directional value transfers to valueTransfers.
            if (firstToSecond != null && secondToFirst != null && completeSubGraphs != null) {
                completeSubGraphs.union(slotIds.first, slotIds.second);
            } else {
                if (firstToSecond != null)
                    valueTransfers.add(firstToSecond);
                if (secondToFirst != null)
                    valueTransfers.add(secondToFirst);
            }
            continue;
        }
        // Outer or semi-joined table ref.
        TableRef tblRef = (ojTblRef != null) ? ojTblRef : sjTblRef;
        Preconditions.checkNotNull(tblRef);
        if (tblRef.getJoinOp() == JoinOperator.FULL_OUTER_JOIN) {
            // full outer joins don't guarantee any value transfer
            continue;
        }
        // this is some form of outer or anti join
        SlotId outerSlot, innerSlot;
        if (tblRef.getId() == getTupleId(slotIds.first)) {
            innerSlot = slotIds.first;
            outerSlot = slotIds.second;
        } else if (tblRef.getId() == getTupleId(slotIds.second)) {
            innerSlot = slotIds.second;
            outerSlot = slotIds.first;
        } else {
            // actually be true
            continue;
        }
        // inverting the condition (paying special attention to NULLs).
        if (tblRef.getJoinOp() == JoinOperator.LEFT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.LEFT_ANTI_JOIN || tblRef.getJoinOp() == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(outerSlot, innerSlot));
        } else if (tblRef.getJoinOp() == JoinOperator.RIGHT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.RIGHT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(innerSlot, outerSlot));
        }
    }
}
#method_after
private void partitionValueTransfers(DisjointSet<SlotId> completeSubGraphs, List<Pair<SlotId, SlotId>> valueTransfers) {
    // transform equality predicates into a transfer graph
    for (ExprId id : globalState_.conjuncts.keySet()) {
        Expr e = globalState_.conjuncts.get(id);
        Pair<SlotId, SlotId> slotIds = BinaryPredicate.getEqSlots(e);
        if (slotIds == null)
            continue;
        boolean isAntiJoin = false;
        TableRef sjTblRef = globalState_.sjClauseByConjunct.get(id);
        Preconditions.checkState(sjTblRef == null || sjTblRef.getJoinOp().isSemiJoin());
        isAntiJoin = sjTblRef != null && sjTblRef.getJoinOp().isAntiJoin();
        TableRef ojTblRef = globalState_.ojClauseByConjunct.get(id);
        Preconditions.checkState(ojTblRef == null || ojTblRef.getJoinOp().isOuterJoin());
        if (ojTblRef == null && !isAntiJoin) {
            // this eq predicate doesn't involve any outer or anti join, ie, it is true for
            // each result row;
            // value transfer is not legal if the receiving slot is in an enclosed
            // scope of the source slot and the receiving slot's block has a limit
            Analyzer firstBlock = globalState_.blockBySlot.get(slotIds.first);
            Analyzer secondBlock = globalState_.blockBySlot.get(slotIds.second);
            if (LOG.isTraceEnabled()) {
                LOG.trace("value transfer: from " + slotIds.first.toString());
            }
            Pair<SlotId, SlotId> firstToSecond = null;
            Pair<SlotId, SlotId> secondToFirst = null;
            if (!(secondBlock.hasLimitOffsetClause_ && secondBlock.ancestors_.contains(firstBlock))) {
                firstToSecond = new Pair<SlotId, SlotId>(slotIds.first, slotIds.second);
            }
            if (!(firstBlock.hasLimitOffsetClause_ && firstBlock.ancestors_.contains(secondBlock))) {
                secondToFirst = new Pair<SlotId, SlotId>(slotIds.second, slotIds.first);
            }
            // uni-directional value transfers to valueTransfers.
            if (firstToSecond != null && secondToFirst != null && completeSubGraphs != null) {
                completeSubGraphs.union(slotIds.first, slotIds.second);
            } else {
                if (firstToSecond != null)
                    valueTransfers.add(firstToSecond);
                if (secondToFirst != null)
                    valueTransfers.add(secondToFirst);
            }
            continue;
        }
        // Outer or semi-joined table ref.
        TableRef tblRef = (ojTblRef != null) ? ojTblRef : sjTblRef;
        Preconditions.checkNotNull(tblRef);
        if (tblRef.getJoinOp() == JoinOperator.FULL_OUTER_JOIN) {
            // full outer joins don't guarantee any value transfer
            continue;
        }
        // this is some form of outer or anti join
        SlotId outerSlot, innerSlot;
        if (tblRef.getId() == getTupleId(slotIds.first)) {
            innerSlot = slotIds.first;
            outerSlot = slotIds.second;
        } else if (tblRef.getId() == getTupleId(slotIds.second)) {
            innerSlot = slotIds.second;
            outerSlot = slotIds.first;
        } else {
            // actually be true
            continue;
        }
        // inverting the condition (paying special attention to NULLs).
        if (tblRef.getJoinOp() == JoinOperator.LEFT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.LEFT_ANTI_JOIN || tblRef.getJoinOp() == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(outerSlot, innerSlot));
        } else if (tblRef.getJoinOp() == JoinOperator.RIGHT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.RIGHT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(innerSlot, outerSlot));
        }
    }
}
#end_block

#method_before
void appendCellValueDebugString(Integer idx, StringBuilder sb) {
    ColumnSchema col = schema.getColumnByIndex(idx);
    if (columnsBitSet != null && !columnsBitSet.get(idx)) {
        sb.append("Column \'");
        sb.append(col.getName());
        sb.append("\' is not set");
        return;
    }
    if (nullsBitSet != null && nullsBitSet.get(idx)) {
        sb.append("NULL");
        return;
    }
    switch(col.getType()) {
        case BOOL:
            sb.append(Bytes.getBoolean(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT8:
            sb.append(Bytes.getByte(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT16:
            sb.append(Bytes.getShort(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT32:
            sb.append(Bytes.getInt(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT64:
            sb.append(Bytes.getLong(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case UNIXTIME_MICROS:
            sb.append(RowResult.timestampToString(Bytes.getLong(rowAlloc, schema.getColumnOffset(idx))));
            return;
        case FLOAT:
            sb.append(Bytes.getFloat(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case DOUBLE:
            sb.append(Bytes.getDouble(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case BINARY:
        case STRING:
            ByteBuffer value = getVarLengthData().get(idx).duplicate();
            // Make sure we start at the beginning.
            value.reset();
            byte[] data = new byte[value.limit()];
            value.get(data);
            if (col.getType() == Type.STRING) {
                sb.append('"');
                StringUtil.appendEscapedSQLString(Bytes.getString(data), sb);
                sb.append('"');
            } else {
                sb.append(Bytes.pretty(data));
            }
            return;
        default:
            throw new RuntimeException("unreachable");
    }
}
#method_after
void appendCellValueDebugString(Integer idx, StringBuilder sb) {
    ColumnSchema col = schema.getColumnByIndex(idx);
    Preconditions.checkState(columnsBitSet.get(idx), "Column %s is not set", col.getName());
    if (nullsBitSet != null && nullsBitSet.get(idx)) {
        sb.append("NULL");
        return;
    }
    switch(col.getType()) {
        case BOOL:
            sb.append(Bytes.getBoolean(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT8:
            sb.append(Bytes.getByte(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT16:
            sb.append(Bytes.getShort(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT32:
            sb.append(Bytes.getInt(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT64:
            sb.append(Bytes.getLong(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case UNIXTIME_MICROS:
            sb.append(RowResult.timestampToString(Bytes.getLong(rowAlloc, schema.getColumnOffset(idx))));
            return;
        case FLOAT:
            sb.append(Bytes.getFloat(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case DOUBLE:
            sb.append(Bytes.getDouble(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case BINARY:
        case STRING:
            ByteBuffer value = getVarLengthData().get(idx).duplicate();
            // Make sure we start at the beginning.
            value.reset();
            byte[] data = new byte[value.limit() - value.position()];
            value.get(data);
            if (col.getType() == Type.STRING) {
                sb.append('"');
                StringUtil.appendEscapedSQLString(Bytes.getString(data), sb);
                sb.append('"');
            } else {
                sb.append(Bytes.pretty(data));
            }
            return;
        default:
            throw new RuntimeException("unreachable");
    }
}
#end_block

#method_before
public AlterTableOptions addColumn(String name, Type type, Object defaultVal) {
    if (defaultVal == null) {
        throw new IllegalArgumentException("A new column must have a default value, " + "use addNullableColumn() to add a NULLABLE column");
    }
    AlterTableRequestPB.Step.Builder step = pb.addAlterSchemaStepsBuilder();
    step.setType(AlterTableRequestPB.StepType.ADD_COLUMN);
    step.setAddColumn(AlterTableRequestPB.AddColumn.newBuilder().setSchema(ProtobufHelper.columnToPb(new ColumnSchema.ColumnSchemaBuilder(name, type).defaultValue(defaultVal).build())));
    return this;
}
#method_after
public AlterTableOptions addColumn(ColumnSchema colSchema) {
    if (!colSchema.isNullable() && colSchema.getDefaultValue() == null) {
        throw new IllegalArgumentException("A new non-null column must have a default value");
    }
    if (colSchema.isKey()) {
        throw new IllegalArgumentException("Key columns cannot be added");
    }
    AlterTableRequestPB.Step.Builder step = pb.addAlterSchemaStepsBuilder();
    step.setType(AlterTableRequestPB.StepType.ADD_COLUMN);
    step.setAddColumn(AlterTableRequestPB.AddColumn.newBuilder().setSchema(ProtobufHelper.columnToPb(colSchema)));
    return this;
}
#end_block

#method_before
public AlterTableOptions addColumn(String name, Type type, Object defaultVal) {
    if (defaultVal == null) {
        throw new IllegalArgumentException("A new column must have a default value, " + "use addNullableColumn() to add a NULLABLE column");
    }
    AlterTableRequestPB.Step.Builder step = pb.addAlterSchemaStepsBuilder();
    step.setType(AlterTableRequestPB.StepType.ADD_COLUMN);
    step.setAddColumn(AlterTableRequestPB.AddColumn.newBuilder().setSchema(ProtobufHelper.columnToPb(new ColumnSchema.ColumnSchemaBuilder(name, type).defaultValue(defaultVal).build())));
    return this;
}
#method_after
public AlterTableOptions addColumn(String name, Type type, Object defaultVal) {
    return addColumn(new ColumnSchema.ColumnSchemaBuilder(name, type).defaultValue(defaultVal).build());
}
#end_block

#method_before
public AlterTableOptions addNullableColumn(String name, Type type, Object defaultVal) {
    AlterTableRequestPB.Step.Builder step = pb.addAlterSchemaStepsBuilder();
    step.setType(AlterTableRequestPB.StepType.ADD_COLUMN);
    step.setAddColumn(AlterTableRequestPB.AddColumn.newBuilder().setSchema(ProtobufHelper.columnToPb(new ColumnSchema.ColumnSchemaBuilder(name, type).nullable(true).defaultValue(defaultVal).build())));
    return this;
}
#method_after
public AlterTableOptions addNullableColumn(String name, Type type, Object defaultVal) {
    return addColumn(new ColumnSchema.ColumnSchemaBuilder(name, type).nullable(true).defaultValue(defaultVal).build());
}
#end_block

#method_before
@Test
public void testAlterAddColumns() throws Exception {
    KuduTable table = createTable(ImmutableList.<Pair<Integer, Integer>>of());
    insertRows(table, 0, 100);
    assertEquals(100, countRowsInTable(table));
    syncClient.alterTable(tableName, new AlterTableOptions().addColumn("addNonNull", Type.INT32, 100).addNullableColumn("addNullable", Type.INT32).addNullableColumn("addNullableDef", Type.INT32, 200));
    boolean done = syncClient.isAlterTableDone(tableName);
    assertTrue(done);
    // Reopen table for the new schema.
    table = syncClient.openTable(tableName);
    assertEquals(5, table.getSchema().getColumnCount());
    // Check defaults applied.
    List<String> actual = scanTableToStrings(table);
    List<String> expected = new ArrayList<>(200);
    for (int i = 0; i < 100; i++) {
        expected.add(i, String.format("INT32 c0=%d, INT32 c1=%d, INT32 addNonNull=100" + ", INT32 addNullable=NULL, INT32 addNullableDef=200", i, i));
    }
    Collections.sort(expected);
    assertArrayEquals(expected.toArray(new String[0]), actual.toArray(new String[0]));
}
#method_after
@Test
public void testAlterAddColumns() throws Exception {
    KuduTable table = createTable(ImmutableList.<Pair<Integer, Integer>>of());
    insertRows(table, 0, 100);
    assertEquals(100, countRowsInTable(table));
    syncClient.alterTable(tableName, new AlterTableOptions().addColumn("addNonNull", Type.INT32, 100).addNullableColumn("addNullable", Type.INT32).addNullableColumn("addNullableDef", Type.INT32, 200));
    boolean done = syncClient.isAlterTableDone(tableName);
    assertTrue(done);
    // Reopen table for the new schema.
    table = syncClient.openTable(tableName);
    assertEquals(5, table.getSchema().getColumnCount());
    // Add a row with addNullableDef=null
    KuduSession session = syncClient.newSession();
    Insert insert = table.newInsert();
    PartialRow row = insert.getRow();
    row.addInt("c0", 101);
    row.addInt("c1", 101);
    row.addInt("addNonNull", 101);
    row.addInt("addNullable", 101);
    row.setNull("addNullableDef");
    session.apply(insert);
    session.flush();
    RowError[] rowErrors = session.getPendingErrors().getRowErrors();
    assertEquals(String.format("row errors: %s", Arrays.toString(rowErrors)), 0, rowErrors.length);
    // Check defaults applied, and that row key=101
    List<String> actual = scanTableToStrings(table);
    List<String> expected = new ArrayList<>(101);
    for (int i = 0; i < 100; i++) {
        expected.add(i, String.format("INT32 c0=%d, INT32 c1=%d, INT32 addNonNull=100" + ", INT32 addNullable=NULL, INT32 addNullableDef=200", i, i));
    }
    expected.add("INT32 c0=101, INT32 c1=101, INT32 addNonNull=101" + ", INT32 addNullable=101, INT32 addNullableDef=NULL");
    Collections.sort(expected);
    assertArrayEquals(expected.toArray(new String[0]), actual.toArray(new String[0]));
}
#end_block

#method_before
@Test
public void TestCreateManagedKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Test primary keys and partition by clauses
    AnalyzesOk("create table tab (x int primary key) partition by hash(x) " + "partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, primary key(x)) partition by hash(x) " + "partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) " + "partition by hash(x, y) partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x)) " + "partition by hash(x) partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) " + "partition by hash(y) partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, y string, primary key (x)) partition by " + "hash (x) partitions 3, range (x) (partition values < 1, partition " + "1 <= values < 10, partition 10 <= values < 20, partition value = 30) " + "stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) partition by " + "range (x, y) (partition value = (2001, 1), partition value = (2002, 1), " + "partition value = (2003, 2)) stored as kudu");
    // Non-literal boundary values in range partitions
    AnalyzesOk("create table tab (x int, y int, primary key (x)) partition by " + "range (x) (partition values < 1 + 1, partition (1+3) + 2 < values < 10, " + "partition factorial(4) < values < factorial(5), " + "partition value = factorial(6)) stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) partition by " + "range(x, y) (partition value = (1+1, 2+2), partition value = ((1+1+1)+1, 10), " + "partition value = (cast (30 as int), factorial(5))) stored as kudu");
    AnalysisError("create table tab (x int primary key) partition by range (x) " + "(partition values < x + 1) stored as kudu", "Only constant values are allowed " + "for range-partition bounds: x + 1");
    AnalysisError("create table tab (x int primary key) partition by range (x) " + "(partition values <= isnull(null, null)) stored as kudu", "Range partition " + "values cannot be NULL. Range partition: 'PARTITION VALUES <= " + "isnull(NULL, NULL)'");
    AnalysisError("create table tab (x int primary key) partition by range (x) " + "(partition values <= (select count(*) from functional.alltypestiny)) " + "stored as kudu", "Only constant values are allowed for range-partition " + "bounds: (SELECT count(*) FROM functional.alltypestiny)");
    // Multilevel partitioning. Data is split into 3 buckets based on 'x' and each
    // bucket is partitioned into 4 tablets based on the range partitions of 'y'.
    AnalyzesOk("create table tab (x int, y string, primary key(x, y)) " + "partition by hash(x) partitions 3, range(y) " + "(partition values < 'aa', partition 'aa' <= values < 'bb', " + "partition 'bb' <= values < 'cc', partition 'cc' <= values) " + "stored as kudu");
    // Key column in upper case
    AnalyzesOk("create table tab (x int, y int, primary key (X)) " + "partition by hash (x) partitions 8 stored as kudu");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "partition by hash (a, b) partitions 8, hash(c) into 2 stored as " + "kudu");
    // No columns specified in the PARTITION BY HASH clause
    AnalyzesOk("create table tab (a int primary key, b int, c int, d int) " + "partition by hash partitions 8 stored as kudu");
    // Distribute range data types are picked up during analysis and forwarded to Kudu.
    // Column names in distribute params should also be case-insensitive.
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key(a, b, c, d))" + "partition by hash (a, B, c) partitions 8, " + "range (A) (partition values < 1, partition 1 <= values < 2, " + "partition 2 <= values < 3, partition 3 <= values < 4, partition 4 <= values) " + "stored as kudu");
    // Allowing range partitioning on a subset of the primary keys
    AnalyzesOk("create table tab (id int, name string, valf float, vali bigint, " + "primary key (id, name)) partition by range (name) " + "(partition 'aa' < values <= 'bb') stored as kudu");
    // Null values in range partition values
    AnalysisError("create table tab (id int, name string, primary key(id, name)) " + "partition by hash (id) partitions 3, range (name) " + "(partition value = null, partition value = 1) stored as kudu", "Range partition values cannot be NULL. Range partition: 'PARTITION " + "VALUE = NULL'");
    // Primary key specified in tblproperties
    AnalysisError(String.format("create table tab (x int) partition by hash (x) " + "partitions 8 stored as kudu tblproperties ('%s' = 'x')", KuduTable.KEY_KEY_COLUMNS), "PRIMARY KEY must be used instead of the table " + "property");
    // Primary key column that doesn't exist
    AnalysisError("create table tab (x int, y int, primary key (z)) " + "partition by hash (x) partitions 8 stored as kudu", "PRIMARY KEY column 'z' does not exist in the table");
    // Invalid composite primary key
    AnalysisError("create table tab (x int primary key, primary key(x)) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    AnalysisError("create table tab (x int primary key, y int primary key) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    // Specifying the same primary key column multiple times
    AnalysisError("create table tab (x int, primary key (x, x)) partition by hash (x) " + "partitions 8 stored as kudu", "Column 'x' is listed multiple times as a PRIMARY KEY.");
    // Number of range partition boundary values should be equal to the number of range
    // columns.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "partition by range(a) (partition value = (1, 2), " + "partition value = 3, partition value = 4) stored as kudu", "Number of specified range partition values is different than the number of " + "partitioning columns: (2 vs 1). Range partition: 'PARTITION VALUE = (1,2)'");
    // Key ranges must match the column types.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "partition by hash (a, b, c) partitions 8, range (a) " + "(partition value = 1, partition value = 'abc', partition 3 <= values) " + "stored as kudu", "Range partition value 'abc' (type: STRING) is not type " + "compatible with partitioning column 'a' (type: INT).");
    AnalysisError("create table tab (a tinyint primary key) partition by range (a) " + "(partition value = 128) stored as kudu", "Range partition value 128 " + "(type: SMALLINT) is not type compatible with partitioning column 'a' " + "(type: TINYINT)");
    AnalysisError("create table tab (a smallint primary key) partition by range (a) " + "(partition value = 32768) stored as kudu", "Range partition value 32768 " + "(type: INT) is not type compatible with partitioning column 'a' " + "(type: SMALLINT)");
    AnalysisError("create table tab (a int primary key) partition by range (a) " + "(partition value = 2147483648) stored as kudu", "Range partition value " + "2147483648 (type: BIGINT) is not type compatible with partitioning column 'a' " + "(type: INT)");
    AnalysisError("create table tab (a bigint primary key) partition by range (a) " + "(partition value = 9223372036854775808) stored as kudu", "Range partition " + "value 9223372036854775808 (type: DECIMAL(19,0)) is not type compatible with " + "partitioning column 'a' (type: BIGINT)");
    // Test implicit casting/folding of partition values.
    AnalyzesOk("create table tab (a int primary key) partition by range (a) " + "(partition value = false, partition value = true) stored as kudu");
    // Non-key column used in PARTITION BY
    AnalysisError("create table tab (a int, b string, c bigint, primary key (a)) " + "partition by range (b) (partition value = 'abc') stored as kudu", "Column 'b' in 'RANGE (b) (PARTITION VALUE = 'abc')' is not a key column. " + "Only key columns can be used in PARTITION BY.");
    // No float range partition values
    AnalysisError("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "partition by hash (a, b, c) partitions 8, " + "range (a) (partition value = 1.2, partition value = 2) stored as kudu", "Range partition value 1.2 (type: DECIMAL(2,1)) is not type compatible with " + "partitioning column 'a' (type: INT).");
    // Non-existing column used in PARTITION BY
    AnalysisError("create table tab (a int, b int, primary key (a, b)) " + "partition by range(unknown_column) (partition value = 'abc') stored as kudu", "Column 'unknown_column' in 'RANGE (unknown_column) (PARTITION VALUE = 'abc')' " + "is not a key column. Only key columns can be used in PARTITION BY");
    // Kudu table name is specified in tblproperties
    AnalyzesOk("create table tab (x int primary key) partition by hash (x) " + "partitions 8 stored as kudu tblproperties ('kudu.table_name'='tab_1'," + "'kudu.num_tablet_replicas'='1'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081')");
    // No port is specified in kudu master address
    AnalyzesOk("create table tdata_no_port (id int primary key, name string, " + "valf float, vali bigint) partition by range(id) (partition values <= 10, " + "partition 10 < values <= 30, partition 30 < values) " + "stored as kudu tblproperties('kudu.master_addresses'='127.0.0.1')");
    // Not using the STORED AS KUDU syntax to specify a Kudu table
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    AnalysisError("create table tab (x int primary key) stored as kudu tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    // Invalid value for number of replicas
    AnalysisError("create table t (x int primary key) stored as kudu tblproperties (" + "'kudu.num_tablet_replicas'='1.1')", "Table property 'kudu.num_tablet_replicas' must be an integer.");
    // Don't allow caching
    AnalysisError("create table tab (x int primary key) stored as kudu cached in " + "'testPool'", "A Kudu table cannot be cached in HDFS.");
    // LOCATION cannot be used with Kudu tables
    AnalysisError("create table tab (a int primary key) partition by hash (a) " + "partitions 3 stored as kudu location '/test-warehouse/'", "LOCATION cannot be specified for a Kudu table.");
    // PARTITION BY is required for managed tables.
    AnalysisError("create table tab (a int, primary key (a)) stored as kudu", "Table partitioning must be specified for managed Kudu tables.");
    AnalysisError("create table tab (a int) stored as kudu", "A primary key is required for a Kudu table.");
    // Using ROW FORMAT with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "row format delimited escaped by 'X' stored as kudu", "ROW FORMAT cannot be specified for file format KUDU.");
    // Using PARTITIONED BY with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "partitioned by (y int) stored as kudu", "PARTITIONED BY cannot be used " + "in Kudu tables.");
    // Test unsupported Kudu types
    List<String> unsupportedTypes = Lists.newArrayList("DECIMAL(9,0)", "TIMESTAMP", "VARCHAR(20)", "CHAR(20)", "STRUCT<F1:INT,F2:STRING>", "ARRAY<INT>", "MAP<STRING,STRING>");
    for (String t : unsupportedTypes) {
        String expectedError = String.format("Cannot create table 'tab': Type %s is not supported in Kudu", t);
        // Unsupported type is PK and partition col
        String stmt = String.format("create table tab (x %s primary key) " + "partition by hash(x) partitions 3 stored as kudu", t);
        AnalysisError(stmt, expectedError);
        // Unsupported type is not PK/partition col
        stmt = String.format("create table tab (x int primary key, y %s) " + "partition by hash(x) partitions 3 stored as kudu", t);
        AnalysisError(stmt, expectedError);
    }
    // Test column options
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (Encoding enc : Encoding.values()) {
        for (CompressionAlgorithm comp : CompressionAlgorithm.values()) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        AnalyzesOk(String.format("create table tab (x int primary key " + "not null encoding %s compression %s %s %s, y int encoding %s " + "compression %s %s %s %s) partition by hash (x) " + "partitions 3 stored as kudu", enc, comp, def, block, enc, comp, def, nul, block));
                    }
                }
            }
        }
    }
    // Primary key specified using the PRIMARY KEY clause
    AnalyzesOk("create table tab (x int not null encoding plain_encoding " + "compression snappy block_size 1, y int null encoding rle compression lz4 " + "default 1, primary key(x)) partition by hash (x) partitions 3 " + "stored as kudu");
    // Primary keys can't be null
    AnalysisError("create table tab (x int primary key null, y int not null) " + "partition by hash (x) partitions 3 stored as kudu", "Primary key columns " + "cannot be nullable: x INT PRIMARY KEY NULL");
    AnalysisError("create table tab (x int not null, y int null, primary key (x, y)) " + "partition by hash (x) partitions 3 stored as kudu", "Primary key columns " + "cannot be nullable: y INT NULL");
    // Unsupported encoding value
    AnalysisError("create table tab (x int primary key, y int encoding invalid_enc) " + "partition by hash (x) partitions 3 stored as kudu", "Unsupported encoding " + "value 'INVALID_ENC'. Supported encoding values are: " + Joiner.on(", ").join(Encoding.values()));
    // Unsupported compression algorithm
    AnalysisError("create table tab (x int primary key, y int compression " + "invalid_comp) partition by hash (x) partitions 3 stored as kudu", "Unsupported compression algorithm 'INVALID_COMP'. Supported compression " + "algorithms are: " + Joiner.on(", ").join(CompressionAlgorithm.values()));
    // Default values
    AnalyzesOk("create table tab (i1 tinyint default 1, i2 smallint default 10, " + "i3 int default 100, i4 bigint default 1000, vals string default 'test', " + "valf float default cast(1.2 as float), vald double default " + "cast(3.1452 as double), valb boolean default true, " + "primary key (i1, i2, i3, i4, vals)) partition by hash (i1) partitions 3 " + "stored as kudu");
    AnalyzesOk("create table tab (i int primary key default 1+1+1) " + "partition by hash (i) partitions 3 stored as kudu");
    AnalyzesOk("create table tab (i int primary key default factorial(5)) " + "partition by hash (i) partitions 3 stored as kudu");
    AnalyzesOk("create table tab (i int primary key, x int null default " + "isnull(null, null)) partition by hash (i) partitions 3 stored as kudu");
    // Invalid default values
    AnalysisError("create table tab (i int primary key default 'string_val') " + "partition by hash (i) partitions 3 stored as kudu", "Default value " + "'string_val' (type: STRING) is not compatible with column 'i' (type: INT).");
    AnalysisError("create table tab (i int primary key, x int default 1.1) " + "partition by hash (i) partitions 3 stored as kudu", "Default value 1.1 (type: DECIMAL(2,1)) is not compatible with column " + "'x' (type: INT).");
    AnalysisError("create table tab (i tinyint primary key default 128) " + "partition by hash (i) partitions 3 stored as kudu", "Default value " + "128 (type: SMALLINT) is not compatible with column 'i' (type: TINYINT).");
    AnalysisError("create table tab (i int primary key default isnull(null, null)) " + "partition by hash (i) partitions 3 stored as kudu", "Default value of " + "NULL not allowed on non-nullable column: 'i'");
    AnalysisError("create table tab (i int primary key, x int not null " + "default isnull(null, null)) partition by hash (i) partitions 3 " + "stored as kudu", "Default value of NULL not allowed on non-nullable column: " + "'x'");
    // Invalid block_size values
    AnalysisError("create table tab (i int primary key block_size 1.1) " + "partition by hash (i) partitions 3 stored as kudu", "Invalid value " + "for BLOCK_SIZE: 1.1. A positive INTEGER value is expected.");
    AnalysisError("create table tab (i int primary key block_size 'val') " + "partition by hash (i) partitions 3 stored as kudu", "Invalid value " + "for BLOCK_SIZE: 'val'. A positive INTEGER value is expected.");
}
#method_after
@Test
public void TestCreateManagedKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Test primary keys and partition by clauses
    AnalyzesOk("create table tab (x int primary key) partition by hash(x) " + "partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, primary key(x)) partition by hash(x) " + "partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) " + "partition by hash(x, y) partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x)) " + "partition by hash(x) partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) " + "partition by hash(y) partitions 8 stored as kudu");
    AnalyzesOk("create table tab (x int, y string, primary key (x)) partition by " + "hash (x) partitions 3, range (x) (partition values < 1, partition " + "1 <= values < 10, partition 10 <= values < 20, partition value = 30) " + "stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) partition by " + "range (x, y) (partition value = (2001, 1), partition value = (2002, 1), " + "partition value = (2003, 2)) stored as kudu");
    // Non-literal boundary values in range partitions
    AnalyzesOk("create table tab (x int, y int, primary key (x)) partition by " + "range (x) (partition values < 1 + 1, partition (1+3) + 2 < values < 10, " + "partition factorial(4) < values < factorial(5), " + "partition value = factorial(6)) stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) partition by " + "range(x, y) (partition value = (1+1, 2+2), partition value = ((1+1+1)+1, 10), " + "partition value = (cast (30 as int), factorial(5))) stored as kudu");
    AnalysisError("create table tab (x int primary key) partition by range (x) " + "(partition values < x + 1) stored as kudu", "Only constant values are allowed " + "for range-partition bounds: x + 1");
    AnalysisError("create table tab (x int primary key) partition by range (x) " + "(partition values <= isnull(null, null)) stored as kudu", "Range partition " + "values cannot be NULL. Range partition: 'PARTITION VALUES <= " + "isnull(NULL, NULL)'");
    AnalysisError("create table tab (x int primary key) partition by range (x) " + "(partition values <= (select count(*) from functional.alltypestiny)) " + "stored as kudu", "Only constant values are allowed for range-partition " + "bounds: (SELECT count(*) FROM functional.alltypestiny)");
    // Multilevel partitioning. Data is split into 3 buckets based on 'x' and each
    // bucket is partitioned into 4 tablets based on the range partitions of 'y'.
    AnalyzesOk("create table tab (x int, y string, primary key(x, y)) " + "partition by hash(x) partitions 3, range(y) " + "(partition values < 'aa', partition 'aa' <= values < 'bb', " + "partition 'bb' <= values < 'cc', partition 'cc' <= values) " + "stored as kudu");
    // Key column in upper case
    AnalyzesOk("create table tab (x int, y int, primary key (X)) " + "partition by hash (x) partitions 8 stored as kudu");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "partition by hash (a, b) partitions 8, hash(c) partitions 2 stored as " + "kudu");
    // No columns specified in the PARTITION BY HASH clause
    AnalyzesOk("create table tab (a int primary key, b int, c int, d int) " + "partition by hash partitions 8 stored as kudu");
    // Distribute range data types are picked up during analysis and forwarded to Kudu.
    // Column names in distribute params should also be case-insensitive.
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key(a, b, c, d))" + "partition by hash (a, B, c) partitions 8, " + "range (A) (partition values < 1, partition 1 <= values < 2, " + "partition 2 <= values < 3, partition 3 <= values < 4, partition 4 <= values) " + "stored as kudu");
    // Allowing range partitioning on a subset of the primary keys
    AnalyzesOk("create table tab (id int, name string, valf float, vali bigint, " + "primary key (id, name)) partition by range (name) " + "(partition 'aa' < values <= 'bb') stored as kudu");
    // Null values in range partition values
    AnalysisError("create table tab (id int, name string, primary key(id, name)) " + "partition by hash (id) partitions 3, range (name) " + "(partition value = null, partition value = 1) stored as kudu", "Range partition values cannot be NULL. Range partition: 'PARTITION " + "VALUE = NULL'");
    // Primary key specified in tblproperties
    AnalysisError(String.format("create table tab (x int) partition by hash (x) " + "partitions 8 stored as kudu tblproperties ('%s' = 'x')", KuduTable.KEY_KEY_COLUMNS), "PRIMARY KEY must be used instead of the table " + "property");
    // Primary key column that doesn't exist
    AnalysisError("create table tab (x int, y int, primary key (z)) " + "partition by hash (x) partitions 8 stored as kudu", "PRIMARY KEY column 'z' does not exist in the table");
    // Invalid composite primary key
    AnalysisError("create table tab (x int primary key, primary key(x)) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    AnalysisError("create table tab (x int primary key, y int primary key) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    // Specifying the same primary key column multiple times
    AnalysisError("create table tab (x int, primary key (x, x)) partition by hash (x) " + "partitions 8 stored as kudu", "Column 'x' is listed multiple times as a PRIMARY KEY.");
    // Number of range partition boundary values should be equal to the number of range
    // columns.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "partition by range(a) (partition value = (1, 2), " + "partition value = 3, partition value = 4) stored as kudu", "Number of specified range partition values is different than the number of " + "partitioning columns: (2 vs 1). Range partition: 'PARTITION VALUE = (1,2)'");
    // Key ranges must match the column types.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "partition by hash (a, b, c) partitions 8, range (a) " + "(partition value = 1, partition value = 'abc', partition 3 <= values) " + "stored as kudu", "Range partition value 'abc' (type: STRING) is not type " + "compatible with partitioning column 'a' (type: INT).");
    AnalysisError("create table tab (a tinyint primary key) partition by range (a) " + "(partition value = 128) stored as kudu", "Range partition value 128 " + "(type: SMALLINT) is not type compatible with partitioning column 'a' " + "(type: TINYINT)");
    AnalysisError("create table tab (a smallint primary key) partition by range (a) " + "(partition value = 32768) stored as kudu", "Range partition value 32768 " + "(type: INT) is not type compatible with partitioning column 'a' " + "(type: SMALLINT)");
    AnalysisError("create table tab (a int primary key) partition by range (a) " + "(partition value = 2147483648) stored as kudu", "Range partition value " + "2147483648 (type: BIGINT) is not type compatible with partitioning column 'a' " + "(type: INT)");
    AnalysisError("create table tab (a bigint primary key) partition by range (a) " + "(partition value = 9223372036854775808) stored as kudu", "Range partition " + "value 9223372036854775808 (type: DECIMAL(19,0)) is not type compatible with " + "partitioning column 'a' (type: BIGINT)");
    // Test implicit casting/folding of partition values.
    AnalyzesOk("create table tab (a int primary key) partition by range (a) " + "(partition value = false, partition value = true) stored as kudu");
    // Non-key column used in PARTITION BY
    AnalysisError("create table tab (a int, b string, c bigint, primary key (a)) " + "partition by range (b) (partition value = 'abc') stored as kudu", "Column 'b' in 'RANGE (b) (PARTITION VALUE = 'abc')' is not a key column. " + "Only key columns can be used in PARTITION BY.");
    // No float range partition values
    AnalysisError("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "partition by hash (a, b, c) partitions 8, " + "range (a) (partition value = 1.2, partition value = 2) stored as kudu", "Range partition value 1.2 (type: DECIMAL(2,1)) is not type compatible with " + "partitioning column 'a' (type: INT).");
    // Non-existing column used in PARTITION BY
    AnalysisError("create table tab (a int, b int, primary key (a, b)) " + "partition by range(unknown_column) (partition value = 'abc') stored as kudu", "Column 'unknown_column' in 'RANGE (unknown_column) (PARTITION VALUE = 'abc')' " + "is not a key column. Only key columns can be used in PARTITION BY");
    // Kudu table name is specified in tblproperties
    AnalyzesOk("create table tab (x int primary key) partition by hash (x) " + "partitions 8 stored as kudu tblproperties ('kudu.table_name'='tab_1'," + "'kudu.num_tablet_replicas'='1'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081')");
    // No port is specified in kudu master address
    AnalyzesOk("create table tdata_no_port (id int primary key, name string, " + "valf float, vali bigint) partition by range(id) (partition values <= 10, " + "partition 10 < values <= 30, partition 30 < values) " + "stored as kudu tblproperties('kudu.master_addresses'='127.0.0.1')");
    // Not using the STORED AS KUDU syntax to specify a Kudu table
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    AnalysisError("create table tab (x int primary key) stored as kudu tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    // Invalid value for number of replicas
    AnalysisError("create table t (x int primary key) stored as kudu tblproperties (" + "'kudu.num_tablet_replicas'='1.1')", "Table property 'kudu.num_tablet_replicas' must be an integer.");
    // Don't allow caching
    AnalysisError("create table tab (x int primary key) stored as kudu cached in " + "'testPool'", "A Kudu table cannot be cached in HDFS.");
    // LOCATION cannot be used with Kudu tables
    AnalysisError("create table tab (a int primary key) partition by hash (a) " + "partitions 3 stored as kudu location '/test-warehouse/'", "LOCATION cannot be specified for a Kudu table.");
    // PARTITION BY is required for managed tables.
    AnalysisError("create table tab (a int, primary key (a)) stored as kudu", "Table partitioning must be specified for managed Kudu tables.");
    AnalysisError("create table tab (a int) stored as kudu", "A primary key is required for a Kudu table.");
    // Using ROW FORMAT with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "row format delimited escaped by 'X' stored as kudu", "ROW FORMAT cannot be specified for file format KUDU.");
    // Using PARTITIONED BY with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "partitioned by (y int) stored as kudu", "PARTITIONED BY cannot be used " + "in Kudu tables.");
    // Test unsupported Kudu types
    List<String> unsupportedTypes = Lists.newArrayList("DECIMAL(9,0)", "TIMESTAMP", "VARCHAR(20)", "CHAR(20)", "STRUCT<F1:INT,F2:STRING>", "ARRAY<INT>", "MAP<STRING,STRING>");
    for (String t : unsupportedTypes) {
        String expectedError = String.format("Cannot create table 'tab': Type %s is not supported in Kudu", t);
        // Unsupported type is PK and partition col
        String stmt = String.format("create table tab (x %s primary key) " + "partition by hash(x) partitions 3 stored as kudu", t);
        AnalysisError(stmt, expectedError);
        // Unsupported type is not PK/partition col
        stmt = String.format("create table tab (x int primary key, y %s) " + "partition by hash(x) partitions 3 stored as kudu", t);
        AnalysisError(stmt, expectedError);
    }
    // Test column options
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (Encoding enc : Encoding.values()) {
        for (CompressionAlgorithm comp : CompressionAlgorithm.values()) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        AnalyzesOk(String.format("create table tab (x int primary key " + "not null encoding %s compression %s %s %s, y int encoding %s " + "compression %s %s %s %s) partition by hash (x) " + "partitions 3 stored as kudu", enc, comp, def, block, enc, comp, def, nul, block));
                    }
                }
            }
        }
    }
    // Primary key specified using the PRIMARY KEY clause
    AnalyzesOk("create table tab (x int not null encoding plain_encoding " + "compression snappy block_size 1, y int null encoding rle compression lz4 " + "default 1, primary key(x)) partition by hash (x) partitions 3 " + "stored as kudu");
    // Primary keys can't be null
    AnalysisError("create table tab (x int primary key null, y int not null) " + "partition by hash (x) partitions 3 stored as kudu", "Primary key columns " + "cannot be nullable: x INT PRIMARY KEY NULL");
    AnalysisError("create table tab (x int not null, y int null, primary key (x, y)) " + "partition by hash (x) partitions 3 stored as kudu", "Primary key columns " + "cannot be nullable: y INT NULL");
    // Unsupported encoding value
    AnalysisError("create table tab (x int primary key, y int encoding invalid_enc) " + "partition by hash (x) partitions 3 stored as kudu", "Unsupported encoding " + "value 'INVALID_ENC'. Supported encoding values are: " + Joiner.on(", ").join(Encoding.values()));
    // Unsupported compression algorithm
    AnalysisError("create table tab (x int primary key, y int compression " + "invalid_comp) partition by hash (x) partitions 3 stored as kudu", "Unsupported compression algorithm 'INVALID_COMP'. Supported compression " + "algorithms are: " + Joiner.on(", ").join(CompressionAlgorithm.values()));
    // Default values
    AnalyzesOk("create table tab (i1 tinyint default 1, i2 smallint default 10, " + "i3 int default 100, i4 bigint default 1000, vals string default 'test', " + "valf float default cast(1.2 as float), vald double default " + "cast(3.1452 as double), valb boolean default true, " + "primary key (i1, i2, i3, i4, vals)) partition by hash (i1) partitions 3 " + "stored as kudu");
    AnalyzesOk("create table tab (i int primary key default 1+1+1) " + "partition by hash (i) partitions 3 stored as kudu");
    AnalyzesOk("create table tab (i int primary key default factorial(5)) " + "partition by hash (i) partitions 3 stored as kudu");
    AnalyzesOk("create table tab (i int primary key, x int null default " + "isnull(null, null)) partition by hash (i) partitions 3 stored as kudu");
    // Invalid default values
    AnalysisError("create table tab (i int primary key default 'string_val') " + "partition by hash (i) partitions 3 stored as kudu", "Default value " + "'string_val' (type: STRING) is not compatible with column 'i' (type: INT).");
    AnalysisError("create table tab (i int primary key, x int default 1.1) " + "partition by hash (i) partitions 3 stored as kudu", "Default value 1.1 (type: DECIMAL(2,1)) is not compatible with column " + "'x' (type: INT).");
    AnalysisError("create table tab (i tinyint primary key default 128) " + "partition by hash (i) partitions 3 stored as kudu", "Default value " + "128 (type: SMALLINT) is not compatible with column 'i' (type: TINYINT).");
    AnalysisError("create table tab (i int primary key default isnull(null, null)) " + "partition by hash (i) partitions 3 stored as kudu", "Default value of " + "NULL not allowed on non-nullable column: 'i'");
    AnalysisError("create table tab (i int primary key, x int not null " + "default isnull(null, null)) partition by hash (i) partitions 3 " + "stored as kudu", "Default value of NULL not allowed on non-nullable column: " + "'x'");
    // Invalid block_size values
    AnalysisError("create table tab (i int primary key block_size 1.1) " + "partition by hash (i) partitions 3 stored as kudu", "Invalid value " + "for BLOCK_SIZE: 1.1. A positive INTEGER value is expected.");
    AnalysisError("create table tab (i int primary key block_size 'val') " + "partition by hash (i) partitions 3 stored as kudu", "Invalid value " + "for BLOCK_SIZE: 'val'. A positive INTEGER value is expected.");
}
#end_block

#method_before
private Expr buildPartitionPredicate(HdfsPartition partition, Analyzer analyzer) throws ImpalaException {
    // construct smap
    ExprSubstitutionMap sMap = new ExprSubstitutionMap();
    for (int i = 0; i < refdKeys_.size(); ++i) {
        sMap.put(lhsSlotRefs_.get(i), partition.getPartitionValues().get(refdKeys_.get(i)));
    }
    Expr literalPredicate = predicate_.substitute(sMap, analyzer, false);
    if (LOG.isTraceEnabled()) {
        LOG.trace("buildPartitionPredicate: " + literalPredicate.toSql() + " " + literalPredicate.debugString());
    }
    if (!literalPredicate.isConstant()) {
        throw new NotImplementedException(String.format("Detected non-deterministic predicate: %s\nThis error is thrown to prevent " + "returning wrong results because Impala cannot assign/evaluate " + "this non-deterministic predicate correctly.", predicate_.toSql()));
    }
    return literalPredicate;
}
#method_after
private Expr buildPartitionPredicate(HdfsPartition partition, Analyzer analyzer) throws ImpalaException {
    // construct smap
    ExprSubstitutionMap sMap = new ExprSubstitutionMap();
    for (int i = 0; i < refdKeys_.size(); ++i) {
        sMap.put(lhsSlotRefs_.get(i), partition.getPartitionValues().get(refdKeys_.get(i)));
    }
    Expr literalPredicate = predicate_.substitute(sMap, analyzer, false);
    if (LOG.isTraceEnabled()) {
        LOG.trace("buildPartitionPredicate: " + literalPredicate.toSql() + " " + literalPredicate.debugString());
    }
    if (!literalPredicate.isConstant()) {
        throw new NotImplementedException("Unsupported non-deterministic predicate: " + predicate_.toSql());
    }
    return literalPredicate;
}
#end_block

#method_before
public List<Expr> getEqJoinConjuncts(List<TupleId> lhsTblRefIds, List<TupleId> rhsTblRefIds) {
    // Contains all equi-join conjuncts that have one child fully bound by one of the
    // rhs table ref ids (the other child is not bound by that rhs table ref id).
    List<ExprId> conjunctIds = Lists.newArrayList();
    for (TupleId rhsId : rhsTblRefIds) {
        List<ExprId> cids = globalState_.eqJoinConjuncts.get(rhsId);
        if (cids == null)
            continue;
        for (ExprId eid : cids) {
            if (!conjunctIds.contains(eid))
                conjunctIds.add(eid);
        }
    }
    // Since we currently prevent join re-reordering across outer joins, we can never
    // have a bushy outer join with multiple rhs table ref ids. A busy outer join can
    // only be constructed with an inline view (which has a single table ref id).
    List<ExprId> ojClauseConjuncts = null;
    if (rhsTblRefIds.size() == 1) {
        ojClauseConjuncts = globalState_.conjunctsByOjClause.get(rhsTblRefIds.get(0));
    }
    // List of table ref ids that the join node will 'materialize'.
    List<TupleId> nodeTblRefIds = Lists.newArrayList(lhsTblRefIds);
    nodeTblRefIds.addAll(rhsTblRefIds);
    List<Expr> result = Lists.newArrayList();
    for (ExprId conjunctId : conjunctIds) {
        Expr e = globalState_.conjuncts.get(conjunctId);
        Preconditions.checkState(e != null);
        if (!canEvalFullOuterJoinedConjunct(e, nodeTblRefIds) || !canEvalAntiJoinedConjunct(e, nodeTblRefIds)) {
            continue;
        }
        if (ojClauseConjuncts != null && !ojClauseConjuncts.contains(conjunctId))
            continue;
        result.add(e);
    }
    return result;
}
#method_after
public List<Expr> getEqJoinConjuncts(List<TupleId> lhsTblRefIds, List<TupleId> rhsTblRefIds) {
    // Contains all equi-join conjuncts that have one child fully bound by one of the
    // rhs table ref ids (the other child is not bound by that rhs table ref id).
    List<ExprId> conjunctIds = Lists.newArrayList();
    for (TupleId rhsId : rhsTblRefIds) {
        List<ExprId> cids = globalState_.eqJoinConjuncts.get(rhsId);
        if (cids == null)
            continue;
        for (ExprId eid : cids) {
            if (!conjunctIds.contains(eid))
                conjunctIds.add(eid);
        }
    }
    // Since we currently prevent join re-reordering across outer joins, we can never
    // have a bushy outer join with multiple rhs table ref ids. A busy outer join can
    // only be constructed with an inline view (which has a single table ref id).
    List<ExprId> ojClauseConjuncts = null;
    if (rhsTblRefIds.size() == 1) {
        ojClauseConjuncts = globalState_.conjunctsByOjClause.get(rhsTblRefIds.get(0));
    }
    // List of table ref ids that the join node will 'materialize'.
    List<TupleId> nodeTblRefIds = Lists.newArrayList(lhsTblRefIds);
    nodeTblRefIds.addAll(rhsTblRefIds);
    List<Expr> result = Lists.newArrayList();
    for (ExprId conjunctId : conjunctIds) {
        Expr e = globalState_.conjuncts.get(conjunctId);
        Preconditions.checkState(e != null);
        if (!canEvalFullOuterJoinedConjunct(e, nodeTblRefIds) || !canEvalAntiJoinedConjunct(e, nodeTblRefIds) || !canEvalOuterJoinedConjunct(e, nodeTblRefIds)) {
            continue;
        }
        if (ojClauseConjuncts != null && !ojClauseConjuncts.contains(conjunctId))
            continue;
        result.add(e);
    }
    return result;
}
#end_block

#method_before
public ArrayList<Expr> getBoundPredicates(TupleId destTid, Set<SlotId> ignoreSlots, boolean markAssigned) {
    ArrayList<Expr> result = Lists.newArrayList();
    for (ExprId srcConjunctId : globalState_.singleTidConjuncts) {
        Expr srcConjunct = globalState_.conjuncts.get(srcConjunctId);
        if (srcConjunct instanceof SlotRef)
            continue;
        Preconditions.checkNotNull(srcConjunct);
        List<TupleId> srcTids = Lists.newArrayList();
        List<SlotId> srcSids = Lists.newArrayList();
        srcConjunct.getIds(srcTids, srcSids);
        Preconditions.checkState(srcTids.size() == 1);
        // Generate slot-mappings to bind srcConjunct to destTid.
        TupleId srcTid = srcTids.get(0);
        List<List<SlotId>> allDestSids = getEquivDestSlotIds(srcTid, srcSids, destTid, ignoreSlots);
        if (allDestSids.isEmpty())
            continue;
        // Indicates whether the source slots have equivalent slots that belong
        // to an outer-joined tuple.
        boolean hasOuterJoinedTuple = false;
        for (SlotId srcSid : srcSids) {
            if (hasOuterJoinedTuple(globalState_.equivClassBySlotId.get(srcSid))) {
                hasOuterJoinedTuple = true;
                break;
            }
        }
        // relative to 'srcConjunct'.
        if (hasOuterJoinedTuple && isTrueWithNullSlots(srcConjunct))
            continue;
        // (otherwise srcConjunct needn't be true when destTid is set)
        if (globalState_.ojClauseByConjunct.containsKey(srcConjunct.getId())) {
            if (!globalState_.outerJoinedTupleIds.containsKey(destTid))
                continue;
            if (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(destTid)) {
                continue;
            }
            // Do not propagate conjuncts from the on-clause of full-outer or anti-joins.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(srcConjunct.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                continue;
        }
        // join node.
        if (isAntiJoinedConjunct(srcConjunct))
            continue;
        // Generate predicates for all src-to-dest slot mappings.
        for (List<SlotId> destSids : allDestSids) {
            Preconditions.checkState(destSids.size() == srcSids.size());
            Expr p;
            if (srcSids.containsAll(destSids)) {
                p = srcConjunct;
            } else {
                ExprSubstitutionMap smap = new ExprSubstitutionMap();
                for (int i = 0; i < srcSids.size(); ++i) {
                    smap.put(new SlotRef(globalState_.descTbl.getSlotDesc(srcSids.get(i))), new SlotRef(globalState_.descTbl.getSlotDesc(destSids.get(i))));
                }
                try {
                    p = srcConjunct.trySubstitute(smap, this, false);
                } catch (ImpalaException exc) {
                    // not an executable predicate; ignore
                    continue;
                }
                // Unset the id because this bound predicate itself is not registered, and
                // to prevent callers from inadvertently marking the srcConjunct as assigned.
                p.setId(null);
                if (p instanceof BinaryPredicate)
                    ((BinaryPredicate) p).setIsInferred();
                if (LOG.isTraceEnabled()) {
                    LOG.trace("new pred: " + p.toSql() + " " + p.debugString());
                }
            }
            if (markAssigned) {
                // predicate assignment doesn't hold if:
                // - the application against slotId doesn't transfer the value back to its
                // originating slot
                // - the original predicate is on an OJ'd table but doesn't originate from
                // that table's OJ clause's ON clause (if it comes from anywhere but that
                // ON clause, it needs to be evaluated directly by the join node that
                // materializes the OJ'd table)
                boolean reverseValueTransfer = true;
                for (int i = 0; i < srcSids.size(); ++i) {
                    if (!hasValueTransfer(destSids.get(i), srcSids.get(i))) {
                        reverseValueTransfer = false;
                        break;
                    }
                }
                // Check if either srcConjunct or the generated predicate needs to be evaluated
                // at a join node (IMPALA-2018).
                boolean evalByJoin = (evalByJoin(srcConjunct) && (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(srcTid))) || (evalByJoin(p) && (globalState_.ojClauseByConjunct.get(p.getId()) != globalState_.outerJoinedTupleIds.get(destTid)));
                // mark all bound predicates including duplicate ones
                if (reverseValueTransfer && !evalByJoin)
                    markConjunctAssigned(srcConjunct);
            }
            // check if we already created this predicate
            if (!result.contains(p))
                result.add(p);
        }
    }
    return result;
}
#method_after
public ArrayList<Expr> getBoundPredicates(TupleId destTid, Set<SlotId> ignoreSlots, boolean markAssigned) {
    ArrayList<Expr> result = Lists.newArrayList();
    for (ExprId srcConjunctId : globalState_.singleTidConjuncts) {
        Expr srcConjunct = globalState_.conjuncts.get(srcConjunctId);
        if (srcConjunct instanceof SlotRef)
            continue;
        Preconditions.checkNotNull(srcConjunct);
        List<TupleId> srcTids = Lists.newArrayList();
        List<SlotId> srcSids = Lists.newArrayList();
        srcConjunct.getIds(srcTids, srcSids);
        Preconditions.checkState(srcTids.size() == 1);
        // Generate slot-mappings to bind srcConjunct to destTid.
        TupleId srcTid = srcTids.get(0);
        List<List<SlotId>> allDestSids = getEquivDestSlotIds(srcTid, srcSids, destTid, ignoreSlots);
        if (allDestSids.isEmpty())
            continue;
        // Indicates whether the source slots have equivalent slots that belong
        // to an outer-joined tuple.
        boolean hasOuterJoinedTuple = false;
        for (SlotId srcSid : srcSids) {
            if (hasOuterJoinedTuple(globalState_.equivClassBySlotId.get(srcSid))) {
                hasOuterJoinedTuple = true;
                break;
            }
        }
        // relative to 'srcConjunct'.
        if (hasOuterJoinedTuple && isTrueWithNullSlots(srcConjunct))
            continue;
        // (otherwise srcConjunct needn't be true when destTid is set)
        if (globalState_.ojClauseByConjunct.containsKey(srcConjunct.getId())) {
            if (!globalState_.outerJoinedTupleIds.containsKey(destTid))
                continue;
            if (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(destTid)) {
                continue;
            }
            // Do not propagate conjuncts from the on-clause of full-outer or anti-joins.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(srcConjunct.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                continue;
        }
        // join node.
        if (isAntiJoinedConjunct(srcConjunct))
            continue;
        // Generate predicates for all src-to-dest slot mappings.
        for (List<SlotId> destSids : allDestSids) {
            Preconditions.checkState(destSids.size() == srcSids.size());
            Expr p;
            if (srcSids.containsAll(destSids)) {
                p = srcConjunct;
            } else {
                ExprSubstitutionMap smap = new ExprSubstitutionMap();
                for (int i = 0; i < srcSids.size(); ++i) {
                    smap.put(new SlotRef(globalState_.descTbl.getSlotDesc(srcSids.get(i))), new SlotRef(globalState_.descTbl.getSlotDesc(destSids.get(i))));
                }
                try {
                    p = srcConjunct.trySubstitute(smap, this, false);
                } catch (ImpalaException exc) {
                    // not an executable predicate; ignore
                    continue;
                }
                // Unset the id because this bound predicate itself is not registered, and
                // to prevent callers from inadvertently marking the srcConjunct as assigned.
                p.setId(null);
                if (p instanceof BinaryPredicate)
                    ((BinaryPredicate) p).setIsInferred();
                if (LOG.isTraceEnabled()) {
                    LOG.trace("new pred: " + p.toSql() + " " + p.debugString());
                }
            }
            if (markAssigned) {
                // predicate assignment doesn't hold if:
                // - the application against slotId doesn't transfer the value back to its
                // originating slot
                // - the original predicate is on an OJ'd table but doesn't originate from
                // that table's OJ clause's ON clause (if it comes from anywhere but that
                // ON clause, it needs to be evaluated directly by the join node that
                // materializes the OJ'd table)
                boolean reverseValueTransfer = true;
                for (int i = 0; i < srcSids.size(); ++i) {
                    if (!hasValueTransfer(destSids.get(i), srcSids.get(i))) {
                        reverseValueTransfer = false;
                        break;
                    }
                }
                // IMPALA-2018/4379: Check if srcConjunct or the generated predicate need to
                // be evaluated again at a later point in the plan, e.g., by a join that makes
                // referenced tuples nullable. The first condition is conservative but takes
                // into account that On-clause conjuncts can sometimes be legitimately assigned
                // below their originating join.
                boolean evalAfterJoin = (hasOuterJoinedTuple && !srcConjunct.isOnClauseConjunct_) || (evalAfterJoin(srcConjunct) && (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(srcTid))) || (evalAfterJoin(p) && (globalState_.ojClauseByConjunct.get(p.getId()) != globalState_.outerJoinedTupleIds.get(destTid)));
                // mark all bound predicates including duplicate ones
                if (reverseValueTransfer && !evalAfterJoin)
                    markConjunctAssigned(srcConjunct);
            }
            // check if we already created this predicate
            if (!result.contains(p))
                result.add(p);
        }
    }
    return result;
}
#end_block

#method_before
public Table getTable(TableName tableName, Privilege privilege, boolean addAccessEvent) throws AnalysisException {
    Preconditions.checkNotNull(tableName);
    Preconditions.checkNotNull(privilege);
    Table table = null;
    tableName = new TableName(getTargetDbName(tableName), tableName.getTbl());
    if (privilege == Privilege.ANY) {
        registerPrivReq(new PrivilegeRequestBuilder().any().onAnyColumn(tableName.getDb(), tableName.getTbl()).toRequest());
    } else {
        registerPrivReq(new PrivilegeRequestBuilder().allOf(privilege).onTable(tableName.getDb(), tableName.getTbl()).toRequest());
    }
    // AnalysisExceptions.
    try {
        table = getTable(tableName.getDb(), tableName.getTbl());
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    Preconditions.checkNotNull(table);
    if (addAccessEvent) {
        // Add an audit event for this access
        TCatalogObjectType objectType = TCatalogObjectType.TABLE;
        if (table instanceof View)
            objectType = TCatalogObjectType.VIEW;
        globalState_.accessEvents.add(new TAccessEvent(tableName.toString(), objectType, privilege.toString()));
    }
    return table;
}
#method_after
public Table getTable(TableName tableName, Privilege privilege, boolean addAccessEvent) throws AnalysisException, TableLoadingException {
    Preconditions.checkNotNull(tableName);
    Preconditions.checkNotNull(privilege);
    Table table = null;
    tableName = new TableName(getTargetDbName(tableName), tableName.getTbl());
    if (privilege == Privilege.ANY) {
        registerPrivReq(new PrivilegeRequestBuilder().any().onAnyColumn(tableName.getDb(), tableName.getTbl()).toRequest());
    } else {
        registerPrivReq(new PrivilegeRequestBuilder().allOf(privilege).onTable(tableName.getDb(), tableName.getTbl()).toRequest());
    }
    table = getTable(tableName.getDb(), tableName.getTbl());
    Preconditions.checkNotNull(table);
    if (addAccessEvent) {
        // Add an audit event for this access
        TCatalogObjectType objectType = TCatalogObjectType.TABLE;
        if (table instanceof View)
            objectType = TCatalogObjectType.VIEW;
        globalState_.accessEvents.add(new TAccessEvent(tableName.toString(), objectType, privilege.toString()));
    }
    return table;
}
#end_block

#method_before
public Table getTable(TableName tableName, Privilege privilege) throws AnalysisException {
    return getTable(tableName, privilege, true);
}
#method_after
public Table getTable(TableName tableName, Privilege privilege) throws AnalysisException {
    // AnalysisExceptions.
    try {
        return getTable(tableName, privilege, true);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
}
#end_block

#method_before
public static String getCreateTableSql(Table table) throws CatalogException {
    Preconditions.checkNotNull(table);
    if (table instanceof View)
        return getCreateViewSql((View) table);
    org.apache.hadoop.hive.metastore.api.Table msTable = table.getMetaStoreTable();
    HashMap<String, String> properties = Maps.newHashMap(msTable.getParameters());
    if (properties.containsKey("transient_lastDdlTime")) {
        properties.remove("transient_lastDdlTime");
    }
    boolean isExternal = msTable.getTableType() != null && msTable.getTableType().equals(TableType.EXTERNAL_TABLE.toString());
    String comment = properties.get("comment");
    for (String hiddenProperty : HIDDEN_TABLE_PROPERTIES) {
        properties.remove(hiddenProperty);
    }
    ArrayList<String> colsSql = Lists.newArrayList();
    ArrayList<String> partitionColsSql = Lists.newArrayList();
    boolean isHbaseTable = table instanceof HBaseTable;
    for (int i = 0; i < table.getColumns().size(); i++) {
        if (!isHbaseTable && i < table.getNumClusteringCols()) {
            partitionColsSql.add(columnToSql(table.getColumns().get(i)));
        } else {
            colsSql.add(columnToSql(table.getColumns().get(i)));
        }
    }
    RowFormat rowFormat = RowFormat.fromStorageDescriptor(msTable.getSd());
    HdfsFileFormat format = HdfsFileFormat.fromHdfsInputFormatClass(msTable.getSd().getInputFormat());
    HdfsCompression compression = HdfsCompression.fromHdfsInputFormatClass(msTable.getSd().getInputFormat());
    String location = isHbaseTable ? null : msTable.getSd().getLocation();
    Map<String, String> serdeParameters = msTable.getSd().getSerdeInfo().getParameters();
    String storageHandlerClassName = table.getStorageHandlerClassName();
    List<String> primaryKeySql = Lists.newArrayList();
    String kuduPartitionByParams = null;
    if (table instanceof KuduTable) {
        KuduTable kuduTable = (KuduTable) table;
        // Kudu tables don't use LOCATION syntax
        location = null;
        format = HdfsFileFormat.KUDU;
        // Kudu tables cannot use the Hive DDL syntax for the storage handler
        storageHandlerClassName = null;
        properties.remove(KuduTable.KEY_STORAGE_HANDLER);
        String kuduTableName = properties.get(KuduTable.KEY_TABLE_NAME);
        Preconditions.checkNotNull(kuduTableName);
        if (kuduTableName.equals(KuduUtil.getDefaultCreateKuduTableName(table.getDb().getName(), table.getName()))) {
            properties.remove(KuduTable.KEY_TABLE_NAME);
        }
        // Internal property, should not be exposed to the user.
        properties.remove(StatsSetupConst.DO_NOT_UPDATE_STATS);
        if (!isExternal) {
            primaryKeySql.addAll(kuduTable.getPrimaryKeyColumnNames());
            List<String> paramsSql = Lists.newArrayList();
            for (PartitionParam param : kuduTable.getPartitionBy()) {
                paramsSql.add(param.toSql());
            }
            kuduPartitionByParams = Joiner.on(", ").join(paramsSql);
        } else {
            // We shouldn't output the columns for external tables
            colsSql = null;
        }
    }
    HdfsUri tableLocation = location == null ? null : new HdfsUri(location);
    return getCreateTableSql(table.getDb().getName(), table.getName(), comment, colsSql, partitionColsSql, primaryKeySql, kuduPartitionByParams, properties, serdeParameters, isExternal, false, rowFormat, format, compression, storageHandlerClassName, tableLocation);
}
#method_after
public static String getCreateTableSql(Table table) throws CatalogException {
    Preconditions.checkNotNull(table);
    if (table instanceof View)
        return getCreateViewSql((View) table);
    org.apache.hadoop.hive.metastore.api.Table msTable = table.getMetaStoreTable();
    HashMap<String, String> properties = Maps.newHashMap(msTable.getParameters());
    if (properties.containsKey("transient_lastDdlTime")) {
        properties.remove("transient_lastDdlTime");
    }
    boolean isExternal = msTable.getTableType() != null && msTable.getTableType().equals(TableType.EXTERNAL_TABLE.toString());
    String comment = properties.get("comment");
    for (String hiddenProperty : HIDDEN_TABLE_PROPERTIES) {
        properties.remove(hiddenProperty);
    }
    ArrayList<String> colsSql = Lists.newArrayList();
    ArrayList<String> partitionColsSql = Lists.newArrayList();
    boolean isHbaseTable = table instanceof HBaseTable;
    for (int i = 0; i < table.getColumns().size(); i++) {
        if (!isHbaseTable && i < table.getNumClusteringCols()) {
            partitionColsSql.add(columnToSql(table.getColumns().get(i)));
        } else {
            colsSql.add(columnToSql(table.getColumns().get(i)));
        }
    }
    RowFormat rowFormat = RowFormat.fromStorageDescriptor(msTable.getSd());
    HdfsFileFormat format = HdfsFileFormat.fromHdfsInputFormatClass(msTable.getSd().getInputFormat());
    HdfsCompression compression = HdfsCompression.fromHdfsInputFormatClass(msTable.getSd().getInputFormat());
    String location = isHbaseTable ? null : msTable.getSd().getLocation();
    Map<String, String> serdeParameters = msTable.getSd().getSerdeInfo().getParameters();
    String storageHandlerClassName = table.getStorageHandlerClassName();
    List<String> primaryKeySql = Lists.newArrayList();
    String kuduPartitionByParams = null;
    if (table instanceof KuduTable) {
        KuduTable kuduTable = (KuduTable) table;
        // Kudu tables don't use LOCATION syntax
        location = null;
        format = HdfsFileFormat.KUDU;
        // Kudu tables cannot use the Hive DDL syntax for the storage handler
        storageHandlerClassName = null;
        properties.remove(KuduTable.KEY_STORAGE_HANDLER);
        String kuduTableName = properties.get(KuduTable.KEY_TABLE_NAME);
        Preconditions.checkNotNull(kuduTableName);
        if (kuduTableName.equals(KuduUtil.getDefaultCreateKuduTableName(table.getDb().getName(), table.getName()))) {
            properties.remove(KuduTable.KEY_TABLE_NAME);
        }
        // Internal property, should not be exposed to the user.
        properties.remove(StatsSetupConst.DO_NOT_UPDATE_STATS);
        if (!isExternal) {
            primaryKeySql.addAll(kuduTable.getPrimaryKeyColumnNames());
            List<String> paramsSql = Lists.newArrayList();
            for (KuduPartitionParam param : kuduTable.getPartitionBy()) {
                paramsSql.add(param.toSql());
            }
            kuduPartitionByParams = Joiner.on(", ").join(paramsSql);
        } else {
            // We shouldn't output the columns for external tables
            colsSql = null;
        }
    }
    HdfsUri tableLocation = location == null ? null : new HdfsUri(location);
    return getCreateTableSql(table.getDb().getName(), table.getName(), comment, colsSql, partitionColsSql, primaryKeySql, kuduPartitionByParams, properties, serdeParameters, isExternal, false, rowFormat, format, compression, storageHandlerClassName, tableLocation);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support the (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE, KUDU"));
    }
    if (createStmt_.getFileFormat() == THdfsFileFormat.KUDU && createStmt_.isExternal()) {
        // TODO: Add support for CTAS on external Kudu tables (see IMPALA-4318)
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT is not " + "supported for external Kudu tables."));
    }
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        // Subqueries need to be rewritten by the StmtRewriter first.
        if (analyzer.containsSubquery())
            return;
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the partition clause to the create statement.
    if (partitionKeys_ != null) {
        int colCnt = tmpQueryStmt.getColLabels().size();
        int partColCnt = partitionKeys_.size();
        if (partColCnt >= colCnt) {
            throw new AnalysisException(String.format("Number of partition columns (%s) " + "must be smaller than the number of columns in the select statement (%s).", partColCnt, colCnt));
        }
        int firstCol = colCnt - partColCnt;
        for (int i = firstCol, j = 0; i < colCnt; ++i, ++j) {
            String partitionLabel = partitionKeys_.get(j);
            String colLabel = tmpQueryStmt.getColLabels().get(i);
            // input column list.
            if (!partitionLabel.equals(colLabel)) {
                throw new AnalysisException(String.format("Partition column name " + "mismatch: %s != %s", partitionLabel, colLabel));
            }
            ColumnDef colDef = new ColumnDef(colLabel, null);
            colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
            createStmt_.getPartitionColumnDefs().add(colDef);
        }
        // Remove partition columns from table column list.
        tmpQueryStmt.getColLabels().subList(firstCol, colCnt).clear();
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    createStmt_.getColumnDefs().clear();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, Collections.<ColumnDef.Option, Object>emptyMap());
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    try (MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient()) {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        Table tmpTable = null;
        if (KuduTable.isKuduTable(msTbl)) {
            tmpTable = KuduTable.createCtasTarget(db, msTbl, createStmt_.getColumnDefs(), createStmt_.getTblPrimaryKeyColumnNames(), createStmt_.getPartitionParams());
        } else {
            // TODO: Creating a tmp table using load() is confusing.
            // Refactor it to use a 'createCtasTarget()' function similar to Kudu table.
            tmpTable = Table.fromMetastoreTable(db, msTbl);
            tmpTable.load(true, client.getHiveClient(), msTbl);
        }
        Preconditions.checkState(tmpTable != null && (tmpTable instanceof HdfsTable || tmpTable instanceof KuduTable));
        insertStmt_.setTargetTable(tmpTable);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support the (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE, KUDU"));
    }
    if (createStmt_.getFileFormat() == THdfsFileFormat.KUDU && createStmt_.isExternal()) {
        // TODO: Add support for CTAS on external Kudu tables (see IMPALA-4318)
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT is not " + "supported for external Kudu tables."));
    }
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        // Subqueries need to be rewritten by the StmtRewriter first.
        if (analyzer.containsSubquery())
            return;
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the partition clause to the create statement.
    if (partitionKeys_ != null) {
        int colCnt = tmpQueryStmt.getColLabels().size();
        int partColCnt = partitionKeys_.size();
        if (partColCnt >= colCnt) {
            throw new AnalysisException(String.format("Number of partition columns (%s) " + "must be smaller than the number of columns in the select statement (%s).", partColCnt, colCnt));
        }
        int firstCol = colCnt - partColCnt;
        for (int i = firstCol, j = 0; i < colCnt; ++i, ++j) {
            String partitionLabel = partitionKeys_.get(j);
            String colLabel = tmpQueryStmt.getColLabels().get(i);
            // input column list.
            if (!partitionLabel.equals(colLabel)) {
                throw new AnalysisException(String.format("Partition column name " + "mismatch: %s != %s", partitionLabel, colLabel));
            }
            ColumnDef colDef = new ColumnDef(colLabel, null);
            colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
            createStmt_.getPartitionColumnDefs().add(colDef);
        }
        // Remove partition columns from table column list.
        tmpQueryStmt.getColLabels().subList(firstCol, colCnt).clear();
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    createStmt_.getColumnDefs().clear();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, Collections.<ColumnDef.Option, Object>emptyMap());
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    try (MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient()) {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        Table tmpTable = null;
        if (KuduTable.isKuduTable(msTbl)) {
            tmpTable = KuduTable.createCtasTarget(db, msTbl, createStmt_.getColumnDefs(), createStmt_.getTblPrimaryKeyColumnNames(), createStmt_.getKuduPartitionParams());
        } else {
            // TODO: Creating a tmp table using load() is confusing.
            // Refactor it to use a 'createCtasTarget()' function similar to Kudu table.
            tmpTable = Table.fromMetastoreTable(db, msTbl);
            tmpTable.load(true, client.getHiveClient(), msTbl);
        }
        Preconditions.checkState(tmpTable != null && (tmpTable instanceof HdfsTable || tmpTable instanceof KuduTable));
        insertStmt_.setTargetTable(tmpTable);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#end_block

#method_before
public TCreateTableParams toThrift() {
    TCreateTableParams params = new TCreateTableParams();
    params.setTable_name(new TTableName(getDb(), getTbl()));
    List<org.apache.impala.thrift.TColumn> tColumns = Lists.newArrayList();
    for (ColumnDef col : getColumnDefs()) tColumns.add(col.toThrift());
    params.setColumns(tColumns);
    for (ColumnDef col : getPartitionColumnDefs()) {
        params.addToPartition_columns(col.toThrift());
    }
    params.setOwner(getOwner());
    params.setIs_external(isExternal());
    params.setComment(getComment());
    params.setLocation(getLocation() == null ? null : getLocation().toString());
    if (getCachingOp() != null)
        params.setCache_op(getCachingOp().toThrift());
    if (getRowFormat() != null)
        params.setRow_format(getRowFormat().toThrift());
    params.setFile_format(getFileFormat());
    params.setIf_not_exists(getIfNotExists());
    params.setTable_properties(getTblProperties());
    params.setSerde_properties(getSerdeProperties());
    for (PartitionParam d : getPartitionParams()) {
        params.addToPartition_by(d.toThrift());
    }
    for (ColumnDef pkColDef : getPrimaryKeyColumnDefs()) {
        params.addToPrimary_key_column_names(pkColDef.getColName());
    }
    return params;
}
#method_after
public TCreateTableParams toThrift() {
    TCreateTableParams params = new TCreateTableParams();
    params.setTable_name(new TTableName(getDb(), getTbl()));
    List<org.apache.impala.thrift.TColumn> tColumns = Lists.newArrayList();
    for (ColumnDef col : getColumnDefs()) tColumns.add(col.toThrift());
    params.setColumns(tColumns);
    for (ColumnDef col : getPartitionColumnDefs()) {
        params.addToPartition_columns(col.toThrift());
    }
    params.setOwner(getOwner());
    params.setIs_external(isExternal());
    params.setComment(getComment());
    params.setLocation(getLocation() == null ? null : getLocation().toString());
    if (getCachingOp() != null)
        params.setCache_op(getCachingOp().toThrift());
    if (getRowFormat() != null)
        params.setRow_format(getRowFormat().toThrift());
    params.setFile_format(getFileFormat());
    params.setIf_not_exists(getIfNotExists());
    params.setTable_properties(getTblProperties());
    params.setSerde_properties(getSerdeProperties());
    for (KuduPartitionParam d : getKuduPartitionParams()) {
        params.addToPartition_by(d.toThrift());
    }
    for (ColumnDef pkColDef : getPrimaryKeyColumnDefs()) {
        params.addToPrimary_key_column_names(pkColDef.getColName());
    }
    return params;
}
#end_block

#method_before
private void analyzeKuduFormat(Analyzer analyzer) throws AnalysisException {
    if (getFileFormat() != THdfsFileFormat.KUDU) {
        if (KuduTable.KUDU_STORAGE_HANDLER.equals(getTblProperties().get(KuduTable.KEY_STORAGE_HANDLER))) {
            throw new AnalysisException(KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
        }
        AnalysisUtils.throwIfNotEmpty(getPartitionParams(), "Only Kudu tables can use the PARTITION BY clause.");
        if (hasPrimaryKey()) {
            throw new AnalysisException("Only Kudu tables can specify a PRIMARY KEY.");
        }
        return;
    }
    analyzeKuduTableProperties(analyzer);
    if (isExternal()) {
        analyzeExternalKuduTableParams(analyzer);
    } else {
        analyzeManagedKuduTableParams(analyzer);
    }
}
#method_after
private void analyzeKuduFormat(Analyzer analyzer) throws AnalysisException {
    if (getFileFormat() != THdfsFileFormat.KUDU) {
        if (KuduTable.KUDU_STORAGE_HANDLER.equals(getTblProperties().get(KuduTable.KEY_STORAGE_HANDLER))) {
            throw new AnalysisException(KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
        }
        AnalysisUtils.throwIfNotEmpty(getKuduPartitionParams(), "Only Kudu tables can use the PARTITION BY clause.");
        if (hasPrimaryKey()) {
            throw new AnalysisException("Only Kudu tables can specify a PRIMARY KEY.");
        }
        return;
    }
    analyzeKuduTableProperties(analyzer);
    if (isExternal()) {
        analyzeExternalKuduTableParams(analyzer);
    } else {
        analyzeManagedKuduTableParams(analyzer);
    }
}
#end_block

#method_before
private void analyzeExternalKuduTableParams(Analyzer analyzer) throws AnalysisException {
    if (analyzer.getAuthzConfig().isEnabled()) {
        // Today there is no comprehensive way of enforcing a Sentry authorization policy
        // against tables stored in Kudu. This is why only users with ALL privileges on
        // SERVER may create external Kudu tables. See IMPALA-4000 for details.
        String authzServer = analyzer.getAuthzConfig().getServerName();
        Preconditions.checkNotNull(authzServer);
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onServer(authzServer).all().toRequest());
    }
    AnalysisUtils.throwIfNull(getTblProperties().get(KuduTable.KEY_TABLE_NAME), String.format("Table property %s must be specified when creating " + "an external Kudu table.", KuduTable.KEY_TABLE_NAME));
    if (hasPrimaryKey() || getTblProperties().containsKey(KuduTable.KEY_KEY_COLUMNS)) {
        throw new AnalysisException("Primary keys cannot be specified for an external " + "Kudu table");
    }
    AnalysisUtils.throwIfNotNull(getTblProperties().get(KuduTable.KEY_TABLET_REPLICAS), String.format("Table property '%s' cannot be used with an external Kudu table.", KuduTable.KEY_TABLET_REPLICAS));
    AnalysisUtils.throwIfNotEmpty(getColumnDefs(), "Columns cannot be specified with an external Kudu table.");
    AnalysisUtils.throwIfNotEmpty(getPartitionParams(), "PARTITION BY cannot be used with an external Kudu table.");
}
#method_after
private void analyzeExternalKuduTableParams(Analyzer analyzer) throws AnalysisException {
    if (analyzer.getAuthzConfig().isEnabled()) {
        // Today there is no comprehensive way of enforcing a Sentry authorization policy
        // against tables stored in Kudu. This is why only users with ALL privileges on
        // SERVER may create external Kudu tables. See IMPALA-4000 for details.
        String authzServer = analyzer.getAuthzConfig().getServerName();
        Preconditions.checkNotNull(authzServer);
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onServer(authzServer).all().toRequest());
    }
    AnalysisUtils.throwIfNull(getTblProperties().get(KuduTable.KEY_TABLE_NAME), String.format("Table property %s must be specified when creating " + "an external Kudu table.", KuduTable.KEY_TABLE_NAME));
    if (hasPrimaryKey() || getTblProperties().containsKey(KuduTable.KEY_KEY_COLUMNS)) {
        throw new AnalysisException("Primary keys cannot be specified for an external " + "Kudu table");
    }
    AnalysisUtils.throwIfNotNull(getTblProperties().get(KuduTable.KEY_TABLET_REPLICAS), String.format("Table property '%s' cannot be used with an external Kudu table.", KuduTable.KEY_TABLET_REPLICAS));
    AnalysisUtils.throwIfNotEmpty(getColumnDefs(), "Columns cannot be specified with an external Kudu table.");
    AnalysisUtils.throwIfNotEmpty(getKuduPartitionParams(), "PARTITION BY cannot be used with an external Kudu table.");
}
#end_block

#method_before
private void analyzeManagedKuduTableParams(Analyzer analyzer) throws AnalysisException {
    // current database as a prefix to avoid conflicts in Kudu.
    if (!getTblProperties().containsKey(KuduTable.KEY_TABLE_NAME)) {
        getTblProperties().put(KuduTable.KEY_TABLE_NAME, KuduUtil.getDefaultCreateKuduTableName(getDb(), getTbl()));
    }
    // Check column types are valid Kudu types
    for (ColumnDef col : getColumnDefs()) {
        try {
            KuduUtil.fromImpalaType(col.getType());
        } catch (ImpalaRuntimeException e) {
            throw new AnalysisException(String.format("Cannot create table '%s': %s", getTbl(), e.getMessage()));
        }
    }
    AnalysisUtils.throwIfNotNull(getTblProperties().get(KuduTable.KEY_KEY_COLUMNS), String.format("PRIMARY KEY must be used instead of the table property '%s'.", KuduTable.KEY_KEY_COLUMNS));
    if (!hasPrimaryKey()) {
        throw new AnalysisException("A primary key is required for a Kudu table.");
    }
    String tabletReplicas = getTblProperties().get(KuduTable.KEY_TABLET_REPLICAS);
    if (tabletReplicas != null) {
        Integer r = Ints.tryParse(tabletReplicas);
        if (r == null) {
            throw new AnalysisException(String.format("Table property '%s' must be an integer.", KuduTable.KEY_TABLET_REPLICAS));
        }
        if (r <= 0) {
            throw new AnalysisException("Number of tablet replicas must be greater than " + "zero. Given number of replicas is: " + r.toString());
        }
    }
    if (!getPartitionParams().isEmpty()) {
        analyzePartitionParams(analyzer);
    } else {
        throw new AnalysisException("Table partitioning must be specified for " + "managed Kudu tables.");
    }
}
#method_after
private void analyzeManagedKuduTableParams(Analyzer analyzer) throws AnalysisException {
    // current database as a prefix to avoid conflicts in Kudu.
    if (!getTblProperties().containsKey(KuduTable.KEY_TABLE_NAME)) {
        getTblProperties().put(KuduTable.KEY_TABLE_NAME, KuduUtil.getDefaultCreateKuduTableName(getDb(), getTbl()));
    }
    // Check column types are valid Kudu types
    for (ColumnDef col : getColumnDefs()) {
        try {
            KuduUtil.fromImpalaType(col.getType());
        } catch (ImpalaRuntimeException e) {
            throw new AnalysisException(String.format("Cannot create table '%s': %s", getTbl(), e.getMessage()));
        }
    }
    AnalysisUtils.throwIfNotNull(getTblProperties().get(KuduTable.KEY_KEY_COLUMNS), String.format("PRIMARY KEY must be used instead of the table property '%s'.", KuduTable.KEY_KEY_COLUMNS));
    if (!hasPrimaryKey()) {
        throw new AnalysisException("A primary key is required for a Kudu table.");
    }
    String tabletReplicas = getTblProperties().get(KuduTable.KEY_TABLET_REPLICAS);
    if (tabletReplicas != null) {
        Integer r = Ints.tryParse(tabletReplicas);
        if (r == null) {
            throw new AnalysisException(String.format("Table property '%s' must be an integer.", KuduTable.KEY_TABLET_REPLICAS));
        }
        if (r <= 0) {
            throw new AnalysisException("Number of tablet replicas must be greater than " + "zero. Given number of replicas is: " + r.toString());
        }
    }
    if (!getKuduPartitionParams().isEmpty()) {
        analyzeKuduPartitionParams(analyzer);
    } else {
        throw new AnalysisException("Table partitioning must be specified for " + "managed Kudu tables.");
    }
}
#end_block

#method_before
@Test
public void TestDrop() throws AnalysisException {
    AnalyzesOk("drop database functional");
    AnalyzesOk("drop database functional cascade");
    AnalyzesOk("drop database functional restrict");
    AnalyzesOk("drop table functional.alltypes");
    AnalyzesOk("drop view functional.alltypes_view");
    // If the database does not exist, and the user hasn't specified "IF EXISTS",
    // an analysis error should be thrown
    AnalysisError("drop database db_does_not_exist", "Database does not exist: db_does_not_exist");
    AnalysisError("drop database db_does_not_exist cascade", "Database does not exist: db_does_not_exist");
    AnalysisError("drop database db_does_not_exist restrict", "Database does not exist: db_does_not_exist");
    AnalysisError("drop table db_does_not_exist.alltypes", "Database does not exist: db_does_not_exist");
    AnalysisError("drop view db_does_not_exist.alltypes_view", "Database does not exist: db_does_not_exist");
    // Invalid name reports non-existence instead of invalidity.
    AnalysisError("drop database `???`", "Database does not exist: ???");
    AnalysisError("drop database `???` cascade", "Database does not exist: ???");
    AnalysisError("drop database `???` restrict", "Database does not exist: ???");
    AnalysisError("drop table functional.`%^&`", "Table does not exist: functional.%^&");
    AnalysisError("drop view functional.`@#$%`", "Table does not exist: functional.@#$%");
    // If the database exist but the table doesn't, and the user hasn't specified
    // "IF EXISTS", an analysis error should be thrown
    AnalysisError("drop table functional.badtable", "Table does not exist: functional.badtable");
    AnalysisError("drop view functional.badview", "Table does not exist: functional.badview");
    // No error is thrown if the user specifies IF EXISTS
    AnalyzesOk("drop database if exists db_does_not_exist");
    AnalyzesOk("drop database if exists db_does_not_exist cascade");
    AnalyzesOk("drop database if exists db_does_not_exist restrict");
    // No error is thrown if the database does not exist
    AnalyzesOk("drop table if exists db_does_not_exist.alltypes");
    AnalyzesOk("drop view if exists db_does_not_exist.alltypes");
    // No error is thrown if the database table does not exist and IF EXISTS
    // is true
    AnalyzesOk("drop table if exists functional.notbl");
    AnalyzesOk("drop view if exists functional.notbl");
    // Cannot drop a view with DROP TABLE.
    AnalysisError("drop table functional.alltypes_view", "DROP TABLE not allowed on a view: functional.alltypes_view");
    // Cannot drop a table with DROP VIEW.
    AnalysisError("drop view functional.alltypes", "DROP VIEW not allowed on a table: functional.alltypes");
}
#method_after
@Test
public void TestDrop() throws AnalysisException {
    AnalyzesOk("drop database functional");
    AnalyzesOk("drop database functional cascade");
    AnalyzesOk("drop database functional restrict");
    AnalyzesOk("drop table functional.alltypes");
    AnalyzesOk("drop view functional.alltypes_view");
    // If the database does not exist, and the user hasn't specified "IF EXISTS",
    // an analysis error should be thrown
    AnalysisError("drop database db_does_not_exist", "Database does not exist: db_does_not_exist");
    AnalysisError("drop database db_does_not_exist cascade", "Database does not exist: db_does_not_exist");
    AnalysisError("drop database db_does_not_exist restrict", "Database does not exist: db_does_not_exist");
    AnalysisError("drop table db_does_not_exist.alltypes", "Database does not exist: db_does_not_exist");
    AnalysisError("drop view db_does_not_exist.alltypes_view", "Database does not exist: db_does_not_exist");
    // Invalid name reports non-existence instead of invalidity.
    AnalysisError("drop database `???`", "Database does not exist: ???");
    AnalysisError("drop database `???` cascade", "Database does not exist: ???");
    AnalysisError("drop database `???` restrict", "Database does not exist: ???");
    AnalysisError("drop table functional.`%^&`", "Table does not exist: functional.%^&");
    AnalysisError("drop view functional.`@#$%`", "Table does not exist: functional.@#$%");
    // If the database exist but the table doesn't, and the user hasn't specified
    // "IF EXISTS", an analysis error should be thrown
    AnalysisError("drop table functional.badtable", "Table does not exist: functional.badtable");
    AnalysisError("drop view functional.badview", "Table does not exist: functional.badview");
    // No error is thrown if the user specifies IF EXISTS
    AnalyzesOk("drop database if exists db_does_not_exist");
    AnalyzesOk("drop database if exists db_does_not_exist cascade");
    AnalyzesOk("drop database if exists db_does_not_exist restrict");
    // No error is thrown if the database does not exist
    AnalyzesOk("drop table if exists db_does_not_exist.alltypes");
    AnalyzesOk("drop view if exists db_does_not_exist.alltypes");
    // No error is thrown if the database table does not exist and IF EXISTS
    // is true
    AnalyzesOk("drop table if exists functional.notbl");
    AnalyzesOk("drop view if exists functional.notbl");
    // Cannot drop a view with DROP TABLE.
    AnalysisError("drop table functional.alltypes_view", "DROP TABLE not allowed on a view: functional.alltypes_view");
    // Cannot drop a table with DROP VIEW.
    AnalysisError("drop view functional.alltypes", "DROP VIEW not allowed on a table: functional.alltypes");
    // No analysis error for tables that can't be loaded.
    AnalyzesOk("drop table functional.unsupported_partition_types");
}
#end_block

#method_before
static TableDataLayout createPartitionedLayout(List<ColumnDef> partitionColumnDefs) {
    return new TableDataLayout(partitionColumnDefs, Lists.<PartitionParam>newArrayList());
}
#method_after
static TableDataLayout createPartitionedLayout(List<ColumnDef> partitionColumnDefs) {
    return new TableDataLayout(partitionColumnDefs, Lists.<KuduPartitionParam>newArrayList());
}
#end_block

#method_before
static TableDataLayout createEmptyLayout() {
    return new TableDataLayout(Lists.<ColumnDef>newArrayList(), Lists.<PartitionParam>newArrayList());
}
#method_after
static TableDataLayout createEmptyLayout() {
    return new TableDataLayout(Lists.<ColumnDef>newArrayList(), Lists.<KuduPartitionParam>newArrayList());
}
#end_block

#method_before
public List<PartitionParam> getPartitionBy() {
    return ImmutableList.copyOf(partitionBy_);
}
#method_after
public List<KuduPartitionParam> getPartitionBy() {
    return ImmutableList.copyOf(partitionBy_);
}
#end_block

#method_before
private PartitionParam getRangePartitioning() {
    for (PartitionParam partitionParam : partitionBy_) {
        if (partitionParam.getType() == PartitionParam.Type.RANGE) {
            return partitionParam;
        }
    }
    return null;
}
#method_after
private KuduPartitionParam getRangePartitioning() {
    for (KuduPartitionParam partitionParam : partitionBy_) {
        if (partitionParam.getType() == KuduPartitionParam.Type.RANGE) {
            return partitionParam;
        }
    }
    return null;
}
#end_block

#method_before
public List<String> getRangePartitioningColNames() {
    PartitionParam rangePartitioning = getRangePartitioning();
    if (rangePartitioning == null)
        return Collections.<String>emptyList();
    return rangePartitioning.getColumnNames();
}
#method_after
public List<String> getRangePartitioningColNames() {
    KuduPartitionParam rangePartitioning = getRangePartitioning();
    if (rangePartitioning == null)
        return Collections.<String>emptyList();
    return rangePartitioning.getColumnNames();
}
#end_block

#method_before
private void loadPartitionByParams(org.apache.kudu.client.KuduTable kuduTable) {
    Preconditions.checkNotNull(kuduTable);
    Schema tableSchema = kuduTable.getSchema();
    PartitionSchema partitionSchema = kuduTable.getPartitionSchema();
    Preconditions.checkState(!colsByPos_.isEmpty());
    partitionBy_.clear();
    for (HashBucketSchema hashBucketSchema : partitionSchema.getHashBucketSchemas()) {
        List<String> columnNames = Lists.newArrayList();
        for (int colId : hashBucketSchema.getColumnIds()) {
            columnNames.add(getColumnNameById(tableSchema, colId));
        }
        partitionBy_.add(PartitionParam.createHashParam(columnNames, hashBucketSchema.getNumBuckets()));
    }
    RangeSchema rangeSchema = partitionSchema.getRangeSchema();
    List<Integer> columnIds = rangeSchema.getColumns();
    if (columnIds.isEmpty())
        return;
    List<String> columnNames = Lists.newArrayList();
    for (int colId : columnIds) columnNames.add(getColumnNameById(tableSchema, colId));
    // We don't populate the split values because Kudu's API doesn't currently support
    // retrieving the split values for range partitions.
    // TODO: File a Kudu JIRA.
    partitionBy_.add(PartitionParam.createRangeParam(columnNames, null));
}
#method_after
private void loadPartitionByParams(org.apache.kudu.client.KuduTable kuduTable) {
    Preconditions.checkNotNull(kuduTable);
    Schema tableSchema = kuduTable.getSchema();
    PartitionSchema partitionSchema = kuduTable.getPartitionSchema();
    Preconditions.checkState(!colsByPos_.isEmpty());
    partitionBy_.clear();
    for (HashBucketSchema hashBucketSchema : partitionSchema.getHashBucketSchemas()) {
        List<String> columnNames = Lists.newArrayList();
        for (int colId : hashBucketSchema.getColumnIds()) {
            columnNames.add(getColumnNameById(tableSchema, colId));
        }
        partitionBy_.add(KuduPartitionParam.createHashParam(columnNames, hashBucketSchema.getNumBuckets()));
    }
    RangeSchema rangeSchema = partitionSchema.getRangeSchema();
    List<Integer> columnIds = rangeSchema.getColumns();
    if (columnIds.isEmpty())
        return;
    List<String> columnNames = Lists.newArrayList();
    for (int colId : columnIds) columnNames.add(getColumnNameById(tableSchema, colId));
    // We don't populate the split values because Kudu's API doesn't currently support
    // retrieving the split values for range partitions.
    // TODO: File a Kudu JIRA.
    partitionBy_.add(KuduPartitionParam.createRangeParam(columnNames, null));
}
#end_block

#method_before
public static KuduTable createCtasTarget(Db db, org.apache.hadoop.hive.metastore.api.Table msTbl, List<ColumnDef> columnDefs, List<String> primaryKeyColumnNames, List<PartitionParam> partitionParams) {
    KuduTable tmpTable = new KuduTable(msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    int pos = 0;
    for (ColumnDef colDef : columnDefs) {
        tmpTable.addColumn(new Column(colDef.getColName(), colDef.getType(), pos++));
    }
    tmpTable.primaryKeyColumnNames_.addAll(primaryKeyColumnNames);
    tmpTable.partitionBy_.addAll(partitionParams);
    return tmpTable;
}
#method_after
public static KuduTable createCtasTarget(Db db, org.apache.hadoop.hive.metastore.api.Table msTbl, List<ColumnDef> columnDefs, List<String> primaryKeyColumnNames, List<KuduPartitionParam> partitionParams) {
    KuduTable tmpTable = new KuduTable(msTbl, db, msTbl.getTableName(), msTbl.getOwner());
    int pos = 0;
    for (ColumnDef colDef : columnDefs) {
        tmpTable.addColumn(new Column(colDef.getColName(), colDef.getType(), pos++));
    }
    tmpTable.primaryKeyColumnNames_.addAll(primaryKeyColumnNames);
    tmpTable.partitionBy_.addAll(partitionParams);
    return tmpTable;
}
#end_block

#method_before
private void loadPartitionByParamsFromThrift(List<TPartitionParam> params) {
    partitionBy_.clear();
    for (TPartitionParam param : params) {
        if (param.isSetBy_hash_param()) {
            TPartitionByHashParam hashParam = param.getBy_hash_param();
            partitionBy_.add(PartitionParam.createHashParam(hashParam.getColumns(), hashParam.getNum_buckets()));
        } else {
            Preconditions.checkState(param.isSetBy_range_param());
            TPartitionByRangeParam rangeParam = param.getBy_range_param();
            partitionBy_.add(PartitionParam.createRangeParam(rangeParam.getColumns(), null));
        }
    }
}
#method_after
private void loadPartitionByParamsFromThrift(List<TKuduPartitionParam> params) {
    partitionBy_.clear();
    for (TKuduPartitionParam param : params) {
        if (param.isSetBy_hash_param()) {
            TKuduPartitionByHashParam hashParam = param.getBy_hash_param();
            partitionBy_.add(KuduPartitionParam.createHashParam(hashParam.getColumns(), hashParam.getNum_buckets()));
        } else {
            Preconditions.checkState(param.isSetBy_range_param());
            TKuduPartitionByRangeParam rangeParam = param.getBy_range_param();
            partitionBy_.add(KuduPartitionParam.createRangeParam(rangeParam.getColumns(), null));
        }
    }
}
#end_block

#method_before
private TKuduTable getTKuduTable() {
    TKuduTable tbl = new TKuduTable();
    tbl.setKey_columns(Preconditions.checkNotNull(primaryKeyColumnNames_));
    tbl.setMaster_addresses(Lists.newArrayList(kuduMasters_.split(",")));
    tbl.setTable_name(kuduTableName_);
    Preconditions.checkNotNull(partitionBy_);
    for (PartitionParam partitionParam : partitionBy_) {
        tbl.addToPartition_by(partitionParam.toThrift());
    }
    return tbl;
}
#method_after
private TKuduTable getTKuduTable() {
    TKuduTable tbl = new TKuduTable();
    tbl.setKey_columns(Preconditions.checkNotNull(primaryKeyColumnNames_));
    tbl.setMaster_addresses(Lists.newArrayList(kuduMasters_.split(",")));
    tbl.setTable_name(kuduTableName_);
    Preconditions.checkNotNull(partitionBy_);
    for (KuduPartitionParam partitionParam : partitionBy_) {
        tbl.addToPartition_by(partitionParam.toThrift());
    }
    return tbl;
}
#end_block

#method_before
public void analyze(Analyzer analyzer, List<ColumnDef> partitioningColDefs) throws AnalysisException {
    analyzeBoundaryValues(lowerBound_, partitioningColDefs, analyzer);
    if (!isSingletonRange_) {
        analyzeBoundaryValues(upperBound_, partitioningColDefs, analyzer);
    }
}
#method_after
public void analyze(Analyzer analyzer, List<ColumnDef> partColDefs) throws AnalysisException {
    analyzeBoundaryValues(lowerBound_, partColDefs, analyzer);
    if (!isSingletonRange_) {
        analyzeBoundaryValues(upperBound_, partColDefs, analyzer);
    }
}
#end_block

#method_before
private void analyzeBoundaryValues(List<Expr> boundaryValues, List<ColumnDef> partitioningColDefs, Analyzer analyzer) throws AnalysisException {
    if (!boundaryValues.isEmpty() && boundaryValues.size() != partitioningColDefs.size()) {
        throw new AnalysisException(String.format("Number of specified range " + "partition values is different than the number of partitioning " + "columns: (%d vs %d). Range partition: '%s'", boundaryValues.size(), partitioningColDefs.size(), toSql()));
    }
    for (int i = 0; i < boundaryValues.size(); ++i) {
        LiteralExpr literal = analyzeBoundaryValue(boundaryValues.get(i), partitioningColDefs.get(i), analyzer);
        Preconditions.checkNotNull(literal);
        boundaryValues.set(i, literal);
    }
}
#method_after
private void analyzeBoundaryValues(List<Expr> boundaryValues, List<ColumnDef> partColDefs, Analyzer analyzer) throws AnalysisException {
    if (!boundaryValues.isEmpty() && boundaryValues.size() != partColDefs.size()) {
        throw new AnalysisException(String.format("Number of specified range " + "partition values is different than the number of partitioning " + "columns: (%d vs %d). Range partition: '%s'", boundaryValues.size(), partColDefs.size(), toSql()));
    }
    for (int i = 0; i < boundaryValues.size(); ++i) {
        LiteralExpr literal = analyzeBoundaryValue(boundaryValues.get(i), partColDefs.get(i), analyzer);
        Preconditions.checkNotNull(literal);
        boundaryValues.set(i, literal);
    }
}
#end_block

#method_before
static void createManagedTable(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params) throws ImpalaRuntimeException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String kuduTableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Creating table '%s' in master '%s'", kuduTableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        // (see KUDU-1710).
        if (kudu.tableExists(kuduTableName)) {
            if (params.if_not_exists)
                return;
            throw new ImpalaRuntimeException(String.format("Table '%s' already exists in Kudu.", kuduTableName));
        }
        Schema schema = createTableSchema(params);
        CreateTableOptions tableOpts = buildTableOptions(msTbl, params, schema);
        kudu.createTable(kuduTableName, schema, tableOpts);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error creating Kudu table '%s'", kuduTableName), e);
    }
}
#method_after
static void createManagedTable(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params) throws ImpalaRuntimeException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String kuduTableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Creating table '%s' in master '%s'", kuduTableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        // (see KUDU-1710).
        if (kudu.tableExists(kuduTableName)) {
            if (params.if_not_exists)
                return;
            throw new ImpalaRuntimeException(String.format("Table '%s' already exists in Kudu.", kuduTableName));
        }
        Schema schema = createTableSchema(params);
        CreateTableOptions tableOpts = buildTableOptions(msTbl, params, schema);
        kudu.createTable(kuduTableName, schema, tableOpts);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error creating Kudu table '%s'", kuduTableName), e);
    }
}
#end_block

#method_before
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the partitioning schemes
    List<TPartitionParam> partitionParams = params.getPartition_by();
    if (partitionParams != null) {
        boolean hasRangePartitioning = false;
        for (TPartitionParam partParam : partitionParams) {
            if (partParam.isSetBy_hash_param()) {
                Preconditions.checkState(!partParam.isSetBy_range_param());
                tableOpts.addHashPartitions(partParam.getBy_hash_param().getColumns(), partParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(partParam.isSetBy_range_param());
                hasRangePartitioning = true;
                List<String> rangePartitionColumns = partParam.getBy_range_param().getColumns();
                tableOpts.setRangePartitionColumns(rangePartitionColumns);
                for (TRangePartition rangePartition : partParam.getBy_range_param().getRange_partitions()) {
                    List<Pair<PartialRow, RangePartitionBound>> rangeBounds = getRangePartitionBounds(rangePartition, schema, rangePartitionColumns);
                    Preconditions.checkState(rangeBounds.size() == 2);
                    Pair<PartialRow, RangePartitionBound> lowerBound = rangeBounds.get(0);
                    Pair<PartialRow, RangePartitionBound> upperBound = rangeBounds.get(1);
                    tableOpts.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        int parsedReplicas = -1;
        try {
            parsedReplicas = Integer.parseInt(replication);
            Preconditions.checkState(parsedReplicas > 0, "Invalid number of replicas table property:" + replication);
        } catch (Exception e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication));
        }
        tableOpts.setNumReplicas(parsedReplicas);
    }
    return tableOpts;
}
#method_after
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the partitioning schemes
    List<TKuduPartitionParam> partitionParams = params.getPartition_by();
    if (partitionParams != null) {
        boolean hasRangePartitioning = false;
        for (TKuduPartitionParam partParam : partitionParams) {
            if (partParam.isSetBy_hash_param()) {
                Preconditions.checkState(!partParam.isSetBy_range_param());
                tableOpts.addHashPartitions(partParam.getBy_hash_param().getColumns(), partParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(partParam.isSetBy_range_param());
                hasRangePartitioning = true;
                List<String> rangePartitionColumns = partParam.getBy_range_param().getColumns();
                tableOpts.setRangePartitionColumns(rangePartitionColumns);
                for (TRangePartition rangePartition : partParam.getBy_range_param().getRange_partitions()) {
                    List<Pair<PartialRow, RangePartitionBound>> rangeBounds = getRangePartitionBounds(rangePartition, schema, rangePartitionColumns);
                    Preconditions.checkState(rangeBounds.size() == 2);
                    Pair<PartialRow, RangePartitionBound> lowerBound = rangeBounds.get(0);
                    Pair<PartialRow, RangePartitionBound> upperBound = rangeBounds.get(1);
                    tableOpts.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        int parsedReplicas = -1;
        try {
            parsedReplicas = Integer.parseInt(replication);
            Preconditions.checkState(parsedReplicas > 0, "Invalid number of replicas table property:" + replication);
        } catch (Exception e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication));
        }
        tableOpts.setNumReplicas(parsedReplicas);
    }
    return tableOpts;
}
#end_block

#method_before
static void dropTable(org.apache.hadoop.hive.metastore.api.Table msTbl, boolean ifExists) throws ImpalaRuntimeException, TableNotFoundException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String tableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Dropping table '%s' from master '%s'", tableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        Preconditions.checkState(!Strings.isNullOrEmpty(tableName));
        // (see KUDU-1710).
        if (kudu.tableExists(tableName)) {
            kudu.deleteTable(tableName);
        } else if (!ifExists) {
            throw new TableNotFoundException(String.format("Table '%s' does not exist in Kudu master(s) '%s'.", tableName, masterHosts));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error dropping table '%s'", tableName), e);
    }
}
#method_after
static void dropTable(org.apache.hadoop.hive.metastore.api.Table msTbl, boolean ifExists) throws ImpalaRuntimeException, TableNotFoundException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String tableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Dropping table '%s' from master '%s'", tableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        Preconditions.checkState(!Strings.isNullOrEmpty(tableName));
        // (see KUDU-1710).
        if (kudu.tableExists(tableName)) {
            kudu.deleteTable(tableName);
        } else if (!ifExists) {
            throw new TableNotFoundException(String.format("Table '%s' does not exist in Kudu master(s) '%s'.", tableName, masterHosts));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error dropping table '%s'", tableName), e);
    }
}
#end_block

#method_before
public static void populateColumnsFromKudu(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    org.apache.hadoop.hive.metastore.api.Table msTblCopy = msTbl.deepCopy();
    List<FieldSchema> cols = msTblCopy.getSd().getCols();
    String kuduTableName = msTblCopy.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    String masterHosts = msTblCopy.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Loading schema of table '%s' from master '%s'", kuduTableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        if (!kudu.tableExists(kuduTableName)) {
            throw new ImpalaRuntimeException(String.format("Table does not exist in Kudu: " + "'%s'", kuduTableName));
        }
        org.apache.kudu.client.KuduTable kuduTable = kudu.openTable(kuduTableName);
        // Replace the columns in the Metastore table with the columns from the recently
        // accessed Kudu schema.
        cols.clear();
        for (ColumnSchema colSchema : kuduTable.getSchema().getColumns()) {
            Type type = KuduUtil.toImpalaType(colSchema.getType());
            cols.add(new FieldSchema(colSchema.getName(), type.toSql().toLowerCase(), null));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error loading schema of table " + "'%s'", kuduTableName), e);
    }
    List<FieldSchema> newCols = msTbl.getSd().getCols();
    newCols.clear();
    newCols.addAll(cols);
}
#method_after
public static void populateColumnsFromKudu(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    org.apache.hadoop.hive.metastore.api.Table msTblCopy = msTbl.deepCopy();
    List<FieldSchema> cols = msTblCopy.getSd().getCols();
    String kuduTableName = msTblCopy.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    String masterHosts = msTblCopy.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Loading schema of table '%s' from master '%s'", kuduTableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        if (!kudu.tableExists(kuduTableName)) {
            throw new ImpalaRuntimeException(String.format("Table does not exist in Kudu: " + "'%s'", kuduTableName));
        }
        org.apache.kudu.client.KuduTable kuduTable = kudu.openTable(kuduTableName);
        // Replace the columns in the Metastore table with the columns from the recently
        // accessed Kudu schema.
        cols.clear();
        for (ColumnSchema colSchema : kuduTable.getSchema().getColumns()) {
            Type type = KuduUtil.toImpalaType(colSchema.getType());
            cols.add(new FieldSchema(colSchema.getName(), type.toSql().toLowerCase(), null));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error loading schema of table " + "'%s'", kuduTableName), e);
    }
    List<FieldSchema> newCols = msTbl.getSd().getCols();
    newCols.clear();
    newCols.addAll(cols);
}
#end_block

#method_before
public static void renameTable(KuduTable tbl, String newName) throws ImpalaRuntimeException {
    Preconditions.checkState(!Strings.isNullOrEmpty(newName));
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    alterTableOptions.renameTable(newName);
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
    } catch (KuduException e) {
        throw new ImpalaRuntimeException(String.format("Error renaming Kudu table " + "%s to %s", tbl.getName(), newName), e);
    }
}
#method_after
public static void renameTable(KuduTable tbl, String newName) throws ImpalaRuntimeException {
    Preconditions.checkState(!Strings.isNullOrEmpty(newName));
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    alterTableOptions.renameTable(newName);
    String errMsg = String.format("Error renaming Kudu table " + "%s to %s", tbl.getKuduTableName(), newName);
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
        if (!client.isAlterTableDone(newName)) {
            throw new ImpalaRuntimeException(errMsg + ": Kudu operation timed out");
        }
    } catch (KuduException e) {
        throw new ImpalaRuntimeException(errMsg, e);
    }
}
#end_block

#method_before
public static void addDropRangePartition(KuduTable tbl, TAlterTableAddDropRangePartitionParams params) throws ImpalaRuntimeException {
    TRangePartition rangePartition = params.getRange_partition_spec();
    List<Pair<PartialRow, RangePartitionBound>> rangeBounds = getRangePartitionBounds(rangePartition, tbl);
    Preconditions.checkState(rangeBounds.size() == 2);
    Pair<PartialRow, RangePartitionBound> lowerBound = rangeBounds.get(0);
    Pair<PartialRow, RangePartitionBound> upperBound = rangeBounds.get(1);
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    TRangePartitionOperationType type = params.getType();
    if (type == TRangePartitionOperationType.ADD) {
        alterTableOptions.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
    } else {
        alterTableOptions.dropRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
    }
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
    } catch (KuduException e) {
        if (!params.isIgnore_errors()) {
            throw new ImpalaRuntimeException(String.format("Error %s range partition in " + "table %s", (type == TRangePartitionOperationType.ADD ? "adding" : "dropping"), tbl.getName()), e);
        }
    }
}
#method_after
public static void addDropRangePartition(KuduTable tbl, TAlterTableAddDropRangePartitionParams params) throws ImpalaRuntimeException {
    TRangePartition rangePartition = params.getRange_partition_spec();
    List<Pair<PartialRow, RangePartitionBound>> rangeBounds = getRangePartitionBounds(rangePartition, tbl);
    Preconditions.checkState(rangeBounds.size() == 2);
    Pair<PartialRow, RangePartitionBound> lowerBound = rangeBounds.get(0);
    Pair<PartialRow, RangePartitionBound> upperBound = rangeBounds.get(1);
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    TRangePartitionOperationType type = params.getType();
    if (type == TRangePartitionOperationType.ADD) {
        alterTableOptions.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
    } else {
        alterTableOptions.dropRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
    }
    String errMsg = String.format("Error %s range partition in " + "table %s", (type == TRangePartitionOperationType.ADD ? "adding" : "dropping"), tbl.getName());
    try {
        alterKuduTable(tbl, alterTableOptions, errMsg);
    } catch (ImpalaRuntimeException e) {
        if (!params.isIgnore_errors())
            throw e;
    }
}
#end_block

#method_before
public static void addColumn(KuduTable tbl, List<TColumn> columns) throws ImpalaRuntimeException {
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    for (TColumn column : columns) {
        Type type = Type.fromThrift(column.getColumnType());
        Preconditions.checkState(type != null);
        org.apache.kudu.Type kuduType = KuduUtil.fromImpalaType(type);
        boolean isNullable = !column.isSetIs_nullable() ? true : column.isIs_nullable();
        if (isNullable) {
            if (column.isSetDefault_value()) {
                // See KUDU-1747
                throw new ImpalaRuntimeException(String.format("Error adding nullable " + "column to Kudu table %s. Cannot specify a default value for a nullable " + "column", tbl.getKuduTableName()));
            }
            alterTableOptions.addNullableColumn(column.getColumnName(), kuduType);
        } else {
            Object defaultValue = null;
            if (column.isSetDefault_value()) {
                defaultValue = KuduUtil.getKuduDefaultValue(column.getDefault_value(), kuduType, column.getColumnName());
            }
            try {
                alterTableOptions.addColumn(column.getColumnName(), kuduType, defaultValue);
            } catch (IllegalArgumentException e) {
                // TODO: Remove this when KUDU-1747 is fixed
                throw new ImpalaRuntimeException("Error adding non-nullable column to " + "Kudu table " + tbl.getKuduTableName(), e);
            }
        }
    }
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
    } catch (KuduException e) {
        throw new ImpalaRuntimeException("Error adding columns to Kudu table " + tbl.getKuduTableName(), e);
    }
}
#method_after
public static void addColumn(KuduTable tbl, List<TColumn> columns) throws ImpalaRuntimeException {
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    for (TColumn column : columns) {
        Type type = Type.fromThrift(column.getColumnType());
        Preconditions.checkState(type != null);
        org.apache.kudu.Type kuduType = KuduUtil.fromImpalaType(type);
        boolean isNullable = !column.isSetIs_nullable() ? true : column.isIs_nullable();
        if (isNullable) {
            if (column.isSetDefault_value()) {
                // See KUDU-1747
                throw new ImpalaRuntimeException(String.format("Error adding nullable " + "column to Kudu table %s. Cannot specify a default value for a nullable " + "column", tbl.getKuduTableName()));
            }
            alterTableOptions.addNullableColumn(column.getColumnName(), kuduType);
        } else {
            Object defaultValue = null;
            if (column.isSetDefault_value()) {
                defaultValue = KuduUtil.getKuduDefaultValue(column.getDefault_value(), kuduType, column.getColumnName());
            }
            try {
                alterTableOptions.addColumn(column.getColumnName(), kuduType, defaultValue);
            } catch (IllegalArgumentException e) {
                // TODO: Remove this when KUDU-1747 is fixed
                throw new ImpalaRuntimeException("Error adding non-nullable column to " + "Kudu table " + tbl.getKuduTableName(), e);
            }
        }
    }
    String errMsg = "Error adding columns to Kudu table " + tbl.getName();
    alterKuduTable(tbl, alterTableOptions, errMsg);
}
#end_block

#method_before
public static void dropColumn(KuduTable tbl, String colName) throws ImpalaRuntimeException {
    Preconditions.checkState(!Strings.isNullOrEmpty(colName));
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    alterTableOptions.dropColumn(colName);
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
    } catch (KuduException e) {
        throw new ImpalaRuntimeException(String.format("Error dropping column %s from " + "Kudu table %s", colName, tbl.getName()), e);
    }
}
#method_after
public static void dropColumn(KuduTable tbl, String colName) throws ImpalaRuntimeException {
    Preconditions.checkState(!Strings.isNullOrEmpty(colName));
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    alterTableOptions.dropColumn(colName);
    String errMsg = String.format("Error dropping column %s from " + "Kudu table %s", colName, tbl.getName());
    alterKuduTable(tbl, alterTableOptions, errMsg);
}
#end_block

#method_before
public static void renameColumn(KuduTable tbl, String oldName, TColumn newCol) throws ImpalaRuntimeException {
    Preconditions.checkState(!Strings.isNullOrEmpty(oldName));
    Preconditions.checkNotNull(newCol);
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    alterTableOptions.renameColumn(oldName, newCol.getColumnName());
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
    } catch (KuduException e) {
        throw new ImpalaRuntimeException(String.format("Error renaming column %s to %s " + "for Kudu table %s", oldName, newCol.getColumnName(), tbl.getName()), e);
    }
}
#method_after
public static void renameColumn(KuduTable tbl, String oldName, TColumn newCol) throws ImpalaRuntimeException {
    Preconditions.checkState(!Strings.isNullOrEmpty(oldName));
    Preconditions.checkNotNull(newCol);
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    alterTableOptions.renameColumn(oldName, newCol.getColumnName());
    String errMsg = String.format("Error renaming column %s to %s " + "for Kudu table %s", oldName, newCol.getColumnName(), tbl.getName());
    alterKuduTable(tbl, alterTableOptions, errMsg);
}
#end_block

#method_before
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    List<String> rawPath = tableRef.getPath();
    Path resolvedPath = null;
    try {
        resolvedPath = resolvePath(tableRef.getPath(), PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath.size() > 1) {
                registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath.get(0), rawPath.get(1)).allOf(Privilege.SELECT).toRequest());
            }
            registerPrivReq(new PrivilegeRequestBuilder().onTable(getDefaultDb(), rawPath.get(0)).allOf(Privilege.SELECT).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath)), e);
    }
    Preconditions.checkNotNull(resolvedPath);
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof KuduTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef, resolvedPath);
    } else {
        return new CollectionTableRef(tableRef, resolvedPath);
    }
}
#method_after
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    List<String> rawPath = tableRef.getPath();
    Path resolvedPath = null;
    try {
        resolvedPath = resolvePath(tableRef.getPath(), PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath.size() > 1) {
                registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath.get(0), rawPath.get(1)).allOf(tableRef.getPrivilege()).toRequest());
            }
            registerPrivReq(new PrivilegeRequestBuilder().onTable(getDefaultDb(), rawPath.get(0)).allOf(tableRef.getPrivilege()).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath)), e);
    }
    Preconditions.checkNotNull(resolvedPath);
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof KuduTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef, resolvedPath);
    } else {
        return new CollectionTableRef(tableRef, resolvedPath);
    }
}
#end_block

#method_before
public void registerFullOuterJoinedConjunct(Expr e) {
    Preconditions.checkState(!globalState_.fullOuterJoinedConjuncts.containsKey(e.getId()));
    List<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    for (TupleId tid : tids) {
        if (!globalState_.fullOuterJoinedTupleIds.containsKey(tid))
            continue;
        TableRef currentOuterJoin = globalState_.fullOuterJoinedTupleIds.get(tid);
        globalState_.fullOuterJoinedConjuncts.put(e.getId(), currentOuterJoin);
        break;
    }
    LOG.trace("registerFullOuterJoinedConjunct: " + globalState_.fullOuterJoinedConjuncts.toString());
}
#method_after
public void registerFullOuterJoinedConjunct(Expr e) {
    Preconditions.checkState(!globalState_.fullOuterJoinedConjuncts.containsKey(e.getId()));
    List<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    for (TupleId tid : tids) {
        if (!globalState_.fullOuterJoinedTupleIds.containsKey(tid))
            continue;
        TableRef currentOuterJoin = globalState_.fullOuterJoinedTupleIds.get(tid);
        globalState_.fullOuterJoinedConjuncts.put(e.getId(), currentOuterJoin);
        break;
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerFullOuterJoinedConjunct: " + globalState_.fullOuterJoinedConjuncts.toString());
    }
}
#end_block

#method_before
public void registerFullOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.fullOuterJoinedTupleIds.put(tid, rhsRef);
    }
    LOG.trace("registerFullOuterJoinedTids: " + globalState_.fullOuterJoinedTupleIds.toString());
}
#method_after
public void registerFullOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.fullOuterJoinedTupleIds.put(tid, rhsRef);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerFullOuterJoinedTids: " + globalState_.fullOuterJoinedTupleIds.toString());
    }
}
#end_block

#method_before
public void registerOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.outerJoinedTupleIds.put(tid, rhsRef);
    }
    LOG.trace("registerOuterJoinedTids: " + globalState_.outerJoinedTupleIds.toString());
}
#method_after
public void registerOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.outerJoinedTupleIds.put(tid, rhsRef);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerOuterJoinedTids: " + globalState_.outerJoinedTupleIds.toString());
    }
}
#end_block

#method_before
private void registerConjunct(Expr e) {
    // always generate a new expr id; this might be a cloned conjunct that already
    // has the id of its origin set
    e.setId(globalState_.conjunctIdGenerator.getNextId());
    globalState_.conjuncts.put(e.getId(), e);
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    ArrayList<SlotId> slotIds = Lists.newArrayList();
    e.getIds(tupleIds, slotIds);
    registerFullOuterJoinedConjunct(e);
    // register single tid conjuncts
    if (tupleIds.size() == 1)
        globalState_.singleTidConjuncts.add(e.getId());
    LOG.trace("register tuple/slotConjunct: " + Integer.toString(e.getId().asInt()) + " " + e.toSql() + " " + e.debugString());
    if (!(e instanceof BinaryPredicate))
        return;
    BinaryPredicate binaryPred = (BinaryPredicate) e;
    // exactly one tuple id
    if (binaryPred.getOp() != BinaryPredicate.Operator.EQ && binaryPred.getOp() != BinaryPredicate.Operator.NULL_MATCHING_EQ && binaryPred.getOp() != BinaryPredicate.Operator.NOT_DISTINCT) {
        return;
    }
    // the binary predicate must refer to at least two tuples to be an eqJoinConjunct
    if (tupleIds.size() < 2)
        return;
    // examine children and update eqJoinConjuncts
    for (int i = 0; i < 2; ++i) {
        tupleIds = Lists.newArrayList();
        binaryPred.getChild(i).getIds(tupleIds, null);
        if (tupleIds.size() == 1) {
            if (!globalState_.eqJoinConjuncts.containsKey(tupleIds.get(0))) {
                List<ExprId> conjunctIds = Lists.newArrayList();
                conjunctIds.add(e.getId());
                globalState_.eqJoinConjuncts.put(tupleIds.get(0), conjunctIds);
            } else {
                globalState_.eqJoinConjuncts.get(tupleIds.get(0)).add(e.getId());
            }
            binaryPred.setIsEqJoinConjunct(true);
            LOG.trace("register eqJoinConjunct: " + Integer.toString(e.getId().asInt()));
        }
    }
}
#method_after
private void registerConjunct(Expr e) {
    // always generate a new expr id; this might be a cloned conjunct that already
    // has the id of its origin set
    e.setId(globalState_.conjunctIdGenerator.getNextId());
    globalState_.conjuncts.put(e.getId(), e);
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    ArrayList<SlotId> slotIds = Lists.newArrayList();
    e.getIds(tupleIds, slotIds);
    registerFullOuterJoinedConjunct(e);
    // register single tid conjuncts
    if (tupleIds.size() == 1)
        globalState_.singleTidConjuncts.add(e.getId());
    if (LOG.isTraceEnabled()) {
        LOG.trace("register tuple/slotConjunct: " + Integer.toString(e.getId().asInt()) + " " + e.toSql() + " " + e.debugString());
    }
    if (!(e instanceof BinaryPredicate))
        return;
    BinaryPredicate binaryPred = (BinaryPredicate) e;
    // exactly one tuple id
    if (binaryPred.getOp() != BinaryPredicate.Operator.EQ && binaryPred.getOp() != BinaryPredicate.Operator.NULL_MATCHING_EQ && binaryPred.getOp() != BinaryPredicate.Operator.NOT_DISTINCT) {
        return;
    }
    // the binary predicate must refer to at least two tuples to be an eqJoinConjunct
    if (tupleIds.size() < 2)
        return;
    // examine children and update eqJoinConjuncts
    for (int i = 0; i < 2; ++i) {
        tupleIds = Lists.newArrayList();
        binaryPred.getChild(i).getIds(tupleIds, null);
        if (tupleIds.size() == 1) {
            if (!globalState_.eqJoinConjuncts.containsKey(tupleIds.get(0))) {
                List<ExprId> conjunctIds = Lists.newArrayList();
                conjunctIds.add(e.getId());
                globalState_.eqJoinConjuncts.put(tupleIds.get(0), conjunctIds);
            } else {
                globalState_.eqJoinConjuncts.get(tupleIds.get(0)).add(e.getId());
            }
            binaryPred.setIsEqJoinConjunct(true);
            LOG.trace("register eqJoinConjunct: " + Integer.toString(e.getId().asInt()));
        }
    }
}
#end_block

#method_before
public void createAuxEquivPredicate(Expr lhs, Expr rhs) {
    // implicitly cast to a type different than NULL.
    if (lhs instanceof NullLiteral || rhs instanceof NullLiteral || lhs.getType().isNull() || rhs.getType().isNull()) {
        return;
    }
    // create an eq predicate between lhs and rhs
    BinaryPredicate p = new BinaryPredicate(BinaryPredicate.Operator.EQ, lhs, rhs);
    p.setIsAuxExpr();
    LOG.trace("register equiv predicate: " + p.toSql() + " " + p.debugString());
    registerConjunct(p);
}
#method_after
public void createAuxEquivPredicate(Expr lhs, Expr rhs) {
    // implicitly cast to a type different than NULL.
    if (lhs instanceof NullLiteral || rhs instanceof NullLiteral || lhs.getType().isNull() || rhs.getType().isNull()) {
        return;
    }
    // create an eq predicate between lhs and rhs
    BinaryPredicate p = new BinaryPredicate(BinaryPredicate.Operator.EQ, lhs, rhs);
    p.setIsAuxExpr();
    if (LOG.isTraceEnabled()) {
        LOG.trace("register equiv predicate: " + p.toSql() + " " + p.debugString());
    }
    registerConjunct(p);
}
#end_block

#method_before
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds, boolean inclOjConjuncts) {
    LOG.trace("getUnassignedConjuncts for " + Id.printIds(tupleIds));
    List<Expr> result = Lists.newArrayList();
    for (Expr e : globalState_.conjuncts.values()) {
        if (e.isBoundByTupleIds(tupleIds) && !e.isAuxExpr() && !globalState_.assignedConjuncts.contains(e.getId()) && ((inclOjConjuncts && !e.isConstant()) || !globalState_.ojClauseByConjunct.containsKey(e.getId()))) {
            result.add(e);
            LOG.trace("getUnassignedConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds, boolean inclOjConjuncts) {
    List<Expr> result = Lists.newArrayList();
    for (Expr e : globalState_.conjuncts.values()) {
        if (e.isBoundByTupleIds(tupleIds) && !e.isAuxExpr() && !globalState_.assignedConjuncts.contains(e.getId()) && ((inclOjConjuncts && !e.isConstant()) || !globalState_.ojClauseByConjunct.containsKey(e.getId()))) {
            result.add(e);
        }
    }
    return result;
}
#end_block

#method_before
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds) {
    LOG.trace("getUnassignedConjuncts for node with " + Id.printIds(tupleIds));
    List<Expr> result = Lists.newArrayList();
    for (Expr e : getUnassignedConjuncts(tupleIds, true)) {
        if (canEvalPredicate(tupleIds, e)) {
            result.add(e);
            LOG.trace("getUnassignedConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds) {
    List<Expr> result = Lists.newArrayList();
    for (Expr e : getUnassignedConjuncts(tupleIds, true)) {
        if (canEvalPredicate(tupleIds, e))
            result.add(e);
    }
    return result;
}
#end_block

#method_before
public List<Expr> getUnassignedOjConjuncts(TableRef ref) {
    Preconditions.checkState(ref.getJoinOp().isOuterJoin());
    List<Expr> result = Lists.newArrayList();
    List<ExprId> candidates = globalState_.conjunctsByOjClause.get(ref.getId());
    if (candidates == null)
        return result;
    for (ExprId conjunctId : candidates) {
        if (!globalState_.assignedConjuncts.contains(conjunctId)) {
            Expr e = globalState_.conjuncts.get(conjunctId);
            Preconditions.checkNotNull(e);
            result.add(e);
            LOG.trace("getUnassignedOjConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedOjConjuncts(TableRef ref) {
    Preconditions.checkState(ref.getJoinOp().isOuterJoin());
    List<Expr> result = Lists.newArrayList();
    List<ExprId> candidates = globalState_.conjunctsByOjClause.get(ref.getId());
    if (candidates == null)
        return result;
    for (ExprId conjunctId : candidates) {
        if (!globalState_.assignedConjuncts.contains(conjunctId)) {
            Expr e = globalState_.conjuncts.get(conjunctId);
            Preconditions.checkNotNull(e);
            result.add(e);
        }
    }
    return result;
}
#end_block

#method_before
public List<Expr> getEqJoinConjuncts(List<TupleId> lhsTblRefIds, List<TupleId> rhsTblRefIds) {
    // Contains all equi-join conjuncts that have one child fully bound by one of the
    // rhs table ref ids (the other child is not bound by that rhs table ref id).
    List<ExprId> conjunctIds = Lists.newArrayList();
    for (TupleId rhsId : rhsTblRefIds) {
        List<ExprId> cids = globalState_.eqJoinConjuncts.get(rhsId);
        if (cids == null)
            continue;
        for (ExprId eid : cids) {
            if (!conjunctIds.contains(eid))
                conjunctIds.add(eid);
        }
    }
    // Since we currently prevent join re-reordering across outer joins, we can never
    // have a bushy outer join with multiple rhs table ref ids. A busy outer join can
    // only be constructed with an inline view (which has a single table ref id).
    List<ExprId> ojClauseConjuncts = null;
    if (rhsTblRefIds.size() == 1) {
        ojClauseConjuncts = globalState_.conjunctsByOjClause.get(rhsTblRefIds.get(0));
    }
    // List of table ref ids that the join node will 'materialize'.
    List<TupleId> nodeTblRefIds = Lists.newArrayList(lhsTblRefIds);
    nodeTblRefIds.addAll(rhsTblRefIds);
    List<Expr> result = Lists.newArrayList();
    for (ExprId conjunctId : conjunctIds) {
        Expr e = globalState_.conjuncts.get(conjunctId);
        Preconditions.checkState(e != null);
        if (!canEvalFullOuterJoinedConjunct(e, nodeTblRefIds) || !canEvalAntiJoinedConjunct(e, nodeTblRefIds)) {
            continue;
        }
        if (ojClauseConjuncts != null && !ojClauseConjuncts.contains(conjunctId))
            continue;
        result.add(e);
    }
    return result;
}
#method_after
public List<Expr> getEqJoinConjuncts(List<TupleId> lhsTblRefIds, List<TupleId> rhsTblRefIds) {
    // Contains all equi-join conjuncts that have one child fully bound by one of the
    // rhs table ref ids (the other child is not bound by that rhs table ref id).
    List<ExprId> conjunctIds = Lists.newArrayList();
    for (TupleId rhsId : rhsTblRefIds) {
        List<ExprId> cids = globalState_.eqJoinConjuncts.get(rhsId);
        if (cids == null)
            continue;
        for (ExprId eid : cids) {
            if (!conjunctIds.contains(eid))
                conjunctIds.add(eid);
        }
    }
    // Since we currently prevent join re-reordering across outer joins, we can never
    // have a bushy outer join with multiple rhs table ref ids. A busy outer join can
    // only be constructed with an inline view (which has a single table ref id).
    List<ExprId> ojClauseConjuncts = null;
    if (rhsTblRefIds.size() == 1) {
        ojClauseConjuncts = globalState_.conjunctsByOjClause.get(rhsTblRefIds.get(0));
    }
    // List of table ref ids that the join node will 'materialize'.
    List<TupleId> nodeTblRefIds = Lists.newArrayList(lhsTblRefIds);
    nodeTblRefIds.addAll(rhsTblRefIds);
    List<Expr> result = Lists.newArrayList();
    for (ExprId conjunctId : conjunctIds) {
        Expr e = globalState_.conjuncts.get(conjunctId);
        Preconditions.checkState(e != null);
        if (!canEvalFullOuterJoinedConjunct(e, nodeTblRefIds) || !canEvalAntiJoinedConjunct(e, nodeTblRefIds) || !canEvalOuterJoinedConjunct(e, nodeTblRefIds)) {
            continue;
        }
        if (ojClauseConjuncts != null && !ojClauseConjuncts.contains(conjunctId))
            continue;
        result.add(e);
    }
    return result;
}
#end_block

#method_before
public boolean canEvalPredicate(List<TupleId> tupleIds, Expr e) {
    LOG.trace("canEval: " + e.toSql() + " " + e.debugString() + " " + Id.printIds(tupleIds));
    if (!e.isBoundByTupleIds(tupleIds))
        return false;
    ArrayList<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    if (tids.isEmpty())
        return true;
    if (e.isOnClauseConjunct()) {
        if (tids.size() > 1) {
            // assign it to this node.
            if (isAntiJoinedConjunct(e))
                return canEvalAntiJoinedConjunct(e, tupleIds);
            // it up later via getUnassignedOjConjuncts()
            if (globalState_.ojClauseByConjunct.containsKey(e.getId()))
                return false;
            // assigned below the full outer join node that outer-joined it.
            return canEvalFullOuterJoinedConjunct(e, tupleIds);
        }
        TupleId tid = tids.get(0);
        if (globalState_.ojClauseByConjunct.containsKey(e.getId())) {
            // (otherwise e needn't be true when that tuple is set)
            if (!globalState_.outerJoinedTupleIds.containsKey(tid))
                return false;
            if (globalState_.ojClauseByConjunct.get(e.getId()) != globalState_.outerJoinedTupleIds.get(tid)) {
                return false;
            }
            // Single tuple id conjuncts specified in the FOJ On-clause are not allowed to be
            // assigned below that full outer join in the operator tree.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(e.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                return false;
        } else {
            // Non-OJ On-clause conjunct.
            if (isOuterJoined(tid)) {
                // If the conjunct references an outer-joined tuple, then evaluate the
                // conjunct at the join that the On-clause belongs to.
                TableRef onClauseTableRef = globalState_.ijClauseByConjunct.get(e.getId());
                Preconditions.checkNotNull(onClauseTableRef);
                return tupleIds.containsAll(onClauseTableRef.getAllTableRefIds());
            }
            // can assign it to this node.
            if (isAntiJoinedConjunct(e))
                return canEvalAntiJoinedConjunct(e, tupleIds);
        }
        // operator tree.
        return canEvalFullOuterJoinedConjunct(e, tupleIds);
    }
    if (isAntiJoinedConjunct(e))
        return canEvalAntiJoinedConjunct(e, tupleIds);
    for (TupleId tid : tids) {
        LOG.trace("canEval: checking tid " + tid.toString());
        TableRef rhsRef = getLastOjClause(tid);
        // this is not outer-joined; ignore
        if (rhsRef == null)
            continue;
        // check whether the last join to outer-join 'tid' is materialized by tupleIds
        boolean contains = tupleIds.containsAll(rhsRef.getAllTableRefIds());
        LOG.trace("canEval: contains=" + (contains ? "true " : "false ") + Id.printIds(tupleIds) + " " + Id.printIds(rhsRef.getAllTableRefIds()));
        if (!tupleIds.containsAll(rhsRef.getAllTableRefIds()))
            return false;
    }
    return true;
}
#method_after
public boolean canEvalPredicate(List<TupleId> tupleIds, Expr e) {
    if (!e.isBoundByTupleIds(tupleIds))
        return false;
    ArrayList<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    if (tids.isEmpty())
        return true;
    if (e.isOnClauseConjunct()) {
        if (tids.size() > 1) {
            // assign it to this node.
            if (isAntiJoinedConjunct(e))
                return canEvalAntiJoinedConjunct(e, tupleIds);
            // it up later via getUnassignedOjConjuncts()
            if (globalState_.ojClauseByConjunct.containsKey(e.getId()))
                return false;
            // assigned below the full outer join node that outer-joined it.
            return canEvalFullOuterJoinedConjunct(e, tupleIds);
        }
        TupleId tid = tids.get(0);
        if (globalState_.ojClauseByConjunct.containsKey(e.getId())) {
            // (otherwise e needn't be true when that tuple is set)
            if (!globalState_.outerJoinedTupleIds.containsKey(tid))
                return false;
            if (globalState_.ojClauseByConjunct.get(e.getId()) != globalState_.outerJoinedTupleIds.get(tid)) {
                return false;
            }
            // Single tuple id conjuncts specified in the FOJ On-clause are not allowed to be
            // assigned below that full outer join in the operator tree.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(e.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                return false;
        } else {
            // Non-OJ On-clause conjunct.
            if (isOuterJoined(tid)) {
                // If the conjunct references an outer-joined tuple, then evaluate the
                // conjunct at the join that the On-clause belongs to.
                TableRef onClauseTableRef = globalState_.ijClauseByConjunct.get(e.getId());
                Preconditions.checkNotNull(onClauseTableRef);
                return tupleIds.containsAll(onClauseTableRef.getAllTableRefIds());
            }
            // can assign it to this node.
            if (isAntiJoinedConjunct(e))
                return canEvalAntiJoinedConjunct(e, tupleIds);
        }
        // operator tree.
        return canEvalFullOuterJoinedConjunct(e, tupleIds);
    }
    if (isAntiJoinedConjunct(e))
        return canEvalAntiJoinedConjunct(e, tupleIds);
    for (TupleId tid : tids) {
        TableRef rhsRef = getLastOjClause(tid);
        // this is not outer-joined; ignore
        if (rhsRef == null)
            continue;
        // check whether the last join to outer-join 'tid' is materialized by tupleIds
        if (!tupleIds.containsAll(rhsRef.getAllTableRefIds()))
            return false;
    }
    return true;
}
#end_block

#method_before
public ArrayList<Expr> getBoundPredicates(TupleId destTid, Set<SlotId> ignoreSlots, boolean markAssigned) {
    ArrayList<Expr> result = Lists.newArrayList();
    for (ExprId srcConjunctId : globalState_.singleTidConjuncts) {
        Expr srcConjunct = globalState_.conjuncts.get(srcConjunctId);
        if (srcConjunct instanceof SlotRef)
            continue;
        Preconditions.checkNotNull(srcConjunct);
        List<TupleId> srcTids = Lists.newArrayList();
        List<SlotId> srcSids = Lists.newArrayList();
        srcConjunct.getIds(srcTids, srcSids);
        Preconditions.checkState(srcTids.size() == 1);
        // Generate slot-mappings to bind srcConjunct to destTid.
        TupleId srcTid = srcTids.get(0);
        List<List<SlotId>> allDestSids = getEquivDestSlotIds(srcTid, srcSids, destTid, ignoreSlots);
        if (allDestSids.isEmpty())
            continue;
        // Indicates whether the source slots have equivalent slots that belong
        // to an outer-joined tuple.
        boolean hasOuterJoinedTuple = false;
        for (SlotId srcSid : srcSids) {
            if (hasOuterJoinedTuple(globalState_.equivClassBySlotId.get(srcSid))) {
                hasOuterJoinedTuple = true;
                break;
            }
        }
        // relative to 'srcConjunct'.
        if (hasOuterJoinedTuple && isTrueWithNullSlots(srcConjunct))
            continue;
        // (otherwise srcConjunct needn't be true when destTid is set)
        if (globalState_.ojClauseByConjunct.containsKey(srcConjunct.getId())) {
            if (!globalState_.outerJoinedTupleIds.containsKey(destTid))
                continue;
            if (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(destTid)) {
                continue;
            }
            // Do not propagate conjuncts from the on-clause of full-outer or anti-joins.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(srcConjunct.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                continue;
        }
        // join node.
        if (isAntiJoinedConjunct(srcConjunct))
            continue;
        // Generate predicates for all src-to-dest slot mappings.
        for (List<SlotId> destSids : allDestSids) {
            Preconditions.checkState(destSids.size() == srcSids.size());
            Expr p;
            if (srcSids.containsAll(destSids)) {
                p = srcConjunct;
            } else {
                ExprSubstitutionMap smap = new ExprSubstitutionMap();
                for (int i = 0; i < srcSids.size(); ++i) {
                    smap.put(new SlotRef(globalState_.descTbl.getSlotDesc(srcSids.get(i))), new SlotRef(globalState_.descTbl.getSlotDesc(destSids.get(i))));
                }
                try {
                    p = srcConjunct.trySubstitute(smap, this, false);
                } catch (ImpalaException exc) {
                    // not an executable predicate; ignore
                    continue;
                }
                // Unset the id because this bound predicate itself is not registered, and
                // to prevent callers from inadvertently marking the srcConjunct as assigned.
                p.setId(null);
                if (p instanceof BinaryPredicate)
                    ((BinaryPredicate) p).setIsInferred();
                LOG.trace("new pred: " + p.toSql() + " " + p.debugString());
            }
            if (markAssigned) {
                // predicate assignment doesn't hold if:
                // - the application against slotId doesn't transfer the value back to its
                // originating slot
                // - the original predicate is on an OJ'd table but doesn't originate from
                // that table's OJ clause's ON clause (if it comes from anywhere but that
                // ON clause, it needs to be evaluated directly by the join node that
                // materializes the OJ'd table)
                boolean reverseValueTransfer = true;
                for (int i = 0; i < srcSids.size(); ++i) {
                    if (!hasValueTransfer(destSids.get(i), srcSids.get(i))) {
                        reverseValueTransfer = false;
                        break;
                    }
                }
                // Check if either srcConjunct or the generated predicate needs to be evaluated
                // at a join node (IMPALA-2018/IMPALA-4379). The first condition is conservative
                // but takes into account that On-clause conjuncts can sometimes be legitimately
                // assigned below their originating join.
                boolean evalByJoin = (hasOuterJoinedTuple && !srcConjunct.isOnClauseConjunct_) || (evalByJoin(srcConjunct) && (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(srcTid))) || (evalByJoin(p) && (globalState_.ojClauseByConjunct.get(p.getId()) != globalState_.outerJoinedTupleIds.get(destTid)));
                // mark all bound predicates including duplicate ones
                if (reverseValueTransfer && !evalByJoin)
                    markConjunctAssigned(srcConjunct);
            }
            // check if we already created this predicate
            if (!result.contains(p))
                result.add(p);
        }
    }
    return result;
}
#method_after
public ArrayList<Expr> getBoundPredicates(TupleId destTid, Set<SlotId> ignoreSlots, boolean markAssigned) {
    ArrayList<Expr> result = Lists.newArrayList();
    for (ExprId srcConjunctId : globalState_.singleTidConjuncts) {
        Expr srcConjunct = globalState_.conjuncts.get(srcConjunctId);
        if (srcConjunct instanceof SlotRef)
            continue;
        Preconditions.checkNotNull(srcConjunct);
        List<TupleId> srcTids = Lists.newArrayList();
        List<SlotId> srcSids = Lists.newArrayList();
        srcConjunct.getIds(srcTids, srcSids);
        Preconditions.checkState(srcTids.size() == 1);
        // Generate slot-mappings to bind srcConjunct to destTid.
        TupleId srcTid = srcTids.get(0);
        List<List<SlotId>> allDestSids = getEquivDestSlotIds(srcTid, srcSids, destTid, ignoreSlots);
        if (allDestSids.isEmpty())
            continue;
        // Indicates whether the source slots have equivalent slots that belong
        // to an outer-joined tuple.
        boolean hasOuterJoinedTuple = false;
        for (SlotId srcSid : srcSids) {
            if (hasOuterJoinedTuple(globalState_.equivClassBySlotId.get(srcSid))) {
                hasOuterJoinedTuple = true;
                break;
            }
        }
        // relative to 'srcConjunct'.
        if (hasOuterJoinedTuple && isTrueWithNullSlots(srcConjunct))
            continue;
        // (otherwise srcConjunct needn't be true when destTid is set)
        if (globalState_.ojClauseByConjunct.containsKey(srcConjunct.getId())) {
            if (!globalState_.outerJoinedTupleIds.containsKey(destTid))
                continue;
            if (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(destTid)) {
                continue;
            }
            // Do not propagate conjuncts from the on-clause of full-outer or anti-joins.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(srcConjunct.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                continue;
        }
        // join node.
        if (isAntiJoinedConjunct(srcConjunct))
            continue;
        // Generate predicates for all src-to-dest slot mappings.
        for (List<SlotId> destSids : allDestSids) {
            Preconditions.checkState(destSids.size() == srcSids.size());
            Expr p;
            if (srcSids.containsAll(destSids)) {
                p = srcConjunct;
            } else {
                ExprSubstitutionMap smap = new ExprSubstitutionMap();
                for (int i = 0; i < srcSids.size(); ++i) {
                    smap.put(new SlotRef(globalState_.descTbl.getSlotDesc(srcSids.get(i))), new SlotRef(globalState_.descTbl.getSlotDesc(destSids.get(i))));
                }
                try {
                    p = srcConjunct.trySubstitute(smap, this, false);
                } catch (ImpalaException exc) {
                    // not an executable predicate; ignore
                    continue;
                }
                // Unset the id because this bound predicate itself is not registered, and
                // to prevent callers from inadvertently marking the srcConjunct as assigned.
                p.setId(null);
                if (p instanceof BinaryPredicate)
                    ((BinaryPredicate) p).setIsInferred();
                if (LOG.isTraceEnabled()) {
                    LOG.trace("new pred: " + p.toSql() + " " + p.debugString());
                }
            }
            if (markAssigned) {
                // predicate assignment doesn't hold if:
                // - the application against slotId doesn't transfer the value back to its
                // originating slot
                // - the original predicate is on an OJ'd table but doesn't originate from
                // that table's OJ clause's ON clause (if it comes from anywhere but that
                // ON clause, it needs to be evaluated directly by the join node that
                // materializes the OJ'd table)
                boolean reverseValueTransfer = true;
                for (int i = 0; i < srcSids.size(); ++i) {
                    if (!hasValueTransfer(destSids.get(i), srcSids.get(i))) {
                        reverseValueTransfer = false;
                        break;
                    }
                }
                // IMPALA-2018/4379: Check if srcConjunct or the generated predicate need to
                // be evaluated again at a later point in the plan, e.g., by a join that makes
                // referenced tuples nullable. The first condition is conservative but takes
                // into account that On-clause conjuncts can sometimes be legitimately assigned
                // below their originating join.
                boolean evalAfterJoin = (hasOuterJoinedTuple && !srcConjunct.isOnClauseConjunct_) || (evalAfterJoin(srcConjunct) && (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(srcTid))) || (evalAfterJoin(p) && (globalState_.ojClauseByConjunct.get(p.getId()) != globalState_.outerJoinedTupleIds.get(destTid)));
                // mark all bound predicates including duplicate ones
                if (reverseValueTransfer && !evalAfterJoin)
                    markConjunctAssigned(srcConjunct);
            }
            // check if we already created this predicate
            if (!result.contains(p))
                result.add(p);
        }
    }
    return result;
}
#end_block

#method_before
public List<SlotId> getEquivSlots(SlotId slotId, List<TupleId> tupleIds) {
    List<SlotId> result = Lists.newArrayList();
    LOG.trace("getequivslots: slotid=" + Integer.toString(slotId.asInt()));
    EquivalenceClassId classId = globalState_.equivClassBySlotId.get(slotId);
    for (SlotId memberId : globalState_.equivClassMembers.get(classId)) {
        if (tupleIds.contains(globalState_.descTbl.getSlotDesc(memberId).getParent().getId())) {
            result.add(memberId);
        }
    }
    return result;
}
#method_after
public List<SlotId> getEquivSlots(SlotId slotId, List<TupleId> tupleIds) {
    List<SlotId> result = Lists.newArrayList();
    EquivalenceClassId classId = globalState_.equivClassBySlotId.get(slotId);
    for (SlotId memberId : globalState_.equivClassMembers.get(classId)) {
        if (tupleIds.contains(globalState_.descTbl.getSlotDesc(memberId).getParent().getId())) {
            result.add(memberId);
        }
    }
    return result;
}
#end_block

#method_before
public void markConjunctsAssigned(List<Expr> conjuncts) {
    if (conjuncts == null)
        return;
    for (Expr p : conjuncts) {
        globalState_.assignedConjuncts.add(p.getId());
        LOG.trace("markAssigned " + p.toSql() + " " + p.debugString());
    }
}
#method_after
public void markConjunctsAssigned(List<Expr> conjuncts) {
    if (conjuncts == null)
        return;
    for (Expr p : conjuncts) {
        globalState_.assignedConjuncts.add(p.getId());
    }
}
#end_block

#method_before
public void markConjunctAssigned(Expr conjunct) {
    LOG.trace("markAssigned " + conjunct.toSql() + " " + conjunct.debugString());
    globalState_.assignedConjuncts.add(conjunct.getId());
}
#method_after
public void markConjunctAssigned(Expr conjunct) {
    globalState_.assignedConjuncts.add(conjunct.getId());
}
#end_block

#method_before
public boolean hasUnassignedConjuncts() {
    for (ExprId id : globalState_.conjuncts.keySet()) {
        if (globalState_.assignedConjuncts.contains(id))
            continue;
        Expr e = globalState_.conjuncts.get(id);
        if (e.isAuxExpr())
            continue;
        LOG.trace("unassigned: " + e.toSql() + " " + e.debugString());
        return true;
    }
    return false;
}
#method_after
public boolean hasUnassignedConjuncts() {
    for (ExprId id : globalState_.conjuncts.keySet()) {
        if (globalState_.assignedConjuncts.contains(id))
            continue;
        Expr e = globalState_.conjuncts.get(id);
        if (e.isAuxExpr())
            continue;
        return true;
    }
    return false;
}
#end_block

#method_before
public Table getTable(TableName tableName, Privilege privilege, boolean addAccessEvent) throws AnalysisException {
    Preconditions.checkNotNull(tableName);
    Preconditions.checkNotNull(privilege);
    Table table = null;
    tableName = new TableName(getTargetDbName(tableName), tableName.getTbl());
    if (privilege == Privilege.ANY) {
        registerPrivReq(new PrivilegeRequestBuilder().any().onAnyColumn(tableName.getDb(), tableName.getTbl()).toRequest());
    } else {
        registerPrivReq(new PrivilegeRequestBuilder().allOf(privilege).onTable(tableName.getDb(), tableName.getTbl()).toRequest());
    }
    // AnalysisExceptions.
    try {
        table = getTable(tableName.getDb(), tableName.getTbl());
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    Preconditions.checkNotNull(table);
    if (addAccessEvent) {
        // Add an audit event for this access
        TCatalogObjectType objectType = TCatalogObjectType.TABLE;
        if (table instanceof View)
            objectType = TCatalogObjectType.VIEW;
        globalState_.accessEvents.add(new TAccessEvent(tableName.toString(), objectType, privilege.toString()));
    }
    return table;
}
#method_after
public Table getTable(TableName tableName, Privilege privilege, boolean addAccessEvent) throws AnalysisException, TableLoadingException {
    Preconditions.checkNotNull(tableName);
    Preconditions.checkNotNull(privilege);
    Table table = null;
    tableName = new TableName(getTargetDbName(tableName), tableName.getTbl());
    if (privilege == Privilege.ANY) {
        registerPrivReq(new PrivilegeRequestBuilder().any().onAnyColumn(tableName.getDb(), tableName.getTbl()).toRequest());
    } else {
        registerPrivReq(new PrivilegeRequestBuilder().allOf(privilege).onTable(tableName.getDb(), tableName.getTbl()).toRequest());
    }
    table = getTable(tableName.getDb(), tableName.getTbl());
    Preconditions.checkNotNull(table);
    if (addAccessEvent) {
        // Add an audit event for this access
        TCatalogObjectType objectType = TCatalogObjectType.TABLE;
        if (table instanceof View)
            objectType = TCatalogObjectType.VIEW;
        globalState_.accessEvents.add(new TAccessEvent(tableName.toString(), objectType, privilege.toString()));
    }
    return table;
}
#end_block

#method_before
public Table getTable(TableName tableName, Privilege privilege) throws AnalysisException {
    return getTable(tableName, privilege, true);
}
#method_after
public Table getTable(TableName tableName, Privilege privilege) throws AnalysisException {
    // AnalysisExceptions.
    try {
        return getTable(tableName, privilege, true);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
}
#end_block

#method_before
public void registerAuthAndAuditEvent(Table table, Analyzer analyzer) {
    // Add access event for auditing.
    if (table instanceof View) {
        View view = (View) table;
        Preconditions.checkState(!view.isLocalView());
        analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, Privilege.SELECT.toString()));
    } else {
        analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, Privilege.SELECT.toString()));
    }
    // Add privilege request.
    TableName tableName = table.getTableName();
    analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(Privilege.SELECT).toRequest());
}
#method_after
public void registerAuthAndAuditEvent(Table table, Privilege priv) {
    // Add access event for auditing.
    if (table instanceof View) {
        View view = (View) table;
        Preconditions.checkState(!view.isLocalView());
        addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, priv.toString()));
    } else {
        addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, priv.toString()));
    }
    // Add privilege request.
    TableName tableName = table.getTableName();
    registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(priv).toRequest());
}
#end_block

#method_before
public void computeValueTransfers() {
    long start = System.currentTimeMillis();
    // Step1: Compute complete subgraphs and get uni-directional value transfers.
    List<Pair<SlotId, SlotId>> origValueTransfers = Lists.newArrayList();
    partitionValueTransfers(completeSubGraphs_, origValueTransfers);
    // Coalesce complete subgraphs into a single slot and assign new slot ids.
    coalescedSlots_ = new int[numSlots_];
    Arrays.fill(coalescedSlots_, -1);
    for (Set<SlotId> equivClass : completeSubGraphs_.getSets()) {
        int representative = nextCoalescedSlotId_;
        for (SlotId slotId : equivClass) {
            coalescedSlots_[slotId.asInt()] = representative;
        }
        ++nextCoalescedSlotId_;
    }
    // Step 2: Map uni-directional value transfers onto the new slot domain, and
    // store the connected components in graphPartitions.
    List<Pair<Integer, Integer>> coalescedValueTransfers = Lists.newArrayList();
    // A graph partition is a set of slot ids that are connected by uni-directional
    // value transfers. The graph corresponding to a graph partition is a DAG.
    DisjointSet<Integer> graphPartitions = new DisjointSet<Integer>();
    mapSlots(origValueTransfers, coalescedValueTransfers, graphPartitions);
    mapSlots(globalState_.registeredValueTransfers, coalescedValueTransfers, graphPartitions);
    // Step 3: Group the coalesced value transfers by the graph partition they
    // belong to. Maps from the graph partition to its list of value transfers.
    // TODO: Implement a specialized DisjointSet data structure to avoid this step.
    Map<Set<Integer>, List<Pair<Integer, Integer>>> partitionedValueTransfers = Maps.newHashMap();
    for (Pair<Integer, Integer> vt : coalescedValueTransfers) {
        Set<Integer> partition = graphPartitions.get(vt.first.intValue());
        List<Pair<Integer, Integer>> l = partitionedValueTransfers.get(partition);
        if (l == null) {
            l = Lists.newArrayList();
            partitionedValueTransfers.put(partition, l);
        }
        l.add(vt);
    }
    // Initialize the value transfer graph.
    int numCoalescedSlots = nextCoalescedSlotId_ + 1;
    valueTransfer_ = new boolean[numCoalescedSlots][numCoalescedSlots];
    for (int i = 0; i < numCoalescedSlots; ++i) {
        valueTransfer_[i][i] = true;
    }
    // Step 4: Compute the transitive closure for each graph partition.
    for (Map.Entry<Set<Integer>, List<Pair<Integer, Integer>>> graphPartition : partitionedValueTransfers.entrySet()) {
        // Set value transfers of this partition.
        for (Pair<Integer, Integer> vt : graphPartition.getValue()) {
            valueTransfer_[vt.first][vt.second] = true;
        }
        Set<Integer> partitionSlotIds = graphPartition.getKey();
        // No transitive value transfers.
        if (partitionSlotIds.size() <= 2)
            continue;
        // Indirection vector into valueTransfer_. Contains one entry for each distinct
        // slot id referenced in a value transfer of this partition.
        int[] p = new int[partitionSlotIds.size()];
        int numPartitionSlots = 0;
        for (Integer slotId : partitionSlotIds) {
            p[numPartitionSlots++] = slotId;
        }
        // Compute the transitive closure of this graph partition.
        // TODO: Since we are operating on a DAG the performance can be improved if
        // necessary (e.g., topological sort + backwards propagation of the transitive
        // closure).
        boolean changed = false;
        do {
            changed = false;
            for (int i = 0; i < numPartitionSlots; ++i) {
                for (int j = 0; j < numPartitionSlots; ++j) {
                    for (int k = 0; k < numPartitionSlots; ++k) {
                        if (valueTransfer_[p[i]][p[j]] && valueTransfer_[p[j]][p[k]] && !valueTransfer_[p[i]][p[k]]) {
                            valueTransfer_[p[i]][p[k]] = true;
                            changed = true;
                        }
                    }
                }
            }
        } while (changed);
    }
    long end = System.currentTimeMillis();
    LOG.trace("Time taken in computeValueTransfers(): " + (end - start) + "ms");
}
#method_after
public void computeValueTransfers() {
    long start = System.currentTimeMillis();
    // Step1: Compute complete subgraphs and get uni-directional value transfers.
    List<Pair<SlotId, SlotId>> origValueTransfers = Lists.newArrayList();
    partitionValueTransfers(completeSubGraphs_, origValueTransfers);
    // Coalesce complete subgraphs into a single slot and assign new slot ids.
    coalescedSlots_ = new int[numSlots_];
    Arrays.fill(coalescedSlots_, -1);
    for (Set<SlotId> equivClass : completeSubGraphs_.getSets()) {
        int representative = nextCoalescedSlotId_;
        for (SlotId slotId : equivClass) {
            coalescedSlots_[slotId.asInt()] = representative;
        }
        ++nextCoalescedSlotId_;
    }
    // Step 2: Map uni-directional value transfers onto the new slot domain, and
    // store the connected components in graphPartitions.
    List<Pair<Integer, Integer>> coalescedValueTransfers = Lists.newArrayList();
    // A graph partition is a set of slot ids that are connected by uni-directional
    // value transfers. The graph corresponding to a graph partition is a DAG.
    DisjointSet<Integer> graphPartitions = new DisjointSet<Integer>();
    mapSlots(origValueTransfers, coalescedValueTransfers, graphPartitions);
    mapSlots(globalState_.registeredValueTransfers, coalescedValueTransfers, graphPartitions);
    // Step 3: Group the coalesced value transfers by the graph partition they
    // belong to. Maps from the graph partition to its list of value transfers.
    // TODO: Implement a specialized DisjointSet data structure to avoid this step.
    Map<Set<Integer>, List<Pair<Integer, Integer>>> partitionedValueTransfers = Maps.newHashMap();
    for (Pair<Integer, Integer> vt : coalescedValueTransfers) {
        Set<Integer> partition = graphPartitions.get(vt.first.intValue());
        List<Pair<Integer, Integer>> l = partitionedValueTransfers.get(partition);
        if (l == null) {
            l = Lists.newArrayList();
            partitionedValueTransfers.put(partition, l);
        }
        l.add(vt);
    }
    // Initialize the value transfer graph.
    int numCoalescedSlots = nextCoalescedSlotId_ + 1;
    valueTransfer_ = new boolean[numCoalescedSlots][numCoalescedSlots];
    for (int i = 0; i < numCoalescedSlots; ++i) {
        valueTransfer_[i][i] = true;
    }
    // Step 4: Compute the transitive closure for each graph partition.
    for (Map.Entry<Set<Integer>, List<Pair<Integer, Integer>>> graphPartition : partitionedValueTransfers.entrySet()) {
        // Set value transfers of this partition.
        for (Pair<Integer, Integer> vt : graphPartition.getValue()) {
            valueTransfer_[vt.first][vt.second] = true;
        }
        Set<Integer> partitionSlotIds = graphPartition.getKey();
        // No transitive value transfers.
        if (partitionSlotIds.size() <= 2)
            continue;
        // Indirection vector into valueTransfer_. Contains one entry for each distinct
        // slot id referenced in a value transfer of this partition.
        int[] p = new int[partitionSlotIds.size()];
        int numPartitionSlots = 0;
        for (Integer slotId : partitionSlotIds) {
            p[numPartitionSlots++] = slotId;
        }
        // Compute the transitive closure of this graph partition.
        // TODO: Since we are operating on a DAG the performance can be improved if
        // necessary (e.g., topological sort + backwards propagation of the transitive
        // closure).
        boolean changed = false;
        do {
            changed = false;
            for (int i = 0; i < numPartitionSlots; ++i) {
                for (int j = 0; j < numPartitionSlots; ++j) {
                    for (int k = 0; k < numPartitionSlots; ++k) {
                        if (valueTransfer_[p[i]][p[j]] && valueTransfer_[p[j]][p[k]] && !valueTransfer_[p[i]][p[k]]) {
                            valueTransfer_[p[i]][p[k]] = true;
                            changed = true;
                        }
                    }
                }
            }
        } while (changed);
    }
    long end = System.currentTimeMillis();
    if (LOG.isDebugEnabled()) {
        LOG.trace("Time taken in computeValueTransfers(): " + (end - start) + "ms");
    }
}
#end_block

#method_before
private void partitionValueTransfers(DisjointSet<SlotId> completeSubGraphs, List<Pair<SlotId, SlotId>> valueTransfers) {
    // transform equality predicates into a transfer graph
    for (ExprId id : globalState_.conjuncts.keySet()) {
        Expr e = globalState_.conjuncts.get(id);
        Pair<SlotId, SlotId> slotIds = BinaryPredicate.getEqSlots(e);
        if (slotIds == null)
            continue;
        boolean isAntiJoin = false;
        TableRef sjTblRef = globalState_.sjClauseByConjunct.get(id);
        Preconditions.checkState(sjTblRef == null || sjTblRef.getJoinOp().isSemiJoin());
        isAntiJoin = sjTblRef != null && sjTblRef.getJoinOp().isAntiJoin();
        TableRef ojTblRef = globalState_.ojClauseByConjunct.get(id);
        Preconditions.checkState(ojTblRef == null || ojTblRef.getJoinOp().isOuterJoin());
        if (ojTblRef == null && !isAntiJoin) {
            // this eq predicate doesn't involve any outer or anti join, ie, it is true for
            // each result row;
            // value transfer is not legal if the receiving slot is in an enclosed
            // scope of the source slot and the receiving slot's block has a limit
            Analyzer firstBlock = globalState_.blockBySlot.get(slotIds.first);
            Analyzer secondBlock = globalState_.blockBySlot.get(slotIds.second);
            LOG.trace("value transfer: from " + slotIds.first.toString());
            Pair<SlotId, SlotId> firstToSecond = null;
            Pair<SlotId, SlotId> secondToFirst = null;
            if (!(secondBlock.hasLimitOffsetClause_ && secondBlock.ancestors_.contains(firstBlock))) {
                firstToSecond = new Pair<SlotId, SlotId>(slotIds.first, slotIds.second);
            }
            if (!(firstBlock.hasLimitOffsetClause_ && firstBlock.ancestors_.contains(secondBlock))) {
                secondToFirst = new Pair<SlotId, SlotId>(slotIds.second, slotIds.first);
            }
            // uni-directional value transfers to valueTransfers.
            if (firstToSecond != null && secondToFirst != null && completeSubGraphs != null) {
                completeSubGraphs.union(slotIds.first, slotIds.second);
            } else {
                if (firstToSecond != null)
                    valueTransfers.add(firstToSecond);
                if (secondToFirst != null)
                    valueTransfers.add(secondToFirst);
            }
            continue;
        }
        // Outer or semi-joined table ref.
        TableRef tblRef = (ojTblRef != null) ? ojTblRef : sjTblRef;
        Preconditions.checkNotNull(tblRef);
        if (tblRef.getJoinOp() == JoinOperator.FULL_OUTER_JOIN) {
            // full outer joins don't guarantee any value transfer
            continue;
        }
        // this is some form of outer or anti join
        SlotId outerSlot, innerSlot;
        if (tblRef.getId() == getTupleId(slotIds.first)) {
            innerSlot = slotIds.first;
            outerSlot = slotIds.second;
        } else if (tblRef.getId() == getTupleId(slotIds.second)) {
            innerSlot = slotIds.second;
            outerSlot = slotIds.first;
        } else {
            // actually be true
            continue;
        }
        // inverting the condition (paying special attention to NULLs).
        if (tblRef.getJoinOp() == JoinOperator.LEFT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.LEFT_ANTI_JOIN || tblRef.getJoinOp() == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(outerSlot, innerSlot));
        } else if (tblRef.getJoinOp() == JoinOperator.RIGHT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.RIGHT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(innerSlot, outerSlot));
        }
    }
}
#method_after
private void partitionValueTransfers(DisjointSet<SlotId> completeSubGraphs, List<Pair<SlotId, SlotId>> valueTransfers) {
    // transform equality predicates into a transfer graph
    for (ExprId id : globalState_.conjuncts.keySet()) {
        Expr e = globalState_.conjuncts.get(id);
        Pair<SlotId, SlotId> slotIds = BinaryPredicate.getEqSlots(e);
        if (slotIds == null)
            continue;
        boolean isAntiJoin = false;
        TableRef sjTblRef = globalState_.sjClauseByConjunct.get(id);
        Preconditions.checkState(sjTblRef == null || sjTblRef.getJoinOp().isSemiJoin());
        isAntiJoin = sjTblRef != null && sjTblRef.getJoinOp().isAntiJoin();
        TableRef ojTblRef = globalState_.ojClauseByConjunct.get(id);
        Preconditions.checkState(ojTblRef == null || ojTblRef.getJoinOp().isOuterJoin());
        if (ojTblRef == null && !isAntiJoin) {
            // this eq predicate doesn't involve any outer or anti join, ie, it is true for
            // each result row;
            // value transfer is not legal if the receiving slot is in an enclosed
            // scope of the source slot and the receiving slot's block has a limit
            Analyzer firstBlock = globalState_.blockBySlot.get(slotIds.first);
            Analyzer secondBlock = globalState_.blockBySlot.get(slotIds.second);
            if (LOG.isTraceEnabled()) {
                LOG.trace("value transfer: from " + slotIds.first.toString());
            }
            Pair<SlotId, SlotId> firstToSecond = null;
            Pair<SlotId, SlotId> secondToFirst = null;
            if (!(secondBlock.hasLimitOffsetClause_ && secondBlock.ancestors_.contains(firstBlock))) {
                firstToSecond = new Pair<SlotId, SlotId>(slotIds.first, slotIds.second);
            }
            if (!(firstBlock.hasLimitOffsetClause_ && firstBlock.ancestors_.contains(secondBlock))) {
                secondToFirst = new Pair<SlotId, SlotId>(slotIds.second, slotIds.first);
            }
            // uni-directional value transfers to valueTransfers.
            if (firstToSecond != null && secondToFirst != null && completeSubGraphs != null) {
                completeSubGraphs.union(slotIds.first, slotIds.second);
            } else {
                if (firstToSecond != null)
                    valueTransfers.add(firstToSecond);
                if (secondToFirst != null)
                    valueTransfers.add(secondToFirst);
            }
            continue;
        }
        // Outer or semi-joined table ref.
        TableRef tblRef = (ojTblRef != null) ? ojTblRef : sjTblRef;
        Preconditions.checkNotNull(tblRef);
        if (tblRef.getJoinOp() == JoinOperator.FULL_OUTER_JOIN) {
            // full outer joins don't guarantee any value transfer
            continue;
        }
        // this is some form of outer or anti join
        SlotId outerSlot, innerSlot;
        if (tblRef.getId() == getTupleId(slotIds.first)) {
            innerSlot = slotIds.first;
            outerSlot = slotIds.second;
        } else if (tblRef.getId() == getTupleId(slotIds.second)) {
            innerSlot = slotIds.second;
            outerSlot = slotIds.first;
        } else {
            // actually be true
            continue;
        }
        // inverting the condition (paying special attention to NULLs).
        if (tblRef.getJoinOp() == JoinOperator.LEFT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.LEFT_ANTI_JOIN || tblRef.getJoinOp() == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(outerSlot, innerSlot));
        } else if (tblRef.getJoinOp() == JoinOperator.RIGHT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.RIGHT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(innerSlot, outerSlot));
        }
    }
}
#end_block

#method_before
public PlanNode createSingleNodePlan() throws ImpalaException {
    QueryStmt queryStmt = ctx_.getQueryStmt();
    // Use the stmt's analyzer which is not necessarily the root analyzer
    // to detect empty result sets.
    Analyzer analyzer = queryStmt.getAnalyzer();
    analyzer.computeEquivClasses();
    analyzer.getTimeline().markEvent("Equivalence classes computed");
    // TODO 2: should the materialization decision be cost-based?
    if (queryStmt.getBaseTblResultExprs() != null) {
        analyzer.materializeSlots(queryStmt.getBaseTblResultExprs());
    }
    LOG.trace("desctbl: " + analyzer.getDescTbl().debugString());
    PlanNode singleNodePlan = createQueryPlan(queryStmt, analyzer, ctx_.getQueryOptions().isDisable_outermost_topn());
    Preconditions.checkNotNull(singleNodePlan);
    return singleNodePlan;
}
#method_after
public PlanNode createSingleNodePlan() throws ImpalaException {
    QueryStmt queryStmt = ctx_.getQueryStmt();
    // Use the stmt's analyzer which is not necessarily the root analyzer
    // to detect empty result sets.
    Analyzer analyzer = queryStmt.getAnalyzer();
    analyzer.computeEquivClasses();
    analyzer.getTimeline().markEvent("Equivalence classes computed");
    // TODO 2: should the materialization decision be cost-based?
    if (queryStmt.getBaseTblResultExprs() != null) {
        analyzer.materializeSlots(queryStmt.getBaseTblResultExprs());
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("desctbl: " + analyzer.getDescTbl().debugString());
    }
    PlanNode singleNodePlan = createQueryPlan(queryStmt, analyzer, ctx_.getQueryOptions().isDisable_outermost_topn());
    Preconditions.checkNotNull(singleNodePlan);
    return singleNodePlan;
}
#end_block

#method_before
public void validatePlan(PlanNode planNode) throws NotImplementedException {
    if (ctx_.getQueryOptions().mt_dop > 0 && !RuntimeEnv.INSTANCE.isTestEnv() && (planNode instanceof JoinNode || ctx_.hasTableSink())) {
        throw new NotImplementedException("MT_DOP not supported for plans with base table joins or table sinks.");
    }
    // As long as MT_DOP == 0 any join can run in a single-node plan.
    if (ctx_.isSingleNodeExec() && ctx_.getQueryOptions().mt_dop == 0)
        return;
    if (planNode instanceof NestedLoopJoinNode) {
        JoinNode joinNode = (JoinNode) planNode;
        JoinOperator joinOp = joinNode.getJoinOp();
        if ((joinOp.isRightSemiJoin() || joinOp.isFullOuterJoin() || joinOp == JoinOperator.RIGHT_OUTER_JOIN) && joinNode.getEqJoinConjuncts().isEmpty()) {
            throw new NotImplementedException(String.format("Error generating a valid " + "execution plan for this query. A %s type with no equi-join " + "predicates can only be executed with a single node plan.", joinOp.toString()));
        }
    }
    if (planNode instanceof SubplanNode) {
        // Right and full outer joins with no equi-join conjuncts are ok in the right
        // child of a SubplanNode.
        validatePlan(planNode.getChild(0));
    } else {
        for (PlanNode child : planNode.getChildren()) {
            validatePlan(child);
        }
    }
}
#method_after
public void validatePlan(PlanNode planNode) throws NotImplementedException {
    if (ctx_.getQueryOptions().isSetMt_dop() && ctx_.getQueryOptions().mt_dop > 0 && !RuntimeEnv.INSTANCE.isTestEnv() && (planNode instanceof JoinNode || ctx_.hasTableSink())) {
        throw new NotImplementedException("MT_DOP not supported for plans with base table joins or table sinks.");
    }
    // As long as MT_DOP is unset or 0 any join can run in a single-node plan.
    if (ctx_.isSingleNodeExec() && (!ctx_.getQueryOptions().isSetMt_dop() || ctx_.getQueryOptions().mt_dop == 0)) {
        return;
    }
    if (planNode instanceof NestedLoopJoinNode) {
        JoinNode joinNode = (JoinNode) planNode;
        JoinOperator joinOp = joinNode.getJoinOp();
        if ((joinOp.isRightSemiJoin() || joinOp.isFullOuterJoin() || joinOp == JoinOperator.RIGHT_OUTER_JOIN) && joinNode.getEqJoinConjuncts().isEmpty()) {
            throw new NotImplementedException(String.format("Error generating a valid " + "execution plan for this query. A %s type with no equi-join " + "predicates can only be executed with a single node plan.", joinOp.toString()));
        }
    }
    if (planNode instanceof SubplanNode) {
        // Right and full outer joins with no equi-join conjuncts are ok in the right
        // child of a SubplanNode.
        validatePlan(planNode.getChild(0));
    } else {
        for (PlanNode child : planNode.getChildren()) {
            validatePlan(child);
        }
    }
}
#end_block

#method_before
private PlanNode createCheapestJoinPlan(Analyzer analyzer, List<Pair<TableRef, PlanNode>> parentRefPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    LOG.trace("createCheapestJoinPlan");
    if (parentRefPlans.size() == 1)
        return parentRefPlans.get(0).second;
    // collect eligible candidates for the leftmost input; list contains
    // (plan, materialized size)
    ArrayList<Pair<TableRef, Long>> candidates = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : parentRefPlans) {
        TableRef ref = entry.first;
        JoinOperator joinOp = ref.getJoinOp();
        // consideration of the joinOps that result from such a re-ordering (IMPALA-1281).
        if (joinOp.isOuterJoin() || joinOp.isSemiJoin() || joinOp.isCrossJoin())
            continue;
        PlanNode plan = entry.second;
        if (plan.getCardinality() == -1) {
            // use 0 for the size to avoid it becoming the leftmost input
            // TODO: Consider raw size of scanned partitions in the absence of stats.
            candidates.add(new Pair(ref, new Long(0)));
            LOG.trace("candidate " + ref.getUniqueAlias() + ": 0");
            continue;
        }
        Preconditions.checkState(ref.isAnalyzed());
        long materializedSize = (long) Math.ceil(plan.getAvgRowSize() * (double) plan.getCardinality());
        candidates.add(new Pair(ref, new Long(materializedSize)));
        LOG.trace("candidate " + ref.getUniqueAlias() + ": " + Long.toString(materializedSize));
    }
    if (candidates.isEmpty())
        return null;
    // order candidates by descending materialized size; we want to minimize the memory
    // consumption of the materialized hash tables required for the join sequence
    Collections.sort(candidates, new Comparator<Pair<TableRef, Long>>() {

        public int compare(Pair<TableRef, Long> a, Pair<TableRef, Long> b) {
            long diff = b.second - a.second;
            return (diff < 0 ? -1 : (diff > 0 ? 1 : 0));
        }
    });
    for (Pair<TableRef, Long> candidate : candidates) {
        PlanNode result = createJoinPlan(analyzer, candidate.first, parentRefPlans, subplanRefs);
        if (result != null)
            return result;
    }
    return null;
}
#method_after
private PlanNode createCheapestJoinPlan(Analyzer analyzer, List<Pair<TableRef, PlanNode>> parentRefPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    LOG.trace("createCheapestJoinPlan");
    if (parentRefPlans.size() == 1)
        return parentRefPlans.get(0).second;
    // collect eligible candidates for the leftmost input; list contains
    // (plan, materialized size)
    ArrayList<Pair<TableRef, Long>> candidates = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : parentRefPlans) {
        TableRef ref = entry.first;
        JoinOperator joinOp = ref.getJoinOp();
        // consideration of the joinOps that result from such a re-ordering (IMPALA-1281).
        if (joinOp.isOuterJoin() || joinOp.isSemiJoin() || joinOp.isCrossJoin())
            continue;
        PlanNode plan = entry.second;
        if (plan.getCardinality() == -1) {
            // use 0 for the size to avoid it becoming the leftmost input
            // TODO: Consider raw size of scanned partitions in the absence of stats.
            candidates.add(new Pair(ref, new Long(0)));
            if (LOG.isTraceEnabled()) {
                LOG.trace("candidate " + ref.getUniqueAlias() + ": 0");
            }
            continue;
        }
        Preconditions.checkState(ref.isAnalyzed());
        long materializedSize = (long) Math.ceil(plan.getAvgRowSize() * (double) plan.getCardinality());
        candidates.add(new Pair(ref, new Long(materializedSize)));
        if (LOG.isTraceEnabled()) {
            LOG.trace("candidate " + ref.getUniqueAlias() + ": " + Long.toString(materializedSize));
        }
    }
    if (candidates.isEmpty())
        return null;
    // order candidates by descending materialized size; we want to minimize the memory
    // consumption of the materialized hash tables required for the join sequence
    Collections.sort(candidates, new Comparator<Pair<TableRef, Long>>() {

        public int compare(Pair<TableRef, Long> a, Pair<TableRef, Long> b) {
            long diff = b.second - a.second;
            return (diff < 0 ? -1 : (diff > 0 ? 1 : 0));
        }
    });
    for (Pair<TableRef, Long> candidate : candidates) {
        PlanNode result = createJoinPlan(analyzer, candidate.first, parentRefPlans, subplanRefs);
        if (result != null)
            return result;
    }
    return null;
}
#end_block

#method_before
private PlanNode createJoinPlan(Analyzer analyzer, TableRef leftmostRef, List<Pair<TableRef, PlanNode>> refPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    LOG.trace("createJoinPlan: " + leftmostRef.getUniqueAlias());
    // the refs that have yet to be joined
    List<Pair<TableRef, PlanNode>> remainingRefs = Lists.newArrayList();
    // root of accumulated join plan
    PlanNode root = null;
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        if (entry.first == leftmostRef) {
            root = entry.second;
        } else {
            remainingRefs.add(entry);
        }
    }
    Preconditions.checkNotNull(root);
    // Maps from a TableRef in refPlans with an outer/semi join op to the set of
    // TableRefs that precede it refPlans (i.e., in FROM-clause order).
    // The map is used to place outer/semi joins at a fixed position in the plan tree
    // (IMPALA-860), s.t. all the tables appearing to the left/right of an outer/semi
    // join in the original query still remain to the left/right after join ordering.
    // This prevents join re-ordering across outer/semi joins which is generally wrong.
    Map<TableRef, Set<TableRef>> precedingRefs = Maps.newHashMap();
    List<TableRef> tmpTblRefs = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        TableRef tblRef = entry.first;
        if (tblRef.getJoinOp().isOuterJoin() || tblRef.getJoinOp().isSemiJoin()) {
            precedingRefs.put(tblRef, Sets.newHashSet(tmpTblRefs));
        }
        tmpTblRefs.add(tblRef);
    }
    // Refs that have been joined. The union of joinedRefs and the refs in remainingRefs
    // are the set of all table refs.
    Set<TableRef> joinedRefs = Sets.newHashSet(leftmostRef);
    long numOps = 0;
    int i = 0;
    while (!remainingRefs.isEmpty()) {
        // We minimize the resulting cardinality at each step in the join chain,
        // which minimizes the total number of hash table lookups.
        PlanNode newRoot = null;
        Pair<TableRef, PlanNode> minEntry = null;
        for (Pair<TableRef, PlanNode> entry : remainingRefs) {
            TableRef ref = entry.first;
            JoinOperator joinOp = ref.getJoinOp();
            // Place outer/semi joins at a fixed position in the plan tree.
            Set<TableRef> requiredRefs = precedingRefs.get(ref);
            if (requiredRefs != null) {
                Preconditions.checkState(joinOp.isOuterJoin() || joinOp.isSemiJoin());
                // outer/semi joins.
                if (!requiredRefs.equals(joinedRefs))
                    break;
            }
            analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
            PlanNode candidate = createJoinNode(root, entry.second, ref, analyzer);
            if (candidate == null)
                continue;
            LOG.trace("cardinality=" + Long.toString(candidate.getCardinality()));
            // position in the plan.
            if (joinOp.isOuterJoin() || joinOp.isSemiJoin()) {
                newRoot = candidate;
                minEntry = entry;
                break;
            }
            // infrastructure.
            if (newRoot == null || (candidate.getClass().equals(newRoot.getClass()) && candidate.getCardinality() < newRoot.getCardinality()) || (candidate instanceof HashJoinNode && newRoot instanceof NestedLoopJoinNode)) {
                newRoot = candidate;
                minEntry = entry;
            }
        }
        if (newRoot == null) {
            // Could not generate a valid plan.
            return null;
        }
        // we need to insert every rhs row into the hash table and then look up
        // every lhs row
        long lhsCardinality = root.getCardinality();
        long rhsCardinality = minEntry.second.getCardinality();
        numOps += lhsCardinality + rhsCardinality;
        LOG.debug(Integer.toString(i) + " chose " + minEntry.first.getUniqueAlias() + " #lhs=" + Long.toString(lhsCardinality) + " #rhs=" + Long.toString(rhsCardinality) + " #ops=" + Long.toString(numOps));
        remainingRefs.remove(minEntry);
        joinedRefs.add(minEntry.first);
        root = newRoot;
        // Create a Subplan on top of the new root for all the subplan refs that can be
        // evaluated at this point.
        // TODO: Once we have stats on nested collections, we should consider the join
        // order in conjunction with the placement of SubplanNodes, i.e., move the creation
        // of SubplanNodes into the join-ordering loop above.
        root = createSubplan(root, subplanRefs, false, analyzer);
        // with a dense sequence of node ids
        if (root instanceof SubplanNode)
            root.getChild(0).setId(ctx_.getNextNodeId());
        root.setId(ctx_.getNextNodeId());
        analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
        ++i;
    }
    return root;
}
#method_after
private PlanNode createJoinPlan(Analyzer analyzer, TableRef leftmostRef, List<Pair<TableRef, PlanNode>> refPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    if (LOG.isTraceEnabled()) {
        LOG.trace("createJoinPlan: " + leftmostRef.getUniqueAlias());
    }
    // the refs that have yet to be joined
    List<Pair<TableRef, PlanNode>> remainingRefs = Lists.newArrayList();
    // root of accumulated join plan
    PlanNode root = null;
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        if (entry.first == leftmostRef) {
            root = entry.second;
        } else {
            remainingRefs.add(entry);
        }
    }
    Preconditions.checkNotNull(root);
    // Maps from a TableRef in refPlans with an outer/semi join op to the set of
    // TableRefs that precede it refPlans (i.e., in FROM-clause order).
    // The map is used to place outer/semi joins at a fixed position in the plan tree
    // (IMPALA-860), s.t. all the tables appearing to the left/right of an outer/semi
    // join in the original query still remain to the left/right after join ordering.
    // This prevents join re-ordering across outer/semi joins which is generally wrong.
    Map<TableRef, Set<TableRef>> precedingRefs = Maps.newHashMap();
    List<TableRef> tmpTblRefs = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        TableRef tblRef = entry.first;
        if (tblRef.getJoinOp().isOuterJoin() || tblRef.getJoinOp().isSemiJoin()) {
            precedingRefs.put(tblRef, Sets.newHashSet(tmpTblRefs));
        }
        tmpTblRefs.add(tblRef);
    }
    // Refs that have been joined. The union of joinedRefs and the refs in remainingRefs
    // are the set of all table refs.
    Set<TableRef> joinedRefs = Sets.newHashSet(leftmostRef);
    long numOps = 0;
    int i = 0;
    while (!remainingRefs.isEmpty()) {
        // We minimize the resulting cardinality at each step in the join chain,
        // which minimizes the total number of hash table lookups.
        PlanNode newRoot = null;
        Pair<TableRef, PlanNode> minEntry = null;
        for (Pair<TableRef, PlanNode> entry : remainingRefs) {
            TableRef ref = entry.first;
            JoinOperator joinOp = ref.getJoinOp();
            // Place outer/semi joins at a fixed position in the plan tree.
            Set<TableRef> requiredRefs = precedingRefs.get(ref);
            if (requiredRefs != null) {
                Preconditions.checkState(joinOp.isOuterJoin() || joinOp.isSemiJoin());
                // outer/semi joins.
                if (!requiredRefs.equals(joinedRefs))
                    break;
            }
            analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
            PlanNode candidate = createJoinNode(root, entry.second, ref, analyzer);
            if (candidate == null)
                continue;
            if (LOG.isTraceEnabled()) {
                LOG.trace("cardinality=" + Long.toString(candidate.getCardinality()));
            }
            // position in the plan.
            if (joinOp.isOuterJoin() || joinOp.isSemiJoin()) {
                newRoot = candidate;
                minEntry = entry;
                break;
            }
            // infrastructure.
            if (newRoot == null || (candidate.getClass().equals(newRoot.getClass()) && candidate.getCardinality() < newRoot.getCardinality()) || (candidate instanceof HashJoinNode && newRoot instanceof NestedLoopJoinNode)) {
                newRoot = candidate;
                minEntry = entry;
            }
        }
        if (newRoot == null) {
            // Could not generate a valid plan.
            return null;
        }
        // we need to insert every rhs row into the hash table and then look up
        // every lhs row
        long lhsCardinality = root.getCardinality();
        long rhsCardinality = minEntry.second.getCardinality();
        numOps += lhsCardinality + rhsCardinality;
        if (LOG.isTraceEnabled()) {
            LOG.trace(Integer.toString(i) + " chose " + minEntry.first.getUniqueAlias() + " #lhs=" + Long.toString(lhsCardinality) + " #rhs=" + Long.toString(rhsCardinality) + " #ops=" + Long.toString(numOps));
        }
        remainingRefs.remove(minEntry);
        joinedRefs.add(minEntry.first);
        root = newRoot;
        // Create a Subplan on top of the new root for all the subplan refs that can be
        // evaluated at this point.
        // TODO: Once we have stats on nested collections, we should consider the join
        // order in conjunction with the placement of SubplanNodes, i.e., move the creation
        // of SubplanNodes into the join-ordering loop above.
        root = createSubplan(root, subplanRefs, false, analyzer);
        // with a dense sequence of node ids
        if (root instanceof SubplanNode)
            root.getChild(0).setId(ctx_.getNextNodeId());
        root.setId(ctx_.getNextNodeId());
        analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
        ++i;
    }
    return root;
}
#end_block

#method_before
private PlanNode createConstantSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws InternalException {
    Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
    ArrayList<Expr> resultExprs = selectStmt.getResultExprs();
    // Create tuple descriptor for materialized tuple.
    TupleDescriptor tupleDesc = createResultTupleDescriptor(selectStmt, "union", analyzer);
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
    // Analysis guarantees that selects without a FROM clause only have constant exprs.
    unionNode.addConstExprList(Lists.newArrayList(resultExprs));
    // Replace the select stmt's resultExprs with SlotRefs into tupleDesc.
    for (int i = 0; i < resultExprs.size(); ++i) {
        SlotRef slotRef = new SlotRef(tupleDesc.getSlots().get(i));
        resultExprs.set(i, slotRef);
    }
    // UnionNode.init() needs tupleDesc to have been initialized
    unionNode.init(analyzer);
    return unionNode;
}
#method_after
private PlanNode createConstantSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws InternalException {
    Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
    List<Expr> resultExprs = selectStmt.getResultExprs();
    // Create tuple descriptor for materialized tuple.
    TupleDescriptor tupleDesc = createResultTupleDescriptor(selectStmt, "union", analyzer);
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
    // Analysis guarantees that selects without a FROM clause only have constant exprs.
    unionNode.addConstExprList(Lists.newArrayList(resultExprs));
    // Replace the select stmt's resultExprs with SlotRefs into tupleDesc.
    for (int i = 0; i < resultExprs.size(); ++i) {
        SlotRef slotRef = new SlotRef(tupleDesc.getSlots().get(i));
        resultExprs.set(i, slotRef);
    }
    // UnionNode.init() needs tupleDesc to have been initialized
    unionNode.init(analyzer);
    return unionNode;
}
#end_block

#method_before
private TupleDescriptor createResultTupleDescriptor(SelectStmt selectStmt, String debugName, Analyzer analyzer) {
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor(debugName);
    tupleDesc.setIsMaterialized(true);
    ArrayList<Expr> resultExprs = selectStmt.getResultExprs();
    ArrayList<String> colLabels = selectStmt.getColLabels();
    for (int i = 0; i < resultExprs.size(); ++i) {
        Expr resultExpr = resultExprs.get(i);
        String colLabel = colLabels.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(colLabel);
        slotDesc.setSourceExpr(resultExpr);
        slotDesc.setType(resultExpr.getType());
        slotDesc.setStats(ColumnStats.fromExpr(resultExpr));
        slotDesc.setIsMaterialized(true);
    }
    tupleDesc.computeMemLayout();
    return tupleDesc;
}
#method_after
private TupleDescriptor createResultTupleDescriptor(SelectStmt selectStmt, String debugName, Analyzer analyzer) {
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor(debugName);
    tupleDesc.setIsMaterialized(true);
    List<Expr> resultExprs = selectStmt.getResultExprs();
    List<String> colLabels = selectStmt.getColLabels();
    for (int i = 0; i < resultExprs.size(); ++i) {
        Expr resultExpr = resultExprs.get(i);
        String colLabel = colLabels.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(colLabel);
        slotDesc.setSourceExpr(resultExpr);
        slotDesc.setType(resultExpr.getType());
        slotDesc.setStats(ColumnStats.fromExpr(resultExpr));
        slotDesc.setIsMaterialized(true);
    }
    tupleDesc.computeMemLayout();
    return tupleDesc;
}
#end_block

#method_before
private PlanNode createHdfsScanPlan(TableRef hdfsTblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    TupleDescriptor tupleDesc = hdfsTblRef.getDesc();
    // Get all predicates bound by the tuple.
    List<Expr> conjuncts = Lists.newArrayList();
    conjuncts.addAll(analyzer.getBoundPredicates(tupleDesc.getId()));
    // Also add remaining unassigned conjuncts
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(tupleDesc.getId().asList());
    conjuncts.addAll(unassigned);
    analyzer.markConjunctsAssigned(unassigned);
    analyzer.createEquivConjuncts(tupleDesc.getId(), conjuncts);
    Expr.removeDuplicates(conjuncts);
    // Do partition pruning before deciding which slots to materialize,
    // We might end up removing some predicates.
    HdfsPartitionPruner pruner = new HdfsPartitionPruner(tupleDesc);
    List<HdfsPartition> partitions = pruner.prunePartitions(analyzer, conjuncts);
    // Mark all slots referenced by the remaining conjuncts as materialized.
    analyzer.materializeSlots(conjuncts);
    // try evaluating with metadata first. If not, fall back to scanning.
    if (fastPartitionKeyScans && tupleDesc.hasClusteringColsOnly()) {
        HashSet<List<Expr>> uniqueExprs = new HashSet<List<Expr>>();
        for (HdfsPartition partition : partitions) {
            // Ignore empty partitions to match the behavior of the scan based approach.
            if (partition.isDefaultPartition() || partition.getSize() == 0) {
                continue;
            }
            List<Expr> exprs = Lists.newArrayList();
            for (SlotDescriptor slotDesc : tupleDesc.getSlots()) {
                // slots, use dummy null values. UnionNode will filter out unmaterialized slots.
                if (!slotDesc.isMaterialized()) {
                    exprs.add(NullLiteral.create(slotDesc.getType()));
                } else {
                    int pos = slotDesc.getColumn().getPosition();
                    exprs.add(partition.getPartitionValue(pos));
                }
            }
            uniqueExprs.add(exprs);
        }
        // Create a UNION node with all unique partition keys.
        UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
        for (List<Expr> exprList : uniqueExprs) {
            unionNode.addConstExprList(exprList);
        }
        unionNode.init(analyzer);
        return unionNode;
    } else {
        ScanNode scanNode = new HdfsScanNode(ctx_.getNextNodeId(), tupleDesc, conjuncts, partitions, hdfsTblRef);
        scanNode.init(analyzer);
        return scanNode;
    }
}
#method_after
private PlanNode createHdfsScanPlan(TableRef hdfsTblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    TupleDescriptor tupleDesc = hdfsTblRef.getDesc();
    // Get all predicates bound by the tuple.
    List<Expr> conjuncts = Lists.newArrayList();
    conjuncts.addAll(analyzer.getBoundPredicates(tupleDesc.getId()));
    // Also add remaining unassigned conjuncts
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(tupleDesc.getId().asList());
    conjuncts.addAll(unassigned);
    analyzer.markConjunctsAssigned(unassigned);
    analyzer.createEquivConjuncts(tupleDesc.getId(), conjuncts);
    Expr.removeDuplicates(conjuncts);
    // Do partition pruning before deciding which slots to materialize,
    // We might end up removing some predicates.
    HdfsPartitionPruner pruner = new HdfsPartitionPruner(tupleDesc);
    List<HdfsPartition> partitions = pruner.prunePartitions(analyzer, conjuncts, false);
    // Mark all slots referenced by the remaining conjuncts as materialized.
    analyzer.materializeSlots(conjuncts);
    // try evaluating with metadata first. If not, fall back to scanning.
    if (fastPartitionKeyScans && tupleDesc.hasClusteringColsOnly()) {
        HashSet<List<Expr>> uniqueExprs = new HashSet<List<Expr>>();
        for (HdfsPartition partition : partitions) {
            // Ignore empty partitions to match the behavior of the scan based approach.
            if (partition.isDefaultPartition() || partition.getSize() == 0) {
                continue;
            }
            List<Expr> exprs = Lists.newArrayList();
            for (SlotDescriptor slotDesc : tupleDesc.getSlots()) {
                // slots, use dummy null values. UnionNode will filter out unmaterialized slots.
                if (!slotDesc.isMaterialized()) {
                    exprs.add(NullLiteral.create(slotDesc.getType()));
                } else {
                    int pos = slotDesc.getColumn().getPosition();
                    exprs.add(partition.getPartitionValue(pos));
                }
            }
            uniqueExprs.add(exprs);
        }
        // Create a UNION node with all unique partition keys.
        UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
        for (List<Expr> exprList : uniqueExprs) {
            unionNode.addConstExprList(exprList);
        }
        unionNode.init(analyzer);
        return unionNode;
    } else {
        ScanNode scanNode = new HdfsScanNode(ctx_.getNextNodeId(), tupleDesc, conjuncts, partitions, hdfsTblRef);
        scanNode.init(analyzer);
        return scanNode;
    }
}
#end_block

#method_before
public static void renameTable(KuduTable tbl, String newName) throws ImpalaRuntimeException {
    Preconditions.checkState(!Strings.isNullOrEmpty(newName));
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    alterTableOptions.renameTable(newName);
    String errMsg = String.format("Error renaming Kudu table " + "%s to %s", tbl.getKuduTableName(), newName);
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
        if (!client.isAlterTableDone(newName)) {
            throw new ImpalaRuntimeException(errMsg);
        }
    } catch (KuduException e) {
        throw new ImpalaRuntimeException(errMsg, e);
    }
}
#method_after
public static void renameTable(KuduTable tbl, String newName) throws ImpalaRuntimeException {
    Preconditions.checkState(!Strings.isNullOrEmpty(newName));
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    alterTableOptions.renameTable(newName);
    String errMsg = String.format("Error renaming Kudu table " + "%s to %s", tbl.getKuduTableName(), newName);
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
        if (!client.isAlterTableDone(newName)) {
            throw new ImpalaRuntimeException(errMsg + ": Kudu operation timed out");
        }
    } catch (KuduException e) {
        throw new ImpalaRuntimeException(errMsg, e);
    }
}
#end_block

#method_before
public static void alterKuduTable(KuduTable tbl, AlterTableOptions ato, String errMsg) throws ImpalaRuntimeException {
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), ato);
        if (!client.isAlterTableDone(tbl.getKuduTableName())) {
            throw new ImpalaRuntimeException(errMsg);
        }
    } catch (KuduException e) {
        throw new ImpalaRuntimeException(errMsg, e);
    }
}
#method_after
public static void alterKuduTable(KuduTable tbl, AlterTableOptions ato, String errMsg) throws ImpalaRuntimeException {
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), ato);
        if (!client.isAlterTableDone(tbl.getKuduTableName())) {
            throw new ImpalaRuntimeException(errMsg + ": Kudu operation timed out");
        }
    } catch (KuduException e) {
        throw new ImpalaRuntimeException(errMsg, e);
    }
}
#end_block

#method_before
@Override
protected void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    // Update the number of nodes to reflect the hosts that have relevant data.
    numNodes_ = hostIndexSet_.size();
    // Update the cardinality
    inputCardinality_ = cardinality_ = kuduTable_.getNumRows();
    cardinality_ *= computeSelectivity();
    cardinality_ = Math.min(Math.max(1, cardinality_), kuduTable_.getNumRows());
    cardinality_ = capAtLimit(cardinality_);
    if (LOG.isDebugEnabled()) {
        LOG.debug("computeStats KuduScan: cardinality=" + Long.toString(cardinality_));
    }
}
#method_after
@Override
protected void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    // Update the number of nodes to reflect the hosts that have relevant data.
    numNodes_ = hostIndexSet_.size();
    // Update the cardinality
    inputCardinality_ = cardinality_ = kuduTable_.getNumRows();
    cardinality_ *= computeSelectivity();
    cardinality_ = Math.min(Math.max(1, cardinality_), kuduTable_.getNumRows());
    cardinality_ = capAtLimit(cardinality_);
    if (LOG.isTraceEnabled()) {
        LOG.trace("computeStats KuduScan: cardinality=" + Long.toString(cardinality_));
    }
}
#end_block

#method_before
private boolean tryConvertInListKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof InPredicate))
        return false;
    InPredicate predicate = (InPredicate) expr;
    // convert InPredicates w/ subqueries.
    if (predicate.isNotIn() || !predicate.hasValuesList())
        return false;
    // Do not convert if there is an implicit cast.
    if (!(predicate.getChild(0) instanceof SlotRef))
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    PrimitiveType type = ref.getType().getPrimitiveType();
    // KuduPredicate takes a list of values as Objects.
    List<Object> values = Lists.newArrayList();
    for (int i = 1; i < predicate.getChildren().size(); ++i) {
        if (!(predicate.getChild(i) instanceof LiteralExpr))
            return false;
        // Get the value as an Object, or null if the type isn't support or the type doesn't
        // exactly match.
        Object value = getKuduInListValue(type, predicate.getChild(i));
        if (value == null)
            return false;
        values.add(value);
    }
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    kuduPredicates_.add(KuduPredicate.newInListPredicate(column, values));
    kuduConjuncts_.add(predicate);
    return true;
}
#method_after
private boolean tryConvertInListKuduPredicate(Analyzer analyzer, org.apache.kudu.client.KuduTable table, Expr expr) {
    if (!(expr instanceof InPredicate))
        return false;
    InPredicate predicate = (InPredicate) expr;
    // Only convert IN predicates, i.e. cannot convert NOT IN.
    if (predicate.isNotIn())
        return false;
    // Do not convert if there is an implicit cast.
    if (!(predicate.getChild(0) instanceof SlotRef))
        return false;
    SlotRef ref = (SlotRef) predicate.getChild(0);
    // KuduPredicate takes a list of values as Objects.
    List<Object> values = Lists.newArrayList();
    for (int i = 1; i < predicate.getChildren().size(); ++i) {
        if (!(predicate.getChild(i).isLiteral()))
            return false;
        Object value = getKuduInListValue((LiteralExpr) predicate.getChild(i));
        Preconditions.checkNotNull(value == null);
        values.add(value);
    }
    String colName = ref.getDesc().getColumn().getName();
    ColumnSchema column = table.getSchema().getColumn(colName);
    kuduPredicates_.add(KuduPredicate.newInListPredicate(column, values));
    kuduConjuncts_.add(predicate);
    return true;
}
#end_block

#method_before
private static Object getKuduInListValue(PrimitiveType type, Expr e) {
    Preconditions.checkArgument(e instanceof LiteralExpr);
    if (type != e.getType().getPrimitiveType())
        return null;
    switch(type) {
        case BOOLEAN:
            return ((BoolLiteral) e).getValue();
        case TINYINT:
            return (byte) ((NumericLiteral) e).getLongValue();
        case SMALLINT:
            return (short) ((NumericLiteral) e).getLongValue();
        case INT:
            return (int) ((NumericLiteral) e).getLongValue();
        case BIGINT:
            return ((NumericLiteral) e).getLongValue();
        case FLOAT:
            return (float) ((NumericLiteral) e).getDoubleValue();
        case DOUBLE:
            return ((NumericLiteral) e).getDoubleValue();
        case STRING:
            return ((StringLiteral) e).getValue();
        default:
            return null;
    }
}
#method_after
private static Object getKuduInListValue(LiteralExpr e) {
    switch(e.getType().getPrimitiveType()) {
        case BOOLEAN:
            return ((BoolLiteral) e).getValue();
        case TINYINT:
            return (byte) ((NumericLiteral) e).getLongValue();
        case SMALLINT:
            return (short) ((NumericLiteral) e).getLongValue();
        case INT:
            return (int) ((NumericLiteral) e).getLongValue();
        case BIGINT:
            return ((NumericLiteral) e).getLongValue();
        case FLOAT:
            return (float) ((NumericLiteral) e).getDoubleValue();
        case DOUBLE:
            return ((NumericLiteral) e).getDoubleValue();
        case STRING:
            return ((StringLiteral) e).getValue();
        default:
            Preconditions.checkState(false, "Unsupported Kudu type considered for predicate: %s", e.getType().toSql());
    }
    return null;
}
#end_block

#method_before
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    List<String> rawPath = tableRef.getPath();
    Path resolvedPath = null;
    try {
        resolvedPath = resolvePath(tableRef.getPath(), PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath.size() > 1) {
                registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath.get(0), rawPath.get(1)).allOf(Privilege.SELECT).toRequest());
            }
            registerPrivReq(new PrivilegeRequestBuilder().onTable(getDefaultDb(), rawPath.get(0)).allOf(Privilege.SELECT).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath)), e);
    }
    Preconditions.checkNotNull(resolvedPath);
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof KuduTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef, resolvedPath);
    } else {
        return new CollectionTableRef(tableRef, resolvedPath);
    }
}
#method_after
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    List<String> rawPath = tableRef.getPath();
    Path resolvedPath = null;
    try {
        resolvedPath = resolvePath(tableRef.getPath(), PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath.size() > 1) {
                registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath.get(0), rawPath.get(1)).allOf(tableRef.getPrivilege()).toRequest());
            }
            registerPrivReq(new PrivilegeRequestBuilder().onTable(getDefaultDb(), rawPath.get(0)).allOf(tableRef.getPrivilege()).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath)), e);
    }
    Preconditions.checkNotNull(resolvedPath);
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof KuduTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef, resolvedPath);
    } else {
        return new CollectionTableRef(tableRef, resolvedPath);
    }
}
#end_block

#method_before
public void registerFullOuterJoinedConjunct(Expr e) {
    Preconditions.checkState(!globalState_.fullOuterJoinedConjuncts.containsKey(e.getId()));
    List<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    for (TupleId tid : tids) {
        if (!globalState_.fullOuterJoinedTupleIds.containsKey(tid))
            continue;
        TableRef currentOuterJoin = globalState_.fullOuterJoinedTupleIds.get(tid);
        globalState_.fullOuterJoinedConjuncts.put(e.getId(), currentOuterJoin);
        break;
    }
    LOG.trace("registerFullOuterJoinedConjunct: " + globalState_.fullOuterJoinedConjuncts.toString());
}
#method_after
public void registerFullOuterJoinedConjunct(Expr e) {
    Preconditions.checkState(!globalState_.fullOuterJoinedConjuncts.containsKey(e.getId()));
    List<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    for (TupleId tid : tids) {
        if (!globalState_.fullOuterJoinedTupleIds.containsKey(tid))
            continue;
        TableRef currentOuterJoin = globalState_.fullOuterJoinedTupleIds.get(tid);
        globalState_.fullOuterJoinedConjuncts.put(e.getId(), currentOuterJoin);
        break;
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerFullOuterJoinedConjunct: " + globalState_.fullOuterJoinedConjuncts.toString());
    }
}
#end_block

#method_before
public void registerFullOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.fullOuterJoinedTupleIds.put(tid, rhsRef);
    }
    LOG.trace("registerFullOuterJoinedTids: " + globalState_.fullOuterJoinedTupleIds.toString());
}
#method_after
public void registerFullOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.fullOuterJoinedTupleIds.put(tid, rhsRef);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerFullOuterJoinedTids: " + globalState_.fullOuterJoinedTupleIds.toString());
    }
}
#end_block

#method_before
public void registerOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.outerJoinedTupleIds.put(tid, rhsRef);
    }
    LOG.trace("registerOuterJoinedTids: " + globalState_.outerJoinedTupleIds.toString());
}
#method_after
public void registerOuterJoinedTids(List<TupleId> tids, TableRef rhsRef) {
    for (TupleId tid : tids) {
        globalState_.outerJoinedTupleIds.put(tid, rhsRef);
    }
    if (LOG.isTraceEnabled()) {
        LOG.trace("registerOuterJoinedTids: " + globalState_.outerJoinedTupleIds.toString());
    }
}
#end_block

#method_before
private void registerConjunct(Expr e) {
    // always generate a new expr id; this might be a cloned conjunct that already
    // has the id of its origin set
    e.setId(globalState_.conjunctIdGenerator.getNextId());
    globalState_.conjuncts.put(e.getId(), e);
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    ArrayList<SlotId> slotIds = Lists.newArrayList();
    e.getIds(tupleIds, slotIds);
    registerFullOuterJoinedConjunct(e);
    // register single tid conjuncts
    if (tupleIds.size() == 1)
        globalState_.singleTidConjuncts.add(e.getId());
    LOG.trace("register tuple/slotConjunct: " + Integer.toString(e.getId().asInt()) + " " + e.toSql() + " " + e.debugString());
    if (!(e instanceof BinaryPredicate))
        return;
    BinaryPredicate binaryPred = (BinaryPredicate) e;
    // exactly one tuple id
    if (binaryPred.getOp() != BinaryPredicate.Operator.EQ && binaryPred.getOp() != BinaryPredicate.Operator.NULL_MATCHING_EQ && binaryPred.getOp() != BinaryPredicate.Operator.NOT_DISTINCT) {
        return;
    }
    // the binary predicate must refer to at least two tuples to be an eqJoinConjunct
    if (tupleIds.size() < 2)
        return;
    // examine children and update eqJoinConjuncts
    for (int i = 0; i < 2; ++i) {
        tupleIds = Lists.newArrayList();
        binaryPred.getChild(i).getIds(tupleIds, null);
        if (tupleIds.size() == 1) {
            if (!globalState_.eqJoinConjuncts.containsKey(tupleIds.get(0))) {
                List<ExprId> conjunctIds = Lists.newArrayList();
                conjunctIds.add(e.getId());
                globalState_.eqJoinConjuncts.put(tupleIds.get(0), conjunctIds);
            } else {
                globalState_.eqJoinConjuncts.get(tupleIds.get(0)).add(e.getId());
            }
            binaryPred.setIsEqJoinConjunct(true);
            LOG.trace("register eqJoinConjunct: " + Integer.toString(e.getId().asInt()));
        }
    }
}
#method_after
private void registerConjunct(Expr e) {
    // always generate a new expr id; this might be a cloned conjunct that already
    // has the id of its origin set
    e.setId(globalState_.conjunctIdGenerator.getNextId());
    globalState_.conjuncts.put(e.getId(), e);
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    ArrayList<SlotId> slotIds = Lists.newArrayList();
    e.getIds(tupleIds, slotIds);
    registerFullOuterJoinedConjunct(e);
    // register single tid conjuncts
    if (tupleIds.size() == 1)
        globalState_.singleTidConjuncts.add(e.getId());
    if (LOG.isTraceEnabled()) {
        LOG.trace("register tuple/slotConjunct: " + Integer.toString(e.getId().asInt()) + " " + e.toSql() + " " + e.debugString());
    }
    if (!(e instanceof BinaryPredicate))
        return;
    BinaryPredicate binaryPred = (BinaryPredicate) e;
    // exactly one tuple id
    if (binaryPred.getOp() != BinaryPredicate.Operator.EQ && binaryPred.getOp() != BinaryPredicate.Operator.NULL_MATCHING_EQ && binaryPred.getOp() != BinaryPredicate.Operator.NOT_DISTINCT) {
        return;
    }
    // the binary predicate must refer to at least two tuples to be an eqJoinConjunct
    if (tupleIds.size() < 2)
        return;
    // examine children and update eqJoinConjuncts
    for (int i = 0; i < 2; ++i) {
        tupleIds = Lists.newArrayList();
        binaryPred.getChild(i).getIds(tupleIds, null);
        if (tupleIds.size() == 1) {
            if (!globalState_.eqJoinConjuncts.containsKey(tupleIds.get(0))) {
                List<ExprId> conjunctIds = Lists.newArrayList();
                conjunctIds.add(e.getId());
                globalState_.eqJoinConjuncts.put(tupleIds.get(0), conjunctIds);
            } else {
                globalState_.eqJoinConjuncts.get(tupleIds.get(0)).add(e.getId());
            }
            binaryPred.setIsEqJoinConjunct(true);
            LOG.trace("register eqJoinConjunct: " + Integer.toString(e.getId().asInt()));
        }
    }
}
#end_block

#method_before
public void createAuxEquivPredicate(Expr lhs, Expr rhs) {
    // implicitly cast to a type different than NULL.
    if (lhs instanceof NullLiteral || rhs instanceof NullLiteral || lhs.getType().isNull() || rhs.getType().isNull()) {
        return;
    }
    // create an eq predicate between lhs and rhs
    BinaryPredicate p = new BinaryPredicate(BinaryPredicate.Operator.EQ, lhs, rhs);
    p.setIsAuxExpr();
    LOG.trace("register equiv predicate: " + p.toSql() + " " + p.debugString());
    registerConjunct(p);
}
#method_after
public void createAuxEquivPredicate(Expr lhs, Expr rhs) {
    // implicitly cast to a type different than NULL.
    if (lhs instanceof NullLiteral || rhs instanceof NullLiteral || lhs.getType().isNull() || rhs.getType().isNull()) {
        return;
    }
    // create an eq predicate between lhs and rhs
    BinaryPredicate p = new BinaryPredicate(BinaryPredicate.Operator.EQ, lhs, rhs);
    p.setIsAuxExpr();
    if (LOG.isTraceEnabled()) {
        LOG.trace("register equiv predicate: " + p.toSql() + " " + p.debugString());
    }
    registerConjunct(p);
}
#end_block

#method_before
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds, boolean inclOjConjuncts) {
    LOG.trace("getUnassignedConjuncts for " + Id.printIds(tupleIds));
    List<Expr> result = Lists.newArrayList();
    for (Expr e : globalState_.conjuncts.values()) {
        if (e.isBoundByTupleIds(tupleIds) && !e.isAuxExpr() && !globalState_.assignedConjuncts.contains(e.getId()) && ((inclOjConjuncts && !e.isConstant()) || !globalState_.ojClauseByConjunct.containsKey(e.getId()))) {
            result.add(e);
            LOG.trace("getUnassignedConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds, boolean inclOjConjuncts) {
    List<Expr> result = Lists.newArrayList();
    for (Expr e : globalState_.conjuncts.values()) {
        if (e.isBoundByTupleIds(tupleIds) && !e.isAuxExpr() && !globalState_.assignedConjuncts.contains(e.getId()) && ((inclOjConjuncts && !e.isConstant()) || !globalState_.ojClauseByConjunct.containsKey(e.getId()))) {
            result.add(e);
        }
    }
    return result;
}
#end_block

#method_before
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds) {
    LOG.trace("getUnassignedConjuncts for node with " + Id.printIds(tupleIds));
    List<Expr> result = Lists.newArrayList();
    for (Expr e : getUnassignedConjuncts(tupleIds, true)) {
        if (canEvalPredicate(tupleIds, e)) {
            result.add(e);
            LOG.trace("getUnassignedConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedConjuncts(List<TupleId> tupleIds) {
    List<Expr> result = Lists.newArrayList();
    for (Expr e : getUnassignedConjuncts(tupleIds, true)) {
        if (canEvalPredicate(tupleIds, e))
            result.add(e);
    }
    return result;
}
#end_block

#method_before
public List<Expr> getUnassignedOjConjuncts(TableRef ref) {
    Preconditions.checkState(ref.getJoinOp().isOuterJoin());
    List<Expr> result = Lists.newArrayList();
    List<ExprId> candidates = globalState_.conjunctsByOjClause.get(ref.getId());
    if (candidates == null)
        return result;
    for (ExprId conjunctId : candidates) {
        if (!globalState_.assignedConjuncts.contains(conjunctId)) {
            Expr e = globalState_.conjuncts.get(conjunctId);
            Preconditions.checkNotNull(e);
            result.add(e);
            LOG.trace("getUnassignedOjConjunct: " + e.toSql());
        }
    }
    return result;
}
#method_after
public List<Expr> getUnassignedOjConjuncts(TableRef ref) {
    Preconditions.checkState(ref.getJoinOp().isOuterJoin());
    List<Expr> result = Lists.newArrayList();
    List<ExprId> candidates = globalState_.conjunctsByOjClause.get(ref.getId());
    if (candidates == null)
        return result;
    for (ExprId conjunctId : candidates) {
        if (!globalState_.assignedConjuncts.contains(conjunctId)) {
            Expr e = globalState_.conjuncts.get(conjunctId);
            Preconditions.checkNotNull(e);
            result.add(e);
        }
    }
    return result;
}
#end_block

#method_before
public boolean canEvalPredicate(List<TupleId> tupleIds, Expr e) {
    LOG.trace("canEval: " + e.toSql() + " " + e.debugString() + " " + Id.printIds(tupleIds));
    if (!e.isBoundByTupleIds(tupleIds))
        return false;
    ArrayList<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    if (tids.isEmpty())
        return true;
    if (e.isOnClauseConjunct()) {
        if (tids.size() > 1) {
            // assign it to this node.
            if (isAntiJoinedConjunct(e))
                return canEvalAntiJoinedConjunct(e, tupleIds);
            // it up later via getUnassignedOjConjuncts()
            if (globalState_.ojClauseByConjunct.containsKey(e.getId()))
                return false;
            // assigned below the full outer join node that outer-joined it.
            return canEvalFullOuterJoinedConjunct(e, tupleIds);
        }
        TupleId tid = tids.get(0);
        if (globalState_.ojClauseByConjunct.containsKey(e.getId())) {
            // (otherwise e needn't be true when that tuple is set)
            if (!globalState_.outerJoinedTupleIds.containsKey(tid))
                return false;
            if (globalState_.ojClauseByConjunct.get(e.getId()) != globalState_.outerJoinedTupleIds.get(tid)) {
                return false;
            }
            // Single tuple id conjuncts specified in the FOJ On-clause are not allowed to be
            // assigned below that full outer join in the operator tree.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(e.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                return false;
        } else {
            // Non-OJ On-clause conjunct.
            if (isOuterJoined(tid)) {
                // If the conjunct references an outer-joined tuple, then evaluate the
                // conjunct at the join that the On-clause belongs to.
                TableRef onClauseTableRef = globalState_.ijClauseByConjunct.get(e.getId());
                Preconditions.checkNotNull(onClauseTableRef);
                return tupleIds.containsAll(onClauseTableRef.getAllTableRefIds());
            }
            // can assign it to this node.
            if (isAntiJoinedConjunct(e))
                return canEvalAntiJoinedConjunct(e, tupleIds);
        }
        // operator tree.
        return canEvalFullOuterJoinedConjunct(e, tupleIds);
    }
    if (isAntiJoinedConjunct(e))
        return canEvalAntiJoinedConjunct(e, tupleIds);
    for (TupleId tid : tids) {
        LOG.trace("canEval: checking tid " + tid.toString());
        TableRef rhsRef = getLastOjClause(tid);
        // this is not outer-joined; ignore
        if (rhsRef == null)
            continue;
        // check whether the last join to outer-join 'tid' is materialized by tupleIds
        boolean contains = tupleIds.containsAll(rhsRef.getAllTableRefIds());
        LOG.trace("canEval: contains=" + (contains ? "true " : "false ") + Id.printIds(tupleIds) + " " + Id.printIds(rhsRef.getAllTableRefIds()));
        if (!tupleIds.containsAll(rhsRef.getAllTableRefIds()))
            return false;
    }
    return true;
}
#method_after
public boolean canEvalPredicate(List<TupleId> tupleIds, Expr e) {
    if (!e.isBoundByTupleIds(tupleIds))
        return false;
    ArrayList<TupleId> tids = Lists.newArrayList();
    e.getIds(tids, null);
    if (tids.isEmpty())
        return true;
    if (e.isOnClauseConjunct()) {
        if (tids.size() > 1) {
            // assign it to this node.
            if (isAntiJoinedConjunct(e))
                return canEvalAntiJoinedConjunct(e, tupleIds);
            // it up later via getUnassignedOjConjuncts()
            if (globalState_.ojClauseByConjunct.containsKey(e.getId()))
                return false;
            // assigned below the full outer join node that outer-joined it.
            return canEvalFullOuterJoinedConjunct(e, tupleIds);
        }
        TupleId tid = tids.get(0);
        if (globalState_.ojClauseByConjunct.containsKey(e.getId())) {
            // (otherwise e needn't be true when that tuple is set)
            if (!globalState_.outerJoinedTupleIds.containsKey(tid))
                return false;
            if (globalState_.ojClauseByConjunct.get(e.getId()) != globalState_.outerJoinedTupleIds.get(tid)) {
                return false;
            }
            // Single tuple id conjuncts specified in the FOJ On-clause are not allowed to be
            // assigned below that full outer join in the operator tree.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(e.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                return false;
        } else {
            // Non-OJ On-clause conjunct.
            if (isOuterJoined(tid)) {
                // If the conjunct references an outer-joined tuple, then evaluate the
                // conjunct at the join that the On-clause belongs to.
                TableRef onClauseTableRef = globalState_.ijClauseByConjunct.get(e.getId());
                Preconditions.checkNotNull(onClauseTableRef);
                return tupleIds.containsAll(onClauseTableRef.getAllTableRefIds());
            }
            // can assign it to this node.
            if (isAntiJoinedConjunct(e))
                return canEvalAntiJoinedConjunct(e, tupleIds);
        }
        // operator tree.
        return canEvalFullOuterJoinedConjunct(e, tupleIds);
    }
    if (isAntiJoinedConjunct(e))
        return canEvalAntiJoinedConjunct(e, tupleIds);
    for (TupleId tid : tids) {
        TableRef rhsRef = getLastOjClause(tid);
        // this is not outer-joined; ignore
        if (rhsRef == null)
            continue;
        // check whether the last join to outer-join 'tid' is materialized by tupleIds
        if (!tupleIds.containsAll(rhsRef.getAllTableRefIds()))
            return false;
    }
    return true;
}
#end_block

#method_before
public ArrayList<Expr> getBoundPredicates(TupleId destTid, Set<SlotId> ignoreSlots, boolean markAssigned) {
    ArrayList<Expr> result = Lists.newArrayList();
    for (ExprId srcConjunctId : globalState_.singleTidConjuncts) {
        Expr srcConjunct = globalState_.conjuncts.get(srcConjunctId);
        if (srcConjunct instanceof SlotRef)
            continue;
        Preconditions.checkNotNull(srcConjunct);
        List<TupleId> srcTids = Lists.newArrayList();
        List<SlotId> srcSids = Lists.newArrayList();
        srcConjunct.getIds(srcTids, srcSids);
        Preconditions.checkState(srcTids.size() == 1);
        // Generate slot-mappings to bind srcConjunct to destTid.
        TupleId srcTid = srcTids.get(0);
        List<List<SlotId>> allDestSids = getEquivDestSlotIds(srcTid, srcSids, destTid, ignoreSlots);
        if (allDestSids.isEmpty())
            continue;
        // Indicates whether the source slots have equivalent slots that belong
        // to an outer-joined tuple.
        boolean hasOuterJoinedTuple = false;
        for (SlotId srcSid : srcSids) {
            if (hasOuterJoinedTuple(globalState_.equivClassBySlotId.get(srcSid))) {
                hasOuterJoinedTuple = true;
                break;
            }
        }
        // relative to 'srcConjunct'.
        if (hasOuterJoinedTuple && isTrueWithNullSlots(srcConjunct))
            continue;
        // (otherwise srcConjunct needn't be true when destTid is set)
        if (globalState_.ojClauseByConjunct.containsKey(srcConjunct.getId())) {
            if (!globalState_.outerJoinedTupleIds.containsKey(destTid))
                continue;
            if (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(destTid)) {
                continue;
            }
            // Do not propagate conjuncts from the on-clause of full-outer or anti-joins.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(srcConjunct.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                continue;
        }
        // join node.
        if (isAntiJoinedConjunct(srcConjunct))
            continue;
        // Generate predicates for all src-to-dest slot mappings.
        for (List<SlotId> destSids : allDestSids) {
            Preconditions.checkState(destSids.size() == srcSids.size());
            Expr p;
            if (srcSids.containsAll(destSids)) {
                p = srcConjunct;
            } else {
                ExprSubstitutionMap smap = new ExprSubstitutionMap();
                for (int i = 0; i < srcSids.size(); ++i) {
                    smap.put(new SlotRef(globalState_.descTbl.getSlotDesc(srcSids.get(i))), new SlotRef(globalState_.descTbl.getSlotDesc(destSids.get(i))));
                }
                try {
                    p = srcConjunct.trySubstitute(smap, this, false);
                } catch (ImpalaException exc) {
                    // not an executable predicate; ignore
                    continue;
                }
                // Unset the id because this bound predicate itself is not registered, and
                // to prevent callers from inadvertently marking the srcConjunct as assigned.
                p.setId(null);
                if (p instanceof BinaryPredicate)
                    ((BinaryPredicate) p).setIsInferred();
                LOG.trace("new pred: " + p.toSql() + " " + p.debugString());
            }
            if (markAssigned) {
                // predicate assignment doesn't hold if:
                // - the application against slotId doesn't transfer the value back to its
                // originating slot
                // - the original predicate is on an OJ'd table but doesn't originate from
                // that table's OJ clause's ON clause (if it comes from anywhere but that
                // ON clause, it needs to be evaluated directly by the join node that
                // materializes the OJ'd table)
                boolean reverseValueTransfer = true;
                for (int i = 0; i < srcSids.size(); ++i) {
                    if (!hasValueTransfer(destSids.get(i), srcSids.get(i))) {
                        reverseValueTransfer = false;
                        break;
                    }
                }
                // Check if either srcConjunct or the generated predicate needs to be evaluated
                // at a join node (IMPALA-2018).
                boolean evalByJoin = (evalByJoin(srcConjunct) && (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(srcTid))) || (evalByJoin(p) && (globalState_.ojClauseByConjunct.get(p.getId()) != globalState_.outerJoinedTupleIds.get(destTid)));
                // mark all bound predicates including duplicate ones
                if (reverseValueTransfer && !evalByJoin)
                    markConjunctAssigned(srcConjunct);
            }
            // check if we already created this predicate
            if (!result.contains(p))
                result.add(p);
        }
    }
    return result;
}
#method_after
public ArrayList<Expr> getBoundPredicates(TupleId destTid, Set<SlotId> ignoreSlots, boolean markAssigned) {
    ArrayList<Expr> result = Lists.newArrayList();
    for (ExprId srcConjunctId : globalState_.singleTidConjuncts) {
        Expr srcConjunct = globalState_.conjuncts.get(srcConjunctId);
        if (srcConjunct instanceof SlotRef)
            continue;
        Preconditions.checkNotNull(srcConjunct);
        List<TupleId> srcTids = Lists.newArrayList();
        List<SlotId> srcSids = Lists.newArrayList();
        srcConjunct.getIds(srcTids, srcSids);
        Preconditions.checkState(srcTids.size() == 1);
        // Generate slot-mappings to bind srcConjunct to destTid.
        TupleId srcTid = srcTids.get(0);
        List<List<SlotId>> allDestSids = getEquivDestSlotIds(srcTid, srcSids, destTid, ignoreSlots);
        if (allDestSids.isEmpty())
            continue;
        // Indicates whether the source slots have equivalent slots that belong
        // to an outer-joined tuple.
        boolean hasOuterJoinedTuple = false;
        for (SlotId srcSid : srcSids) {
            if (hasOuterJoinedTuple(globalState_.equivClassBySlotId.get(srcSid))) {
                hasOuterJoinedTuple = true;
                break;
            }
        }
        // relative to 'srcConjunct'.
        if (hasOuterJoinedTuple && isTrueWithNullSlots(srcConjunct))
            continue;
        // (otherwise srcConjunct needn't be true when destTid is set)
        if (globalState_.ojClauseByConjunct.containsKey(srcConjunct.getId())) {
            if (!globalState_.outerJoinedTupleIds.containsKey(destTid))
                continue;
            if (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(destTid)) {
                continue;
            }
            // Do not propagate conjuncts from the on-clause of full-outer or anti-joins.
            TableRef tblRef = globalState_.ojClauseByConjunct.get(srcConjunct.getId());
            if (tblRef.getJoinOp().isFullOuterJoin())
                continue;
        }
        // join node.
        if (isAntiJoinedConjunct(srcConjunct))
            continue;
        // Generate predicates for all src-to-dest slot mappings.
        for (List<SlotId> destSids : allDestSids) {
            Preconditions.checkState(destSids.size() == srcSids.size());
            Expr p;
            if (srcSids.containsAll(destSids)) {
                p = srcConjunct;
            } else {
                ExprSubstitutionMap smap = new ExprSubstitutionMap();
                for (int i = 0; i < srcSids.size(); ++i) {
                    smap.put(new SlotRef(globalState_.descTbl.getSlotDesc(srcSids.get(i))), new SlotRef(globalState_.descTbl.getSlotDesc(destSids.get(i))));
                }
                try {
                    p = srcConjunct.trySubstitute(smap, this, false);
                } catch (ImpalaException exc) {
                    // not an executable predicate; ignore
                    continue;
                }
                // Unset the id because this bound predicate itself is not registered, and
                // to prevent callers from inadvertently marking the srcConjunct as assigned.
                p.setId(null);
                if (p instanceof BinaryPredicate)
                    ((BinaryPredicate) p).setIsInferred();
                if (LOG.isTraceEnabled()) {
                    LOG.trace("new pred: " + p.toSql() + " " + p.debugString());
                }
            }
            if (markAssigned) {
                // predicate assignment doesn't hold if:
                // - the application against slotId doesn't transfer the value back to its
                // originating slot
                // - the original predicate is on an OJ'd table but doesn't originate from
                // that table's OJ clause's ON clause (if it comes from anywhere but that
                // ON clause, it needs to be evaluated directly by the join node that
                // materializes the OJ'd table)
                boolean reverseValueTransfer = true;
                for (int i = 0; i < srcSids.size(); ++i) {
                    if (!hasValueTransfer(destSids.get(i), srcSids.get(i))) {
                        reverseValueTransfer = false;
                        break;
                    }
                }
                // Check if either srcConjunct or the generated predicate needs to be evaluated
                // at a join node (IMPALA-2018).
                boolean evalByJoin = (evalByJoin(srcConjunct) && (globalState_.ojClauseByConjunct.get(srcConjunct.getId()) != globalState_.outerJoinedTupleIds.get(srcTid))) || (evalByJoin(p) && (globalState_.ojClauseByConjunct.get(p.getId()) != globalState_.outerJoinedTupleIds.get(destTid)));
                // mark all bound predicates including duplicate ones
                if (reverseValueTransfer && !evalByJoin)
                    markConjunctAssigned(srcConjunct);
            }
            // check if we already created this predicate
            if (!result.contains(p))
                result.add(p);
        }
    }
    return result;
}
#end_block

#method_before
public List<SlotId> getEquivSlots(SlotId slotId, List<TupleId> tupleIds) {
    List<SlotId> result = Lists.newArrayList();
    LOG.trace("getequivslots: slotid=" + Integer.toString(slotId.asInt()));
    EquivalenceClassId classId = globalState_.equivClassBySlotId.get(slotId);
    for (SlotId memberId : globalState_.equivClassMembers.get(classId)) {
        if (tupleIds.contains(globalState_.descTbl.getSlotDesc(memberId).getParent().getId())) {
            result.add(memberId);
        }
    }
    return result;
}
#method_after
public List<SlotId> getEquivSlots(SlotId slotId, List<TupleId> tupleIds) {
    List<SlotId> result = Lists.newArrayList();
    EquivalenceClassId classId = globalState_.equivClassBySlotId.get(slotId);
    for (SlotId memberId : globalState_.equivClassMembers.get(classId)) {
        if (tupleIds.contains(globalState_.descTbl.getSlotDesc(memberId).getParent().getId())) {
            result.add(memberId);
        }
    }
    return result;
}
#end_block

#method_before
public void markConjunctsAssigned(List<Expr> conjuncts) {
    if (conjuncts == null)
        return;
    for (Expr p : conjuncts) {
        globalState_.assignedConjuncts.add(p.getId());
        LOG.trace("markAssigned " + p.toSql() + " " + p.debugString());
    }
}
#method_after
public void markConjunctsAssigned(List<Expr> conjuncts) {
    if (conjuncts == null)
        return;
    for (Expr p : conjuncts) {
        globalState_.assignedConjuncts.add(p.getId());
    }
}
#end_block

#method_before
public void markConjunctAssigned(Expr conjunct) {
    LOG.trace("markAssigned " + conjunct.toSql() + " " + conjunct.debugString());
    globalState_.assignedConjuncts.add(conjunct.getId());
}
#method_after
public void markConjunctAssigned(Expr conjunct) {
    globalState_.assignedConjuncts.add(conjunct.getId());
}
#end_block

#method_before
public boolean hasUnassignedConjuncts() {
    for (ExprId id : globalState_.conjuncts.keySet()) {
        if (globalState_.assignedConjuncts.contains(id))
            continue;
        Expr e = globalState_.conjuncts.get(id);
        if (e.isAuxExpr())
            continue;
        LOG.trace("unassigned: " + e.toSql() + " " + e.debugString());
        return true;
    }
    return false;
}
#method_after
public boolean hasUnassignedConjuncts() {
    for (ExprId id : globalState_.conjuncts.keySet()) {
        if (globalState_.assignedConjuncts.contains(id))
            continue;
        Expr e = globalState_.conjuncts.get(id);
        if (e.isAuxExpr())
            continue;
        return true;
    }
    return false;
}
#end_block

#method_before
public Table getTable(TableName tableName, Privilege privilege, boolean addAccessEvent) throws AnalysisException {
    Preconditions.checkNotNull(tableName);
    Preconditions.checkNotNull(privilege);
    Table table = null;
    tableName = new TableName(getTargetDbName(tableName), tableName.getTbl());
    if (privilege == Privilege.ANY) {
        registerPrivReq(new PrivilegeRequestBuilder().any().onAnyColumn(tableName.getDb(), tableName.getTbl()).toRequest());
    } else {
        registerPrivReq(new PrivilegeRequestBuilder().allOf(privilege).onTable(tableName.getDb(), tableName.getTbl()).toRequest());
    }
    // AnalysisExceptions.
    try {
        table = getTable(tableName.getDb(), tableName.getTbl());
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    Preconditions.checkNotNull(table);
    if (addAccessEvent) {
        // Add an audit event for this access
        TCatalogObjectType objectType = TCatalogObjectType.TABLE;
        if (table instanceof View)
            objectType = TCatalogObjectType.VIEW;
        globalState_.accessEvents.add(new TAccessEvent(tableName.toString(), objectType, privilege.toString()));
    }
    return table;
}
#method_after
public Table getTable(TableName tableName, Privilege privilege, boolean addAccessEvent) throws AnalysisException, TableLoadingException {
    Preconditions.checkNotNull(tableName);
    Preconditions.checkNotNull(privilege);
    Table table = null;
    tableName = new TableName(getTargetDbName(tableName), tableName.getTbl());
    if (privilege == Privilege.ANY) {
        registerPrivReq(new PrivilegeRequestBuilder().any().onAnyColumn(tableName.getDb(), tableName.getTbl()).toRequest());
    } else {
        registerPrivReq(new PrivilegeRequestBuilder().allOf(privilege).onTable(tableName.getDb(), tableName.getTbl()).toRequest());
    }
    table = getTable(tableName.getDb(), tableName.getTbl());
    Preconditions.checkNotNull(table);
    if (addAccessEvent) {
        // Add an audit event for this access
        TCatalogObjectType objectType = TCatalogObjectType.TABLE;
        if (table instanceof View)
            objectType = TCatalogObjectType.VIEW;
        globalState_.accessEvents.add(new TAccessEvent(tableName.toString(), objectType, privilege.toString()));
    }
    return table;
}
#end_block

#method_before
public Table getTable(TableName tableName, Privilege privilege) throws AnalysisException {
    return getTable(tableName, privilege, true);
}
#method_after
public Table getTable(TableName tableName, Privilege privilege) throws AnalysisException {
    // AnalysisExceptions.
    try {
        return getTable(tableName, privilege, true);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
}
#end_block

#method_before
public void registerAuthAndAuditEvent(Table table, Analyzer analyzer) {
    // Add access event for auditing.
    if (table instanceof View) {
        View view = (View) table;
        Preconditions.checkState(!view.isLocalView());
        analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, Privilege.SELECT.toString()));
    } else {
        analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, Privilege.SELECT.toString()));
    }
    // Add privilege request.
    TableName tableName = table.getTableName();
    analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(Privilege.SELECT).toRequest());
}
#method_after
public void registerAuthAndAuditEvent(Table table, Privilege priv) {
    // Add access event for auditing.
    if (table instanceof View) {
        View view = (View) table;
        Preconditions.checkState(!view.isLocalView());
        addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, priv.toString()));
    } else {
        addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, priv.toString()));
    }
    // Add privilege request.
    TableName tableName = table.getTableName();
    registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(priv).toRequest());
}
#end_block

#method_before
public void computeValueTransfers() {
    long start = System.currentTimeMillis();
    // Step1: Compute complete subgraphs and get uni-directional value transfers.
    List<Pair<SlotId, SlotId>> origValueTransfers = Lists.newArrayList();
    partitionValueTransfers(completeSubGraphs_, origValueTransfers);
    // Coalesce complete subgraphs into a single slot and assign new slot ids.
    coalescedSlots_ = new int[numSlots_];
    Arrays.fill(coalescedSlots_, -1);
    for (Set<SlotId> equivClass : completeSubGraphs_.getSets()) {
        int representative = nextCoalescedSlotId_;
        for (SlotId slotId : equivClass) {
            coalescedSlots_[slotId.asInt()] = representative;
        }
        ++nextCoalescedSlotId_;
    }
    // Step 2: Map uni-directional value transfers onto the new slot domain, and
    // store the connected components in graphPartitions.
    List<Pair<Integer, Integer>> coalescedValueTransfers = Lists.newArrayList();
    // A graph partition is a set of slot ids that are connected by uni-directional
    // value transfers. The graph corresponding to a graph partition is a DAG.
    DisjointSet<Integer> graphPartitions = new DisjointSet<Integer>();
    mapSlots(origValueTransfers, coalescedValueTransfers, graphPartitions);
    mapSlots(globalState_.registeredValueTransfers, coalescedValueTransfers, graphPartitions);
    // Step 3: Group the coalesced value transfers by the graph partition they
    // belong to. Maps from the graph partition to its list of value transfers.
    // TODO: Implement a specialized DisjointSet data structure to avoid this step.
    Map<Set<Integer>, List<Pair<Integer, Integer>>> partitionedValueTransfers = Maps.newHashMap();
    for (Pair<Integer, Integer> vt : coalescedValueTransfers) {
        Set<Integer> partition = graphPartitions.get(vt.first.intValue());
        List<Pair<Integer, Integer>> l = partitionedValueTransfers.get(partition);
        if (l == null) {
            l = Lists.newArrayList();
            partitionedValueTransfers.put(partition, l);
        }
        l.add(vt);
    }
    // Initialize the value transfer graph.
    int numCoalescedSlots = nextCoalescedSlotId_ + 1;
    valueTransfer_ = new boolean[numCoalescedSlots][numCoalescedSlots];
    for (int i = 0; i < numCoalescedSlots; ++i) {
        valueTransfer_[i][i] = true;
    }
    // Step 4: Compute the transitive closure for each graph partition.
    for (Map.Entry<Set<Integer>, List<Pair<Integer, Integer>>> graphPartition : partitionedValueTransfers.entrySet()) {
        // Set value transfers of this partition.
        for (Pair<Integer, Integer> vt : graphPartition.getValue()) {
            valueTransfer_[vt.first][vt.second] = true;
        }
        Set<Integer> partitionSlotIds = graphPartition.getKey();
        // No transitive value transfers.
        if (partitionSlotIds.size() <= 2)
            continue;
        // Indirection vector into valueTransfer_. Contains one entry for each distinct
        // slot id referenced in a value transfer of this partition.
        int[] p = new int[partitionSlotIds.size()];
        int numPartitionSlots = 0;
        for (Integer slotId : partitionSlotIds) {
            p[numPartitionSlots++] = slotId;
        }
        // Compute the transitive closure of this graph partition.
        // TODO: Since we are operating on a DAG the performance can be improved if
        // necessary (e.g., topological sort + backwards propagation of the transitive
        // closure).
        boolean changed = false;
        do {
            changed = false;
            for (int i = 0; i < numPartitionSlots; ++i) {
                for (int j = 0; j < numPartitionSlots; ++j) {
                    for (int k = 0; k < numPartitionSlots; ++k) {
                        if (valueTransfer_[p[i]][p[j]] && valueTransfer_[p[j]][p[k]] && !valueTransfer_[p[i]][p[k]]) {
                            valueTransfer_[p[i]][p[k]] = true;
                            changed = true;
                        }
                    }
                }
            }
        } while (changed);
    }
    long end = System.currentTimeMillis();
    LOG.trace("Time taken in computeValueTransfers(): " + (end - start) + "ms");
}
#method_after
public void computeValueTransfers() {
    long start = System.currentTimeMillis();
    // Step1: Compute complete subgraphs and get uni-directional value transfers.
    List<Pair<SlotId, SlotId>> origValueTransfers = Lists.newArrayList();
    partitionValueTransfers(completeSubGraphs_, origValueTransfers);
    // Coalesce complete subgraphs into a single slot and assign new slot ids.
    coalescedSlots_ = new int[numSlots_];
    Arrays.fill(coalescedSlots_, -1);
    for (Set<SlotId> equivClass : completeSubGraphs_.getSets()) {
        int representative = nextCoalescedSlotId_;
        for (SlotId slotId : equivClass) {
            coalescedSlots_[slotId.asInt()] = representative;
        }
        ++nextCoalescedSlotId_;
    }
    // Step 2: Map uni-directional value transfers onto the new slot domain, and
    // store the connected components in graphPartitions.
    List<Pair<Integer, Integer>> coalescedValueTransfers = Lists.newArrayList();
    // A graph partition is a set of slot ids that are connected by uni-directional
    // value transfers. The graph corresponding to a graph partition is a DAG.
    DisjointSet<Integer> graphPartitions = new DisjointSet<Integer>();
    mapSlots(origValueTransfers, coalescedValueTransfers, graphPartitions);
    mapSlots(globalState_.registeredValueTransfers, coalescedValueTransfers, graphPartitions);
    // Step 3: Group the coalesced value transfers by the graph partition they
    // belong to. Maps from the graph partition to its list of value transfers.
    // TODO: Implement a specialized DisjointSet data structure to avoid this step.
    Map<Set<Integer>, List<Pair<Integer, Integer>>> partitionedValueTransfers = Maps.newHashMap();
    for (Pair<Integer, Integer> vt : coalescedValueTransfers) {
        Set<Integer> partition = graphPartitions.get(vt.first.intValue());
        List<Pair<Integer, Integer>> l = partitionedValueTransfers.get(partition);
        if (l == null) {
            l = Lists.newArrayList();
            partitionedValueTransfers.put(partition, l);
        }
        l.add(vt);
    }
    // Initialize the value transfer graph.
    int numCoalescedSlots = nextCoalescedSlotId_ + 1;
    valueTransfer_ = new boolean[numCoalescedSlots][numCoalescedSlots];
    for (int i = 0; i < numCoalescedSlots; ++i) {
        valueTransfer_[i][i] = true;
    }
    // Step 4: Compute the transitive closure for each graph partition.
    for (Map.Entry<Set<Integer>, List<Pair<Integer, Integer>>> graphPartition : partitionedValueTransfers.entrySet()) {
        // Set value transfers of this partition.
        for (Pair<Integer, Integer> vt : graphPartition.getValue()) {
            valueTransfer_[vt.first][vt.second] = true;
        }
        Set<Integer> partitionSlotIds = graphPartition.getKey();
        // No transitive value transfers.
        if (partitionSlotIds.size() <= 2)
            continue;
        // Indirection vector into valueTransfer_. Contains one entry for each distinct
        // slot id referenced in a value transfer of this partition.
        int[] p = new int[partitionSlotIds.size()];
        int numPartitionSlots = 0;
        for (Integer slotId : partitionSlotIds) {
            p[numPartitionSlots++] = slotId;
        }
        // Compute the transitive closure of this graph partition.
        // TODO: Since we are operating on a DAG the performance can be improved if
        // necessary (e.g., topological sort + backwards propagation of the transitive
        // closure).
        boolean changed = false;
        do {
            changed = false;
            for (int i = 0; i < numPartitionSlots; ++i) {
                for (int j = 0; j < numPartitionSlots; ++j) {
                    for (int k = 0; k < numPartitionSlots; ++k) {
                        if (valueTransfer_[p[i]][p[j]] && valueTransfer_[p[j]][p[k]] && !valueTransfer_[p[i]][p[k]]) {
                            valueTransfer_[p[i]][p[k]] = true;
                            changed = true;
                        }
                    }
                }
            }
        } while (changed);
    }
    long end = System.currentTimeMillis();
    if (LOG.isDebugEnabled()) {
        LOG.trace("Time taken in computeValueTransfers(): " + (end - start) + "ms");
    }
}
#end_block

#method_before
private void partitionValueTransfers(DisjointSet<SlotId> completeSubGraphs, List<Pair<SlotId, SlotId>> valueTransfers) {
    // transform equality predicates into a transfer graph
    for (ExprId id : globalState_.conjuncts.keySet()) {
        Expr e = globalState_.conjuncts.get(id);
        Pair<SlotId, SlotId> slotIds = BinaryPredicate.getEqSlots(e);
        if (slotIds == null)
            continue;
        boolean isAntiJoin = false;
        TableRef sjTblRef = globalState_.sjClauseByConjunct.get(id);
        Preconditions.checkState(sjTblRef == null || sjTblRef.getJoinOp().isSemiJoin());
        isAntiJoin = sjTblRef != null && sjTblRef.getJoinOp().isAntiJoin();
        TableRef ojTblRef = globalState_.ojClauseByConjunct.get(id);
        Preconditions.checkState(ojTblRef == null || ojTblRef.getJoinOp().isOuterJoin());
        if (ojTblRef == null && !isAntiJoin) {
            // this eq predicate doesn't involve any outer or anti join, ie, it is true for
            // each result row;
            // value transfer is not legal if the receiving slot is in an enclosed
            // scope of the source slot and the receiving slot's block has a limit
            Analyzer firstBlock = globalState_.blockBySlot.get(slotIds.first);
            Analyzer secondBlock = globalState_.blockBySlot.get(slotIds.second);
            LOG.trace("value transfer: from " + slotIds.first.toString());
            Pair<SlotId, SlotId> firstToSecond = null;
            Pair<SlotId, SlotId> secondToFirst = null;
            if (!(secondBlock.hasLimitOffsetClause_ && secondBlock.ancestors_.contains(firstBlock))) {
                firstToSecond = new Pair<SlotId, SlotId>(slotIds.first, slotIds.second);
            }
            if (!(firstBlock.hasLimitOffsetClause_ && firstBlock.ancestors_.contains(secondBlock))) {
                secondToFirst = new Pair<SlotId, SlotId>(slotIds.second, slotIds.first);
            }
            // uni-directional value transfers to valueTransfers.
            if (firstToSecond != null && secondToFirst != null && completeSubGraphs != null) {
                completeSubGraphs.union(slotIds.first, slotIds.second);
            } else {
                if (firstToSecond != null)
                    valueTransfers.add(firstToSecond);
                if (secondToFirst != null)
                    valueTransfers.add(secondToFirst);
            }
            continue;
        }
        // Outer or semi-joined table ref.
        TableRef tblRef = (ojTblRef != null) ? ojTblRef : sjTblRef;
        Preconditions.checkNotNull(tblRef);
        if (tblRef.getJoinOp() == JoinOperator.FULL_OUTER_JOIN) {
            // full outer joins don't guarantee any value transfer
            continue;
        }
        // this is some form of outer or anti join
        SlotId outerSlot, innerSlot;
        if (tblRef.getId() == getTupleId(slotIds.first)) {
            innerSlot = slotIds.first;
            outerSlot = slotIds.second;
        } else if (tblRef.getId() == getTupleId(slotIds.second)) {
            innerSlot = slotIds.second;
            outerSlot = slotIds.first;
        } else {
            // actually be true
            continue;
        }
        // inverting the condition (paying special attention to NULLs).
        if (tblRef.getJoinOp() == JoinOperator.LEFT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.LEFT_ANTI_JOIN || tblRef.getJoinOp() == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(outerSlot, innerSlot));
        } else if (tblRef.getJoinOp() == JoinOperator.RIGHT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.RIGHT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(innerSlot, outerSlot));
        }
    }
}
#method_after
private void partitionValueTransfers(DisjointSet<SlotId> completeSubGraphs, List<Pair<SlotId, SlotId>> valueTransfers) {
    // transform equality predicates into a transfer graph
    for (ExprId id : globalState_.conjuncts.keySet()) {
        Expr e = globalState_.conjuncts.get(id);
        Pair<SlotId, SlotId> slotIds = BinaryPredicate.getEqSlots(e);
        if (slotIds == null)
            continue;
        boolean isAntiJoin = false;
        TableRef sjTblRef = globalState_.sjClauseByConjunct.get(id);
        Preconditions.checkState(sjTblRef == null || sjTblRef.getJoinOp().isSemiJoin());
        isAntiJoin = sjTblRef != null && sjTblRef.getJoinOp().isAntiJoin();
        TableRef ojTblRef = globalState_.ojClauseByConjunct.get(id);
        Preconditions.checkState(ojTblRef == null || ojTblRef.getJoinOp().isOuterJoin());
        if (ojTblRef == null && !isAntiJoin) {
            // this eq predicate doesn't involve any outer or anti join, ie, it is true for
            // each result row;
            // value transfer is not legal if the receiving slot is in an enclosed
            // scope of the source slot and the receiving slot's block has a limit
            Analyzer firstBlock = globalState_.blockBySlot.get(slotIds.first);
            Analyzer secondBlock = globalState_.blockBySlot.get(slotIds.second);
            if (LOG.isTraceEnabled()) {
                LOG.trace("value transfer: from " + slotIds.first.toString());
            }
            Pair<SlotId, SlotId> firstToSecond = null;
            Pair<SlotId, SlotId> secondToFirst = null;
            if (!(secondBlock.hasLimitOffsetClause_ && secondBlock.ancestors_.contains(firstBlock))) {
                firstToSecond = new Pair<SlotId, SlotId>(slotIds.first, slotIds.second);
            }
            if (!(firstBlock.hasLimitOffsetClause_ && firstBlock.ancestors_.contains(secondBlock))) {
                secondToFirst = new Pair<SlotId, SlotId>(slotIds.second, slotIds.first);
            }
            // uni-directional value transfers to valueTransfers.
            if (firstToSecond != null && secondToFirst != null && completeSubGraphs != null) {
                completeSubGraphs.union(slotIds.first, slotIds.second);
            } else {
                if (firstToSecond != null)
                    valueTransfers.add(firstToSecond);
                if (secondToFirst != null)
                    valueTransfers.add(secondToFirst);
            }
            continue;
        }
        // Outer or semi-joined table ref.
        TableRef tblRef = (ojTblRef != null) ? ojTblRef : sjTblRef;
        Preconditions.checkNotNull(tblRef);
        if (tblRef.getJoinOp() == JoinOperator.FULL_OUTER_JOIN) {
            // full outer joins don't guarantee any value transfer
            continue;
        }
        // this is some form of outer or anti join
        SlotId outerSlot, innerSlot;
        if (tblRef.getId() == getTupleId(slotIds.first)) {
            innerSlot = slotIds.first;
            outerSlot = slotIds.second;
        } else if (tblRef.getId() == getTupleId(slotIds.second)) {
            innerSlot = slotIds.second;
            outerSlot = slotIds.first;
        } else {
            // actually be true
            continue;
        }
        // inverting the condition (paying special attention to NULLs).
        if (tblRef.getJoinOp() == JoinOperator.LEFT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.LEFT_ANTI_JOIN || tblRef.getJoinOp() == JoinOperator.NULL_AWARE_LEFT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(outerSlot, innerSlot));
        } else if (tblRef.getJoinOp() == JoinOperator.RIGHT_OUTER_JOIN || tblRef.getJoinOp() == JoinOperator.RIGHT_ANTI_JOIN) {
            valueTransfers.add(new Pair<SlotId, SlotId>(innerSlot, outerSlot));
        }
    }
}
#end_block

#method_before
private void unnestOperands(Analyzer analyzer) throws AnalysisException {
    if (operands_.size() == 1) {
        // ValuesStmt for a single row.
        allOperands_.add(operands_.get(0));
        return;
    }
    // find index of first ALL operand
    int firstUnionAllIdx = operands_.size();
    for (int i = 1; i < operands_.size(); ++i) {
        UnionOperand operand = operands_.get(i);
        if (operand.getQualifier() == Qualifier.ALL) {
            firstUnionAllIdx = (i == 1 ? 0 : i);
            break;
        }
    }
    // operands[0] is always implicitly ALL, so operands[1] can't be the
    // first one
    Preconditions.checkState(firstUnionAllIdx != 1);
    // unnest DISTINCT operands
    Preconditions.checkState(distinctOperands_.isEmpty());
    for (int i = 0; i < firstUnionAllIdx; ++i) {
        unnestOperand(distinctOperands_, Qualifier.DISTINCT, operands_.get(i));
    }
    // unnest ALL operands
    Preconditions.checkState(allOperands_.isEmpty());
    for (int i = firstUnionAllIdx; i < operands_.size(); ++i) {
        unnestOperand(allOperands_, Qualifier.ALL, operands_.get(i));
    }
    operands_.clear();
    for (UnionOperand op : distinctOperands_) {
        op.setQualifier(Qualifier.DISTINCT);
    }
    for (UnionOperand op : allOperands_) {
        op.setQualifier(Qualifier.ALL);
    }
    operands_.addAll(distinctOperands_);
    operands_.addAll(allOperands_);
}
#method_after
private void unnestOperands(Analyzer analyzer) throws AnalysisException {
    if (operands_.size() == 1) {
        // ValuesStmt for a single row.
        allOperands_.add(operands_.get(0));
        return;
    }
    // find index of first ALL operand
    int firstUnionAllIdx = operands_.size();
    for (int i = 1; i < operands_.size(); ++i) {
        UnionOperand operand = operands_.get(i);
        if (operand.getQualifier() == Qualifier.ALL) {
            firstUnionAllIdx = (i == 1 ? 0 : i);
            break;
        }
    }
    // operands[0] is always implicitly ALL, so operands[1] can't be the
    // first one
    Preconditions.checkState(firstUnionAllIdx != 1);
    // unnest DISTINCT operands
    Preconditions.checkState(distinctOperands_.isEmpty());
    for (int i = 0; i < firstUnionAllIdx; ++i) {
        unnestOperand(distinctOperands_, Qualifier.DISTINCT, operands_.get(i));
    }
    // unnest ALL operands
    Preconditions.checkState(allOperands_.isEmpty());
    for (int i = firstUnionAllIdx; i < operands_.size(); ++i) {
        unnestOperand(allOperands_, Qualifier.ALL, operands_.get(i));
    }
    for (UnionOperand op : distinctOperands_) op.setQualifier(Qualifier.DISTINCT);
    for (UnionOperand op : allOperands_) op.setQualifier(Qualifier.ALL);
    operands_.clear();
    operands_.addAll(distinctOperands_);
    operands_.addAll(allOperands_);
}
#end_block

#method_before
private void createMetadata(Analyzer analyzer) throws AnalysisException {
    // Create tuple descriptor for materialized tuple created by the union.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    tupleId_ = tupleDesc.getId();
    LOG.trace("UnionStmt.createMetadata: tupleId=" + tupleId_.toString());
    // One slot per expr in the select blocks. Use first select block as representative.
    List<Expr> firstSelectExprs = operands_.get(0).getQueryStmt().getResultExprs();
    // Compute column stats for the materialized slots from the source exprs.
    List<ColumnStats> columnStats = Lists.newArrayList();
    for (int i = 0; i < operands_.size(); ++i) {
        List<Expr> selectExprs = operands_.get(i).getQueryStmt().getResultExprs();
        for (int j = 0; j < selectExprs.size(); ++j) {
            ColumnStats statsToAdd = ColumnStats.fromExpr(selectExprs.get(j));
            if (i == 0) {
                columnStats.add(statsToAdd);
            } else {
                columnStats.get(j).add(statsToAdd);
            }
        }
    }
    // Create tuple descriptor and slots.
    for (int i = 0; i < firstSelectExprs.size(); ++i) {
        Expr expr = firstSelectExprs.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(getColLabels().get(i));
        slotDesc.setType(expr.getType());
        slotDesc.setStats(columnStats.get(i));
        SlotRef outputSlotRef = new SlotRef(slotDesc);
        resultExprs_.add(outputSlotRef);
        // Add to aliasSMap so that column refs in "order by" can be resolved.
        if (orderByElements_ != null) {
            SlotRef aliasRef = new SlotRef(getColLabels().get(i));
            if (aliasSmap_.containsMappingFor(aliasRef)) {
                ambiguousAliasList_.add(aliasRef);
            } else {
                aliasSmap_.put(aliasRef, outputSlotRef);
            }
        }
        // (see Planner.createInlineViewPlan() for the reasoning)
        for (UnionOperand op : operands_) {
            Expr resultExpr = op.getQueryStmt().getResultExprs().get(i);
            slotDesc.addSourceExpr(resultExpr);
            if (op.hasAnalyticExprs())
                continue;
            SlotRef slotRef = resultExpr.unwrapSlotRef(true);
            if (slotRef == null)
                continue;
            analyzer.registerValueTransfer(outputSlotRef.getSlotId(), slotRef.getSlotId());
        }
    }
    baseTblResultExprs_ = resultExprs_;
}
#method_after
private void createMetadata(Analyzer analyzer) throws AnalysisException {
    // Create tuple descriptor for materialized tuple created by the union.
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor("union");
    tupleDesc.setIsMaterialized(true);
    tupleId_ = tupleDesc.getId();
    if (LOG.isTraceEnabled()) {
        LOG.trace("UnionStmt.createMetadata: tupleId=" + tupleId_.toString());
    }
    // One slot per expr in the select blocks. Use first select block as representative.
    List<Expr> firstSelectExprs = operands_.get(0).getQueryStmt().getResultExprs();
    // Compute column stats for the materialized slots from the source exprs.
    List<ColumnStats> columnStats = Lists.newArrayList();
    for (int i = 0; i < operands_.size(); ++i) {
        List<Expr> selectExprs = operands_.get(i).getQueryStmt().getResultExprs();
        for (int j = 0; j < selectExprs.size(); ++j) {
            ColumnStats statsToAdd = ColumnStats.fromExpr(selectExprs.get(j));
            if (i == 0) {
                columnStats.add(statsToAdd);
            } else {
                columnStats.get(j).add(statsToAdd);
            }
        }
    }
    // Create tuple descriptor and slots.
    for (int i = 0; i < firstSelectExprs.size(); ++i) {
        Expr expr = firstSelectExprs.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(getColLabels().get(i));
        slotDesc.setType(expr.getType());
        slotDesc.setStats(columnStats.get(i));
        SlotRef outputSlotRef = new SlotRef(slotDesc);
        resultExprs_.add(outputSlotRef);
        // Add to aliasSMap so that column refs in "order by" can be resolved.
        if (orderByElements_ != null) {
            SlotRef aliasRef = new SlotRef(getColLabels().get(i));
            if (aliasSmap_.containsMappingFor(aliasRef)) {
                ambiguousAliasList_.add(aliasRef);
            } else {
                aliasSmap_.put(aliasRef, outputSlotRef);
            }
        }
        // (see Planner.createInlineViewPlan() for the reasoning)
        for (UnionOperand op : operands_) {
            Expr resultExpr = op.getQueryStmt().getResultExprs().get(i);
            slotDesc.addSourceExpr(resultExpr);
            if (op.hasAnalyticExprs())
                continue;
            SlotRef slotRef = resultExpr.unwrapSlotRef(true);
            if (slotRef == null)
                continue;
            analyzer.registerValueTransfer(outputSlotRef.getSlotId(), slotRef.getSlotId());
        }
    }
    baseTblResultExprs_ = resultExprs_;
}
#end_block

#method_before
@Override
public ArrayList<String> getColLabels() {
    Preconditions.checkState(operands_.size() > 0);
    return operands_.get(0).getQueryStmt().getColLabels();
}
#method_after
@Override
public List<String> getColLabels() {
    Preconditions.checkState(operands_.size() > 0);
    return operands_.get(0).getQueryStmt().getColLabels();
}
#end_block

#method_before
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    if (LOG.isDebugEnabled()) {
        LOG.debug("collecting partitions for table " + tbl_.getName());
    }
    numPartitionsMissingStats_ = 0;
    totalFiles_ = 0;
    totalBytes_ = 0;
    if (tbl_.getNumClusteringCols() == 0) {
        cardinality_ = tbl_.getNumRows();
        if (cardinality_ < -1 || (cardinality_ == 0 && tbl_.getTotalHdfsBytes() > 0)) {
            hasCorruptTableStats_ = true;
        }
        if (partitions_.isEmpty()) {
            // Nothing to scan. Definitely a cardinality of 0 even if we have no stats.
            cardinality_ = 0;
        } else {
            Preconditions.checkState(partitions_.size() == 1);
            totalFiles_ += partitions_.get(0).getFileDescriptors().size();
            totalBytes_ += partitions_.get(0).getSize();
        }
    } else {
        cardinality_ = 0;
        boolean hasValidPartitionCardinality = false;
        for (HdfsPartition p : partitions_) {
            // Check for corrupt table stats
            if (p.getNumRows() < -1 || (p.getNumRows() == 0 && p.getSize() > 0)) {
                hasCorruptTableStats_ = true;
            }
            // enough to change the planning outcome
            if (p.getNumRows() > -1) {
                cardinality_ = addCardinalities(cardinality_, p.getNumRows());
                hasValidPartitionCardinality = true;
            } else {
                ++numPartitionsMissingStats_;
            }
            totalFiles_ += p.getFileDescriptors().size();
            totalBytes_ += p.getSize();
        }
        if (!partitions_.isEmpty() && !hasValidPartitionCardinality) {
            // if none of the partitions knew its number of rows, we fall back on
            // the table stats
            cardinality_ = tbl_.getNumRows();
        }
    }
    // Adjust cardinality for all collections referenced along the tuple's path.
    if (cardinality_ != -1) {
        for (Type t : desc_.getPath().getMatchedTypes()) {
            if (t.isCollectionType())
                cardinality_ *= PlannerContext.AVG_COLLECTION_SIZE;
        }
    }
    inputCardinality_ = cardinality_;
    // Sanity check scan node cardinality.
    if (cardinality_ < -1) {
        hasCorruptTableStats_ = true;
        cardinality_ = -1;
    }
    if (cardinality_ > 0) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("cardinality_=" + Long.toString(cardinality_) + " sel=" + Double.toString(computeSelectivity()));
        }
        cardinality_ = Math.round(cardinality_ * computeSelectivity());
        // IMPALA-2165: Avoid setting the cardinality to 0 after rounding.
        cardinality_ = Math.max(cardinality_, 1);
    }
    cardinality_ = capAtLimit(cardinality_);
    if (LOG.isDebugEnabled()) {
        LOG.debug("computeStats HdfsScan: cardinality_=" + Long.toString(cardinality_));
    }
    computeNumNodes(analyzer, cardinality_);
    if (LOG.isDebugEnabled()) {
        LOG.debug("computeStats HdfsScan: #nodes=" + Integer.toString(numNodes_));
    }
}
#method_after
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    if (LOG.isTraceEnabled()) {
        LOG.trace("collecting partitions for table " + tbl_.getName());
    }
    numPartitionsMissingStats_ = 0;
    totalFiles_ = 0;
    totalBytes_ = 0;
    if (tbl_.getNumClusteringCols() == 0) {
        cardinality_ = tbl_.getNumRows();
        if (cardinality_ < -1 || (cardinality_ == 0 && tbl_.getTotalHdfsBytes() > 0)) {
            hasCorruptTableStats_ = true;
        }
        if (partitions_.isEmpty()) {
            // Nothing to scan. Definitely a cardinality of 0 even if we have no stats.
            cardinality_ = 0;
        } else {
            Preconditions.checkState(partitions_.size() == 1);
            totalFiles_ += partitions_.get(0).getFileDescriptors().size();
            totalBytes_ += partitions_.get(0).getSize();
        }
    } else {
        cardinality_ = 0;
        boolean hasValidPartitionCardinality = false;
        for (HdfsPartition p : partitions_) {
            // Check for corrupt table stats
            if (p.getNumRows() < -1 || (p.getNumRows() == 0 && p.getSize() > 0)) {
                hasCorruptTableStats_ = true;
            }
            // enough to change the planning outcome
            if (p.getNumRows() > -1) {
                cardinality_ = addCardinalities(cardinality_, p.getNumRows());
                hasValidPartitionCardinality = true;
            } else {
                ++numPartitionsMissingStats_;
            }
            totalFiles_ += p.getFileDescriptors().size();
            totalBytes_ += p.getSize();
        }
        if (!partitions_.isEmpty() && !hasValidPartitionCardinality) {
            // if none of the partitions knew its number of rows, we fall back on
            // the table stats
            cardinality_ = tbl_.getNumRows();
        }
    }
    // Adjust cardinality for all collections referenced along the tuple's path.
    if (cardinality_ != -1) {
        for (Type t : desc_.getPath().getMatchedTypes()) {
            if (t.isCollectionType())
                cardinality_ *= PlannerContext.AVG_COLLECTION_SIZE;
        }
    }
    inputCardinality_ = cardinality_;
    // Sanity check scan node cardinality.
    if (cardinality_ < -1) {
        hasCorruptTableStats_ = true;
        cardinality_ = -1;
    }
    if (cardinality_ > 0) {
        if (LOG.isTraceEnabled()) {
            LOG.trace("cardinality_=" + Long.toString(cardinality_) + " sel=" + Double.toString(computeSelectivity()));
        }
        cardinality_ = Math.round(cardinality_ * computeSelectivity());
        // IMPALA-2165: Avoid setting the cardinality to 0 after rounding.
        cardinality_ = Math.max(cardinality_, 1);
    }
    cardinality_ = capAtLimit(cardinality_);
    if (LOG.isTraceEnabled()) {
        LOG.trace("computeStats HdfsScan: cardinality_=" + Long.toString(cardinality_));
    }
    computeNumNodes(analyzer, cardinality_);
    if (LOG.isTraceEnabled()) {
        LOG.trace("computeStats HdfsScan: #nodes=" + Integer.toString(numNodes_));
    }
}
#end_block

#method_before
protected void computeNumNodes(Analyzer analyzer, long cardinality) {
    Preconditions.checkNotNull(scanRanges_);
    MembershipSnapshot cluster = MembershipSnapshot.getCluster();
    HashSet<TNetworkAddress> localHostSet = Sets.newHashSet();
    int totalNodes = 0;
    int numLocalRanges = 0;
    int numRemoteRanges = 0;
    for (TScanRangeLocationList range : scanRanges_) {
        boolean anyLocal = false;
        for (TScanRangeLocation loc : range.locations) {
            TNetworkAddress dataNode = analyzer.getHostIndex().getEntry(loc.getHost_idx());
            if (cluster.contains(dataNode)) {
                anyLocal = true;
                // Use the full datanode address (including port) to account for the test
                // minicluster where there are multiple datanodes and impalads on a single
                // host.  This assumes that when an impalad is colocated with a datanode,
                // there are the same number of impalads as datanodes on this host in this
                // cluster.
                localHostSet.add(dataNode);
            }
        }
        // will be scheduled on one of those nodes.
        if (anyLocal) {
            ++numLocalRanges;
        } else {
            ++numRemoteRanges;
        }
        // Approximate the number of nodes that will execute locally assigned ranges to
        // be the smaller of the number of locally assigned ranges and the number of
        // hosts that hold block replica for those ranges.
        int numLocalNodes = Math.min(numLocalRanges, localHostSet.size());
        // The remote ranges are round-robined across all the impalads.
        int numRemoteNodes = Math.min(numRemoteRanges, cluster.numNodes());
        // The local and remote assignments may overlap, but we don't know by how much so
        // conservatively assume no overlap.
        totalNodes = Math.min(numLocalNodes + numRemoteNodes, cluster.numNodes());
        // in case the number of scan ranges dominates the number of nodes.
        if (totalNodes == cluster.numNodes())
            break;
    }
    // Tables can reside on 0 nodes (empty table), but a plan node must always be
    // executed on at least one node.
    numNodes_ = (cardinality == 0 || totalNodes == 0) ? 1 : totalNodes;
    if (LOG.isDebugEnabled()) {
        LOG.debug("computeNumNodes totalRanges=" + scanRanges_.size() + " localRanges=" + numLocalRanges + " remoteRanges=" + numRemoteRanges + " localHostSet.size=" + localHostSet.size() + " clusterNodes=" + cluster.numNodes());
    }
}
#method_after
protected void computeNumNodes(Analyzer analyzer, long cardinality) {
    Preconditions.checkNotNull(scanRanges_);
    MembershipSnapshot cluster = MembershipSnapshot.getCluster();
    HashSet<TNetworkAddress> localHostSet = Sets.newHashSet();
    int totalNodes = 0;
    int numLocalRanges = 0;
    int numRemoteRanges = 0;
    for (TScanRangeLocationList range : scanRanges_) {
        boolean anyLocal = false;
        for (TScanRangeLocation loc : range.locations) {
            TNetworkAddress dataNode = analyzer.getHostIndex().getEntry(loc.getHost_idx());
            if (cluster.contains(dataNode)) {
                anyLocal = true;
                // Use the full datanode address (including port) to account for the test
                // minicluster where there are multiple datanodes and impalads on a single
                // host.  This assumes that when an impalad is colocated with a datanode,
                // there are the same number of impalads as datanodes on this host in this
                // cluster.
                localHostSet.add(dataNode);
            }
        }
        // will be scheduled on one of those nodes.
        if (anyLocal) {
            ++numLocalRanges;
        } else {
            ++numRemoteRanges;
        }
        // Approximate the number of nodes that will execute locally assigned ranges to
        // be the smaller of the number of locally assigned ranges and the number of
        // hosts that hold block replica for those ranges.
        int numLocalNodes = Math.min(numLocalRanges, localHostSet.size());
        // The remote ranges are round-robined across all the impalads.
        int numRemoteNodes = Math.min(numRemoteRanges, cluster.numNodes());
        // The local and remote assignments may overlap, but we don't know by how much so
        // conservatively assume no overlap.
        totalNodes = Math.min(numLocalNodes + numRemoteNodes, cluster.numNodes());
        // in case the number of scan ranges dominates the number of nodes.
        if (totalNodes == cluster.numNodes())
            break;
    }
    // Tables can reside on 0 nodes (empty table), but a plan node must always be
    // executed on at least one node.
    numNodes_ = (cardinality == 0 || totalNodes == 0) ? 1 : totalNodes;
    if (LOG.isTraceEnabled()) {
        LOG.trace("computeNumNodes totalRanges=" + scanRanges_.size() + " localRanges=" + numLocalRanges + " remoteRanges=" + numRemoteRanges + " localHostSet.size=" + localHostSet.size() + " clusterNodes=" + cluster.numNodes());
    }
}
#end_block

#method_before
@Test
public void testComputeStatsMtDop() {
    for (int mtDop : new int[] { -1, 0, 1, 16 }) {
        // MT_DOP is not set automatically for stmt other than COMPUTE STATS.
        testEffectiveMtDop("select * from functional_parquet.alltypes", mtDop, mtDop);
        // MT_DOP is not set automatically for COMPUTE STATS on non-Parquet tables.
        testEffectiveMtDop("compute stats functional.alltypes", mtDop, mtDop);
    }
    // MT_DOP is set automatically for COMPUTE STATS on Parquet tables,
    // but can be overridden by a user-provided MT_DOP.
    testEffectiveMtDop("compute stats functional_parquet.alltypes", -1, 4);
    testEffectiveMtDop("compute stats functional_parquet.alltypes", 0, 0);
    testEffectiveMtDop("compute stats functional_parquet.alltypes", 1, 1);
    testEffectiveMtDop("compute stats functional_parquet.alltypes", 16, 16);
}
#method_after
@Test
public void testComputeStatsMtDop() {
    for (int mtDop : new int[] { -1, 0, 1, 16 }) {
        int effectiveMtDop = (mtDop != -1) ? mtDop : 0;
        // MT_DOP is not set automatically for stmt other than COMPUTE STATS.
        testEffectiveMtDop("select * from functional_parquet.alltypes", mtDop, effectiveMtDop);
        // MT_DOP is not set automatically for COMPUTE STATS on non-Parquet tables.
        testEffectiveMtDop("compute stats functional.alltypes", mtDop, effectiveMtDop);
    }
    // MT_DOP is set automatically for COMPUTE STATS on Parquet tables,
    // but can be overridden by a user-provided MT_DOP.
    testEffectiveMtDop("compute stats functional_parquet.alltypes", -1, 4);
    testEffectiveMtDop("compute stats functional_parquet.alltypes", 0, 0);
    testEffectiveMtDop("compute stats functional_parquet.alltypes", 1, 1);
    testEffectiveMtDop("compute stats functional_parquet.alltypes", 16, 16);
}
#end_block

#method_before
private PlanNode createJoinPlan(Analyzer analyzer, TableRef leftmostRef, List<Pair<TableRef, PlanNode>> refPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    if (LOG.isTraceEnabled()) {
        LOG.trace("createJoinPlan: " + leftmostRef.getUniqueAlias());
    }
    // the refs that have yet to be joined
    List<Pair<TableRef, PlanNode>> remainingRefs = Lists.newArrayList();
    // root of accumulated join plan
    PlanNode root = null;
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        if (entry.first == leftmostRef) {
            root = entry.second;
        } else {
            remainingRefs.add(entry);
        }
    }
    Preconditions.checkNotNull(root);
    // Maps from a TableRef in refPlans with an outer/semi join op to the set of
    // TableRefs that precede it refPlans (i.e., in FROM-clause order).
    // The map is used to place outer/semi joins at a fixed position in the plan tree
    // (IMPALA-860), s.t. all the tables appearing to the left/right of an outer/semi
    // join in the original query still remain to the left/right after join ordering.
    // This prevents join re-ordering across outer/semi joins which is generally wrong.
    Map<TableRef, Set<TableRef>> precedingRefs = Maps.newHashMap();
    List<TableRef> tmpTblRefs = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        TableRef tblRef = entry.first;
        if (tblRef.getJoinOp().isOuterJoin() || tblRef.getJoinOp().isSemiJoin()) {
            precedingRefs.put(tblRef, Sets.newHashSet(tmpTblRefs));
        }
        tmpTblRefs.add(tblRef);
    }
    // Refs that have been joined. The union of joinedRefs and the refs in remainingRefs
    // are the set of all table refs.
    Set<TableRef> joinedRefs = Sets.newHashSet(leftmostRef);
    long numOps = 0;
    int i = 0;
    while (!remainingRefs.isEmpty()) {
        // We minimize the resulting cardinality at each step in the join chain,
        // which minimizes the total number of hash table lookups.
        PlanNode newRoot = null;
        Pair<TableRef, PlanNode> minEntry = null;
        for (Pair<TableRef, PlanNode> entry : remainingRefs) {
            TableRef ref = entry.first;
            JoinOperator joinOp = ref.getJoinOp();
            // Place outer/semi joins at a fixed position in the plan tree.
            Set<TableRef> requiredRefs = precedingRefs.get(ref);
            if (requiredRefs != null) {
                Preconditions.checkState(joinOp.isOuterJoin() || joinOp.isSemiJoin());
                // outer/semi joins.
                if (!requiredRefs.equals(joinedRefs))
                    break;
            }
            analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
            PlanNode candidate = createJoinNode(root, entry.second, ref, analyzer);
            if (candidate == null)
                continue;
            if (LOG.isTraceEnabled()) {
                LOG.trace("cardinality=" + Long.toString(candidate.getCardinality()));
            }
            // position in the plan.
            if (joinOp.isOuterJoin() || joinOp.isSemiJoin()) {
                newRoot = candidate;
                minEntry = entry;
                break;
            }
            // infrastructure.
            if (newRoot == null || (candidate.getClass().equals(newRoot.getClass()) && candidate.getCardinality() < newRoot.getCardinality()) || (candidate instanceof HashJoinNode && newRoot instanceof NestedLoopJoinNode)) {
                newRoot = candidate;
                minEntry = entry;
            }
        }
        if (newRoot == null) {
            // Could not generate a valid plan.
            return null;
        }
        // we need to insert every rhs row into the hash table and then look up
        // every lhs row
        long lhsCardinality = root.getCardinality();
        long rhsCardinality = minEntry.second.getCardinality();
        numOps += lhsCardinality + rhsCardinality;
        if (LOG.isDebugEnabled()) {
            LOG.debug(Integer.toString(i) + " chose " + minEntry.first.getUniqueAlias() + " #lhs=" + Long.toString(lhsCardinality) + " #rhs=" + Long.toString(rhsCardinality) + " #ops=" + Long.toString(numOps));
        }
        remainingRefs.remove(minEntry);
        joinedRefs.add(minEntry.first);
        root = newRoot;
        // Create a Subplan on top of the new root for all the subplan refs that can be
        // evaluated at this point.
        // TODO: Once we have stats on nested collections, we should consider the join
        // order in conjunction with the placement of SubplanNodes, i.e., move the creation
        // of SubplanNodes into the join-ordering loop above.
        root = createSubplan(root, subplanRefs, false, analyzer);
        // with a dense sequence of node ids
        if (root instanceof SubplanNode)
            root.getChild(0).setId(ctx_.getNextNodeId());
        root.setId(ctx_.getNextNodeId());
        analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
        ++i;
    }
    return root;
}
#method_after
private PlanNode createJoinPlan(Analyzer analyzer, TableRef leftmostRef, List<Pair<TableRef, PlanNode>> refPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    if (LOG.isTraceEnabled()) {
        LOG.trace("createJoinPlan: " + leftmostRef.getUniqueAlias());
    }
    // the refs that have yet to be joined
    List<Pair<TableRef, PlanNode>> remainingRefs = Lists.newArrayList();
    // root of accumulated join plan
    PlanNode root = null;
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        if (entry.first == leftmostRef) {
            root = entry.second;
        } else {
            remainingRefs.add(entry);
        }
    }
    Preconditions.checkNotNull(root);
    // Maps from a TableRef in refPlans with an outer/semi join op to the set of
    // TableRefs that precede it refPlans (i.e., in FROM-clause order).
    // The map is used to place outer/semi joins at a fixed position in the plan tree
    // (IMPALA-860), s.t. all the tables appearing to the left/right of an outer/semi
    // join in the original query still remain to the left/right after join ordering.
    // This prevents join re-ordering across outer/semi joins which is generally wrong.
    Map<TableRef, Set<TableRef>> precedingRefs = Maps.newHashMap();
    List<TableRef> tmpTblRefs = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        TableRef tblRef = entry.first;
        if (tblRef.getJoinOp().isOuterJoin() || tblRef.getJoinOp().isSemiJoin()) {
            precedingRefs.put(tblRef, Sets.newHashSet(tmpTblRefs));
        }
        tmpTblRefs.add(tblRef);
    }
    // Refs that have been joined. The union of joinedRefs and the refs in remainingRefs
    // are the set of all table refs.
    Set<TableRef> joinedRefs = Sets.newHashSet(leftmostRef);
    long numOps = 0;
    int i = 0;
    while (!remainingRefs.isEmpty()) {
        // We minimize the resulting cardinality at each step in the join chain,
        // which minimizes the total number of hash table lookups.
        PlanNode newRoot = null;
        Pair<TableRef, PlanNode> minEntry = null;
        for (Pair<TableRef, PlanNode> entry : remainingRefs) {
            TableRef ref = entry.first;
            JoinOperator joinOp = ref.getJoinOp();
            // Place outer/semi joins at a fixed position in the plan tree.
            Set<TableRef> requiredRefs = precedingRefs.get(ref);
            if (requiredRefs != null) {
                Preconditions.checkState(joinOp.isOuterJoin() || joinOp.isSemiJoin());
                // outer/semi joins.
                if (!requiredRefs.equals(joinedRefs))
                    break;
            }
            analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
            PlanNode candidate = createJoinNode(root, entry.second, ref, analyzer);
            if (candidate == null)
                continue;
            if (LOG.isTraceEnabled()) {
                LOG.trace("cardinality=" + Long.toString(candidate.getCardinality()));
            }
            // position in the plan.
            if (joinOp.isOuterJoin() || joinOp.isSemiJoin()) {
                newRoot = candidate;
                minEntry = entry;
                break;
            }
            // infrastructure.
            if (newRoot == null || (candidate.getClass().equals(newRoot.getClass()) && candidate.getCardinality() < newRoot.getCardinality()) || (candidate instanceof HashJoinNode && newRoot instanceof NestedLoopJoinNode)) {
                newRoot = candidate;
                minEntry = entry;
            }
        }
        if (newRoot == null) {
            // Could not generate a valid plan.
            return null;
        }
        // we need to insert every rhs row into the hash table and then look up
        // every lhs row
        long lhsCardinality = root.getCardinality();
        long rhsCardinality = minEntry.second.getCardinality();
        numOps += lhsCardinality + rhsCardinality;
        if (LOG.isTraceEnabled()) {
            LOG.trace(Integer.toString(i) + " chose " + minEntry.first.getUniqueAlias() + " #lhs=" + Long.toString(lhsCardinality) + " #rhs=" + Long.toString(rhsCardinality) + " #ops=" + Long.toString(numOps));
        }
        remainingRefs.remove(minEntry);
        joinedRefs.add(minEntry.first);
        root = newRoot;
        // Create a Subplan on top of the new root for all the subplan refs that can be
        // evaluated at this point.
        // TODO: Once we have stats on nested collections, we should consider the join
        // order in conjunction with the placement of SubplanNodes, i.e., move the creation
        // of SubplanNodes into the join-ordering loop above.
        root = createSubplan(root, subplanRefs, false, analyzer);
        // with a dense sequence of node ids
        if (root instanceof SubplanNode)
            root.getChild(0).setId(ctx_.getNextNodeId());
        root.setId(ctx_.getNextNodeId());
        analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
        ++i;
    }
    return root;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Resolve and analyze this table ref so we can evaluate partition predicates.
    TableRef tableRef = new TableRef(tableName_.toPath(), null, Privilege.ALTER);
    tableRef = analyzer.resolveTableRef(tableRef);
    Preconditions.checkNotNull(tableRef);
    tableRef.analyze(analyzer);
    if (tableRef instanceof InlineViewRef) {
        throw new AnalysisException(String.format("COMPUTE STATS not supported for view: %s", tableName_));
    }
    if (tableRef instanceof CollectionTableRef) {
        throw new AnalysisException(String.format("COMPUTE STATS not supported for nested collection: %s", tableName_));
    }
    table_ = analyzer.getTable(tableName_, Privilege.ALTER);
    if (!(table_ instanceof HdfsTable)) {
        if (partitionSet_ != null) {
            throw new AnalysisException("COMPUTE INCREMENTAL ... PARTITION not supported " + "for non-HDFS table " + tableName_);
        }
        isIncremental_ = false;
    }
    // Ensure that we write an entry for every partition if this isn't incremental
    if (!isIncremental_)
        expectAllPartitions_ = true;
    HdfsTable hdfsTable = null;
    if (table_ instanceof HdfsTable) {
        hdfsTable = (HdfsTable) table_;
        if (isIncremental_ && hdfsTable.getNumClusteringCols() == 0 && partitionSet_ != null) {
            throw new AnalysisException(String.format("Can't compute PARTITION stats on an unpartitioned table: %s", tableName_));
        } else if (partitionSet_ != null) {
            Preconditions.checkState(tableRef instanceof BaseTableRef);
            partitionSet_.setPartitionShouldExist();
            partitionSet_.analyze(analyzer);
        }
        // error if the estimate is greater than --inc_stats_size_limit_bytes in bytes
        if (isIncremental_) {
            long incStatMaxSize = BackendConfig.INSTANCE.getIncStatsMaxSize();
            long statsSizeEstimate = hdfsTable.getColumns().size() * hdfsTable.getPartitions().size() * HdfsTable.STATS_SIZE_PER_COLUMN_BYTES;
            if (statsSizeEstimate > incStatMaxSize) {
                LOG.error("Incremental stats size estimate for table " + hdfsTable.getName() + " exceeded " + incStatMaxSize + ", estimate = " + statsSizeEstimate);
                throw new AnalysisException("Incremental stats size estimate exceeds " + PrintUtils.printBytes(incStatMaxSize) + ". Please try COMPUTE STATS instead.");
            }
        }
    }
    // Build partition filters that only select partitions without valid statistics for
    // incremental computation.
    List<String> filterPreds = Lists.newArrayList();
    if (isIncremental_) {
        if (partitionSet_ == null) {
            // If any column does not have stats, we recompute statistics for all partitions
            // TODO: need a better way to invalidate stats for all partitions, so that we can
            // use this logic to only recompute new / changed columns.
            boolean tableIsMissingColStats = false;
            // We'll warn the user if a column is missing stats (and therefore we rescan the
            // whole table), but if all columns are missing stats, the table just doesn't have
            // any stats and there's no need to warn.
            boolean allColumnsMissingStats = true;
            String exampleColumnMissingStats = null;
            // Partition columns always have stats, so exclude them from this search
            for (Column col : table_.getNonClusteringColumns()) {
                if (!col.getStats().hasStats()) {
                    if (!tableIsMissingColStats) {
                        tableIsMissingColStats = true;
                        exampleColumnMissingStats = col.getName();
                    }
                } else {
                    allColumnsMissingStats = false;
                }
            }
            if (tableIsMissingColStats && !allColumnsMissingStats) {
                analyzer.addWarning("Column " + exampleColumnMissingStats + " does not have statistics, recomputing stats for the whole table");
            }
            for (HdfsPartition p : hdfsTable.getPartitions()) {
                if (p.isDefaultPartition())
                    continue;
                TPartitionStats partStats = p.getPartitionStats();
                if (!p.hasIncrementalStats() || tableIsMissingColStats) {
                    if (partStats == null) {
                        if (LOG.isTraceEnabled())
                            LOG.trace(p.toString() + " does not have stats");
                    }
                    if (!tableIsMissingColStats)
                        filterPreds.add(p.getConjunctSql());
                    List<String> partValues = Lists.newArrayList();
                    for (LiteralExpr partValue : p.getPartitionValues()) {
                        partValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, "NULL"));
                    }
                    expectedPartitions_.add(partValues);
                } else {
                    if (LOG.isTraceEnabled())
                        LOG.trace(p.toString() + " does have statistics");
                    validPartStats_.add(partStats);
                }
            }
            if (expectedPartitions_.size() == hdfsTable.getPartitions().size() - 1) {
                expectedPartitions_.clear();
                expectAllPartitions_ = true;
            }
        } else {
            List<HdfsPartition> targetPartitions = partitionSet_.getPartitions();
            // Always compute stats on a set of partitions when told to.
            List<String> partitionConjuncts = Lists.newArrayList();
            for (HdfsPartition targetPartition : targetPartitions) {
                partitionConjuncts.add(targetPartition.getConjunctSql());
                List<String> partValues = Lists.newArrayList();
                for (LiteralExpr partValue : targetPartition.getPartitionValues()) {
                    partValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, "NULL"));
                }
                expectedPartitions_.add(partValues);
            }
            filterPreds.add("(" + Joiner.on(" AND ").join(partitionConjuncts) + ")");
            for (HdfsPartition p : hdfsTable.getPartitions()) {
                if (p.isDefaultPartition())
                    continue;
                if (targetPartitions.contains(p))
                    continue;
                TPartitionStats partStats = p.getPartitionStats();
                if (partStats != null)
                    validPartStats_.add(partStats);
            }
        }
        if (filterPreds.size() == 0 && validPartStats_.size() != 0) {
            if (LOG.isDebugEnabled()) {
                LOG.debug("No partitions selected for incremental stats update");
            }
            analyzer.addWarning("No partitions selected for incremental stats update");
            return;
        }
    }
    if (filterPreds.size() > MAX_INCREMENTAL_PARTITIONS) {
        // TODO: Consider simply running for MAX_INCREMENTAL_PARTITIONS partitions, and then
        // advising the user to iterate.
        analyzer.addWarning("Too many partitions selected, doing full recomputation of incremental stats");
        filterPreds.clear();
        validPartStats_.clear();
    }
    List<String> groupByCols = Lists.newArrayList();
    List<String> partitionColsSelectList = Lists.newArrayList();
    // Only add group by clause for HdfsTables.
    if (hdfsTable != null) {
        if (hdfsTable.isAvroTable())
            checkIncompleteAvroSchema(hdfsTable);
        addPartitionCols(hdfsTable, partitionColsSelectList, groupByCols);
    }
    // Query for getting the per-partition row count and the total row count.
    StringBuilder tableStatsQueryBuilder = new StringBuilder("SELECT ");
    List<String> tableStatsSelectList = Lists.newArrayList();
    tableStatsSelectList.add("COUNT(*)");
    tableStatsSelectList.addAll(partitionColsSelectList);
    tableStatsQueryBuilder.append(Joiner.on(", ").join(tableStatsSelectList));
    tableStatsQueryBuilder.append(" FROM " + tableName_.toSql());
    // Query for getting the per-column NDVs and number of NULLs.
    List<String> columnStatsSelectList = getBaseColumnStatsQuerySelectList(analyzer);
    if (isIncremental_)
        columnStatsSelectList.addAll(partitionColsSelectList);
    StringBuilder columnStatsQueryBuilder = new StringBuilder("SELECT ");
    columnStatsQueryBuilder.append(Joiner.on(", ").join(columnStatsSelectList));
    columnStatsQueryBuilder.append(" FROM " + tableName_.toSql());
    // selected in).
    if (filterPreds.size() > 0 && (validPartStats_.size() > 0 || partitionSet_ != null)) {
        String filterClause = " WHERE " + Joiner.on(" OR ").join(filterPreds);
        columnStatsQueryBuilder.append(filterClause);
        tableStatsQueryBuilder.append(filterClause);
    }
    if (groupByCols.size() > 0) {
        String groupBy = " GROUP BY " + Joiner.on(", ").join(groupByCols);
        if (isIncremental_)
            columnStatsQueryBuilder.append(groupBy);
        tableStatsQueryBuilder.append(groupBy);
    }
    tableStatsQueryStr_ = tableStatsQueryBuilder.toString();
    if (LOG.isDebugEnabled())
        LOG.debug("Table stats query: " + tableStatsQueryStr_);
    if (columnStatsSelectList.isEmpty()) {
        // Table doesn't have any columns that we can compute stats for.
        if (LOG.isDebugEnabled()) {
            LOG.debug("No supported column types in table " + table_.getTableName() + ", no column statistics will be gathered.");
        }
        columnStatsQueryStr_ = null;
        return;
    }
    columnStatsQueryStr_ = columnStatsQueryBuilder.toString();
    if (LOG.isDebugEnabled())
        LOG.debug("Column stats query: " + columnStatsQueryStr_);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Resolve and analyze this table ref so we can evaluate partition predicates.
    TableRef tableRef = new TableRef(tableName_.toPath(), null, Privilege.ALTER);
    tableRef = analyzer.resolveTableRef(tableRef);
    Preconditions.checkNotNull(tableRef);
    tableRef.analyze(analyzer);
    if (tableRef instanceof InlineViewRef) {
        throw new AnalysisException(String.format("COMPUTE STATS not supported for view: %s", tableName_));
    }
    if (tableRef instanceof CollectionTableRef) {
        throw new AnalysisException(String.format("COMPUTE STATS not supported for nested collection: %s", tableName_));
    }
    table_ = analyzer.getTable(tableName_, Privilege.ALTER);
    if (!(table_ instanceof HdfsTable)) {
        if (partitionSet_ != null) {
            throw new AnalysisException("COMPUTE INCREMENTAL ... PARTITION not supported " + "for non-HDFS table " + tableName_);
        }
        isIncremental_ = false;
    }
    // Ensure that we write an entry for every partition if this isn't incremental
    if (!isIncremental_)
        expectAllPartitions_ = true;
    HdfsTable hdfsTable = null;
    if (table_ instanceof HdfsTable) {
        hdfsTable = (HdfsTable) table_;
        if (isIncremental_ && hdfsTable.getNumClusteringCols() == 0 && partitionSet_ != null) {
            throw new AnalysisException(String.format("Can't compute PARTITION stats on an unpartitioned table: %s", tableName_));
        } else if (partitionSet_ != null) {
            Preconditions.checkState(tableRef instanceof BaseTableRef);
            partitionSet_.setPartitionShouldExist();
            partitionSet_.analyze(analyzer);
        }
        // error if the estimate is greater than --inc_stats_size_limit_bytes in bytes
        if (isIncremental_) {
            long incStatMaxSize = BackendConfig.INSTANCE.getIncStatsMaxSize();
            long statsSizeEstimate = hdfsTable.getColumns().size() * hdfsTable.getPartitions().size() * HdfsTable.STATS_SIZE_PER_COLUMN_BYTES;
            if (statsSizeEstimate > incStatMaxSize) {
                LOG.error("Incremental stats size estimate for table " + hdfsTable.getName() + " exceeded " + incStatMaxSize + ", estimate = " + statsSizeEstimate);
                throw new AnalysisException("Incremental stats size estimate exceeds " + PrintUtils.printBytes(incStatMaxSize) + ". Please try COMPUTE STATS instead.");
            }
        }
    }
    // Build partition filters that only select partitions without valid statistics for
    // incremental computation.
    List<String> filterPreds = Lists.newArrayList();
    if (isIncremental_) {
        if (partitionSet_ == null) {
            // If any column does not have stats, we recompute statistics for all partitions
            // TODO: need a better way to invalidate stats for all partitions, so that we can
            // use this logic to only recompute new / changed columns.
            boolean tableIsMissingColStats = false;
            // We'll warn the user if a column is missing stats (and therefore we rescan the
            // whole table), but if all columns are missing stats, the table just doesn't have
            // any stats and there's no need to warn.
            boolean allColumnsMissingStats = true;
            String exampleColumnMissingStats = null;
            // Partition columns always have stats, so exclude them from this search
            for (Column col : table_.getNonClusteringColumns()) {
                if (!col.getStats().hasStats()) {
                    if (!tableIsMissingColStats) {
                        tableIsMissingColStats = true;
                        exampleColumnMissingStats = col.getName();
                    }
                } else {
                    allColumnsMissingStats = false;
                }
            }
            if (tableIsMissingColStats && !allColumnsMissingStats) {
                analyzer.addWarning("Column " + exampleColumnMissingStats + " does not have statistics, recomputing stats for the whole table");
            }
            for (HdfsPartition p : hdfsTable.getPartitions()) {
                if (p.isDefaultPartition())
                    continue;
                TPartitionStats partStats = p.getPartitionStats();
                if (!p.hasIncrementalStats() || tableIsMissingColStats) {
                    if (partStats == null) {
                        if (LOG.isTraceEnabled())
                            LOG.trace(p.toString() + " does not have stats");
                    }
                    if (!tableIsMissingColStats)
                        filterPreds.add(p.getConjunctSql());
                    List<String> partValues = Lists.newArrayList();
                    for (LiteralExpr partValue : p.getPartitionValues()) {
                        partValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, "NULL"));
                    }
                    expectedPartitions_.add(partValues);
                } else {
                    if (LOG.isTraceEnabled())
                        LOG.trace(p.toString() + " does have statistics");
                    validPartStats_.add(partStats);
                }
            }
            if (expectedPartitions_.size() == hdfsTable.getPartitions().size() - 1) {
                expectedPartitions_.clear();
                expectAllPartitions_ = true;
            }
        } else {
            List<HdfsPartition> targetPartitions = partitionSet_.getPartitions();
            // Always compute stats on a set of partitions when told to.
            List<String> partitionConjuncts = Lists.newArrayList();
            for (HdfsPartition targetPartition : targetPartitions) {
                partitionConjuncts.add(targetPartition.getConjunctSql());
                List<String> partValues = Lists.newArrayList();
                for (LiteralExpr partValue : targetPartition.getPartitionValues()) {
                    partValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, "NULL"));
                }
                expectedPartitions_.add(partValues);
            }
            filterPreds.add("(" + Joiner.on(" AND ").join(partitionConjuncts) + ")");
            for (HdfsPartition p : hdfsTable.getPartitions()) {
                if (p.isDefaultPartition())
                    continue;
                if (targetPartitions.contains(p))
                    continue;
                TPartitionStats partStats = p.getPartitionStats();
                if (partStats != null)
                    validPartStats_.add(partStats);
            }
        }
        if (filterPreds.size() == 0 && validPartStats_.size() != 0) {
            if (LOG.isTraceEnabled()) {
                LOG.trace("No partitions selected for incremental stats update");
            }
            analyzer.addWarning("No partitions selected for incremental stats update");
            return;
        }
    }
    if (filterPreds.size() > MAX_INCREMENTAL_PARTITIONS) {
        // TODO: Consider simply running for MAX_INCREMENTAL_PARTITIONS partitions, and then
        // advising the user to iterate.
        analyzer.addWarning("Too many partitions selected, doing full recomputation of incremental stats");
        filterPreds.clear();
        validPartStats_.clear();
    }
    List<String> groupByCols = Lists.newArrayList();
    List<String> partitionColsSelectList = Lists.newArrayList();
    // Only add group by clause for HdfsTables.
    if (hdfsTable != null) {
        if (hdfsTable.isAvroTable())
            checkIncompleteAvroSchema(hdfsTable);
        addPartitionCols(hdfsTable, partitionColsSelectList, groupByCols);
    }
    // Query for getting the per-partition row count and the total row count.
    StringBuilder tableStatsQueryBuilder = new StringBuilder("SELECT ");
    List<String> tableStatsSelectList = Lists.newArrayList();
    tableStatsSelectList.add("COUNT(*)");
    tableStatsSelectList.addAll(partitionColsSelectList);
    tableStatsQueryBuilder.append(Joiner.on(", ").join(tableStatsSelectList));
    tableStatsQueryBuilder.append(" FROM " + tableName_.toSql());
    // Query for getting the per-column NDVs and number of NULLs.
    List<String> columnStatsSelectList = getBaseColumnStatsQuerySelectList(analyzer);
    if (isIncremental_)
        columnStatsSelectList.addAll(partitionColsSelectList);
    StringBuilder columnStatsQueryBuilder = new StringBuilder("SELECT ");
    columnStatsQueryBuilder.append(Joiner.on(", ").join(columnStatsSelectList));
    columnStatsQueryBuilder.append(" FROM " + tableName_.toSql());
    // selected in).
    if (filterPreds.size() > 0 && (validPartStats_.size() > 0 || partitionSet_ != null)) {
        String filterClause = " WHERE " + Joiner.on(" OR ").join(filterPreds);
        columnStatsQueryBuilder.append(filterClause);
        tableStatsQueryBuilder.append(filterClause);
    }
    if (groupByCols.size() > 0) {
        String groupBy = " GROUP BY " + Joiner.on(", ").join(groupByCols);
        if (isIncremental_)
            columnStatsQueryBuilder.append(groupBy);
        tableStatsQueryBuilder.append(groupBy);
    }
    tableStatsQueryStr_ = tableStatsQueryBuilder.toString();
    if (LOG.isTraceEnabled())
        LOG.trace("Table stats query: " + tableStatsQueryStr_);
    if (columnStatsSelectList.isEmpty()) {
        // Table doesn't have any columns that we can compute stats for.
        if (LOG.isTraceEnabled()) {
            LOG.trace("No supported column types in table " + table_.getTableName() + ", no column statistics will be gathered.");
        }
        columnStatsQueryStr_ = null;
        return;
    }
    columnStatsQueryStr_ = columnStatsQueryBuilder.toString();
    if (LOG.isTraceEnabled())
        LOG.trace("Column stats query: " + columnStatsQueryStr_);
}
#end_block

#method_before
private AnalysisContext.AnalysisResult analyzeStmt(TQueryCtx queryCtx) throws AnalysisException, InternalException, AuthorizationException {
    if (!impaladCatalog_.isReady()) {
        throw new AnalysisException("This Impala daemon is not ready to accept user " + "requests. Status: Waiting for catalog update from the StateStore.");
    }
    AnalysisContext analysisCtx = new AnalysisContext(impaladCatalog_, queryCtx, authzConfig_);
    if (LOG.isDebugEnabled())
        LOG.debug("analyze query " + queryCtx.request.stmt);
    // 3) Analysis fails with an AuthorizationException.
    try {
        while (true) {
            try {
                analysisCtx.analyze(queryCtx.request.stmt);
                Preconditions.checkState(analysisCtx.getAnalyzer().getMissingTbls().isEmpty());
                return analysisCtx.getAnalysisResult();
            } catch (AnalysisException e) {
                Set<TableName> missingTbls = analysisCtx.getAnalyzer().getMissingTbls();
                // Only re-throw the AnalysisException if there were no missing tables.
                if (missingTbls.isEmpty())
                    throw e;
                // Some tables/views were missing, request and wait for them to load.
                if (!requestTblLoadAndWait(missingTbls, MISSING_TBL_LOAD_WAIT_TIMEOUT_MS)) {
                    if (LOG.isDebugEnabled()) {
                        LOG.debug(String.format("Missing tables were not received in %dms. Load " + "request will be retried.", MISSING_TBL_LOAD_WAIT_TIMEOUT_MS));
                    }
                }
            }
        }
    } finally {
        // Authorize all accesses.
        // AuthorizationExceptions must take precedence over any AnalysisException
        // that has been thrown, so perform the authorization first.
        analysisCtx.authorize(getAuthzChecker());
    }
}
#method_after
private AnalysisContext.AnalysisResult analyzeStmt(TQueryCtx queryCtx) throws AnalysisException, InternalException, AuthorizationException {
    if (!impaladCatalog_.isReady()) {
        throw new AnalysisException("This Impala daemon is not ready to accept user " + "requests. Status: Waiting for catalog update from the StateStore.");
    }
    AnalysisContext analysisCtx = new AnalysisContext(impaladCatalog_, queryCtx, authzConfig_);
    if (LOG.isTraceEnabled())
        LOG.trace("analyze query " + queryCtx.request.stmt);
    // 3) Analysis fails with an AuthorizationException.
    try {
        while (true) {
            try {
                analysisCtx.analyze(queryCtx.request.stmt);
                Preconditions.checkState(analysisCtx.getAnalyzer().getMissingTbls().isEmpty());
                return analysisCtx.getAnalysisResult();
            } catch (AnalysisException e) {
                Set<TableName> missingTbls = analysisCtx.getAnalyzer().getMissingTbls();
                // Only re-throw the AnalysisException if there were no missing tables.
                if (missingTbls.isEmpty())
                    throw e;
                // Some tables/views were missing, request and wait for them to load.
                if (!requestTblLoadAndWait(missingTbls, MISSING_TBL_LOAD_WAIT_TIMEOUT_MS)) {
                    if (LOG.isTraceEnabled()) {
                        LOG.trace(String.format("Missing tables were not received in %dms. Load " + "request will be retried.", MISSING_TBL_LOAD_WAIT_TIMEOUT_MS));
                    }
                }
            }
        }
    } finally {
        // Authorize all accesses.
        // AuthorizationExceptions must take precedence over any AnalysisException
        // that has been thrown, so perform the authorization first.
        analysisCtx.authorize(getAuthzChecker());
    }
}
#end_block

#method_before
private TQueryExecRequest createExecRequest(Planner planner, StringBuilder explainString) throws ImpalaException {
    TQueryCtx queryCtx = planner.getQueryCtx();
    AnalysisContext.AnalysisResult analysisResult = planner.getAnalysisResult();
    boolean isMtExec = analysisResult.isQueryStmt() && queryCtx.request.query_options.isSetMt_dop() && queryCtx.request.query_options.mt_dop > 0;
    List<PlanFragment> planRoots = Lists.newArrayList();
    TQueryExecRequest result = new TQueryExecRequest();
    if (isMtExec) {
        LOG.debug("create mt plan");
        planRoots.addAll(planner.createParallelPlans());
    } else {
        LOG.debug("create plan");
        planRoots.add(planner.createPlan().get(0));
    }
    // assembling a warning message
    for (PlanFragment planRoot : planRoots) {
        result.addToPlan_exec_info(createPlanExecInfo(planRoot, planner, queryCtx, result));
    }
    // Optionally disable spilling in the backend. Allow spilling if there are plan hints
    // or if all tables have stats.
    boolean disableSpilling = queryCtx.request.query_options.isDisable_unsafe_spills() && !queryCtx.tables_missing_stats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints();
    // for now, always disable spilling for multi-threaded execution
    if (isMtExec || disableSpilling)
        queryCtx.setDisable_spilling(true);
    // assign fragment idx
    int idx = 0;
    for (TPlanExecInfo planExecInfo : result.plan_exec_info) {
        for (TPlanFragment fragment : planExecInfo.fragments) fragment.setIdx(idx++);
    }
    // create EXPLAIN output after setting everything else
    // needed by getExplainString()
    result.setQuery_ctx(queryCtx);
    ArrayList<PlanFragment> allFragments = planRoots.get(0).getNodesPreOrder();
    explainString.append(planner.getExplainString(allFragments, result));
    result.setQuery_plan(explainString.toString());
    return result;
}
#method_after
private TQueryExecRequest createExecRequest(Planner planner, StringBuilder explainString) throws ImpalaException {
    TQueryCtx queryCtx = planner.getQueryCtx();
    AnalysisContext.AnalysisResult analysisResult = planner.getAnalysisResult();
    boolean isMtExec = analysisResult.isQueryStmt() && queryCtx.request.query_options.isSetMt_dop() && queryCtx.request.query_options.mt_dop > 0;
    List<PlanFragment> planRoots = Lists.newArrayList();
    TQueryExecRequest result = new TQueryExecRequest();
    if (isMtExec) {
        LOG.trace("create mt plan");
        planRoots.addAll(planner.createParallelPlans());
    } else {
        LOG.trace("create plan");
        planRoots.add(planner.createPlan().get(0));
    }
    // assembling a warning message
    for (PlanFragment planRoot : planRoots) {
        result.addToPlan_exec_info(createPlanExecInfo(planRoot, planner, queryCtx, result));
    }
    // Optionally disable spilling in the backend. Allow spilling if there are plan hints
    // or if all tables have stats.
    boolean disableSpilling = queryCtx.request.query_options.isDisable_unsafe_spills() && !queryCtx.tables_missing_stats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints();
    // for now, always disable spilling for multi-threaded execution
    if (isMtExec || disableSpilling)
        queryCtx.setDisable_spilling(true);
    // assign fragment idx
    int idx = 0;
    for (TPlanExecInfo planExecInfo : result.plan_exec_info) {
        for (TPlanFragment fragment : planExecInfo.fragments) fragment.setIdx(idx++);
    }
    // create EXPLAIN output after setting everything else
    // needed by getExplainString()
    result.setQuery_ctx(queryCtx);
    ArrayList<PlanFragment> allFragments = planRoots.get(0).getNodesPreOrder();
    explainString.append(planner.getExplainString(allFragments, result));
    result.setQuery_plan(explainString.toString());
    return result;
}
#end_block

#method_before
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        TLineageGraph thriftLineageGraph = analysisResult.getThriftLineageGraph();
        if (thriftLineageGraph != null && thriftLineageGraph.isSetQuery_text()) {
            result.catalog_op_request.setLineage_graph(thriftLineageGraph);
        }
        // provided another value for MT_DOP.
        if (!queryCtx.request.query_options.isSetMt_dop() && analysisResult.isComputeStatsStmt() && analysisResult.getComputeStatsStmt().isParquetOnly()) {
            queryCtx.request.query_options.setMt_dop(4);
        }
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt() || analysisResult.isUpdateStmt() || analysisResult.isDeleteStmt());
    Planner planner = new Planner(analysisResult, queryCtx);
    TQueryExecRequest queryExecRequest = createExecRequest(planner, explainString);
    queryExecRequest.setDesc_tbl(planner.getAnalysisResult().getAnalyzer().getDescTbl().toThrift());
    queryExecRequest.setQuery_ctx(queryCtx);
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    TLineageGraph thriftLineageGraph = analysisResult.getThriftLineageGraph();
    if (thriftLineageGraph != null && thriftLineageGraph.isSetQuery_text()) {
        queryExecRequest.setLineage_graph(thriftLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else if (analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt()) {
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(DescriptorTable.TABLE_SINK_ID);
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    } else {
        Preconditions.checkState(analysisResult.isUpdateStmt() || analysisResult.isDeleteStmt());
        result.stmt_type = TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
    }
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#method_after
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    TQueryOptions queryOptions = queryCtx.request.query_options;
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        TLineageGraph thriftLineageGraph = analysisResult.getThriftLineageGraph();
        if (thriftLineageGraph != null && thriftLineageGraph.isSetQuery_text()) {
            result.catalog_op_request.setLineage_graph(thriftLineageGraph);
        }
        // provided another value for MT_DOP.
        if (!queryOptions.isSetMt_dop() && analysisResult.isComputeStatsStmt() && analysisResult.getComputeStatsStmt().isParquetOnly()) {
            queryOptions.setMt_dop(4);
        }
        // If unset, set MT_DOP to 0 to simplify the rest of the code.
        if (!queryOptions.isSetMt_dop())
            queryOptions.setMt_dop(0);
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // If unset, set MT_DOP to 0 to simplify the rest of the code.
    if (!queryOptions.isSetMt_dop())
        queryOptions.setMt_dop(0);
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt() || analysisResult.isUpdateStmt() || analysisResult.isDeleteStmt());
    Planner planner = new Planner(analysisResult, queryCtx);
    TQueryExecRequest queryExecRequest = createExecRequest(planner, explainString);
    queryExecRequest.setDesc_tbl(planner.getAnalysisResult().getAnalyzer().getDescTbl().toThrift());
    queryExecRequest.setQuery_ctx(queryCtx);
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    TLineageGraph thriftLineageGraph = analysisResult.getThriftLineageGraph();
    if (thriftLineageGraph != null && thriftLineageGraph.isSetQuery_text()) {
        queryExecRequest.setLineage_graph(thriftLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.trace("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else if (analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt()) {
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(DescriptorTable.TABLE_SINK_ID);
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    } else {
        Preconditions.checkState(analysisResult.isUpdateStmt() || analysisResult.isDeleteStmt());
        result.stmt_type = TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
    }
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#end_block

#method_before
public static int getDiskId(String host, String storageId) {
    Preconditions.checkState(!Strings.isNullOrEmpty(host));
    // Initialize the diskId as -1 to indicate it is unknown
    int diskId = -1;
    if (Strings.isNullOrEmpty(storageId))
        return diskId;
    // Check if an existing mapping is already preset
    if (storageIdToInt.containsKey(storageId)) {
        return storageIdToInt.get(storageId).intValue();
    }
    // No mapping exists, create a new disk ID for 'storageId' on 'host'
    if (storageIdGenerator.containsKey(host)) {
        diskId = storageIdGenerator.get(host).incrementAndGet();
    } else {
        // Host hasn't been populated yet. Start with a 0-based id.
        diskId = 0;
        storageIdGenerator.put(host, new AtomicInteger(diskId));
    }
    storageIdToInt.put(storageId, new Integer(diskId));
    return diskId;
}
#method_after
public int getDiskId(String host, String storageUuid) {
    Preconditions.checkState(!Strings.isNullOrEmpty(host));
    // Initialize the diskId as -1 to indicate it is unknown
    int diskId = -1;
    // Check if an existing mapping is already present. This is intentionally kept
    // out of the synchronized block to avoid contention for lookups. Once a reasonable
    // amount of data loading is done and storageIdtoInt is populated with storage IDs
    // across the cluster, we expect to have a good hit rate.
    Integer intId = storageUuidToDiskId.get(storageUuid);
    if (intId != null)
        return intId;
    synchronized (storageIdGenerator) {
        // Mapping might have been added by another thread that entered the synchronized
        // block first.
        intId = storageUuidToDiskId.get(storageUuid);
        if (intId != null)
            return intId;
        // No mapping exists, create a new disk ID for 'storageUuid'
        if (storageIdGenerator.containsKey(host)) {
            diskId = storageIdGenerator.get(host) + 1;
        } else {
            // First diskId of this host.
            diskId = 0;
        }
        storageIdGenerator.put(host, new Integer(diskId));
        storageUuidToDiskId.put(storageUuid, new Integer(diskId));
    }
    return diskId;
}
#end_block

#method_before
private void loadBlockMetadata(FileSystem fs, Path dirPath, FilteredPartitionPathsMd pathMd, Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    // block location metadata based on file formats.
    if (!FileSystemUtil.supportsBlockLocationAPI(fs)) {
        synthesizeBlockMetadata(fs, pathMd);
        return;
    }
    Preconditions.checkNotNull(perFsFileBlocks);
    try {
        // No need to load blocks for empty directories.
        if (pathMd.filteredParitionPaths.size() == 0 || !fs.exists(dirPath))
            return;
        LOG.debug("Loading block md for " + name_ + " directory " + dirPath.toString());
        RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
        while (fileStatusIter.hasNext()) {
            LocatedFileStatus fileStatus = fileStatusIter.next();
            if (!pathMd.filteredParitionPaths.contains(fileStatus.getPath()))
                continue;
            BlockLocation[] locations = fileStatus.getBlockLocations();
            // Get the FileDescriptor corresponding to the file path index.
            FileDescriptor fd = pathMd.pathFds.get(pathMd.filteredParitionPaths.indexOf(fileStatus.getPath()));
            // Loop over all blocks in the file.
            for (BlockLocation loc : locations) {
                Preconditions.checkNotNull(loc);
                // Get the location of all block replicas in ip:port format.
                String[] blockHostPorts = loc.getNames();
                // Get the hostnames for all block replicas. Used to resolve which hosts
                // contain cached data. The results are returned in the same order as
                // block.getNames() so it allows us to match a host specified as ip:port to
                // corresponding hostname using the same array index.
                String[] blockHostNames = loc.getHosts();
                Preconditions.checkState(blockHostNames.length == blockHostPorts.length);
                // Get the hostnames that contain cached replicas of this block.
                Set<String> cachedHosts = Sets.newHashSet(Arrays.asList(loc.getCachedHosts()));
                Preconditions.checkState(cachedHosts.size() <= blockHostNames.length);
                // Now enumerate all replicas of the block, adding any unknown hosts
                // to hostMap_/hostList_. The host ID (index in to the hostList_) for each
                // replica is stored in replicaHostIdxs.
                List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(blockHostPorts.length);
                for (int i = 0; i < blockHostPorts.length; ++i) {
                    TNetworkAddress networkAddress = BlockReplica.parseLocation(blockHostPorts[i]);
                    Preconditions.checkState(networkAddress != null);
                    replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(blockHostNames[i])));
                }
                fd.addFileBlock(new FileBlock(loc.getOffset(), loc.getLength(), replicas));
            }
            // Remember the THdfsFileBlocks and corresponding BlockLocations.  Once all the
            // blocks are collected, the disk IDs will be queried in one batch per filesystem.
            addPerFsFileBlocks(perFsFileBlocks, fs, fd.getFileBlocks(), Arrays.asList(locations));
        }
    } catch (IOException e) {
        throw new RuntimeException("Error loading block md for directory " + dirPath.toString() + ": " + e.getMessage(), e);
    }
}
#method_after
private void loadBlockMetadata(Path dirPath, HashMap<Path, List<HdfsPartition>> partsByPath) {
    try {
        FileSystem fs = dirPath.getFileSystem(CONF);
        // No need to load blocks for empty partitions list.
        if (partsByPath.size() == 0 || !fs.exists(dirPath))
            return;
        if (LOG.isTraceEnabled()) {
            LOG.trace("Loading block md for " + name_ + " directory " + dirPath.toString());
        }
        // on the current snapshot of files in the directory.
        for (Map.Entry<Path, List<HdfsPartition>> entry : partsByPath.entrySet()) {
            Path partDir = entry.getKey();
            if (!FileSystemUtil.isDescendantPath(partDir, dirPath))
                continue;
            for (HdfsPartition partition : entry.getValue()) {
                partition.setFileDescriptors(new ArrayList<FileDescriptor>());
            }
        }
        // block location metadata based on file formats.
        if (!FileSystemUtil.supportsStorageIds(fs)) {
            synthesizeBlockMetadata(fs, dirPath, partsByPath);
            return;
        }
        int unknownDiskIdCount = 0;
        RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
        while (fileStatusIter.hasNext()) {
            LocatedFileStatus fileStatus = fileStatusIter.next();
            if (!FileSystemUtil.isValidDataFile(fileStatus))
                continue;
            // Find the partition that this file belongs (if any).
            Path partPathDir = fileStatus.getPath().getParent();
            Preconditions.checkNotNull(partPathDir);
            List<HdfsPartition> partitions = partsByPath.get(partPathDir);
            // Skip if this file does not belong to any known partition.
            if (partitions == null) {
                if (LOG.isTraceEnabled()) {
                    LOG.trace("File " + fileStatus.getPath().toString() + " doesn't correspond " + " to a known partition. Skipping metadata load for this file.");
                }
                continue;
            }
            String fileName = fileStatus.getPath().getName();
            FileDescriptor fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
            BlockLocation[] locations = fileStatus.getBlockLocations();
            String partPathDirName = partPathDir.toString();
            for (BlockLocation loc : locations) {
                Set<String> cachedHosts = Sets.newHashSet(loc.getCachedHosts());
                // Enumerate all replicas of the block, adding any unknown hosts
                // to hostIndex_. We pick the network address from getNames() and
                // map it to the corresponding hostname from getHosts().
                List<BlockReplica> replicas = Lists.newArrayListWithExpectedSize(loc.getNames().length);
                for (int i = 0; i < loc.getNames().length; ++i) {
                    TNetworkAddress networkAddress = BlockReplica.parseLocation(loc.getNames()[i]);
                    replicas.add(new BlockReplica(hostIndex_.getIndex(networkAddress), cachedHosts.contains(loc.getHosts()[i])));
                }
                FileBlock currentBlock = new FileBlock(loc.getOffset(), loc.getLength(), replicas);
                THdfsFileBlock tHdfsFileBlock = currentBlock.toThrift();
                fd.addThriftFileBlock(tHdfsFileBlock);
                unknownDiskIdCount += loadDiskIds(loc, tHdfsFileBlock);
            }
            if (LOG.isTraceEnabled()) {
                LOG.trace("Adding file md dir: " + partPathDirName + " file: " + fileName);
            }
            // Update the partitions' metadata that this file belongs to.
            for (HdfsPartition partition : partitions) {
                partition.getFileDescriptors().add(fd);
                numHdfsFiles_++;
                totalHdfsBytes_ += fd.getFileLength();
            }
        }
        if (unknownDiskIdCount > 0) {
            if (LOG.isWarnEnabled()) {
                LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
            }
        }
    } catch (IOException e) {
        throw new RuntimeException("Error loading block metadata for directory " + dirPath.toString() + ": " + e.getMessage(), e);
    }
}
#end_block

#method_before
private void loadDiskIds(Map<FsKey, FileBlocksInfo> perFsFileBlocks) {
    // for all the blocks.
    for (FsKey fsKey : perFsFileBlocks.keySet()) {
        FileSystem fs = fsKey.filesystem;
        // Only build the storage IDs fs instances that support the BlockLocation API.
        if (!FileSystemUtil.supportsBlockLocationAPI(fs))
            continue;
        LOG.debug("Loading storage ids for: " + getFullName() + ". nodes: " + hostIndex_.size() + ". filesystem: " + fsKey);
        DistributedFileSystem dfs = (DistributedFileSystem) fs;
        FileBlocksInfo blockLists = perFsFileBlocks.get(fsKey);
        Preconditions.checkNotNull(blockLists);
        long unknownDiskIdCount = 0;
        String[] storageIds = null;
        String[] hosts = null;
        for (int idx = 0; idx < blockLists.locations.size(); ++idx) {
            BlockLocation currentBlock = blockLists.locations.get(idx);
            storageIds = currentBlock.getStorageIds();
            if (storageIds == null) {
                LOG.error("Couldn't get storage IDs for block: " + currentBlock.toString());
                continue;
            }
            try {
                hosts = currentBlock.getHosts();
            } catch (IOException e) {
                LOG.error("Couldn't get hosts for block: " + currentBlock.toString(), e);
            }
            Preconditions.checkNotNull(hosts);
            THdfsFileBlock block = blockLists.blocks.get(idx);
            int[] diskIDs = new int[storageIds.length];
            for (int i = 0; i < storageIds.length; ++i) {
                diskIDs[i] = DiskIdMapper.getDiskId(hosts[i], storageIds[i]);
                if (diskIDs[i] < 0)
                    ++unknownDiskIdCount;
            }
            FileBlock.setDiskIds(diskIDs, block);
        }
        if (unknownDiskIdCount > 0) {
            LOG.warn("Unknown disk id count for filesystem " + fs + ":" + unknownDiskIdCount);
        }
    }
}
#method_after
private int loadDiskIds(BlockLocation location, THdfsFileBlock fileBlock) {
    int unknownDiskIdCount = 0;
    String[] storageIds = location.getStorageIds();
    String[] hosts;
    try {
        hosts = location.getHosts();
    } catch (IOException e) {
        LOG.error("Couldn't get hosts for block: " + location.toString(), e);
        return unknownDiskIdCount;
    }
    if (storageIds.length != hosts.length) {
        LOG.error("Number of storage IDs and number of hosts for block: " + location.toString() + " mismatch. Skipping disk ID loading for this block.");
        return unknownDiskIdCount;
    }
    int[] diskIDs = new int[storageIds.length];
    for (int i = 0; i < storageIds.length; ++i) {
        if (Strings.isNullOrEmpty(storageIds[i])) {
            diskIDs[i] = -1;
            ++unknownDiskIdCount;
        } else {
            diskIDs[i] = DiskIdMapper.INSTANCE.getDiskId(hosts[i], storageIds[i]);
        }
    }
    FileBlock.setDiskIds(diskIDs, fileBlock);
    return unknownDiskIdCount;
}
#end_block

#method_before
private void synthesizeBlockMetadata(FileSystem fs, FilteredPartitionPathsMd fileMd) {
    Preconditions.checkState(fileMd.pathFds.size() == fileMd.fileFormats.size());
    for (int i = 0; i < fileMd.pathFds.size(); ++i) {
        FileDescriptor fd = fileMd.pathFds.get(i);
        HdfsFileFormat fileFormat = fileMd.fileFormats.get(i);
        long start = 0;
        long remaining = fd.getFileLength();
        // Workaround HADOOP-11584 by using the filesystem default block size rather than
        // the block size from the FileStatus.
        // TODO: after HADOOP-11584 is resolved, get the block size from the FileStatus.
        long blockSize = fs.getDefaultBlockSize();
        if (blockSize < MIN_SYNTHETIC_BLOCK_SIZE)
            blockSize = MIN_SYNTHETIC_BLOCK_SIZE;
        if (!fileFormat.isSplittable(HdfsCompression.fromFileName(fd.getFileName()))) {
            blockSize = remaining;
        }
        while (remaining > 0) {
            long len = Math.min(remaining, blockSize);
            List<BlockReplica> replicas = Lists.newArrayList(new BlockReplica(hostIndex_.getIndex(REMOTE_NETWORK_ADDRESS), false));
            fd.addFileBlock(new FileBlock(start, len, replicas));
            remaining -= len;
            start += len;
        }
    }
}
#method_after
private void synthesizeBlockMetadata(FileSystem fs, Path dirPath, HashMap<Path, List<HdfsPartition>> partsByPath) throws IOException {
    RemoteIterator<LocatedFileStatus> fileStatusIter = fs.listFiles(dirPath, true);
    while (fileStatusIter.hasNext()) {
        LocatedFileStatus fileStatus = fileStatusIter.next();
        if (!FileSystemUtil.isValidDataFile(fileStatus))
            continue;
        Path partPathDir = fileStatus.getPath().getParent();
        Preconditions.checkNotNull(partPathDir);
        List<HdfsPartition> partitions = partsByPath.get(partPathDir);
        // Skip if this file does not belong to any known partition.
        if (partitions == null) {
            if (LOG.isTraceEnabled()) {
                LOG.trace("File " + fileStatus.getPath().toString() + " doesn't correspond " + " to a known partition. Skipping metadata load for this file.");
            }
            continue;
        }
        String fileName = fileStatus.getPath().getName();
        FileDescriptor fd = new FileDescriptor(fileName, fileStatus.getLen(), fileStatus.getModificationTime());
        Preconditions.checkState(partitions.size() > 0);
        // For the purpose of synthesizing block metadata, we assume that all partitions
        // with the same location have the same file format.
        HdfsFileFormat fileFormat = partitions.get(0).getFileFormat();
        synthesizeFdBlockMetadata(fs, fd, fileFormat);
        // Update the partitions' metadata that this file belongs to.
        for (HdfsPartition partition : partitions) {
            partition.getFileDescriptors().add(fd);
            numHdfsFiles_++;
            totalHdfsBytes_ += fd.getFileLength();
        }
    }
}
#end_block

#method_before
private void resetPartitions() {
    partitionIds_.clear();
    partitionMap_.clear();
    nameToPartitionMap_.clear();
    partitionValuesMap_.clear();
    nullPartitionIds_.clear();
    perPartitionFileDescMap_.clear();
    // Initialize partitionValuesMap_ and nullPartitionIds_. Also reset column stats.
    for (int i = 0; i < numClusteringCols_; ++i) {
        getColumns().get(i).getStats().setNumNulls(0);
        getColumns().get(i).getStats().setNumDistinctValues(0);
        partitionValuesMap_.add(Maps.<LiteralExpr, HashSet<Long>>newTreeMap());
        nullPartitionIds_.add(Sets.<Long>newHashSet());
    }
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
}
#method_after
private void resetPartitions() {
    partitionIds_.clear();
    partitionMap_.clear();
    nameToPartitionMap_.clear();
    partitionValuesMap_.clear();
    nullPartitionIds_.clear();
    // Initialize partitionValuesMap_ and nullPartitionIds_. Also reset column stats.
    for (int i = 0; i < numClusteringCols_; ++i) {
        getColumns().get(i).getStats().setNumNulls(0);
        getColumns().get(i).getStats().setNumDistinctValues(0);
        partitionValuesMap_.add(Maps.<LiteralExpr, HashSet<Long>>newTreeMap());
        nullPartitionIds_.add(Sets.<Long>newHashSet());
    }
    numHdfsFiles_ = 0;
    totalHdfsBytes_ = 0;
}
#end_block

#method_before
private void loadAllPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl) throws IOException, CatalogException {
    Preconditions.checkNotNull(msTbl);
    initializePartitionMetadata(msTbl);
    // Map of filesystem to the file blocks for new/modified FileDescriptors. Blocks in
    // this map will have their disk volume IDs information (re)loaded. This is used to
    // speed up the incremental refresh of a table's metadata by skipping unmodified,
    // previously loaded blocks.
    Map<FsKey, FileBlocksInfo> blocksToLoad = Maps.newHashMap();
    FilteredPartitionPathsMd pathMd = new FilteredPartitionPathsMd();
    Path tblLocation = getHdfsBaseDirPath();
    // List of directories that we scan for block locations. We optimize the block metadata
    // loading to reduce the number of RPCs to the NN by separately loading partitions
    // with default directory paths (under the base table directory) and non-default
    // directory paths. For the former we issue a single RPC to the NN to load all the
    // blocks from hdfsBaseDir_ and for the latter we load each of the partition directory
    // separately.
    // TODO: We can still do some advanced optimization by grouping all the partition
    // directories under the same ancestor path up the tree.
    List<Path> dirsToLoad = Lists.newArrayList(tblLocation);
    FileSystem fs = tblLocation.getFileSystem(CONF);
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null, pathMd);
        if (isMarkedCached_)
            part.markCached();
        addPartition(part);
        if (fs.exists(tblLocation)) {
            accessLevel_ = getAvailableAccessLevel(fs, tblLocation);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, pathMd);
            addPartition(partition);
            // to this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null) {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
            // Check if the partition directory's is a child dir of hdfsBaseDir_
            Path partDir = new Path(msPartition.getSd().getLocation());
            if (dirsToLoad.contains(partDir) || FileSystemUtil.isPathChild(partDir, getHdfsBaseDirPath()))
                continue;
            // Add the partition to the list of paths to load block MD.
            dirsToLoad.add(partDir);
        }
    }
    for (Path p : dirsToLoad) {
        LOG.debug("Accepted directory for loading block MD: " + p.toString());
    }
    // Load the block metadata
    loadMetadataAndDiskIds(fs, dirsToLoad, pathMd);
}
#method_after
private void loadAllPartitions(List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions, org.apache.hadoop.hive.metastore.api.Table msTbl) throws IOException, CatalogException {
    Preconditions.checkNotNull(msTbl);
    initializePartitionMetadata(msTbl);
    // Map of partition paths to their corresponding HdfsPartition objects. Populated
    // using createPartition() calls. A single partition path can correspond to multiple
    // partitions.
    HashMap<Path, List<HdfsPartition>> partsByPath = Maps.newHashMap();
    Path tblLocation = getHdfsBaseDirPath();
    // List of directories that we scan for block locations. We optimize the block metadata
    // loading to reduce the number of RPCs to the NN by separately loading partitions
    // with default directory paths (under the base table directory) and non-default
    // directory paths. For the former we issue a single RPC to the NN to load all the
    // blocks from hdfsBaseDir_ and for the latter we load each of the partition directory
    // separately.
    // TODO: We can still do some advanced optimization by grouping all the partition
    // directories under the same ancestor path up the tree.
    List<Path> dirsToLoad = Lists.newArrayList(tblLocation);
    FileSystem fs = tblLocation.getFileSystem(CONF);
    if (msTbl.getPartitionKeysSize() == 0) {
        Preconditions.checkArgument(msPartitions == null || msPartitions.isEmpty());
        // This table has no partition key, which means it has no declared partitions.
        // We model partitions slightly differently to Hive - every file must exist in a
        // partition, so add a single partition with no keys which will get all the
        // files in the table's root directory.
        HdfsPartition part = createPartition(msTbl.getSd(), null);
        partsByPath.put(tblLocation, Lists.newArrayList(part));
        if (isMarkedCached_)
            part.markCached();
        addPartition(part);
        if (fs.exists(tblLocation)) {
            accessLevel_ = getAvailableAccessLevel(fs, tblLocation);
        }
    } else {
        for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
            HdfsPartition partition = createPartition(msPartition.getSd(), msPartition);
            addPartition(partition);
            // to this table's partition list. Skip the partition.
            if (partition == null)
                continue;
            if (msPartition.getParameters() != null) {
                partition.setNumRows(getRowCount(msPartition.getParameters()));
            }
            if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
                // TODO: READ_ONLY isn't exactly correct because the it's possible the
                // partition does not have READ permissions either. When we start checking
                // whether we can READ from a table, this should be updated to set the
                // table's access level to the "lowest" effective level across all
                // partitions. That is, if one partition has READ_ONLY and another has
                // WRITE_ONLY the table's access level should be NONE.
                accessLevel_ = TAccessLevel.READ_ONLY;
            }
            Path partDir = new Path(msPartition.getSd().getLocation());
            List<HdfsPartition> parts = partsByPath.get(partDir);
            if (parts == null) {
                partsByPath.put(partDir, Lists.newArrayList(partition));
            } else {
                parts.add(partition);
            }
            if (!dirsToLoad.contains(partDir) && !FileSystemUtil.isDescendantPath(partDir, tblLocation)) {
                // This partition has a custom filesystem location. Load its file/block
                // metadata separately by adding it to the list of dirs to load.
                dirsToLoad.add(partDir);
            }
        }
    }
    if (LOG.isTraceEnabled())
        LOG.trace("partsByPath size: " + partsByPath.size());
    loadMetadataAndDiskIds(dirsToLoad, partsByPath);
}
#end_block

#method_before
private void loadMetadataAndDiskIds(FileSystem fs, List<Path> locations, FilteredPartitionPathsMd pathMd) {
    Map<FsKey, FileBlocksInfo> fileBlocksToLoad = Maps.newHashMap();
    for (Path partDirPath : locations) {
        loadBlockMetadata(fs, partDirPath, pathMd, fileBlocksToLoad);
    }
    loadDiskIds(fileBlocksToLoad);
}
#method_after
private void loadMetadataAndDiskIds(HdfsPartition partition) throws CatalogException {
    Path partDirPath = partition.getLocationPath();
    HashMap<Path, List<HdfsPartition>> partsByPath = Maps.newHashMap();
    partsByPath.put(partDirPath, Lists.newArrayList(partition));
    loadMetadataAndDiskIds(Lists.newArrayList(partDirPath), partsByPath);
}
#end_block

#method_before
public HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws CatalogException {
    FilteredPartitionPathsMd pathMd = new FilteredPartitionPathsMd();
    HdfsPartition hdfsPartition = createPartition(storageDescriptor, msPartition, pathMd);
    try {
        // Load file block metadata and the corresponding disk IDs.
        Path partDirPath = new Path(hdfsPartition.getLocation());
        FileSystem fs = partDirPath.getFileSystem(CONF);
        loadMetadataAndDiskIds(fs, partDirPath, pathMd);
    } catch (IOException e) {
        throw new CatalogException("Error loading block md for partition path: " + hdfsPartition.getLocation(), e);
    }
    return hdfsPartition;
}
#method_after
private HdfsPartition createPartition(StorageDescriptor storageDescriptor, org.apache.hadoop.hive.metastore.api.Partition msPartition) throws CatalogException {
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    List<LiteralExpr> keyValues = Lists.newArrayList();
    if (msPartition != null) {
        // Load key values
        for (String partitionKey : msPartition.getValues()) {
            Type type = getColumns().get(keyValues.size()).getType();
            // Deal with Hive's special NULL partition key.
            if (partitionKey.equals(nullPartitionKeyValue_)) {
                keyValues.add(NullLiteral.create(type));
            } else {
                try {
                    keyValues.add(LiteralExpr.create(partitionKey, type));
                } catch (Exception ex) {
                    LOG.warn("Failed to create literal expression of type: " + type, ex);
                    throw new CatalogException("Invalid partition key value of type: " + type, ex);
                }
            }
        }
        for (Expr v : keyValues) v.analyzeNoThrow(null);
    }
    Path partDirPath = new Path(storageDescriptor.getLocation());
    try {
        FileSystem fs = partDirPath.getFileSystem(CONF);
        multipleFileSystems_ = multipleFileSystems_ || !FileSystemUtil.isPathOnFileSystem(new Path(getLocation()), fs);
        if (msPartition != null) {
            HdfsCachingUtil.validateCacheParams(msPartition.getParameters());
        }
        HdfsPartition partition = new HdfsPartition(this, msPartition, keyValues, fileFormatDescriptor, new ArrayList<FileDescriptor>(), getAvailableAccessLevel(fs, partDirPath));
        partition.checkWellFormed();
        return partition;
    } catch (IOException e) {
        throw new CatalogException("Error initializing partition", e);
    }
}
#end_block

#method_before
private HdfsPartition dropPartition(HdfsPartition partition) {
    if (partition == null)
        return null;
    totalHdfsBytes_ -= partition.getSize();
    numHdfsFiles_ -= partition.getNumFileDescriptors();
    Preconditions.checkArgument(partition.getPartitionValues().size() == numClusteringCols_);
    Long partitionId = partition.getId();
    // Remove the partition id from the list of partition ids and other mappings.
    partitionIds_.remove(partitionId);
    partitionMap_.remove(partitionId);
    nameToPartitionMap_.remove(partition.getPartitionName());
    perPartitionFileDescMap_.remove(partition.getLocation());
    for (int i = 0; i < partition.getPartitionValues().size(); ++i) {
        ColumnStats stats = getColumns().get(i).getStats();
        LiteralExpr literal = partition.getPartitionValues().get(i);
        // Check if this is a null literal.
        if (literal instanceof NullLiteral) {
            nullPartitionIds_.get(i).remove(partitionId);
            stats.setNumNulls(stats.getNumNulls() - 1);
            if (nullPartitionIds_.get(i).isEmpty()) {
                stats.setNumDistinctValues(stats.getNumDistinctValues() - 1);
            }
            continue;
        }
        HashSet<Long> partitionIds = partitionValuesMap_.get(i).get(literal);
        // only this id. Otherwise, remove the <literal, id> pair.
        if (partitionIds.size() > 1)
            partitionIds.remove(partitionId);
        else {
            partitionValuesMap_.get(i).remove(literal);
            stats.setNumDistinctValues(stats.getNumDistinctValues() - 1);
        }
    }
    return partition;
}
#method_after
private HdfsPartition dropPartition(HdfsPartition partition) {
    if (partition == null)
        return null;
    totalHdfsBytes_ -= partition.getSize();
    numHdfsFiles_ -= partition.getNumFileDescriptors();
    Preconditions.checkArgument(partition.getPartitionValues().size() == numClusteringCols_);
    Long partitionId = partition.getId();
    // Remove the partition id from the list of partition ids and other mappings.
    partitionIds_.remove(partitionId);
    partitionMap_.remove(partitionId);
    nameToPartitionMap_.remove(partition.getPartitionName());
    for (int i = 0; i < partition.getPartitionValues().size(); ++i) {
        ColumnStats stats = getColumns().get(i).getStats();
        LiteralExpr literal = partition.getPartitionValues().get(i);
        // Check if this is a null literal.
        if (literal instanceof NullLiteral) {
            nullPartitionIds_.get(i).remove(partitionId);
            stats.setNumNulls(stats.getNumNulls() - 1);
            if (nullPartitionIds_.get(i).isEmpty()) {
                stats.setNumDistinctValues(stats.getNumDistinctValues() - 1);
            }
            continue;
        }
        HashSet<Long> partitionIds = partitionValuesMap_.get(i).get(literal);
        // only this id. Otherwise, remove the <literal, id> pair.
        if (partitionIds.size() > 1)
            partitionIds.remove(partitionId);
        else {
            partitionValuesMap_.get(i).remove(literal);
            stats.setNumDistinctValues(stats.getNumDistinctValues() - 1);
        }
    }
    return partition;
}
#end_block

#method_before
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl, boolean loadFileMetadata, boolean loadTableSchema, Set<String> partitionsToUpdate) throws TableLoadingException {
    // turn all exceptions into TableLoadingException
    msTable_ = msTbl;
    try {
        if (loadTableSchema)
            loadSchema(client, msTbl);
        if (reuseMetadata && getCatalogVersion() == Catalog.INITIAL_CATALOG_VERSION) {
            // This is the special case of CTAS that creates a 'temp' table that does not
            // actually exist in the Hive Metastore.
            initializePartitionMetadata(msTbl);
            updateStatsFromHmsTable(msTbl);
            return;
        }
        // Load partition and file metadata
        if (reuseMetadata) {
            // Incrementally update this table's partitions and file metadata
            LOG.debug("incremental update for table: " + db_.getName() + "." + name_);
            Preconditions.checkState(partitionsToUpdate == null || loadFileMetadata);
            updateMdFromHmsTable(msTbl);
            if (msTbl.getPartitionKeysSize() == 0) {
                if (loadFileMetadata)
                    updateUnpartitionedTableFileMd();
            } else {
                updatePartitionsFromHms(client, partitionsToUpdate, loadFileMetadata);
            }
        } else {
            // Load all partitions from Hive Metastore, including file metadata.
            LOG.debug("load table from Hive Metastore: " + db_.getName() + "." + name_);
            List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
            loadAllPartitions(msPartitions, msTbl);
        }
        if (loadTableSchema)
            setAvroSchema(client, msTbl);
        updateStatsFromHmsTable(msTbl);
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#method_after
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl, boolean loadFileMetadata, boolean loadTableSchema, Set<String> partitionsToUpdate) throws TableLoadingException {
    // turn all exceptions into TableLoadingException
    msTable_ = msTbl;
    try {
        if (loadTableSchema)
            loadSchema(client, msTbl);
        if (reuseMetadata && getCatalogVersion() == Catalog.INITIAL_CATALOG_VERSION) {
            // This is the special case of CTAS that creates a 'temp' table that does not
            // actually exist in the Hive Metastore.
            initializePartitionMetadata(msTbl);
            updateStatsFromHmsTable(msTbl);
            return;
        }
        // Load partition and file metadata
        if (reuseMetadata) {
            // Incrementally update this table's partitions and file metadata
            if (LOG.isTraceEnabled()) {
                LOG.trace("incremental update for table: " + db_.getName() + "." + name_);
            }
            Preconditions.checkState(partitionsToUpdate == null || loadFileMetadata);
            updateMdFromHmsTable(msTbl);
            if (msTbl.getPartitionKeysSize() == 0) {
                if (loadFileMetadata)
                    updateUnpartitionedTableFileMd();
            } else {
                updatePartitionsFromHms(client, partitionsToUpdate, loadFileMetadata);
            }
        } else {
            // Load all partitions from Hive Metastore, including file metadata.
            if (LOG.isTraceEnabled()) {
                LOG.trace("load table from Hive Metastore: " + db_.getName() + "." + name_);
            }
            List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
            loadAllPartitions(msPartitions, msTbl);
        }
        if (loadTableSchema)
            setAvroSchema(client, msTbl);
        updateStatsFromHmsTable(msTbl);
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#end_block

#method_before
private void updateUnpartitionedTableFileMd() throws Exception {
    LOG.debug("update unpartitioned table: " + name_);
    resetPartitions();
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    addDefaultPartition(msTbl.getSd());
    FilteredPartitionPathsMd pathMd = new FilteredPartitionPathsMd();
    HdfsPartition part = createPartition(msTbl.getSd(), null, pathMd);
    addPartition(part);
    // Load file block metadata and the corresponding disk IDs.
    Path tblDirPath = getHdfsBaseDirPath();
    FileSystem fs = tblDirPath.getFileSystem(CONF);
    loadMetadataAndDiskIds(fs, tblDirPath, pathMd);
    if (isMarkedCached_)
        part.markCached();
}
#method_after
private void updateUnpartitionedTableFileMd() throws CatalogException {
    if (LOG.isTraceEnabled()) {
        LOG.trace("update unpartitioned table: " + name_);
    }
    resetPartitions();
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    addDefaultPartition(msTbl.getSd());
    HdfsPartition part = createPartition(msTbl.getSd(), null);
    addPartition(part);
    loadMetadataAndDiskIds(part);
    if (isMarkedCached_)
        part.markCached();
}
#end_block

#method_before
private void updatePartitionsFromHms(IMetaStoreClient client, Set<String> partitionsToUpdate, boolean loadFileMetadata) throws Exception {
    LOG.debug("sync table partitions: " + name_);
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    Preconditions.checkState(msTbl.getPartitionKeysSize() != 0);
    Preconditions.checkState(loadFileMetadata || partitionsToUpdate == null);
    // Retrieve all the partition names from the Hive Metastore. We need this to
    // identify the delta between partitions of the local HdfsTable and the table entry
    // in the Hive Metastore. Note: This is a relatively "cheap" operation
    // (~.3 secs for 30K partitions).
    Set<String> msPartitionNames = Sets.newHashSet();
    msPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
    // Names of loaded partitions in this table
    Set<String> partitionNames = Sets.newHashSet();
    // Partitions for which file metadata must be loaded
    List<HdfsPartition> partitionsToUpdateFileMd = Lists.newArrayList();
    // Partitions that need to be dropped and recreated from scratch
    List<HdfsPartition> dirtyPartitions = Lists.newArrayList();
    // Partitions that need to be removed from this table. That includes dirty
    // partitions as well as partitions that were removed from the Hive Metastore.
    List<HdfsPartition> partitionsToRemove = Lists.newArrayList();
    // partitions that no longer exist in the Hive Metastore.
    for (HdfsPartition partition : partitionMap_.values()) {
        // Ignore the default partition
        if (partition.isDefaultPartition())
            continue;
        // that were removed from HMS using some external process, e.g. Hive.
        if (!msPartitionNames.contains(partition.getPartitionName())) {
            partitionsToRemove.add(partition);
        }
        if (partition.isDirty()) {
            // Dirty partitions are updated by removing them from table's partition
            // list and loading them from the Hive Metastore.
            dirtyPartitions.add(partition);
        } else {
            if (partitionsToUpdate == null && loadFileMetadata) {
                partitionsToUpdateFileMd.add(partition);
            }
        }
        Preconditions.checkNotNull(partition.getCachedMsPartitionDescriptor());
        partitionNames.add(partition.getPartitionName());
    }
    partitionsToRemove.addAll(dirtyPartitions);
    dropPartitions(partitionsToRemove);
    // Load dirty partitions from Hive Metastore
    loadPartitionsFromMetastore(dirtyPartitions, client);
    // Identify and load partitions that were added in the Hive Metastore but don't
    // exist in this table.
    Set<String> newPartitionsInHms = Sets.difference(msPartitionNames, partitionNames);
    loadPartitionsFromMetastore(newPartitionsInHms, client);
    // reloaded by loadPartitionsFromMetastore().
    if (partitionsToUpdate != null) {
        partitionsToUpdate.removeAll(newPartitionsInHms);
    }
    // descriptors and block metadata of a table (e.g. REFRESH statement).
    if (loadFileMetadata) {
        if (partitionsToUpdate != null) {
            // Only reload file metadata of partitions specified in 'partitionsToUpdate'
            Preconditions.checkState(partitionsToUpdateFileMd.isEmpty());
            partitionsToUpdateFileMd = getPartitionsByName(partitionsToUpdate);
        }
        loadPartitionFileMetadata(partitionsToUpdateFileMd);
    }
}
#method_after
private void updatePartitionsFromHms(IMetaStoreClient client, Set<String> partitionsToUpdate, boolean loadFileMetadata) throws Exception {
    if (LOG.isTraceEnabled()) {
        LOG.trace("sync table partitions: " + name_);
    }
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    Preconditions.checkState(msTbl.getPartitionKeysSize() != 0);
    Preconditions.checkState(loadFileMetadata || partitionsToUpdate == null);
    // Retrieve all the partition names from the Hive Metastore. We need this to
    // identify the delta between partitions of the local HdfsTable and the table entry
    // in the Hive Metastore. Note: This is a relatively "cheap" operation
    // (~.3 secs for 30K partitions).
    Set<String> msPartitionNames = Sets.newHashSet();
    msPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
    // Names of loaded partitions in this table
    Set<String> partitionNames = Sets.newHashSet();
    // Partitions for which file metadata must be loaded
    List<HdfsPartition> partitionsToUpdateFileMd = Lists.newArrayList();
    // Partitions that need to be dropped and recreated from scratch
    List<HdfsPartition> dirtyPartitions = Lists.newArrayList();
    // Partitions that need to be removed from this table. That includes dirty
    // partitions as well as partitions that were removed from the Hive Metastore.
    List<HdfsPartition> partitionsToRemove = Lists.newArrayList();
    // partitions that no longer exist in the Hive Metastore.
    for (HdfsPartition partition : partitionMap_.values()) {
        // Ignore the default partition
        if (partition.isDefaultPartition())
            continue;
        // that were removed from HMS using some external process, e.g. Hive.
        if (!msPartitionNames.contains(partition.getPartitionName())) {
            partitionsToRemove.add(partition);
        }
        if (partition.isDirty()) {
            // Dirty partitions are updated by removing them from table's partition
            // list and loading them from the Hive Metastore.
            dirtyPartitions.add(partition);
        } else {
            if (partitionsToUpdate == null && loadFileMetadata) {
                partitionsToUpdateFileMd.add(partition);
            }
        }
        Preconditions.checkNotNull(partition.getCachedMsPartitionDescriptor());
        partitionNames.add(partition.getPartitionName());
    }
    partitionsToRemove.addAll(dirtyPartitions);
    dropPartitions(partitionsToRemove);
    // Load dirty partitions from Hive Metastore
    loadPartitionsFromMetastore(dirtyPartitions, client);
    // Identify and load partitions that were added in the Hive Metastore but don't
    // exist in this table.
    Set<String> newPartitionsInHms = Sets.difference(msPartitionNames, partitionNames);
    loadPartitionsFromMetastore(newPartitionsInHms, client);
    // reloaded by loadPartitionsFromMetastore().
    if (partitionsToUpdate != null) {
        partitionsToUpdate.removeAll(newPartitionsInHms);
    }
    // descriptors and block metadata of a table (e.g. REFRESH statement).
    if (loadFileMetadata) {
        if (partitionsToUpdate != null) {
            // Only reload file metadata of partitions specified in 'partitionsToUpdate'
            Preconditions.checkState(partitionsToUpdateFileMd.isEmpty());
            partitionsToUpdateFileMd = getPartitionsByName(partitionsToUpdate);
        }
        loadPartitionFileMetadata(partitionsToUpdateFileMd);
    }
}
#end_block

#method_before
private void loadPartitionsFromMetastore(List<HdfsPartition> partitions, IMetaStoreClient client) throws Exception {
    Preconditions.checkNotNull(partitions);
    if (partitions.isEmpty())
        return;
    LOG.info(String.format("Incrementally updating %d/%d partitions.", partitions.size(), partitionMap_.size()));
    Set<String> partitionNames = Sets.newHashSet();
    for (HdfsPartition part : partitions) {
        partitionNames.add(part.getPartitionName());
    }
    loadPartitionsFromMetastore(partitionNames, client);
}
#method_after
private void loadPartitionsFromMetastore(List<HdfsPartition> partitions, IMetaStoreClient client) throws Exception {
    Preconditions.checkNotNull(partitions);
    if (partitions.isEmpty())
        return;
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("Incrementally updating %d/%d partitions.", partitions.size(), partitionMap_.size()));
    }
    Set<String> partitionNames = Sets.newHashSet();
    for (HdfsPartition part : partitions) {
        partitionNames.add(part.getPartitionName());
    }
    loadPartitionsFromMetastore(partitionNames, client);
}
#end_block

#method_before
private void loadPartitionsFromMetastore(Set<String> partitionNames, IMetaStoreClient client) throws Exception {
    Preconditions.checkNotNull(partitionNames);
    if (partitionNames.isEmpty())
        return;
    // Load partition metadata from Hive Metastore.
    List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
    msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(partitionNames), db_.getName(), name_));
    for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
        FilteredPartitionPathsMd pathMd = new FilteredPartitionPathsMd();
        HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, pathMd);
        addPartition(partition);
        // this table's partition list. Skip the partition.
        if (partition == null)
            continue;
        if (partition.getFileFormat() == HdfsFileFormat.AVRO)
            hasAvroData_ = true;
        if (msPartition.getParameters() != null) {
            partition.setNumRows(getRowCount(msPartition.getParameters()));
        }
        if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
            // TODO: READ_ONLY isn't exactly correct because the it's possible the
            // partition does not have READ permissions either. When we start checking
            // whether we can READ from a table, this should be updated to set the
            // table's access level to the "lowest" effective level across all
            // partitions. That is, if one partition has READ_ONLY and another has
            // WRITE_ONLY the table's access level should be NONE.
            accessLevel_ = TAccessLevel.READ_ONLY;
        }
        // Load file block metadata and the corresponding disk IDs.
        Path partDirPath = new Path(msPartition.getSd().getLocation());
        FileSystem fs = partDirPath.getFileSystem(CONF);
        loadMetadataAndDiskIds(fs, partDirPath, pathMd);
    }
}
#method_after
private void loadPartitionsFromMetastore(Set<String> partitionNames, IMetaStoreClient client) throws Exception {
    Preconditions.checkNotNull(partitionNames);
    if (partitionNames.isEmpty())
        return;
    // Load partition metadata from Hive Metastore.
    List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
    msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(partitionNames), db_.getName(), name_));
    for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
        HdfsPartition partition = createPartition(msPartition.getSd(), msPartition);
        addPartition(partition);
        // this table's partition list. Skip the partition.
        if (partition == null)
            continue;
        if (partition.getFileFormat() == HdfsFileFormat.AVRO)
            hasAvroData_ = true;
        if (msPartition.getParameters() != null) {
            partition.setNumRows(getRowCount(msPartition.getParameters()));
        }
        if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
            // TODO: READ_ONLY isn't exactly correct because the it's possible the
            // partition does not have READ permissions either. When we start checking
            // whether we can READ from a table, this should be updated to set the
            // table's access level to the "lowest" effective level across all
            // partitions. That is, if one partition has READ_ONLY and another has
            // WRITE_ONLY the table's access level should be NONE.
            accessLevel_ = TAccessLevel.READ_ONLY;
        }
        loadMetadataAndDiskIds(partition);
    }
}
#end_block

#method_before
private void loadPartitionFileMetadata(List<HdfsPartition> partitions) throws Exception {
    Preconditions.checkNotNull(partitions);
    LOG.info(String.format("loading file metadata for %d partitions", partitions.size()));
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, msTbl.getSd());
    for (HdfsPartition partition : partitions) {
        FilteredPartitionPathsMd pathMd = new FilteredPartitionPathsMd();
        org.apache.hadoop.hive.metastore.api.Partition msPart = partition.toHmsPartition();
        StorageDescriptor sd = null;
        if (msPart == null) {
            // If this partition is not stored in the Hive Metastore (e.g. default partition
            // of an unpartitioned table), use the table's storage descriptor to load file
            // metadata.
            sd = msTbl.getSd();
        } else {
            sd = msPart.getSd();
        }
        loadPartitionFileMetadata(sd, partition, fileFormatDescriptor.getFileFormat(), pathMd);
        // Load the partition block metadata for each partition dir.
        Path partDirPath = new Path(partition.getLocation());
        FileSystem fs = partDirPath.getFileSystem(CONF);
        loadMetadataAndDiskIds(fs, partDirPath, pathMd);
    }
}
#method_after
private void loadPartitionFileMetadata(List<HdfsPartition> partitions) throws Exception {
    Preconditions.checkNotNull(partitions);
    if (LOG.isTraceEnabled()) {
        LOG.trace(String.format("loading file metadata for %d partitions", partitions.size()));
    }
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    HdfsStorageDescriptor fileFormatDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, msTbl.getSd());
    for (HdfsPartition partition : partitions) {
        org.apache.hadoop.hive.metastore.api.Partition msPart = partition.toHmsPartition();
        if (msPart != null) {
            HdfsCachingUtil.validateCacheParams(msPart.getParameters());
        }
        StorageDescriptor sd = null;
        if (msPart == null) {
            // If this partition is not stored in the Hive Metastore (e.g. default partition
            // of an unpartitioned table), use the table's storage descriptor to load file
            // metadata.
            sd = msTbl.getSd();
        } else {
            sd = msPart.getSd();
        }
        loadPartitionFileMetadata(sd, partition);
    }
}
#end_block

#method_before
private void loadPartitionFileMetadata(StorageDescriptor storageDescriptor, HdfsPartition partition, HdfsFileFormat fileFormat, FilteredPartitionPathsMd pathMd) throws Exception {
    Preconditions.checkNotNull(storageDescriptor);
    Preconditions.checkNotNull(partition);
    org.apache.hadoop.hive.metastore.api.Partition msPart = partition.toHmsPartition();
    boolean isMarkedCached = isMarkedCached_;
    if (msPart != null) {
        isMarkedCached = HdfsCachingUtil.validateCacheParams(msPart.getParameters());
    }
    Path partDirPath = new Path(storageDescriptor.getLocation());
    FileSystem fs = partDirPath.getFileSystem(CONF);
    if (!fs.exists(partDirPath))
        return;
    String partitionDir = partDirPath.toString();
    numHdfsFiles_ -= partition.getNumFileDescriptors();
    totalHdfsBytes_ -= partition.getSize();
    Preconditions.checkState(numHdfsFiles_ >= 0 && totalHdfsBytes_ >= 0);
    updatePartitionFds(partDirPath, isMarkedCached, fileFormat, pathMd);
    List<FileDescriptor> fileDescs = Lists.newArrayList(perPartitionFileDescMap_.get(partDirPath.toString()).values());
    partition.setFileDescriptors(fileDescs);
    totalHdfsBytes_ += partition.getSize();
    numHdfsFiles_ += fileDescs.size();
}
#method_after
private void loadPartitionFileMetadata(StorageDescriptor storageDescriptor, HdfsPartition partition) throws Exception {
    Preconditions.checkNotNull(storageDescriptor);
    Preconditions.checkNotNull(partition);
    org.apache.hadoop.hive.metastore.api.Partition msPart = partition.toHmsPartition();
    Path partDirPath = new Path(storageDescriptor.getLocation());
    FileSystem fs = partDirPath.getFileSystem(CONF);
    if (!fs.exists(partDirPath))
        return;
    numHdfsFiles_ -= partition.getNumFileDescriptors();
    totalHdfsBytes_ -= partition.getSize();
    Preconditions.checkState(numHdfsFiles_ >= 0 && totalHdfsBytes_ >= 0);
    loadMetadataAndDiskIds(partition);
}
#end_block

#method_before
private void getAllPartitionsNotInHms(Path path, List<String> partitionKeys, int depth, FileSystem fs, List<String> partitionValues, List<LiteralExpr> partitionExprs, List<List<LiteralExpr>> existingPartitions, List<List<String>> partitionsNotInHms) throws IOException {
    if (depth == partitionKeys.size()) {
        if (existingPartitions.contains(partitionExprs)) {
            LOG.trace(String.format("Skip recovery of path '%s' because it already exists " + "in metastore", path.toString()));
        } else {
            partitionsNotInHms.add(partitionValues);
            existingPartitions.add(partitionExprs);
        }
        return;
    }
    FileStatus[] statuses = fs.listStatus(path);
    for (FileStatus status : statuses) {
        if (!status.isDirectory())
            continue;
        Pair<String, LiteralExpr> keyValues = getTypeCompatibleValue(status.getPath(), partitionKeys.get(depth));
        if (keyValues == null)
            continue;
        List<String> currentPartitionValues = Lists.newArrayList(partitionValues);
        List<LiteralExpr> currentPartitionExprs = Lists.newArrayList(partitionExprs);
        currentPartitionValues.add(keyValues.first);
        currentPartitionExprs.add(keyValues.second);
        getAllPartitionsNotInHms(status.getPath(), partitionKeys, depth + 1, fs, currentPartitionValues, currentPartitionExprs, existingPartitions, partitionsNotInHms);
    }
}
#method_after
private void getAllPartitionsNotInHms(Path path, List<String> partitionKeys, int depth, FileSystem fs, List<String> partitionValues, List<LiteralExpr> partitionExprs, List<List<LiteralExpr>> existingPartitions, List<List<String>> partitionsNotInHms) throws IOException {
    if (depth == partitionKeys.size()) {
        if (existingPartitions.contains(partitionExprs)) {
            if (LOG.isTraceEnabled()) {
                LOG.trace(String.format("Skip recovery of path '%s' because it already " + "exists in metastore", path.toString()));
            }
        } else {
            partitionsNotInHms.add(partitionValues);
            existingPartitions.add(partitionExprs);
        }
        return;
    }
    FileStatus[] statuses = fs.listStatus(path);
    for (FileStatus status : statuses) {
        if (!status.isDirectory())
            continue;
        Pair<String, LiteralExpr> keyValues = getTypeCompatibleValue(status.getPath(), partitionKeys.get(depth));
        if (keyValues == null)
            continue;
        List<String> currentPartitionValues = Lists.newArrayList(partitionValues);
        List<LiteralExpr> currentPartitionExprs = Lists.newArrayList(partitionExprs);
        currentPartitionValues.add(keyValues.first);
        currentPartitionExprs.add(keyValues.second);
        getAllPartitionsNotInHms(status.getPath(), partitionKeys, depth + 1, fs, currentPartitionValues, currentPartitionExprs, existingPartitions, partitionsNotInHms);
    }
}
#end_block

#method_before
private Pair<String, LiteralExpr> getTypeCompatibleValue(Path path, String partitionKey) {
    String[] partName = path.getName().split("=");
    if (partName.length != 2 || !partName[0].equals(partitionKey))
        return null;
    // Check Type compatibility for Partition value.
    Column column = getColumn(partName[0]);
    Preconditions.checkNotNull(column);
    Type type = column.getType();
    LiteralExpr expr = null;
    if (!partName[1].equals(getNullPartitionKeyValue())) {
        try {
            expr = LiteralExpr.create(partName[1], type);
            // Skip large value which exceeds the MAX VALUE of specified Type.
            if (expr instanceof NumericLiteral) {
                if (NumericLiteral.isOverflow(((NumericLiteral) expr).getValue(), type)) {
                    LOG.warn(String.format("Skip the overflow value (%s) for Type (%s).", partName[1], type.toSql()));
                    return null;
                }
            }
        } catch (Exception ex) {
            LOG.debug(String.format("Invalid partition value (%s) for Type (%s).", partName[1], type.toSql()));
            return null;
        }
    } else {
        expr = new NullLiteral();
    }
    return new Pair<String, LiteralExpr>(partName[1], expr);
}
#method_after
private Pair<String, LiteralExpr> getTypeCompatibleValue(Path path, String partitionKey) {
    String[] partName = path.getName().split("=");
    if (partName.length != 2 || !partName[0].equals(partitionKey))
        return null;
    // Check Type compatibility for Partition value.
    Column column = getColumn(partName[0]);
    Preconditions.checkNotNull(column);
    Type type = column.getType();
    LiteralExpr expr = null;
    if (!partName[1].equals(getNullPartitionKeyValue())) {
        try {
            expr = LiteralExpr.create(partName[1], type);
            // Skip large value which exceeds the MAX VALUE of specified Type.
            if (expr instanceof NumericLiteral) {
                if (NumericLiteral.isOverflow(((NumericLiteral) expr).getValue(), type)) {
                    LOG.warn(String.format("Skip the overflow value (%s) for Type (%s).", partName[1], type.toSql()));
                    return null;
                }
            }
        } catch (Exception ex) {
            if (LOG.isTraceEnabled()) {
                LOG.trace(String.format("Invalid partition value (%s) for Type (%s).", partName[1], type.toSql()));
            }
            return null;
        }
    } else {
        expr = new NullLiteral();
    }
    return new Pair<String, LiteralExpr>(partName[1], expr);
}
#end_block

#method_before
public void reloadPartition(HdfsPartition oldPartition, Partition hmsPartition) throws CatalogException {
    HdfsPartition refreshedPartition = createPartition(hmsPartition.getSd(), hmsPartition);
    Preconditions.checkArgument(oldPartition == null || oldPartition.compareTo(refreshedPartition) == 0);
    dropPartition(oldPartition);
    addPartition(refreshedPartition);
}
#method_after
public void reloadPartition(HdfsPartition oldPartition, Partition hmsPartition) throws CatalogException {
    HdfsPartition refreshedPartition = createAndLoadPartition(hmsPartition.getSd(), hmsPartition);
    Preconditions.checkArgument(oldPartition == null || oldPartition.compareTo(refreshedPartition) == 0);
    dropPartition(oldPartition);
    addPartition(refreshedPartition);
}
#end_block

#method_before
public static int deleteAllVisibleFiles(Path directory) throws IOException {
    FileSystem fs = directory.getFileSystem(CONF);
    Preconditions.checkState(fs.getFileStatus(directory).isDirectory());
    int numFilesDeleted = 0;
    for (FileStatus fStatus : fs.listStatus(directory)) {
        // Only delete files that are not hidden.
        if (fStatus.isFile() && !isHiddenFile(fStatus.getPath().getName())) {
            LOG.debug("Removing: " + fStatus.getPath());
            fs.delete(fStatus.getPath(), false);
            ++numFilesDeleted;
        }
    }
    return numFilesDeleted;
}
#method_after
public static int deleteAllVisibleFiles(Path directory) throws IOException {
    FileSystem fs = directory.getFileSystem(CONF);
    Preconditions.checkState(fs.getFileStatus(directory).isDirectory());
    int numFilesDeleted = 0;
    for (FileStatus fStatus : fs.listStatus(directory)) {
        // Only delete files that are not hidden.
        if (fStatus.isFile() && !isHiddenFile(fStatus.getPath().getName())) {
            if (LOG.isTraceEnabled())
                LOG.trace("Removing: " + fStatus.getPath());
            fs.delete(fStatus.getPath(), false);
            ++numFilesDeleted;
        }
    }
    return numFilesDeleted;
}
#end_block

#method_before
public static int relocateAllVisibleFiles(Path sourceDir, Path destDir) throws IOException {
    FileSystem destFs = destDir.getFileSystem(CONF);
    FileSystem sourceFs = sourceDir.getFileSystem(CONF);
    Preconditions.checkState(destFs.isDirectory(destDir));
    Preconditions.checkState(sourceFs.isDirectory(sourceDir));
    // Use the same UUID to resolve all file name conflicts. This helps mitigate problems
    // that might happen if there is a conflict moving a set of files that have
    // dependent file names. For example, foo.lzo and foo.lzo_index.
    UUID uuid = UUID.randomUUID();
    // Enumerate all the files in the source
    int numFilesMoved = 0;
    for (FileStatus fStatus : sourceFs.listStatus(sourceDir)) {
        if (fStatus.isDirectory()) {
            LOG.debug("Skipping copy of directory: " + fStatus.getPath());
            continue;
        } else if (isHiddenFile(fStatus.getPath().getName())) {
            continue;
        }
        Path destFile = new Path(destDir, fStatus.getPath().getName());
        if (destFs.exists(destFile)) {
            destFile = new Path(destDir, appendToBaseFileName(destFile.getName(), uuid.toString()));
        }
        FileSystemUtil.relocateFile(fStatus.getPath(), destFile, false);
        ++numFilesMoved;
    }
    return numFilesMoved;
}
#method_after
public static int relocateAllVisibleFiles(Path sourceDir, Path destDir) throws IOException {
    FileSystem destFs = destDir.getFileSystem(CONF);
    FileSystem sourceFs = sourceDir.getFileSystem(CONF);
    Preconditions.checkState(destFs.isDirectory(destDir));
    Preconditions.checkState(sourceFs.isDirectory(sourceDir));
    // Use the same UUID to resolve all file name conflicts. This helps mitigate problems
    // that might happen if there is a conflict moving a set of files that have
    // dependent file names. For example, foo.lzo and foo.lzo_index.
    UUID uuid = UUID.randomUUID();
    // Enumerate all the files in the source
    int numFilesMoved = 0;
    for (FileStatus fStatus : sourceFs.listStatus(sourceDir)) {
        if (fStatus.isDirectory()) {
            if (LOG.isTraceEnabled()) {
                LOG.trace("Skipping copy of directory: " + fStatus.getPath());
            }
            continue;
        } else if (isHiddenFile(fStatus.getPath().getName())) {
            continue;
        }
        Path destFile = new Path(destDir, fStatus.getPath().getName());
        if (destFs.exists(destFile)) {
            destFile = new Path(destDir, appendToBaseFileName(destFile.getName(), uuid.toString()));
        }
        FileSystemUtil.relocateFile(fStatus.getPath(), destFile, false);
        ++numFilesMoved;
    }
    return numFilesMoved;
}
#end_block

#method_before
public static void relocateFile(Path sourceFile, Path dest, boolean renameIfAlreadyExists) throws IOException {
    FileSystem destFs = dest.getFileSystem(CONF);
    FileSystem sourceFs = sourceFile.getFileSystem(CONF);
    Path destFile = destFs.isDirectory(dest) ? new Path(dest, sourceFile.getName()) : dest;
    // then use the same file name. Otherwise, generate a unique file name.
    if (renameIfAlreadyExists && destFs.exists(destFile)) {
        Path destDir = destFs.isDirectory(dest) ? dest : dest.getParent();
        destFile = new Path(destDir, appendToBaseFileName(destFile.getName(), UUID.randomUUID().toString()));
    }
    boolean sameFileSystem = isPathOnFileSystem(sourceFile, destFs);
    boolean destIsDfs = isDistributedFileSystem(destFs);
    // If the source and the destination are on different file systems, or in different
    // encryption zones, files can't be moved from one location to the other and must be
    // copied instead.
    boolean sameEncryptionZone = arePathsInSameHdfsEncryptionZone(destFs, sourceFile, destFile);
    // We can do a rename if the src and dst are in the same encryption zone in the same
    // distributed filesystem.
    boolean doRename = destIsDfs && sameFileSystem && sameEncryptionZone;
    // non-distributed filesystem.
    if (!doRename)
        doRename = !destIsDfs && sameFileSystem;
    if (doRename) {
        LOG.debug(String.format("Moving '%s' to '%s'", sourceFile.toString(), destFile.toString()));
        // Move (rename) the file.
        destFs.rename(sourceFile, destFile);
        return;
    }
    if (destIsDfs && sameFileSystem) {
        Preconditions.checkState(!doRename);
        // We must copy rather than move if the source and dest are in different
        // encryption zones. A move would return an error from the NN because a move is a
        // metadata-only operation and the files would not be encrypted/decrypted properly
        // on the DNs.
        LOG.info(String.format("Copying source '%s' to '%s' because HDFS encryption zones are different.", sourceFile, destFile));
    } else {
        Preconditions.checkState(!sameFileSystem);
        LOG.info(String.format("Copying '%s' to '%s' between filesystems.", sourceFile, destFile));
    }
    FileUtil.copy(sourceFs, sourceFile, destFs, destFile, true, true, CONF);
}
#method_after
public static void relocateFile(Path sourceFile, Path dest, boolean renameIfAlreadyExists) throws IOException {
    FileSystem destFs = dest.getFileSystem(CONF);
    FileSystem sourceFs = sourceFile.getFileSystem(CONF);
    Path destFile = destFs.isDirectory(dest) ? new Path(dest, sourceFile.getName()) : dest;
    // then use the same file name. Otherwise, generate a unique file name.
    if (renameIfAlreadyExists && destFs.exists(destFile)) {
        Path destDir = destFs.isDirectory(dest) ? dest : dest.getParent();
        destFile = new Path(destDir, appendToBaseFileName(destFile.getName(), UUID.randomUUID().toString()));
    }
    boolean sameFileSystem = isPathOnFileSystem(sourceFile, destFs);
    boolean destIsDfs = isDistributedFileSystem(destFs);
    // If the source and the destination are on different file systems, or in different
    // encryption zones, files can't be moved from one location to the other and must be
    // copied instead.
    boolean sameEncryptionZone = arePathsInSameHdfsEncryptionZone(destFs, sourceFile, destFile);
    // We can do a rename if the src and dst are in the same encryption zone in the same
    // distributed filesystem.
    boolean doRename = destIsDfs && sameFileSystem && sameEncryptionZone;
    // non-distributed filesystem.
    if (!doRename)
        doRename = !destIsDfs && sameFileSystem;
    if (doRename) {
        if (LOG.isTraceEnabled()) {
            LOG.trace(String.format("Moving '%s' to '%s'", sourceFile.toString(), destFile.toString()));
        }
        // Move (rename) the file.
        destFs.rename(sourceFile, destFile);
        return;
    }
    if (destIsDfs && sameFileSystem) {
        Preconditions.checkState(!doRename);
        // on the DNs.
        if (LOG.isTraceEnabled()) {
            LOG.trace(String.format("Copying source '%s' to '%s' because HDFS encryption zones are different.", sourceFile, destFile));
        }
    } else {
        Preconditions.checkState(!sameFileSystem);
        if (LOG.isTraceEnabled()) {
            LOG.trace(String.format("Copying '%s' to '%s' between filesystems.", sourceFile, destFile));
        }
    }
    FileUtil.copy(sourceFs, sourceFile, destFs, destFile, true, true, CONF);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    dbName_ = analyzer.getTargetDbName(tableName_);
    try {
        Table table = analyzer.getTable(tableName_, Privilege.DROP);
        Preconditions.checkNotNull(table);
        if (table instanceof View && dropTable_) {
            throw new AnalysisException(String.format("DROP TABLE not allowed on a view: %s.%s", dbName_, getTbl()));
        }
        if (!(table instanceof View) && !dropTable_) {
            throw new AnalysisException(String.format("DROP VIEW not allowed on a table: %s.%s", dbName_, getTbl()));
        }
    } catch (AnalysisException e) {
        if (ifExists_ && analyzer.getMissingTbls().isEmpty())
            return;
        // from Kudu externally and we should allow the DROP to happen.
        if (e.getMessage().contains(KuduTable.ERROR_OPENING_TABLE_MSG))
            return;
        throw e;
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    dbName_ = analyzer.getTargetDbName(tableName_);
    try {
        Table table = analyzer.getTable(tableName_, Privilege.DROP, true);
        Preconditions.checkNotNull(table);
        if (table instanceof View && dropTable_) {
            throw new AnalysisException(String.format("DROP TABLE not allowed on a view: %s.%s", dbName_, getTbl()));
        }
        if (!(table instanceof View) && !dropTable_) {
            throw new AnalysisException(String.format("DROP VIEW not allowed on a table: %s.%s", dbName_, getTbl()));
        }
    } catch (TableLoadingException e) {
        // We should still try to DROP tables that failed to load, so that tables that are
        // in a bad state, eg. deleted externally from Kudu, can be dropped.
        // We still need an access event - we don't know if this is a TABLE or a VIEW, so
        // we set it as TABLE as VIEW loading is unlikely to fail and even if it does
        // TABLE -> VIEW is a small difference.
        analyzer.addAccessEvent(new TAccessEvent(tableName_.toString(), TCatalogObjectType.TABLE, Privilege.DROP.toString()));
    } catch (AnalysisException e) {
        if (ifExists_ && analyzer.getMissingTbls().isEmpty())
            return;
        throw e;
    }
}
#end_block

#method_before
static void createManagedTable(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params) throws ImpalaRuntimeException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String kuduTableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Creating table '%s' in master '%s'", kuduTableName, masterHosts));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        // (see KUDU-1710).
        if (kudu.tableExists(kuduTableName)) {
            if (params.if_not_exists)
                return;
            throw new ImpalaRuntimeException(String.format("Table '%s' already exists in Kudu.", kuduTableName));
        }
        Schema schema = createTableSchema(params);
        CreateTableOptions tableOpts = buildTableOptions(msTbl, params, schema);
        kudu.createTable(kuduTableName, schema, tableOpts);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error creating Kudu table '%s'", kuduTableName), e);
    }
}
#method_after
static void createManagedTable(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params) throws ImpalaRuntimeException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String kuduTableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Creating table '%s' in master '%s'", kuduTableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        // (see KUDU-1710).
        if (kudu.tableExists(kuduTableName)) {
            if (params.if_not_exists)
                return;
            throw new ImpalaRuntimeException(String.format("Table '%s' already exists in Kudu.", kuduTableName));
        }
        Schema schema = createTableSchema(params);
        CreateTableOptions tableOpts = buildTableOptions(msTbl, params, schema);
        kudu.createTable(kuduTableName, schema, tableOpts);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error creating Kudu table '%s'", kuduTableName), e);
    }
}
#end_block

#method_before
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the distribution schemes
    List<TDistributeParam> distributeParams = params.getDistribute_by();
    if (distributeParams != null) {
        boolean hasRangePartitioning = false;
        for (TDistributeParam distParam : distributeParams) {
            if (distParam.isSetBy_hash_param()) {
                Preconditions.checkState(!distParam.isSetBy_range_param());
                tableOpts.addHashPartitions(distParam.getBy_hash_param().getColumns(), distParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(distParam.isSetBy_range_param());
                hasRangePartitioning = true;
                List<String> rangePartitionColumns = distParam.getBy_range_param().getColumns();
                tableOpts.setRangePartitionColumns(rangePartitionColumns);
                for (TRangePartition rangePartition : distParam.getBy_range_param().getRange_partitions()) {
                    Preconditions.checkState(rangePartition.isSetLower_bound_values() || rangePartition.isSetUpper_bound_values());
                    Pair<PartialRow, RangePartitionBound> lowerBound = KuduUtil.buildRangePartitionBound(schema, rangePartitionColumns, rangePartition.getLower_bound_values(), rangePartition.isIs_lower_bound_inclusive());
                    Pair<PartialRow, RangePartitionBound> upperBound = KuduUtil.buildRangePartitionBound(schema, rangePartitionColumns, rangePartition.getUpper_bound_values(), rangePartition.isIs_upper_bound_inclusive());
                    tableOpts.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        int parsedReplicas = -1;
        try {
            parsedReplicas = Integer.parseInt(replication);
            Preconditions.checkState(parsedReplicas > 0, "Invalid number of replicas table property:" + replication);
        } catch (Exception e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication));
        }
        tableOpts.setNumReplicas(parsedReplicas);
    }
    return tableOpts;
}
#method_after
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the distribution schemes
    List<TDistributeParam> distributeParams = params.getDistribute_by();
    if (distributeParams != null) {
        boolean hasRangePartitioning = false;
        for (TDistributeParam distParam : distributeParams) {
            if (distParam.isSetBy_hash_param()) {
                Preconditions.checkState(!distParam.isSetBy_range_param());
                tableOpts.addHashPartitions(distParam.getBy_hash_param().getColumns(), distParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(distParam.isSetBy_range_param());
                hasRangePartitioning = true;
                List<String> rangePartitionColumns = distParam.getBy_range_param().getColumns();
                tableOpts.setRangePartitionColumns(rangePartitionColumns);
                for (TRangePartition rangePartition : distParam.getBy_range_param().getRange_partitions()) {
                    List<Pair<PartialRow, RangePartitionBound>> rangeBounds = getRangePartitionBounds(rangePartition, schema, rangePartitionColumns);
                    Preconditions.checkState(rangeBounds.size() == 2);
                    Pair<PartialRow, RangePartitionBound> lowerBound = rangeBounds.get(0);
                    Pair<PartialRow, RangePartitionBound> upperBound = rangeBounds.get(1);
                    tableOpts.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        int parsedReplicas = -1;
        try {
            parsedReplicas = Integer.parseInt(replication);
            Preconditions.checkState(parsedReplicas > 0, "Invalid number of replicas table property:" + replication);
        } catch (Exception e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication));
        }
        tableOpts.setNumReplicas(parsedReplicas);
    }
    return tableOpts;
}
#end_block

#method_before
static void dropTable(org.apache.hadoop.hive.metastore.api.Table msTbl, boolean ifExists) throws ImpalaRuntimeException, TableNotFoundException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String tableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Dropping table '%s' from master '%s'", tableName, masterHosts));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        Preconditions.checkState(!Strings.isNullOrEmpty(tableName));
        // (see KUDU-1710).
        if (kudu.tableExists(tableName)) {
            kudu.deleteTable(tableName);
        } else if (!ifExists) {
            throw new TableNotFoundException(String.format("Table '%s' does not exist in Kudu master(s) '%s'.", tableName, masterHosts));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error dropping table '%s'", tableName), e);
    }
}
#method_after
static void dropTable(org.apache.hadoop.hive.metastore.api.Table msTbl, boolean ifExists) throws ImpalaRuntimeException, TableNotFoundException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String tableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Dropping table '%s' from master '%s'", tableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        Preconditions.checkState(!Strings.isNullOrEmpty(tableName));
        // (see KUDU-1710).
        if (kudu.tableExists(tableName)) {
            kudu.deleteTable(tableName);
        } else if (!ifExists) {
            throw new TableNotFoundException(String.format("Table '%s' does not exist in Kudu master(s) '%s'.", tableName, masterHosts));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error dropping table '%s'", tableName), e);
    }
}
#end_block

#method_before
public static void populateColumnsFromKudu(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    org.apache.hadoop.hive.metastore.api.Table msTblCopy = msTbl.deepCopy();
    List<FieldSchema> cols = msTblCopy.getSd().getCols();
    String kuduTableName = msTblCopy.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    String masterHosts = msTblCopy.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Loading schema of table '%s' from master '%s'", kuduTableName, masterHosts));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        if (!kudu.tableExists(kuduTableName)) {
            throw new ImpalaRuntimeException(String.format("Table does not exist in Kudu: " + "'%s'", kuduTableName));
        }
        org.apache.kudu.client.KuduTable kuduTable = kudu.openTable(kuduTableName);
        // Replace the columns in the Metastore table with the columns from the recently
        // accessed Kudu schema.
        cols.clear();
        for (ColumnSchema colSchema : kuduTable.getSchema().getColumns()) {
            Type type = KuduUtil.toImpalaType(colSchema.getType());
            cols.add(new FieldSchema(colSchema.getName(), type.toSql().toLowerCase(), null));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error loading schema of table " + "'%s'", kuduTableName), e);
    }
    List<FieldSchema> newCols = msTbl.getSd().getCols();
    newCols.clear();
    newCols.addAll(cols);
}
#method_after
public static void populateColumnsFromKudu(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    org.apache.hadoop.hive.metastore.api.Table msTblCopy = msTbl.deepCopy();
    List<FieldSchema> cols = msTblCopy.getSd().getCols();
    String kuduTableName = msTblCopy.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    String masterHosts = msTblCopy.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    if (LOG.isDebugEnabled()) {
        LOG.debug(String.format("Loading schema of table '%s' from master '%s'", kuduTableName, masterHosts));
    }
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        if (!kudu.tableExists(kuduTableName)) {
            throw new ImpalaRuntimeException(String.format("Table does not exist in Kudu: " + "'%s'", kuduTableName));
        }
        org.apache.kudu.client.KuduTable kuduTable = kudu.openTable(kuduTableName);
        // Replace the columns in the Metastore table with the columns from the recently
        // accessed Kudu schema.
        cols.clear();
        for (ColumnSchema colSchema : kuduTable.getSchema().getColumns()) {
            Type type = KuduUtil.toImpalaType(colSchema.getType());
            cols.add(new FieldSchema(colSchema.getName(), type.toSql().toLowerCase(), null));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error loading schema of table " + "'%s'", kuduTableName), e);
    }
    List<FieldSchema> newCols = msTbl.getSd().getCols();
    newCols.clear();
    newCols.addAll(cols);
}
#end_block

#method_before
private void startCluster(int numMasters, int numTservers) throws Exception {
    Preconditions.checkArgument(numMasters > 0, "Need at least one master");
    // The following props are set via kudu-client's pom.
    String baseDirPath = TestUtils.getBaseDir();
    long now = System.currentTimeMillis();
    LOG.info("Starting {} masters...", numMasters);
    int startPort = startMasters(PORT_START, numMasters, baseDirPath, bindHost);
    LOG.info("Starting {} tablet servers...", numTservers);
    List<Integer> ports = TestUtils.findFreePorts(startPort, numTservers * 2);
    for (int i = 0; i < numTservers; i++) {
        int rpcPort = ports.get(i * 2);
        tserverPorts.add(rpcPort);
        String dataDirPath = baseDirPath + "/ts-" + i + "-" + now;
        String flagsPath = TestUtils.getFlagsPath();
        List<String> commandLine = Lists.newArrayList(TestUtils.findBinary("kudu-tserver"), "--flagfile=" + flagsPath, "--fs_wal_dir=" + dataDirPath, "--fs_data_dirs=" + dataDirPath, "--flush_threshold_mb=1", "--tserver_master_addrs=" + masterAddresses, "--webserver_interface=" + bindHost, "--local_ip_for_outbound_sockets=" + bindHost, "--webserver_port=" + (rpcPort + 1), "--rpc_bind_addresses=" + bindHost + ":" + rpcPort);
        if (miniKdc != null) {
            commandLine.add("--keytab=" + keytab);
            commandLine.add("--kerberos_principal=kudu/" + bindHost);
            commandLine.add("--server_require_kerberos");
        }
        for (String extraTserverFlag : extraTserverFlags) {
            commandLine.add(extraTserverFlag);
        }
        tserverProcesses.put(rpcPort, configureAndStartProcess(rpcPort, commandLine));
        commandLines.put(rpcPort, commandLine);
        if (flagsPath.startsWith(baseDirPath)) {
            // We made a temporary copy of the flags; delete them later.
            pathsToDelete.add(flagsPath);
        }
        pathsToDelete.add(dataDirPath);
    }
}
#method_after
private void startCluster(int numMasters, int numTservers) throws Exception {
    Preconditions.checkArgument(numMasters > 0, "Need at least one master");
    // The following props are set via kudu-client's pom.
    String baseDirPath = TestUtils.getBaseDir();
    long now = System.currentTimeMillis();
    LOG.info("Starting {} masters...", numMasters);
    int startPort = startMasters(PORT_START, numMasters, baseDirPath, bindHost);
    LOG.info("Starting {} tablet servers...", numTservers);
    List<Integer> ports = TestUtils.findFreePorts(startPort, numTservers * 2);
    for (int i = 0; i < numTservers; i++) {
        int rpcPort = ports.get(i * 2);
        tserverPorts.add(rpcPort);
        String dataDirPath = baseDirPath + "/ts-" + i + "-" + now;
        String flagsPath = TestUtils.getFlagsPath();
        List<String> commandLine = Lists.newArrayList(TestUtils.findBinary("kudu-tserver"), "--flagfile=" + flagsPath, "--fs_wal_dir=" + dataDirPath, "--fs_data_dirs=" + dataDirPath, "--flush_threshold_mb=1", "--tserver_master_addrs=" + masterAddresses, "--webserver_interface=" + bindHost, "--local_ip_for_outbound_sockets=" + bindHost, "--webserver_port=" + (rpcPort + 1), "--rpc_bind_addresses=" + bindHost + ":" + rpcPort);
        if (miniKdc != null) {
            commandLine.add("--keytab=" + keytab);
            commandLine.add("--kerberos_principal=kudu/" + bindHost);
            commandLine.add("--server_require_kerberos");
        }
        commandLine.addAll(extraTserverFlags);
        tserverProcesses.put(rpcPort, configureAndStartProcess(rpcPort, commandLine));
        commandLines.put(rpcPort, commandLine);
        if (flagsPath.startsWith(baseDirPath)) {
            // We made a temporary copy of the flags; delete them later.
            pathsToDelete.add(flagsPath);
        }
        pathsToDelete.add(dataDirPath);
    }
}
#end_block

#method_before
private int startMasters(int masterStartPort, int numMasters, String baseDirPath, String bindHost) throws Exception {
    LOG.info("Starting {} masters...", numMasters);
    // Get the list of web and RPC ports to use for the master consensus configuration:
    // request NUM_MASTERS * 2 free ports as we want to also reserve the web
    // ports for the consensus configuration.
    List<Integer> ports = TestUtils.findFreePorts(masterStartPort, numMasters * 2);
    int lastFreePort = ports.get(ports.size() - 1);
    List<Integer> masterRpcPorts = Lists.newArrayListWithCapacity(numMasters);
    List<Integer> masterWebPorts = Lists.newArrayListWithCapacity(numMasters);
    for (int i = 0; i < numMasters * 2; i++) {
        if (i % 2 == 0) {
            masterRpcPorts.add(ports.get(i));
            masterHostPorts.add(HostAndPort.fromParts(bindHost, ports.get(i)));
        } else {
            masterWebPorts.add(ports.get(i));
        }
    }
    masterAddresses = NetUtil.hostsAndPortsToString(masterHostPorts);
    long now = System.currentTimeMillis();
    for (int i = 0; i < numMasters; i++) {
        int port = masterRpcPorts.get(i);
        String dataDirPath = baseDirPath + "/master-" + i + "-" + now;
        String flagsPath = TestUtils.getFlagsPath();
        // The web port must be reserved in the call to findFreePorts above and specified
        // to avoid the scenario where:
        // 1) findFreePorts finds RPC ports a, b, c for the 3 masters.
        // 2) start master 1 with RPC port and let it bind to any (specified as 0) web port.
        // 3) master 1 happens to bind to port b for the web port, as master 2 hasn't been
        // started yet and findFreePort(s) is "check-time-of-use" (it does not reserve the
        // ports, only checks that when it was last called, these ports could be used).
        List<String> commandLine = Lists.newArrayList(TestUtils.findBinary("kudu-master"), "--flagfile=" + flagsPath, "--fs_wal_dir=" + dataDirPath, "--fs_data_dirs=" + dataDirPath, "--webserver_interface=" + bindHost, "--local_ip_for_outbound_sockets=" + bindHost, "--rpc_bind_addresses=" + bindHost + ":" + port, "--webserver_port=" + masterWebPorts.get(i), // make leader elections faster for faster tests
        "--raft_heartbeat_interval_ms=200");
        if (numMasters > 1) {
            commandLine.add("--master_addresses=" + masterAddresses);
        }
        if (miniKdc != null) {
            commandLine.add("--keytab=" + keytab);
            commandLine.add("--kerberos_principal=kudu/" + bindHost);
            commandLine.add("--server_require_kerberos");
        }
        for (String extraMasterFlag : extraMasterFlags) {
            commandLine.add(extraMasterFlag);
        }
        masterProcesses.put(port, configureAndStartProcess(port, commandLine));
        commandLines.put(port, commandLine);
        if (flagsPath.startsWith(baseDirPath)) {
            // We made a temporary copy of the flags; delete them later.
            pathsToDelete.add(flagsPath);
        }
        pathsToDelete.add(dataDirPath);
    }
    return lastFreePort + 1;
}
#method_after
private int startMasters(int masterStartPort, int numMasters, String baseDirPath, String bindHost) throws Exception {
    LOG.info("Starting {} masters...", numMasters);
    // Get the list of web and RPC ports to use for the master consensus configuration:
    // request NUM_MASTERS * 2 free ports as we want to also reserve the web
    // ports for the consensus configuration.
    List<Integer> ports = TestUtils.findFreePorts(masterStartPort, numMasters * 2);
    int lastFreePort = ports.get(ports.size() - 1);
    List<Integer> masterRpcPorts = Lists.newArrayListWithCapacity(numMasters);
    List<Integer> masterWebPorts = Lists.newArrayListWithCapacity(numMasters);
    for (int i = 0; i < numMasters * 2; i++) {
        if (i % 2 == 0) {
            masterRpcPorts.add(ports.get(i));
            masterHostPorts.add(HostAndPort.fromParts(bindHost, ports.get(i)));
        } else {
            masterWebPorts.add(ports.get(i));
        }
    }
    masterAddresses = NetUtil.hostsAndPortsToString(masterHostPorts);
    long now = System.currentTimeMillis();
    for (int i = 0; i < numMasters; i++) {
        int port = masterRpcPorts.get(i);
        String dataDirPath = baseDirPath + "/master-" + i + "-" + now;
        String flagsPath = TestUtils.getFlagsPath();
        // The web port must be reserved in the call to findFreePorts above and specified
        // to avoid the scenario where:
        // 1) findFreePorts finds RPC ports a, b, c for the 3 masters.
        // 2) start master 1 with RPC port and let it bind to any (specified as 0) web port.
        // 3) master 1 happens to bind to port b for the web port, as master 2 hasn't been
        // started yet and findFreePort(s) is "check-time-of-use" (it does not reserve the
        // ports, only checks that when it was last called, these ports could be used).
        List<String> commandLine = Lists.newArrayList(TestUtils.findBinary("kudu-master"), "--flagfile=" + flagsPath, "--fs_wal_dir=" + dataDirPath, "--fs_data_dirs=" + dataDirPath, "--webserver_interface=" + bindHost, "--local_ip_for_outbound_sockets=" + bindHost, "--rpc_bind_addresses=" + bindHost + ":" + port, "--webserver_port=" + masterWebPorts.get(i), // make leader elections faster for faster tests
        "--raft_heartbeat_interval_ms=200");
        if (numMasters > 1) {
            commandLine.add("--master_addresses=" + masterAddresses);
        }
        if (miniKdc != null) {
            commandLine.add("--keytab=" + keytab);
            commandLine.add("--kerberos_principal=kudu/" + bindHost);
            commandLine.add("--server_require_kerberos");
        }
        commandLine.addAll(extraMasterFlags);
        masterProcesses.put(port, configureAndStartProcess(port, commandLine));
        commandLines.put(port, commandLine);
        if (flagsPath.startsWith(baseDirPath)) {
            // We made a temporary copy of the flags; delete them later.
            pathsToDelete.add(flagsPath);
        }
        pathsToDelete.add(dataDirPath);
    }
    return lastFreePort + 1;
}
#end_block

#method_before
public static String getIdentSql(String ident) {
    boolean hiveNeedsQuotes = true;
    HiveLexer hiveLexer = new HiveLexer(new ANTLRStringStream(ident));
    try {
        Token t = hiveLexer.nextToken();
        // Check that the lexer recognizes an identifier and then EOF.
        boolean identFound = t.getType() == HiveLexer.Identifier;
        t = hiveLexer.nextToken();
        // No enclosing quotes are necessary for Hive.
        hiveNeedsQuotes = !(identFound && t.getType() == HiveLexer.EOF);
    } catch (Exception e) {
    // Ignore exception and just quote the identifier to be safe.
    }
    boolean isImpalaKeyword = SqlScanner.isKeyword(ident.toUpperCase());
    // Impala's scanner recognizes the ".123" portion of "db.123_tbl" as a decimal,
    // so while the quoting is not necessary for the given identifier itself, the quotes
    // are needed if this identifier will be preceded by a ".".
    boolean startsWithNumber = false;
    if (!hiveNeedsQuotes && !isImpalaKeyword) {
        try {
            Integer.parseInt(ident.substring(0, 1));
            startsWithNumber = true;
        } catch (NumberFormatException e) {
        // Ignore exception, identifier does not start with number.
        }
    }
    if (hiveNeedsQuotes || isImpalaKeyword || startsWithNumber)
        return "`" + ident + "`";
    return ident;
}
#method_after
public static String getIdentSql(String ident) {
    boolean hiveNeedsQuotes = true;
    HiveLexer hiveLexer = new HiveLexer(new ANTLRStringStream(ident));
    try {
        Token t = hiveLexer.nextToken();
        // Check that the lexer recognizes an identifier and then EOF.
        boolean identFound = t.getType() == HiveLexer.Identifier;
        t = hiveLexer.nextToken();
        // No enclosing quotes are necessary for Hive.
        hiveNeedsQuotes = !(identFound && t.getType() == HiveLexer.EOF);
    } catch (Exception e) {
    // Ignore exception and just quote the identifier to be safe.
    }
    boolean isImpalaKeyword = SqlScanner.isKeyword(ident.toUpperCase());
    // Impala's scanner recognizes the ".123" portion of "db.123_tbl" as a decimal,
    // so while the quoting is not necessary for the given identifier itself, the quotes
    // are needed if this identifier will be preceded by a ".".
    boolean startsWithNumber = false;
    if (!hiveNeedsQuotes && !isImpalaKeyword) {
        startsWithNumber = Character.isDigit(ident.charAt(0));
    }
    if (hiveNeedsQuotes || isImpalaKeyword || startsWithNumber)
        return "`" + ident + "`";
    return ident;
}
#end_block

#method_before
public List<String> getRangeDistributionColNames() {
    DistributeParam rangeDistribution = getRangeDistribution();
    if (rangeDistribution == null)
        return Collections.<String>emptyList();
    return ImmutableList.copyOf(rangeDistribution.getColumnNames());
}
#method_after
public List<String> getRangeDistributionColNames() {
    DistributeParam rangeDistribution = getRangeDistribution();
    if (rangeDistribution == null)
        return Collections.<String>emptyList();
    return rangeDistribution.getColumnNames();
}
#end_block

#method_before
private void loadDistributeByParams(org.apache.kudu.client.KuduTable kuduTable) {
    Preconditions.checkNotNull(kuduTable);
    Schema tableSchema = kuduTable.getSchema();
    PartitionSchema partitionSchema = kuduTable.getPartitionSchema();
    Preconditions.checkState(!colsByPos_.isEmpty());
    distributeBy_.clear();
    for (HashBucketSchema hashBucketSchema : partitionSchema.getHashBucketSchemas()) {
        List<String> columnNames = Lists.newArrayList();
        for (int colId : hashBucketSchema.getColumnIds()) {
            ColumnSchema col = tableSchema.getColumnByIndex(tableSchema.getColumnIndex(colId));
            Preconditions.checkNotNull(col);
            columnNames.add(col.getName());
        }
        distributeBy_.add(DistributeParam.createHashParam(columnNames, hashBucketSchema.getNumBuckets()));
    }
    RangeSchema rangeSchema = partitionSchema.getRangeSchema();
    List<Integer> columnIds = rangeSchema.getColumns();
    if (columnIds.isEmpty())
        return;
    List<String> columnNames = Lists.newArrayList();
    for (int colId : columnIds) {
        ColumnSchema col = tableSchema.getColumnByIndex(tableSchema.getColumnIndex(colId));
        Preconditions.checkNotNull(col);
        columnNames.add(col.getName());
    }
    // We don't populate the split values because Kudu's API doesn't currently support
    // retrieving the split values for range partitions.
    // TODO: File a Kudu JIRA.
    distributeBy_.add(DistributeParam.createRangeParam(columnNames, null));
}
#method_after
private void loadDistributeByParams(org.apache.kudu.client.KuduTable kuduTable) {
    Preconditions.checkNotNull(kuduTable);
    Schema tableSchema = kuduTable.getSchema();
    PartitionSchema partitionSchema = kuduTable.getPartitionSchema();
    Preconditions.checkState(!colsByPos_.isEmpty());
    distributeBy_.clear();
    for (HashBucketSchema hashBucketSchema : partitionSchema.getHashBucketSchemas()) {
        List<String> columnNames = Lists.newArrayList();
        for (int colId : hashBucketSchema.getColumnIds()) {
            columnNames.add(getColumnNameById(tableSchema, colId));
        }
        distributeBy_.add(DistributeParam.createHashParam(columnNames, hashBucketSchema.getNumBuckets()));
    }
    RangeSchema rangeSchema = partitionSchema.getRangeSchema();
    List<Integer> columnIds = rangeSchema.getColumns();
    if (columnIds.isEmpty())
        return;
    List<String> columnNames = Lists.newArrayList();
    for (int colId : columnIds) columnNames.add(getColumnNameById(tableSchema, colId));
    // We don't populate the split values because Kudu's API doesn't currently support
    // retrieving the split values for range partitions.
    // TODO: File a Kudu JIRA.
    distributeBy_.add(DistributeParam.createRangeParam(columnNames, null));
}
#end_block

#method_before
public boolean hasKuduOptions() {
    return isPrimaryKey_ || isNullable_ != null || encodingVal_ != null || compressionVal_ != null || defaultValue_ != null || blockSize_ != null;
}
#method_after
public boolean hasKuduOptions() {
    return isPrimaryKey() || isNullabilitySet() || hasEncoding() || hasCompression() || hasDefaultValue() || hasBlockSize();
}
#end_block

#method_before
public boolean hasEncoding() {
    return encoding_ != null;
}
#method_after
public boolean hasEncoding() {
    return encodingVal_ != null;
}
#end_block

#method_before
public boolean hasCompression() {
    return compression_ != null;
}
#method_after
public boolean hasCompression() {
    return compressionVal_ != null;
}
#end_block

#method_before
public boolean isNullable() {
    return isNullable_ != null && isNullable_;
}
#method_after
public boolean isNullable() {
    return isNullabilitySet() && isNullable_;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    Table t = getTargetTable();
    // mappings along with the table columns.
    if (t instanceof HBaseTable) {
        throw new AnalysisException("ALTER TABLE CHANGE COLUMN not currently supported " + "on HBase tables.");
    }
    String tableName = getDb() + "." + getTbl();
    // Verify there are no conflicts with partition columns.
    for (FieldSchema fs : t.getMetaStoreTable().getPartitionKeys()) {
        if (fs.getName().toLowerCase().equals(colName_.toLowerCase())) {
            throw new AnalysisException("Cannot modify partition column: " + colName_);
        }
        if (fs.getName().toLowerCase().equals(newColDef_.getColName().toLowerCase())) {
            throw new AnalysisException("Column name conflicts with existing partition column: " + newColDef_.getColName());
        }
    }
    // Verify the column being modified exists in the table
    if (t.getColumn(colName_) == null) {
        throw new AnalysisException(String.format("Column '%s' does not exist in table: %s", colName_, tableName));
    }
    // Check that the new column def's name is valid.
    newColDef_.analyze(analyzer);
    // with an existing column.
    if (!colName_.toLowerCase().equals(newColDef_.getColName().toLowerCase()) && t.getColumn(newColDef_.getColName()) != null) {
        throw new AnalysisException("Column already exists: " + newColDef_.getColName());
    }
    if (newColDef_.hasKuduOptions()) {
        throw new AnalysisException("Unsupported column options in ALTER TABLE CHANGE " + "COLUMN statement: " + newColDef_.toString());
    }
    if (t instanceof KuduTable) {
        Column col = t.getColumn(colName_);
        if (col.getType() != newColDef_.getType()) {
            throw new AnalysisException(String.format("Cannot change the type of a Kudu " + "column using an ALTER TABLE CHANGE COLUMN statement: (%s vs %s)", col.getType().toSql(), newColDef_.getType().toSql()));
        }
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    Table t = getTargetTable();
    // mappings along with the table columns.
    if (t instanceof HBaseTable) {
        throw new AnalysisException("ALTER TABLE CHANGE COLUMN not currently supported " + "on HBase tables.");
    }
    String tableName = getDb() + "." + getTbl();
    // Verify there are no conflicts with partition columns.
    for (FieldSchema fs : t.getMetaStoreTable().getPartitionKeys()) {
        if (fs.getName().toLowerCase().equals(colName_.toLowerCase())) {
            throw new AnalysisException("Cannot modify partition column: " + colName_);
        }
        if (fs.getName().toLowerCase().equals(newColDef_.getColName().toLowerCase())) {
            throw new AnalysisException("Column name conflicts with existing partition column: " + newColDef_.getColName());
        }
    }
    // Verify the column being modified exists in the table
    if (t.getColumn(colName_) == null) {
        throw new AnalysisException(String.format("Column '%s' does not exist in table: %s", colName_, tableName));
    }
    // Check that the new column def's name is valid.
    newColDef_.analyze(analyzer);
    // with an existing column.
    if (!colName_.toLowerCase().equals(newColDef_.getColName().toLowerCase()) && t.getColumn(newColDef_.getColName()) != null) {
        throw new AnalysisException("Column already exists: " + newColDef_.getColName());
    }
    if (newColDef_.hasKuduOptions()) {
        throw new AnalysisException("Unsupported column options in ALTER TABLE CHANGE " + "COLUMN statement: " + newColDef_.toString());
    }
    if (t instanceof KuduTable) {
        Column col = t.getColumn(colName_);
        if (!col.getType().equals(newColDef_.getType())) {
            throw new AnalysisException(String.format("Cannot change the type of a Kudu " + "column using an ALTER TABLE CHANGE COLUMN statement: (%s vs %s)", col.getType().toSql(), newColDef_.getType().toSql()));
        }
    }
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder sb = new StringBuilder("ALTER TABLE " + getTbl());
    sb.append(" DROP ");
    if (ifExists_)
        sb.append("IF EXISTS ");
    if (partitionSet_ != null)
        sb.append(partitionSet_.toSql());
    if (rangePartitionSpec_ != null)
        sb.append(rangePartitionSpec_.toSql());
    if (purgePartition_)
        sb.append(" PURGE");
    return sb.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder sb = new StringBuilder("ALTER TABLE " + getTbl());
    sb.append(" DROP ");
    if (ifExists_)
        sb.append("IF EXISTS ");
    sb.append(partitionSet_.toSql());
    if (purgePartition_)
        sb.append(" PURGE");
    return sb.toString();
}
#end_block

#method_before
@Override
public TAlterTableParams toThrift() {
    TAlterTableParams params = super.toThrift();
    params.setAlter_type(TAlterTableType.DROP_PARTITION);
    TAlterTableDropPartitionParams addPartParams = new TAlterTableDropPartitionParams();
    if (partitionSet_ != null) {
        addPartParams.setPartition_set(partitionSet_.toThrift());
        addPartParams.setIf_exists(!partitionSet_.getPartitionShouldExist());
    }
    if (rangePartitionSpec_ != null) {
        addPartParams.setRange_partition_spec(rangePartitionSpec_.toThrift());
    }
    addPartParams.setIf_exists(ifExists_);
    addPartParams.setPurge(purgePartition_);
    params.setDrop_partition_params(addPartParams);
    return params;
}
#method_after
@Override
public TAlterTableParams toThrift() {
    TAlterTableParams params = super.toThrift();
    params.setAlter_type(TAlterTableType.DROP_PARTITION);
    TAlterTableDropPartitionParams addPartParams = new TAlterTableDropPartitionParams();
    addPartParams.setPartition_set(partitionSet_.toThrift());
    addPartParams.setIf_exists(!partitionSet_.getPartitionShouldExist());
    addPartParams.setPurge(purgePartition_);
    params.setDrop_partition_params(addPartParams);
    return params;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    Preconditions.checkState(partitionSet_ != null ^ rangePartitionSpec_ != null);
    Table table = getTargetTable();
    if (table instanceof KuduTable) {
        if (partitionSet_ != null) {
            throw new AnalysisException("Key-value partition specs are not supported for " + "Kudu tables: " + partitionSet_.toSql());
        }
        Preconditions.checkState(!purgePartition_);
        AlterTableStmt.analyzeAddDropRangePartition(rangePartitionSpec_, (KuduTable) table, analyzer);
        return;
    }
    if (rangePartitionSpec_ != null) {
        throw new AnalysisException(String.format("Table %s does not support range " + "partitions: RANGE %s", table.getFullName(), rangePartitionSpec_.toSql()));
    }
    if (!ifExists_)
        partitionSet_.setPartitionShouldExist();
    partitionSet_.setPrivilegeRequirement(Privilege.ALTER);
    partitionSet_.analyze(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    Table table = getTargetTable();
    if (table instanceof KuduTable) {
        throw new AnalysisException("ALTER TABLE DROP PARTITION is not supported for " + "Kudu tables: " + partitionSet_.toSql());
    }
    if (!ifExists_)
        partitionSet_.setPartitionShouldExist();
    partitionSet_.setPrivilegeRequirement(Privilege.ALTER);
    partitionSet_.analyze(analyzer);
}
#end_block

#method_before
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='" + long_property_value + "') " + "tblproperties ('" + long_property_key + "'='" + long_property_value + "')");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("create table new_table (i int) " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='value')", "Serde property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('key'='" + long_property_value + "')", "Serde property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
    }
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    String[] fileFormatsStr = { "TEXT", "SEQUENCE_FILE", "PARQUET", "PARQUET", "RC_FILE" };
    int formatIndx = 0;
    for (String format : fileFormats) {
        for (String create : ImmutableList.of("create table", "create external table")) {
            AnalyzesOk(String.format("%s new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", create, format));
            // No column definitions.
            AnalysisError(String.format("%s new_table " + "partitioned by (d decimal) comment 'c' stored as %s", create, format), "Table requires at least 1 column");
        }
        AnalysisError(String.format("create table t (i int primary key) stored as %s", format), String.format("Unsupported column options for file format " + "'%s': 'i INT PRIMARY KEY'", fileFormatsStr[formatIndx]));
        AnalysisError(String.format("create table t (i int, primary key(i)) stored as %s", format), "Only Kudu tables can specify a PRIMARY KEY");
        formatIndx++;
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    // IMPALA-2251: it should not be possible to create text tables with the same
    // delimiter character used for multiple purposes.
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\001' lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\012'", "Field delimiter and line delimiter have same value: byte 10");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by '\001'", "Field delimiter and escape character have same value: byte 1. " + "Escape character will be ignored");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by 'x' lines terminated by 'x'", "Line delimiter and escape character have same value: byte 120. " + "Escape character will be ignored");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: i");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    AnalysisError("create table cached_tbl(i int) location " + "'file:///test-warehouse/cache_tbl' cached in 'testPool'", "Location 'file:/test-warehouse/cache_tbl' cannot be cached. " + "Please retry without caching: CREATE TABLE ... UNCACHED");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Test HMS constraint on type name length.
    AnalyzesOk(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH)));
    AnalysisError(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH + 1)), "Type of column 'i' exceeds maximum type length of 4000 characters:");
    // Test HMS constraint on comment length.
    AnalyzesOk(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH)));
    AnalysisError(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH + 1)), "Comment of column 'i' exceeds maximum length of 256 characters:");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#method_after
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672), " + "s3 varchar(65535))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65536))", "Varchar size must be <= 65535: 65536");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='" + long_property_value + "') " + "tblproperties ('" + long_property_key + "'='" + long_property_value + "')");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("create table new_table (i int) " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='value')", "Serde property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('key'='" + long_property_value + "')", "Serde property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
    }
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    String[] fileFormatsStr = { "TEXT", "SEQUENCE_FILE", "PARQUET", "PARQUET", "RC_FILE" };
    int formatIndx = 0;
    for (String format : fileFormats) {
        for (String create : ImmutableList.of("create table", "create external table")) {
            AnalyzesOk(String.format("%s new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", create, format));
            // No column definitions.
            AnalysisError(String.format("%s new_table " + "partitioned by (d decimal) comment 'c' stored as %s", create, format), "Table requires at least 1 column");
        }
        AnalysisError(String.format("create table t (i int primary key) stored as %s", format), String.format("Unsupported column options for file format " + "'%s': 'i INT PRIMARY KEY'", fileFormatsStr[formatIndx]));
        AnalysisError(String.format("create table t (i int, primary key(i)) stored as %s", format), "Only Kudu tables can specify a PRIMARY KEY");
        formatIndx++;
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    // IMPALA-2251: it should not be possible to create text tables with the same
    // delimiter character used for multiple purposes.
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\001' lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\012'", "Field delimiter and line delimiter have same value: byte 10");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by '\001'", "Field delimiter and escape character have same value: byte 1. " + "Escape character will be ignored");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by 'x' lines terminated by 'x'", "Line delimiter and escape character have same value: byte 120. " + "Escape character will be ignored");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: i");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    AnalysisError("create table cached_tbl(i int) location " + "'file:///test-warehouse/cache_tbl' cached in 'testPool'", "Location 'file:/test-warehouse/cache_tbl' cannot be cached. " + "Please retry without caching: CREATE TABLE ... UNCACHED");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Test HMS constraint on type name length.
    AnalyzesOk(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH)));
    AnalysisError(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH + 1)), "Type of column 'i' exceeds maximum type length of 4000 characters:");
    // Test HMS constraint on comment length.
    AnalyzesOk(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH)));
    AnalysisError(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH + 1)), "Comment of column 'i' exceeds maximum length of 256 characters:");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#end_block

#method_before
@Test
public void TestAlterKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // ALTER TABLE ADD/DROP range partitions
    String[] addDrop = { "add if not exists", "drop if exists" };
    for (String kw : addDrop) {
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition 10 <= values < 20", kw));
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition value = 30", kw));
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition values < 100", kw));
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition 10 <= values", kw));
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition 1+1 <= values <= factorial(3)", kw));
        AnalysisError(String.format("alter table functional.alltypes %s range " + "partition 10 < values < 20", kw), "Table functional.alltypes does not " + "support range partitions: RANGE PARTITION 10 < VALUES < 20");
        AnalysisError(String.format("alter table functional_kudu.testtbl %s range " + "partition values < isnull(null, null)", kw), "Range partition values " + "cannot be NULL. Range partition: 'PARTITION VALUES < isnull(NULL, NULL)'");
    }
    // ALTER TABLE ADD COLUMNS
    // Columns with different supported data types
    AnalyzesOk("alter table functional_kudu.testtbl add columns (a1 tinyint null, a2 " + "smallint null, a3 int null, a4 bigint null, a5 string null, a6 float null, " + "a7 double null, a8 boolean null comment 'boolean')");
    // Complex types
    AnalysisError("alter table functional_kudu.testtbl add columns ( " + "a struct<f1:int>)", "Kudu tables do not support complex types: " + "a STRUCT<f1:INT>");
    // Add primary key
    AnalysisError("alter table functional_kudu.testtbl add columns (a int primary key)", "Cannot add a primary key using an ALTER TABLE ADD COLUMNS statement: " + "a INT PRIMARY KEY");
    // Non-nullable columns require a default value
    AnalyzesOk("alter table functional_kudu.testtbl add columns (a1 int not null " + "default 10)");
    // Non-nullable columns require a default value
    AnalysisError("alter table functional_kudu.testtbl add columns (a1 int)", "New non-nullable columns must have a default value: a1 INT");
    // Nullable column with a default value
    AnalysisError("alter table functional_kudu.testtbl add columns (a1 int null " + "default 10)", "Only non-nullable columns can have a default value: " + "a1 INT NULL DEFAULT 10");
    // Unsupported column options
    AnalysisError("alter table functional_kudu.testtbl add columns " + "(a1 int encoding rle compression lz4 block_size 1024)", "ENCODING, " + "COMPRESSION and BLOCK_SIZE options cannot be specified in an ALTER TABLE " + "ADD COLUMNS statement: a1 INT ENCODING RLE COMPRESSION LZ4 BLOCK_SIZE 1024");
    // REPLACE columns is not supported for Kudu tables
    AnalysisError("alter table functional_kudu.testtbl replace columns (a int null)", "ALTER TABLE REPLACE COLUMNS not currently supported on Kudu tables");
    // Conflict with existing column
    AnalysisError("alter table functional_kudu.testtbl add columns (zip int)", "Column already exists: zip");
    // Kudu column options on an HDFS table
    AnalysisError("alter table functional.alltypes add columns (a int not null)", "Unsupported column options: a INT NOT NULL");
    // ALTER TABLE DROP COLUMN
    AnalyzesOk("alter table functional_kudu.testtbl drop column name");
    AnalysisError("alter table functional_kudu.testtbl drop column no_col", "Column 'no_col' does not exist in table: functional_kudu.testtbl");
    // ALTER TABLE CHANGE COLUMN on Kudu tables
    AnalyzesOk("alter table functional_kudu.testtbl change column name new_name string");
    // Unsupported column options
    AnalysisError("alter table functional_kudu.testtbl change column zip zip_code int " + "encoding rle compression lz4 default 90000", "Unsupported column options in " + "ALTER TABLE CHANGE COLUMN statement: zip_code INT ENCODING RLE COMPRESSION " + "LZ4 DEFAULT 90000");
    // Changing the column type is not supported for Kudu tables
    AnalysisError("alter table functional_kudu.testtbl change column zip zip bigint", "Cannot change the type of a Kudu column using an ALTER TABLE CHANGE COLUMN " + "statement: (INT vs BIGINT)");
    // Rename the underlying Kudu table
    AnalyzesOk("ALTER TABLE functional_kudu.testtbl SET " + "TBLPROPERTIES ('kudu.table_name' = 'Hans')");
    // ALTER TABLE RENAME TO
    AnalyzesOk("ALTER TABLE functional_kudu.testtbl RENAME TO new_testtbl");
}
#method_after
@Test
public void TestAlterKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // ALTER TABLE ADD/DROP range partitions
    String[] addDrop = { "add if not exists", "add", "drop if exists", "drop" };
    for (String kw : addDrop) {
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition 10 <= values < 20", kw));
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition value = 30", kw));
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition values < 100", kw));
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition 10 <= values", kw));
        AnalyzesOk(String.format("alter table functional_kudu.testtbl %s range " + "partition 1+1 <= values <= factorial(3)", kw));
        AnalysisError(String.format("alter table functional.alltypes %s range " + "partition 10 < values < 20", kw), "Table functional.alltypes does not " + "support range partitions: RANGE PARTITION 10 < VALUES < 20");
        AnalysisError(String.format("alter table functional_kudu.testtbl %s range " + "partition values < isnull(null, null)", kw), "Range partition values " + "cannot be NULL. Range partition: 'PARTITION VALUES < isnull(NULL, NULL)'");
    }
    // ALTER TABLE ADD COLUMNS
    // Columns with different supported data types
    AnalyzesOk("alter table functional_kudu.testtbl add columns (a1 tinyint null, a2 " + "smallint null, a3 int null, a4 bigint null, a5 string null, a6 float null, " + "a7 double null, a8 boolean null comment 'boolean')");
    // Complex types
    AnalysisError("alter table functional_kudu.testtbl add columns ( " + "a struct<f1:int>)", "Kudu tables do not support complex types: " + "a STRUCT<f1:INT>");
    // Add primary key
    AnalysisError("alter table functional_kudu.testtbl add columns (a int primary key)", "Cannot add a primary key using an ALTER TABLE ADD COLUMNS statement: " + "a INT PRIMARY KEY");
    // Non-nullable columns require a default value
    AnalyzesOk("alter table functional_kudu.testtbl add columns (a1 int not null " + "default 10)");
    // Unsupported column options
    String[] unsupportedColOptions = { "encoding rle", "compression lz4", "block_size 10" };
    for (String colOption : unsupportedColOptions) {
        AnalysisError(String.format("alter table functional_kudu.testtbl add columns " + "(a1 int %s)", colOption), String.format("ENCODING, COMPRESSION and " + "BLOCK_SIZE options cannot be specified in an ALTER TABLE ADD COLUMNS " + "statement: a1 INT %s", colOption.toUpperCase()));
    }
    // REPLACE columns is not supported for Kudu tables
    AnalysisError("alter table functional_kudu.testtbl replace columns (a int null)", "ALTER TABLE REPLACE COLUMNS is not supported on Kudu tables");
    // Conflict with existing column
    AnalysisError("alter table functional_kudu.testtbl add columns (zip int)", "Column already exists: zip");
    // Kudu column options on an HDFS table
    AnalysisError("alter table functional.alltypes add columns (a int not null)", "The specified column options are only supported in Kudu tables: a INT NOT NULL");
    // ALTER TABLE DROP COLUMN
    AnalyzesOk("alter table functional_kudu.testtbl drop column name");
    AnalysisError("alter table functional_kudu.testtbl drop column no_col", "Column 'no_col' does not exist in table: functional_kudu.testtbl");
    // ALTER TABLE CHANGE COLUMN on Kudu tables
    AnalyzesOk("alter table functional_kudu.testtbl change column name new_name string");
    // Unsupported column options
    AnalysisError("alter table functional_kudu.testtbl change column zip zip_code int " + "encoding rle compression lz4 default 90000", "Unsupported column options in " + "ALTER TABLE CHANGE COLUMN statement: zip_code INT ENCODING RLE COMPRESSION " + "LZ4 DEFAULT 90000");
    // Changing the column type is not supported for Kudu tables
    AnalysisError("alter table functional_kudu.testtbl change column zip zip bigint", "Cannot change the type of a Kudu column using an ALTER TABLE CHANGE COLUMN " + "statement: (INT vs BIGINT)");
    // Rename the underlying Kudu table
    AnalyzesOk("ALTER TABLE functional_kudu.testtbl SET " + "TBLPROPERTIES ('kudu.table_name' = 'Hans')");
    // ALTER TABLE RENAME TO
    AnalyzesOk("ALTER TABLE functional_kudu.testtbl RENAME TO new_testtbl");
}
#end_block

#method_before
@Test
public void TestDescribe() throws AnalysisException {
    AnalyzesOk("describe formatted functional.alltypes");
    AnalyzesOk("describe functional.alltypes");
    AnalysisError("describe formatted nodb.alltypes", "Could not resolve path: 'nodb.alltypes'");
    AnalysisError("describe functional.notbl", "Could not resolve path: 'functional.notbl'");
    // Complex typed fields.
    AnalyzesOk("describe functional_parquet.allcomplextypes.int_array_col");
    AnalyzesOk("describe functional_parquet.allcomplextypes.map_array_col");
    AnalyzesOk("describe functional_parquet.allcomplextypes.map_map_col");
    AnalyzesOk("describe functional_parquet.allcomplextypes.map_map_col.value");
    AnalyzesOk("describe functional_parquet.allcomplextypes.complex_struct_col");
    AnalyzesOk("describe functional_parquet.allcomplextypes.complex_struct_col.f3");
    AnalysisError("describe formatted functional_parquet.allcomplextypes.int_array_col", "DESCRIBE FORMATTED|EXTENDED must refer to a table");
    AnalysisError("describe functional_parquet.allcomplextypes.id", "Cannot describe path 'functional_parquet.allcomplextypes.id' targeting " + "scalar type: INT");
    AnalysisError("describe functional_parquet.allcomplextypes.nonexistent", "Could not resolve path: 'functional_parquet.allcomplextypes.nonexistent'");
    // Handling of ambiguous paths.
    addTestDb("ambig", null);
    addTestTable("create table ambig.ambig (ambig struct<ambig:array<int>>)");
    // Single element path can only be resolved as <table>.
    DescribeTableStmt describe = (DescribeTableStmt) AnalyzesOk("describe ambig", createAnalyzer("ambig"));
    Assert.assertEquals("ambig", describe.toThrift().db);
    Assert.assertEquals("ambig", describe.toThrift().table_name, "ambig");
    StructType colStructType = new StructType(Lists.newArrayList(new StructField("ambig", new ArrayType(Type.INT))));
    StructType tableStructType = new StructType(Lists.newArrayList(new StructField("ambig", colStructType)));
    Assert.assertEquals(tableStructType.toSql(), Type.fromThrift(describe.toThrift().result_struct).toSql());
    // Path could be resolved as either <db>.<table> or <table>.<complex field>
    AnalysisError("describe ambig.ambig", createAnalyzer("ambig"), "Path is ambiguous: 'ambig.ambig'");
    // Path could be resolved as either <db>.<table>.<field> or <table>.<field>.<field>
    AnalysisError("describe ambig.ambig.ambig", createAnalyzer("ambig"), "Path is ambiguous: 'ambig.ambig.ambig'");
    // 4 element path can only be resolved to nested array.
    AnalyzesOk("describe ambig.ambig.ambig.ambig", createAnalyzer("ambig"));
}
#method_after
@Test
public void TestDescribe() throws AnalysisException {
    AnalyzesOk("describe formatted functional.alltypes");
    AnalyzesOk("describe functional.alltypes");
    AnalysisError("describe formatted nodb.alltypes", "Could not resolve path: 'nodb.alltypes'");
    AnalysisError("describe functional.notbl", "Could not resolve path: 'functional.notbl'");
    // Complex typed fields.
    AnalyzesOk("describe functional_parquet.allcomplextypes.int_array_col");
    AnalyzesOk("describe functional_parquet.allcomplextypes.map_array_col");
    AnalyzesOk("describe functional_parquet.allcomplextypes.map_map_col");
    AnalyzesOk("describe functional_parquet.allcomplextypes.map_map_col.value");
    AnalyzesOk("describe functional_parquet.allcomplextypes.complex_struct_col");
    AnalyzesOk("describe functional_parquet.allcomplextypes.complex_struct_col.f3");
    AnalysisError("describe formatted functional_parquet.allcomplextypes.int_array_col", "DESCRIBE FORMATTED|EXTENDED must refer to a table");
    AnalysisError("describe functional_parquet.allcomplextypes.id", "Cannot describe path 'functional_parquet.allcomplextypes.id' targeting " + "scalar type: INT");
    AnalysisError("describe functional_parquet.allcomplextypes.nonexistent", "Could not resolve path: 'functional_parquet.allcomplextypes.nonexistent'");
    // Handling of ambiguous paths.
    addTestDb("ambig", null);
    addTestTable("create table ambig.ambig (ambig struct<ambig:array<int>>)");
    // Single element path can only be resolved as <table>.
    DescribeTableStmt describe = (DescribeTableStmt) AnalyzesOk("describe ambig", createAnalyzer("ambig"));
    TDescribeTableParams tdesc = (TDescribeTableParams) describe.toThrift();
    Assert.assertTrue(tdesc.isSetTable_name());
    Assert.assertEquals("ambig", tdesc.table_name.getDb_name());
    Assert.assertEquals("ambig", tdesc.table_name.getTable_name(), "ambig");
    Assert.assertFalse(tdesc.isSetResult_struct());
    // Path could be resolved as either <db>.<table> or <table>.<complex field>
    AnalysisError("describe ambig.ambig", createAnalyzer("ambig"), "Path is ambiguous: 'ambig.ambig'");
    // Path could be resolved as either <db>.<table>.<field> or <table>.<field>.<field>
    AnalysisError("describe ambig.ambig.ambig", createAnalyzer("ambig"), "Path is ambiguous: 'ambig.ambig.ambig'");
    // 4 element path can only be resolved to nested array.
    describe = (DescribeTableStmt) AnalyzesOk("describe ambig.ambig.ambig.ambig", createAnalyzer("ambig"));
    tdesc = (TDescribeTableParams) describe.toThrift();
    Type expectedType = org.apache.impala.analysis.Path.getTypeAsStruct(new ArrayType(Type.INT));
    Assert.assertTrue(tdesc.isSetResult_struct());
    Assert.assertEquals(expectedType, Type.fromThrift(tdesc.getResult_struct()));
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder sb = new StringBuilder("ALTER TABLE " + getTbl());
    sb.append(" ADD ");
    if (ifNotExists_) {
        sb.append("IF NOT EXISTS ");
    }
    if (partitionSpec_ != null)
        sb.append(" " + partitionSpec_.toSql());
    if (rangePartitionSpec_ != null)
        sb.append(" " + rangePartitionSpec_.toSql());
    if (location_ != null)
        sb.append(String.format(" LOCATION '%s'", location_));
    if (cacheOp_ != null)
        sb.append(cacheOp_.toSql());
    return sb.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder sb = new StringBuilder("ALTER TABLE " + getTbl());
    sb.append(" ADD ");
    if (ifNotExists_) {
        sb.append("IF NOT EXISTS ");
    }
    sb.append(" " + partitionSpec_.toSql());
    if (location_ != null)
        sb.append(String.format(" LOCATION '%s'", location_));
    if (cacheOp_ != null)
        sb.append(cacheOp_.toSql());
    return sb.toString();
}
#end_block

#method_before
@Override
public TAlterTableParams toThrift() {
    TAlterTableParams params = super.toThrift();
    params.setAlter_type(TAlterTableType.ADD_PARTITION);
    TAlterTableAddPartitionParams addPartParams = new TAlterTableAddPartitionParams();
    if (partitionSpec_ != null) {
        addPartParams.setPartition_spec(partitionSpec_.toThrift());
    }
    if (rangePartitionSpec_ != null) {
        addPartParams.setRange_partition_spec(rangePartitionSpec_.toThrift());
    }
    addPartParams.setLocation(location_ == null ? null : location_.toString());
    addPartParams.setIf_not_exists(ifNotExists_);
    if (cacheOp_ != null)
        addPartParams.setCache_op(cacheOp_.toThrift());
    params.setAdd_partition_params(addPartParams);
    return params;
}
#method_after
@Override
public TAlterTableParams toThrift() {
    TAlterTableParams params = super.toThrift();
    params.setAlter_type(TAlterTableType.ADD_PARTITION);
    TAlterTableAddPartitionParams addPartParams = new TAlterTableAddPartitionParams();
    addPartParams.setPartition_spec(partitionSpec_.toThrift());
    addPartParams.setLocation(location_ == null ? null : location_.toString());
    addPartParams.setIf_not_exists(ifNotExists_);
    if (cacheOp_ != null)
        addPartParams.setCache_op(cacheOp_.toThrift());
    params.setAdd_partition_params(addPartParams);
    return params;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    Preconditions.checkState(partitionSpec_ != null ^ rangePartitionSpec_ != null);
    Table table = getTargetTable();
    if (table instanceof KuduTable) {
        if (partitionSpec_ != null) {
            throw new AnalysisException("Key-value partition specs are not supported for " + "Kudu tables: " + partitionSpec_.toSql());
        }
        Preconditions.checkState(location_ == null);
        Preconditions.checkState(cacheOp_ == null);
        AlterTableStmt.analyzeAddDropRangePartition(rangePartitionSpec_, (KuduTable) table, analyzer);
        return;
    }
    if (rangePartitionSpec_ != null) {
        throw new AnalysisException(String.format("Table %s does not support range " + "partitions: RANGE %s", table.getFullName(), rangePartitionSpec_.toSql()));
    }
    if (!ifNotExists_)
        partitionSpec_.setPartitionShouldNotExist();
    partitionSpec_.setPrivilegeRequirement(Privilege.ALTER);
    partitionSpec_.analyze(analyzer);
    if (location_ != null) {
        location_.analyze(analyzer, Privilege.ALL, FsAction.READ_WRITE);
    }
    boolean shouldCache = false;
    if (cacheOp_ != null) {
        cacheOp_.analyze(analyzer);
        shouldCache = cacheOp_.shouldCache();
    } else if (table instanceof HdfsTable) {
        shouldCache = ((HdfsTable) table).isMarkedCached();
    }
    if (shouldCache) {
        if (!(table instanceof HdfsTable)) {
            throw new AnalysisException("Caching must target a HDFS table: " + table.getFullName());
        }
        HdfsTable hdfsTable = (HdfsTable) table;
        if ((location_ != null && !FileSystemUtil.isPathCacheable(location_.getPath())) || (location_ == null && !hdfsTable.isLocationCacheable())) {
            throw new AnalysisException(String.format("Location '%s' cannot be cached. " + "Please retry without caching: ALTER TABLE %s ADD PARTITION ... UNCACHED", (location_ != null) ? location_.toString() : hdfsTable.getLocation(), table.getFullName()));
        }
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    Table table = getTargetTable();
    if (table instanceof KuduTable) {
        throw new AnalysisException("ALTER TABLE ADD PARTITION is not supported for " + "Kudu tables: " + partitionSpec_.toSql());
    }
    if (!ifNotExists_)
        partitionSpec_.setPartitionShouldNotExist();
    partitionSpec_.setPrivilegeRequirement(Privilege.ALTER);
    partitionSpec_.analyze(analyzer);
    if (location_ != null) {
        location_.analyze(analyzer, Privilege.ALL, FsAction.READ_WRITE);
    }
    boolean shouldCache = false;
    if (cacheOp_ != null) {
        cacheOp_.analyze(analyzer);
        shouldCache = cacheOp_.shouldCache();
    } else if (table instanceof HdfsTable) {
        shouldCache = ((HdfsTable) table).isMarkedCached();
    }
    if (shouldCache) {
        if (!(table instanceof HdfsTable)) {
            throw new AnalysisException("Caching must target a HDFS table: " + table.getFullName());
        }
        HdfsTable hdfsTable = (HdfsTable) table;
        if ((location_ != null && !FileSystemUtil.isPathCacheable(location_.getPath())) || (location_ == null && !hdfsTable.isLocationCacheable())) {
            throw new AnalysisException(String.format("Location '%s' cannot be cached. " + "Please retry without caching: ALTER TABLE %s ADD PARTITION ... UNCACHED", (location_ != null) ? location_.toString() : hdfsTable.getLocation(), table.getFullName()));
        }
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    Table t = getTargetTable();
    // mappings along with the table columns.
    if (t instanceof HBaseTable) {
        throw new AnalysisException("ALTER TABLE ADD|REPLACE COLUMNS not currently " + "supported on HBase tables.");
    }
    boolean isKuduTable = t instanceof KuduTable;
    if (isKuduTable && replaceExistingCols_) {
        throw new AnalysisException("ALTER TABLE REPLACE COLUMNS not currently " + "supported on Kudu tables.");
    }
    // Build a set of the partition keys for the table.
    Set<String> existingPartitionKeys = Sets.newHashSet();
    for (FieldSchema fs : t.getMetaStoreTable().getPartitionKeys()) {
        existingPartitionKeys.add(fs.getName().toLowerCase());
    }
    // Make sure the new columns don't already exist in the table, that the names
    // are all valid and unique, and that none of the columns conflict with
    // partition columns.
    Set<String> colNames = Sets.newHashSet();
    for (ColumnDef c : columnDefs_) {
        c.analyze(analyzer);
        String colName = c.getColName().toLowerCase();
        if (existingPartitionKeys.contains(colName)) {
            throw new AnalysisException("Column name conflicts with existing partition column: " + colName);
        }
        Column col = t.getColumn(colName);
        if (col != null && !replaceExistingCols_) {
            throw new AnalysisException("Column already exists: " + colName);
        } else if (!colNames.add(colName)) {
            throw new AnalysisException("Duplicate column name: " + colName);
        }
        if (isKuduTable) {
            if (c.getType().isComplexType()) {
                throw new AnalysisException("Kudu tables do not support complex types: " + c.toString());
            }
            if (c.isPrimaryKey()) {
                throw new AnalysisException("Cannot add a primary key using an ALTER TABLE " + "ADD COLUMNS statement: " + c.toString());
            }
            if (c.hasEncoding() || c.hasCompression() || c.hasBlockSize()) {
                // block size on a newly added column (see KUDU-1746).
                throw new AnalysisException("ENCODING, COMPRESSION and " + "BLOCK_SIZE options cannot be specified in an ALTER TABLE ADD COLUMNS " + "statement: " + c.toString());
            }
            if (!(c.isNullable() ^ c.hasDefaultValue())) {
                // no default value or non-nullable with default value (see KUDU-1747).
                if (c.isNullable()) {
                    throw new AnalysisException("Only non-nullable columns can have a default " + "value: " + c.toString());
                } else {
                    throw new AnalysisException("New non-nullable columns must have a default " + "value: " + c.toString());
                }
            }
        } else if (c.hasKuduOptions()) {
            throw new AnalysisException("Unsupported column options: " + c.toString());
        }
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    Table t = getTargetTable();
    // mappings along with the table columns.
    if (t instanceof HBaseTable) {
        throw new AnalysisException("ALTER TABLE ADD|REPLACE COLUMNS not currently " + "supported on HBase tables.");
    }
    boolean isKuduTable = t instanceof KuduTable;
    if (isKuduTable && replaceExistingCols_) {
        throw new AnalysisException("ALTER TABLE REPLACE COLUMNS is not " + "supported on Kudu tables.");
    }
    // Build a set of the partition keys for the table.
    Set<String> existingPartitionKeys = Sets.newHashSet();
    for (FieldSchema fs : t.getMetaStoreTable().getPartitionKeys()) {
        existingPartitionKeys.add(fs.getName().toLowerCase());
    }
    // Make sure the new columns don't already exist in the table, that the names
    // are all valid and unique, and that none of the columns conflict with
    // partition columns.
    Set<String> colNames = Sets.newHashSet();
    for (ColumnDef c : columnDefs_) {
        c.analyze(analyzer);
        String colName = c.getColName().toLowerCase();
        if (existingPartitionKeys.contains(colName)) {
            throw new AnalysisException("Column name conflicts with existing partition column: " + colName);
        }
        Column col = t.getColumn(colName);
        if (col != null && !replaceExistingCols_) {
            throw new AnalysisException("Column already exists: " + colName);
        } else if (!colNames.add(colName)) {
            throw new AnalysisException("Duplicate column name: " + colName);
        }
        if (isKuduTable) {
            if (c.getType().isComplexType()) {
                throw new AnalysisException("Kudu tables do not support complex types: " + c.toString());
            }
            if (c.isPrimaryKey()) {
                throw new AnalysisException("Cannot add a primary key using an ALTER TABLE " + "ADD COLUMNS statement: " + c.toString());
            }
            if (c.hasEncoding() || c.hasCompression() || c.hasBlockSize()) {
                // block size on a newly added column (see KUDU-1746).
                throw new AnalysisException("ENCODING, COMPRESSION and " + "BLOCK_SIZE options cannot be specified in an ALTER TABLE ADD COLUMNS " + "statement: " + c.toString());
            }
        } else if (c.hasKuduOptions()) {
            throw new AnalysisException("The specified column options are only supported " + "in Kudu tables: " + c.toString());
        }
    }
}
#end_block

#method_before
private void alterTable(TAlterTableParams params, TDdlExecResponse response) throws ImpalaException {
    // When true, loads the file/block metadata.
    boolean reloadFileMetadata = false;
    // When true, loads the table schema and the column stats from the Hive Metastore.
    boolean reloadTableSchema = false;
    // When true, sets the result to be reported to the client.
    boolean setResultSet = false;
    TColumnValue resultColVal = new TColumnValue();
    Reference<Long> numUpdatedPartitions = new Reference<>(0L);
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Table tbl = getExistingTable(tableName.getDb(), tableName.getTbl());
    catalog_.getLock().writeLock().lock();
    synchronized (tbl) {
        if (params.getAlter_type() == TAlterTableType.RENAME_VIEW || params.getAlter_type() == TAlterTableType.RENAME_TABLE) {
            // the catalog lock.
            try {
                alterTableOrViewRename(tbl, TableName.fromThrift(params.getRename_params().getNew_table_name()), response);
                return;
            } finally {
                catalog_.getLock().writeLock().unlock();
            }
        }
        // Get a new catalog version to assign to the table being altered.
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        boolean reloadMetadata = true;
        catalog_.getLock().writeLock().unlock();
        switch(params.getAlter_type()) {
            case ADD_REPLACE_COLUMNS:
                TAlterTableAddReplaceColsParams addReplaceColParams = params.getAdd_replace_cols_params();
                if (tbl instanceof KuduTable) {
                    KuduCatalogOpExecutor.addColumn((KuduTable) tbl, addReplaceColParams.getColumns());
                    break;
                }
                alterTableAddReplaceCols(tbl, addReplaceColParams.getColumns(), addReplaceColParams.isReplace_existing_cols());
                reloadTableSchema = true;
                break;
            case ADD_PARTITION:
                TAlterTableAddPartitionParams addPartParams = params.getAdd_partition_params();
                if (tbl instanceof KuduTable) {
                    Preconditions.checkState(addPartParams.isSetRange_partition_spec());
                    KuduCatalogOpExecutor.addRangePartition((KuduTable) tbl, addPartParams.getRange_partition_spec(), addPartParams.isIf_not_exists());
                    break;
                }
                Preconditions.checkState(addPartParams.isSetPartition_spec());
                // Create and add HdfsPartition object to the corresponding HdfsTable and load
                // its block metadata. Get the new table object with an updated catalog
                // version. If the partition already exists in Hive and "IfNotExists" is true,
                // then return without populating the response object.
                Table refreshedTable = alterTableAddPartition(tbl, addPartParams.getPartition_spec(), addPartParams.isIf_not_exists(), addPartParams.getLocation(), addPartParams.getCache_op());
                if (refreshedTable != null) {
                    refreshedTable.setCatalogVersion(newCatalogVersion);
                    addTableToCatalogUpdate(refreshedTable, response.result);
                }
                reloadMetadata = false;
                break;
            case DROP_COLUMN:
                TAlterTableDropColParams dropColParams = params.getDrop_col_params();
                if (tbl instanceof KuduTable) {
                    KuduCatalogOpExecutor.dropColumn((KuduTable) tbl, dropColParams.getCol_name());
                    break;
                }
                alterTableDropCol(tbl, dropColParams.getCol_name());
                reloadTableSchema = true;
                break;
            case CHANGE_COLUMN:
                TAlterTableChangeColParams changeColParams = params.getChange_col_params();
                if (tbl instanceof KuduTable) {
                    KuduCatalogOpExecutor.renameColumn((KuduTable) tbl, changeColParams.getCol_name(), changeColParams.getNew_col_def());
                    break;
                }
                alterTableChangeCol(tbl, changeColParams.getCol_name(), changeColParams.getNew_col_def());
                reloadTableSchema = true;
                break;
            case DROP_PARTITION:
                TAlterTableDropPartitionParams dropPartParams = params.getDrop_partition_params();
                if (tbl instanceof KuduTable) {
                    Preconditions.checkState(dropPartParams.isSetRange_partition_spec());
                    KuduCatalogOpExecutor.dropRangePartition((KuduTable) tbl, dropPartParams.getRange_partition_spec(), dropPartParams.isIf_exists());
                    break;
                }
                // Drop the partition from the corresponding table. Get the table object
                // with an updated catalog version. If the partition does not exist and
                // "IfExists" is true, null is returned. If "purge" option is specified
                // partition data is purged by skipping Trash, if configured.
                refreshedTable = alterTableDropPartition(tbl, dropPartParams.getPartition_set(), dropPartParams.isIf_exists(), dropPartParams.isPurge(), numUpdatedPartitions);
                if (refreshedTable != null) {
                    refreshedTable.setCatalogVersion(newCatalogVersion);
                    addTableToCatalogUpdate(refreshedTable, response.result);
                }
                resultColVal.setString_val("Dropped " + numUpdatedPartitions.getRef() + " partition(s).");
                setResultSet = true;
                reloadMetadata = false;
                break;
            case RENAME_TABLE:
            case RENAME_VIEW:
                Preconditions.checkState(false, "RENAME TABLE/VIEW operation has been processed");
                break;
            case SET_FILE_FORMAT:
                TAlterTableSetFileFormatParams fileFormatParams = params.getSet_file_format_params();
                reloadFileMetadata = alterTableSetFileFormat(tbl, fileFormatParams.getPartition_set(), fileFormatParams.getFile_format(), numUpdatedPartitions);
                if (fileFormatParams.isSetPartition_set()) {
                    resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s).");
                } else {
                    resultColVal.setString_val("Updated table.");
                }
                setResultSet = true;
                break;
            case SET_LOCATION:
                TAlterTableSetLocationParams setLocationParams = params.getSet_location_params();
                reloadFileMetadata = alterTableSetLocation(tbl, setLocationParams.getPartition_spec(), setLocationParams.getLocation());
                break;
            case SET_TBL_PROPERTIES:
                alterTableSetTblProperties(tbl, params.getSet_tbl_properties_params(), numUpdatedPartitions);
                if (params.getSet_tbl_properties_params().isSetPartition_set()) {
                    resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s).");
                } else {
                    resultColVal.setString_val("Updated table.");
                }
                setResultSet = true;
                break;
            case UPDATE_STATS:
                Preconditions.checkState(params.isSetUpdate_stats_params());
                Reference<Long> numUpdatedColumns = new Reference<>(0L);
                alterTableUpdateStats(tbl, params.getUpdate_stats_params(), response, numUpdatedPartitions, numUpdatedColumns);
                reloadTableSchema = true;
                resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s) and " + numUpdatedColumns.getRef() + " column(s).");
                setResultSet = true;
                break;
            case SET_CACHED:
                Preconditions.checkState(params.isSetSet_cached_params());
                String op = params.getSet_cached_params().getCache_op().isSet_cached() ? "Cached " : "Uncached ";
                if (params.getSet_cached_params().getPartition_set() == null) {
                    reloadFileMetadata = alterTableSetCached(tbl, params.getSet_cached_params());
                    resultColVal.setString_val(op + "table.");
                } else {
                    alterPartitionSetCached(tbl, params.getSet_cached_params(), numUpdatedPartitions);
                    resultColVal.setString_val(op + numUpdatedPartitions.getRef() + " partition(s).");
                }
                setResultSet = true;
                break;
            case RECOVER_PARTITIONS:
                alterTableRecoverPartitions(tbl);
                break;
            default:
                throw new UnsupportedOperationException("Unknown ALTER TABLE operation type: " + params.getAlter_type());
        }
        if (reloadMetadata) {
            loadTableMetadata(tbl, newCatalogVersion, reloadFileMetadata, reloadTableSchema, null);
            addTableToCatalogUpdate(tbl, response.result);
        }
        if (setResultSet) {
            TResultSet resultSet = new TResultSet();
            resultSet.setSchema(new TResultSetMetadata(Lists.newArrayList(new TColumn("summary", Type.STRING.toThrift()))));
            TResultRow resultRow = new TResultRow();
            resultRow.setColVals(Lists.newArrayList(resultColVal));
            resultSet.setRows(Lists.newArrayList(resultRow));
            response.setResult_set(resultSet);
        }
    }
// end of synchronized block
}
#method_after
private void alterTable(TAlterTableParams params, TDdlExecResponse response) throws ImpalaException {
    // When true, loads the file/block metadata.
    boolean reloadFileMetadata = false;
    // When true, loads the table schema and the column stats from the Hive Metastore.
    boolean reloadTableSchema = false;
    // When true, sets the result to be reported to the client.
    boolean setResultSet = false;
    TColumnValue resultColVal = new TColumnValue();
    Reference<Long> numUpdatedPartitions = new Reference<>(0L);
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Table tbl = getExistingTable(tableName.getDb(), tableName.getTbl());
    catalog_.getLock().writeLock().lock();
    synchronized (tbl) {
        if (params.getAlter_type() == TAlterTableType.RENAME_VIEW || params.getAlter_type() == TAlterTableType.RENAME_TABLE) {
            // the catalog lock.
            try {
                alterTableOrViewRename(tbl, TableName.fromThrift(params.getRename_params().getNew_table_name()), response);
                return;
            } finally {
                catalog_.getLock().writeLock().unlock();
            }
        }
        // Get a new catalog version to assign to the table being altered.
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        boolean reloadMetadata = true;
        catalog_.getLock().writeLock().unlock();
        if (tbl instanceof KuduTable && altersKuduTable(params.getAlter_type())) {
            alterKuduTable(params, response, (KuduTable) tbl, newCatalogVersion);
            return;
        }
        switch(params.getAlter_type()) {
            case ADD_REPLACE_COLUMNS:
                TAlterTableAddReplaceColsParams addReplaceColParams = params.getAdd_replace_cols_params();
                alterTableAddReplaceCols(tbl, addReplaceColParams.getColumns(), addReplaceColParams.isReplace_existing_cols());
                reloadTableSchema = true;
                break;
            case ADD_PARTITION:
                TAlterTableAddPartitionParams addPartParams = params.getAdd_partition_params();
                // Create and add HdfsPartition object to the corresponding HdfsTable and load
                // its block metadata. Get the new table object with an updated catalog
                // version. If the partition already exists in Hive and "IfNotExists" is true,
                // then return without populating the response object.
                Table refreshedTable = alterTableAddPartition(tbl, addPartParams.getPartition_spec(), addPartParams.isIf_not_exists(), addPartParams.getLocation(), addPartParams.getCache_op());
                if (refreshedTable != null) {
                    refreshedTable.setCatalogVersion(newCatalogVersion);
                    addTableToCatalogUpdate(refreshedTable, response.result);
                }
                reloadMetadata = false;
                break;
            case DROP_COLUMN:
                TAlterTableDropColParams dropColParams = params.getDrop_col_params();
                alterTableDropCol(tbl, dropColParams.getCol_name());
                reloadTableSchema = true;
                break;
            case CHANGE_COLUMN:
                TAlterTableChangeColParams changeColParams = params.getChange_col_params();
                alterTableChangeCol(tbl, changeColParams.getCol_name(), changeColParams.getNew_col_def());
                reloadTableSchema = true;
                break;
            case DROP_PARTITION:
                TAlterTableDropPartitionParams dropPartParams = params.getDrop_partition_params();
                // Drop the partition from the corresponding table. Get the table object
                // with an updated catalog version. If the partition does not exist and
                // "IfExists" is true, null is returned. If "purge" option is specified
                // partition data is purged by skipping Trash, if configured.
                refreshedTable = alterTableDropPartition(tbl, dropPartParams.getPartition_set(), dropPartParams.isIf_exists(), dropPartParams.isPurge(), numUpdatedPartitions);
                if (refreshedTable != null) {
                    refreshedTable.setCatalogVersion(newCatalogVersion);
                    addTableToCatalogUpdate(refreshedTable, response.result);
                }
                resultColVal.setString_val("Dropped " + numUpdatedPartitions.getRef() + " partition(s).");
                setResultSet = true;
                reloadMetadata = false;
                break;
            case RENAME_TABLE:
            case RENAME_VIEW:
                Preconditions.checkState(false, "RENAME TABLE/VIEW operation has been processed");
                break;
            case SET_FILE_FORMAT:
                TAlterTableSetFileFormatParams fileFormatParams = params.getSet_file_format_params();
                reloadFileMetadata = alterTableSetFileFormat(tbl, fileFormatParams.getPartition_set(), fileFormatParams.getFile_format(), numUpdatedPartitions);
                if (fileFormatParams.isSetPartition_set()) {
                    resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s).");
                } else {
                    resultColVal.setString_val("Updated table.");
                }
                setResultSet = true;
                break;
            case SET_LOCATION:
                TAlterTableSetLocationParams setLocationParams = params.getSet_location_params();
                reloadFileMetadata = alterTableSetLocation(tbl, setLocationParams.getPartition_spec(), setLocationParams.getLocation());
                break;
            case SET_TBL_PROPERTIES:
                alterTableSetTblProperties(tbl, params.getSet_tbl_properties_params(), numUpdatedPartitions);
                if (params.getSet_tbl_properties_params().isSetPartition_set()) {
                    resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s).");
                } else {
                    resultColVal.setString_val("Updated table.");
                }
                setResultSet = true;
                break;
            case UPDATE_STATS:
                Preconditions.checkState(params.isSetUpdate_stats_params());
                Reference<Long> numUpdatedColumns = new Reference<>(0L);
                alterTableUpdateStats(tbl, params.getUpdate_stats_params(), response, numUpdatedPartitions, numUpdatedColumns);
                reloadTableSchema = true;
                resultColVal.setString_val("Updated " + numUpdatedPartitions.getRef() + " partition(s) and " + numUpdatedColumns.getRef() + " column(s).");
                setResultSet = true;
                break;
            case SET_CACHED:
                Preconditions.checkState(params.isSetSet_cached_params());
                String op = params.getSet_cached_params().getCache_op().isSet_cached() ? "Cached " : "Uncached ";
                if (params.getSet_cached_params().getPartition_set() == null) {
                    reloadFileMetadata = alterTableSetCached(tbl, params.getSet_cached_params());
                    resultColVal.setString_val(op + "table.");
                } else {
                    alterPartitionSetCached(tbl, params.getSet_cached_params(), numUpdatedPartitions);
                    resultColVal.setString_val(op + numUpdatedPartitions.getRef() + " partition(s).");
                }
                setResultSet = true;
                break;
            case RECOVER_PARTITIONS:
                alterTableRecoverPartitions(tbl);
                break;
            default:
                throw new UnsupportedOperationException("Unknown ALTER TABLE operation type: " + params.getAlter_type());
        }
        if (reloadMetadata) {
            loadTableMetadata(tbl, newCatalogVersion, reloadFileMetadata, reloadTableSchema, null);
            addTableToCatalogUpdate(tbl, response.result);
        }
        if (setResultSet) {
            TResultSet resultSet = new TResultSet();
            resultSet.setSchema(new TResultSetMetadata(Lists.newArrayList(new TColumn("summary", Type.STRING.toThrift()))));
            TResultRow resultRow = new TResultRow();
            resultRow.setColVals(Lists.newArrayList(resultColVal));
            resultSet.setRows(Lists.newArrayList(resultRow));
            response.setResult_set(resultSet);
        }
    }
// end of synchronized block
}
#end_block

#method_before
private void alterTableSetTblProperties(Table tbl, TAlterTableSetTblPropertiesParams params, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    Map<String, String> properties = params.getProperties();
    Preconditions.checkNotNull(properties);
    if (params.isSetPartition_set()) {
        Preconditions.checkArgument(tbl instanceof HdfsTable);
        List<HdfsPartition> partitions = ((HdfsTable) tbl).getPartitionsFromPartitionSet(params.getPartition_set());
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        for (HdfsPartition partition : partitions) {
            switch(params.getTarget()) {
                case TBL_PROPERTY:
                    partition.getParameters().putAll(properties);
                    break;
                case SERDE_PROPERTY:
                    partition.getSerdeInfo().getParameters().putAll(properties);
                    break;
                default:
                    throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
            }
            modifiedParts.add(partition);
        }
        TableName tableName = tbl.getTableName();
        try {
            bulkAlterPartitions(tableName.getDb(), tableName.getTbl(), modifiedParts);
        } finally {
            for (HdfsPartition modifiedPart : modifiedParts) {
                modifiedPart.markDirty();
            }
        }
        numUpdatedPartitions.setRef((long) modifiedParts.size());
    } else {
        // Alter table params.
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        switch(params.getTarget()) {
            case TBL_PROPERTY:
                if (KuduTable.isKuduTable(msTbl)) {
                    if (properties.containsKey(KuduTable.KEY_TABLE_NAME) && !properties.get(KuduTable.KEY_TABLE_NAME).equals(msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME))) {
                        // Rename the underlying Kudu table
                        KuduCatalogOpExecutor.renameTable((KuduTable) tbl, properties.get(KuduTable.KEY_TABLE_NAME));
                    }
                    msTbl.getParameters().putAll(properties);
                    // Validate that the new table properties are valid and that
                    // the Kudu table is accessible.
                    KuduCatalogOpExecutor.validateKuduTblExists(msTbl);
                } else {
                    msTbl.getParameters().putAll(properties);
                }
                break;
            case SERDE_PROPERTY:
                msTbl.getSd().getSerdeInfo().getParameters().putAll(properties);
                break;
            default:
                throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
        }
        applyAlterTable(msTbl);
    }
}
#method_after
private void alterTableSetTblProperties(Table tbl, TAlterTableSetTblPropertiesParams params, Reference<Long> numUpdatedPartitions) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    Map<String, String> properties = params.getProperties();
    Preconditions.checkNotNull(properties);
    if (params.isSetPartition_set()) {
        Preconditions.checkArgument(tbl instanceof HdfsTable);
        List<HdfsPartition> partitions = ((HdfsTable) tbl).getPartitionsFromPartitionSet(params.getPartition_set());
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        for (HdfsPartition partition : partitions) {
            switch(params.getTarget()) {
                case TBL_PROPERTY:
                    partition.getParameters().putAll(properties);
                    break;
                case SERDE_PROPERTY:
                    partition.getSerdeInfo().getParameters().putAll(properties);
                    break;
                default:
                    throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
            }
            modifiedParts.add(partition);
        }
        TableName tableName = tbl.getTableName();
        try {
            bulkAlterPartitions(tableName.getDb(), tableName.getTbl(), modifiedParts);
        } finally {
            for (HdfsPartition modifiedPart : modifiedParts) {
                modifiedPart.markDirty();
            }
        }
        numUpdatedPartitions.setRef((long) modifiedParts.size());
    } else {
        // Alter table params.
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        switch(params.getTarget()) {
            case TBL_PROPERTY:
                if (KuduTable.isKuduTable(msTbl)) {
                    // the underlying Kudu table.
                    if (properties.containsKey(KuduTable.KEY_TABLE_NAME) && !properties.get(KuduTable.KEY_TABLE_NAME).equals(msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME)) && !Table.isExternalTable(msTbl)) {
                        KuduCatalogOpExecutor.renameTable((KuduTable) tbl, properties.get(KuduTable.KEY_TABLE_NAME));
                    }
                    msTbl.getParameters().putAll(properties);
                    // Validate that the new table properties are valid and that
                    // the Kudu table is accessible.
                    KuduCatalogOpExecutor.validateKuduTblExists(msTbl);
                } else {
                    msTbl.getParameters().putAll(properties);
                }
                break;
            case SERDE_PROPERTY:
                msTbl.getSd().getSerdeInfo().getParameters().putAll(properties);
                break;
            default:
                throw new UnsupportedOperationException("Unknown target TTablePropertyType: " + params.getTarget());
        }
        applyAlterTable(msTbl);
    }
}
#end_block

#method_before
public static void addColumn(KuduTable tbl, List<TColumn> columns) throws ImpalaRuntimeException {
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    for (TColumn column : columns) {
        Type type = Type.fromThrift(column.getColumnType());
        Preconditions.checkState(type != null);
        org.apache.kudu.Type kuduType = KuduUtil.fromImpalaType(type);
        boolean isNullable = column.isSetIs_nullable() && column.isIs_nullable();
        Preconditions.checkState(isNullable ^ column.isSetDefault_value());
        if (isNullable) {
            alterTableOptions.addNullableColumn(column.getColumnName(), kuduType);
        } else {
            alterTableOptions.addColumn(column.getColumnName(), kuduType, KuduUtil.getKuduDefaultValue(column.getDefault_value(), kuduType, column.getColumnName()));
        }
    }
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
    } catch (KuduException e) {
        throw new ImpalaRuntimeException("Error adding columns to Kudu table " + tbl.getKuduTableName(), e);
    }
}
#method_after
public static void addColumn(KuduTable tbl, List<TColumn> columns) throws ImpalaRuntimeException {
    AlterTableOptions alterTableOptions = new AlterTableOptions();
    for (TColumn column : columns) {
        Type type = Type.fromThrift(column.getColumnType());
        Preconditions.checkState(type != null);
        org.apache.kudu.Type kuduType = KuduUtil.fromImpalaType(type);
        boolean isNullable = column.isSetIs_nullable() && column.isIs_nullable();
        if (isNullable) {
            if (column.isSetDefault_value()) {
                // See KUDU-1747
                throw new ImpalaRuntimeException(String.format("Error adding nullable " + "column to Kudu table %s. Cannot specify a default value for a nullable " + "column", tbl.getKuduTableName()));
            }
            alterTableOptions.addNullableColumn(column.getColumnName(), kuduType);
        } else {
            Object defaultValue = null;
            if (column.isSetDefault_value()) {
                defaultValue = KuduUtil.getKuduDefaultValue(column.getDefault_value(), kuduType, column.getColumnName());
            }
            try {
                alterTableOptions.addColumn(column.getColumnName(), kuduType, defaultValue);
            } catch (IllegalArgumentException e) {
                // TODO: Remove this when KUDU-1747 is fixed
                throw new ImpalaRuntimeException("Error adding non-nullable column to " + "Kudu table " + tbl.getKuduTableName(), e);
            }
        }
    }
    try (KuduClient client = KuduUtil.createKuduClient(tbl.getKuduMasterHosts())) {
        client.alterTable(tbl.getKuduTableName(), alterTableOptions);
    } catch (KuduException e) {
        throw new ImpalaRuntimeException("Error adding columns to Kudu table " + tbl.getKuduTableName(), e);
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Resolve and analyze this table ref so we can evaluate partition predicates.
    TableRef tableRef = new TableRef(tableName_.toPath(), null, Privilege.ALTER);
    tableRef = analyzer.resolveTableRef(tableRef);
    Preconditions.checkNotNull(tableRef);
    tableRef.analyze(analyzer);
    if (tableRef instanceof InlineViewRef) {
        throw new AnalysisException(String.format("ALTER TABLE not allowed on a view: %s", tableName_));
    }
    if (tableRef instanceof CollectionTableRef) {
        throw new AnalysisException(String.format("ALTER TABLE not allowed on a nested collection: %s", tableName_));
    }
    Preconditions.checkState(tableRef instanceof BaseTableRef);
    table_ = tableRef.getTable();
    if (table_ instanceof KuduTable && !(this instanceof AlterTableSetTblProperties) && !(this instanceof AlterTableSetColumnStats) && !(this instanceof AlterTableOrViewRenameStmt) && !(this instanceof AlterTableAddPartitionStmt) && !(this instanceof AlterTableDropPartitionStmt) && !(this instanceof AlterTableAddReplaceColsStmt) && !(this instanceof AlterTableDropColStmt) && !(this instanceof AlterTableChangeColStmt) && !(this instanceof AlterTableOrViewRenameStmt)) {
        throw new AnalysisException(String.format("ALTER TABLE not allowed on Kudu table: %s", tableName_));
    }
    if (table_ instanceof DataSourceTable && !(this instanceof AlterTableSetColumnStats)) {
        throw new AnalysisException(String.format("ALTER TABLE not allowed on a table produced by a data source: %s", tableName_));
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Resolve and analyze this table ref so we can evaluate partition predicates.
    TableRef tableRef = new TableRef(tableName_.toPath(), null, Privilege.ALTER);
    tableRef = analyzer.resolveTableRef(tableRef);
    Preconditions.checkNotNull(tableRef);
    tableRef.analyze(analyzer);
    if (tableRef instanceof InlineViewRef) {
        throw new AnalysisException(String.format("ALTER TABLE not allowed on a view: %s", tableName_));
    }
    if (tableRef instanceof CollectionTableRef) {
        throw new AnalysisException(String.format("ALTER TABLE not allowed on a nested collection: %s", tableName_));
    }
    Preconditions.checkState(tableRef instanceof BaseTableRef);
    table_ = tableRef.getTable();
    if (table_ instanceof DataSourceTable && !(this instanceof AlterTableSetColumnStats)) {
        throw new AnalysisException(String.format("ALTER TABLE not allowed on a table produced by a data source: %s", tableName_));
    }
}
#end_block

#method_before
@Test
public void AnalyzeGrantRevokePriv() throws AnalysisException {
    boolean[] isGrantVals = { true, false };
    for (boolean isGrant : isGrantVals) {
        Object[] formatArgs = new String[] { "REVOKE", "FROM" };
        if (isGrant)
            formatArgs = new String[] { "GRANT", "TO" };
        // ALL privileges
        AnalyzesOk(String.format("%s ALL ON TABLE alltypes %s myrole", formatArgs), createAnalyzer("functional"));
        AnalyzesOk(String.format("%s ALL ON TABLE functional.alltypes %s myrole", formatArgs));
        AnalyzesOk(String.format("%s ALL ON DATABASE functional %s myrole", formatArgs));
        AnalyzesOk(String.format("%s ALL ON SERVER %s myrole", formatArgs));
        AnalyzesOk(String.format("%s ALL ON SERVER server1 %s myrole", formatArgs));
        AnalyzesOk(String.format("%s ALL ON URI 'hdfs:////abc//123' %s myrole", formatArgs));
        AnalysisError(String.format("%s ALL ON URI 'xxxx:////abc//123' %s myrole", formatArgs), "No FileSystem for scheme: xxxx");
        AnalysisError(String.format("%s ALL ON DATABASE does_not_exist %s myrole", formatArgs), "Error setting privileges for database 'does_not_exist'. " + "Verify that the database exists and that you have permissions to issue " + "a GRANT/REVOKE statement.");
        AnalysisError(String.format("%s ALL ON TABLE does_not_exist %s myrole", formatArgs), "Error setting privileges for table 'does_not_exist'. " + "Verify that the table exists and that you have permissions to issue " + "a GRANT/REVOKE statement.");
        AnalysisError(String.format("%s ALL ON SERVER does_not_exist %s myrole", formatArgs), "Specified server name 'does_not_exist' does not match the " + "configured server name 'server1'");
        // INSERT privilege
        AnalyzesOk(String.format("%s INSERT ON TABLE alltypesagg %s myrole", formatArgs), createAnalyzer("functional"));
        AnalyzesOk(String.format("%s INSERT ON TABLE functional.alltypesagg %s myrole", formatArgs));
        AnalyzesOk(String.format("%s INSERT ON DATABASE functional %s myrole", formatArgs));
        AnalysisError(String.format("%s INSERT ON SERVER %s myrole", formatArgs), "Only 'ALL' privilege may be applied at SERVER scope in privilege spec.");
        AnalysisError(String.format("%s INSERT ON URI 'hdfs:////abc//123' %s myrole", formatArgs), "Only 'ALL' privilege may be applied at URI scope in privilege " + "spec.");
        // SELECT privilege
        AnalyzesOk(String.format("%s SELECT ON TABLE alltypessmall %s myrole", formatArgs), createAnalyzer("functional"));
        AnalyzesOk(String.format("%s SELECT ON TABLE functional.alltypessmall %s myrole", formatArgs));
        AnalyzesOk(String.format("%s SELECT ON DATABASE functional %s myrole", formatArgs));
        AnalysisError(String.format("%s SELECT ON SERVER %s myrole", formatArgs), "Only 'ALL' privilege may be applied at SERVER scope in privilege spec.");
        AnalysisError(String.format("%s SELECT ON URI 'hdfs:////abc//123' %s myrole", formatArgs), "Only 'ALL' privilege may be applied at URI scope in privilege " + "spec.");
        // SELECT privileges on columns
        AnalyzesOk(String.format("%s SELECT (id, int_col) ON TABLE functional.alltypes " + "%s myrole", formatArgs));
        AnalyzesOk(String.format("%s SELECT (id, id) ON TABLE functional.alltypes " + "%s myrole", formatArgs));
        // SELECT privilege on both regular and partition columns
        AnalyzesOk(String.format("%s SELECT (id, int_col, year, month) ON TABLE " + "alltypes %s myrole", formatArgs), createAnalyzer("functional"));
        // Empty column list
        AnalysisError(String.format("%s SELECT () ON TABLE functional.alltypes " + "%s myrole", formatArgs), "Empty column list in column privilege spec.");
        // INSERT/ALL privileges on columns
        AnalysisError(String.format("%s INSERT (id, tinyint_col) ON TABLE " + "functional.alltypes %s myrole", formatArgs), "Only 'SELECT' privileges " + "are allowed in a column privilege spec.");
        AnalysisError(String.format("%s ALL (id, tinyint_col) ON TABLE " + "functional.alltypes %s myrole", formatArgs), "Only 'SELECT' privileges " + "are allowed in a column privilege spec.");
        // Column-level privileges on a VIEW
        AnalysisError(String.format("%s SELECT (id, bool_col) ON TABLE " + "functional.alltypes_hive_view %s myrole", formatArgs), "Column-level " + "privileges on views are not supported.");
        // Column-level privileges on a KUDU table
        AnalysisError(String.format("%s SELECT (id, bool_col) ON TABLE " + "functional_kudu.alltypessmall %s myrole", formatArgs), "Column-level " + "privileges on Kudu tables are not supported.");
        // Columns/table that don't exist
        AnalysisError(String.format("%s SELECT (invalid_col) ON TABLE " + "functional.alltypes %s myrole", formatArgs), "Error setting column-level " + "privileges for table 'functional.alltypes'. Verify that both table and " + "columns exist and that you have permissions to issue a GRANT/REVOKE " + "statement.");
        AnalysisError(String.format("%s SELECT (id, int_col) ON TABLE " + "functional.does_not_exist %s myrole", formatArgs), "Error setting " + "privileges for table 'functional.does_not_exist'. Verify that the table " + "exists and that you have permissions to issue a GRANT/REVOKE statement.");
    }
    Analyzer authDisabledAnalyzer = createAuthDisabledAnalyzer(Catalog.DEFAULT_DB);
    AnalysisError("GRANT ALL ON SERVER TO myRole", authDisabledAnalyzer, "Authorization is not enabled.");
    AnalysisError("REVOKE ALL ON SERVER FROM myRole", authDisabledAnalyzer, "Authorization is not enabled.");
    TQueryCtx queryCtxNoUsername = TestUtils.createQueryContext("default", "");
    Analyzer noUsernameAnalyzer = new Analyzer(catalog_, queryCtxNoUsername, AuthorizationConfig.createHadoopGroupAuthConfig("server1", null, null));
    AnalysisError("GRANT ALL ON SERVER TO myRole", noUsernameAnalyzer, "Cannot execute authorization statement with an empty username.");
}
#method_after
@Test
public void AnalyzeGrantRevokePriv() throws AnalysisException {
    boolean[] isGrantVals = { true, false };
    for (boolean isGrant : isGrantVals) {
        Object[] formatArgs = new String[] { "REVOKE", "FROM" };
        if (isGrant)
            formatArgs = new String[] { "GRANT", "TO" };
        // ALL privileges
        AnalyzesOk(String.format("%s ALL ON TABLE alltypes %s myrole", formatArgs), createAnalyzer("functional"));
        AnalyzesOk(String.format("%s ALL ON TABLE functional.alltypes %s myrole", formatArgs));
        AnalyzesOk(String.format("%s ALL ON DATABASE functional %s myrole", formatArgs));
        AnalyzesOk(String.format("%s ALL ON SERVER %s myrole", formatArgs));
        AnalyzesOk(String.format("%s ALL ON SERVER server1 %s myrole", formatArgs));
        AnalyzesOk(String.format("%s ALL ON URI 'hdfs:////abc//123' %s myrole", formatArgs));
        AnalysisError(String.format("%s ALL ON URI 'xxxx:////abc//123' %s myrole", formatArgs), "No FileSystem for scheme: xxxx");
        AnalysisError(String.format("%s ALL ON DATABASE does_not_exist %s myrole", formatArgs), "Error setting privileges for database 'does_not_exist'. " + "Verify that the database exists and that you have permissions to issue " + "a GRANT/REVOKE statement.");
        AnalysisError(String.format("%s ALL ON TABLE does_not_exist %s myrole", formatArgs), "Error setting privileges for table 'does_not_exist'. " + "Verify that the table exists and that you have permissions to issue " + "a GRANT/REVOKE statement.");
        AnalysisError(String.format("%s ALL ON SERVER does_not_exist %s myrole", formatArgs), "Specified server name 'does_not_exist' does not match the " + "configured server name 'server1'");
        // INSERT privilege
        AnalyzesOk(String.format("%s INSERT ON TABLE alltypesagg %s myrole", formatArgs), createAnalyzer("functional"));
        AnalyzesOk(String.format("%s INSERT ON TABLE functional.alltypesagg %s myrole", formatArgs));
        AnalyzesOk(String.format("%s INSERT ON DATABASE functional %s myrole", formatArgs));
        AnalysisError(String.format("%s INSERT ON SERVER %s myrole", formatArgs), "Only 'ALL' privilege may be applied at SERVER scope in privilege spec.");
        AnalysisError(String.format("%s INSERT ON URI 'hdfs:////abc//123' %s myrole", formatArgs), "Only 'ALL' privilege may be applied at URI scope in privilege " + "spec.");
        // IMPALA-4000: Insert privilege on a Kudu table
        AnalysisError(String.format("%s SELECT ON TABLE functional_kudu.alltypessmall %s myrole", formatArgs), "Kudu tables only support the ALL privilege level.");
        AnalysisError(String.format("%s INSERT ON TABLE functional_kudu.alltypessmall %s myrole", formatArgs), "Kudu tables only support the ALL privilege level.");
        // SELECT privilege
        AnalyzesOk(String.format("%s SELECT ON TABLE alltypessmall %s myrole", formatArgs), createAnalyzer("functional"));
        AnalyzesOk(String.format("%s SELECT ON TABLE functional.alltypessmall %s myrole", formatArgs));
        AnalyzesOk(String.format("%s SELECT ON DATABASE functional %s myrole", formatArgs));
        AnalysisError(String.format("%s SELECT ON SERVER %s myrole", formatArgs), "Only 'ALL' privilege may be applied at SERVER scope in privilege spec.");
        AnalysisError(String.format("%s SELECT ON URI 'hdfs:////abc//123' %s myrole", formatArgs), "Only 'ALL' privilege may be applied at URI scope in privilege " + "spec.");
        // SELECT privileges on columns
        AnalyzesOk(String.format("%s SELECT (id, int_col) ON TABLE functional.alltypes " + "%s myrole", formatArgs));
        AnalyzesOk(String.format("%s SELECT (id, id) ON TABLE functional.alltypes " + "%s myrole", formatArgs));
        // SELECT privilege on both regular and partition columns
        AnalyzesOk(String.format("%s SELECT (id, int_col, year, month) ON TABLE " + "alltypes %s myrole", formatArgs), createAnalyzer("functional"));
        // Empty column list
        AnalysisError(String.format("%s SELECT () ON TABLE functional.alltypes " + "%s myrole", formatArgs), "Empty column list in column privilege spec.");
        // INSERT/ALL privileges on columns
        AnalysisError(String.format("%s INSERT (id, tinyint_col) ON TABLE " + "functional.alltypes %s myrole", formatArgs), "Only 'SELECT' privileges " + "are allowed in a column privilege spec.");
        AnalysisError(String.format("%s ALL (id, tinyint_col) ON TABLE " + "functional.alltypes %s myrole", formatArgs), "Only 'SELECT' privileges " + "are allowed in a column privilege spec.");
        // Column-level privileges on a VIEW
        AnalysisError(String.format("%s SELECT (id, bool_col) ON TABLE " + "functional.alltypes_hive_view %s myrole", formatArgs), "Column-level " + "privileges on views are not supported.");
        // IMPALA-4000: Column-level privileges on a KUDU table
        AnalysisError(String.format("%s SELECT (id, bool_col) ON TABLE " + "functional_kudu.alltypessmall %s myrole", formatArgs), "Kudu tables only support the ALL privilege level.");
        // Columns/table that don't exist
        AnalysisError(String.format("%s SELECT (invalid_col) ON TABLE " + "functional.alltypes %s myrole", formatArgs), "Error setting column-level " + "privileges for table 'functional.alltypes'. Verify that both table and " + "columns exist and that you have permissions to issue a GRANT/REVOKE " + "statement.");
        AnalysisError(String.format("%s SELECT (id, int_col) ON TABLE " + "functional.does_not_exist %s myrole", formatArgs), "Error setting " + "privileges for table 'functional.does_not_exist'. Verify that the table " + "exists and that you have permissions to issue a GRANT/REVOKE statement.");
    }
    Analyzer authDisabledAnalyzer = createAuthDisabledAnalyzer(Catalog.DEFAULT_DB);
    AnalysisError("GRANT ALL ON SERVER TO myRole", authDisabledAnalyzer, "Authorization is not enabled.");
    AnalysisError("REVOKE ALL ON SERVER FROM myRole", authDisabledAnalyzer, "Authorization is not enabled.");
    TQueryCtx queryCtxNoUsername = TestUtils.createQueryContext("default", "");
    Analyzer noUsernameAnalyzer = new Analyzer(catalog_, queryCtxNoUsername, AuthorizationConfig.createHadoopGroupAuthConfig("server1", null, null));
    AnalysisError("GRANT ALL ON SERVER TO myRole", noUsernameAnalyzer, "Cannot execute authorization statement with an empty username.");
}
#end_block

#method_before
private void analyzeColumnPrivScope(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(scope_ == TPrivilegeScope.COLUMN);
    Preconditions.checkNotNull(columnNames_);
    if (columnNames_.isEmpty()) {
        throw new AnalysisException("Empty column list in column privilege spec.");
    }
    if (privilegeLevel_ != TPrivilegeLevel.SELECT) {
        throw new AnalysisException("Only 'SELECT' privileges are allowed " + "in a column privilege spec.");
    }
    Table table = analyzeTargetTable(analyzer);
    if (table instanceof View) {
        throw new AnalysisException("Column-level privileges on views are not " + "supported.");
    }
    if (table instanceof DataSourceTable) {
        throw new AnalysisException("Column-level privileges on external data " + "source tables are not supported.");
    }
    if (table instanceof KuduTable) {
        throw new AnalysisException("Column-level privileges on Kudu " + "tables are not supported.");
    }
    for (String columnName : columnNames_) {
        if (table.getColumn(columnName) == null) {
            // The error message should not reveal the existence or absence of a column.
            throw new AnalysisException(String.format("Error setting column-level " + "privileges for table '%s'. Verify that both table and columns exist " + "and that you have permissions to issue a GRANT/REVOKE statement.", tableName_.toString()));
        }
    }
}
#method_after
private void analyzeColumnPrivScope(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(scope_ == TPrivilegeScope.COLUMN);
    Preconditions.checkNotNull(columnNames_);
    if (columnNames_.isEmpty()) {
        throw new AnalysisException("Empty column list in column privilege spec.");
    }
    if (privilegeLevel_ != TPrivilegeLevel.SELECT) {
        throw new AnalysisException("Only 'SELECT' privileges are allowed " + "in a column privilege spec.");
    }
    Table table = analyzeTargetTable(analyzer);
    if (table instanceof View) {
        throw new AnalysisException("Column-level privileges on views are not " + "supported.");
    }
    if (table instanceof DataSourceTable) {
        throw new AnalysisException("Column-level privileges on external data " + "source tables are not supported.");
    }
    for (String columnName : columnNames_) {
        if (table.getColumn(columnName) == null) {
            // The error message should not reveal the existence or absence of a column.
            throw new AnalysisException(String.format("Error setting column-level " + "privileges for table '%s'. Verify that both table and columns exist " + "and that you have permissions to issue a GRANT/REVOKE statement.", tableName_.toString()));
        }
    }
}
#end_block

#method_before
private Table analyzeTargetTable(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(scope_ == TPrivilegeScope.TABLE || scope_ == TPrivilegeScope.COLUMN);
    Preconditions.checkState(!Strings.isNullOrEmpty(tableName_.getTbl()));
    Table table = null;
    try {
        dbName_ = analyzer.getTargetDbName(tableName_);
        Preconditions.checkNotNull(dbName_);
        table = analyzer.getTable(dbName_, tableName_.getTbl());
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    } catch (AnalysisException e) {
        if (analyzer.hasMissingTbls())
            throw e;
        throw new AnalysisException(String.format("Error setting privileges for " + "table '%s'. Verify that the table exists and that you have permissions " + "to issue a GRANT/REVOKE statement.", tableName_.toString()));
    }
    Preconditions.checkNotNull(table);
    return table;
}
#method_after
private Table analyzeTargetTable(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(scope_ == TPrivilegeScope.TABLE || scope_ == TPrivilegeScope.COLUMN);
    Preconditions.checkState(!Strings.isNullOrEmpty(tableName_.getTbl()));
    Table table = null;
    try {
        dbName_ = analyzer.getTargetDbName(tableName_);
        Preconditions.checkNotNull(dbName_);
        table = analyzer.getTable(dbName_, tableName_.getTbl());
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    } catch (AnalysisException e) {
        if (analyzer.hasMissingTbls())
            throw e;
        throw new AnalysisException(String.format("Error setting privileges for " + "table '%s'. Verify that the table exists and that you have permissions " + "to issue a GRANT/REVOKE statement.", tableName_.toString()));
    }
    Preconditions.checkNotNull(table);
    if (table instanceof KuduTable) {
        // levels (DELETE/UPDATE) are not available. See IMPALA-4000 for details.
        if (privilegeLevel_ != TPrivilegeLevel.ALL) {
            throw new AnalysisException("Kudu tables only support the ALL privilege level.");
        }
        if (scope_ == TPrivilegeScope.COLUMN) {
            throw new AnalysisException("Column-level privileges on Kudu " + "tables are not supported.");
        }
    }
    return table;
}
#end_block

#method_before
public Deferred<RowResultIterator> nextRows() {
    if (closed) {
        // We're already done scanning.
        return Deferred.fromResult(null);
    } else if (tablet == null) {
        Callback<Deferred<RowResultIterator>, AsyncKuduScanner.Response> cb = new Callback<Deferred<RowResultIterator>, Response>() {

            @Override
            public Deferred<RowResultIterator> call(Response resp) throws Exception {
                if (!resp.more || resp.scannerId == null) {
                    scanFinished();
                    // there might be data to return
                    return Deferred.fromResult(resp.data);
                }
                scannerId = resp.scannerId;
                sequenceId++;
                hasMore = resp.more;
                if (resp.propagatedTimestamp != AsyncKuduClient.NO_TIMESTAMP) {
                    client.updateLastPropagatedTimestamp(resp.propagatedTimestamp);
                }
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Scanner " + Bytes.pretty(scannerId) + " opened on " + tablet);
                }
                return Deferred.fromResult(resp.data);
            }

            public String toString() {
                return "scanner opened";
            }
        };
        Callback<Deferred<RowResultIterator>, Exception> eb = new Callback<Deferred<RowResultIterator>, Exception>() {

            @Override
            public Deferred<RowResultIterator> call(Exception e) throws Exception {
                invalidate();
                if (e instanceof NonCoveredRangeException) {
                    NonCoveredRangeException ncre = (NonCoveredRangeException) e;
                    pruner.removePartitionKeyRange(ncre.getNonCoveredRangeEnd());
                    // Stop scanning if the non-covered range is past the end partition key.
                    if (!pruner.hasMorePartitionKeyRanges()) {
                        hasMore = false;
                        // the scanner is closed on the other side at this point
                        closed = true;
                        return Deferred.fromResult(RowResultIterator.empty());
                    }
                    scannerId = null;
                    sequenceId = 0;
                    return nextRows();
                } else {
                    LOG.warn("Can not open scanner", e);
                    // Let the error propogate.
                    return Deferred.fromError(e);
                }
            }

            public String toString() {
                return "open scanner errback";
            }
        };
        // We need to open the scanner first.
        return client.sendRpcToTablet(getOpenRequest()).addCallbackDeferring(cb).addErrback(eb);
    } else if (prefetching && prefetcherDeferred != null) {
        // TODO KUDU-1260 - Check if this works and add a test
        prefetcherDeferred.chain(new Deferred<RowResultIterator>().addCallback(prefetch));
        return prefetcherDeferred;
    }
    final Deferred<RowResultIterator> d = client.scanNextRows(this).addCallbacks(gotNextRow, nextRowErrback());
    if (prefetching) {
        d.chain(new Deferred<RowResultIterator>().addCallback(prefetch));
    }
    return d;
}
#method_after
public Deferred<RowResultIterator> nextRows() {
    if (closed) {
        // We're already done scanning.
        return Deferred.fromResult(null);
    } else if (tablet == null) {
        Callback<Deferred<RowResultIterator>, AsyncKuduScanner.Response> cb = new Callback<Deferred<RowResultIterator>, Response>() {

            @Override
            public Deferred<RowResultIterator> call(Response resp) throws Exception {
                if (htTimestamp == AsyncKuduClient.NO_TIMESTAMP && resp.scanTimestamp != AsyncKuduClient.NO_TIMESTAMP) {
                    // If the server-assigned timestamp is present in the tablet
                    // server's response, store it in the scanner. The stored value
                    // is used for read operations at other tablet servers in the
                    // context of the same scan.
                    htTimestamp = resp.scanTimestamp;
                }
                if (resp.propagatedTimestamp != AsyncKuduClient.NO_TIMESTAMP) {
                    client.updateLastPropagatedTimestamp(resp.propagatedTimestamp);
                }
                if (!resp.more || resp.scannerId == null) {
                    scanFinished();
                    // there might be data to return
                    return Deferred.fromResult(resp.data);
                }
                scannerId = resp.scannerId;
                sequenceId++;
                hasMore = resp.more;
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Scanner " + Bytes.pretty(scannerId) + " opened on " + tablet);
                }
                return Deferred.fromResult(resp.data);
            }

            public String toString() {
                return "scanner opened";
            }
        };
        Callback<Deferred<RowResultIterator>, Exception> eb = new Callback<Deferred<RowResultIterator>, Exception>() {

            @Override
            public Deferred<RowResultIterator> call(Exception e) throws Exception {
                invalidate();
                if (e instanceof NonCoveredRangeException) {
                    NonCoveredRangeException ncre = (NonCoveredRangeException) e;
                    pruner.removePartitionKeyRange(ncre.getNonCoveredRangeEnd());
                    // Stop scanning if the non-covered range is past the end partition key.
                    if (!pruner.hasMorePartitionKeyRanges()) {
                        hasMore = false;
                        // the scanner is closed on the other side at this point
                        closed = true;
                        return Deferred.fromResult(RowResultIterator.empty());
                    }
                    scannerId = null;
                    sequenceId = 0;
                    return nextRows();
                } else {
                    LOG.warn("Can not open scanner", e);
                    // Let the error propogate.
                    return Deferred.fromError(e);
                }
            }

            public String toString() {
                return "open scanner errback";
            }
        };
        // We need to open the scanner first.
        return client.sendRpcToTablet(getOpenRequest()).addCallbackDeferring(cb).addErrback(eb);
    } else if (prefetching && prefetcherDeferred != null) {
        // TODO KUDU-1260 - Check if this works and add a test
        prefetcherDeferred.chain(new Deferred<RowResultIterator>().addCallback(prefetch));
        return prefetcherDeferred;
    }
    final Deferred<RowResultIterator> d = client.scanNextRows(this).addCallbacks(gotNextRow, nextRowErrback());
    if (prefetching) {
        d.chain(new Deferred<RowResultIterator>().addCallback(prefetch));
    }
    return d;
}
#end_block

#method_before
public String toString() {
    return "AsyncKuduScanner$Response(scannerId=" + Bytes.pretty(scannerId) + ", data=" + data + ", more=" + more + ") ";
}
#method_after
public String toString() {
    String ret = "AsyncKuduScanner$Response(scannerId=" + Bytes.pretty(scannerId) + ", data=" + data + ", more=" + more;
    if (scanTimestamp != AsyncKuduClient.NO_TIMESTAMP) {
        ret += ", responseScanTimestamp =" + scanTimestamp;
    }
    ret += ")";
    return ret;
}
#end_block

#method_before
@Override
Pair<Response, Object> deserialize(final CallResponse callResponse, String tsUUID) throws KuduException {
    ScanResponsePB.Builder builder = ScanResponsePB.newBuilder();
    readProtobuf(callResponse.getPBMessage(), builder);
    ScanResponsePB resp = builder.build();
    final byte[] id = resp.getScannerId().toByteArray();
    TabletServerErrorPB error = resp.hasError() ? resp.getError() : null;
    if (error != null && error.getCode().equals(TabletServerErrorPB.Code.TABLET_NOT_FOUND)) {
        if (state == State.OPENING) {
            // Doing this will trigger finding the new location.
            return new Pair<Response, Object>(null, error);
        } else {
            Status statusIncomplete = Status.Incomplete("Cannot continue scanning, " + "the tablet has moved and this isn't a fault tolerant scan");
            throw new NonRecoverableException(statusIncomplete);
        }
    }
    RowResultIterator iterator = RowResultIterator.makeRowResultIterator(deadlineTracker.getElapsedMillis(), tsUUID, schema, resp.getData(), callResponse);
    boolean hasMore = resp.getHasMoreResults();
    if (id.length != 0 && scannerId != null && !Bytes.equals(scannerId, id)) {
        Status statusIllegalState = Status.IllegalState("Scan RPC response was for scanner" + " ID " + Bytes.pretty(id) + " but we expected " + Bytes.pretty(scannerId));
        throw new NonRecoverableException(statusIllegalState);
    }
    Response response = new Response(id, iterator, hasMore, resp.hasPropagatedTimestamp() ? resp.getPropagatedTimestamp() : -1);
    if (LOG.isDebugEnabled()) {
        LOG.debug(response.toString());
    }
    return new Pair<Response, Object>(response, error);
}
#method_after
@Override
Pair<Response, Object> deserialize(final CallResponse callResponse, String tsUUID) throws KuduException {
    ScanResponsePB.Builder builder = ScanResponsePB.newBuilder();
    readProtobuf(callResponse.getPBMessage(), builder);
    ScanResponsePB resp = builder.build();
    final byte[] id = resp.getScannerId().toByteArray();
    TabletServerErrorPB error = resp.hasError() ? resp.getError() : null;
    if (error != null && error.getCode().equals(TabletServerErrorPB.Code.TABLET_NOT_FOUND)) {
        if (state == State.OPENING) {
            // Doing this will trigger finding the new location.
            return new Pair<Response, Object>(null, error);
        } else {
            Status statusIncomplete = Status.Incomplete("Cannot continue scanning, " + "the tablet has moved and this isn't a fault tolerant scan");
            throw new NonRecoverableException(statusIncomplete);
        }
    }
    RowResultIterator iterator = RowResultIterator.makeRowResultIterator(deadlineTracker.getElapsedMillis(), tsUUID, schema, resp.getData(), callResponse);
    boolean hasMore = resp.getHasMoreResults();
    if (id.length != 0 && scannerId != null && !Bytes.equals(scannerId, id)) {
        Status statusIllegalState = Status.IllegalState("Scan RPC response was for scanner" + " ID " + Bytes.pretty(id) + " but we expected " + Bytes.pretty(scannerId));
        throw new NonRecoverableException(statusIllegalState);
    }
    Response response = new Response(id, iterator, hasMore, resp.hasSnapTimestamp() ? resp.getSnapTimestamp() : AsyncKuduClient.NO_TIMESTAMP, resp.hasPropagatedTimestamp() ? resp.getPropagatedTimestamp() : AsyncKuduClient.NO_TIMESTAMP);
    if (LOG.isDebugEnabled()) {
        LOG.debug(response.toString());
    }
    return new Pair<Response, Object>(response, error);
}
#end_block

#method_before
protected synchronized void updateLastPropagatedTimestamp(long lastPropagatedTimestamp) {
    if (this.lastPropagatedTimestamp == NO_TIMESTAMP || this.lastPropagatedTimestamp < lastPropagatedTimestamp) {
        this.lastPropagatedTimestamp = lastPropagatedTimestamp;
    }
}
#method_after
public synchronized void updateLastPropagatedTimestamp(long lastPropagatedTimestamp) {
    if (this.lastPropagatedTimestamp == NO_TIMESTAMP || this.lastPropagatedTimestamp < lastPropagatedTimestamp) {
        this.lastPropagatedTimestamp = lastPropagatedTimestamp;
    }
}
#end_block

#method_before
protected synchronized long getLastPropagatedTimestamp() {
    return lastPropagatedTimestamp;
}
#method_after
public synchronized long getLastPropagatedTimestamp() {
    return lastPropagatedTimestamp;
}
#end_block

#method_before
public AsyncKuduClient build() {
    return new AsyncKuduClient(this);
}
#method_after
public AsyncKuduClient build() {
    AccessControlContext context = AccessController.getContext();
    subject = Subject.getSubject(context);
    return new AsyncKuduClient(this);
}
#end_block

#method_before
public Deferred<RowResultIterator> nextRows() {
    if (closed) {
        // We're already done scanning.
        return Deferred.fromResult(null);
    } else if (tablet == null) {
        Callback<Deferred<RowResultIterator>, AsyncKuduScanner.Response> cb = new Callback<Deferred<RowResultIterator>, Response>() {

            @Override
            public Deferred<RowResultIterator> call(Response resp) throws Exception {
                if (serverScanTimestamp == AsyncKuduClient.NO_TIMESTAMP && resp.serverScanTimestamp != AsyncKuduClient.NO_TIMESTAMP) {
                    // If the server-assigned timestamp is present in the tablet
                    // server's response, store it in the scanner. The stored value
                    // is used for read operations at other tablet servers in the
                    // context of the same scan.
                    serverScanTimestamp = resp.serverScanTimestamp;
                }
                if (!resp.more || resp.scannerId == null) {
                    scanFinished();
                    // there might be data to return
                    return Deferred.fromResult(resp.data);
                }
                scannerId = resp.scannerId;
                sequenceId++;
                hasMore = resp.more;
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Scanner " + Bytes.pretty(scannerId) + " opened on " + tablet);
                }
                return Deferred.fromResult(resp.data);
            }

            public String toString() {
                return "scanner opened";
            }
        };
        Callback<Deferred<RowResultIterator>, Exception> eb = new Callback<Deferred<RowResultIterator>, Exception>() {

            @Override
            public Deferred<RowResultIterator> call(Exception e) throws Exception {
                invalidate();
                if (e instanceof NonCoveredRangeException) {
                    NonCoveredRangeException ncre = (NonCoveredRangeException) e;
                    pruner.removePartitionKeyRange(ncre.getNonCoveredRangeEnd());
                    // Stop scanning if the non-covered range is past the end partition key.
                    if (!pruner.hasMorePartitionKeyRanges()) {
                        hasMore = false;
                        // the scanner is closed on the other side at this point
                        closed = true;
                        return Deferred.fromResult(RowResultIterator.empty());
                    }
                    scannerId = null;
                    sequenceId = 0;
                    return nextRows();
                } else {
                    LOG.warn("Can not open scanner", e);
                    // Let the error propogate.
                    return Deferred.fromError(e);
                }
            }

            public String toString() {
                return "open scanner errback";
            }
        };
        // We need to open the scanner first.
        return client.sendRpcToTablet(getOpenRequest()).addCallbackDeferring(cb).addErrback(eb);
    } else if (prefetching && prefetcherDeferred != null) {
        // TODO KUDU-1260 - Check if this works and add a test
        prefetcherDeferred.chain(new Deferred<RowResultIterator>().addCallback(prefetch));
        return prefetcherDeferred;
    }
    final Deferred<RowResultIterator> d = client.scanNextRows(this).addCallbacks(gotNextRow, nextRowErrback());
    if (prefetching) {
        d.chain(new Deferred<RowResultIterator>().addCallback(prefetch));
    }
    return d;
}
#method_after
public Deferred<RowResultIterator> nextRows() {
    if (closed) {
        // We're already done scanning.
        return Deferred.fromResult(null);
    } else if (tablet == null) {
        Callback<Deferred<RowResultIterator>, AsyncKuduScanner.Response> cb = new Callback<Deferred<RowResultIterator>, Response>() {

            @Override
            public Deferred<RowResultIterator> call(Response resp) throws Exception {
                if (htTimestamp == AsyncKuduClient.NO_TIMESTAMP && resp.scanTimestamp != AsyncKuduClient.NO_TIMESTAMP) {
                    // If the server-assigned timestamp is present in the tablet
                    // server's response, store it in the scanner. The stored value
                    // is used for read operations at other tablet servers in the
                    // context of the same scan.
                    htTimestamp = resp.scanTimestamp;
                }
                if (!resp.more || resp.scannerId == null) {
                    scanFinished();
                    // there might be data to return
                    return Deferred.fromResult(resp.data);
                }
                scannerId = resp.scannerId;
                sequenceId++;
                hasMore = resp.more;
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Scanner " + Bytes.pretty(scannerId) + " opened on " + tablet);
                }
                return Deferred.fromResult(resp.data);
            }

            public String toString() {
                return "scanner opened";
            }
        };
        Callback<Deferred<RowResultIterator>, Exception> eb = new Callback<Deferred<RowResultIterator>, Exception>() {

            @Override
            public Deferred<RowResultIterator> call(Exception e) throws Exception {
                invalidate();
                if (e instanceof NonCoveredRangeException) {
                    NonCoveredRangeException ncre = (NonCoveredRangeException) e;
                    pruner.removePartitionKeyRange(ncre.getNonCoveredRangeEnd());
                    // Stop scanning if the non-covered range is past the end partition key.
                    if (!pruner.hasMorePartitionKeyRanges()) {
                        hasMore = false;
                        // the scanner is closed on the other side at this point
                        closed = true;
                        return Deferred.fromResult(RowResultIterator.empty());
                    }
                    scannerId = null;
                    sequenceId = 0;
                    return nextRows();
                } else {
                    LOG.warn("Can not open scanner", e);
                    // Let the error propogate.
                    return Deferred.fromError(e);
                }
            }

            public String toString() {
                return "open scanner errback";
            }
        };
        // We need to open the scanner first.
        return client.sendRpcToTablet(getOpenRequest()).addCallbackDeferring(cb).addErrback(eb);
    } else if (prefetching && prefetcherDeferred != null) {
        // TODO KUDU-1260 - Check if this works and add a test
        prefetcherDeferred.chain(new Deferred<RowResultIterator>().addCallback(prefetch));
        return prefetcherDeferred;
    }
    final Deferred<RowResultIterator> d = client.scanNextRows(this).addCallbacks(gotNextRow, nextRowErrback());
    if (prefetching) {
        d.chain(new Deferred<RowResultIterator>().addCallback(prefetch));
    }
    return d;
}
#end_block

#method_before
public String toString() {
    String ret = "AsyncKuduScanner$Response(scannerId=" + Bytes.pretty(scannerId) + ", data=" + data + ", more=" + more;
    if (serverScanTimestamp != -1) {
        ret += ", serverScanTimestamp =" + serverScanTimestamp;
    }
    ret += ")";
    return ret;
}
#method_after
public String toString() {
    String ret = "AsyncKuduScanner$Response(scannerId=" + Bytes.pretty(scannerId) + ", data=" + data + ", more=" + more;
    if (scanTimestamp != AsyncKuduClient.NO_TIMESTAMP) {
        ret += ", responseScanTimestamp =" + scanTimestamp;
    }
    ret += ")";
    return ret;
}
#end_block

#method_before
ChannelBuffer serialize(Message header) {
    final ScanRequestPB.Builder builder = ScanRequestPB.newBuilder();
    switch(state) {
        case OPENING:
            // Save the tablet in the AsyncKuduScanner.  This kind of a kludge but it really
            // is the easiest way.
            AsyncKuduScanner.this.tablet = super.getTablet();
            NewScanRequestPB.Builder newBuilder = NewScanRequestPB.newBuilder();
            // currently ignored
            newBuilder.setLimit(limit);
            newBuilder.addAllProjectedColumns(ProtobufHelper.schemaToListPb(schema));
            newBuilder.setTabletId(ZeroCopyLiteralByteString.wrap(tablet.getTabletIdAsBytes()));
            newBuilder.setReadMode(AsyncKuduScanner.this.getReadMode().pbVersion());
            newBuilder.setOrderMode(AsyncKuduScanner.this.getOrderMode());
            newBuilder.setCacheBlocks(cacheBlocks);
            // if the last propagated timestamp is set send it with the scan
            if (table.getAsyncClient().getLastPropagatedTimestamp() != AsyncKuduClient.NO_TIMESTAMP) {
                newBuilder.setPropagatedTimestamp(table.getAsyncClient().getLastPropagatedTimestamp());
            }
            newBuilder.setReadMode(AsyncKuduScanner.this.getReadMode().pbVersion());
            // if the mode is set to read on snapshot sent the snapshot timestamp
            if (AsyncKuduScanner.this.getReadMode() == ReadMode.READ_AT_SNAPSHOT) {
                if (AsyncKuduScanner.this.getSnapshotTimestamp() != AsyncKuduClient.NO_TIMESTAMP) {
                    // The case of the explicitly set scan timestamp.
                    newBuilder.setSnapTimestamp(AsyncKuduScanner.this.getSnapshotTimestamp());
                } else if (AsyncKuduScanner.this.serverScanTimestamp != AsyncKuduClient.NO_TIMESTAMP) {
                    // The case when the scan timestamp is assigned by the first
                    // tablet server contacted in the context of the scan operation.
                    newBuilder.setSnapTimestamp(AsyncKuduScanner.this.serverScanTimestamp);
                }
            }
            if (AsyncKuduScanner.this.startPrimaryKey.length > 0) {
                newBuilder.setStartPrimaryKey(ZeroCopyLiteralByteString.copyFrom(startPrimaryKey));
            }
            if (AsyncKuduScanner.this.endPrimaryKey.length > 0) {
                newBuilder.setStopPrimaryKey(ZeroCopyLiteralByteString.copyFrom(endPrimaryKey));
            }
            for (KuduPredicate pred : predicates.values()) {
                newBuilder.addColumnPredicates(pred.toPB());
            }
            builder.setNewScanRequest(newBuilder.build()).setBatchSizeBytes(batchSizeBytes);
            break;
        case NEXT:
            setTablet(AsyncKuduScanner.this.tablet);
            builder.setScannerId(ZeroCopyLiteralByteString.wrap(scannerId)).setCallSeqId(AsyncKuduScanner.this.sequenceId).setBatchSizeBytes(batchSizeBytes);
            break;
        case CLOSING:
            setTablet(AsyncKuduScanner.this.tablet);
            builder.setScannerId(ZeroCopyLiteralByteString.wrap(scannerId)).setBatchSizeBytes(0).setCloseScanner(true);
            break;
        default:
            throw new RuntimeException("unreachable!");
    }
    ScanRequestPB request = builder.build();
    if (LOG.isDebugEnabled()) {
        LOG.debug("Sending scan req: " + request.toString());
    }
    return toChannelBuffer(header, request);
}
#method_after
ChannelBuffer serialize(Message header) {
    final ScanRequestPB.Builder builder = ScanRequestPB.newBuilder();
    switch(state) {
        case OPENING:
            // Save the tablet in the AsyncKuduScanner.  This kind of a kludge but it really
            // is the easiest way.
            AsyncKuduScanner.this.tablet = super.getTablet();
            NewScanRequestPB.Builder newBuilder = NewScanRequestPB.newBuilder();
            // currently ignored
            newBuilder.setLimit(limit);
            newBuilder.addAllProjectedColumns(ProtobufHelper.schemaToListPb(schema));
            newBuilder.setTabletId(ZeroCopyLiteralByteString.wrap(tablet.getTabletIdAsBytes()));
            newBuilder.setOrderMode(AsyncKuduScanner.this.getOrderMode());
            newBuilder.setCacheBlocks(cacheBlocks);
            // if the last propagated timestamp is set send it with the scan
            if (table.getAsyncClient().getLastPropagatedTimestamp() != AsyncKuduClient.NO_TIMESTAMP) {
                newBuilder.setPropagatedTimestamp(table.getAsyncClient().getLastPropagatedTimestamp());
            }
            newBuilder.setReadMode(AsyncKuduScanner.this.getReadMode().pbVersion());
            // if the mode is set to read on snapshot sent the snapshot timestamp
            if (AsyncKuduScanner.this.getReadMode() == ReadMode.READ_AT_SNAPSHOT && AsyncKuduScanner.this.getSnapshotTimestamp() != AsyncKuduClient.NO_TIMESTAMP) {
                newBuilder.setSnapTimestamp(AsyncKuduScanner.this.getSnapshotTimestamp());
            }
            if (AsyncKuduScanner.this.startPrimaryKey.length > 0) {
                newBuilder.setStartPrimaryKey(ZeroCopyLiteralByteString.copyFrom(startPrimaryKey));
            }
            if (AsyncKuduScanner.this.endPrimaryKey.length > 0) {
                newBuilder.setStopPrimaryKey(ZeroCopyLiteralByteString.copyFrom(endPrimaryKey));
            }
            for (KuduPredicate pred : predicates.values()) {
                newBuilder.addColumnPredicates(pred.toPB());
            }
            builder.setNewScanRequest(newBuilder.build()).setBatchSizeBytes(batchSizeBytes);
            break;
        case NEXT:
            setTablet(AsyncKuduScanner.this.tablet);
            builder.setScannerId(ZeroCopyLiteralByteString.wrap(scannerId)).setCallSeqId(AsyncKuduScanner.this.sequenceId).setBatchSizeBytes(batchSizeBytes);
            break;
        case CLOSING:
            setTablet(AsyncKuduScanner.this.tablet);
            builder.setScannerId(ZeroCopyLiteralByteString.wrap(scannerId)).setBatchSizeBytes(0).setCloseScanner(true);
            break;
        default:
            throw new RuntimeException("unreachable!");
    }
    ScanRequestPB request = builder.build();
    if (LOG.isDebugEnabled()) {
        LOG.debug("Sending scan req: " + request.toString());
    }
    return toChannelBuffer(header, request);
}
#end_block

#method_before
private Process startProcessWithKrbEnv(String... argv) throws IOException {
    ProcessBuilder procBuilder = new ProcessBuilder(argv);
    procBuilder.environment().putAll(getEnvVars());
    LOG.debug("executing '{}', env: '{}'", Joiner.on(" ").join(procBuilder.command()), Joiner.on(", ").withKeyValueSeparator("=").join(procBuilder.environment()));
    return procBuilder.redirectErrorStream(true).redirectInput(ProcessBuilder.Redirect.PIPE).start();
}
#method_after
private Process startProcessWithKrbEnv(String... argv) throws IOException {
    ProcessBuilder procBuilder = new ProcessBuilder(argv);
    procBuilder.environment().putAll(getEnvVars());
    LOG.debug("executing '{}', env: '{}'", Joiner.on(" ").join(procBuilder.command()), Joiner.on(", ").withKeyValueSeparator("=").join(procBuilder.environment()));
    return procBuilder.redirectErrorStream(true).start();
}
#end_block

#method_before
private void startCluster(int numMasters, int numTservers) throws Exception {
    Preconditions.checkArgument(numMasters > 0, "Need at least one master");
    // The following props are set via kudu-client's pom.
    String baseDirPath = TestUtils.getBaseDir();
    long now = System.currentTimeMillis();
    LOG.info("Starting {} masters...", numMasters);
    int startPort = startMasters(PORT_START, numMasters, baseDirPath, bindHost);
    LOG.info("Starting {} tablet servers...", numTservers);
    List<Integer> ports = TestUtils.findFreePorts(startPort, numTservers * 2);
    for (int i = 0; i < numTservers; i++) {
        int rpcPort = ports.get(i * 2);
        tserverPorts.add(rpcPort);
        String dataDirPath = baseDirPath + "/ts-" + i + "-" + now;
        String flagsPath = TestUtils.getFlagsPath();
        List<String> commandLine = Lists.newArrayList(TestUtils.findBinary("kudu-tserver"), "--flagfile=" + flagsPath, "--fs_wal_dir=" + dataDirPath, "--fs_data_dirs=" + dataDirPath, "--flush_threshold_mb=1", "--tserver_master_addrs=" + masterAddresses, "--webserver_interface=" + bindHost, "--local_ip_for_outbound_sockets=" + bindHost, "--webserver_port=" + (rpcPort + 1), "--rpc_bind_addresses=" + bindHost + ":" + rpcPort);
        Map<String, String> env = new HashMap<>();
        if (miniKdc != null) {
            String spn = "kudu/" + bindHost;
            commandLine.add("--keytab=" + keytab);
            commandLine.add("--kerberos_principal=" + spn);
            commandLine.add("--server_require_kerberos");
            env.putAll(miniKdc.getEnvVars());
        }
        tserverProcesses.put(rpcPort, configureAndStartProcess(rpcPort, commandLine, env));
        commandLines.put(rpcPort, commandLine);
        environments.put(rpcPort, env);
        if (flagsPath.startsWith(baseDirPath)) {
            // We made a temporary copy of the flags; delete them later.
            pathsToDelete.add(flagsPath);
        }
        pathsToDelete.add(dataDirPath);
    }
}
#method_after
private void startCluster(int numMasters, int numTservers) throws Exception {
    Preconditions.checkArgument(numMasters > 0, "Need at least one master");
    // The following props are set via kudu-client's pom.
    String baseDirPath = TestUtils.getBaseDir();
    long now = System.currentTimeMillis();
    LOG.info("Starting {} masters...", numMasters);
    int startPort = startMasters(PORT_START, numMasters, baseDirPath, bindHost);
    LOG.info("Starting {} tablet servers...", numTservers);
    List<Integer> ports = TestUtils.findFreePorts(startPort, numTservers * 2);
    for (int i = 0; i < numTservers; i++) {
        int rpcPort = ports.get(i * 2);
        tserverPorts.add(rpcPort);
        String dataDirPath = baseDirPath + "/ts-" + i + "-" + now;
        String flagsPath = TestUtils.getFlagsPath();
        List<String> commandLine = Lists.newArrayList(TestUtils.findBinary("kudu-tserver"), "--flagfile=" + flagsPath, "--fs_wal_dir=" + dataDirPath, "--fs_data_dirs=" + dataDirPath, "--flush_threshold_mb=1", "--tserver_master_addrs=" + masterAddresses, "--webserver_interface=" + bindHost, "--local_ip_for_outbound_sockets=" + bindHost, "--webserver_port=" + (rpcPort + 1), "--rpc_bind_addresses=" + bindHost + ":" + rpcPort);
        if (miniKdc != null) {
            commandLine.add("--keytab=" + keytab);
            commandLine.add("--kerberos_principal=kudu/" + bindHost);
            commandLine.add("--server_require_kerberos");
        }
        tserverProcesses.put(rpcPort, configureAndStartProcess(rpcPort, commandLine));
        commandLines.put(rpcPort, commandLine);
        if (flagsPath.startsWith(baseDirPath)) {
            // We made a temporary copy of the flags; delete them later.
            pathsToDelete.add(flagsPath);
        }
        pathsToDelete.add(dataDirPath);
    }
}
#end_block

#method_before
private int startMasters(int masterStartPort, int numMasters, String baseDirPath, String bindHost) throws Exception {
    LOG.info("Starting {} masters...", numMasters);
    // Get the list of web and RPC ports to use for the master consensus configuration:
    // request NUM_MASTERS * 2 free ports as we want to also reserve the web
    // ports for the consensus configuration.
    List<Integer> ports = TestUtils.findFreePorts(masterStartPort, numMasters * 2);
    int lastFreePort = ports.get(ports.size() - 1);
    List<Integer> masterRpcPorts = Lists.newArrayListWithCapacity(numMasters);
    List<Integer> masterWebPorts = Lists.newArrayListWithCapacity(numMasters);
    for (int i = 0; i < numMasters * 2; i++) {
        if (i % 2 == 0) {
            masterRpcPorts.add(ports.get(i));
            masterHostPorts.add(HostAndPort.fromParts(bindHost, ports.get(i)));
        } else {
            masterWebPorts.add(ports.get(i));
        }
    }
    masterAddresses = NetUtil.hostsAndPortsToString(masterHostPorts);
    long now = System.currentTimeMillis();
    for (int i = 0; i < numMasters; i++) {
        int port = masterRpcPorts.get(i);
        String dataDirPath = baseDirPath + "/master-" + i + "-" + now;
        String flagsPath = TestUtils.getFlagsPath();
        // The web port must be reserved in the call to findFreePorts above and specified
        // to avoid the scenario where:
        // 1) findFreePorts finds RPC ports a, b, c for the 3 masters.
        // 2) start master 1 with RPC port and let it bind to any (specified as 0) web port.
        // 3) master 1 happens to bind to port b for the web port, as master 2 hasn't been
        // started yet and findFreePort(s) is "check-time-of-use" (it does not reserve the
        // ports, only checks that when it was last called, these ports could be used).
        List<String> commandLine = Lists.newArrayList(TestUtils.findBinary("kudu-master"), "--flagfile=" + flagsPath, "--fs_wal_dir=" + dataDirPath, "--fs_data_dirs=" + dataDirPath, "--webserver_interface=" + bindHost, "--local_ip_for_outbound_sockets=" + bindHost, "--rpc_bind_addresses=" + bindHost + ":" + port, "--webserver_port=" + masterWebPorts.get(i), // make leader elections faster for faster tests
        "--raft_heartbeat_interval_ms=200");
        if (numMasters > 1) {
            commandLine.add("--master_addresses=" + masterAddresses);
        }
        Map<String, String> env = new HashMap<>();
        if (miniKdc != null) {
            String spn = "kudu/" + bindHost;
            commandLine.add("--keytab=" + keytab);
            commandLine.add("--kerberos_principal=" + spn);
            commandLine.add("--server_require_kerberos");
            env.putAll(miniKdc.getEnvVars());
        }
        masterProcesses.put(port, configureAndStartProcess(port, commandLine, env));
        commandLines.put(port, commandLine);
        environments.put(port, env);
        if (flagsPath.startsWith(baseDirPath)) {
            // We made a temporary copy of the flags; delete them later.
            pathsToDelete.add(flagsPath);
        }
        pathsToDelete.add(dataDirPath);
    }
    return lastFreePort + 1;
}
#method_after
private int startMasters(int masterStartPort, int numMasters, String baseDirPath, String bindHost) throws Exception {
    LOG.info("Starting {} masters...", numMasters);
    // Get the list of web and RPC ports to use for the master consensus configuration:
    // request NUM_MASTERS * 2 free ports as we want to also reserve the web
    // ports for the consensus configuration.
    List<Integer> ports = TestUtils.findFreePorts(masterStartPort, numMasters * 2);
    int lastFreePort = ports.get(ports.size() - 1);
    List<Integer> masterRpcPorts = Lists.newArrayListWithCapacity(numMasters);
    List<Integer> masterWebPorts = Lists.newArrayListWithCapacity(numMasters);
    for (int i = 0; i < numMasters * 2; i++) {
        if (i % 2 == 0) {
            masterRpcPorts.add(ports.get(i));
            masterHostPorts.add(HostAndPort.fromParts(bindHost, ports.get(i)));
        } else {
            masterWebPorts.add(ports.get(i));
        }
    }
    masterAddresses = NetUtil.hostsAndPortsToString(masterHostPorts);
    long now = System.currentTimeMillis();
    for (int i = 0; i < numMasters; i++) {
        int port = masterRpcPorts.get(i);
        String dataDirPath = baseDirPath + "/master-" + i + "-" + now;
        String flagsPath = TestUtils.getFlagsPath();
        // The web port must be reserved in the call to findFreePorts above and specified
        // to avoid the scenario where:
        // 1) findFreePorts finds RPC ports a, b, c for the 3 masters.
        // 2) start master 1 with RPC port and let it bind to any (specified as 0) web port.
        // 3) master 1 happens to bind to port b for the web port, as master 2 hasn't been
        // started yet and findFreePort(s) is "check-time-of-use" (it does not reserve the
        // ports, only checks that when it was last called, these ports could be used).
        List<String> commandLine = Lists.newArrayList(TestUtils.findBinary("kudu-master"), "--flagfile=" + flagsPath, "--fs_wal_dir=" + dataDirPath, "--fs_data_dirs=" + dataDirPath, "--webserver_interface=" + bindHost, "--local_ip_for_outbound_sockets=" + bindHost, "--rpc_bind_addresses=" + bindHost + ":" + port, "--webserver_port=" + masterWebPorts.get(i), // make leader elections faster for faster tests
        "--raft_heartbeat_interval_ms=200");
        if (numMasters > 1) {
            commandLine.add("--master_addresses=" + masterAddresses);
        }
        if (miniKdc != null) {
            commandLine.add("--keytab=" + keytab);
            commandLine.add("--kerberos_principal=kudu/" + bindHost);
            commandLine.add("--server_require_kerberos");
        }
        masterProcesses.put(port, configureAndStartProcess(port, commandLine));
        commandLines.put(port, commandLine);
        if (flagsPath.startsWith(baseDirPath)) {
            // We made a temporary copy of the flags; delete them later.
            pathsToDelete.add(flagsPath);
        }
        pathsToDelete.add(dataDirPath);
    }
    return lastFreePort + 1;
}
#end_block

#method_before
private Process configureAndStartProcess(int port, List<String> command, Map<String, String> env) throws Exception {
    ProcessBuilder processBuilder = new ProcessBuilder(command);
    processBuilder.redirectErrorStream(true);
    processBuilder.environment().putAll(env);
    LOG.info("Starting process: {}, environment: {}", Joiner.on(" ").join(processBuilder.command()), Joiner.on(",").withKeyValueSeparator("=").join(processBuilder.environment()));
    Process proc = processBuilder.start();
    ProcessInputStreamLogPrinterRunnable printer = new ProcessInputStreamLogPrinterRunnable(proc.getInputStream());
    Thread thread = new Thread(printer);
    thread.setDaemon(true);
    thread.setName(Iterables.getLast(Splitter.on(File.separatorChar).split(command.get(0))) + ":" + port);
    PROCESS_INPUT_PRINTERS.add(thread);
    thread.start();
    Thread.sleep(300);
    try {
        int ev = proc.exitValue();
        throw new Exception(String.format("We tried starting a process (%s) but it exited with value=%s", command.get(0), ev));
    } catch (IllegalThreadStateException ex) {
    // This means the process is still alive, it's like reverse psychology.
    }
    return proc;
}
#method_after
private Process configureAndStartProcess(int port, List<String> command) throws Exception {
    ProcessBuilder processBuilder = new ProcessBuilder(command);
    processBuilder.redirectErrorStream(true);
    if (miniKdc != null) {
        processBuilder.environment().putAll(miniKdc.getEnvVars());
    }
    Process proc = processBuilder.start();
    ProcessInputStreamLogPrinterRunnable printer = new ProcessInputStreamLogPrinterRunnable(proc.getInputStream());
    Thread thread = new Thread(printer);
    thread.setDaemon(true);
    thread.setName(Iterables.getLast(Splitter.on(File.separatorChar).split(command.get(0))) + ":" + port);
    PROCESS_INPUT_PRINTERS.add(thread);
    thread.start();
    Thread.sleep(300);
    try {
        int ev = proc.exitValue();
        throw new Exception(String.format("We tried starting a process (%s) but it exited with value=%s", command.get(0), ev));
    } catch (IllegalThreadStateException ex) {
    // This means the process is still alive, it's like reverse psychology.
    }
    return proc;
}
#end_block

#method_before
private void restartDeadProcessOnPort(int port, Map<Integer, Process> map) throws Exception {
    if (!commandLines.containsKey(port)) {
        String message = "Cannot start process on unknown port " + port;
        LOG.warn(message);
        throw new RuntimeException(message);
    }
    if (map.containsKey(port)) {
        String message = "Process already exists on port " + port;
        LOG.warn(message);
        throw new RuntimeException(message);
    }
    map.put(port, configureAndStartProcess(port, commandLines.get(port), environments.get(port)));
}
#method_after
private void restartDeadProcessOnPort(int port, Map<Integer, Process> map) throws Exception {
    if (!commandLines.containsKey(port)) {
        String message = "Cannot start process on unknown port " + port;
        LOG.warn(message);
        throw new RuntimeException(message);
    }
    if (map.containsKey(port)) {
        String message = "Process already exists on port " + port;
        LOG.warn(message);
        throw new RuntimeException(message);
    }
    map.put(port, configureAndStartProcess(port, commandLines.get(port)));
}
#end_block

#method_before
private void typeCastTest(Type type1, Type type2, boolean op1IsLiteral, ArithmeticExpr.Operator arithmeticOp, BinaryPredicate.Operator cmpOp, Type opType) throws AnalysisException {
    Preconditions.checkState((arithmeticOp == null) != (cmpOp == null));
    boolean arithmeticMode = arithmeticOp != null;
    String op1 = "";
    if (type1 != null) {
        if (op1IsLiteral) {
            op1 = typeToLiteralValue_.get(type1);
        } else {
            op1 = TestSchemaUtils.getAllTypesColumn(type1);
        }
    }
    String op2 = TestSchemaUtils.getAllTypesColumn(type2);
    String queryStr = null;
    if (arithmeticMode) {
        queryStr = "select " + op1 + " " + arithmeticOp.toString() + " " + op2 + " AS a from functional.alltypes";
    } else {
        queryStr = "select int_col from functional.alltypes " + "where " + op1 + " " + cmpOp.toString() + " " + op2;
    }
    SelectStmt select = (SelectStmt) AnalyzesOk(queryStr);
    Expr expr = null;
    if (arithmeticMode) {
        ArrayList<Expr> selectListExprs = select.getResultExprs();
        assertNotNull(selectListExprs);
        assertEquals(selectListExprs.size(), 1);
        // check the first expr in select list
        expr = selectListExprs.get(0);
        Assert.assertEquals("opType= " + opType + " exprType=" + expr.getType(), opType, expr.getType());
    } else {
        // check the where clause
        expr = select.getWhereClause();
        if (!expr.getType().isNull()) {
            assertEquals(Type.BOOLEAN, expr.getType());
        }
    }
    checkCasts(expr);
    // The children's types must be NULL or equal to the requested opType.
    Type child1Type = expr.getChild(0).getType();
    Type child2Type = type1 == null ? null : expr.getChild(1).getType();
    Assert.assertTrue("opType= " + opType + " child1Type=" + child1Type, opType.equals(child1Type) || opType.isNull() || child1Type.isNull());
    if (type1 != null) {
        Assert.assertTrue("opType= " + opType + " child2Type=" + child2Type, opType.equals(child2Type) || opType.isNull() || child2Type.isNull());
    }
}
#method_after
private void typeCastTest(Type type1, Type type2, boolean op1IsLiteral, ArithmeticExpr.Operator arithmeticOp, BinaryPredicate.Operator cmpOp, Type opType) throws AnalysisException {
    Preconditions.checkState((arithmeticOp == null) != (cmpOp == null));
    boolean arithmeticMode = arithmeticOp != null;
    String op1 = "";
    if (type1 != null) {
        if (op1IsLiteral) {
            op1 = typeToLiteralValue_.get(type1);
        } else {
            op1 = TestSchemaUtils.getAllTypesColumn(type1);
        }
    }
    String op2 = TestSchemaUtils.getAllTypesColumn(type2);
    String queryStr = null;
    if (arithmeticMode) {
        queryStr = "select " + op1 + " " + arithmeticOp.toString() + " " + op2 + " AS a from functional.alltypes";
    } else {
        queryStr = "select int_col from functional.alltypes " + "where " + op1 + " " + cmpOp.toString() + " " + op2;
    }
    SelectStmt select = (SelectStmt) AnalyzesOk(queryStr);
    Expr expr = null;
    if (arithmeticMode) {
        List<Expr> selectListExprs = select.getResultExprs();
        assertNotNull(selectListExprs);
        assertEquals(selectListExprs.size(), 1);
        // check the first expr in select list
        expr = selectListExprs.get(0);
        Assert.assertEquals("opType= " + opType + " exprType=" + expr.getType(), opType, expr.getType());
    } else {
        // check the where clause
        expr = select.getWhereClause();
        if (!expr.getType().isNull()) {
            assertEquals(Type.BOOLEAN, expr.getType());
        }
    }
    checkCasts(expr);
    // The children's types must be NULL or equal to the requested opType.
    Type child1Type = expr.getChild(0).getType();
    Type child2Type = type1 == null ? null : expr.getChild(1).getType();
    Assert.assertTrue("opType= " + opType + " child1Type=" + child1Type, opType.equals(child1Type) || opType.isNull() || child1Type.isNull());
    if (type1 != null) {
        Assert.assertTrue("opType= " + opType + " child2Type=" + child2Type, opType.equals(child2Type) || opType.isNull() || child2Type.isNull());
    }
}
#end_block

#method_before
private void checkReturnType(String stmt, Type resultType) {
    SelectStmt select = (SelectStmt) AnalyzesOk(stmt);
    ArrayList<Expr> selectListExprs = select.getResultExprs();
    assertNotNull(selectListExprs);
    assertEquals(selectListExprs.size(), 1);
    // check the first expr in select list
    Expr expr = selectListExprs.get(0);
    assertEquals("Expected: " + resultType + " != " + expr.getType(), resultType, expr.getType());
}
#method_after
private void checkReturnType(String stmt, Type resultType) {
    SelectStmt select = (SelectStmt) AnalyzesOk(stmt);
    List<Expr> selectListExprs = select.getResultExprs();
    assertNotNull(selectListExprs);
    assertEquals(selectListExprs.size(), 1);
    // check the first expr in select list
    Expr expr = selectListExprs.get(0);
    assertEquals("Expected: " + resultType + " != " + expr.getType(), resultType, expr.getType());
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Subqueries need to be rewritten by the StmtRewriter first.
            if (analyzer.containsSubquery())
                return;
            // Use getResultExprs() and not getBaseTblResultExprs() here because the final
            // substitution with TupleIsNullPredicate() wrapping happens in planning.
            selectListExprs = Expr.cloneList(queryStmt_.getResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    analyzeTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions, unless the target is
    // a Kudu table, in which case we don't want to overwrite unmentioned columns with
    // NULL.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Use getResultExprs() and not getBaseTblResultExprs() here because the final
            // substitution with TupleIsNullPredicate() wrapping happens in planning.
            selectListExprs = Expr.cloneList(queryStmt_.getResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    analyzeTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions, unless the target is
    // a Kudu table, in which case we don't want to overwrite unmentioned columns with
    // NULL.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#end_block

#method_before
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    boolean isKuduTable = table_ instanceof KuduTable;
    // Finally, 'undo' the permutation so that the selectListExprs are in Hive column
    // order, and add NULL expressions to all missing columns, unless this is an UPSERT.
    ArrayList<Column> columns = table_.getColumnsInHiveOrder();
    for (int col = 0; col < columns.size(); ++col) {
        Column tblColumn = columns.get(col);
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                if (isKuduTable)
                    mentionedColumns_.add(col);
                matchFound = true;
                break;
            }
        }
        // expression if this is an INSERT and the target is not a Kudu table.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                if (isKuduTable) {
                    Preconditions.checkState(tblColumn instanceof KuduColumn);
                    KuduColumn kuduCol = (KuduColumn) tblColumn;
                    if (!kuduCol.hasDefaultValue() && !kuduCol.isNullable()) {
                        throw new AnalysisException("Missing values for column that is not " + "nullable and has no default value " + kuduCol.getName());
                    }
                } else {
                    // Unmentioned non-clustering columns get NULL literals with the appropriate
                    // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                    resultExprs_.add(NullLiteral.create(tblColumn.getType()));
                }
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && isKuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#method_after
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getLiteralValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    boolean isKuduTable = table_ instanceof KuduTable;
    // Finally, 'undo' the permutation so that the selectListExprs are in Hive column
    // order, and add NULL expressions to all missing columns, unless this is an UPSERT.
    ArrayList<Column> columns = table_.getColumnsInHiveOrder();
    for (int col = 0; col < columns.size(); ++col) {
        Column tblColumn = columns.get(col);
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                if (isKuduTable)
                    mentionedColumns_.add(col);
                matchFound = true;
                break;
            }
        }
        // expression if this is an INSERT and the target is not a Kudu table.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                if (isKuduTable) {
                    Preconditions.checkState(tblColumn instanceof KuduColumn);
                    KuduColumn kuduCol = (KuduColumn) tblColumn;
                    if (!kuduCol.hasDefaultValue() && !kuduCol.isNullable()) {
                        throw new AnalysisException("Missing values for column that is not " + "nullable and has no default value " + kuduCol.getName());
                    }
                } else {
                    // Unmentioned non-clustering columns get NULL literals with the appropriate
                    // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                    resultExprs_.add(NullLiteral.create(tblColumn.getType()));
                }
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && isKuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#end_block

#method_before
public ArrayList<Expr> getResultExprs() {
    return resultExprs_;
}
#method_after
@Override
public ArrayList<Expr> getResultExprs() {
    return resultExprs_;
}
#end_block

#method_before
public List<String> getMentionedColumns() {
    List<String> result = Lists.newArrayList();
    List<Column> columns = table_.getColumns();
    for (int i = 0; i < resultExprs_.size(); ++i) {
        result.add(columns.get(mentionedColumns_.get(i)).getName());
    }
    return result;
}
#method_after
public List<String> getMentionedColumns() {
    List<String> result = Lists.newArrayList();
    List<Column> columns = table_.getColumns();
    for (Integer i : mentionedColumns_) result.add(columns.get(i).getName());
    return result;
}
#end_block

#method_before
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    return TableSink.create(table_, isUpsert_ ? TableSink.Op.UPSERT : TableSink.Op.INSERT, partitionKeyExprs_, mentionedColumns_, overwrite_);
}
#method_after
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    return TableSink.create(table_, isUpsert_ ? TableSink.Op.UPSERT : TableSink.Op.INSERT, partitionKeyExprs_, mentionedColumns_, overwrite_, hasClusteredHint_);
}
#end_block

#method_before
private void init(Analyzer analyzer) {
    Preconditions.checkNotNull(analyzer);
    Preconditions.checkState(analyzer.isRootAnalyzer());
    TQueryCtx queryCtx = analyzer.getQueryCtx();
    if (queryCtx.request.isSetRedacted_stmt()) {
        queryStr_ = queryCtx.request.redacted_stmt;
    } else {
        queryStr_ = queryCtx.request.stmt;
    }
    Preconditions.checkNotNull(queryStr_);
    SimpleDateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
    try {
        timestamp_ = df.parse(queryCtx.now_string).getTime() / 1000;
    } catch (java.text.ParseException e) {
        LOG.error("Error parsing timestamp value: " + queryCtx.now_string + " " + e.getMessage());
        timestamp_ = new Date().getTime() / 1000;
    }
    descTbl_ = analyzer.getDescTbl();
    user_ = analyzer.getUser().getName();
}
#method_after
private void init(Analyzer analyzer) {
    Preconditions.checkNotNull(analyzer);
    Preconditions.checkState(analyzer.isRootAnalyzer());
    TQueryCtx queryCtx = analyzer.getQueryCtx();
    if (queryCtx.request.isSetRedacted_stmt()) {
        queryStr_ = queryCtx.request.redacted_stmt;
    } else {
        queryStr_ = queryCtx.request.stmt;
    }
    Preconditions.checkNotNull(queryStr_);
    timestamp_ = queryCtx.start_unix_millis / 1000;
    descTbl_ = analyzer.getDescTbl();
    user_ = analyzer.getUser().getName();
}
#end_block

#method_before
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createClusteringSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (BackendConfig.INSTANCE.getComputeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            List<Expr> exprs = Lists.newArrayList();
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            Preconditions.checkNotNull(targetTable);
            if (targetTable instanceof KuduTable) {
                // Lineage is disabled for UPDATE AND DELETE statements
                if (ctx_.isUpdateOrDelete())
                    return fragments;
                if (ctx_.isInsert()) {
                    graph.addTargetColumnLabels(ctx_.getAnalysisResult().getInsertStmt());
                } else {
                    graph.addTargetColumnLabels(targetTable);
                }
                exprs.addAll(resultExprs);
            } else if (targetTable instanceof HBaseTable) {
                graph.addTargetColumnLabels(targetTable);
                exprs.addAll(resultExprs);
            } else {
                graph.addTargetColumnLabels(targetTable);
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#method_after
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryCtx().disable_codegen_hint = true;
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createClusteringSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (BackendConfig.INSTANCE.getComputeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Lineage is disabled for UPDATE AND DELETE statements
        if (ctx_.isUpdateOrDelete())
            return fragments;
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
            List<Expr> exprs = Lists.newArrayList();
            Table targetTable = insertStmt.getTargetTable();
            Preconditions.checkNotNull(targetTable);
            if (targetTable instanceof KuduTable) {
                if (ctx_.isInsert()) {
                    // For insert statements on Kudu tables, we only need to consider
                    // the labels of columns mentioned in the column list.
                    List<String> mentionedColumns = insertStmt.getMentionedColumns();
                    Preconditions.checkState(!mentionedColumns.isEmpty());
                    List<String> targetColLabels = Lists.newArrayList();
                    String tblFullName = targetTable.getFullName();
                    for (String column : mentionedColumns) {
                        targetColLabels.add(tblFullName + "." + column);
                    }
                    graph.addTargetColumnLabels(targetColLabels);
                } else {
                    graph.addTargetColumnLabels(targetTable);
                }
                exprs.addAll(resultExprs);
            } else if (targetTable instanceof HBaseTable) {
                graph.addTargetColumnLabels(targetTable);
                exprs.addAll(resultExprs);
            } else {
                graph.addTargetColumnLabels(targetTable);
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#end_block

#method_before
@Override
public boolean isConstant() {
    // Aggregate functions are never constant.
    if (fn_ instanceof AggregateFunction)
        return false;
    String fnName = fnName_.getFunction();
    if (fnName == null) {
        List<String> path = fnName_.getFnNamePath();
        fnName = path.get(path.size() - 1);
    }
    // Non-deterministic functions are never constant.
    if (fnName.equalsIgnoreCase("rand") || fnName.equalsIgnoreCase("random")) {
        return false;
    }
    // Sleep is a special function for testing.
    if (fnName.equalsIgnoreCase("sleep"))
        return false;
    return super.isConstant();
}
#method_after
@Override
public boolean isConstant() {
    // Aggregate functions are never constant.
    if (fn_ instanceof AggregateFunction)
        return false;
    String fnName = fnName_.getFunction();
    if (fnName == null) {
        // This expr has not been analyzed yet, get the function name from the path.
        List<String> path = fnName_.getFnNamePath();
        fnName = path.get(path.size() - 1);
    }
    // Non-deterministic functions are never constant.
    if (fnName.equalsIgnoreCase("rand") || fnName.equalsIgnoreCase("random")) {
        return false;
    }
    // Sleep is a special function for testing.
    if (fnName.equalsIgnoreCase("sleep"))
        return false;
    return super.isConstant();
}
#end_block

#method_before
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createClusteringSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    // Rewrite sink exprs and final result exprs.
    if (ctx_.getQueryOptions().enable_expr_rewrites) {
        if (rootFragment.getSink() instanceof HdfsTableSink) {
            HdfsTableSink sink = ((HdfsTableSink) rootFragment.getSink());
            sink.rewriteExprs(ctx_.getRewriter(), ctx_.getRootAnalyzer());
        }
        ctx_.getRewriter().rewriteList(resultExprs, ctx_.getRootAnalyzer());
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (BackendConfig.INSTANCE.getComputeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#method_after
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryCtx().disable_codegen_hint = true;
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createClusteringSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (BackendConfig.INSTANCE.getComputeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            // Lineage is not currently supported for Kudu tables (see IMPALA-4283)
            if (targetTable instanceof KuduTable)
                return fragments;
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#end_block

#method_before
private PlanNode useNljForSingularRowBuilds(PlanNode root, Analyzer analyzer) throws ImpalaException {
    for (int i = 0; i < root.getChildren().size(); ++i) {
        root.setChild(i, useNljForSingularRowBuilds(root.getChild(i), analyzer));
    }
    if (!(root instanceof JoinNode))
        return root;
    if (root instanceof NestedLoopJoinNode)
        return root;
    if (!(root.getChild(1) instanceof SingularRowSrcNode))
        return root;
    JoinNode joinNode = (JoinNode) root;
    if (joinNode.getJoinOp().isNullAwareLeftAntiJoin()) {
        Preconditions.checkState(joinNode instanceof HashJoinNode);
        return root;
    }
    List<Expr> otherJoinConjuncts = Lists.newArrayList(joinNode.getOtherJoinConjuncts());
    otherJoinConjuncts.addAll(joinNode.getEqJoinConjuncts());
    JoinNode newJoinNode = new NestedLoopJoinNode(joinNode.getChild(0), joinNode.getChild(1), joinNode.isStraightJoin(), joinNode.getDistributionModeHint(), joinNode.getJoinOp(), otherJoinConjuncts);
    newJoinNode.getConjuncts().addAll(joinNode.getConjuncts());
    newJoinNode.setId(joinNode.getId());
    newJoinNode.initNoRewrite(analyzer);
    return newJoinNode;
}
#method_after
private PlanNode useNljForSingularRowBuilds(PlanNode root, Analyzer analyzer) throws ImpalaException {
    for (int i = 0; i < root.getChildren().size(); ++i) {
        root.setChild(i, useNljForSingularRowBuilds(root.getChild(i), analyzer));
    }
    if (!(root instanceof JoinNode))
        return root;
    if (root instanceof NestedLoopJoinNode)
        return root;
    if (!(root.getChild(1) instanceof SingularRowSrcNode))
        return root;
    JoinNode joinNode = (JoinNode) root;
    if (joinNode.getJoinOp().isNullAwareLeftAntiJoin()) {
        Preconditions.checkState(joinNode instanceof HashJoinNode);
        return root;
    }
    List<Expr> otherJoinConjuncts = Lists.newArrayList(joinNode.getOtherJoinConjuncts());
    otherJoinConjuncts.addAll(joinNode.getEqJoinConjuncts());
    JoinNode newJoinNode = new NestedLoopJoinNode(joinNode.getChild(0), joinNode.getChild(1), joinNode.isStraightJoin(), joinNode.getDistributionModeHint(), joinNode.getJoinOp(), otherJoinConjuncts);
    newJoinNode.getConjuncts().addAll(joinNode.getConjuncts());
    newJoinNode.setId(joinNode.getId());
    newJoinNode.init(analyzer);
    return newJoinNode;
}
#end_block

#method_before
public void createClusteringSort(InsertStmt insertStmt, PlanFragment inputFragment, Analyzer analyzer) throws ImpalaException {
    if (!insertStmt.hasClusteredHint())
        return;
    List<Expr> orderingExprs;
    if (insertStmt.getTargetTable() instanceof KuduTable) {
        orderingExprs = Lists.newArrayList(insertStmt.getPrimaryKeyExprs());
    } else {
        orderingExprs = Lists.newArrayList(insertStmt.getPartitionKeyExprs());
    }
    // Ignore constants for the sake of clustering.
    Expr.removeConstants(orderingExprs);
    if (orderingExprs.isEmpty())
        return;
    // Build sortinfo to sort by the ordering exprs.
    List<Boolean> isAscOrder = Collections.nCopies(orderingExprs.size(), false);
    List<Boolean> nullsFirstParams = Collections.nCopies(orderingExprs.size(), false);
    SortInfo sortInfo = new SortInfo(orderingExprs, isAscOrder, nullsFirstParams);
    ExprSubstitutionMap smap = sortInfo.createSortTupleInfo(insertStmt.getResultExprs(), analyzer);
    sortInfo.getSortTupleDescriptor().materializeSlots();
    insertStmt.substituteResultExprs(smap, analyzer);
    SortNode sortNode = new SortNode(ctx_.getNextNodeId(), inputFragment.getPlanRoot(), sortInfo, false, 0);
    sortNode.init(ctx_.getRewriter(), analyzer);
    inputFragment.setPlanRoot(sortNode);
}
#method_after
public void createClusteringSort(InsertStmt insertStmt, PlanFragment inputFragment, Analyzer analyzer) throws ImpalaException {
    if (!insertStmt.hasClusteredHint())
        return;
    List<Expr> orderingExprs;
    if (insertStmt.getTargetTable() instanceof KuduTable) {
        orderingExprs = Lists.newArrayList(insertStmt.getPrimaryKeyExprs());
    } else {
        orderingExprs = Lists.newArrayList(insertStmt.getPartitionKeyExprs());
    }
    // Ignore constants for the sake of clustering.
    Expr.removeConstants(orderingExprs);
    if (orderingExprs.isEmpty())
        return;
    // Build sortinfo to sort by the ordering exprs.
    List<Boolean> isAscOrder = Collections.nCopies(orderingExprs.size(), false);
    List<Boolean> nullsFirstParams = Collections.nCopies(orderingExprs.size(), false);
    SortInfo sortInfo = new SortInfo(orderingExprs, isAscOrder, nullsFirstParams);
    ExprSubstitutionMap smap = sortInfo.createSortTupleInfo(insertStmt.getResultExprs(), analyzer);
    sortInfo.getSortTupleDescriptor().materializeSlots();
    insertStmt.substituteResultExprs(smap, analyzer);
    SortNode sortNode = new SortNode(ctx_.getNextNodeId(), inputFragment.getPlanRoot(), sortInfo, false, 0);
    sortNode.init(analyzer);
    inputFragment.setPlanRoot(sortNode);
}
#end_block

#method_before
@Override
public void init(ExprRewriter rewriter, Analyzer analyzer) throws ImpalaException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    if (analyzer.getQueryOptions().enable_expr_rewrites && rewriter != null) {
        rewriteExprs(rewriter, analyzer);
    }
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = KuduUtil.createKuduClient(kuduTable_.getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Compute mem layout before the scan range locations because creation of the Kudu
        // scan tokens depends on having a mem layout.
        computeMemLayout(analyzer);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeStats(analyzer);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = KuduUtil.createKuduClient(kuduTable_.getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Compute mem layout before the scan range locations because creation of the Kudu
        // scan tokens depends on having a mem layout.
        computeMemLayout(analyzer);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeStats(analyzer);
}
#end_block

#method_before
private static BinaryPredicate normalizeSlotRefComparison(BinaryPredicate predicate, Analyzer analyzer) {
    if (!predicate.getChild(0).isLiteral() && !predicate.getChild(1).isLiteral()) {
        return null;
    }
    // Re-analyze the predicate to make sure it has the minimal types and casts.
    predicate.reset();
    predicate.analyzeNoThrow(analyzer);
    SlotRef ref = null;
    if (predicate.getChild(0) instanceof SlotRef) {
        ref = (SlotRef) predicate.getChild(0);
    } else if (predicate.getChild(1) instanceof SlotRef) {
        ref = (SlotRef) predicate.getChild(1);
    }
    if (ref == null)
        return null;
    if (ref != predicate.getChild(0))
        predicate.reverse();
    if (!(predicate.getChild(1) instanceof LiteralExpr))
        return null;
    return predicate;
}
#method_after
private static BinaryPredicate normalizeSlotRefComparison(BinaryPredicate predicate, Analyzer analyzer) {
    SlotRef ref = null;
    if (predicate.getChild(0) instanceof SlotRef) {
        ref = (SlotRef) predicate.getChild(0);
    } else if (predicate.getChild(1) instanceof SlotRef) {
        ref = (SlotRef) predicate.getChild(1);
    }
    if (ref == null)
        return null;
    if (ref != predicate.getChild(0)) {
        Preconditions.checkState(ref == predicate.getChild(1));
        predicate = new BinaryPredicate(predicate.getOp().converse(), ref, predicate.getChild(0));
        predicate.analyzeNoThrow(analyzer);
    }
    if (!(predicate.getChild(1) instanceof LiteralExpr))
        return null;
    return predicate;
}
#end_block

#method_before
@Override
public void init(ExprRewriter rewriter, Analyzer analyzer) throws ImpalaException {
    checkForSupportedFileFormats();
    assignConjuncts(analyzer);
    if (analyzer.getQueryOptions().enable_expr_rewrites && rewriter != null) {
        rewriteExprs(rewriter, analyzer);
    }
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    setStartStopKey(analyzer);
    // Convert predicates to HBase filters_.
    createHBaseFilters(analyzer);
    // materialize slots in remaining conjuncts_
    analyzer.materializeSlots(conjuncts_);
    computeMemLayout(analyzer);
    computeScanRangeLocations(analyzer);
    // Call computeStats() after materializing slots and computing the mem layout.
    computeStats(analyzer);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    checkForSupportedFileFormats();
    assignConjuncts(analyzer);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    setStartStopKey(analyzer);
    // Convert predicates to HBase filters_.
    createHBaseFilters(analyzer);
    // materialize slots in remaining conjuncts_
    analyzer.materializeSlots(conjuncts_);
    computeMemLayout(analyzer);
    computeScanRangeLocations(analyzer);
    // Call computeStats() after materializing slots and computing the mem layout.
    computeStats(analyzer);
}
#end_block

#method_before
@Override
public Expr apply(Expr expr, Analyzer analyzer) throws AnalysisException {
    // children should have been folded at this point.
    for (Expr child : expr.getChildren()) {
        if (!child.isLiteral())
            return expr;
    }
    if (expr.isLiteral() || !expr.isConstant())
        return expr;
    Expr result = LiteralExpr.create(expr, analyzer.getQueryCtx());
    // Preserve original type so parent Exprs do not need to be re-analyzed.
    if (result != null)
        return result.castTo(expr.getType());
    return expr;
}
#method_after
@Override
public Expr apply(Expr expr, Analyzer analyzer) throws AnalysisException {
    // children should have been folded at this point.
    for (Expr child : expr.getChildren()) if (!child.isLiteral())
        return expr;
    if (expr.isLiteral() || !expr.isConstant())
        return expr;
    // after analysis (e.g., aggregate functions).
    if (!expr.isAnalyzed()) {
        expr.analyze(analyzer);
        if (!expr.isConstant())
            return expr;
    }
    Expr result = LiteralExpr.create(expr, analyzer.getQueryCtx());
    // Preserve original type so parent Exprs do not need to be re-analyzed.
    if (result != null)
        return result.castTo(expr.getType());
    return expr;
}
#end_block

#method_before
private Expr applyRuleRepeatedly(Expr expr, ExprRewriteRule rule, Analyzer analyzer) throws AnalysisException {
    int oldNumChanges;
    Expr rewrittenExpr = expr;
    do {
        oldNumChanges = numChanges_;
        rewrittenExpr = applyRuleBottomUp(rewrittenExpr, rule, analyzer);
        Preconditions.checkState(expr.getType().equals(rewrittenExpr.getType()));
    } while (oldNumChanges != numChanges_);
    return rewrittenExpr;
}
#method_after
private Expr applyRuleRepeatedly(Expr expr, ExprRewriteRule rule, Analyzer analyzer) throws AnalysisException {
    int oldNumChanges;
    Expr rewrittenExpr = expr;
    do {
        oldNumChanges = numChanges_;
        rewrittenExpr = applyRuleBottomUp(rewrittenExpr, rule, analyzer);
    } while (oldNumChanges != numChanges_);
    return rewrittenExpr;
}
#end_block

#method_before
@Override
public ArrayList<String> getColLabels() {
    return colLabels_;
}
#method_after
@Override
public List<String> getColLabels() {
    return colLabels_;
}
#end_block

#method_before
public Expr RewritesOk(String expr, ExprRewriteRule rule, String expectedExpr) throws AnalysisException {
    String stmtStr = "select " + expr + " from functional.alltypes";
    SelectStmt stmt = (SelectStmt) ParsesOk(stmtStr);
    Analyzer analyzer = createAnalyzer(Catalog.DEFAULT_DB);
    stmt.analyze(analyzer);
    Expr origExpr = stmt.getResultExprs().get(0);
    String origSql = origExpr.toSql();
    // Create a rewriter with only a single rule.
    List<ExprRewriteRule> rules = Lists.newArrayList();
    rules.add(rule);
    ExprRewriter rewriter = new ExprRewriter(rules);
    Expr rewrittenExpr = rewriter.rewrite(origExpr, analyzer);
    String rewrittenSql = rewrittenExpr.toSql();
    boolean expectChange = expectedExpr != null;
    if (expectedExpr != null) {
        assertEquals(expectedExpr, rewrittenSql);
    } else {
        assertEquals(origSql, rewrittenSql);
    }
    Assert.assertEquals(expectChange, rewriter.changed());
    return rewrittenExpr;
}
#method_after
public Expr RewritesOk(String expr, ExprRewriteRule rule, String expectedExpr) throws AnalysisException {
    System.out.println(expr);
    String stmtStr = "select " + expr + " from functional.alltypessmall";
    SelectStmt stmt = (SelectStmt) ParsesOk(stmtStr);
    Analyzer analyzer = createAnalyzer(Catalog.DEFAULT_DB);
    stmt.analyze(analyzer);
    Expr origExpr = stmt.getResultExprs().get(0);
    String origSql = origExpr.toSql();
    // Create a rewriter with only a single rule.
    List<ExprRewriteRule> rules = Lists.newArrayList();
    rules.add(rule);
    ExprRewriter rewriter = new ExprRewriter(rules);
    Expr rewrittenExpr = rewriter.rewrite(origExpr, analyzer);
    String rewrittenSql = rewrittenExpr.toSql();
    boolean expectChange = expectedExpr != null;
    if (expectedExpr != null) {
        assertEquals(expectedExpr, rewrittenSql);
    } else {
        assertEquals(origSql, rewrittenSql);
    }
    Assert.assertEquals(expectChange, rewriter.changed());
    return rewrittenExpr;
}
#end_block

#method_before
@Test
public void TestFoldConstantsRule() throws AnalysisException {
    ExprRewriteRule rule = FoldConstantsRule.INSTANCE;
    RewritesOk("1 + 1", rule, "2");
    RewritesOk("1 + 1 + 1 + 1 + 1", rule, "5");
    RewritesOk("10 - 5 - 2 - 1 - 8", rule, "-6");
    RewritesOk("cast('2016-11-09' as timestamp)", rule, "TIMESTAMP '2016-11-09 00:00:00'");
    RewritesOk("cast('2016-11-09' as timestamp) + interval 1 year", rule, "TIMESTAMP '2017-11-09 00:00:00'");
    // Tests correct handling of strings with escape sequences.
    RewritesOk("'_' LIKE '\\\\_'", rule, "TRUE");
    RewritesOk("base64decode(base64encode('\\047\\001\\132\\060')) = " + "'\\047\\001\\132\\060'", rule, "TRUE");
    // Tests correct handling of strings with chars > 127. Should not be folded.
    RewritesOk("hex(unhex(hex(unhex('D3'))))", rule, null);
    // Tests that non-deterministic functions are not folded.
    RewritesOk("rand()", rule, null);
}
#method_after
@Test
public void TestFoldConstantsRule() throws AnalysisException {
    ExprRewriteRule rule = FoldConstantsRule.INSTANCE;
    RewritesOk("1 + 1", rule, "2");
    RewritesOk("1 + 1 + 1 + 1 + 1", rule, "5");
    RewritesOk("10 - 5 - 2 - 1 - 8", rule, "-6");
    RewritesOk("cast('2016-11-09' as timestamp)", rule, "TIMESTAMP '2016-11-09 00:00:00'");
    RewritesOk("cast('2016-11-09' as timestamp) + interval 1 year", rule, "TIMESTAMP '2017-11-09 00:00:00'");
    // Tests correct handling of strings with escape sequences.
    RewritesOk("'_' LIKE '\\\\_'", rule, "TRUE");
    RewritesOk("base64decode(base64encode('\\047\\001\\132\\060')) = " + "'\\047\\001\\132\\060'", rule, "TRUE");
    // Tests correct handling of strings with chars > 127. Should not be folded.
    RewritesOk("hex(unhex(hex(unhex('D3'))))", rule, null);
    // Tests that non-deterministic functions are not folded.
    RewritesOk("rand()", rule, null);
    // Tests that exprs that warn during their evaluation are not folded.
    RewritesOk("coalesce(1.8, cast(int_col as decimal(38,38)))", rule, null);
}
#end_block

#method_before
public static LiteralExpr create(Expr constExpr, TQueryCtx queryCtx) throws AnalysisException {
    Preconditions.checkState(constExpr.isConstant());
    Preconditions.checkState(constExpr.getType().isValid());
    if (constExpr instanceof LiteralExpr)
        return (LiteralExpr) constExpr;
    TColumnValue val = null;
    try {
        val = FeSupport.EvalConstExpr(constExpr, queryCtx);
    } catch (InternalException e) {
        throw new AnalysisException(String.format("Failed to evaluate expr '%s'", constExpr.toSql()), e);
    }
    LiteralExpr result = null;
    switch(constExpr.getType().getPrimitiveType()) {
        case NULL_TYPE:
            result = new NullLiteral();
            break;
        case BOOLEAN:
            if (val.isSetBool_val())
                result = new BoolLiteral(val.bool_val);
            break;
        case TINYINT:
            if (val.isSetByte_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.byte_val));
            }
            break;
        case SMALLINT:
            if (val.isSetShort_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.short_val));
            }
            break;
        case INT:
            if (val.isSetInt_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.int_val));
            }
            break;
        case BIGINT:
            if (val.isSetLong_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.long_val));
            }
            break;
        case FLOAT:
        case DOUBLE:
            if (val.isSetDouble_val()) {
                if (Double.isNaN(val.double_val) || Double.isInfinite(val.double_val))
                    return null;
                // Test for negative zero which cannot be represented by NumericLiteral.
                if (val.double_val == 0 && 1.0 / val.double_val < 0)
                    return null;
                result = new NumericLiteral(new BigDecimal(val.double_val), constExpr.getType());
            }
            break;
        case DECIMAL:
            if (val.isSetString_val()) {
                result = new NumericLiteral(new BigDecimal(val.string_val), constExpr.getType());
            }
            break;
        case STRING:
        case VARCHAR:
        case CHAR:
            if (val.isSetBinary_val()) {
                byte[] bytes = new byte[val.binary_val.remaining()];
                val.binary_val.get(bytes);
                // producing incorrect results.
                for (byte b : bytes) {
                    if (b < 0)
                        return null;
                }
                try {
                    // US-ASCII is 7-bit ASCII.
                    String strVal = new String(bytes, "US-ASCII");
                    // The evaluation result is a raw string that must not be unescaped.
                    result = new StringLiteral(strVal, constExpr.getType(), false);
                } catch (UnsupportedEncodingException e) {
                    return null;
                }
            }
            break;
        case TIMESTAMP:
            // representation as well as the string representation.
            if (val.isSetBinary_val() && val.isSetString_val()) {
                result = new TimestampLiteral(val.getBinary_val(), val.getString_val());
            }
            break;
        case DATE:
        case DATETIME:
            return null;
        default:
            Preconditions.checkState(false, String.format("Literals of type '%s' not supported.", constExpr.getType().toSql()));
    }
    // None of the fields in the thrift struct were set indicating a NULL.
    if (result == null)
        result = new NullLiteral();
    result.analyze(null);
    return (LiteralExpr) result;
}
#method_after
public static LiteralExpr create(Expr constExpr, TQueryCtx queryCtx) throws AnalysisException {
    Preconditions.checkState(constExpr.isConstant());
    Preconditions.checkState(constExpr.getType().isValid());
    if (constExpr instanceof LiteralExpr)
        return (LiteralExpr) constExpr;
    TColumnValue val = null;
    try {
        val = FeSupport.EvalConstExpr(constExpr, queryCtx);
    } catch (InternalException e) {
        LOG.debug(String.format("Failed to evaluate expr '%s'", constExpr.toSql(), e.getMessage()));
        return null;
    }
    LiteralExpr result = null;
    switch(constExpr.getType().getPrimitiveType()) {
        case NULL_TYPE:
            result = new NullLiteral();
            break;
        case BOOLEAN:
            if (val.isSetBool_val())
                result = new BoolLiteral(val.bool_val);
            break;
        case TINYINT:
            if (val.isSetByte_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.byte_val));
            }
            break;
        case SMALLINT:
            if (val.isSetShort_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.short_val));
            }
            break;
        case INT:
            if (val.isSetInt_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.int_val));
            }
            break;
        case BIGINT:
            if (val.isSetLong_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.long_val));
            }
            break;
        case FLOAT:
        case DOUBLE:
            if (val.isSetDouble_val()) {
                // A NumericLiteral cannot represent NaN, infinity or negative zero.
                if (!NumericLiteral.isValidLiteral(val.double_val))
                    return null;
                result = new NumericLiteral(new BigDecimal(val.double_val), constExpr.getType());
            }
            break;
        case DECIMAL:
            if (val.isSetString_val()) {
                result = new NumericLiteral(new BigDecimal(val.string_val), constExpr.getType());
            }
            break;
        case STRING:
        case VARCHAR:
        case CHAR:
            if (val.isSetBinary_val()) {
                byte[] bytes = new byte[val.binary_val.remaining()];
                val.binary_val.get(bytes);
                // producing incorrect results.
                for (byte b : bytes) if (b < 0)
                    return null;
                try {
                    // US-ASCII is 7-bit ASCII.
                    String strVal = new String(bytes, "US-ASCII");
                    // The evaluation result is a raw string that must not be unescaped.
                    result = new StringLiteral(strVal, constExpr.getType(), false);
                } catch (UnsupportedEncodingException e) {
                    return null;
                }
            }
            break;
        case TIMESTAMP:
            // representation as well as the string representation.
            if (val.isSetBinary_val() && val.isSetString_val()) {
                result = new TimestampLiteral(val.getBinary_val(), val.getString_val());
            }
            break;
        case DATE:
        case DATETIME:
            return null;
        default:
            Preconditions.checkState(false, String.format("Literals of type '%s' not supported.", constExpr.getType().toSql()));
    }
    // None of the fields in the thrift struct were set indicating a NULL.
    if (result == null)
        result = new NullLiteral();
    result.analyzeNoThrow(null);
    return (LiteralExpr) result;
}
#end_block

#method_before
private PlanNode createEmptyNode(QueryStmt stmt, Analyzer analyzer) {
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    stmt.getMaterializedTupleIds(tupleIds);
    if (tupleIds.isEmpty()) {
        // Constant selects do not have materialized tuples at this stage.
        Preconditions.checkState(stmt instanceof SelectStmt, "Only constant selects should have no materialized tuples");
        SelectStmt selectStmt = (SelectStmt) stmt;
        Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
        tupleIds.add(createResultTupleDescriptor(selectStmt, "empty", analyzer).getId());
    }
    unmarkCollectionSlots(stmt);
    EmptySetNode node = new EmptySetNode(ctx_.getNextNodeId(), tupleIds);
    node.init(ctx_.getRewriter(), analyzer);
    // Not needed for a UnionStmt because it materializes its input operands.
    if (stmt instanceof SelectStmt) {
        node.setOutputSmap(((SelectStmt) stmt).getBaseTblSmap());
    }
    return node;
}
#method_after
private PlanNode createEmptyNode(QueryStmt stmt, Analyzer analyzer) {
    ArrayList<TupleId> tupleIds = Lists.newArrayList();
    stmt.getMaterializedTupleIds(tupleIds);
    if (tupleIds.isEmpty()) {
        // Constant selects do not have materialized tuples at this stage.
        Preconditions.checkState(stmt instanceof SelectStmt, "Only constant selects should have no materialized tuples");
        SelectStmt selectStmt = (SelectStmt) stmt;
        Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
        tupleIds.add(createResultTupleDescriptor(selectStmt, "empty", analyzer).getId());
    }
    unmarkCollectionSlots(stmt);
    EmptySetNode node = new EmptySetNode(ctx_.getNextNodeId(), tupleIds);
    node.init(analyzer);
    // Not needed for a UnionStmt because it materializes its input operands.
    if (stmt instanceof SelectStmt) {
        node.setOutputSmap(((SelectStmt) stmt).getBaseTblSmap());
    }
    return node;
}
#end_block

#method_before
private PlanNode createQueryPlan(QueryStmt stmt, Analyzer analyzer, boolean disableTopN) throws ImpalaException {
    if (analyzer.hasEmptyResultSet())
        return createEmptyNode(stmt, analyzer);
    PlanNode root;
    if (stmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) stmt;
        root = createSelectPlan(selectStmt, analyzer);
        // insert possible AnalyticEvalNode before SortNode
        if (((SelectStmt) stmt).getAnalyticInfo() != null) {
            AnalyticInfo analyticInfo = selectStmt.getAnalyticInfo();
            AnalyticPlanner analyticPlanner = new AnalyticPlanner(analyticInfo, analyzer, ctx_);
            List<Expr> inputPartitionExprs = Lists.newArrayList();
            AggregateInfo aggInfo = selectStmt.getAggInfo();
            root = analyticPlanner.createSingleNodePlan(root, aggInfo != null ? aggInfo.getGroupingExprs() : null, inputPartitionExprs);
            if (aggInfo != null && !inputPartitionExprs.isEmpty()) {
                // analytic computation will benefit from a partition on inputPartitionExprs
                aggInfo.setPartitionExprs(inputPartitionExprs);
            }
        }
    } else {
        Preconditions.checkState(stmt instanceof UnionStmt);
        root = createUnionPlan((UnionStmt) stmt, analyzer);
    }
    // Avoid adding a sort node if the sort tuple has no materialized slots.
    boolean sortHasMaterializedSlots = false;
    if (stmt.evaluateOrderBy()) {
        for (SlotDescriptor sortSlotDesc : stmt.getSortInfo().getSortTupleDescriptor().getSlots()) {
            if (sortSlotDesc.isMaterialized()) {
                sortHasMaterializedSlots = true;
                break;
            }
        }
    }
    if (stmt.evaluateOrderBy() && sortHasMaterializedSlots) {
        long limit = stmt.getLimit();
        // TODO: External sort could be used for very large limits
        // not just unlimited order-by
        boolean useTopN = stmt.hasLimit() && !disableTopN;
        root = new SortNode(ctx_.getNextNodeId(), root, stmt.getSortInfo(), useTopN, stmt.getOffset());
        Preconditions.checkState(root.hasValidStats());
        root.setLimit(limit);
        root.init(ctx_.getRewriter(), analyzer);
    } else {
        root.setLimit(stmt.getLimit());
        root.computeStats(analyzer);
    }
    return root;
}
#method_after
private PlanNode createQueryPlan(QueryStmt stmt, Analyzer analyzer, boolean disableTopN) throws ImpalaException {
    if (analyzer.hasEmptyResultSet())
        return createEmptyNode(stmt, analyzer);
    PlanNode root;
    if (stmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) stmt;
        root = createSelectPlan(selectStmt, analyzer);
        // insert possible AnalyticEvalNode before SortNode
        if (((SelectStmt) stmt).getAnalyticInfo() != null) {
            AnalyticInfo analyticInfo = selectStmt.getAnalyticInfo();
            AnalyticPlanner analyticPlanner = new AnalyticPlanner(analyticInfo, analyzer, ctx_);
            List<Expr> inputPartitionExprs = Lists.newArrayList();
            AggregateInfo aggInfo = selectStmt.getAggInfo();
            root = analyticPlanner.createSingleNodePlan(root, aggInfo != null ? aggInfo.getGroupingExprs() : null, inputPartitionExprs);
            if (aggInfo != null && !inputPartitionExprs.isEmpty()) {
                // analytic computation will benefit from a partition on inputPartitionExprs
                aggInfo.setPartitionExprs(inputPartitionExprs);
            }
        }
    } else {
        Preconditions.checkState(stmt instanceof UnionStmt);
        root = createUnionPlan((UnionStmt) stmt, analyzer);
    }
    // Avoid adding a sort node if the sort tuple has no materialized slots.
    boolean sortHasMaterializedSlots = false;
    if (stmt.evaluateOrderBy()) {
        for (SlotDescriptor sortSlotDesc : stmt.getSortInfo().getSortTupleDescriptor().getSlots()) {
            if (sortSlotDesc.isMaterialized()) {
                sortHasMaterializedSlots = true;
                break;
            }
        }
    }
    if (stmt.evaluateOrderBy() && sortHasMaterializedSlots) {
        long limit = stmt.getLimit();
        // TODO: External sort could be used for very large limits
        // not just unlimited order-by
        boolean useTopN = stmt.hasLimit() && !disableTopN;
        root = new SortNode(ctx_.getNextNodeId(), root, stmt.getSortInfo(), useTopN, stmt.getOffset());
        Preconditions.checkState(root.hasValidStats());
        root.setLimit(limit);
        root.init(analyzer);
    } else {
        root.setLimit(stmt.getLimit());
        root.computeStats(analyzer);
    }
    return root;
}
#end_block

#method_before
private PlanNode addUnassignedConjuncts(Analyzer analyzer, List<TupleId> tupleIds, PlanNode root) throws ImpalaException {
    // No point in adding SelectNode on top of an EmptyNode.
    if (root instanceof EmptySetNode)
        return root;
    Preconditions.checkNotNull(root);
    // Gather unassigned conjuncts and generate predicates to enfore
    // slot equivalences for each tuple id.
    List<Expr> conjuncts = analyzer.getUnassignedConjuncts(root);
    for (TupleId tid : tupleIds) {
        analyzer.createEquivConjuncts(tid, conjuncts);
    }
    if (conjuncts.isEmpty())
        return root;
    // evaluate conjuncts in SelectNode
    SelectNode selectNode = new SelectNode(ctx_.getNextNodeId(), root, conjuncts);
    // init() marks conjuncts as assigned
    selectNode.init(ctx_.getRewriter(), analyzer);
    Preconditions.checkState(selectNode.hasValidStats());
    return selectNode;
}
#method_after
private PlanNode addUnassignedConjuncts(Analyzer analyzer, List<TupleId> tupleIds, PlanNode root) {
    // No point in adding SelectNode on top of an EmptyNode.
    if (root instanceof EmptySetNode)
        return root;
    Preconditions.checkNotNull(root);
    // Gather unassigned conjuncts and generate predicates to enfore
    // slot equivalences for each tuple id.
    List<Expr> conjuncts = analyzer.getUnassignedConjuncts(root);
    for (TupleId tid : tupleIds) {
        analyzer.createEquivConjuncts(tid, conjuncts);
    }
    if (conjuncts.isEmpty())
        return root;
    // evaluate conjuncts in SelectNode
    SelectNode selectNode = new SelectNode(ctx_.getNextNodeId(), root, conjuncts);
    // init() marks conjuncts as assigned
    selectNode.init(analyzer);
    Preconditions.checkState(selectNode.hasValidStats());
    return selectNode;
}
#end_block

#method_before
private PlanNode createSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    // no from clause -> materialize the select's exprs with a UnionNode
    if (selectStmt.getTableRefs().isEmpty()) {
        return createConstantSelectPlan(selectStmt, analyzer);
    }
    // Slot materialization:
    // We need to mark all slots as materialized that are needed during the execution
    // of selectStmt, and we need to do that prior to creating plans for the TableRefs
    // (because createTableRefNode() might end up calling computeMemLayout() on one or
    // more TupleDescriptors, at which point all referenced slots need to be marked).
    // 
    // For non-join predicates, slots are marked as follows:
    // - for base table scan predicates, this is done directly by ScanNode.init(), which
    // can do a better job because it doesn't need to materialize slots that are only
    // referenced for partition pruning, for instance
    // - for inline views, non-join predicates are pushed down, at which point the
    // process repeats itself.
    selectStmt.materializeRequiredSlots(analyzer);
    ArrayList<TupleId> rowTuples = Lists.newArrayList();
    // collect output tuples of subtrees
    for (TableRef tblRef : selectStmt.getTableRefs()) {
        rowTuples.addAll(tblRef.getMaterializedTupleIds());
    }
    // computed.
    if (analyzer.hasEmptySpjResultSet()) {
        unmarkCollectionSlots(selectStmt);
        PlanNode emptySetNode = new EmptySetNode(ctx_.getNextNodeId(), rowTuples);
        emptySetNode.init(ctx_.getRewriter(), analyzer);
        emptySetNode.setOutputSmap(selectStmt.getBaseTblSmap());
        return createAggregationPlan(selectStmt, analyzer, emptySetNode);
    }
    AggregateInfo aggInfo = selectStmt.getAggInfo();
    // For queries which contain partition columns only, we may use the metadata instead
    // of table scans. This is only feasible if all materialized aggregate expressions
    // have distinct semantics. Please see createHdfsScanPlan() for details.
    boolean fastPartitionKeyScans = analyzer.getQueryCtx().getRequest().query_options.optimize_partition_key_scans && aggInfo != null && aggInfo.hasAllDistinctAgg();
    // Separate table refs into parent refs (uncorrelated or absolute) and
    // subplan refs (correlated or relative), and generate their plan.
    List<TableRef> parentRefs = Lists.newArrayList();
    List<SubplanRef> subplanRefs = Lists.newArrayList();
    computeParentAndSubplanRefs(selectStmt.getTableRefs(), analyzer.isStraightJoin(), parentRefs, subplanRefs);
    PlanNode root = createTableRefsPlan(parentRefs, subplanRefs, fastPartitionKeyScans, analyzer);
    // add aggregation, if any
    if (aggInfo != null)
        root = createAggregationPlan(selectStmt, analyzer, root);
    // Preconditions.checkState(!analyzer.hasUnassignedConjuncts());
    return root;
}
#method_after
private PlanNode createSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    // no from clause -> materialize the select's exprs with a UnionNode
    if (selectStmt.getTableRefs().isEmpty()) {
        return createConstantSelectPlan(selectStmt, analyzer);
    }
    // Slot materialization:
    // We need to mark all slots as materialized that are needed during the execution
    // of selectStmt, and we need to do that prior to creating plans for the TableRefs
    // (because createTableRefNode() might end up calling computeMemLayout() on one or
    // more TupleDescriptors, at which point all referenced slots need to be marked).
    // 
    // For non-join predicates, slots are marked as follows:
    // - for base table scan predicates, this is done directly by ScanNode.init(), which
    // can do a better job because it doesn't need to materialize slots that are only
    // referenced for partition pruning, for instance
    // - for inline views, non-join predicates are pushed down, at which point the
    // process repeats itself.
    selectStmt.materializeRequiredSlots(analyzer);
    ArrayList<TupleId> rowTuples = Lists.newArrayList();
    // collect output tuples of subtrees
    for (TableRef tblRef : selectStmt.getTableRefs()) {
        rowTuples.addAll(tblRef.getMaterializedTupleIds());
    }
    // computed.
    if (analyzer.hasEmptySpjResultSet()) {
        unmarkCollectionSlots(selectStmt);
        PlanNode emptySetNode = new EmptySetNode(ctx_.getNextNodeId(), rowTuples);
        emptySetNode.init(analyzer);
        emptySetNode.setOutputSmap(selectStmt.getBaseTblSmap());
        return createAggregationPlan(selectStmt, analyzer, emptySetNode);
    }
    AggregateInfo aggInfo = selectStmt.getAggInfo();
    // For queries which contain partition columns only, we may use the metadata instead
    // of table scans. This is only feasible if all materialized aggregate expressions
    // have distinct semantics. Please see createHdfsScanPlan() for details.
    boolean fastPartitionKeyScans = analyzer.getQueryCtx().getRequest().query_options.optimize_partition_key_scans && aggInfo != null && aggInfo.hasAllDistinctAgg();
    // Separate table refs into parent refs (uncorrelated or absolute) and
    // subplan refs (correlated or relative), and generate their plan.
    List<TableRef> parentRefs = Lists.newArrayList();
    List<SubplanRef> subplanRefs = Lists.newArrayList();
    computeParentAndSubplanRefs(selectStmt.getTableRefs(), analyzer.isStraightJoin(), parentRefs, subplanRefs);
    PlanNode root = createTableRefsPlan(parentRefs, subplanRefs, fastPartitionKeyScans, analyzer);
    // add aggregation, if any
    if (aggInfo != null)
        root = createAggregationPlan(selectStmt, analyzer, root);
    // Preconditions.checkState(!analyzer.hasUnassignedConjuncts());
    return root;
}
#end_block

#method_before
private PlanNode createSubplan(PlanNode root, List<SubplanRef> subplanRefs, boolean assignId, Analyzer analyzer) throws ImpalaException {
    Preconditions.checkNotNull(root);
    List<TableRef> applicableRefs = extractApplicableRefs(root, subplanRefs);
    if (applicableRefs.isEmpty())
        return root;
    // Prepend a SingularRowSrcTableRef representing the current row being processed
    // by the SubplanNode from its input (first child).
    Preconditions.checkState(applicableRefs.get(0).getLeftTblRef() == null);
    applicableRefs.add(0, new SingularRowSrcTableRef(root));
    applicableRefs.get(1).setLeftTblRef(applicableRefs.get(0));
    // Construct an incomplete SubplanNode that only knows its input so we can push it
    // into the planner context. The subplan is set after the subplan tree has been
    // constructed.
    SubplanNode subplanNode = new SubplanNode(root);
    if (assignId)
        subplanNode.setId(ctx_.getNextNodeId());
    // Push the SubplanNode such that UnnestNodes and SingularRowSrcNodes can pick up
    // their containing SubplanNode. Also, further plan generation relies on knowing
    // whether we are in a subplan context or not (see computeParentAndSubplanRefs()).
    ctx_.pushSubplan(subplanNode);
    PlanNode subplan = createTableRefsPlan(applicableRefs, subplanRefs, false, analyzer);
    ctx_.popSubplan();
    subplanNode.setSubplan(subplan);
    subplanNode.init(ctx_.getRewriter(), analyzer);
    return subplanNode;
}
#method_after
private PlanNode createSubplan(PlanNode root, List<SubplanRef> subplanRefs, boolean assignId, Analyzer analyzer) throws ImpalaException {
    Preconditions.checkNotNull(root);
    List<TableRef> applicableRefs = extractApplicableRefs(root, subplanRefs);
    if (applicableRefs.isEmpty())
        return root;
    // Prepend a SingularRowSrcTableRef representing the current row being processed
    // by the SubplanNode from its input (first child).
    Preconditions.checkState(applicableRefs.get(0).getLeftTblRef() == null);
    applicableRefs.add(0, new SingularRowSrcTableRef(root));
    applicableRefs.get(1).setLeftTblRef(applicableRefs.get(0));
    // Construct an incomplete SubplanNode that only knows its input so we can push it
    // into the planner context. The subplan is set after the subplan tree has been
    // constructed.
    SubplanNode subplanNode = new SubplanNode(root);
    if (assignId)
        subplanNode.setId(ctx_.getNextNodeId());
    // Push the SubplanNode such that UnnestNodes and SingularRowSrcNodes can pick up
    // their containing SubplanNode. Also, further plan generation relies on knowing
    // whether we are in a subplan context or not (see computeParentAndSubplanRefs()).
    ctx_.pushSubplan(subplanNode);
    PlanNode subplan = createTableRefsPlan(applicableRefs, subplanRefs, false, analyzer);
    ctx_.popSubplan();
    subplanNode.setSubplan(subplan);
    subplanNode.init(analyzer);
    return subplanNode;
}
#end_block

#method_before
private PlanNode createAggregationPlan(SelectStmt selectStmt, Analyzer analyzer, PlanNode root) throws ImpalaException {
    Preconditions.checkState(selectStmt.getAggInfo() != null);
    // add aggregation, if required
    AggregateInfo aggInfo = selectStmt.getAggInfo();
    root = new AggregationNode(ctx_.getNextNodeId(), root, aggInfo);
    root.init(ctx_.getRewriter(), analyzer);
    Preconditions.checkState(root.hasValidStats());
    // 2nd phase agginfo
    if (aggInfo.isDistinctAgg()) {
        ((AggregationNode) root).unsetNeedsFinalize();
        // The output of the 1st phase agg is the 1st phase intermediate.
        ((AggregationNode) root).setIntermediateTuple();
        root = new AggregationNode(ctx_.getNextNodeId(), root, aggInfo.getSecondPhaseDistinctAggInfo());
        root.init(ctx_.getRewriter(), analyzer);
        Preconditions.checkState(root.hasValidStats());
    }
    // add Having clause
    root.assignConjuncts(analyzer);
    return root;
}
#method_after
private PlanNode createAggregationPlan(SelectStmt selectStmt, Analyzer analyzer, PlanNode root) throws ImpalaException {
    Preconditions.checkState(selectStmt.getAggInfo() != null);
    // add aggregation, if required
    AggregateInfo aggInfo = selectStmt.getAggInfo();
    root = new AggregationNode(ctx_.getNextNodeId(), root, aggInfo);
    root.init(analyzer);
    Preconditions.checkState(root.hasValidStats());
    // 2nd phase agginfo
    if (aggInfo.isDistinctAgg()) {
        ((AggregationNode) root).unsetNeedsFinalize();
        // The output of the 1st phase agg is the 1st phase intermediate.
        ((AggregationNode) root).setIntermediateTuple();
        root = new AggregationNode(ctx_.getNextNodeId(), root, aggInfo.getSecondPhaseDistinctAggInfo());
        root.init(analyzer);
        Preconditions.checkState(root.hasValidStats());
    }
    // add Having clause
    root.assignConjuncts(analyzer);
    return root;
}
#end_block

#method_before
private PlanNode createConstantSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
    ArrayList<Expr> resultExprs = selectStmt.getResultExprs();
    // Create tuple descriptor for materialized tuple.
    TupleDescriptor tupleDesc = createResultTupleDescriptor(selectStmt, "union", analyzer);
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
    // Analysis guarantees that selects without a FROM clause only have constant exprs.
    unionNode.addConstExprList(Lists.newArrayList(resultExprs));
    // Replace the select stmt's resultExprs with SlotRefs into tupleDesc.
    for (int i = 0; i < resultExprs.size(); ++i) {
        SlotRef slotRef = new SlotRef(tupleDesc.getSlots().get(i));
        resultExprs.set(i, slotRef);
    }
    // UnionNode.init() needs tupleDesc to have been initialized
    unionNode.init(ctx_.getRewriter(), analyzer);
    return unionNode;
}
#method_after
private PlanNode createConstantSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws InternalException {
    Preconditions.checkState(selectStmt.getTableRefs().isEmpty());
    List<Expr> resultExprs = selectStmt.getResultExprs();
    // Create tuple descriptor for materialized tuple.
    TupleDescriptor tupleDesc = createResultTupleDescriptor(selectStmt, "union", analyzer);
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
    // Analysis guarantees that selects without a FROM clause only have constant exprs.
    unionNode.addConstExprList(Lists.newArrayList(resultExprs));
    // Replace the select stmt's resultExprs with SlotRefs into tupleDesc.
    for (int i = 0; i < resultExprs.size(); ++i) {
        SlotRef slotRef = new SlotRef(tupleDesc.getSlots().get(i));
        resultExprs.set(i, slotRef);
    }
    // UnionNode.init() needs tupleDesc to have been initialized
    unionNode.init(analyzer);
    return unionNode;
}
#end_block

#method_before
private TupleDescriptor createResultTupleDescriptor(SelectStmt selectStmt, String debugName, Analyzer analyzer) {
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor(debugName);
    tupleDesc.setIsMaterialized(true);
    ArrayList<Expr> resultExprs = selectStmt.getResultExprs();
    ArrayList<String> colLabels = selectStmt.getColLabels();
    for (int i = 0; i < resultExprs.size(); ++i) {
        Expr resultExpr = resultExprs.get(i);
        String colLabel = colLabels.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(colLabel);
        slotDesc.setSourceExpr(resultExpr);
        slotDesc.setType(resultExpr.getType());
        slotDesc.setStats(ColumnStats.fromExpr(resultExpr));
        slotDesc.setIsMaterialized(true);
    }
    tupleDesc.computeMemLayout();
    return tupleDesc;
}
#method_after
private TupleDescriptor createResultTupleDescriptor(SelectStmt selectStmt, String debugName, Analyzer analyzer) {
    TupleDescriptor tupleDesc = analyzer.getDescTbl().createTupleDescriptor(debugName);
    tupleDesc.setIsMaterialized(true);
    List<Expr> resultExprs = selectStmt.getResultExprs();
    List<String> colLabels = selectStmt.getColLabels();
    for (int i = 0; i < resultExprs.size(); ++i) {
        Expr resultExpr = resultExprs.get(i);
        String colLabel = colLabels.get(i);
        SlotDescriptor slotDesc = analyzer.addSlotDescriptor(tupleDesc);
        slotDesc.setLabel(colLabel);
        slotDesc.setSourceExpr(resultExpr);
        slotDesc.setType(resultExpr.getType());
        slotDesc.setStats(ColumnStats.fromExpr(resultExpr));
        slotDesc.setIsMaterialized(true);
    }
    tupleDesc.computeMemLayout();
    return tupleDesc;
}
#end_block

#method_before
private PlanNode createInlineViewPlan(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    // If possible, "push down" view predicates; this is needed in order to ensure
    // that predicates such as "x + y = 10" are evaluated in the view's plan tree
    // rather than a SelectNode grafted on top of that plan tree.
    // This doesn't prevent predicate propagation, because predicates like
    // "x = 10" that get pushed down are still connected to equivalent slots
    // via the equality predicates created for the view's select list.
    // Include outer join conjuncts here as well because predicates from the
    // On-clause of an outer join may be pushed into the inline view as well.
    migrateConjunctsToInlineView(analyzer, inlineViewRef);
    // Turn a constant select into a UnionNode that materializes the exprs.
    // TODO: unify this with createConstantSelectPlan(), this is basically the
    // same thing
    QueryStmt viewStmt = inlineViewRef.getViewStmt();
    if (viewStmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) viewStmt;
        if (selectStmt.getTableRefs().isEmpty()) {
            if (inlineViewRef.getAnalyzer().hasEmptyResultSet()) {
                PlanNode emptySetNode = createEmptyNode(viewStmt, inlineViewRef.getAnalyzer());
                // Still substitute exprs in parent nodes with the inline-view's smap to make
                // sure no exprs reference the non-materialized inline view slots. No wrapping
                // with TupleIsNullPredicates is necessary here because we do not migrate
                // conjuncts into outer-joined inline views, so hasEmptyResultSet() cannot be
                // true for an outer-joined inline view that has no table refs.
                Preconditions.checkState(!analyzer.isOuterJoined(inlineViewRef.getId()));
                emptySetNode.setOutputSmap(inlineViewRef.getSmap());
                return emptySetNode;
            }
            // Analysis should have generated a tuple id into which to materialize the exprs.
            Preconditions.checkState(inlineViewRef.getMaterializedTupleIds().size() == 1);
            // we need to materialize all slots of our inline view tuple
            analyzer.getTupleDesc(inlineViewRef.getId()).materializeSlots();
            UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), inlineViewRef.getMaterializedTupleIds().get(0));
            if (analyzer.hasEmptyResultSet())
                return unionNode;
            unionNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
            unionNode.addConstExprList(selectStmt.getBaseTblResultExprs());
            unionNode.init(ctx_.getRewriter(), analyzer);
            return unionNode;
        }
    }
    PlanNode rootNode = createQueryPlan(inlineViewRef.getViewStmt(), inlineViewRef.getAnalyzer(), false);
    // TODO: we should compute the "physical layout" of the view's descriptor, so that
    // the avg row size is available during optimization; however, that means we need to
    // select references to its resultExprs from the enclosing scope(s)
    rootNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
    // The output smap is the composition of the inline view's smap and the output smap
    // of the inline view's plan root. This ensures that all downstream exprs referencing
    // the inline view are replaced with exprs referencing the physical output of the
    // inline view's plan.
    ExprSubstitutionMap outputSmap = ExprSubstitutionMap.compose(inlineViewRef.getSmap(), rootNode.getOutputSmap(), analyzer);
    if (analyzer.isOuterJoined(inlineViewRef.getId())) {
        // Exprs against non-matched rows of an outer join should always return NULL.
        // Make the rhs exprs of the output smap nullable, if necessary. This expr wrapping
        // must be performed on the composed smap, and not on the the inline view's smap,
        // because the rhs exprs must first be resolved against the physical output of
        // 'planRoot' to correctly determine whether wrapping is necessary.
        List<Expr> nullableRhs = TupleIsNullPredicate.wrapExprs(outputSmap.getRhs(), rootNode.getTupleIds(), analyzer);
        outputSmap = new ExprSubstitutionMap(outputSmap.getLhs(), nullableRhs);
    }
    // Set output smap of rootNode *before* creating a SelectNode for proper resolution.
    rootNode.setOutputSmap(outputSmap);
    // place.
    if (!canMigrateConjuncts(inlineViewRef)) {
        rootNode = addUnassignedConjuncts(analyzer, inlineViewRef.getDesc().getId().asList(), rootNode);
    }
    return rootNode;
}
#method_after
private PlanNode createInlineViewPlan(Analyzer analyzer, InlineViewRef inlineViewRef) throws ImpalaException {
    // If possible, "push down" view predicates; this is needed in order to ensure
    // that predicates such as "x + y = 10" are evaluated in the view's plan tree
    // rather than a SelectNode grafted on top of that plan tree.
    // This doesn't prevent predicate propagation, because predicates like
    // "x = 10" that get pushed down are still connected to equivalent slots
    // via the equality predicates created for the view's select list.
    // Include outer join conjuncts here as well because predicates from the
    // On-clause of an outer join may be pushed into the inline view as well.
    migrateConjunctsToInlineView(analyzer, inlineViewRef);
    // Turn a constant select into a UnionNode that materializes the exprs.
    // TODO: unify this with createConstantSelectPlan(), this is basically the
    // same thing
    QueryStmt viewStmt = inlineViewRef.getViewStmt();
    if (viewStmt instanceof SelectStmt) {
        SelectStmt selectStmt = (SelectStmt) viewStmt;
        if (selectStmt.getTableRefs().isEmpty()) {
            if (inlineViewRef.getAnalyzer().hasEmptyResultSet()) {
                PlanNode emptySetNode = createEmptyNode(viewStmt, inlineViewRef.getAnalyzer());
                // Still substitute exprs in parent nodes with the inline-view's smap to make
                // sure no exprs reference the non-materialized inline view slots. No wrapping
                // with TupleIsNullPredicates is necessary here because we do not migrate
                // conjuncts into outer-joined inline views, so hasEmptyResultSet() cannot be
                // true for an outer-joined inline view that has no table refs.
                Preconditions.checkState(!analyzer.isOuterJoined(inlineViewRef.getId()));
                emptySetNode.setOutputSmap(inlineViewRef.getSmap());
                return emptySetNode;
            }
            // Analysis should have generated a tuple id into which to materialize the exprs.
            Preconditions.checkState(inlineViewRef.getMaterializedTupleIds().size() == 1);
            // we need to materialize all slots of our inline view tuple
            analyzer.getTupleDesc(inlineViewRef.getId()).materializeSlots();
            UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), inlineViewRef.getMaterializedTupleIds().get(0));
            if (analyzer.hasEmptyResultSet())
                return unionNode;
            unionNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
            unionNode.addConstExprList(selectStmt.getBaseTblResultExprs());
            unionNode.init(analyzer);
            return unionNode;
        }
    }
    PlanNode rootNode = createQueryPlan(inlineViewRef.getViewStmt(), inlineViewRef.getAnalyzer(), false);
    // TODO: we should compute the "physical layout" of the view's descriptor, so that
    // the avg row size is available during optimization; however, that means we need to
    // select references to its resultExprs from the enclosing scope(s)
    rootNode.setTblRefIds(Lists.newArrayList(inlineViewRef.getId()));
    // The output smap is the composition of the inline view's smap and the output smap
    // of the inline view's plan root. This ensures that all downstream exprs referencing
    // the inline view are replaced with exprs referencing the physical output of the
    // inline view's plan.
    ExprSubstitutionMap outputSmap = ExprSubstitutionMap.compose(inlineViewRef.getSmap(), rootNode.getOutputSmap(), analyzer);
    if (analyzer.isOuterJoined(inlineViewRef.getId())) {
        // Exprs against non-matched rows of an outer join should always return NULL.
        // Make the rhs exprs of the output smap nullable, if necessary. This expr wrapping
        // must be performed on the composed smap, and not on the the inline view's smap,
        // because the rhs exprs must first be resolved against the physical output of
        // 'planRoot' to correctly determine whether wrapping is necessary.
        List<Expr> nullableRhs = TupleIsNullPredicate.wrapExprs(outputSmap.getRhs(), rootNode.getTupleIds(), analyzer);
        outputSmap = new ExprSubstitutionMap(outputSmap.getLhs(), nullableRhs);
    }
    // Set output smap of rootNode *before* creating a SelectNode for proper resolution.
    rootNode.setOutputSmap(outputSmap);
    // place.
    if (!canMigrateConjuncts(inlineViewRef)) {
        rootNode = addUnassignedConjuncts(analyzer, inlineViewRef.getDesc().getId().asList(), rootNode);
    }
    return rootNode;
}
#end_block

#method_before
private PlanNode createHdfsScanPlan(TableRef hdfsTblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    HdfsTable hdfsTable = (HdfsTable) hdfsTblRef.getTable();
    TupleDescriptor tupleDesc = hdfsTblRef.getDesc();
    // Get all predicates bound by the tuple.
    List<Expr> conjuncts = Lists.newArrayList();
    conjuncts.addAll(analyzer.getBoundPredicates(tupleDesc.getId()));
    // Also add remaining unassigned conjuncts
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(tupleDesc.getId().asList());
    conjuncts.addAll(unassigned);
    analyzer.markConjunctsAssigned(unassigned);
    analyzer.createEquivConjuncts(tupleDesc.getId(), conjuncts);
    // Do partition pruning before deciding which slots to materialize,
    // We might end up removing some predicates.
    HdfsPartitionPruner pruner = new HdfsPartitionPruner(tupleDesc);
    List<HdfsPartition> partitions = pruner.prunePartitions(analyzer, conjuncts, false);
    // Mark all slots referenced by the remaining conjuncts as materialized.
    analyzer.materializeSlots(conjuncts);
    // try evaluating with metadata first. If not, fall back to scanning.
    if (fastPartitionKeyScans && tupleDesc.hasClusteringColsOnly()) {
        HashSet<List<Expr>> uniqueExprs = new HashSet<List<Expr>>();
        for (HdfsPartition partition : partitions) {
            // Ignore empty partitions to match the behavior of the scan based approach.
            if (partition.isDefaultPartition() || partition.getSize() == 0) {
                continue;
            }
            List<Expr> exprs = Lists.newArrayList();
            for (SlotDescriptor slotDesc : tupleDesc.getSlots()) {
                // slots, use dummy null values. UnionNode will filter out unmaterialized slots.
                if (!slotDesc.isMaterialized()) {
                    exprs.add(NullLiteral.create(slotDesc.getType()));
                } else {
                    int pos = slotDesc.getColumn().getPosition();
                    exprs.add(partition.getPartitionValue(pos));
                }
            }
            uniqueExprs.add(exprs);
        }
        // Create a UNION node with all unique partition keys.
        UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
        for (List<Expr> exprList : uniqueExprs) {
            unionNode.addConstExprList(exprList);
        }
        unionNode.init(ctx_.getRewriter(), analyzer);
        return unionNode;
    } else {
        ScanNode scanNode = new HdfsScanNode(ctx_.getNextNodeId(), tupleDesc, conjuncts, partitions, hdfsTblRef);
        scanNode.init(ctx_.getRewriter(), analyzer);
        return scanNode;
    }
}
#method_after
private PlanNode createHdfsScanPlan(TableRef hdfsTblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    HdfsTable hdfsTable = (HdfsTable) hdfsTblRef.getTable();
    TupleDescriptor tupleDesc = hdfsTblRef.getDesc();
    // Get all predicates bound by the tuple.
    List<Expr> conjuncts = Lists.newArrayList();
    conjuncts.addAll(analyzer.getBoundPredicates(tupleDesc.getId()));
    // Also add remaining unassigned conjuncts
    List<Expr> unassigned = analyzer.getUnassignedConjuncts(tupleDesc.getId().asList());
    conjuncts.addAll(unassigned);
    analyzer.markConjunctsAssigned(unassigned);
    analyzer.createEquivConjuncts(tupleDesc.getId(), conjuncts);
    // Do partition pruning before deciding which slots to materialize,
    // We might end up removing some predicates.
    HdfsPartitionPruner pruner = new HdfsPartitionPruner(tupleDesc);
    List<HdfsPartition> partitions = pruner.prunePartitions(analyzer, conjuncts, false);
    // Mark all slots referenced by the remaining conjuncts as materialized.
    analyzer.materializeSlots(conjuncts);
    // try evaluating with metadata first. If not, fall back to scanning.
    if (fastPartitionKeyScans && tupleDesc.hasClusteringColsOnly()) {
        HashSet<List<Expr>> uniqueExprs = new HashSet<List<Expr>>();
        for (HdfsPartition partition : partitions) {
            // Ignore empty partitions to match the behavior of the scan based approach.
            if (partition.isDefaultPartition() || partition.getSize() == 0) {
                continue;
            }
            List<Expr> exprs = Lists.newArrayList();
            for (SlotDescriptor slotDesc : tupleDesc.getSlots()) {
                // slots, use dummy null values. UnionNode will filter out unmaterialized slots.
                if (!slotDesc.isMaterialized()) {
                    exprs.add(NullLiteral.create(slotDesc.getType()));
                } else {
                    int pos = slotDesc.getColumn().getPosition();
                    exprs.add(partition.getPartitionValue(pos));
                }
            }
            uniqueExprs.add(exprs);
        }
        // Create a UNION node with all unique partition keys.
        UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), tupleDesc.getId());
        for (List<Expr> exprList : uniqueExprs) {
            unionNode.addConstExprList(exprList);
        }
        unionNode.init(analyzer);
        return unionNode;
    } else {
        ScanNode scanNode = new HdfsScanNode(ctx_.getNextNodeId(), tupleDesc, conjuncts, partitions, hdfsTblRef);
        scanNode.init(analyzer);
        return scanNode;
    }
}
#end_block

#method_before
private PlanNode createScanNode(TableRef tblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    ScanNode scanNode = null;
    Table table = tblRef.getTable();
    if (table instanceof HdfsTable) {
        return createHdfsScanPlan(tblRef, fastPartitionKeyScans, analyzer);
    } else if (table instanceof DataSourceTable) {
        scanNode = new DataSourceScanNode(ctx_.getNextNodeId(), tblRef.getDesc());
        scanNode.init(ctx_.getRewriter(), analyzer);
        return scanNode;
    } else if (table instanceof HBaseTable) {
        // HBase table
        scanNode = new HBaseScanNode(ctx_.getNextNodeId(), tblRef.getDesc());
    } else if (tblRef.getTable() instanceof KuduTable) {
        scanNode = new KuduScanNode(ctx_.getNextNodeId(), tblRef.getDesc());
        scanNode.init(ctx_.getRewriter(), analyzer);
        return scanNode;
    } else {
        throw new NotImplementedException("Planning not implemented for table ref class: " + tblRef.getClass());
    }
    // TODO: move this to HBaseScanNode.init();
    Preconditions.checkState(scanNode instanceof HBaseScanNode);
    List<Expr> conjuncts = analyzer.getUnassignedConjuncts(scanNode);
    // mark conjuncts_ assigned here; they will either end up inside a
    // ValueRange or will be evaluated directly by the node
    analyzer.markConjunctsAssigned(conjuncts);
    List<ValueRange> keyRanges = Lists.newArrayList();
    // determine scan predicates for clustering cols
    for (int i = 0; i < tblRef.getTable().getNumClusteringCols(); ++i) {
        SlotDescriptor slotDesc = analyzer.getColumnSlot(tblRef.getDesc(), tblRef.getTable().getColumns().get(i));
        if (slotDesc == null || !slotDesc.getType().isStringType()) {
            // the hbase row key is mapped to a non-string type
            // (since it's stored in ascii it will be lexicographically ordered,
            // and non-string comparisons won't work)
            keyRanges.add(null);
        } else {
            // create ValueRange from conjuncts_ for slot; also removes conjuncts_ that were
            // used as input for filter
            keyRanges.add(createHBaseValueRange(slotDesc, conjuncts));
        }
    }
    ((HBaseScanNode) scanNode).setKeyRanges(keyRanges);
    scanNode.addConjuncts(conjuncts);
    scanNode.init(ctx_.getRewriter(), analyzer);
    return scanNode;
}
#method_after
private PlanNode createScanNode(TableRef tblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    ScanNode scanNode = null;
    Table table = tblRef.getTable();
    if (table instanceof HdfsTable) {
        return createHdfsScanPlan(tblRef, fastPartitionKeyScans, analyzer);
    } else if (table instanceof DataSourceTable) {
        scanNode = new DataSourceScanNode(ctx_.getNextNodeId(), tblRef.getDesc());
        scanNode.init(analyzer);
        return scanNode;
    } else if (table instanceof HBaseTable) {
        // HBase table
        scanNode = new HBaseScanNode(ctx_.getNextNodeId(), tblRef.getDesc());
    } else if (tblRef.getTable() instanceof KuduTable) {
        scanNode = new KuduScanNode(ctx_.getNextNodeId(), tblRef.getDesc());
        scanNode.init(analyzer);
        return scanNode;
    } else {
        throw new NotImplementedException("Planning not implemented for table ref class: " + tblRef.getClass());
    }
    // TODO: move this to HBaseScanNode.init();
    Preconditions.checkState(scanNode instanceof HBaseScanNode);
    List<Expr> conjuncts = analyzer.getUnassignedConjuncts(scanNode);
    // mark conjuncts_ assigned here; they will either end up inside a
    // ValueRange or will be evaluated directly by the node
    analyzer.markConjunctsAssigned(conjuncts);
    List<ValueRange> keyRanges = Lists.newArrayList();
    // determine scan predicates for clustering cols
    for (int i = 0; i < tblRef.getTable().getNumClusteringCols(); ++i) {
        SlotDescriptor slotDesc = analyzer.getColumnSlot(tblRef.getDesc(), tblRef.getTable().getColumns().get(i));
        if (slotDesc == null || !slotDesc.getType().isStringType()) {
            // the hbase row key is mapped to a non-string type
            // (since it's stored in ascii it will be lexicographically ordered,
            // and non-string comparisons won't work)
            keyRanges.add(null);
        } else {
            // create ValueRange from conjuncts_ for slot; also removes conjuncts_ that were
            // used as input for filter
            keyRanges.add(createHBaseValueRange(slotDesc, conjuncts));
        }
    }
    ((HBaseScanNode) scanNode).setKeyRanges(keyRanges);
    scanNode.addConjuncts(conjuncts);
    scanNode.init(analyzer);
    return scanNode;
}
#end_block

#method_before
private PlanNode createJoinNode(PlanNode outer, PlanNode inner, TableRef innerRef, Analyzer analyzer) throws ImpalaException {
    // get eq join predicates for the TableRefs' ids (not the PlanNodes' ids, which
    // are materialized)
    List<BinaryPredicate> eqJoinConjuncts = getHashLookupJoinConjuncts(outer.getTblRefIds(), inner.getTblRefIds(), analyzer);
    // Outer joins should only use On-clause predicates as eqJoinConjuncts.
    if (!innerRef.getJoinOp().isOuterJoin()) {
        analyzer.createEquivConjuncts(outer.getTblRefIds(), inner.getTblRefIds(), eqJoinConjuncts);
    }
    if (!eqJoinConjuncts.isEmpty() && innerRef.getJoinOp() == JoinOperator.CROSS_JOIN) {
        innerRef.setJoinOp(JoinOperator.INNER_JOIN);
    }
    List<Expr> otherJoinConjuncts = Lists.newArrayList();
    if (innerRef.getJoinOp().isOuterJoin()) {
        // Also assign conjuncts from On clause. All remaining unassigned conjuncts
        // that can be evaluated by this join are assigned in createSelectPlan().
        otherJoinConjuncts = analyzer.getUnassignedOjConjuncts(innerRef);
    } else if (innerRef.getJoinOp().isSemiJoin()) {
        // Unassigned conjuncts bound by the invisible tuple id of a semi join must have
        // come from the join's On-clause, and therefore, must be added to the other join
        // conjuncts to produce correct results.
        // TODO This doesn't handle predicates specified in the On clause which are not
        // bound by any tuple id (e.g. ON (true))
        List<TupleId> tblRefIds = Lists.newArrayList(outer.getTblRefIds());
        tblRefIds.addAll(inner.getTblRefIds());
        otherJoinConjuncts = analyzer.getUnassignedConjuncts(tblRefIds, false);
        if (innerRef.getJoinOp().isNullAwareLeftAntiJoin()) {
            boolean hasNullMatchingEqOperator = false;
            // Keep only the null-matching eq conjunct in the eqJoinConjuncts and move
            // all the others in otherJoinConjuncts. The BE relies on this
            // separation for correct execution of the null-aware left anti join.
            Iterator<BinaryPredicate> it = eqJoinConjuncts.iterator();
            while (it.hasNext()) {
                BinaryPredicate conjunct = it.next();
                if (!conjunct.isNullMatchingEq()) {
                    otherJoinConjuncts.add(conjunct);
                    it.remove();
                } else {
                    // Only one null-matching eq conjunct is allowed
                    Preconditions.checkState(!hasNullMatchingEqOperator);
                    hasNullMatchingEqOperator = true;
                }
            }
            Preconditions.checkState(hasNullMatchingEqOperator);
        }
    }
    analyzer.markConjunctsAssigned(otherJoinConjuncts);
    // Use a nested-loop join if there are no equi-join conjuncts, or if the inner
    // (build side) is a singular row src. A singular row src has a cardinality of 1, so
    // a nested-loop join is certainly cheaper than a hash join.
    JoinNode result = null;
    Preconditions.checkState(!innerRef.getJoinOp().isNullAwareLeftAntiJoin() || !(inner instanceof SingularRowSrcNode));
    if (eqJoinConjuncts.isEmpty() || inner instanceof SingularRowSrcNode) {
        otherJoinConjuncts.addAll(eqJoinConjuncts);
        result = new NestedLoopJoinNode(outer, inner, analyzer.isStraightJoin(), innerRef.getDistributionMode(), innerRef.getJoinOp(), otherJoinConjuncts);
    } else {
        result = new HashJoinNode(outer, inner, analyzer.isStraightJoin(), innerRef.getDistributionMode(), innerRef.getJoinOp(), eqJoinConjuncts, otherJoinConjuncts);
    }
    result.init(ctx_.getRewriter(), analyzer);
    return result;
}
#method_after
private PlanNode createJoinNode(PlanNode outer, PlanNode inner, TableRef innerRef, Analyzer analyzer) throws ImpalaException {
    // get eq join predicates for the TableRefs' ids (not the PlanNodes' ids, which
    // are materialized)
    List<BinaryPredicate> eqJoinConjuncts = getHashLookupJoinConjuncts(outer.getTblRefIds(), inner.getTblRefIds(), analyzer);
    // Outer joins should only use On-clause predicates as eqJoinConjuncts.
    if (!innerRef.getJoinOp().isOuterJoin()) {
        analyzer.createEquivConjuncts(outer.getTblRefIds(), inner.getTblRefIds(), eqJoinConjuncts);
    }
    if (!eqJoinConjuncts.isEmpty() && innerRef.getJoinOp() == JoinOperator.CROSS_JOIN) {
        innerRef.setJoinOp(JoinOperator.INNER_JOIN);
    }
    List<Expr> otherJoinConjuncts = Lists.newArrayList();
    if (innerRef.getJoinOp().isOuterJoin()) {
        // Also assign conjuncts from On clause. All remaining unassigned conjuncts
        // that can be evaluated by this join are assigned in createSelectPlan().
        otherJoinConjuncts = analyzer.getUnassignedOjConjuncts(innerRef);
    } else if (innerRef.getJoinOp().isSemiJoin()) {
        // Unassigned conjuncts bound by the invisible tuple id of a semi join must have
        // come from the join's On-clause, and therefore, must be added to the other join
        // conjuncts to produce correct results.
        // TODO This doesn't handle predicates specified in the On clause which are not
        // bound by any tuple id (e.g. ON (true))
        List<TupleId> tblRefIds = Lists.newArrayList(outer.getTblRefIds());
        tblRefIds.addAll(inner.getTblRefIds());
        otherJoinConjuncts = analyzer.getUnassignedConjuncts(tblRefIds, false);
        if (innerRef.getJoinOp().isNullAwareLeftAntiJoin()) {
            boolean hasNullMatchingEqOperator = false;
            // Keep only the null-matching eq conjunct in the eqJoinConjuncts and move
            // all the others in otherJoinConjuncts. The BE relies on this
            // separation for correct execution of the null-aware left anti join.
            Iterator<BinaryPredicate> it = eqJoinConjuncts.iterator();
            while (it.hasNext()) {
                BinaryPredicate conjunct = it.next();
                if (!conjunct.isNullMatchingEq()) {
                    otherJoinConjuncts.add(conjunct);
                    it.remove();
                } else {
                    // Only one null-matching eq conjunct is allowed
                    Preconditions.checkState(!hasNullMatchingEqOperator);
                    hasNullMatchingEqOperator = true;
                }
            }
            Preconditions.checkState(hasNullMatchingEqOperator);
        }
    }
    analyzer.markConjunctsAssigned(otherJoinConjuncts);
    // Use a nested-loop join if there are no equi-join conjuncts, or if the inner
    // (build side) is a singular row src. A singular row src has a cardinality of 1, so
    // a nested-loop join is certainly cheaper than a hash join.
    JoinNode result = null;
    Preconditions.checkState(!innerRef.getJoinOp().isNullAwareLeftAntiJoin() || !(inner instanceof SingularRowSrcNode));
    if (eqJoinConjuncts.isEmpty() || inner instanceof SingularRowSrcNode) {
        otherJoinConjuncts.addAll(eqJoinConjuncts);
        result = new NestedLoopJoinNode(outer, inner, analyzer.isStraightJoin(), innerRef.getDistributionMode(), innerRef.getJoinOp(), otherJoinConjuncts);
    } else {
        result = new HashJoinNode(outer, inner, analyzer.isStraightJoin(), innerRef.getDistributionMode(), innerRef.getJoinOp(), eqJoinConjuncts, otherJoinConjuncts);
    }
    result.init(analyzer);
    return result;
}
#end_block

#method_before
private PlanNode createTableRefNode(TableRef tblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    PlanNode result = null;
    if (tblRef instanceof BaseTableRef) {
        result = createScanNode(tblRef, fastPartitionKeyScans, analyzer);
    } else if (tblRef instanceof CollectionTableRef) {
        if (tblRef.isRelative()) {
            Preconditions.checkState(ctx_.hasSubplan());
            result = new UnnestNode(ctx_.getNextNodeId(), ctx_.getSubplan(), (CollectionTableRef) tblRef);
            result.init(ctx_.getRewriter(), analyzer);
        } else {
            result = createScanNode(tblRef, false, analyzer);
        }
    } else if (tblRef instanceof InlineViewRef) {
        result = createInlineViewPlan(analyzer, (InlineViewRef) tblRef);
    } else if (tblRef instanceof SingularRowSrcTableRef) {
        Preconditions.checkState(ctx_.hasSubplan());
        result = new SingularRowSrcNode(ctx_.getNextNodeId(), ctx_.getSubplan());
        result.init(ctx_.getRewriter(), analyzer);
    } else {
        throw new NotImplementedException("Planning not implemented for table ref class: " + tblRef.getClass());
    }
    return result;
}
#method_after
private PlanNode createTableRefNode(TableRef tblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    PlanNode result = null;
    if (tblRef instanceof BaseTableRef) {
        result = createScanNode(tblRef, fastPartitionKeyScans, analyzer);
    } else if (tblRef instanceof CollectionTableRef) {
        if (tblRef.isRelative()) {
            Preconditions.checkState(ctx_.hasSubplan());
            result = new UnnestNode(ctx_.getNextNodeId(), ctx_.getSubplan(), (CollectionTableRef) tblRef);
            result.init(analyzer);
        } else {
            result = createScanNode(tblRef, false, analyzer);
        }
    } else if (tblRef instanceof InlineViewRef) {
        result = createInlineViewPlan(analyzer, (InlineViewRef) tblRef);
    } else if (tblRef instanceof SingularRowSrcTableRef) {
        Preconditions.checkState(ctx_.hasSubplan());
        result = new SingularRowSrcNode(ctx_.getNextNodeId(), ctx_.getSubplan());
        result.init(analyzer);
    } else {
        throw new NotImplementedException("Planning not implemented for table ref class: " + tblRef.getClass());
    }
    return result;
}
#end_block

#method_before
private UnionNode createUnionPlan(Analyzer analyzer, UnionStmt unionStmt, List<UnionOperand> unionOperands, PlanNode unionDistinctPlan) throws ImpalaException {
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), unionStmt.getTupleId());
    for (UnionOperand op : unionOperands) {
        if (op.getAnalyzer().hasEmptyResultSet()) {
            unmarkCollectionSlots(op.getQueryStmt());
            continue;
        }
        QueryStmt queryStmt = op.getQueryStmt();
        if (queryStmt instanceof SelectStmt) {
            SelectStmt selectStmt = (SelectStmt) queryStmt;
            if (selectStmt.getTableRefs().isEmpty()) {
                unionNode.addConstExprList(selectStmt.getResultExprs());
                continue;
            }
        }
        PlanNode opPlan = createQueryPlan(queryStmt, op.getAnalyzer(), false);
        // There may still be unassigned conjuncts if the operand has an order by + limit.
        // Place them into a SelectNode on top of the operand's plan.
        opPlan = addUnassignedConjuncts(analyzer, opPlan.getTupleIds(), opPlan);
        if (opPlan instanceof EmptySetNode)
            continue;
        unionNode.addChild(opPlan, op.getQueryStmt().getResultExprs());
    }
    if (unionDistinctPlan != null) {
        Preconditions.checkState(unionStmt.hasDistinctOps());
        Preconditions.checkState(unionDistinctPlan instanceof AggregationNode);
        unionNode.addChild(unionDistinctPlan, unionStmt.getDistinctAggInfo().getGroupingExprs());
    }
    unionNode.init(ctx_.getRewriter(), analyzer);
    return unionNode;
}
#method_after
private UnionNode createUnionPlan(Analyzer analyzer, UnionStmt unionStmt, List<UnionOperand> unionOperands, PlanNode unionDistinctPlan) throws ImpalaException {
    UnionNode unionNode = new UnionNode(ctx_.getNextNodeId(), unionStmt.getTupleId());
    for (UnionOperand op : unionOperands) {
        if (op.getAnalyzer().hasEmptyResultSet()) {
            unmarkCollectionSlots(op.getQueryStmt());
            continue;
        }
        QueryStmt queryStmt = op.getQueryStmt();
        if (queryStmt instanceof SelectStmt) {
            SelectStmt selectStmt = (SelectStmt) queryStmt;
            if (selectStmt.getTableRefs().isEmpty()) {
                unionNode.addConstExprList(selectStmt.getResultExprs());
                continue;
            }
        }
        PlanNode opPlan = createQueryPlan(queryStmt, op.getAnalyzer(), false);
        // There may still be unassigned conjuncts if the operand has an order by + limit.
        // Place them into a SelectNode on top of the operand's plan.
        opPlan = addUnassignedConjuncts(analyzer, opPlan.getTupleIds(), opPlan);
        if (opPlan instanceof EmptySetNode)
            continue;
        unionNode.addChild(opPlan, op.getQueryStmt().getResultExprs());
    }
    if (unionDistinctPlan != null) {
        Preconditions.checkState(unionStmt.hasDistinctOps());
        Preconditions.checkState(unionDistinctPlan instanceof AggregationNode);
        unionNode.addChild(unionDistinctPlan, unionStmt.getDistinctAggInfo().getGroupingExprs());
    }
    unionNode.init(analyzer);
    return unionNode;
}
#end_block

#method_before
private PlanNode createUnionPlan(UnionStmt unionStmt, Analyzer analyzer) throws ImpalaException {
    List<Expr> conjuncts = analyzer.getUnassignedConjuncts(unionStmt.getTupleId().asList(), false);
    if (!unionStmt.hasAnalyticExprs()) {
        // pick up propagated predicates.
        for (UnionOperand op : unionStmt.getOperands()) {
            List<Expr> opConjuncts = Expr.substituteList(conjuncts, op.getSmap(), analyzer, false);
            op.getAnalyzer().registerConjuncts(opConjuncts);
        }
        analyzer.markConjunctsAssigned(conjuncts);
    } else {
        // mark slots referenced by the yet-unassigned conjuncts
        analyzer.materializeSlots(conjuncts);
    }
    // mark slots after predicate propagation but prior to plan tree generation
    unionStmt.materializeRequiredSlots(analyzer);
    PlanNode result = null;
    // create DISTINCT tree
    if (unionStmt.hasDistinctOps()) {
        result = createUnionPlan(analyzer, unionStmt, unionStmt.getDistinctOperands(), null);
        result = new AggregationNode(ctx_.getNextNodeId(), result, unionStmt.getDistinctAggInfo());
        result.init(ctx_.getRewriter(), analyzer);
    }
    // create ALL tree
    if (unionStmt.hasAllOps()) {
        result = createUnionPlan(analyzer, unionStmt, unionStmt.getAllOperands(), result);
    }
    if (unionStmt.hasAnalyticExprs()) {
        result = addUnassignedConjuncts(analyzer, unionStmt.getTupleId().asList(), result);
    }
    return result;
}
#method_after
private PlanNode createUnionPlan(UnionStmt unionStmt, Analyzer analyzer) throws ImpalaException {
    List<Expr> conjuncts = analyzer.getUnassignedConjuncts(unionStmt.getTupleId().asList(), false);
    if (!unionStmt.hasAnalyticExprs()) {
        // pick up propagated predicates.
        for (UnionOperand op : unionStmt.getOperands()) {
            List<Expr> opConjuncts = Expr.substituteList(conjuncts, op.getSmap(), analyzer, false);
            op.getAnalyzer().registerConjuncts(opConjuncts);
        }
        analyzer.markConjunctsAssigned(conjuncts);
    } else {
        // mark slots referenced by the yet-unassigned conjuncts
        analyzer.materializeSlots(conjuncts);
    }
    // mark slots after predicate propagation but prior to plan tree generation
    unionStmt.materializeRequiredSlots(analyzer);
    PlanNode result = null;
    // create DISTINCT tree
    if (unionStmt.hasDistinctOps()) {
        result = createUnionPlan(analyzer, unionStmt, unionStmt.getDistinctOperands(), null);
        result = new AggregationNode(ctx_.getNextNodeId(), result, unionStmt.getDistinctAggInfo());
        result.init(analyzer);
    }
    // create ALL tree
    if (unionStmt.hasAllOps()) {
        result = createUnionPlan(analyzer, unionStmt, unionStmt.getAllOperands(), result);
    }
    if (unionStmt.hasAnalyticExprs()) {
        result = addUnassignedConjuncts(analyzer, unionStmt.getTupleId().asList(), result);
    }
    return result;
}
#end_block

#method_before
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='" + long_property_value + "') " + "tblproperties ('" + long_property_key + "'='" + long_property_value + "')");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("create table new_table (i int) " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='value')", "Serde property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('key'='" + long_property_value + "')", "Serde property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
    }
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    for (String format : fileFormats) {
        for (String create : ImmutableList.of("create table", "create external table")) {
            AnalyzesOk(String.format("%s new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", create, format));
            // No column definitions.
            AnalysisError(String.format("%s new_table " + "partitioned by (d decimal) comment 'c' stored as %s", create, format), "Table requires at least 1 column");
        }
        AnalysisError(String.format("create table t (i int primary key) stored as %s", format), "Only Kudu tables can specify a PRIMARY KEY");
        AnalysisError(String.format("create table t (i int, primary key(i)) stored as %s", format), "Only Kudu tables can specify a PRIMARY KEY");
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    // IMPALA-2251: it should not be possible to create text tables with the same
    // delimiter character used for multiple purposes.
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\001' lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\012'", "Field delimiter and line delimiter have same value: byte 10");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by '\001'", "Field delimiter and escape character have same value: byte 1. " + "Escape character will be ignored");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by 'x' lines terminated by 'x'", "Line delimiter and escape character have same value: byte 120. " + "Escape character will be ignored");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: i");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    AnalysisError("create table cached_tbl(i int) location " + "'file:///test-warehouse/cache_tbl' cached in 'testPool'", "Location 'file:/test-warehouse/cache_tbl' cannot be cached. " + "Please retry without caching: CREATE TABLE ... UNCACHED");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Test HMS constraint on type name length.
    AnalyzesOk(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH)));
    AnalysisError(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH + 1)), "Type of column 'i' exceeds maximum type length of 4000 characters:");
    // Test HMS constraint on comment length.
    AnalyzesOk(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH)));
    AnalysisError(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH + 1)), "Comment of column 'i' exceeds maximum length of 256 characters:");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#method_after
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='" + long_property_value + "') " + "tblproperties ('" + long_property_key + "'='" + long_property_value + "')");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("create table new_table (i int) " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='value')", "Serde property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('key'='" + long_property_value + "')", "Serde property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
    }
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    String[] fileFormatsStr = { "TEXT", "SEQUENCE_FILE", "PARQUET", "PARQUET", "RC_FILE" };
    int formatIndx = 0;
    for (String format : fileFormats) {
        for (String create : ImmutableList.of("create table", "create external table")) {
            AnalyzesOk(String.format("%s new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", create, format));
            // No column definitions.
            AnalysisError(String.format("%s new_table " + "partitioned by (d decimal) comment 'c' stored as %s", create, format), "Table requires at least 1 column");
        }
        AnalysisError(String.format("create table t (i int primary key) stored as %s", format), String.format("Unsupported column options for file format " + "'%s': 'i INT PRIMARY KEY'", fileFormatsStr[formatIndx]));
        AnalysisError(String.format("create table t (i int, primary key(i)) stored as %s", format), "Only Kudu tables can specify a PRIMARY KEY");
        formatIndx++;
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    // IMPALA-2251: it should not be possible to create text tables with the same
    // delimiter character used for multiple purposes.
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\001' lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\012'", "Field delimiter and line delimiter have same value: byte 10");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by '\001'", "Field delimiter and escape character have same value: byte 1. " + "Escape character will be ignored");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by 'x' lines terminated by 'x'", "Line delimiter and escape character have same value: byte 120. " + "Escape character will be ignored");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: i");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    AnalysisError("create table cached_tbl(i int) location " + "'file:///test-warehouse/cache_tbl' cached in 'testPool'", "Location 'file:/test-warehouse/cache_tbl' cannot be cached. " + "Please retry without caching: CREATE TABLE ... UNCACHED");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Test HMS constraint on type name length.
    AnalyzesOk(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH)));
    AnalysisError(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH + 1)), "Type of column 'i' exceeds maximum type length of 4000 characters:");
    // Test HMS constraint on comment length.
    AnalyzesOk(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH)));
    AnalysisError(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH + 1)), "Comment of column 'i' exceeds maximum length of 256 characters:");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#end_block

#method_before
@Test
public void TestCreateManagedKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Test primary keys and distribute by clauses
    AnalyzesOk("create table tab (x int primary key) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, primary key(x)) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) " + "distribute by hash(x, y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x)) " + "distribute by hash(x) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) " + "distribute by hash(y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y string, primary key (x)) distribute by " + "hash (x) into 3 buckets, range (x) (partition values < 1, partition " + "1 <= values < 10, partition 10 <= values < 20, partition value = 30) " + "stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) distribute by " + "range (x, y) (partition value = (2001, 1), partition value = (2002, 1), " + "partition value = (2003, 2)) stored as kudu");
    // Non-literal boundary values in range partitions
    AnalyzesOk("create table tab (x int, y int, primary key (x)) distribute by " + "range (x) (partition values < 1 + 1, partition (1+3) + 2 < values < 10, " + "partition factorial(4) < values < factorial(5), " + "partition value = factorial(6)) stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) distribute by " + "range(x, y) (partition value = (1+1, 2+2), partition value = ((1+1+1)+1, 10), " + "partition value = (cast (30 as int), factorial(5))) stored as kudu");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values < x + 1) stored as kudu", "Only constant values are allowed " + "for range-partition bounds: x + 1");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= isnull(null, null)) stored as kudu", "Range partition " + "values cannot be NULL. Range partition: 'PARTITION VALUES <= " + "isnull(NULL, NULL)'");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= (select count(*) from functional.alltypestiny)) " + "stored as kudu", "Only constant values are allowed for range-partition " + "bounds: (SELECT count(*) FROM functional.alltypestiny)");
    // Multilevel partitioning. Data is split into 3 buckets based on 'x' and each
    // bucket is partitioned into 4 tablets based on the range partitions of 'y'.
    AnalyzesOk("create table tab (x int, y string, primary key(x, y)) " + "distribute by hash(x) into 3 buckets, range(y) " + "(partition values < 'aa', partition 'aa' <= values < 'bb', " + "partition 'bb' <= values < 'cc', partition 'cc' <= values) " + "stored as kudu");
    // Key column in upper case
    AnalyzesOk("create table tab (x int, y int, primary key (X)) " + "distribute by hash (x) into 8 buckets stored as kudu");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b) into 8 buckets, hash(c) into 2 buckets stored as " + "kudu");
    // No columns specified in the DISTRIBUTE BY HASH clause
    AnalyzesOk("create table tab (a int primary key, b int, c int, d int) " + "distribute by hash into 8 buckets stored as kudu");
    // Distribute range data types are picked up during analysis and forwarded to Kudu.
    // Column names in distribute params should also be case-insensitive.
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key(a, b, c, d))" + "distribute by hash (a, B, c) into 8 buckets, " + "range (A) (partition values < 1, partition 1 <= values < 2, " + "partition 2 <= values < 3, partition 3 <= values < 4, partition 4 <= values) " + "stored as kudu");
    // Allowing range distribution on a subset of the primary keys
    AnalyzesOk("create table tab (id int, name string, valf float, vali bigint, " + "primary key (id, name)) distribute by range (name) " + "(partition 'aa' < values <= 'bb') stored as kudu");
    // Null values in range partition values
    AnalysisError("create table tab (id int, name string, primary key(id, name)) " + "distribute by hash (id) into 3 buckets, range (name) " + "(partition value = null, partition value = 1) stored as kudu", "Range partition values cannot be NULL. Range partition: 'PARTITION " + "VALUE = NULL'");
    // Primary key specified in tblproperties
    AnalysisError(String.format("create table tab (x int) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('%s' = 'x')", KuduTable.KEY_KEY_COLUMNS), "PRIMARY KEY must be used instead of the table " + "property");
    // Primary key column that doesn't exist
    AnalysisError("create table tab (x int, y int, primary key (z)) " + "distribute by hash (x) into 8 buckets stored as kudu", "PRIMARY KEY column 'z' does not exist in the table");
    // Invalid composite primary key
    AnalysisError("create table tab (x int primary key, primary key(x)) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    AnalysisError("create table tab (x int primary key, y int primary key) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    // Specifying the same primary key column multiple times
    AnalysisError("create table tab (x int, primary key (x, x)) distribute by hash (x) " + "into 8 buckets stored as kudu", "Column 'x' is listed multiple times as a PRIMARY KEY.");
    // Number of range partition boundary values should be equal to the number of range
    // columns.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by range(a) (partition value = (1, 2), " + "partition value = 3, partition value = 4) stored as kudu", "Number of specified range partition values is different than the number of " + "distribution columns: (2 vs 1). Range partition: 'PARTITION VALUE = (1,2)'");
    // Key ranges must match the column types.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by hash (a, b, c) into 8 buckets, range (a) " + "(partition value = 1, partition value = 'abc', partition 3 <= values) " + "stored as kudu", "Range partition value 'abc' (type: STRING) is not type " + "compatible with distribution column 'a' (type: INT).");
    AnalysisError("create table tab (a tinyint primary key) distribute by range (a) " + "(partition value = 128) stored as kudu", "Range partition value 128 " + "(type: SMALLINT) is not type compatible with distribution column 'a' " + "(type: TINYINT)");
    AnalysisError("create table tab (a smallint primary key) distribute by range (a) " + "(partition value = 32768) stored as kudu", "Range partition value 32768 " + "(type: INT) is not type compatible with distribution column 'a' " + "(type: SMALLINT)");
    AnalysisError("create table tab (a int primary key) distribute by range (a) " + "(partition value = 2147483648) stored as kudu", "Range partition value " + "2147483648 (type: BIGINT) is not type compatible with distribution column 'a' " + "(type: INT)");
    AnalysisError("create table tab (a bigint primary key) distribute by range (a) " + "(partition value = 9223372036854775808) stored as kudu", "Range partition " + "value 9223372036854775808 (type: DECIMAL(19,0)) is not type compatible with " + "distribution column 'a' (type: BIGINT)");
    // Test implicit casting/folding of partition values.
    AnalyzesOk("create table tab (a int primary key) distribute by range (a) " + "(partition value = false, partition value = true) stored as kudu");
    // Non-key column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b string, c bigint, primary key (a)) " + "distribute by range (b) (partition value = 'abc') stored as kudu", "Column 'b' in 'RANGE (b) (PARTITION VALUE = 'abc')' is not a key column. " + "Only key columns can be used in DISTRIBUTE BY.");
    // No float range partition values
    AnalysisError("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b, c) into 8 buckets, " + "range (a) (partition value = 1.2, partition value = 2) stored as kudu", "Range partition value 1.2 (type: DECIMAL(2,1)) is not type compatible with " + "distribution column 'a' (type: INT).");
    // Non-existing column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b int, primary key (a, b)) " + "distribute by range(unknown_column) (partition value = 'abc') stored as kudu", "Column 'unknown_column' in 'RANGE (unknown_column) (PARTITION VALUE = 'abc')' " + "is not a key column. Only key columns can be used in DISTRIBUTE BY");
    // Kudu table name is specified in tblproperties
    AnalyzesOk("create table tab (x int primary key) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('kudu.table_name'='tab_1'," + "'kudu.num_tablet_replicas'='1'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081')");
    // No port is specified in kudu master address
    AnalyzesOk("create table tdata_no_port (id int primary key, name string, " + "valf float, vali bigint) distribute by range(id) (partition values <= 10, " + "partition 10 < values <= 30, partition 30 < values) " + "stored as kudu tblproperties('kudu.master_addresses'='127.0.0.1')");
    // Not using the STORED AS KUDU syntax to specify a Kudu table
    AnalysisError("create table tab (x int primary key) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    AnalysisError("create table tab (x int primary key) stored as kudu tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    // Invalid value for number of replicas
    AnalysisError("create table t (x int primary key) stored as kudu tblproperties (" + "'kudu.num_tablet_replicas'='1.1')", "Table property 'kudu.num_tablet_replicas' must be an integer.");
    // Don't allow caching
    AnalysisError("create table tab (x int primary key) stored as kudu cached in " + "'testPool'", "A Kudu table cannot be cached in HDFS.");
    // LOCATION cannot be used with Kudu tables
    AnalysisError("create table tab (a int primary key) distribute by hash (a) " + "into 3 buckets stored as kudu location '/test-warehouse/'", "LOCATION cannot be specified for a Kudu table.");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int, primary key (a)) stored as kudu", "Table distribution must be specified for managed Kudu tables.");
    AnalysisError("create table tab (a int) stored as kudu", "A primary key is required for a Kudu table.");
    // Using ROW FORMAT with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "row format delimited escaped by 'X' stored as kudu", "ROW FORMAT cannot be specified for file format KUDU.");
    // Using PARTITIONED BY with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "partitioned by (y int) stored as kudu", "PARTITIONED BY cannot be used " + "in Kudu tables.");
    // Test unsupported Kudu types
    List<String> unsupportedTypes = Lists.newArrayList("DECIMAL(9,0)", "TIMESTAMP", "VARCHAR(20)", "CHAR(20)", "STRUCT<F1:INT,F2:STRING>", "ARRAY<INT>", "MAP<STRING,STRING>");
    for (String t : unsupportedTypes) {
        String expectedError = String.format("Cannot create table 'tab': Type %s is not supported in Kudu", t);
        // Unsupported type is PK and partition col
        String stmt = String.format("create table tab (x %s primary key) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
        // Unsupported type is not PK/partition col
        stmt = String.format("create table tab (x int primary key, y %s) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
    }
}
#method_after
@Test
public void TestCreateManagedKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Test primary keys and distribute by clauses
    AnalyzesOk("create table tab (x int primary key) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, primary key(x)) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) " + "distribute by hash(x, y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x)) " + "distribute by hash(x) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) " + "distribute by hash(y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y string, primary key (x)) distribute by " + "hash (x) into 3 buckets, range (x) (partition values < 1, partition " + "1 <= values < 10, partition 10 <= values < 20, partition value = 30) " + "stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) distribute by " + "range (x, y) (partition value = (2001, 1), partition value = (2002, 1), " + "partition value = (2003, 2)) stored as kudu");
    // Non-literal boundary values in range partitions
    AnalyzesOk("create table tab (x int, y int, primary key (x)) distribute by " + "range (x) (partition values < 1 + 1, partition (1+3) + 2 < values < 10, " + "partition factorial(4) < values < factorial(5), " + "partition value = factorial(6)) stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) distribute by " + "range(x, y) (partition value = (1+1, 2+2), partition value = ((1+1+1)+1, 10), " + "partition value = (cast (30 as int), factorial(5))) stored as kudu");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values < x + 1) stored as kudu", "Only constant values are allowed " + "for range-partition bounds: x + 1");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= isnull(null, null)) stored as kudu", "Range partition " + "values cannot be NULL. Range partition: 'PARTITION VALUES <= " + "isnull(NULL, NULL)'");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= (select count(*) from functional.alltypestiny)) " + "stored as kudu", "Only constant values are allowed for range-partition " + "bounds: (SELECT count(*) FROM functional.alltypestiny)");
    // Multilevel partitioning. Data is split into 3 buckets based on 'x' and each
    // bucket is partitioned into 4 tablets based on the range partitions of 'y'.
    AnalyzesOk("create table tab (x int, y string, primary key(x, y)) " + "distribute by hash(x) into 3 buckets, range(y) " + "(partition values < 'aa', partition 'aa' <= values < 'bb', " + "partition 'bb' <= values < 'cc', partition 'cc' <= values) " + "stored as kudu");
    // Key column in upper case
    AnalyzesOk("create table tab (x int, y int, primary key (X)) " + "distribute by hash (x) into 8 buckets stored as kudu");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b) into 8 buckets, hash(c) into 2 buckets stored as " + "kudu");
    // No columns specified in the DISTRIBUTE BY HASH clause
    AnalyzesOk("create table tab (a int primary key, b int, c int, d int) " + "distribute by hash into 8 buckets stored as kudu");
    // Distribute range data types are picked up during analysis and forwarded to Kudu.
    // Column names in distribute params should also be case-insensitive.
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key(a, b, c, d))" + "distribute by hash (a, B, c) into 8 buckets, " + "range (A) (partition values < 1, partition 1 <= values < 2, " + "partition 2 <= values < 3, partition 3 <= values < 4, partition 4 <= values) " + "stored as kudu");
    // Allowing range distribution on a subset of the primary keys
    AnalyzesOk("create table tab (id int, name string, valf float, vali bigint, " + "primary key (id, name)) distribute by range (name) " + "(partition 'aa' < values <= 'bb') stored as kudu");
    // Null values in range partition values
    AnalysisError("create table tab (id int, name string, primary key(id, name)) " + "distribute by hash (id) into 3 buckets, range (name) " + "(partition value = null, partition value = 1) stored as kudu", "Range partition values cannot be NULL. Range partition: 'PARTITION " + "VALUE = NULL'");
    // Primary key specified in tblproperties
    AnalysisError(String.format("create table tab (x int) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('%s' = 'x')", KuduTable.KEY_KEY_COLUMNS), "PRIMARY KEY must be used instead of the table " + "property");
    // Primary key column that doesn't exist
    AnalysisError("create table tab (x int, y int, primary key (z)) " + "distribute by hash (x) into 8 buckets stored as kudu", "PRIMARY KEY column 'z' does not exist in the table");
    // Invalid composite primary key
    AnalysisError("create table tab (x int primary key, primary key(x)) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    AnalysisError("create table tab (x int primary key, y int primary key) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    // Specifying the same primary key column multiple times
    AnalysisError("create table tab (x int, primary key (x, x)) distribute by hash (x) " + "into 8 buckets stored as kudu", "Column 'x' is listed multiple times as a PRIMARY KEY.");
    // Number of range partition boundary values should be equal to the number of range
    // columns.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by range(a) (partition value = (1, 2), " + "partition value = 3, partition value = 4) stored as kudu", "Number of specified range partition values is different than the number of " + "distribution columns: (2 vs 1). Range partition: 'PARTITION VALUE = (1,2)'");
    // Key ranges must match the column types.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by hash (a, b, c) into 8 buckets, range (a) " + "(partition value = 1, partition value = 'abc', partition 3 <= values) " + "stored as kudu", "Range partition value 'abc' (type: STRING) is not type " + "compatible with distribution column 'a' (type: INT).");
    AnalysisError("create table tab (a tinyint primary key) distribute by range (a) " + "(partition value = 128) stored as kudu", "Range partition value 128 " + "(type: SMALLINT) is not type compatible with distribution column 'a' " + "(type: TINYINT)");
    AnalysisError("create table tab (a smallint primary key) distribute by range (a) " + "(partition value = 32768) stored as kudu", "Range partition value 32768 " + "(type: INT) is not type compatible with distribution column 'a' " + "(type: SMALLINT)");
    AnalysisError("create table tab (a int primary key) distribute by range (a) " + "(partition value = 2147483648) stored as kudu", "Range partition value " + "2147483648 (type: BIGINT) is not type compatible with distribution column 'a' " + "(type: INT)");
    AnalysisError("create table tab (a bigint primary key) distribute by range (a) " + "(partition value = 9223372036854775808) stored as kudu", "Range partition " + "value 9223372036854775808 (type: DECIMAL(19,0)) is not type compatible with " + "distribution column 'a' (type: BIGINT)");
    // Test implicit casting/folding of partition values.
    AnalyzesOk("create table tab (a int primary key) distribute by range (a) " + "(partition value = false, partition value = true) stored as kudu");
    // Non-key column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b string, c bigint, primary key (a)) " + "distribute by range (b) (partition value = 'abc') stored as kudu", "Column 'b' in 'RANGE (b) (PARTITION VALUE = 'abc')' is not a key column. " + "Only key columns can be used in DISTRIBUTE BY.");
    // No float range partition values
    AnalysisError("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b, c) into 8 buckets, " + "range (a) (partition value = 1.2, partition value = 2) stored as kudu", "Range partition value 1.2 (type: DECIMAL(2,1)) is not type compatible with " + "distribution column 'a' (type: INT).");
    // Non-existing column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b int, primary key (a, b)) " + "distribute by range(unknown_column) (partition value = 'abc') stored as kudu", "Column 'unknown_column' in 'RANGE (unknown_column) (PARTITION VALUE = 'abc')' " + "is not a key column. Only key columns can be used in DISTRIBUTE BY");
    // Kudu table name is specified in tblproperties
    AnalyzesOk("create table tab (x int primary key) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('kudu.table_name'='tab_1'," + "'kudu.num_tablet_replicas'='1'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081')");
    // No port is specified in kudu master address
    AnalyzesOk("create table tdata_no_port (id int primary key, name string, " + "valf float, vali bigint) distribute by range(id) (partition values <= 10, " + "partition 10 < values <= 30, partition 30 < values) " + "stored as kudu tblproperties('kudu.master_addresses'='127.0.0.1')");
    // Not using the STORED AS KUDU syntax to specify a Kudu table
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    AnalysisError("create table tab (x int primary key) stored as kudu tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    // Invalid value for number of replicas
    AnalysisError("create table t (x int primary key) stored as kudu tblproperties (" + "'kudu.num_tablet_replicas'='1.1')", "Table property 'kudu.num_tablet_replicas' must be an integer.");
    // Don't allow caching
    AnalysisError("create table tab (x int primary key) stored as kudu cached in " + "'testPool'", "A Kudu table cannot be cached in HDFS.");
    // LOCATION cannot be used with Kudu tables
    AnalysisError("create table tab (a int primary key) distribute by hash (a) " + "into 3 buckets stored as kudu location '/test-warehouse/'", "LOCATION cannot be specified for a Kudu table.");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int, primary key (a)) stored as kudu", "Table distribution must be specified for managed Kudu tables.");
    AnalysisError("create table tab (a int) stored as kudu", "A primary key is required for a Kudu table.");
    // Using ROW FORMAT with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "row format delimited escaped by 'X' stored as kudu", "ROW FORMAT cannot be specified for file format KUDU.");
    // Using PARTITIONED BY with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "partitioned by (y int) stored as kudu", "PARTITIONED BY cannot be used " + "in Kudu tables.");
    // Test unsupported Kudu types
    List<String> unsupportedTypes = Lists.newArrayList("DECIMAL(9,0)", "TIMESTAMP", "VARCHAR(20)", "CHAR(20)", "STRUCT<F1:INT,F2:STRING>", "ARRAY<INT>", "MAP<STRING,STRING>");
    for (String t : unsupportedTypes) {
        String expectedError = String.format("Cannot create table 'tab': Type %s is not supported in Kudu", t);
        // Unsupported type is PK and partition col
        String stmt = String.format("create table tab (x %s primary key) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
        // Unsupported type is not PK/partition col
        stmt = String.format("create table tab (x int primary key, y %s) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
    }
    // Test column options
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (Encoding enc : Encoding.values()) {
        for (CompressionAlgorithm comp : CompressionAlgorithm.values()) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        AnalyzesOk(String.format("create table tab (x int primary key " + "not null encoding %s compression %s %s %s, y int encoding %s " + "compression %s %s %s %s) distribute by hash (x) " + "into 3 buckets stored as kudu", enc, comp, def, block, enc, comp, def, nul, block));
                    }
                }
            }
        }
    }
    // Primary key specified using the PRIMARY KEY clause
    AnalyzesOk("create table tab (x int not null encoding plain_encoding " + "compression snappy block_size 1, y int null encoding rle compression lz4 " + "default 1, primary key(x)) distribute by hash (x) into 3 buckets " + "stored as kudu");
    // Primary keys can't be null
    AnalysisError("create table tab (x int primary key null, y int not null) " + "distribute by hash (x) into 3 buckets stored as kudu", "Primary key columns " + "cannot be nullable: x INT PRIMARY KEY NULL");
    AnalysisError("create table tab (x int not null, y int null, primary key (x, y)) " + "distribute by hash (x) into 3 buckets stored as kudu", "Primary key columns " + "cannot be nullable: y INT NULL");
    // Unsupported encoding value
    AnalysisError("create table tab (x int primary key, y int encoding invalid_enc) " + "distribute by hash (x) into 3 buckets stored as kudu", "Unsupported encoding " + "value 'INVALID_ENC'. Supported encoding values are: " + Joiner.on(", ").join(Encoding.values()));
    // Unsupported compression algorithm
    AnalysisError("create table tab (x int primary key, y int compression " + "invalid_comp) distribute by hash (x) into 3 buckets stored as kudu", "Unsupported compression algorithm 'INVALID_COMP'. Supported compression " + "algorithms are: " + Joiner.on(", ").join(CompressionAlgorithm.values()));
    // Default values
    AnalyzesOk("create table tab (i1 tinyint default 1, i2 smallint default 10, " + "i3 int default 100, i4 bigint default 1000, vals string default 'test', " + "valf float default cast(1.2 as float), vald double default " + "cast(3.1452 as double), valb boolean default true, " + "primary key (i1, i2, i3, i4, vals)) distribute by hash (i1) into 3 " + "buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key default 1+1+1) " + "distribute by hash (i) into 3 buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key default factorial(5)) " + "distribute by hash (i) into 3 buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key, x int null default " + "isnull(null, null)) distribute by hash (i) into 3 buckets stored as kudu");
    // Invalid default values
    AnalysisError("create table tab (i int primary key default 'string_val') " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value " + "'string_val' (type: STRING) is not compatible with column 'i' (type: INT).");
    AnalysisError("create table tab (i int primary key, x int default 1.1) " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value 1.1 (type: DECIMAL(2,1)) is not compatible with column " + "'x' (type: INT).");
    AnalysisError("create table tab (i tinyint primary key default 128) " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value " + "128 (type: SMALLINT) is not compatible with column 'i' (type: TINYINT).");
    AnalysisError("create table tab (i int primary key default isnull(null, null)) " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value of " + "NULL not allowed on non-nullable column: 'i'");
    AnalysisError("create table tab (i int primary key, x int not null " + "default isnull(null, null)) distribute by hash (i) into 3 buckets " + "stored as kudu", "Default value of NULL not allowed on non-nullable column: " + "'x'");
    // Invalid block_size values
    AnalysisError("create table tab (i int primary key block_size 1.1) " + "distribute by hash (i) into 3 buckets stored as kudu", "Invalid value " + "for BLOCK_SIZE: 1.1. A positive INTEGER value is expected.");
    AnalysisError("create table tab (i int primary key block_size 'val') " + "distribute by hash (i) into 3 buckets stored as kudu", "Invalid value " + "for BLOCK_SIZE: 'val'. A positive INTEGER value is expected.");
}
#end_block

#method_before
@Test
public void TestCreateAvroTest() {
    String alltypesSchemaLoc = "hdfs:///test-warehouse/avro_schemas/functional/alltypes.json";
    // Analysis of Avro schemas. Column definitions match the Avro schema exactly.
    // Note: Avro does not have a tinyint and smallint type.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc));
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc));
    AnalyzesOk("create table foo_avro (string1 string) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}]}')");
    // No column definitions.
    AnalyzesOk(String.format("create table foo_avro with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc));
    AnalyzesOk(String.format("create table foo_avro stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc));
    AnalyzesOk("create table foo_avro stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}]}')");
    // Analysis of Avro schemas. Column definitions do not match Avro schema.
    AnalyzesOk(String.format("create table foo_avro (id int) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc), "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 11 column(s) but 1 column definition(s) were given.");
    AnalyzesOk(String.format("create table foo_avro (bool_col boolean, string_col string) " + "stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc), "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 11 column(s) but 2 column definition(s) were given.");
    AnalyzesOk("create table foo_avro (string1 string) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"string2\", \"type\": \"string\"}]}')", "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 2 column(s) but 1 column definition(s) were given.");
    // Mismatched name.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, bad_int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc), "Resolved the following name and/or type inconsistencies between the column " + "definitions and the Avro schema.\n" + "Column definition at position 4:  bad_int_col INT\n" + "Avro schema column at position 4: int_col INT\n" + "Resolution at position 4: int_col INT\n" + "Column definition at position 10:  timestamp_col TIMESTAMP\n" + "Avro schema column at position 10: timestamp_col STRING\n" + "Resolution at position 10: timestamp_col STRING");
    // Mismatched type.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col bigint, date_string_col string, string_col string, " + "timestamp_col timestamp) stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc), "Resolved the following name and/or type inconsistencies between the column " + "definitions and the Avro schema.\n" + "Column definition at position 7:  double_col BIGINT\n" + "Avro schema column at position 7: double_col DOUBLE\n" + "Resolution at position 7: double_col DOUBLE\n" + "Column definition at position 10:  timestamp_col TIMESTAMP\n" + "Avro schema column at position 10: timestamp_col STRING\n" + "Resolution at position 10: timestamp_col STRING");
    // Avro schema is inferred from column definitions.
    AnalyzesOk("create table foo_avro (c1 tinyint, c2 smallint, c3 int, c4 bigint, " + "c5 float, c6 double, c7 timestamp, c8 string, c9 char(10), c10 varchar(20)," + "c11 decimal(10, 5), c12 struct<f1:int,f2:string>, c13 array<int>," + "c14 map<string,string>) stored as avro");
    AnalyzesOk("create table foo_avro (c1 tinyint, c2 smallint, c3 int, c4 bigint, " + "c5 float, c6 double, c7 timestamp, c8 string, c9 char(10), c10 varchar(20)," + "c11 decimal(10, 5), c12 struct<f1:int,f2:string>, c13 array<int>," + "c14 map<string,string>) partitioned by (year int, month int) stored as avro");
    // Neither Avro schema nor column definitions.
    AnalysisError("create table foo_avro stored as avro tblproperties ('a'='b')", "An Avro table requires column definitions or an Avro schema.");
    // Invalid schema URL
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='')", "Invalid avro.schema.url: . Can not create a Path from an empty string");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='schema.avsc')", "Invalid avro.schema.url: schema.avsc. Path does not exist.");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='hdfs://invalid*host/schema.avsc')", "Failed to read Avro schema at: hdfs://invalid*host/schema.avsc. " + "Incomplete HDFS URI, no host: hdfs://invalid*host/schema.avsc");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='foo://bar/schema.avsc')", "Failed to read Avro schema at: foo://bar/schema.avsc. " + "No FileSystem for scheme: foo");
    // Decimal parsing
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"precision\":5,\"scale\":2}}]}')");
    // Scale not required (defaults to zero).
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"precision\":5}}]}')");
    // Precision is always required
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":5}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "No 'precision' property specified for 'decimal' logicalType");
    // Precision/Scale must be positive integers
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":5, \"precision\":-20}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "Invalid decimal 'precision' property value: -20");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":-1, \"precision\":20}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "Invalid decimal 'scale' property value: -1");
    // Invalid schema (bad JSON - missing opening bracket for "field" array)
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": {\"name\": \"string1\", \"type\": \"string\"}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "org.codehaus.jackson.JsonParseException: Unexpected close marker ']': " + "expected '}'");
    // Map/Array types in Avro schema.
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"list1\", \"type\": {\"type\":\"array\", \"items\": \"int\"}}]}')");
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"map1\", \"type\": {\"type\":\"map\", \"values\": \"int\"}}]}')");
    // Union is not supported
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"union1\", \"type\": [\"float\", \"boolean\"]}]}')", "Unsupported type 'union' of column 'union1'");
    // TODO: Add COLLECTION ITEMS TERMINATED BY and MAP KEYS TERMINATED BY clauses.
    // Test struct complex type.
    AnalyzesOk("create table functional.new_table (" + "a struct<f1: int, f2: string, f3: timestamp, f4: boolean>, " + "b struct<f1: struct<f11: int>, f2: struct<f21: struct<f22: string>>>, " + "c struct<f1: map<int, string>, f2: array<bigint>>," + "d struct<f1: struct<f11: map<int, string>, f12: array<bigint>>>)");
    // Test array complex type.
    AnalyzesOk("create table functional.new_table (" + "a array<int>, b array<timestamp>, c array<string>, d array<boolean>, " + "e array<array<int>>, f array<array<array<string>>>, " + "g array<struct<f1: int, f2: string>>, " + "h array<map<string,int>>)");
    // Test map complex type.
    AnalyzesOk("create table functional.new_table (" + "a map<string, int>, b map<timestamp, boolean>, c map<bigint, float>, " + "d array<array<int>>, e array<array<array<string>>>, " + "f array<struct<f1: int, f2: string>>," + "g array<map<string,int>>)");
    // Cannot partition by a complex column.
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x array<int>)", "Type 'ARRAY<INT>' is not supported as partition-column type in column: x");
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x map<int,int>)", "Type 'MAP<INT,INT>' is not supported as partition-column type in column: x");
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x struct<f1:int>)", "Type 'STRUCT<f1:INT>' is not supported as partition-column type in column: x");
    // Kudu specific clauses used in an Avro table.
    AnalysisError("create table functional.new_table (i int primary key) " + "distribute by hash(i) into 3 buckets stored as avro", "Only Kudu tables can use the DISTRIBUTE BY clause.");
    AnalysisError("create table functional.new_table (i int primary key) " + "stored as avro", "Only Kudu tables can specify a PRIMARY KEY.");
}
#method_after
@Test
public void TestCreateAvroTest() {
    String alltypesSchemaLoc = "hdfs:///test-warehouse/avro_schemas/functional/alltypes.json";
    // Analysis of Avro schemas. Column definitions match the Avro schema exactly.
    // Note: Avro does not have a tinyint and smallint type.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc));
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc));
    AnalyzesOk("create table foo_avro (string1 string) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}]}')");
    // No column definitions.
    AnalyzesOk(String.format("create table foo_avro with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc));
    AnalyzesOk(String.format("create table foo_avro stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc));
    AnalyzesOk("create table foo_avro stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}]}')");
    // Analysis of Avro schemas. Column definitions do not match Avro schema.
    AnalyzesOk(String.format("create table foo_avro (id int) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc), "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 11 column(s) but 1 column definition(s) were given.");
    AnalyzesOk(String.format("create table foo_avro (bool_col boolean, string_col string) " + "stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc), "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 11 column(s) but 2 column definition(s) were given.");
    AnalyzesOk("create table foo_avro (string1 string) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"string2\", \"type\": \"string\"}]}')", "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 2 column(s) but 1 column definition(s) were given.");
    // Mismatched name.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, bad_int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc), "Resolved the following name and/or type inconsistencies between the column " + "definitions and the Avro schema.\n" + "Column definition at position 4:  bad_int_col INT\n" + "Avro schema column at position 4: int_col INT\n" + "Resolution at position 4: int_col INT\n" + "Column definition at position 10:  timestamp_col TIMESTAMP\n" + "Avro schema column at position 10: timestamp_col STRING\n" + "Resolution at position 10: timestamp_col STRING");
    // Mismatched type.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col bigint, date_string_col string, string_col string, " + "timestamp_col timestamp) stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc), "Resolved the following name and/or type inconsistencies between the column " + "definitions and the Avro schema.\n" + "Column definition at position 7:  double_col BIGINT\n" + "Avro schema column at position 7: double_col DOUBLE\n" + "Resolution at position 7: double_col DOUBLE\n" + "Column definition at position 10:  timestamp_col TIMESTAMP\n" + "Avro schema column at position 10: timestamp_col STRING\n" + "Resolution at position 10: timestamp_col STRING");
    // Avro schema is inferred from column definitions.
    AnalyzesOk("create table foo_avro (c1 tinyint, c2 smallint, c3 int, c4 bigint, " + "c5 float, c6 double, c7 timestamp, c8 string, c9 char(10), c10 varchar(20)," + "c11 decimal(10, 5), c12 struct<f1:int,f2:string>, c13 array<int>," + "c14 map<string,string>) stored as avro");
    AnalyzesOk("create table foo_avro (c1 tinyint, c2 smallint, c3 int, c4 bigint, " + "c5 float, c6 double, c7 timestamp, c8 string, c9 char(10), c10 varchar(20)," + "c11 decimal(10, 5), c12 struct<f1:int,f2:string>, c13 array<int>," + "c14 map<string,string>) partitioned by (year int, month int) stored as avro");
    // Neither Avro schema nor column definitions.
    AnalysisError("create table foo_avro stored as avro tblproperties ('a'='b')", "An Avro table requires column definitions or an Avro schema.");
    // Invalid schema URL
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='')", "Invalid avro.schema.url: . Can not create a Path from an empty string");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='schema.avsc')", "Invalid avro.schema.url: schema.avsc. Path does not exist.");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='hdfs://invalid*host/schema.avsc')", "Failed to read Avro schema at: hdfs://invalid*host/schema.avsc. " + "Incomplete HDFS URI, no host: hdfs://invalid*host/schema.avsc");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='foo://bar/schema.avsc')", "Failed to read Avro schema at: foo://bar/schema.avsc. " + "No FileSystem for scheme: foo");
    // Decimal parsing
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"precision\":5,\"scale\":2}}]}')");
    // Scale not required (defaults to zero).
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"precision\":5}}]}')");
    // Precision is always required
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":5}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "No 'precision' property specified for 'decimal' logicalType");
    // Precision/Scale must be positive integers
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":5, \"precision\":-20}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "Invalid decimal 'precision' property value: -20");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":-1, \"precision\":20}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "Invalid decimal 'scale' property value: -1");
    // Invalid schema (bad JSON - missing opening bracket for "field" array)
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": {\"name\": \"string1\", \"type\": \"string\"}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "org.codehaus.jackson.JsonParseException: Unexpected close marker ']': " + "expected '}'");
    // Map/Array types in Avro schema.
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"list1\", \"type\": {\"type\":\"array\", \"items\": \"int\"}}]}')");
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"map1\", \"type\": {\"type\":\"map\", \"values\": \"int\"}}]}')");
    // Union is not supported
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"union1\", \"type\": [\"float\", \"boolean\"]}]}')", "Unsupported type 'union' of column 'union1'");
    // TODO: Add COLLECTION ITEMS TERMINATED BY and MAP KEYS TERMINATED BY clauses.
    // Test struct complex type.
    AnalyzesOk("create table functional.new_table (" + "a struct<f1: int, f2: string, f3: timestamp, f4: boolean>, " + "b struct<f1: struct<f11: int>, f2: struct<f21: struct<f22: string>>>, " + "c struct<f1: map<int, string>, f2: array<bigint>>," + "d struct<f1: struct<f11: map<int, string>, f12: array<bigint>>>)");
    // Test array complex type.
    AnalyzesOk("create table functional.new_table (" + "a array<int>, b array<timestamp>, c array<string>, d array<boolean>, " + "e array<array<int>>, f array<array<array<string>>>, " + "g array<struct<f1: int, f2: string>>, " + "h array<map<string,int>>)");
    // Test map complex type.
    AnalyzesOk("create table functional.new_table (" + "a map<string, int>, b map<timestamp, boolean>, c map<bigint, float>, " + "d array<array<int>>, e array<array<array<string>>>, " + "f array<struct<f1: int, f2: string>>," + "g array<map<string,int>>)");
    // Cannot partition by a complex column.
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x array<int>)", "Type 'ARRAY<INT>' is not supported as partition-column type in column: x");
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x map<int,int>)", "Type 'MAP<INT,INT>' is not supported as partition-column type in column: x");
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x struct<f1:int>)", "Type 'STRUCT<f1:INT>' is not supported as partition-column type in column: x");
    // Kudu specific clauses used in an Avro table.
    AnalysisError("create table functional.new_table (i int) " + "distribute by hash(i) into 3 buckets stored as avro", "Only Kudu tables can use the DISTRIBUTE BY clause.");
    AnalysisError("create table functional.new_table (i int primary key) " + "stored as avro", "Unsupported column options for file format 'AVRO': " + "'i INT PRIMARY KEY'");
}
#end_block

#method_before
private void createCatalogOpRequest(AnalysisContext.AnalysisResult analysis, TExecRequest result) throws InternalException {
    TCatalogOpRequest ddl = new TCatalogOpRequest();
    TResultSetMetadata metadata = new TResultSetMetadata();
    if (analysis.isUseStmt()) {
        ddl.op_type = TCatalogOpType.USE;
        ddl.setUse_db_params(analysis.getUseStmt().toThrift());
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isShowTablesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_TABLES;
        ddl.setShow_tables_params(analysis.getShowTablesStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowDbsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_DBS;
        ddl.setShow_dbs_params(analysis.getShowDbsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("comment", Type.STRING.toThrift())));
    } else if (analysis.isShowDataSrcsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_DATA_SRCS;
        ddl.setShow_data_srcs_params(analysis.getShowDataSrcsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("location", Type.STRING.toThrift()), new TColumn("class name", Type.STRING.toThrift()), new TColumn("api version", Type.STRING.toThrift())));
    } else if (analysis.isShowStatsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_STATS;
        ddl.setShow_stats_params(analysis.getShowStatsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowFunctionsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_FUNCTIONS;
        ShowFunctionsStmt stmt = (ShowFunctionsStmt) analysis.getStmt();
        ddl.setShow_fns_params(stmt.toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("return type", Type.STRING.toThrift()), new TColumn("signature", Type.STRING.toThrift()), new TColumn("binary type", Type.STRING.toThrift()), new TColumn("is persistent", Type.STRING.toThrift())));
    } else if (analysis.isShowCreateTableStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_CREATE_TABLE;
        ddl.setShow_create_table_params(analysis.getShowCreateTableStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("result", Type.STRING.toThrift())));
    } else if (analysis.isShowCreateFunctionStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_CREATE_FUNCTION;
        ddl.setShow_create_function_params(analysis.getShowCreateFunctionStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("result", Type.STRING.toThrift())));
    } else if (analysis.isShowFilesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_FILES;
        ddl.setShow_files_params(analysis.getShowFilesStmt().toThrift());
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDescribeDbStmt()) {
        ddl.op_type = TCatalogOpType.DESCRIBE_DB;
        ddl.setDescribe_db_params(analysis.getDescribeDbStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("location", Type.STRING.toThrift()), new TColumn("comment", Type.STRING.toThrift())));
    } else if (analysis.isDescribeTableStmt()) {
        ddl.op_type = TCatalogOpType.DESCRIBE_TABLE;
        DescribeTableStmt descStmt = analysis.getDescribeTableStmt();
        ddl.setDescribe_table_params(descStmt.toThrift());
        List<TColumn> columns = Lists.newArrayList(new TColumn("name", Type.STRING.toThrift()), new TColumn("type", Type.STRING.toThrift()), new TColumn("comment", Type.STRING.toThrift()));
        if (descStmt.getTable() instanceof KuduTable && descStmt.getOutputStyle() == TDescribeOutputStyle.MINIMAL) {
            columns.add(new TColumn("primary_key", Type.STRING.toThrift()));
            columns.add(new TColumn("nullable", Type.STRING.toThrift()));
            columns.add(new TColumn("encoding", Type.STRING.toThrift()));
            columns.add(new TColumn("compression", Type.STRING.toThrift()));
        }
        metadata.setColumns(columns);
    } else if (analysis.isAlterTableStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.ALTER_TABLE);
        req.setAlter_table_params(analysis.getAlterTableStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isAlterViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.ALTER_VIEW);
        req.setAlter_view_params(analysis.getAlterViewStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateTableStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE);
        req.setCreate_table_params(analysis.getCreateTableStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateTableAsSelectStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE_AS_SELECT);
        req.setCreate_table_params(analysis.getCreateTableAsSelectStmt().getCreateStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Arrays.asList(new TColumn("summary", Type.STRING.toThrift())));
    } else if (analysis.isCreateTableLikeStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE_LIKE);
        req.setCreate_table_like_params(analysis.getCreateTableLikeStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_VIEW);
        req.setCreate_view_params(analysis.getCreateViewStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateDbStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_DATABASE);
        req.setCreate_db_params(analysis.getCreateDbStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateUdfStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        CreateUdfStmt stmt = (CreateUdfStmt) analysis.getStmt();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_FUNCTION);
        req.setCreate_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateUdaStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_FUNCTION);
        CreateUdaStmt stmt = (CreateUdaStmt) analysis.getStmt();
        req.setCreate_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateDataSrcStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_DATA_SOURCE);
        CreateDataSrcStmt stmt = (CreateDataSrcStmt) analysis.getStmt();
        req.setCreate_data_source_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isComputeStatsStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.COMPUTE_STATS);
        req.setCompute_stats_params(analysis.getComputeStatsStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropDbStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_DATABASE);
        req.setDrop_db_params(analysis.getDropDbStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropTableOrViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        DropTableOrViewStmt stmt = analysis.getDropTableOrViewStmt();
        req.setDdl_type(stmt.isDropTable() ? TDdlType.DROP_TABLE : TDdlType.DROP_VIEW);
        req.setDrop_table_or_view_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isTruncateStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        TruncateStmt stmt = analysis.getTruncateStmt();
        req.setDdl_type(TDdlType.TRUNCATE_TABLE);
        req.setTruncate_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropFunctionStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_FUNCTION);
        DropFunctionStmt stmt = (DropFunctionStmt) analysis.getStmt();
        req.setDrop_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropDataSrcStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_DATA_SOURCE);
        DropDataSrcStmt stmt = (DropDataSrcStmt) analysis.getStmt();
        req.setDrop_data_source_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropStatsStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_STATS);
        DropStatsStmt stmt = (DropStatsStmt) analysis.getStmt();
        req.setDrop_stats_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isResetMetadataStmt()) {
        ddl.op_type = TCatalogOpType.RESET_METADATA;
        ResetMetadataStmt resetMetadataStmt = (ResetMetadataStmt) analysis.getStmt();
        TResetMetadataRequest req = resetMetadataStmt.toThrift();
        ddl.setReset_metadata_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isShowRolesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_ROLES;
        ShowRolesStmt showRolesStmt = (ShowRolesStmt) analysis.getStmt();
        ddl.setShow_roles_params(showRolesStmt.toThrift());
        Set<String> groupNames = getAuthzChecker().getUserGroups(analysis.getAnalyzer().getUser());
        // Check if the user is part of the group (case-sensitive) this SHOW ROLE
        // statement is targeting. If they are already a member of the group,
        // the admin requirement can be removed.
        Preconditions.checkState(ddl.getShow_roles_params().isSetIs_admin_op());
        if (ddl.getShow_roles_params().isSetGrant_group() && groupNames.contains(ddl.getShow_roles_params().getGrant_group())) {
            ddl.getShow_roles_params().setIs_admin_op(false);
        }
        metadata.setColumns(Arrays.asList(new TColumn("role_name", Type.STRING.toThrift())));
    } else if (analysis.isShowGrantRoleStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_GRANT_ROLE;
        ShowGrantRoleStmt showGrantRoleStmt = (ShowGrantRoleStmt) analysis.getStmt();
        ddl.setShow_grant_role_params(showGrantRoleStmt.toThrift());
        Set<String> groupNames = getAuthzChecker().getUserGroups(analysis.getAnalyzer().getUser());
        // User must be an admin to execute this operation if they have not been granted
        // this role.
        ddl.getShow_grant_role_params().setIs_admin_op(Sets.intersection(groupNames, showGrantRoleStmt.getRole().getGrantGroups()).isEmpty());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isCreateDropRoleStmt()) {
        CreateDropRoleStmt createDropRoleStmt = (CreateDropRoleStmt) analysis.getStmt();
        TCreateDropRoleParams params = createDropRoleStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_drop() ? TDdlType.DROP_ROLE : TDdlType.CREATE_ROLE);
        req.setCreate_drop_role_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isGrantRevokeRoleStmt()) {
        GrantRevokeRoleStmt grantRoleStmt = (GrantRevokeRoleStmt) analysis.getStmt();
        TGrantRevokeRoleParams params = grantRoleStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_grant() ? TDdlType.GRANT_ROLE : TDdlType.REVOKE_ROLE);
        req.setGrant_revoke_role_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isGrantRevokePrivStmt()) {
        GrantRevokePrivStmt grantRevokePrivStmt = (GrantRevokePrivStmt) analysis.getStmt();
        TGrantRevokePrivParams params = grantRevokePrivStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_grant() ? TDdlType.GRANT_PRIVILEGE : TDdlType.REVOKE_PRIVILEGE);
        req.setGrant_revoke_priv_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else {
        throw new IllegalStateException("Unexpected CatalogOp statement type.");
    }
    result.setResult_set_metadata(metadata);
    result.setCatalog_op_request(ddl);
    if (ddl.getOp_type() == TCatalogOpType.DDL) {
        TCatalogServiceRequestHeader header = new TCatalogServiceRequestHeader();
        header.setRequesting_user(analysis.getAnalyzer().getUser().getName());
        ddl.getDdl_params().setHeader(header);
    }
}
#method_after
private void createCatalogOpRequest(AnalysisContext.AnalysisResult analysis, TExecRequest result) throws InternalException {
    TCatalogOpRequest ddl = new TCatalogOpRequest();
    TResultSetMetadata metadata = new TResultSetMetadata();
    if (analysis.isUseStmt()) {
        ddl.op_type = TCatalogOpType.USE;
        ddl.setUse_db_params(analysis.getUseStmt().toThrift());
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isShowTablesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_TABLES;
        ddl.setShow_tables_params(analysis.getShowTablesStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowDbsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_DBS;
        ddl.setShow_dbs_params(analysis.getShowDbsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("comment", Type.STRING.toThrift())));
    } else if (analysis.isShowDataSrcsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_DATA_SRCS;
        ddl.setShow_data_srcs_params(analysis.getShowDataSrcsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("location", Type.STRING.toThrift()), new TColumn("class name", Type.STRING.toThrift()), new TColumn("api version", Type.STRING.toThrift())));
    } else if (analysis.isShowStatsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_STATS;
        ddl.setShow_stats_params(analysis.getShowStatsStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isShowFunctionsStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_FUNCTIONS;
        ShowFunctionsStmt stmt = (ShowFunctionsStmt) analysis.getStmt();
        ddl.setShow_fns_params(stmt.toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("return type", Type.STRING.toThrift()), new TColumn("signature", Type.STRING.toThrift()), new TColumn("binary type", Type.STRING.toThrift()), new TColumn("is persistent", Type.STRING.toThrift())));
    } else if (analysis.isShowCreateTableStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_CREATE_TABLE;
        ddl.setShow_create_table_params(analysis.getShowCreateTableStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("result", Type.STRING.toThrift())));
    } else if (analysis.isShowCreateFunctionStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_CREATE_FUNCTION;
        ddl.setShow_create_function_params(analysis.getShowCreateFunctionStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("result", Type.STRING.toThrift())));
    } else if (analysis.isShowFilesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_FILES;
        ddl.setShow_files_params(analysis.getShowFilesStmt().toThrift());
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDescribeDbStmt()) {
        ddl.op_type = TCatalogOpType.DESCRIBE_DB;
        ddl.setDescribe_db_params(analysis.getDescribeDbStmt().toThrift());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift()), new TColumn("location", Type.STRING.toThrift()), new TColumn("comment", Type.STRING.toThrift())));
    } else if (analysis.isDescribeTableStmt()) {
        ddl.op_type = TCatalogOpType.DESCRIBE_TABLE;
        DescribeTableStmt descStmt = analysis.getDescribeTableStmt();
        ddl.setDescribe_table_params(descStmt.toThrift());
        List<TColumn> columns = Lists.newArrayList(new TColumn("name", Type.STRING.toThrift()), new TColumn("type", Type.STRING.toThrift()), new TColumn("comment", Type.STRING.toThrift()));
        if (descStmt.getTable() instanceof KuduTable && descStmt.getOutputStyle() == TDescribeOutputStyle.MINIMAL) {
            columns.add(new TColumn("primary_key", Type.STRING.toThrift()));
            columns.add(new TColumn("nullable", Type.STRING.toThrift()));
            columns.add(new TColumn("default_value", Type.STRING.toThrift()));
            columns.add(new TColumn("encoding", Type.STRING.toThrift()));
            columns.add(new TColumn("compression", Type.STRING.toThrift()));
            columns.add(new TColumn("block_size", Type.STRING.toThrift()));
        }
        metadata.setColumns(columns);
    } else if (analysis.isAlterTableStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.ALTER_TABLE);
        req.setAlter_table_params(analysis.getAlterTableStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isAlterViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.ALTER_VIEW);
        req.setAlter_view_params(analysis.getAlterViewStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateTableStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE);
        req.setCreate_table_params(analysis.getCreateTableStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateTableAsSelectStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE_AS_SELECT);
        req.setCreate_table_params(analysis.getCreateTableAsSelectStmt().getCreateStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Arrays.asList(new TColumn("summary", Type.STRING.toThrift())));
    } else if (analysis.isCreateTableLikeStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_TABLE_LIKE);
        req.setCreate_table_like_params(analysis.getCreateTableLikeStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_VIEW);
        req.setCreate_view_params(analysis.getCreateViewStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateDbStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_DATABASE);
        req.setCreate_db_params(analysis.getCreateDbStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateUdfStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        CreateUdfStmt stmt = (CreateUdfStmt) analysis.getStmt();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_FUNCTION);
        req.setCreate_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateUdaStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_FUNCTION);
        CreateUdaStmt stmt = (CreateUdaStmt) analysis.getStmt();
        req.setCreate_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isCreateDataSrcStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.CREATE_DATA_SOURCE);
        CreateDataSrcStmt stmt = (CreateDataSrcStmt) analysis.getStmt();
        req.setCreate_data_source_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isComputeStatsStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.COMPUTE_STATS);
        req.setCompute_stats_params(analysis.getComputeStatsStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropDbStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_DATABASE);
        req.setDrop_db_params(analysis.getDropDbStmt().toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropTableOrViewStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        DropTableOrViewStmt stmt = analysis.getDropTableOrViewStmt();
        req.setDdl_type(stmt.isDropTable() ? TDdlType.DROP_TABLE : TDdlType.DROP_VIEW);
        req.setDrop_table_or_view_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isTruncateStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        TruncateStmt stmt = analysis.getTruncateStmt();
        req.setDdl_type(TDdlType.TRUNCATE_TABLE);
        req.setTruncate_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropFunctionStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_FUNCTION);
        DropFunctionStmt stmt = (DropFunctionStmt) analysis.getStmt();
        req.setDrop_fn_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropDataSrcStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_DATA_SOURCE);
        DropDataSrcStmt stmt = (DropDataSrcStmt) analysis.getStmt();
        req.setDrop_data_source_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isDropStatsStmt()) {
        ddl.op_type = TCatalogOpType.DDL;
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(TDdlType.DROP_STATS);
        DropStatsStmt stmt = (DropStatsStmt) analysis.getStmt();
        req.setDrop_stats_params(stmt.toThrift());
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isResetMetadataStmt()) {
        ddl.op_type = TCatalogOpType.RESET_METADATA;
        ResetMetadataStmt resetMetadataStmt = (ResetMetadataStmt) analysis.getStmt();
        TResetMetadataRequest req = resetMetadataStmt.toThrift();
        ddl.setReset_metadata_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isShowRolesStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_ROLES;
        ShowRolesStmt showRolesStmt = (ShowRolesStmt) analysis.getStmt();
        ddl.setShow_roles_params(showRolesStmt.toThrift());
        Set<String> groupNames = getAuthzChecker().getUserGroups(analysis.getAnalyzer().getUser());
        // Check if the user is part of the group (case-sensitive) this SHOW ROLE
        // statement is targeting. If they are already a member of the group,
        // the admin requirement can be removed.
        Preconditions.checkState(ddl.getShow_roles_params().isSetIs_admin_op());
        if (ddl.getShow_roles_params().isSetGrant_group() && groupNames.contains(ddl.getShow_roles_params().getGrant_group())) {
            ddl.getShow_roles_params().setIs_admin_op(false);
        }
        metadata.setColumns(Arrays.asList(new TColumn("role_name", Type.STRING.toThrift())));
    } else if (analysis.isShowGrantRoleStmt()) {
        ddl.op_type = TCatalogOpType.SHOW_GRANT_ROLE;
        ShowGrantRoleStmt showGrantRoleStmt = (ShowGrantRoleStmt) analysis.getStmt();
        ddl.setShow_grant_role_params(showGrantRoleStmt.toThrift());
        Set<String> groupNames = getAuthzChecker().getUserGroups(analysis.getAnalyzer().getUser());
        // User must be an admin to execute this operation if they have not been granted
        // this role.
        ddl.getShow_grant_role_params().setIs_admin_op(Sets.intersection(groupNames, showGrantRoleStmt.getRole().getGrantGroups()).isEmpty());
        metadata.setColumns(Arrays.asList(new TColumn("name", Type.STRING.toThrift())));
    } else if (analysis.isCreateDropRoleStmt()) {
        CreateDropRoleStmt createDropRoleStmt = (CreateDropRoleStmt) analysis.getStmt();
        TCreateDropRoleParams params = createDropRoleStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_drop() ? TDdlType.DROP_ROLE : TDdlType.CREATE_ROLE);
        req.setCreate_drop_role_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isGrantRevokeRoleStmt()) {
        GrantRevokeRoleStmt grantRoleStmt = (GrantRevokeRoleStmt) analysis.getStmt();
        TGrantRevokeRoleParams params = grantRoleStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_grant() ? TDdlType.GRANT_ROLE : TDdlType.REVOKE_ROLE);
        req.setGrant_revoke_role_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else if (analysis.isGrantRevokePrivStmt()) {
        GrantRevokePrivStmt grantRevokePrivStmt = (GrantRevokePrivStmt) analysis.getStmt();
        TGrantRevokePrivParams params = grantRevokePrivStmt.toThrift();
        TDdlExecRequest req = new TDdlExecRequest();
        req.setDdl_type(params.isIs_grant() ? TDdlType.GRANT_PRIVILEGE : TDdlType.REVOKE_PRIVILEGE);
        req.setGrant_revoke_priv_params(params);
        ddl.op_type = TCatalogOpType.DDL;
        ddl.setDdl_params(req);
        metadata.setColumns(Collections.<TColumn>emptyList());
    } else {
        throw new IllegalStateException("Unexpected CatalogOp statement type.");
    }
    result.setResult_set_metadata(metadata);
    result.setCatalog_op_request(ddl);
    if (ddl.getOp_type() == TCatalogOpType.DDL) {
        TCatalogServiceRequestHeader header = new TCatalogServiceRequestHeader();
        header.setRequesting_user(analysis.getAnalyzer().getUser().getName());
        ddl.getDdl_params().setHeader(header);
    }
}
#end_block

#method_before
public static TDescribeResult buildDescribeMinimalResult(StructType type) {
    TDescribeResult descResult = new TDescribeResult();
    descResult.results = Lists.newArrayList();
    for (StructField field : type.getFields()) {
        TColumnValue colNameCol = new TColumnValue();
        colNameCol.setString_val(field.getName());
        TColumnValue dataTypeCol = new TColumnValue();
        dataTypeCol.setString_val(field.getType().prettyPrint().toLowerCase());
        TColumnValue commentCol = new TColumnValue();
        commentCol.setString_val(field.getComment() != null ? field.getComment() : "");
        descResult.results.add(new TResultRow(Lists.newArrayList(colNameCol, dataTypeCol, commentCol)));
    }
    return descResult;
}
#method_after
public static TDescribeResult buildDescribeMinimalResult(StructType type) {
    TDescribeResult descResult = new TDescribeResult();
    descResult.results = Lists.newArrayList();
    for (StructField field : type.getFields()) {
        TColumnValue colNameCol = new TColumnValue();
        colNameCol.setString_val(field.getName());
        TColumnValue dataTypeCol = new TColumnValue();
        dataTypeCol.setString_val(field.getType().prettyPrint().toLowerCase());
        TColumnValue commentCol = new TColumnValue();
        commentCol.setString_val(Strings.nullToEmpty(field.getComment()));
        descResult.results.add(new TResultRow(Lists.newArrayList(colNameCol, dataTypeCol, commentCol)));
    }
    return descResult;
}
#end_block

#method_before
public static TDescribeResult buildDescribeMinimalResult(Table table) {
    if (!(table instanceof KuduTable)) {
        return buildDescribeMinimalResult(table.getHiveColumnsAsStruct());
    }
    KuduTable kuduTable = (KuduTable) table;
    TDescribeResult descResult = new TDescribeResult();
    descResult.results = Lists.newArrayList();
    for (Column c : table.getColumnsInHiveOrder()) {
        Preconditions.checkState(c instanceof KuduColumn);
        KuduColumn kuduColumn = (KuduColumn) c;
        // General describe info.
        TColumnValue colNameCol = new TColumnValue();
        colNameCol.setString_val(kuduColumn.getName());
        TColumnValue dataTypeCol = new TColumnValue();
        dataTypeCol.setString_val(kuduColumn.getType().prettyPrint().toLowerCase());
        TColumnValue commentCol = new TColumnValue();
        commentCol.setString_val(kuduColumn.getComment() != null ? kuduColumn.getComment() : "");
        // Kudu-specific describe info.
        TColumnValue pkCol = new TColumnValue();
        pkCol.setString_val(Boolean.toString(kuduTable.isPrimaryKeyColumn(kuduColumn.getName())));
        TColumnValue nullableCol = new TColumnValue();
        nullableCol.setString_val(Boolean.toString(kuduColumn.isNullable()));
        // TODO: Add encoding and compression once available.
        TColumnValue encodingCol = new TColumnValue();
        encodingCol.setString_val("N/A");
        TColumnValue compressionCol = new TColumnValue();
        compressionCol.setString_val("N/A");
        descResult.results.add(new TResultRow(Lists.newArrayList(colNameCol, dataTypeCol, commentCol, pkCol, nullableCol, encodingCol, compressionCol)));
    }
    return descResult;
}
#method_after
public static TDescribeResult buildDescribeMinimalResult(Table table) {
    if (!(table instanceof KuduTable)) {
        return buildDescribeMinimalResult(table.getHiveColumnsAsStruct());
    }
    TDescribeResult descResult = new TDescribeResult();
    descResult.results = Lists.newArrayList();
    for (Column c : table.getColumnsInHiveOrder()) {
        Preconditions.checkState(c instanceof KuduColumn);
        KuduColumn kuduColumn = (KuduColumn) c;
        // General describe info.
        TColumnValue colNameCol = new TColumnValue();
        colNameCol.setString_val(kuduColumn.getName());
        TColumnValue dataTypeCol = new TColumnValue();
        dataTypeCol.setString_val(kuduColumn.getType().prettyPrint().toLowerCase());
        TColumnValue commentCol = new TColumnValue();
        commentCol.setString_val(Strings.nullToEmpty(kuduColumn.getComment()));
        // Kudu-specific describe info.
        TColumnValue pkCol = new TColumnValue();
        pkCol.setString_val(Boolean.toString(kuduColumn.isKey()));
        TColumnValue nullableCol = new TColumnValue();
        nullableCol.setString_val(Boolean.toString(kuduColumn.isNullable()));
        TColumnValue defaultValCol = new TColumnValue();
        if (kuduColumn.hasDefaultValue()) {
            defaultValCol.setString_val(kuduColumn.getDefaultValue().getStringValue());
        } else {
            defaultValCol.setString_val("");
        }
        TColumnValue encodingCol = new TColumnValue();
        encodingCol.setString_val(kuduColumn.getEncoding().toString());
        TColumnValue compressionCol = new TColumnValue();
        compressionCol.setString_val(kuduColumn.getCompression().toString());
        TColumnValue blockSizeCol = new TColumnValue();
        blockSizeCol.setString_val(Integer.toString(kuduColumn.getBlockSize()));
        descResult.results.add(new TResultRow(Lists.newArrayList(colNameCol, dataTypeCol, commentCol, pkCol, nullableCol, defaultValCol, encodingCol, compressionCol, blockSizeCol)));
    }
    return descResult;
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    try {
        path_ = analyzer.resolvePath(rawPath_, PathType.ANY);
    } catch (AnalysisException ae) {
        // table/database if the user is not authorized.
        if (analyzer.hasMissingTbls())
            throw ae;
        if (rawPath_.size() > 1) {
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath_.get(0), rawPath_.get(1)).allOf(getPrivilegeRequirement()).toRequest());
        }
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(analyzer.getDefaultDb(), rawPath_.get(0)).allOf(getPrivilegeRequirement()).toRequest());
        throw ae;
    } catch (TableLoadingException tle) {
        throw new AnalysisException(tle.getMessage(), tle);
    }
    table_ = path_.getRootTable();
    // Register authorization and audit events.
    analyzer.getTable(table_.getTableName(), getPrivilegeRequirement());
    if (path_.destTable() != null) {
        // Describing a table.
        return;
    } else if (path_.destType().isComplexType()) {
        if (outputStyle_ == TDescribeOutputStyle.FORMATTED || outputStyle_ == TDescribeOutputStyle.EXTENDED) {
            throw new AnalysisException("DESCRIBE FORMATTED|EXTENDED must refer to a table");
        }
        // Describing a nested collection.
        Preconditions.checkState(outputStyle_ == TDescribeOutputStyle.MINIMAL);
        resultStruct_ = Path.getTypeAsStruct(path_.destType());
    } else {
        throw new AnalysisException("Cannot describe path '" + Strings.join(rawPath_, ".") + "' targeting scalar type: " + path_.destType().toSql());
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    try {
        path_ = analyzer.resolvePath(rawPath_, PathType.ANY);
    } catch (AnalysisException ae) {
        // table/database if the user is not authorized.
        if (analyzer.hasMissingTbls())
            throw ae;
        if (rawPath_.size() > 1) {
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath_.get(0), rawPath_.get(1)).allOf(getPrivilegeRequirement()).toRequest());
        }
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(analyzer.getDefaultDb(), rawPath_.get(0)).allOf(getPrivilegeRequirement()).toRequest());
        throw ae;
    } catch (TableLoadingException tle) {
        throw new AnalysisException(tle.getMessage(), tle);
    }
    table_ = path_.getRootTable();
    // Register authorization and audit events.
    analyzer.getTable(table_.getTableName(), getPrivilegeRequirement());
    // Describing a table.
    if (path_.destTable() != null)
        return;
    if (path_.destType().isComplexType()) {
        if (outputStyle_ == TDescribeOutputStyle.FORMATTED || outputStyle_ == TDescribeOutputStyle.EXTENDED) {
            throw new AnalysisException("DESCRIBE FORMATTED|EXTENDED must refer to a table");
        }
        // Describing a nested collection.
        Preconditions.checkState(outputStyle_ == TDescribeOutputStyle.MINIMAL);
        resultStruct_ = Path.getTypeAsStruct(path_.destType());
    } else {
        throw new AnalysisException("Cannot describe path '" + Joiner.on('.').join(rawPath_) + "' targeting scalar type: " + path_.destType().toSql());
    }
}
#end_block

#method_before
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryCtx().disable_codegen = true;
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createClusteringSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (BackendConfig.INSTANCE.getComputeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#method_after
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryCtx().disable_codegen_hint = true;
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createClusteringSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (BackendConfig.INSTANCE.getComputeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            // Lineage is not currently supported for Kudu tables (see IMPALA-4283)
            if (targetTable instanceof KuduTable)
                return fragments;
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#end_block

#method_before
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    TableSink tableSink = TableSink.create(table_, TableSink.Op.DELETE, ImmutableList.<Expr>of(), referencedColumns_, false, ignoreNotFound_, false);
    Preconditions.checkState(!referencedColumns_.isEmpty());
    return tableSink;
}
#method_after
@Override
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    TableSink tableSink = TableSink.create(table_, TableSink.Op.DELETE, ImmutableList.<Expr>of(), referencedColumns_, false, false);
    Preconditions.checkState(!referencedColumns_.isEmpty());
    return tableSink;
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder b = new StringBuilder();
    b.append("DELETE");
    if (ignoreNotFound_)
        b.append(" IGNORE");
    if (fromClause_.size() > 1 || targetTableRef_.hasExplicitAlias()) {
        b.append(" ");
        if (targetTableRef_.hasExplicitAlias()) {
            b.append(targetTableRef_.getExplicitAlias());
        } else {
            b.append(targetTableRef_.toSql());
        }
    }
    b.append(fromClause_.toSql());
    if (wherePredicate_ != null) {
        b.append(" WHERE ");
        b.append(wherePredicate_.toSql());
    }
    return b.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder b = new StringBuilder();
    b.append("DELETE");
    if (fromClause_.size() > 1 || targetTableRef_.hasExplicitAlias()) {
        b.append(" ");
        if (targetTableRef_.hasExplicitAlias()) {
            b.append(targetTableRef_.getExplicitAlias());
        } else {
            b.append(targetTableRef_.toSql());
        }
    }
    b.append(fromClause_.toSql());
    if (wherePredicate_ != null) {
        b.append(" WHERE ");
        b.append(wherePredicate_.toSql());
    }
    return b.toString();
}
#end_block

#method_before
public static TableSink create(Table table, Op sinkAction, List<Expr> partitionKeyExprs, List<Integer> referencedColumns, boolean overwrite, boolean ignoreDuplicates, boolean input_is_clustered) {
    if (table instanceof HdfsTable) {
        // Hdfs only supports inserts.
        Preconditions.checkState(sinkAction == Op.INSERT);
        // Referenced columns don't make sense for an Hdfs table.
        Preconditions.checkState(referencedColumns.isEmpty());
        return new HdfsTableSink(table, partitionKeyExprs, overwrite, input_is_clustered);
    } else if (table instanceof HBaseTable) {
        // HBase only supports inserts.
        Preconditions.checkState(sinkAction == Op.INSERT);
        // Partition clause doesn't make sense for an HBase table.
        Preconditions.checkState(partitionKeyExprs.isEmpty());
        // HBase doesn't have a way to perform INSERT OVERWRITE
        Preconditions.checkState(overwrite == false);
        // Referenced columns don't make sense for an HBase table.
        Preconditions.checkState(referencedColumns.isEmpty());
        // Create the HBaseTableSink and return it.
        return new HBaseTableSink(table);
    } else if (table instanceof KuduTable) {
        // Kudu doesn't have a way to perform INSERT OVERWRITE.
        Preconditions.checkState(overwrite == false);
        // Partition clauses don't make sense for Kudu inserts.
        Preconditions.checkState(partitionKeyExprs.isEmpty());
        return new KuduTableSink(table, sinkAction, referencedColumns, ignoreDuplicates);
    } else {
        throw new UnsupportedOperationException("Cannot create data sink into table of type: " + table.getClass().getName());
    }
}
#method_after
public static TableSink create(Table table, Op sinkAction, List<Expr> partitionKeyExprs, List<Integer> referencedColumns, boolean overwrite, boolean inputIsClustered) {
    if (table instanceof HdfsTable) {
        // Hdfs only supports inserts.
        Preconditions.checkState(sinkAction == Op.INSERT);
        // Referenced columns don't make sense for an Hdfs table.
        Preconditions.checkState(referencedColumns.isEmpty());
        return new HdfsTableSink(table, partitionKeyExprs, overwrite, inputIsClustered);
    } else if (table instanceof HBaseTable) {
        // HBase only supports inserts.
        Preconditions.checkState(sinkAction == Op.INSERT);
        // Partition clause doesn't make sense for an HBase table.
        Preconditions.checkState(partitionKeyExprs.isEmpty());
        // HBase doesn't have a way to perform INSERT OVERWRITE
        Preconditions.checkState(overwrite == false);
        // Referenced columns don't make sense for an HBase table.
        Preconditions.checkState(referencedColumns.isEmpty());
        // Create the HBaseTableSink and return it.
        return new HBaseTableSink(table);
    } else if (table instanceof KuduTable) {
        // Kudu doesn't have a way to perform INSERT OVERWRITE.
        Preconditions.checkState(overwrite == false);
        // Partition clauses don't make sense for Kudu inserts.
        Preconditions.checkState(partitionKeyExprs.isEmpty());
        return new KuduTableSink(table, sinkAction, referencedColumns);
    } else {
        throw new UnsupportedOperationException("Cannot create data sink into table of type: " + table.getClass().getName());
    }
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    hasClusteredHint_ = false;
    resultExprs_.clear();
    primaryKeyExprs_.clear();
}
#method_after
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    hasClusteredHint_ = false;
    resultExprs_.clear();
    mentionedColumns_.clear();
    primaryKeyExprs_.clear();
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Subqueries need to be rewritten by the StmtRewriter first.
            if (analyzer.containsSubquery())
                return;
            // Use getResultExprs() and not getBaseTblResultExprs() here because the final
            // substitution with TupleIsNullPredicate() wrapping happens in planning.
            selectListExprs = Expr.cloneList(queryStmt_.getResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    setTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Subqueries need to be rewritten by the StmtRewriter first.
            if (analyzer.containsSubquery())
                return;
            // Use getResultExprs() and not getBaseTblResultExprs() here because the final
            // substitution with TupleIsNullPredicate() wrapping happens in planning.
            selectListExprs = Expr.cloneList(queryStmt_.getResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    analyzeTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions, unless the target is
    // a Kudu table, in which case we don't want to overwrite unmentioned columns with
    // NULL.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#end_block

#method_before
private void checkColumnCoverage(ArrayList<Column> selectExprTargetColumns, Set<String> mentionedColumnNames, int numSelectListExprs, int numStaticPartitionExprs) throws AnalysisException {
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Check that all columns are mentioned by the permutation and partition clauses
    if (selectExprTargetColumns.size() + numStaticPartitionExprs != table_.getColumns().size()) {
        // We've already ruled out too many columns in the permutation and partition clauses
        // by checking that there are no duplicates and that every column mentioned actually
        // exists. So all columns aren't mentioned in the query. If the unmentioned columns
        // include partition columns, this is an error.
        List<String> missingColumnNames = Lists.newArrayList();
        for (Column column : table_.getColumns()) {
            if (!mentionedColumnNames.contains(column.getName())) {
                // otherwise happen by default).
                if (isHBaseTable && column.getPosition() == 0) {
                    throw new AnalysisException("Row-key column '" + column.getName() + "' must be explicitly mentioned in column permutation.");
                }
                if (column.getPosition() < numClusteringCols) {
                    missingColumnNames.add(column.getName());
                }
            }
        }
        if (!missingColumnNames.isEmpty()) {
            throw new AnalysisException("Not enough partition columns mentioned in query. Missing columns are: " + Joiner.on(", ").join(missingColumnNames));
        }
    }
    // Expect the selectListExpr to have entries for every target column
    if (selectExprTargetColumns.size() != numSelectListExprs) {
        String comparator = (selectExprTargetColumns.size() < numSelectListExprs) ? "fewer" : "more";
        String partitionClause = (partitionKeyValues_ == null) ? "returns" : "and PARTITION clause return";
        // select-list and the permutation itself.
        if (columnPermutation_ == null) {
            int totalColumnsMentioned = numSelectListExprs + numStaticPartitionExprs;
            throw new AnalysisException(String.format("Target table '%s' has %s columns (%s) than the SELECT / VALUES clause %s" + " (%s)", table_.getFullName(), comparator, table_.getColumns().size(), partitionClause, totalColumnsMentioned));
        } else {
            String partitionPrefix = (partitionKeyValues_ == null) ? "mentions" : "and PARTITION clause mention";
            throw new AnalysisException(String.format("Column permutation %s %s columns (%s) than " + "the SELECT / VALUES clause %s (%s)", partitionPrefix, comparator, selectExprTargetColumns.size(), partitionClause, numSelectListExprs));
        }
    }
}
#method_after
private void checkColumnCoverage(ArrayList<Column> selectExprTargetColumns, Set<String> mentionedColumnNames, int numSelectListExprs, int numStaticPartitionExprs) throws AnalysisException {
    // Check that all required cols are mentioned by the permutation and partition clauses
    if (selectExprTargetColumns.size() + numStaticPartitionExprs != table_.getColumns().size()) {
        // exists. So all columns aren't mentioned in the query.
        if (table_ instanceof KuduTable) {
            checkRequiredKuduColumns(mentionedColumnNames);
        } else if (table_ instanceof HBaseTable) {
            checkRequiredHBaseColumns(mentionedColumnNames);
        } else if (table_.getNumClusteringCols() > 0) {
            checkRequiredPartitionedColumns(mentionedColumnNames);
        }
    }
    // Expect the selectListExpr to have entries for every target column
    if (selectExprTargetColumns.size() != numSelectListExprs) {
        String comparator = (selectExprTargetColumns.size() < numSelectListExprs) ? "fewer" : "more";
        String partitionClause = (partitionKeyValues_ == null) ? "returns" : "and PARTITION clause return";
        // select-list and the permutation itself.
        if (columnPermutation_ == null) {
            int totalColumnsMentioned = numSelectListExprs + numStaticPartitionExprs;
            throw new AnalysisException(String.format("Target table '%s' has %s columns (%s) than the SELECT / VALUES clause %s" + " (%s)", table_.getFullName(), comparator, table_.getColumns().size(), partitionClause, totalColumnsMentioned));
        } else {
            String partitionPrefix = (partitionKeyValues_ == null) ? "mentions" : "and PARTITION clause mention";
            throw new AnalysisException(String.format("Column permutation %s %s columns (%s) than " + "the SELECT / VALUES clause %s (%s)", partitionPrefix, comparator, selectExprTargetColumns.size(), partitionClause, numSelectListExprs));
        }
    }
}
#end_block

#method_before
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    // order, and add NULL expressions to all missing columns.
    for (Column tblColumn : table_.getColumnsInHiveOrder()) {
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                matchFound = true;
                break;
            }
        }
        // expression.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                // Unmentioned non-clustering columns get NULL literals with the appropriate
                // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                resultExprs_.add(NullLiteral.create(tblColumn.getType()));
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && table_ instanceof KuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#method_after
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    boolean isKuduTable = table_ instanceof KuduTable;
    // Finally, 'undo' the permutation so that the selectListExprs are in Hive column
    // order, and add NULL expressions to all missing columns, unless this is an UPSERT.
    ArrayList<Column> columns = table_.getColumnsInHiveOrder();
    for (int col = 0; col < columns.size(); ++col) {
        Column tblColumn = columns.get(col);
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                if (isKuduTable)
                    mentionedColumns_.add(col);
                matchFound = true;
                break;
            }
        }
        // expression if this is an INSERT and the target is not a Kudu table.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                if (isKuduTable) {
                    Preconditions.checkState(tblColumn instanceof KuduColumn);
                    KuduColumn kuduCol = (KuduColumn) tblColumn;
                    if (!kuduCol.hasDefaultValue() && !kuduCol.isNullable()) {
                        throw new AnalysisException("Missing values for column that is not " + "nullable and has no default value " + kuduCol.getName());
                    }
                } else {
                    // Unmentioned non-clustering columns get NULL literals with the appropriate
                    // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                    resultExprs_.add(NullLiteral.create(tblColumn.getType()));
                }
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && isKuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#end_block

#method_before
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    return TableSink.create(table_, TableSink.Op.INSERT, partitionKeyExprs_, ImmutableList.<Integer>of(), overwrite_, ignoreDuplicates_, hasClusteredHint_);
}
#method_after
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    return TableSink.create(table_, isUpsert_ ? TableSink.Op.UPSERT : TableSink.Op.INSERT, partitionKeyExprs_, mentionedColumns_, overwrite_, hasClusteredHint_);
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    if (withClause_ != null)
        strBuilder.append(withClause_.toSql() + " ");
    strBuilder.append("INSERT ");
    if (overwrite_) {
        strBuilder.append("OVERWRITE ");
    } else {
        if (ignoreDuplicates_)
            strBuilder.append("IGNORE ");
        strBuilder.append("INTO ");
    }
    strBuilder.append("TABLE " + originalTableName_);
    if (columnPermutation_ != null) {
        strBuilder.append("(");
        strBuilder.append(Joiner.on(", ").join(columnPermutation_));
        strBuilder.append(")");
    }
    if (partitionKeyValues_ != null) {
        List<String> values = Lists.newArrayList();
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            values.add(pkv.getColName() + (pkv.getValue() != null ? ("=" + pkv.getValue().toSql()) : ""));
        }
        strBuilder.append(" PARTITION (" + Joiner.on(", ").join(values) + ")");
    }
    if (planHints_ != null) {
        strBuilder.append(" " + ToSqlUtils.getPlanHintsSql(planHints_));
    }
    if (!needsGeneratedQueryStatement_) {
        strBuilder.append(" " + queryStmt_.toSql());
    }
    return strBuilder.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    if (withClause_ != null)
        strBuilder.append(withClause_.toSql() + " ");
    strBuilder.append(getOpName() + " ");
    if (overwrite_) {
        strBuilder.append("OVERWRITE ");
    } else {
        strBuilder.append("INTO ");
    }
    strBuilder.append("TABLE " + originalTableName_);
    if (columnPermutation_ != null) {
        strBuilder.append("(");
        strBuilder.append(Joiner.on(", ").join(columnPermutation_));
        strBuilder.append(")");
    }
    if (partitionKeyValues_ != null) {
        List<String> values = Lists.newArrayList();
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            values.add(pkv.getColName() + (pkv.getValue() != null ? ("=" + pkv.getValue().toSql()) : ""));
        }
        strBuilder.append(" PARTITION (" + Joiner.on(", ").join(values) + ")");
    }
    if (planHints_ != null) {
        strBuilder.append(" " + ToSqlUtils.getPlanHintsSql(planHints_));
    }
    if (!needsGeneratedQueryStatement_) {
        strBuilder.append(" " + queryStmt_.toSql());
    }
    return strBuilder.toString();
}
#end_block

#method_before
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    DataSink dataSink = TableSink.create(table_, TableSink.Op.UPDATE, ImmutableList.<Expr>of(), referencedColumns_, false, ignoreNotFound_, false);
    Preconditions.checkState(!referencedColumns_.isEmpty());
    return dataSink;
}
#method_after
@Override
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    DataSink dataSink = TableSink.create(table_, TableSink.Op.UPDATE, ImmutableList.<Expr>of(), referencedColumns_, false, false);
    Preconditions.checkState(!referencedColumns_.isEmpty());
    return dataSink;
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder b = new StringBuilder();
    b.append("UPDATE ");
    if (ignoreNotFound_)
        b.append("IGNORE ");
    if (fromClause_ == null) {
        b.append(targetTableRef_.toSql());
    } else {
        if (targetTableRef_.hasExplicitAlias()) {
            b.append(targetTableRef_.getExplicitAlias());
        } else {
            b.append(targetTableRef_.toSql());
        }
    }
    b.append(" SET");
    boolean first = true;
    for (Pair<SlotRef, Expr> i : assignments_) {
        if (!first) {
            b.append(",");
        } else {
            first = false;
        }
        b.append(format(" %s = %s", i.first.toSql(), i.second.toSql()));
    }
    b.append(fromClause_.toSql());
    if (wherePredicate_ != null) {
        b.append(" WHERE ");
        b.append(wherePredicate_.toSql());
    }
    return b.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder b = new StringBuilder();
    b.append("UPDATE ");
    if (fromClause_ == null) {
        b.append(targetTableRef_.toSql());
    } else {
        if (targetTableRef_.hasExplicitAlias()) {
            b.append(targetTableRef_.getExplicitAlias());
        } else {
            b.append(targetTableRef_.toSql());
        }
    }
    b.append(" SET");
    boolean first = true;
    for (Pair<SlotRef, Expr> i : assignments_) {
        if (!first) {
            b.append(",");
        } else {
            first = false;
        }
        b.append(format(" %s = %s", i.first.toSql(), i.second.toSql()));
    }
    b.append(fromClause_.toSql());
    if (wherePredicate_ != null) {
        b.append(" WHERE ");
        b.append(wherePredicate_.toSql());
    }
    return b.toString();
}
#end_block

#method_before
@Override
protected TDataSink toThrift() {
    TDataSink result = new TDataSink(TDataSinkType.TABLE_SINK);
    THdfsTableSink hdfsTableSink = new THdfsTableSink(Expr.treesToThrift(partitionKeyExprs_), overwrite_, input_is_clustered_);
    HdfsTable table = (HdfsTable) targetTable_;
    StringBuilder error = new StringBuilder();
    int skipHeaderLineCount = table.parseSkipHeaderLineCount(error);
    // Errors will be caught during analysis.
    Preconditions.checkState(error.length() == 0);
    if (skipHeaderLineCount > 0) {
        hdfsTableSink.setSkip_header_line_count(skipHeaderLineCount);
    }
    TTableSink tTableSink = new TTableSink(DescriptorTable.TABLE_SINK_ID, TTableSinkType.HDFS, sinkOp_.toThrift());
    tTableSink.hdfs_table_sink = hdfsTableSink;
    result.table_sink = tTableSink;
    return result;
}
#method_after
@Override
protected TDataSink toThrift() {
    TDataSink result = new TDataSink(TDataSinkType.TABLE_SINK);
    THdfsTableSink hdfsTableSink = new THdfsTableSink(Expr.treesToThrift(partitionKeyExprs_), overwrite_, inputIsClustered_);
    HdfsTable table = (HdfsTable) targetTable_;
    StringBuilder error = new StringBuilder();
    int skipHeaderLineCount = table.parseSkipHeaderLineCount(error);
    // Errors will be caught during analysis.
    Preconditions.checkState(error.length() == 0);
    if (skipHeaderLineCount > 0) {
        hdfsTableSink.setSkip_header_line_count(skipHeaderLineCount);
    }
    TTableSink tTableSink = new TTableSink(DescriptorTable.TABLE_SINK_ID, TTableSinkType.HDFS, sinkOp_.toThrift());
    tTableSink.hdfs_table_sink = hdfsTableSink;
    result.table_sink = tTableSink;
    return result;
}
#end_block

#method_before
public static TQueryCtx createQueryContext(String defaultDb, String user) {
    TQueryCtx queryCtx = new TQueryCtx();
    queryCtx.setRequest(new TClientRequest("FeTests", new TQueryOptions()));
    queryCtx.setQuery_id(new TUniqueId());
    queryCtx.setSession(new TSessionState(new TUniqueId(), TSessionType.BEESWAX, defaultDb, user, new TNetworkAddress("localhost", 0)));
    SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSSSSSSSS");
    queryCtx.setNow_string(formatter.format(Calendar.getInstance().getTime()));
    queryCtx.setUnix_millis(System.currentTimeMillis());
    queryCtx.setPid(1000);
    return queryCtx;
}
#method_after
public static TQueryCtx createQueryContext(String defaultDb, String user) {
    TQueryCtx queryCtx = new TQueryCtx();
    queryCtx.setRequest(new TClientRequest("FeTests", new TQueryOptions()));
    queryCtx.setQuery_id(new TUniqueId());
    queryCtx.setSession(new TSessionState(new TUniqueId(), TSessionType.BEESWAX, defaultDb, user, new TNetworkAddress("localhost", 0)));
    SimpleDateFormat formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSSSSSSSS");
    queryCtx.setNow_string(formatter.format(Calendar.getInstance().getTime()));
    queryCtx.setStart_unix_millis(System.currentTimeMillis());
    queryCtx.setPid(1000);
    return queryCtx;
}
#end_block

#method_before
private void init(Analyzer analyzer) {
    Preconditions.checkNotNull(analyzer);
    Preconditions.checkState(analyzer.isRootAnalyzer());
    TQueryCtx queryCtx = analyzer.getQueryCtx();
    if (queryCtx.request.isSetRedacted_stmt()) {
        queryStr_ = queryCtx.request.redacted_stmt;
    } else {
        queryStr_ = queryCtx.request.stmt;
    }
    Preconditions.checkNotNull(queryStr_);
    timestamp_ = queryCtx.unix_millis / 1000;
    descTbl_ = analyzer.getDescTbl();
    user_ = analyzer.getUser().getName();
}
#method_after
private void init(Analyzer analyzer) {
    Preconditions.checkNotNull(analyzer);
    Preconditions.checkState(analyzer.isRootAnalyzer());
    TQueryCtx queryCtx = analyzer.getQueryCtx();
    if (queryCtx.request.isSetRedacted_stmt()) {
        queryStr_ = queryCtx.request.redacted_stmt;
    } else {
        queryStr_ = queryCtx.request.stmt;
    }
    Preconditions.checkNotNull(queryStr_);
    timestamp_ = queryCtx.start_unix_millis / 1000;
    descTbl_ = analyzer.getDescTbl();
    user_ = analyzer.getUser().getName();
}
#end_block

#method_before
private static Schema createTableSchema(TCreateTableParams params) throws ImpalaRuntimeException {
    Set<String> keyColNames = new HashSet<>(params.getPrimary_key_column_names());
    List<ColumnSchema> colSchemas = new ArrayList<>(params.getColumnsSize());
    for (TColumn column : params.getColumns()) {
        Type type = Type.fromThrift(column.getColumnType());
        Preconditions.checkState(type != null);
        org.apache.kudu.Type kuduType = KuduUtil.fromImpalaType(type);
        // Create the actual column and check if the column is a key column
        ColumnSchemaBuilder csb = new ColumnSchemaBuilder(column.getColumnName(), kuduType);
        Preconditions.checkState(column.isSetIs_key());
        boolean isKeyCol = column.isIs_key();
        if (isKeyCol) {
            Preconditions.checkState(keyColNames.contains(column.getColumnName()));
        }
        csb.key(isKeyCol);
        if (column.isSetIs_nullable())
            csb.nullable(column.isIs_nullable());
        if (column.isSetDefault_value()) {
            csb.defaultValue(KuduUtil.getKuduDefaultValue(column.getDefault_value(), kuduType, column.getColumnName()));
        }
        if (column.isSetBlock_size())
            csb.desiredBlockSize(column.getBlock_size());
        if (column.isSetEncoding()) {
            csb.encoding(KuduUtil.fromThrift(column.getEncoding()));
        }
        if (column.isSetCompression()) {
            csb.compressionAlgorithm(KuduUtil.fromThrift(column.getCompression()));
        }
        colSchemas.add(csb.build());
    }
    return new Schema(colSchemas);
}
#method_after
private static Schema createTableSchema(TCreateTableParams params) throws ImpalaRuntimeException {
    Set<String> keyColNames = new HashSet<>(params.getPrimary_key_column_names());
    List<ColumnSchema> colSchemas = new ArrayList<>(params.getColumnsSize());
    for (TColumn column : params.getColumns()) {
        Type type = Type.fromThrift(column.getColumnType());
        Preconditions.checkState(type != null);
        org.apache.kudu.Type kuduType = KuduUtil.fromImpalaType(type);
        // Create the actual column and check if the column is a key column
        ColumnSchemaBuilder csb = new ColumnSchemaBuilder(column.getColumnName(), kuduType);
        Preconditions.checkState(column.isSetIs_key());
        csb.key(keyColNames.contains(column.getColumnName()));
        if (column.isSetIs_nullable())
            csb.nullable(column.isIs_nullable());
        if (column.isSetDefault_value()) {
            csb.defaultValue(KuduUtil.getKuduDefaultValue(column.getDefault_value(), kuduType, column.getColumnName()));
        }
        if (column.isSetBlock_size())
            csb.desiredBlockSize(column.getBlock_size());
        if (column.isSetEncoding()) {
            csb.encoding(KuduUtil.fromThrift(column.getEncoding()));
        }
        if (column.isSetCompression()) {
            csb.compressionAlgorithm(KuduUtil.fromThrift(column.getCompression()));
        }
        colSchemas.add(csb.build());
    }
    return new Schema(colSchemas);
}
#end_block

#method_before
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Check whether the column name meets the Metastore's requirements.
    if (!MetaStoreUtils.validateName(colName_)) {
        throw new AnalysisException("Invalid column/field name: " + colName_);
    }
    if (typeDef_ != null) {
        typeDef_.analyze(null);
        type_ = typeDef_.getType();
    }
    Preconditions.checkNotNull(type_);
    Preconditions.checkState(type_.isValid());
    // Check HMS constraints of type and comment.
    String typeSql = type_.toSql();
    if (typeSql.length() > MetaStoreUtil.MAX_TYPE_NAME_LENGTH) {
        throw new AnalysisException(String.format("Type of column '%s' exceeds maximum type length of %d characters:\n" + "%s has %d characters.", colName_, MetaStoreUtil.MAX_TYPE_NAME_LENGTH, typeSql, typeSql.length()));
    }
    if (hasKuduSpecificOptions()) {
        Preconditions.checkNotNull(analyzer);
        analyzeKuduOptions(analyzer);
    }
    if (comment_ != null && comment_.length() > MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH) {
        throw new AnalysisException(String.format("Comment of column '%s' exceeds maximum length of %d characters:\n" + "%s has %d characters.", colName_, MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH, comment_, comment_.length()));
    }
}
#method_after
public void analyze(Analyzer analyzer) throws AnalysisException {
    // Check whether the column name meets the Metastore's requirements.
    if (!MetaStoreUtils.validateName(colName_)) {
        throw new AnalysisException("Invalid column/field name: " + colName_);
    }
    if (typeDef_ != null) {
        typeDef_.analyze(null);
        type_ = typeDef_.getType();
    }
    Preconditions.checkNotNull(type_);
    Preconditions.checkState(type_.isValid());
    // Check HMS constraints of type and comment.
    String typeSql = type_.toSql();
    if (typeSql.length() > MetaStoreUtil.MAX_TYPE_NAME_LENGTH) {
        throw new AnalysisException(String.format("Type of column '%s' exceeds maximum type length of %d characters:\n" + "%s has %d characters.", colName_, MetaStoreUtil.MAX_TYPE_NAME_LENGTH, typeSql, typeSql.length()));
    }
    if (hasKuduOptions()) {
        Preconditions.checkNotNull(analyzer);
        analyzeKuduOptions(analyzer);
    }
    if (comment_ != null && comment_.length() > MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH) {
        throw new AnalysisException(String.format("Comment of column '%s' exceeds maximum length of %d characters:\n" + "%s has %d characters.", colName_, MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH, comment_, comment_.length()));
    }
}
#end_block

#method_before
private void analyzeKuduOptions(Analyzer analyzer) throws AnalysisException {
    if (isPrimaryKey_ && isNullable_ != null && isNullable_) {
        throw new AnalysisException("Primary key columns can't have NULL values: " + toString());
    }
    // Encoding value
    if (!Strings.isNullOrEmpty(encodingVal_)) {
        try {
            encoding_ = Encoding.valueOf(encodingVal_);
        } catch (IllegalArgumentException e) {
            throw new AnalysisException(String.format("Unsupported encoding value '%s'. " + "Supported encoding values are: %s", encodingVal_, Joiner.on(", ").join(Encoding.values())));
        }
    }
    // Compression algorithm
    if (!Strings.isNullOrEmpty(compressionVal_)) {
        try {
            compression_ = CompressionAlgorithm.valueOf(compressionVal_);
        } catch (IllegalArgumentException e) {
            throw new AnalysisException(String.format("Unsupported compression " + "algorithm '%s'. Supported compression algorithms are: %s", compressionVal_, Joiner.on(", ").join(CompressionAlgorithm.values())));
        }
    }
    // single function.
    if (defaultValue_ != null) {
        try {
            defaultValue_.analyze(analyzer);
        } catch (AnalysisException e) {
            throw new AnalysisException(String.format("Only constant values are allowed " + "for default values: %s", defaultValue_.toSql()), e);
        }
        if (!defaultValue_.isConstant()) {
            throw new AnalysisException(String.format("Only constant values are allowed " + "for default values: %s", defaultValue_.toSql()));
        }
        defaultValue_ = LiteralExpr.create(defaultValue_, analyzer.getQueryCtx());
        if (defaultValue_ == null) {
            throw new AnalysisException(String.format("Only constant values are allowed " + "for default values: %s", defaultValue_.toSql()));
        }
        if (defaultValue_.getType().isNull() && ((isNullable_ != null && !isNullable_) || isPrimaryKey_)) {
            throw new AnalysisException(String.format("NULL values are not allowed for " + "column '%s': %s", getColName(), defaultValue_.toSql()));
        }
        if (!Type.isImplicitlyCastable(defaultValue_.getType(), type_, true)) {
            throw new AnalysisException(String.format("Default value %s (type: %s) " + "is not compatible with column '%s' (type: %s).", defaultValue_.toSql(), defaultValue_.getType().toSql(), colName_, type_.toSql()));
        }
        if (!defaultValue_.getType().equals(type_)) {
            Expr castLiteral = defaultValue_.uncheckedCastTo(type_);
            Preconditions.checkNotNull(castLiteral);
            defaultValue_ = LiteralExpr.create(castLiteral, analyzer.getQueryCtx());
        }
        Preconditions.checkNotNull(defaultValue_);
    }
    // Analyze the block size value, if any.
    if (blockSize_ != null) {
        blockSize_.analyze(null);
        Integer val = Ints.tryParse(blockSize_.toSql());
        if (val == null) {
            throw new AnalysisException(String.format("Invalid value for BLOCK_SIZE: %s. " + "A positive INTEGER value is expected.", blockSize_.toSql()));
        }
    }
}
#method_after
private void analyzeKuduOptions(Analyzer analyzer) throws AnalysisException {
    if (isPrimaryKey_ && isNullable_ != null && isNullable_) {
        throw new AnalysisException("Primary key columns cannot be nullable: " + toString());
    }
    // Encoding value
    if (encodingVal_ != null) {
        try {
            encoding_ = Encoding.valueOf(encodingVal_);
        } catch (IllegalArgumentException e) {
            throw new AnalysisException(String.format("Unsupported encoding value '%s'. " + "Supported encoding values are: %s", encodingVal_, Joiner.on(", ").join(Encoding.values())));
        }
    }
    // Compression algorithm
    if (compressionVal_ != null) {
        try {
            compression_ = CompressionAlgorithm.valueOf(compressionVal_);
        } catch (IllegalArgumentException e) {
            throw new AnalysisException(String.format("Unsupported compression " + "algorithm '%s'. Supported compression algorithms are: %s", compressionVal_, Joiner.on(", ").join(CompressionAlgorithm.values())));
        }
    }
    // single function.
    if (defaultValue_ != null) {
        try {
            defaultValue_.analyze(analyzer);
        } catch (AnalysisException e) {
            throw new AnalysisException(String.format("Only constant values are allowed " + "for default values: %s", defaultValue_.toSql()), e);
        }
        if (!defaultValue_.isConstant()) {
            throw new AnalysisException(String.format("Only constant values are allowed " + "for default values: %s", defaultValue_.toSql()));
        }
        defaultValue_ = LiteralExpr.create(defaultValue_, analyzer.getQueryCtx());
        if (defaultValue_ == null) {
            throw new AnalysisException(String.format("Only constant values are allowed " + "for default values: %s", defaultValue_.toSql()));
        }
        if (defaultValue_.getType().isNull() && ((isNullable_ != null && !isNullable_) || isPrimaryKey_)) {
            throw new AnalysisException(String.format("Default value of NULL not allowed " + "on non-nullable column: '%s'", getColName()));
        }
        if (!Type.isImplicitlyCastable(defaultValue_.getType(), type_, true)) {
            throw new AnalysisException(String.format("Default value %s (type: %s) " + "is not compatible with column '%s' (type: %s).", defaultValue_.toSql(), defaultValue_.getType().toSql(), colName_, type_.toSql()));
        }
        if (!defaultValue_.getType().equals(type_)) {
            Expr castLiteral = defaultValue_.uncheckedCastTo(type_);
            Preconditions.checkNotNull(castLiteral);
            defaultValue_ = LiteralExpr.create(castLiteral, analyzer.getQueryCtx());
        }
        Preconditions.checkNotNull(defaultValue_);
    }
    // Analyze the block size value, if any.
    if (blockSize_ != null) {
        blockSize_.analyze(null);
        if (!blockSize_.getType().isIntegerType()) {
            throw new AnalysisException(String.format("Invalid value for BLOCK_SIZE: %s. " + "A positive INTEGER value is expected.", blockSize_.toSql()));
        }
    }
}
#end_block

#method_before
public TColumn toThrift() {
    TColumn col = new TColumn(getColName(), type_.toThrift());
    Integer blockSize = blockSize_ == null ? null : (int) ((NumericLiteral) blockSize_).getIntValue();
    KuduUtil.toTKuduColumnHelper(col, isPrimaryKey_, isNullable_, encoding_, compression_, defaultValue_, blockSize);
    if (comment_ != null)
        col.setComment(comment_);
    return col;
}
#method_after
public TColumn toThrift() {
    TColumn col = new TColumn(getColName(), type_.toThrift());
    Integer blockSize = blockSize_ == null ? null : (int) ((NumericLiteral) blockSize_).getIntValue();
    KuduUtil.setColumnOptions(col, isPrimaryKey_, isNullable_, encoding_, compression_, defaultValue_, blockSize);
    if (comment_ != null)
        col.setComment(comment_);
    return col;
}
#end_block

#method_before
public static List<ColumnDef> reconcileSchemas(List<ColumnDef> colDefs, List<ColumnDef> avroCols, StringBuilder warning) {
    if (colDefs.size() != avroCols.size()) {
        warning.append(String.format("Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has %s column(s) but %s column definition(s) were given.", avroCols.size(), colDefs.size()));
        return avroCols;
    }
    List<ColumnDef> result = Lists.newArrayListWithCapacity(colDefs.size());
    for (int i = 0; i < avroCols.size(); ++i) {
        ColumnDef colDef = colDefs.get(i);
        ColumnDef avroCol = avroCols.get(i);
        Preconditions.checkNotNull(colDef.getType());
        Preconditions.checkNotNull(avroCol.getType());
        // are taken from the Avro schema.
        if ((colDef.getType().isStringType() && avroCol.getType().isStringType())) {
            Preconditions.checkState(avroCol.getType().getPrimitiveType() == PrimitiveType.STRING);
            Map<ColumnDef.Option, Object> option = Maps.newHashMap();
            option.put(ColumnDef.Option.COMMENT, avroCol.getComment());
            ColumnDef reconciledColDef = new ColumnDef(avroCol.getColName(), colDef.getTypeDef(), option);
            try {
                reconciledColDef.analyze(null);
            } catch (AnalysisException e) {
                Preconditions.checkNotNull(null, "reconciledColDef.analyze() should never throw.");
            }
            result.add(reconciledColDef);
        } else {
            result.add(avroCol);
        }
        // Populate warning string if there are name and/or type inconsistencies.
        if (!colDef.getColName().equals(avroCol.getColName()) || !colDef.getType().equals(avroCol.getType())) {
            if (warning.length() == 0) {
                // Add warning preamble for the first mismatch.
                warning.append("Resolved the following name and/or type inconsistencies " + "between the column definitions and the Avro schema.\n");
            }
            warning.append(String.format("Column definition at position %s:  %s %s\n", i, colDefs.get(i).getColName(), colDefs.get(i).getType().toSql()));
            warning.append(String.format("Avro schema column at position %s: %s %s\n", i, avroCols.get(i).getColName(), avroCols.get(i).getType().toSql()));
            warning.append(String.format("Resolution at position %s: %s %s\n", i, result.get(i).getColName(), result.get(i).getType().toSql()));
        }
    }
    Preconditions.checkState(result.size() == avroCols.size());
    Preconditions.checkState(result.size() == colDefs.size());
    return result;
}
#method_after
public static List<ColumnDef> reconcileSchemas(List<ColumnDef> colDefs, List<ColumnDef> avroCols, StringBuilder warning) {
    if (colDefs.size() != avroCols.size()) {
        warning.append(String.format("Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has %s column(s) but %s column definition(s) were given.", avroCols.size(), colDefs.size()));
        return avroCols;
    }
    List<ColumnDef> result = Lists.newArrayListWithCapacity(colDefs.size());
    for (int i = 0; i < avroCols.size(); ++i) {
        ColumnDef colDef = colDefs.get(i);
        ColumnDef avroCol = avroCols.get(i);
        Preconditions.checkNotNull(colDef.getType());
        Preconditions.checkNotNull(avroCol.getType());
        // are taken from the Avro schema.
        if ((colDef.getType().isStringType() && avroCol.getType().isStringType())) {
            Preconditions.checkState(avroCol.getType().getPrimitiveType() == PrimitiveType.STRING);
            Map<ColumnDef.Option, Object> option = Maps.newHashMap();
            String comment = avroCol.getComment();
            if (comment != null)
                option.put(ColumnDef.Option.COMMENT, comment);
            ColumnDef reconciledColDef = new ColumnDef(avroCol.getColName(), colDef.getTypeDef(), option);
            try {
                reconciledColDef.analyze(null);
            } catch (AnalysisException e) {
                Preconditions.checkNotNull(null, "reconciledColDef.analyze() should never throw.");
            }
            result.add(reconciledColDef);
        } else {
            result.add(avroCol);
        }
        // Populate warning string if there are name and/or type inconsistencies.
        if (!colDef.getColName().equals(avroCol.getColName()) || !colDef.getType().equals(avroCol.getType())) {
            if (warning.length() == 0) {
                // Add warning preamble for the first mismatch.
                warning.append("Resolved the following name and/or type inconsistencies " + "between the column definitions and the Avro schema.\n");
            }
            warning.append(String.format("Column definition at position %s:  %s %s\n", i, colDefs.get(i).getColName(), colDefs.get(i).getType().toSql()));
            warning.append(String.format("Avro schema column at position %s: %s %s\n", i, avroCols.get(i).getColName(), avroCols.get(i).getType().toSql()));
            warning.append(String.format("Resolution at position %s: %s %s\n", i, result.get(i).getColName(), result.get(i).getType().toSql()));
        }
    }
    Preconditions.checkState(result.size() == avroCols.size());
    Preconditions.checkState(result.size() == colDefs.size());
    return result;
}
#end_block

#method_before
public static List<FieldSchema> toFieldSchemas(List<Column> columns) {
    return Lists.transform(columns, new Function<Column, FieldSchema>() {

        public FieldSchema apply(Column column) {
            Preconditions.checkNotNull(column.getType());
            return new FieldSchema(column.getName(), column.getType().toSql(), column.getComment());
        }
    });
}
#method_after
public static List<FieldSchema> toFieldSchemas(List<Column> columns) {
    return Lists.transform(columns, new Function<Column, FieldSchema>() {

        public FieldSchema apply(Column column) {
            Preconditions.checkNotNull(column.getType());
            return new FieldSchema(column.getName(), column.getType().toSql().toLowerCase(), column.getComment());
        }
    });
}
#end_block

#method_before
public static List<ColumnDef> parse(String schemaStr) throws SchemaParseException, AnalysisException {
    Schema.Parser avroSchemaParser = new Schema.Parser();
    Schema schema = avroSchemaParser.parse(schemaStr);
    if (!schema.getType().equals(Schema.Type.RECORD)) {
        throw new UnsupportedOperationException("Schema for table must be of type " + "RECORD. Received type: " + schema.getType());
    }
    List<ColumnDef> colDefs = Lists.newArrayListWithCapacity(schema.getFields().size());
    for (Schema.Field field : schema.getFields()) {
        Map<ColumnDef.Option, Object> option = Maps.newHashMap();
        option.put(ColumnDef.Option.COMMENT, field.doc());
        ColumnDef colDef = new ColumnDef(field.name(), new TypeDef(getTypeInfo(field.schema(), field.name())), option);
        colDef.analyze(null);
        colDefs.add(colDef);
    }
    return colDefs;
}
#method_after
public static List<ColumnDef> parse(String schemaStr) throws SchemaParseException, AnalysisException {
    Schema.Parser avroSchemaParser = new Schema.Parser();
    Schema schema = avroSchemaParser.parse(schemaStr);
    if (!schema.getType().equals(Schema.Type.RECORD)) {
        throw new UnsupportedOperationException("Schema for table must be of type " + "RECORD. Received type: " + schema.getType());
    }
    List<ColumnDef> colDefs = Lists.newArrayListWithCapacity(schema.getFields().size());
    for (Schema.Field field : schema.getFields()) {
        Map<ColumnDef.Option, Object> option = Maps.newHashMap();
        String comment = field.doc();
        if (comment != null)
            option.put(ColumnDef.Option.COMMENT, comment);
        ColumnDef colDef = new ColumnDef(field.name(), new TypeDef(getTypeInfo(field.schema(), field.name())), option);
        colDef.analyze(null);
        colDefs.add(colDef);
    }
    return colDefs;
}
#end_block

#method_before
@Test
public void TestAlterTableAddDropPartition() throws CatalogException {
    String[] addDrop = { "add if not exists", "drop if exists" };
    for (String kw : addDrop) {
        // Add different partitions for different column types
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=2050, month=10)");
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(month=10, year=2050)");
        AnalyzesOk("alter table functional.insert_string_partitioned " + kw + " partition(s2='1234')");
        // Can't add/drop partitions to/from unpartitioned tables
        AnalysisError("alter table functional.alltypesnopart " + kw + " partition (i=1)", "Table is not partitioned: functional.alltypesnopart");
        AnalysisError("alter table functional_hbase.alltypesagg " + kw + " partition (i=1)", "Table is not partitioned: functional_hbase.alltypesagg");
        // Duplicate partition key name
        AnalysisError("alter table functional.alltypes " + kw + " partition(year=2050, year=2051)", "Duplicate partition key name: year");
        // Not a partition column
        AnalysisError("alter table functional.alltypes " + kw + " partition(year=2050, int_col=1)", "Column 'int_col' is not a partition column in table: functional.alltypes");
        // NULL partition keys
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=NULL, month=1)");
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=NULL, month=NULL)");
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=ascii(null), month=ascii(NULL))");
        // Empty string partition keys
        AnalyzesOk("alter table functional.insert_string_partitioned " + kw + " partition(s2='')");
        // Arbitrary exprs as partition key values. Constant exprs are ok.
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=-1, month=cast((10+5*4) as INT))");
        // Arbitrary exprs as partition key values. Non-constant exprs should fail.
        AnalysisError("alter table functional.alltypes " + kw + " partition(year=2050, month=int_col)", "Non-constant expressions are not supported as static partition-key values " + "in 'month=int_col'.");
        AnalysisError("alter table functional.alltypes " + kw + " partition(year=cast(int_col as int), month=12)", "Non-constant expressions are not supported as static partition-key values " + "in 'year=CAST(int_col AS INT)'.");
        // Not a valid column
        AnalysisError("alter table functional.alltypes " + kw + " partition(year=2050, blah=1)", "Partition column 'blah' not found in table: functional.alltypes");
        // Data types don't match
        AnalysisError("alter table functional.insert_string_partitioned " + kw + " partition(s2=1234)", "Value of partition spec (column=s2) has incompatible type: 'SMALLINT'. " + "Expected type: 'STRING'.");
        // Loss of precision
        AnalysisError("alter table functional.alltypes " + kw + " partition(year=100000000000, month=10)", "Partition key value may result in loss of precision.\nWould need to cast" + " '100000000000' to 'INT' for partition column: year");
        // Table/Db does not exist
        AnalysisError("alter table db_does_not_exist.alltypes " + kw + " partition (i=1)", "Database does not exist: db_does_not_exist");
        AnalysisError("alter table functional.table_does_not_exist " + kw + " partition (i=1)", "Table does not exist: functional.table_does_not_exist");
        // Cannot ALTER TABLE a view.
        AnalysisError("alter table functional.alltypes_view " + kw + " partition(year=2050, month=10)", "ALTER TABLE not allowed on a view: functional.alltypes_view");
        AnalysisError("alter table functional.alltypes_datasource " + kw + " partition(year=2050, month=10)", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    }
    // IF NOT EXISTS properly checks for partition existence
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10)");
    AnalysisError("alter table functional.alltypes add " + "partition(year=2010, month=10)", "Partition spec already exists: (year=2010, month=10).");
    AnalyzesOk("alter table functional.alltypes add if not exists " + " partition(year=2010, month=10)");
    AnalyzesOk("alter table functional.alltypes add if not exists " + " partition(year=2010, month=10) location " + "'/test-warehouse/alltypes/year=2010/month=10'");
    // IF EXISTS properly checks for partition existence
    AnalyzesOk("alter table functional.alltypes drop " + "partition(year=2010, month=10)");
    AnalysisError("alter table functional.alltypes drop " + "partition(year=2050, month=10)", "Partition spec does not exist: (year=2050, month=10).");
    AnalyzesOk("alter table functional.alltypes drop if exists " + "partition(year=2050, month=10)");
    // Caching ops
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) cached in 'testPool' with replication = 10");
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) uncached");
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, month=10) cached in 'badPool'", "The specified cache pool does not exist: badPool");
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, month=10) location " + "'file:///test-warehouse/alltypes/year=2010/month=10' cached in 'testPool'", "Location 'file:/test-warehouse/alltypes/year=2010/month=10' cannot be cached. " + "Please retry without caching: ALTER TABLE functional.alltypes ADD PARTITION " + "... UNCACHED");
    // Valid URIs.
    AnalyzesOk("alter table functional.alltypes add " + " partition(year=2050, month=10) location " + "'/test-warehouse/alltypes/year=2010/month=10'");
    AnalyzesOk("alter table functional.alltypes add " + " partition(year=2050, month=10) location " + "'hdfs://localhost:20500/test-warehouse/alltypes/year=2010/month=10'");
    AnalyzesOk("alter table functional.alltypes add " + " partition(year=2050, month=10) location " + "'s3n://bucket/test-warehouse/alltypes/year=2010/month=10'");
    AnalyzesOk("alter table functional.alltypes add " + " partition(year=2050, month=10) location " + "'file:///test-warehouse/alltypes/year=2010/month=10'");
    // Invalid URIs.
    AnalysisError("alter table functional.alltypes add " + " partition(year=2050, month=10) location " + "'foofs://bar/test-warehouse/alltypes/year=2010/month=10'", "No FileSystem for scheme: foofs");
    AnalysisError("alter table functional.alltypes add " + " partition(year=2050, month=10) location '  '", "URI path cannot be empty.");
}
#method_after
@Test
public void TestAlterTableAddDropPartition() throws CatalogException {
    String[] addDrop = { "add if not exists", "drop if exists" };
    for (String kw : addDrop) {
        // Add different partitions for different column types
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=2050, month=10)");
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(month=10, year=2050)");
        AnalyzesOk("alter table functional.insert_string_partitioned " + kw + " partition(s2='1234')");
        // Can't add/drop partitions to/from unpartitioned tables
        AnalysisError("alter table functional.alltypesnopart " + kw + " partition (i=1)", "Table is not partitioned: functional.alltypesnopart");
        AnalysisError("alter table functional_hbase.alltypesagg " + kw + " partition (i=1)", "Table is not partitioned: functional_hbase.alltypesagg");
        // Empty string partition keys
        AnalyzesOk("alter table functional.insert_string_partitioned " + kw + " partition(s2='')");
        // Arbitrary exprs as partition key values. Constant exprs are ok.
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=-1, month=cast((10+5*4) as INT))");
        // Table/Db does not exist
        AnalysisError("alter table db_does_not_exist.alltypes " + kw + " partition (i=1)", "Could not resolve table reference: " + "'db_does_not_exist.alltypes'");
        AnalysisError("alter table functional.table_does_not_exist " + kw + " partition (i=1)", "Could not resolve table reference: " + "'functional.table_does_not_exist'");
        // Cannot ALTER TABLE a view.
        AnalysisError("alter table functional.alltypes_view " + kw + " partition(year=2050, month=10)", "ALTER TABLE not allowed on a view: functional.alltypes_view");
        AnalysisError("alter table functional.alltypes_datasource " + kw + " partition(year=2050, month=10)", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
        // NULL partition keys
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=NULL, month=1)");
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=NULL, month=NULL)");
        AnalyzesOk("alter table functional.alltypes " + kw + " partition(year=ascii(null), month=ascii(NULL))");
    }
    // Data types don't match
    AnalysisError("alter table functional.insert_string_partitioned add" + " partition(s2=1234)", "Value of partition spec (column=s2) has incompatible type: " + "'SMALLINT'. Expected type: 'STRING'.");
    AnalysisError("alter table functional.insert_string_partitioned drop" + " partition(s2=1234)", "operands of type STRING and SMALLINT are not comparable: s2 = 1234");
    // Loss of precision
    AnalysisError("alter table functional.alltypes add " + "partition(year=100000000000, month=10) ", "Partition key value may result in loss of precision.\nWould need to cast " + "'100000000000' to 'INT' for partition column: year");
    // Duplicate partition key name
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, year=2051)", "Duplicate partition key name: year");
    // Arbitrary exprs as partition key values. Non-constant exprs should fail.
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, month=int_col) ", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    AnalysisError("alter table functional.alltypes add " + "partition(year=cast(int_col as int), month=12) ", "Non-constant expressions are not supported as static partition-key " + "values in 'year=CAST(int_col AS INT)'.");
    // Not a partition column
    AnalysisError("alter table functional.alltypes drop " + "partition(year=2050, int_col=1)", "Partition exprs cannot contain non-partition column(s): int_col = 1.");
    // Arbitrary exprs as partition key values. Non-partition columns should fail.
    AnalysisError("alter table functional.alltypes drop " + "partition(year=2050, month=int_col) ", "Partition exprs cannot contain non-partition column(s): month = int_col.");
    AnalysisError("alter table functional.alltypes drop " + "partition(year=cast(int_col as int), month=12) ", "Partition exprs cannot contain non-partition column(s): " + "year = CAST(int_col AS INT).");
    // IF NOT EXISTS properly checks for partition existence
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10)");
    AnalysisError("alter table functional.alltypes add " + "partition(year=2010, month=10)", "Partition spec already exists: (year=2010, month=10).");
    AnalyzesOk("alter table functional.alltypes add if not exists " + "partition(year=2010, month=10)");
    AnalyzesOk("alter table functional.alltypes add if not exists " + "partition(year=2010, month=10) location " + "'/test-warehouse/alltypes/year=2010/month=10'");
    // IF EXISTS properly checks for partition existence
    // with a fully specified partition.
    AnalyzesOk("alter table functional.alltypes drop " + "partition(year=2010, month=10)");
    AnalysisError("alter table functional.alltypes drop " + "partition(year=2050, month=10)", "No matching partition(s) found.");
    AnalyzesOk("alter table functional.alltypes drop if exists " + "partition(year=2050, month=10)");
    // NULL partition keys
    AnalysisError("alter table functional.alltypes drop " + "partition(year=NULL, month=1)", "No matching partition(s) found.");
    AnalysisError("alter table functional.alltypes drop " + "partition(year=NULL, month is NULL)", "No matching partition(s) found.");
    // Drop partition using predicates
    // IF EXISTS is added here
    AnalyzesOk("alter table functional.alltypes drop " + "partition(year<2011, month!=10)");
    AnalysisError("alter table functional.alltypes drop " + "partition(1=1, month=10)", "Invalid partition expr 1 = 1. " + "A partition spec may not contain constant predicates.");
    AnalyzesOk("alter table functional.alltypes drop " + "partition(year>1050, month=10)");
    AnalyzesOk("alter table functional.alltypes drop " + "partition(year>1050 and month=10)");
    AnalyzesOk("alter table functional.alltypes drop " + "partition(month=10)");
    AnalyzesOk("alter table functional.alltypes drop " + "partition(month+2000=year)");
    AnalyzesOk("alter table functional.alltypes drop " + "partition(year>9050, month=10)");
    AnalyzesOk("alter table functional.alltypes drop if exists " + "partition(year>9050, month=10)");
    // Not a valid column
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, blah=1)", "Partition column 'blah' not found in table: functional.alltypes");
    AnalysisError("alter table functional.alltypes drop " + "partition(year=2050, blah=1)", "Could not resolve column/field reference: 'blah'");
    // Not a partition column
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, int_col=1) ", "Column 'int_col' is not a partition column in table: functional.alltypes");
    // Caching ops
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) cached in 'testPool' with replication = 10");
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) uncached");
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, month=10) cached in 'badPool'", "The specified cache pool does not exist: badPool");
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, month=10) location " + "'file:///test-warehouse/alltypes/year=2010/month=10' cached in 'testPool'", "Location 'file:/test-warehouse/alltypes/year=2010/month=10' cannot be cached. " + "Please retry without caching: ALTER TABLE functional.alltypes ADD PARTITION " + "... UNCACHED");
    // Valid URIs.
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) location " + "'/test-warehouse/alltypes/year=2010/month=10'");
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) location " + "'hdfs://localhost:20500/test-warehouse/alltypes/year=2010/month=10'");
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) location " + "'s3n://bucket/test-warehouse/alltypes/year=2010/month=10'");
    AnalyzesOk("alter table functional.alltypes add " + "partition(year=2050, month=10) location " + "'file:///test-warehouse/alltypes/year=2010/month=10'");
    // Invalid URIs.
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, month=10) location " + "'foofs://bar/test-warehouse/alltypes/year=2010/month=10'", "No FileSystem for scheme: foofs");
    AnalysisError("alter table functional.alltypes add " + "partition(year=2050, month=10) location '  '", "URI path cannot be empty.");
}
#end_block

#method_before
@Test
public void TestAlterTableAddReplaceColumns() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes add columns (new_col int)");
    AnalyzesOk("alter table functional.alltypes add columns (c1 string comment 'hi')");
    AnalyzesOk("alter table functional.alltypes add columns (c struct<f1:int>)");
    AnalyzesOk("alter table functional.alltypes replace columns (c1 int comment 'c', c2 int)");
    AnalyzesOk("alter table functional.alltypes replace columns (c array<string>)");
    // Column name must be unique for add
    AnalysisError("alter table functional.alltypes add columns (int_col int)", "Column already exists: int_col");
    // Add a column with same name as a partition column
    AnalysisError("alter table functional.alltypes add columns (year int)", "Column name conflicts with existing partition column: year");
    // Invalid column name.
    AnalysisError("alter table functional.alltypes add columns (`???` int)", "Invalid column/field name: ???");
    AnalysisError("alter table functional.alltypes replace columns (`???` int)", "Invalid column/field name: ???");
    // Replace should not throw an error if the column already exists
    AnalyzesOk("alter table functional.alltypes replace columns (int_col int)");
    // It is not possible to replace a partition column
    AnalysisError("alter table functional.alltypes replace columns (Year int)", "Column name conflicts with existing partition column: year");
    // Duplicate column names
    AnalysisError("alter table functional.alltypes add columns (c1 int, c1 int)", "Duplicate column name: c1");
    AnalysisError("alter table functional.alltypes replace columns (c1 int, C1 int)", "Duplicate column name: c1");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes add columns (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist add columns (i int)", "Table does not exist: functional.table_does_not_exist");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view " + "add columns (c1 string comment 'hi')", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource " + "add columns (c1 string comment 'hi')", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE ADD/REPLACE COLUMNS on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes add columns (i int)", "ALTER TABLE ADD|REPLACE COLUMNS not currently supported on HBase tables.");
}
#method_after
@Test
public void TestAlterTableAddReplaceColumns() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes add columns (new_col int)");
    AnalyzesOk("alter table functional.alltypes add columns (c1 string comment 'hi')");
    AnalyzesOk("alter table functional.alltypes add columns (c struct<f1:int>)");
    AnalyzesOk("alter table functional.alltypes replace columns (c1 int comment 'c', c2 int)");
    AnalyzesOk("alter table functional.alltypes replace columns (c array<string>)");
    // Column name must be unique for add
    AnalysisError("alter table functional.alltypes add columns (int_col int)", "Column already exists: int_col");
    // Add a column with same name as a partition column
    AnalysisError("alter table functional.alltypes add columns (year int)", "Column name conflicts with existing partition column: year");
    // Invalid column name.
    AnalysisError("alter table functional.alltypes add columns (`???` int)", "Invalid column/field name: ???");
    AnalysisError("alter table functional.alltypes replace columns (`???` int)", "Invalid column/field name: ???");
    // Replace should not throw an error if the column already exists
    AnalyzesOk("alter table functional.alltypes replace columns (int_col int)");
    // It is not possible to replace a partition column
    AnalysisError("alter table functional.alltypes replace columns (Year int)", "Column name conflicts with existing partition column: year");
    // Duplicate column names
    AnalysisError("alter table functional.alltypes add columns (c1 int, c1 int)", "Duplicate column name: c1");
    AnalysisError("alter table functional.alltypes replace columns (c1 int, C1 int)", "Duplicate column name: c1");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes add columns (i int)", "Could not resolve table reference: 'db_does_not_exist.alltypes'");
    AnalysisError("alter table functional.table_does_not_exist add columns (i int)", "Could not resolve table reference: 'functional.table_does_not_exist'");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view " + "add columns (c1 string comment 'hi')", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE a nested collection.
    AnalysisError("alter table allcomplextypes.int_array_col " + "add columns (c1 string comment 'hi')", createAnalyzer("functional"), "ALTER TABLE not allowed on a nested collection: allcomplextypes.int_array_col");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource " + "add columns (c1 string comment 'hi')", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE ADD/REPLACE COLUMNS on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes add columns (i int)", "ALTER TABLE ADD|REPLACE COLUMNS not currently supported on HBase tables.");
}
#end_block

#method_before
@Test
public void TestAlterTableDropColumn() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes drop column int_col");
    AnalysisError("alter table functional.alltypes drop column no_col", "Column 'no_col' does not exist in table: functional.alltypes");
    AnalysisError("alter table functional.alltypes drop column year", "Cannot drop partition column: year");
    // Tables should always have at least 1 column
    AnalysisError("alter table functional_seq_snap.bad_seq_snap drop column field", "Cannot drop column 'field' from functional_seq_snap.bad_seq_snap. " + "Tables must contain at least 1 column.");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes drop column col1", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist drop column col1", "Table does not exist: functional.table_does_not_exist");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view drop column int_col", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource drop column int_col", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE DROP COLUMN on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes drop column int_col", "ALTER TABLE DROP COLUMN not currently supported on HBase tables.");
}
#method_after
@Test
public void TestAlterTableDropColumn() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes drop column int_col");
    AnalysisError("alter table functional.alltypes drop column no_col", "Column 'no_col' does not exist in table: functional.alltypes");
    AnalysisError("alter table functional.alltypes drop column year", "Cannot drop partition column: year");
    // Tables should always have at least 1 column
    AnalysisError("alter table functional_seq_snap.bad_seq_snap drop column field", "Cannot drop column 'field' from functional_seq_snap.bad_seq_snap. " + "Tables must contain at least 1 column.");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes drop column col1", "Could not resolve table reference: 'db_does_not_exist.alltypes'");
    AnalysisError("alter table functional.table_does_not_exist drop column col1", "Could not resolve table reference: 'functional.table_does_not_exist'");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view drop column int_col", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE a nested collection.
    AnalysisError("alter table allcomplextypes.int_array_col drop column int_col", createAnalyzer("functional"), "ALTER TABLE not allowed on a nested collection: allcomplextypes.int_array_col");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource drop column int_col", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE DROP COLUMN on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes drop column int_col", "ALTER TABLE DROP COLUMN not currently supported on HBase tables.");
}
#end_block

#method_before
@Test
public void TestAlterTableChangeColumn() throws AnalysisException {
    // Rename a column
    AnalyzesOk("alter table functional.alltypes change column int_col int_col2 int");
    // Rename and change the datatype
    AnalyzesOk("alter table functional.alltypes change column int_col c2 string");
    AnalyzesOk("alter table functional.alltypes change column int_col c2 map<int, string>");
    // Change only the datatype
    AnalyzesOk("alter table functional.alltypes change column int_col int_col tinyint");
    // Add a comment to a column.
    AnalyzesOk("alter table functional.alltypes change int_col int_col int comment 'c'");
    AnalysisError("alter table functional.alltypes change column no_col c1 int", "Column 'no_col' does not exist in table: functional.alltypes");
    AnalysisError("alter table functional.alltypes change column year year int", "Cannot modify partition column: year");
    AnalysisError("alter table functional.alltypes change column int_col Tinyint_col int", "Column already exists: tinyint_col");
    // Invalid column name.
    AnalysisError("alter table functional.alltypes change column int_col `???` int", "Invalid column/field name: ???");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes change c1 c2 int", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist change c1 c2 double", "Table does not exist: functional.table_does_not_exist");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view " + "change column int_col int_col2 int", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource " + "change column int_col int_col2 int", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE CHANGE COLUMN on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes CHANGE COLUMN int_col i int", "ALTER TABLE CHANGE COLUMN not currently supported on HBase tables.");
}
#method_after
@Test
public void TestAlterTableChangeColumn() throws AnalysisException {
    // Rename a column
    AnalyzesOk("alter table functional.alltypes change column int_col int_col2 int");
    // Rename and change the datatype
    AnalyzesOk("alter table functional.alltypes change column int_col c2 string");
    AnalyzesOk("alter table functional.alltypes change column int_col c2 map<int, string>");
    // Change only the datatype
    AnalyzesOk("alter table functional.alltypes change column int_col int_col tinyint");
    // Add a comment to a column.
    AnalyzesOk("alter table functional.alltypes change int_col int_col int comment 'c'");
    AnalysisError("alter table functional.alltypes change column no_col c1 int", "Column 'no_col' does not exist in table: functional.alltypes");
    AnalysisError("alter table functional.alltypes change column year year int", "Cannot modify partition column: year");
    AnalysisError("alter table functional.alltypes change column int_col Tinyint_col int", "Column already exists: tinyint_col");
    // Invalid column name.
    AnalysisError("alter table functional.alltypes change column int_col `???` int", "Invalid column/field name: ???");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes change c1 c2 int", "Could not resolve table reference: 'db_does_not_exist.alltypes'");
    AnalysisError("alter table functional.table_does_not_exist change c1 c2 double", "Could not resolve table reference: 'functional.table_does_not_exist'");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view " + "change column int_col int_col2 int", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE a nested collection.
    AnalysisError("alter table allcomplextypes.int_array_col " + "change column int_col int_col2 int", createAnalyzer("functional"), "ALTER TABLE not allowed on a nested collection: allcomplextypes.int_array_col");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource " + "change column int_col int_col2 int", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE CHANGE COLUMN on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes CHANGE COLUMN int_col i int", "ALTER TABLE CHANGE COLUMN not currently supported on HBase tables.");
}
#end_block

#method_before
@Test
public void TestAlterTableSet() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes set serdeproperties('a'='2')");
    AnalyzesOk("alter table functional.alltypes PARTITION (Year=2010, month=11) " + "set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes PARTITION (month=11, year=2010) " + "set fileformat parquetfile");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition1') set fileformat parquet");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='PaRtiTion1') set location '/a/b/c'");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set serdeproperties ('a'='2')");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("alter table functional.alltypes " + "set serdeproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        AnalyzesOk("alter table functional.alltypes " + "set tblproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set tblproperties('storage_handler'='1')", "Changing the 'storage_handler' table property is not supported to protect " + "against metadata corruption.");
    }
    // Arbitrary exprs as partition key values. Constant exprs are ok.
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set location '/a/b'");
    // Arbitrary exprs as partition key values. Non-constant exprs should fail.
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set fileformat sequencefile", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set location '/a/b'", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    // Partition spec does not exist
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set location '/a/b'", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set tblproperties('a'='1')", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010) " + "set tblproperties('a'='1')", "Items in partition spec must exactly match the partition columns " + "in the table definition: functional.alltypes (1 vs 2)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010, year=2010) " + "set location '/a/b'", "Duplicate partition key name: year");
    AnalysisError("alter table functional.alltypes PARTITION (month=11, year=2014) " + "set fileformat sequencefile", "Partition spec does not exist: (month=11, year=2014)");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set fileformat sequencefile", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set location '/a/b/c'", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set location '/a/b'", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set fileformat sequencefile", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set location '/a/b'", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set fileformat sequencefile", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes set fileformat sequencefile", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set fileformat rcfile", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table db_does_not_exist.alltypes set location '/a/b'", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set location '/a/b'", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table functional.no_tbl partition(i=1) set location '/a/b'", "Table does not exist: functional.no_tbl");
    AnalysisError("alter table no_db.alltypes partition(i=1) set fileformat textfile", "Database does not exist: no_db");
    // Valid location
    AnalyzesOk("alter table functional.alltypes set location " + "'hdfs://localhost:20500/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'s3n://bucket/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'file:///test-warehouse/a/b'");
    // Invalid location
    AnalysisError("alter table functional.alltypes set location 'test/warehouse'", "URI path must be absolute: test/warehouse");
    AnalysisError("alter table functional.alltypes set location 'blah:///warehouse/'", "No FileSystem for scheme: blah");
    AnalysisError("alter table functional.alltypes set location ''", "URI path cannot be empty.");
    AnalysisError("alter table functional.alltypes set location '      '", "URI path cannot be empty.");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view set fileformat sequencefile", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource set fileformat parquet", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE SET on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes set tblproperties('a'='b')", "ALTER TABLE SET not currently supported on HBase tables.");
}
#method_after
@Test
public void TestAlterTableSet() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes set serdeproperties('a'='2')");
    AnalyzesOk("alter table functional.alltypes PARTITION (Year=2010, month=11) " + "set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes PARTITION (month=11, year=2010) " + "set fileformat parquetfile");
    AnalyzesOk("alter table functional.alltypes PARTITION (month<=11, year=2010) " + "set fileformat parquetfile");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition1') set fileformat parquet");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='PaRtiTion1') set location '/a/b/c'");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes PARTITION (year<=2010, month=11) " + "set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set serdeproperties ('a'='2')");
    AnalyzesOk("alter table functional.alltypes PARTITION (year<=2010, month=11) " + "set serdeproperties ('a'='2')");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("alter table functional.alltypes " + "set serdeproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        AnalyzesOk("alter table functional.alltypes " + "set tblproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set tblproperties('storage_handler'='1')", "Changing the 'storage_handler' table property is not supported to protect " + "against metadata corruption.");
    }
    // Arbitrary exprs as partition key values. Constant exprs are ok.
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set location '/a/b'");
    // Arbitrary exprs as partition key values. One-partition-column-bound exprs are ok.
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(Year*2=Year+2010, month=11) set fileformat sequencefile");
    // Arbitrary exprs as partition key values. Non-partition-column exprs.
    AnalysisError("alter table functional.alltypes PARTITION " + "(int_col=3) set fileformat sequencefile", "Partition exprs cannot contain non-partition column(s): int_col = 3.");
    // Partition expr matches more than one partition in set location statement.
    AnalysisError("alter table functional.alltypes PARTITION (year!=20) " + "set location '/a/b'", "Partition expr in set location statements can only match " + "one partition. Too many matched partitions year=2009/month=1," + "year=2009/month=2,year=2009/month=3");
    // Partition spec does not exist
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set location '/a/b'", "No matching partition(s) found.");
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set tblproperties('a'='1')", "No matching partition(s) found.");
    AnalysisError("alter table functional.alltypes PARTITION (month=11, year=2014) " + "set fileformat sequencefile", "No matching partition(s) found.");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set fileformat sequencefile", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set location '/a/b/c'", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set location '/a/b'", "No matching partition(s) found.");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set fileformat sequencefile", "No matching partition(s) found.");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set location '/a/b'", "No matching partition(s) found.");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set fileformat sequencefile", "No matching partition(s) found.");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes set fileformat sequencefile", "Could not resolve table reference: 'db_does_not_exist.alltypes'");
    AnalysisError("alter table functional.table_does_not_exist set fileformat rcfile", "Could not resolve table reference: 'functional.table_does_not_exist'");
    AnalysisError("alter table db_does_not_exist.alltypes set location '/a/b'", "Could not resolve table reference: 'db_does_not_exist.alltypes'");
    AnalysisError("alter table functional.table_does_not_exist set location '/a/b'", "Could not resolve table reference: 'functional.table_does_not_exist'");
    AnalysisError("alter table functional.no_tbl partition(i=1) set location '/a/b'", "Could not resolve table reference: 'functional.no_tbl'");
    AnalysisError("alter table no_db.alltypes partition(i=1) set fileformat textfile", "Could not resolve table reference: 'no_db.alltypes'");
    // Valid location
    AnalyzesOk("alter table functional.alltypes set location " + "'hdfs://localhost:20500/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'s3n://bucket/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'file:///test-warehouse/a/b'");
    // Invalid location
    AnalysisError("alter table functional.alltypes set location 'test/warehouse'", "URI path must be absolute: test/warehouse");
    AnalysisError("alter table functional.alltypes set location 'blah:///warehouse/'", "No FileSystem for scheme: blah");
    AnalysisError("alter table functional.alltypes set location ''", "URI path cannot be empty.");
    AnalysisError("alter table functional.alltypes set location '      '", "URI path cannot be empty.");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view set fileformat sequencefile", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE a nested collection.
    AnalysisError("alter table allcomplextypes.int_array_col set fileformat sequencefile", createAnalyzer("functional"), "ALTER TABLE not allowed on a nested collection: allcomplextypes.int_array_col");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource set fileformat parquet", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE SET on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes set tblproperties('a'='b')", "ALTER TABLE SET not currently supported on HBase tables.");
}
#end_block

#method_before
@Test
public void TestAlterTableSetCached() {
    // Positive cases
    AnalyzesOk("alter table functional.alltypesnopart set cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypes set cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypes partition(year=2010, month=12) " + "set cached in 'testPool'");
    // Replication factor
    AnalyzesOk("alter table functional.alltypes set cached in 'testPool' " + "with replication = 10");
    AnalyzesOk("alter table functional.alltypes partition(year=2010, month=12) " + "set cached in 'testPool' with replication = 4");
    AnalysisError("alter table functional.alltypes set cached in 'testPool' " + "with replication = 0", "Cache replication factor must be between 0 and Short.MAX_VALUE");
    AnalysisError("alter table functional.alltypes set cached in 'testPool' " + "with replication = 90000", "Cache replication factor must be between 0 and Short.MAX_VALUE");
    // Attempt to alter a table that is not backed by HDFS.
    AnalysisError("alter table functional_hbase.alltypesnopart set cached in 'testPool'", "ALTER TABLE SET not currently supported on HBase tables.");
    AnalysisError("alter table functional.view_view set cached in 'testPool'", "ALTER TABLE not allowed on a view: functional.view_view");
    AnalysisError("alter table functional.alltypes set cached in 'badPool'", "The specified cache pool does not exist: badPool");
    AnalysisError("alter table functional.alltypes partition(year=2010, month=12) " + "set cached in 'badPool'", "The specified cache pool does not exist: badPool");
    // Attempt to uncache a table that is not cached. Should be a no-op.
    AnalyzesOk("alter table functional.alltypes set uncached");
    AnalyzesOk("alter table functional.alltypes partition(year=2010, month=12) " + "set uncached");
    // Attempt to cache a table that is already cached. Should be a no-op.
    AnalyzesOk("alter table functional.alltypestiny set cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypestiny partition(year=2009, month=1) " + "set cached in 'testPool'");
    // Change location of a cached table/partition
    AnalysisError("alter table functional.alltypestiny set location '/tmp/tiny'", "Target table is cached, please uncache before changing the location using: " + "ALTER TABLE functional.alltypestiny SET UNCACHED");
    AnalysisError("alter table functional.alltypestiny partition (year=2009,month=1) " + "set location '/test-warehouse/new_location'", "Target partition is cached, please uncache before changing the location " + "using: ALTER TABLE functional.alltypestiny PARTITION (year=2009, month=1) " + "SET UNCACHED");
    // Table/db/partition do not exist
    AnalysisError("alter table baddb.alltypestiny set cached in 'testPool'", "Database does not exist: baddb");
    AnalysisError("alter table functional.badtbl set cached in 'testPool'", "Table does not exist: functional.badtbl");
    AnalysisError("alter table functional.alltypestiny partition(year=9999, month=1) " + "set cached in 'testPool'", "Partition spec does not exist: (year=9999, month=1).");
}
#method_after
@Test
public void TestAlterTableSetCached() {
    // Positive cases
    AnalyzesOk("alter table functional.alltypesnopart set cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypes set cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypes partition(year=2010, month=12) " + "set cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypes partition(year<=2010, month<=12) " + "set cached in 'testPool'");
    // Replication factor
    AnalyzesOk("alter table functional.alltypes set cached in 'testPool' " + "with replication = 10");
    AnalyzesOk("alter table functional.alltypes partition(year=2010, month=12) " + "set cached in 'testPool' with replication = 4");
    AnalysisError("alter table functional.alltypes set cached in 'testPool' " + "with replication = 0", "Cache replication factor must be between 0 and Short.MAX_VALUE");
    AnalysisError("alter table functional.alltypes set cached in 'testPool' " + "with replication = 90000", "Cache replication factor must be between 0 and Short.MAX_VALUE");
    // Attempt to alter a table that is not backed by HDFS.
    AnalysisError("alter table functional_hbase.alltypesnopart set cached in 'testPool'", "ALTER TABLE SET not currently supported on HBase tables.");
    AnalysisError("alter table functional.view_view set cached in 'testPool'", "ALTER TABLE not allowed on a view: functional.view_view");
    AnalysisError("alter table allcomplextypes.int_array_col set cached in 'testPool'", createAnalyzer("functional"), "ALTER TABLE not allowed on a nested collection: allcomplextypes.int_array_col");
    AnalysisError("alter table functional.alltypes set cached in 'badPool'", "The specified cache pool does not exist: badPool");
    AnalysisError("alter table functional.alltypes partition(year=2010, month=12) " + "set cached in 'badPool'", "The specified cache pool does not exist: badPool");
    // Attempt to uncache a table that is not cached. Should be a no-op.
    AnalyzesOk("alter table functional.alltypes set uncached");
    AnalyzesOk("alter table functional.alltypes partition(year=2010, month=12) " + "set uncached");
    // Attempt to cache a table that is already cached. Should be a no-op.
    AnalyzesOk("alter table functional.alltypestiny set cached in 'testPool'");
    AnalyzesOk("alter table functional.alltypestiny partition(year=2009, month=1) " + "set cached in 'testPool'");
    // Change location of a cached table/partition
    AnalysisError("alter table functional.alltypestiny set location '/tmp/tiny'", "Target table is cached, please uncache before changing the location using: " + "ALTER TABLE functional.alltypestiny SET UNCACHED");
    AnalysisError("alter table functional.alltypestiny partition (year=2009,month=1) " + "set location '/test-warehouse/new_location'", "Target partition is cached, please uncache before changing the location " + "using: ALTER TABLE functional.alltypestiny PARTITION (year = 2009, month = 1) " + "SET UNCACHED");
    // Table/db/partition do not exist
    AnalysisError("alter table baddb.alltypestiny set cached in 'testPool'", "Could not resolve table reference: 'baddb.alltypestiny'");
    AnalysisError("alter table functional.badtbl set cached in 'testPool'", "Could not resolve table reference: 'functional.badtbl'");
    AnalysisError("alter table functional.alltypestiny partition(year=9999, month=1) " + "set cached in 'testPool'", "No matching partition(s) found.");
}
#end_block

#method_before
@Test
public void TestAlterTableSetColumnStats() {
    // Contains entries of the form 'statsKey'='statsValue' for every
    // stats key. A dummy value is used for 'statsValue'.
    List<String> testKeyValues = Lists.newArrayList();
    for (ColumnStats.StatsKey statsKey : ColumnStats.StatsKey.values()) {
        testKeyValues.add(String.format("'%s'='10'", statsKey));
    }
    // Test updating all stats keys individually.
    for (String kv : testKeyValues) {
        AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col (%s)", kv));
        // Stats key is case-insensitive.
        AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col (%s)", kv.toLowerCase()));
        AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col (%s)", kv.toUpperCase()));
    }
    // Test updating all stats keys at once in a single statement.
    AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col (%s)", Joiner.on(",").join(testKeyValues)));
    // Test setting all stats keys to -1 (unknown).
    for (ColumnStats.StatsKey statsKey : ColumnStats.StatsKey.values()) {
        AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col ('%s'='-1')", statsKey));
    }
    // Duplicate stats keys are valid. The last entry is used.
    AnalyzesOk("alter table functional.alltypes set column stats " + "int_col ('numDVs'='2','numDVs'='3')");
    // Test updating stats on all scalar types.
    for (Type t : Type.getSupportedTypes()) {
        if (t.isNull())
            continue;
        Preconditions.checkState(t.isScalarType());
        String typeStr = t.getPrimitiveType().toString();
        if (t.getPrimitiveType() == PrimitiveType.CHAR || t.getPrimitiveType() == PrimitiveType.VARCHAR) {
            typeStr += "(60)";
        }
        String tblName = "t_" + t.getPrimitiveType();
        addTestTable(String.format("create table %s (c %s)", tblName, typeStr));
        AnalyzesOk(String.format("alter table %s set column stats c ('%s'='100','%s'='10')", tblName, ColumnStats.StatsKey.NUM_DISTINCT_VALUES, ColumnStats.StatsKey.NUM_NULLS));
        // Test setting stats values to -1 (unknown).
        AnalyzesOk(String.format("alter table %s set column stats c ('%s'='-1','%s'='-1')", tblName, ColumnStats.StatsKey.NUM_DISTINCT_VALUES, ColumnStats.StatsKey.NUM_NULLS));
    }
    // Setting stats works on all table types.
    AnalyzesOk("alter table functional_hbase.alltypes set column stats " + "int_col ('numNulls'='2')");
    AnalyzesOk("alter table functional.alltypes_datasource set column stats " + "int_col ('numDVs'='2')");
    if (RuntimeEnv.INSTANCE.isKuduSupported()) {
        AnalyzesOk("alter table functional_kudu.testtbl set column stats " + "name ('numNulls'='2')");
    }
    // Table does not exist.
    AnalysisError("alter table bad_tbl set column stats int_col ('numNulls'='2')", "Table does not exist: default.bad_tbl");
    // Column does not exist.
    AnalysisError("alter table functional.alltypes set column stats bad_col ('numNulls'='2')", "Column 'bad_col' does not exist in table: functional.alltypes");
    // Cannot set column stats of a view.
    AnalysisError("alter table functional.alltypes_view set column stats int_col ('numNulls'='2')", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot set column stats of partition columns.
    AnalysisError("alter table functional.alltypes set column stats month ('numDVs'='10')", "Updating the stats of a partition column is not allowed: month");
    // Cannot set the size of a fixed-length column.
    AnalysisError("alter table functional.alltypes set column stats int_col ('maxSize'='10')", "Cannot update the 'maxSize' stats of column 'int_col' with type 'INT'.\n" + "Changing 'maxSize' is only allowed for variable-length columns.");
    AnalysisError("alter table functional.alltypes set column stats int_col ('avgSize'='10')", "Cannot update the 'avgSize' stats of column 'int_col' with type 'INT'.\n" + "Changing 'avgSize' is only allowed for variable-length columns.");
    // Cannot set column stats of complex-typed columns.
    AnalysisError("alter table functional.allcomplextypes set column stats int_array_col " + "('numNulls'='10')", "Statistics for column 'int_array_col' are not supported because " + "it has type 'ARRAY<INT>'.");
    AnalysisError("alter table functional.allcomplextypes set column stats int_map_col " + "('numDVs'='10')", "Statistics for column 'int_map_col' are not supported because " + "it has type 'MAP<STRING,INT>'.");
    AnalysisError("alter table functional.allcomplextypes set column stats int_struct_col " + "('numDVs'='10')", "Statistics for column 'int_struct_col' are not supported because " + "it has type 'STRUCT<f1:INT,f2:INT>'.");
    // Invalid stats key.
    AnalysisError("alter table functional.alltypes set column stats int_col ('badKey'='10')", "Invalid column stats key: badKey");
    AnalysisError("alter table functional.alltypes set column stats " + "int_col ('numDVs'='10',''='10')", "Invalid column stats key: ");
    // Invalid long stats values.
    AnalysisError("alter table functional.alltypes set column stats int_col ('numDVs'='bad')", "Invalid stats value 'bad' for column stats key: numDVs");
    AnalysisError("alter table functional.alltypes set column stats int_col ('numDVs'='-10')", "Invalid stats value '-10' for column stats key: numDVs");
    // Invalid float stats values.
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='bad')", "Invalid stats value 'bad' for column stats key: avgSize");
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='-1.5')", "Invalid stats value '-1.5' for column stats key: avgSize");
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='-0.5')", "Invalid stats value '-0.5' for column stats key: avgSize");
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='NaN')", "Invalid stats value 'NaN' for column stats key: avgSize");
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='inf')", "Invalid stats value 'inf' for column stats key: avgSize");
}
#method_after
@Test
public void TestAlterTableSetColumnStats() {
    // Contains entries of the form 'statsKey'='statsValue' for every
    // stats key. A dummy value is used for 'statsValue'.
    List<String> testKeyValues = Lists.newArrayList();
    for (ColumnStats.StatsKey statsKey : ColumnStats.StatsKey.values()) {
        testKeyValues.add(String.format("'%s'='10'", statsKey));
    }
    // Test updating all stats keys individually.
    for (String kv : testKeyValues) {
        AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col (%s)", kv));
        // Stats key is case-insensitive.
        AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col (%s)", kv.toLowerCase()));
        AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col (%s)", kv.toUpperCase()));
    }
    // Test updating all stats keys at once in a single statement.
    AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col (%s)", Joiner.on(",").join(testKeyValues)));
    // Test setting all stats keys to -1 (unknown).
    for (ColumnStats.StatsKey statsKey : ColumnStats.StatsKey.values()) {
        AnalyzesOk(String.format("alter table functional.alltypes set column stats string_col ('%s'='-1')", statsKey));
    }
    // Duplicate stats keys are valid. The last entry is used.
    AnalyzesOk("alter table functional.alltypes set column stats " + "int_col ('numDVs'='2','numDVs'='3')");
    // Test updating stats on all scalar types.
    for (Type t : Type.getSupportedTypes()) {
        if (t.isNull())
            continue;
        Preconditions.checkState(t.isScalarType());
        String typeStr = t.getPrimitiveType().toString();
        if (t.getPrimitiveType() == PrimitiveType.CHAR || t.getPrimitiveType() == PrimitiveType.VARCHAR) {
            typeStr += "(60)";
        }
        String tblName = "t_" + t.getPrimitiveType();
        addTestTable(String.format("create table %s (c %s)", tblName, typeStr));
        AnalyzesOk(String.format("alter table %s set column stats c ('%s'='100','%s'='10')", tblName, ColumnStats.StatsKey.NUM_DISTINCT_VALUES, ColumnStats.StatsKey.NUM_NULLS));
        // Test setting stats values to -1 (unknown).
        AnalyzesOk(String.format("alter table %s set column stats c ('%s'='-1','%s'='-1')", tblName, ColumnStats.StatsKey.NUM_DISTINCT_VALUES, ColumnStats.StatsKey.NUM_NULLS));
    }
    // Setting stats works on all table types.
    AnalyzesOk("alter table functional_hbase.alltypes set column stats " + "int_col ('numNulls'='2')");
    AnalyzesOk("alter table functional.alltypes_datasource set column stats " + "int_col ('numDVs'='2')");
    if (RuntimeEnv.INSTANCE.isKuduSupported()) {
        AnalyzesOk("alter table functional_kudu.testtbl set column stats " + "name ('numNulls'='2')");
    }
    // Table does not exist.
    AnalysisError("alter table bad_tbl set column stats int_col ('numNulls'='2')", "Could not resolve table reference: 'bad_tbl'");
    // Column does not exist.
    AnalysisError("alter table functional.alltypes set column stats bad_col ('numNulls'='2')", "Column 'bad_col' does not exist in table: functional.alltypes");
    // Cannot set column stats of a view.
    AnalysisError("alter table functional.alltypes_view set column stats int_col ('numNulls'='2')", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot set column stats of a nested collection.
    AnalysisError("alter table allcomplextypes.int_array_col " + "set column stats int_col ('numNulls'='2')", createAnalyzer("functional"), "ALTER TABLE not allowed on a nested collection: allcomplextypes.int_array_col");
    // Cannot set column stats of partition columns.
    AnalysisError("alter table functional.alltypes set column stats month ('numDVs'='10')", "Updating the stats of a partition column is not allowed: month");
    // Cannot set the size of a fixed-length column.
    AnalysisError("alter table functional.alltypes set column stats int_col ('maxSize'='10')", "Cannot update the 'maxSize' stats of column 'int_col' with type 'INT'.\n" + "Changing 'maxSize' is only allowed for variable-length columns.");
    AnalysisError("alter table functional.alltypes set column stats int_col ('avgSize'='10')", "Cannot update the 'avgSize' stats of column 'int_col' with type 'INT'.\n" + "Changing 'avgSize' is only allowed for variable-length columns.");
    // Cannot set column stats of complex-typed columns.
    AnalysisError("alter table functional.allcomplextypes set column stats int_array_col " + "('numNulls'='10')", "Statistics for column 'int_array_col' are not supported because " + "it has type 'ARRAY<INT>'.");
    AnalysisError("alter table functional.allcomplextypes set column stats int_map_col " + "('numDVs'='10')", "Statistics for column 'int_map_col' are not supported because " + "it has type 'MAP<STRING,INT>'.");
    AnalysisError("alter table functional.allcomplextypes set column stats int_struct_col " + "('numDVs'='10')", "Statistics for column 'int_struct_col' are not supported because " + "it has type 'STRUCT<f1:INT,f2:INT>'.");
    // Invalid stats key.
    AnalysisError("alter table functional.alltypes set column stats int_col ('badKey'='10')", "Invalid column stats key: badKey");
    AnalysisError("alter table functional.alltypes set column stats " + "int_col ('numDVs'='10',''='10')", "Invalid column stats key: ");
    // Invalid long stats values.
    AnalysisError("alter table functional.alltypes set column stats int_col ('numDVs'='bad')", "Invalid stats value 'bad' for column stats key: numDVs");
    AnalysisError("alter table functional.alltypes set column stats int_col ('numDVs'='-10')", "Invalid stats value '-10' for column stats key: numDVs");
    // Invalid float stats values.
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='bad')", "Invalid stats value 'bad' for column stats key: avgSize");
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='-1.5')", "Invalid stats value '-1.5' for column stats key: avgSize");
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='-0.5')", "Invalid stats value '-0.5' for column stats key: avgSize");
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='NaN')", "Invalid stats value 'NaN' for column stats key: avgSize");
    AnalysisError("alter table functional.alltypes set column stats string_col ('avgSize'='inf')", "Invalid stats value 'inf' for column stats key: avgSize");
}
#end_block

#method_before
@Test
public void TestAlterTableRename() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes rename to new_alltypes");
    AnalyzesOk("alter table functional.alltypes rename to functional.new_alltypes");
    AnalysisError("alter table functional.alltypes rename to functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("alter table functional.alltypes rename to functional.alltypesagg", "Table already exists: functional.alltypesagg");
    AnalysisError("alter table functional.table_does_not_exist rename to new_table", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table db_does_not_exist.alltypes rename to new_table", "Database does not exist: db_does_not_exist");
    // Invalid database/table name.
    AnalysisError("alter table functional.alltypes rename to `???`.new_table", "Invalid database name: ???");
    AnalysisError("alter table functional.alltypes rename to functional.`%^&`", "Invalid table/view name: %^&");
    AnalysisError("alter table functional.alltypes rename to db_does_not_exist.new_table", "Database does not exist: db_does_not_exist");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view rename to new_alltypes", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // It should be okay to rename an HBase table.
    AnalyzesOk("alter table functional_hbase.alltypes rename to new_alltypes");
    // It should be okay to rename a table produced by a data source.
    AnalyzesOk("alter table functional.alltypes_datasource rename to new_datasrc_tbl");
}
#method_after
@Test
public void TestAlterTableRename() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes rename to new_alltypes");
    AnalyzesOk("alter table functional.alltypes rename to functional.new_alltypes");
    AnalysisError("alter table functional.alltypes rename to functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("alter table functional.alltypes rename to functional.alltypesagg", "Table already exists: functional.alltypesagg");
    AnalysisError("alter table functional.table_does_not_exist rename to new_table", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table db_does_not_exist.alltypes rename to new_table", "Database does not exist: db_does_not_exist");
    // Invalid database/table name.
    AnalysisError("alter table functional.alltypes rename to `???`.new_table", "Invalid database name: ???");
    AnalysisError("alter table functional.alltypes rename to functional.`%^&`", "Invalid table/view name: %^&");
    AnalysisError("alter table functional.alltypes rename to db_does_not_exist.new_table", "Database does not exist: db_does_not_exist");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view rename to new_alltypes", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE a nested collection.
    AnalysisError("alter table allcomplextypes.int_array_col rename to new_alltypes", createAnalyzer("functional"), "Database does not exist: allcomplextypes");
    // It should be okay to rename an HBase table.
    AnalyzesOk("alter table functional_hbase.alltypes rename to new_alltypes");
    // It should be okay to rename a table produced by a data source.
    AnalyzesOk("alter table functional.alltypes_datasource rename to new_datasrc_tbl");
}
#end_block

#method_before
@Test
public void TestAlterTableRecoverPartitions() throws CatalogException {
    AnalyzesOk("alter table functional.alltypes recover partitions");
    AnalysisError("alter table baddb.alltypes recover partitions", "Database does not exist: baddb");
    AnalysisError("alter table functional.badtbl recover partitions", "Table does not exist: functional.badtbl");
    AnalysisError("alter table functional.alltypesnopart recover partitions", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.view_view recover partitions", "ALTER TABLE not allowed on a view: functional.view_view");
    AnalysisError("alter table functional_hbase.alltypes recover partitions", "ALTER TABLE RECOVER PARTITIONS must target an HDFS table: " + "functional_hbase.alltypes");
}
#method_after
@Test
public void TestAlterTableRecoverPartitions() throws CatalogException {
    AnalyzesOk("alter table functional.alltypes recover partitions");
    AnalysisError("alter table baddb.alltypes recover partitions", "Could not resolve table reference: 'baddb.alltypes'");
    AnalysisError("alter table functional.badtbl recover partitions", "Could not resolve table reference: 'functional.badtbl'");
    AnalysisError("alter table functional.alltypesnopart recover partitions", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.view_view recover partitions", "ALTER TABLE not allowed on a view: functional.view_view");
    AnalysisError("alter table allcomplextypes.int_array_col recover partitions", createAnalyzer("functional"), "ALTER TABLE not allowed on a nested collection: allcomplextypes.int_array_col");
    AnalysisError("alter table functional_hbase.alltypes recover partitions", "ALTER TABLE RECOVER PARTITIONS must target an HDFS table: " + "functional_hbase.alltypes");
}
#end_block

#method_before
@Test
public void TestComputeStats() throws AnalysisException {
    // Analyze the stmt itself as well as the generated child queries.
    checkComputeStatsStmt("compute stats functional.alltypes");
    checkComputeStatsStmt("compute stats functional_hbase.alltypes");
    // Test that complex-typed columns are ignored.
    checkComputeStatsStmt("compute stats functional.allcomplextypes");
    // Cannot compute stats on a database.
    AnalysisError("compute stats tbl_does_not_exist", "Table does not exist: default.tbl_does_not_exist");
    // Cannot compute stats on a view.
    AnalysisError("compute stats functional.alltypes_view", "COMPUTE STATS not supported for view functional.alltypes_view");
    AnalyzesOk("compute stats functional_avro_snap.alltypes");
    // Test mismatched column definitions and Avro schema (HIVE-6308, IMPALA-867).
    // See testdata/avro_schema_resolution/create_table.sql for the CREATE TABLE stmts.
    // Mismatched column type is ok because the conflict is resolved in favor of
    // the type in the column definition list in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_type_mismatch");
    // Missing column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_missing_coldef");
    // Extra column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_extra_coldef");
    // No column definitions are ok.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_no_coldef");
    // Mismatched column name (table was created by Hive).
    AnalysisError("compute stats functional_avro_snap.schema_resolution_test", "Cannot COMPUTE STATS on Avro table 'schema_resolution_test' because its " + "column definitions do not match those in the Avro schema.\nDefinition of " + "column 'col1' of type 'string' does not match the Avro-schema column " + "'boolean1' of type 'BOOLEAN' at position '0'.\nPlease re-create the table " + "with column definitions, e.g., using the result of 'SHOW CREATE TABLE'");
}
#method_after
@Test
public void TestComputeStats() throws AnalysisException {
    // Analyze the stmt itself as well as the generated child queries.
    checkComputeStatsStmt("compute stats functional.alltypes");
    checkComputeStatsStmt("compute stats functional_hbase.alltypes");
    // Test that complex-typed columns are ignored.
    checkComputeStatsStmt("compute stats functional.allcomplextypes");
    // Cannot compute stats on a database.
    AnalysisError("compute stats tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    // Cannot compute stats on a view.
    AnalysisError("compute stats functional.alltypes_view", "COMPUTE STATS not supported for view: functional.alltypes_view");
    AnalyzesOk("compute stats functional_avro_snap.alltypes");
    // Test mismatched column definitions and Avro schema (HIVE-6308, IMPALA-867).
    // See testdata/avro_schema_resolution/create_table.sql for the CREATE TABLE stmts.
    // Mismatched column type is ok because the conflict is resolved in favor of
    // the type in the column definition list in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_type_mismatch");
    // Missing column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_missing_coldef");
    // Extra column definition is ok because the schema mismatch is resolved
    // in the CREATE TABLE.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_extra_coldef");
    // No column definitions are ok.
    AnalyzesOk("compute stats functional_avro_snap.alltypes_no_coldef");
    // Mismatched column name (table was created by Hive).
    AnalysisError("compute stats functional_avro_snap.schema_resolution_test", "Cannot COMPUTE STATS on Avro table 'schema_resolution_test' because its " + "column definitions do not match those in the Avro schema.\nDefinition of " + "column 'col1' of type 'string' does not match the Avro-schema column " + "'boolean1' of type 'BOOLEAN' at position '0'.\nPlease re-create the table " + "with column definitions, e.g., using the result of 'SHOW CREATE TABLE'");
}
#end_block

#method_before
@Test
public void TestComputeIncrementalStats() throws AnalysisException {
    checkComputeStatsStmt("compute incremental stats functional.alltypes");
    checkComputeStatsStmt("compute incremental stats functional.alltypes partition(year=2010, month=10)");
    AnalysisError("compute incremental stats functional.alltypes partition(year=9999, month=10)", "Partition spec does not exist: (year=9999, month=10)");
    AnalysisError("compute incremental stats functional.alltypes partition(year=2010)", "Items in partition spec must exactly match the partition columns in the table " + "definition: functional.alltypes (1 vs 2)");
    AnalysisError("compute incremental stats functional.alltypes partition(year=2010, month)", "Syntax error");
    // Test that NULL partitions generates a valid query
    checkComputeStatsStmt("compute incremental stats functional.alltypesagg " + "partition(year=2010, month=1, day=NULL)");
    AnalysisError("compute incremental stats functional_hbase.alltypes " + "partition(year=2010, month=1)", "COMPUTE INCREMENTAL ... PARTITION not " + "supported for non-HDFS table functional_hbase.alltypes");
    AnalysisError("compute incremental stats functional.view_view", "COMPUTE STATS not supported for view functional.view_view");
}
#method_after
@Test
public void TestComputeIncrementalStats() throws AnalysisException {
    checkComputeStatsStmt("compute incremental stats functional.alltypes");
    checkComputeStatsStmt("compute incremental stats functional.alltypes partition(year=2010, month=10)");
    checkComputeStatsStmt("compute incremental stats functional.alltypes partition(year<=2010)");
    AnalysisError("compute incremental stats functional.alltypes partition(year=9999, month=10)", "No matching partition(s) found.");
    AnalysisError("compute incremental stats functional.alltypes partition(year=2010, month)", "Partition expr requires return type 'BOOLEAN'. Actual type is 'INT'.");
    // Test that NULL partitions generates a valid query
    checkComputeStatsStmt("compute incremental stats functional.alltypesagg " + "partition(year=2010, month=1, day is NULL)");
    AnalysisError("compute incremental stats functional_hbase.alltypes " + "partition(year=2010, month=1)", "COMPUTE INCREMENTAL ... PARTITION not " + "supported for non-HDFS table functional_hbase.alltypes");
    AnalysisError("compute incremental stats functional.view_view", "COMPUTE STATS not supported for view: functional.view_view");
}
#end_block

#method_before
@Test
public void TestDropIncrementalStats() throws AnalysisException {
    AnalyzesOk("drop incremental stats functional.alltypes partition(year=2010, month=10)");
    AnalysisError("drop incremental stats functional.alltypes partition(year=9999, month=10)", "Partition spec does not exist: (year=9999, month=10)");
}
#method_after
@Test
public void TestDropIncrementalStats() throws AnalysisException {
    AnalyzesOk("drop incremental stats functional.alltypes partition(year=2010, month=10)");
    AnalyzesOk("drop incremental stats functional.alltypes partition(year<=2010, month=10)");
    AnalysisError("drop incremental stats functional.alltypes partition(year=9999, month=10)", "No matching partition(s) found.");
}
#end_block

#method_before
@Test
public void TestDropStats() throws AnalysisException {
    AnalyzesOk("drop stats functional.alltypes");
    // Table does not exist
    AnalysisError("drop stats tbl_does_not_exist", "Table does not exist: default.tbl_does_not_exist");
    // Database does not exist
    AnalysisError("drop stats no_db.no_tbl", "Database does not exist: no_db");
    AnalysisError("drop stats functional.alltypes partition(year=2010, month=10)", "Syntax error");
    AnalysisError("drop stats functional.alltypes partition(year, month)", "Syntax error");
}
#method_after
@Test
public void TestDropStats() throws AnalysisException {
    AnalyzesOk("drop stats functional.alltypes");
    // Table does not exist
    AnalysisError("drop stats tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    // Database does not exist
    AnalysisError("drop stats no_db.no_tbl", "Could not resolve table reference: 'no_db.no_tbl'");
    AnalysisError("drop stats functional.alltypes partition(year=2010, month=10)", "Syntax error");
    AnalysisError("drop stats functional.alltypes partition(year, month)", "Syntax error");
}
#end_block

#method_before
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='" + long_property_value + "') " + "tblproperties ('" + long_property_key + "'='" + long_property_value + "')");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("create table new_table (i int) " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='value')", "Serde property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('key'='" + long_property_value + "')", "Serde property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
    }
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    for (String format : fileFormats) {
        for (String create : ImmutableList.of("create table", "create external table")) {
            AnalyzesOk(String.format("%s new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", create, format));
            // No column definitions.
            AnalysisError(String.format("%s new_table " + "partitioned by (d decimal) comment 'c' stored as %s", create, format), "Table requires at least 1 column");
        }
        AnalysisError(String.format("create table t (i int primary key) stored as %s", format), "Only Kudu tables can specify a PRIMARY KEY");
        AnalysisError(String.format("create table t (i int, primary key(i)) stored as %s", format), "Only Kudu tables can specify a PRIMARY KEY");
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    // IMPALA-2251: it should not be possible to create text tables with the same
    // delimiter character used for multiple purposes.
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\001' lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\012'", "Field delimiter and line delimiter have same value: byte 10");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by '\001'", "Field delimiter and escape character have same value: byte 1. " + "Escape character will be ignored");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by 'x' lines terminated by 'x'", "Line delimiter and escape character have same value: byte 120. " + "Escape character will be ignored");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: i");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    AnalysisError("create table cached_tbl(i int) location " + "'file:///test-warehouse/cache_tbl' cached in 'testPool'", "Location 'file:/test-warehouse/cache_tbl' cannot be cached. " + "Please retry without caching: CREATE TABLE ... UNCACHED");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Test HMS constraint on type name length.
    AnalyzesOk(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH)));
    AnalysisError(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH + 1)), "Type of column 'i' exceeds maximum type length of 4000 characters:");
    // Test HMS constraint on comment length.
    AnalyzesOk(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH)));
    AnalysisError(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH + 1)), "Comment of column 'i' exceeds maximum length of 256 characters:");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#method_after
@Test
public void TestCreateTable() throws AnalysisException {
    AnalyzesOk("create table functional.new_table (i int)");
    AnalyzesOk("create table if not exists functional.alltypes (i int)");
    AnalysisError("create table functional.alltypes", "Table already exists: functional.alltypes");
    AnalysisError("create table functional.alltypes (i int)", "Table already exists: functional.alltypes");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '|'");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal)");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (d decimal(3,1))");
    AnalyzesOk("create table new_table(d1 decimal, d2 decimal(10), d3 decimal(5, 2))");
    AnalysisError("create table new_table (i int) PARTITIONED BY (d decimal(40,1))", "Decimal precision must be <= 38: 40");
    AnalyzesOk("create table new_table(s1 varchar(1), s2 varchar(32672))");
    AnalysisError("create table new_table(s1 varchar(0))", "Varchar size must be > 0: 0");
    AnalysisError("create table new_table(s1 varchar(65356))", "Varchar size must be <= 65355: 65356");
    AnalysisError("create table new_table(s1 char(0))", "Char size must be > 0: 0");
    AnalysisError("create table new_table(s1 Char(256))", "Char size must be <= 255: 256");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (s varchar(3))");
    AnalyzesOk("create table functional.new_table (c char(250))");
    AnalyzesOk("create table new_table (i int) PARTITIONED BY (c char(3))");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='" + long_property_value + "') " + "tblproperties ('" + long_property_key + "'='" + long_property_value + "')");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("create table new_table (i int) " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('" + long_property_key + "'='value')", "Serde property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("create table new_table (i int) " + "with serdeproperties ('key'='" + long_property_value + "')", "Serde property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
    }
    // Supported file formats. Exclude Avro since it is tested separately.
    String[] fileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE" };
    String[] fileFormatsStr = { "TEXT", "SEQUENCE_FILE", "PARQUET", "PARQUET", "RC_FILE" };
    int formatIndx = 0;
    for (String format : fileFormats) {
        for (String create : ImmutableList.of("create table", "create external table")) {
            AnalyzesOk(String.format("%s new_table (i int) " + "partitioned by (d decimal) comment 'c' stored as %s", create, format));
            // No column definitions.
            AnalysisError(String.format("%s new_table " + "partitioned by (d decimal) comment 'c' stored as %s", create, format), "Table requires at least 1 column");
        }
        AnalysisError(String.format("create table t (i int primary key) stored as %s", format), String.format("Unsupported column options for file format " + "'%s': 'i INT PRIMARY KEY'", fileFormatsStr[formatIndx]));
        AnalysisError(String.format("create table t (i int, primary key(i)) stored as %s", format), "Only Kudu tables can specify a PRIMARY KEY");
        formatIndx++;
    }
    // Note: Backslashes need to be escaped twice - once for Java and once for Impala.
    // For example, if this were a real query the value '\' would be stored in the
    // metastore for the ESCAPED BY field.
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\t' escaped by '\\\\' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '\\001' escaped by '\\002' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-2' escaped by '-3' lines terminated by '\\n'");
    AnalyzesOk("create table functional.new_table (i int) row format delimited fields " + "terminated by '-128' escaped by '127' lines terminated by '40'");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '128' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 128");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-2' escaped by '127' lines terminated by '255'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: 255");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '-129' escaped by '127' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: -129");
    AnalysisError("create table functional.new_table (i int) row format delimited " + "fields terminated by '||' escaped by '\\\\' lines terminated by '\\n'", "ESCAPED BY values and LINE/FIELD terminators must be specified as a single " + "character or as a decimal value in the range [-128:127]: ||");
    // IMPALA-2251: it should not be possible to create text tables with the same
    // delimiter character used for multiple purposes.
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\001' lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited lines terminated by '\001'", "Field delimiter and line delimiter have same value: byte 1");
    AnalysisError("create table functional.broken_text_table (c int) " + "row format delimited fields terminated by '\012'", "Field delimiter and line delimiter have same value: byte 10");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by '\001'", "Field delimiter and escape character have same value: byte 1. " + "Escape character will be ignored");
    AnalyzesOk("create table functional.broken_text_table (c int) " + "row format delimited escaped by 'x' lines terminated by 'x'", "Line delimiter and escape character have same value: byte 120. " + "Escape character will be ignored");
    AnalysisError("create table db_does_not_exist.new_table (i int)", "Database does not exist: db_does_not_exist");
    AnalysisError("create table new_table (i int, I string)", "Duplicate column name: i");
    AnalysisError("create table new_table (c1 double, col2 int, c1 double, c4 string)", "Duplicate column name: c1");
    AnalysisError("create table new_table (i int, s string) PARTITIONED BY (i int)", "Duplicate column name: i");
    AnalysisError("create table new_table (i int) PARTITIONED BY (C int, c2 int, c int)", "Duplicate column name: c");
    // Unsupported partition-column types.
    AnalysisError("create table new_table (i int) PARTITIONED BY (t timestamp)", "Type 'TIMESTAMP' is not supported as partition-column type in column: t");
    // Caching ops
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) uncached");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' cached in 'testPool'");
    AnalyzesOk("create table cached_tbl(i int) partitioned by(j int) " + "location '/test-warehouse/' uncached");
    AnalysisError("create table cached_tbl(i int) location " + "'file:///test-warehouse/cache_tbl' cached in 'testPool'", "Location 'file:/test-warehouse/cache_tbl' cannot be cached. " + "Please retry without caching: CREATE TABLE ... UNCACHED");
    // Invalid database name.
    AnalysisError("create table `???`.new_table (x int) PARTITIONED BY (y int)", "Invalid database name: ???");
    // Invalid table/view name.
    AnalysisError("create table functional.`^&*` (x int) PARTITIONED BY (y int)", "Invalid table/view name: ^&*");
    // Invalid column names.
    AnalysisError("create table new_table (`???` int) PARTITIONED BY (i int)", "Invalid column/field name: ???");
    AnalysisError("create table new_table (i int) PARTITIONED BY (`^&*` int)", "Invalid column/field name: ^&*");
    // Test HMS constraint on type name length.
    AnalyzesOk(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH)));
    AnalysisError(String.format("create table t (i %s)", genTypeSql(MetaStoreUtil.MAX_TYPE_NAME_LENGTH + 1)), "Type of column 'i' exceeds maximum type length of 4000 characters:");
    // Test HMS constraint on comment length.
    AnalyzesOk(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH)));
    AnalysisError(String.format("create table t (i int comment '%s')", StringUtils.repeat("c", MetaStoreUtil.CREATE_MAX_COMMENT_LENGTH + 1)), "Comment of column 'i' exceeds maximum length of 256 characters:");
    // Valid URI values.
    AnalyzesOk("create table tbl (i int) location '/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'hdfs://localhost:20500/test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'file:///test-warehouse/new_table'");
    AnalyzesOk("create table tbl (i int) location " + "'s3n://bucket/test-warehouse/new_table'");
    AnalyzesOk("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'file://test-warehouse/new_table'");
    // Invalid URI values.
    AnalysisError("create table functional.foo (x int) location " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("create table functional.foo (x int) location " + "'  '", "URI path cannot be empty.");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'foofs://test-warehouse/new_table'", "No FileSystem for scheme: foofs");
    AnalysisError("ALTER TABLE functional_seq_snap.alltypes SET LOCATION " + "'  '", "URI path cannot be empty.");
    // Create table PRODUCED BY DATA SOURCE
    final String DATA_SOURCE_NAME = "TestDataSource1";
    catalog_.addDataSource(new DataSource(DATA_SOURCE_NAME, "/foo.jar", "foo.Bar", "V1"));
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME.toLowerCase());
    AnalyzesOk("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME + "(\"\")");
    AnalyzesOk("CREATE TABLE DataSrcTable1 (a tinyint, b smallint, c int, d bigint, " + "e float, f double, g boolean, h string) PRODUCED BY DATA SOURCE " + DATA_SOURCE_NAME);
    AnalysisError("CREATE TABLE DataSrcTable1 (x int) PRODUCED BY DATA SOURCE " + "not_a_data_src(\"\")", "Data source does not exist");
    for (Type t : Type.getSupportedTypes()) {
        PrimitiveType type = t.getPrimitiveType();
        if (DataSourceTable.isSupportedPrimitiveType(type) || t.isNull())
            continue;
        String typeSpec = type.name();
        if (type == PrimitiveType.CHAR || type == PrimitiveType.DECIMAL || type == PrimitiveType.VARCHAR) {
            typeSpec += "(10)";
        }
        AnalysisError("CREATE TABLE DataSrcTable1 (x " + typeSpec + ") PRODUCED " + "BY DATA SOURCE " + DATA_SOURCE_NAME, "Tables produced by an external data source do not support the column type: " + type.name());
    }
}
#end_block

#method_before
@Test
public void TestCreateManagedKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Test primary keys and distribute by clauses
    AnalyzesOk("create table tab (x int primary key) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, primary key(x)) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) " + "distribute by hash(x, y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x)) " + "distribute by hash(x) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) " + "distribute by hash(y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y string, primary key (x)) distribute by " + "hash (x) into 3 buckets, range (x) (partition values < 1, partition " + "1 <= values < 10, partition 10 <= values < 20, partition value = 30) " + "stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) distribute by " + "range (x, y) (partition value = (2001, 1), partition value = (2002, 1), " + "partition value = (2003, 2)) stored as kudu");
    // Non-literal boundary values in range partitions
    AnalyzesOk("create table tab (x int, y int, primary key (x)) distribute by " + "range (x) (partition values < 1 + 1, partition (1+3) + 2 < values < 10, " + "partition factorial(4) < values < factorial(5), " + "partition value = factorial(6)) stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) distribute by " + "range(x, y) (partition value = (1+1, 2+2), partition value = ((1+1+1)+1, 10), " + "partition value = (cast (30 as int), factorial(5))) stored as kudu");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values < x + 1) stored as kudu", "Only constant values are allowed " + "for range-partition bounds: x + 1");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= isnull(null, null)) stored as kudu", "Range partition " + "values cannot be NULL. Range partition: 'PARTITION VALUES <= " + "isnull(NULL, NULL)'");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= (select count(*) from functional.alltypestiny)) " + "stored as kudu", "Only constant values are allowed for range-partition " + "bounds: (SELECT count(*) FROM functional.alltypestiny)");
    // Multilevel partitioning. Data is split into 3 buckets based on 'x' and each
    // bucket is partitioned into 4 tablets based on the range partitions of 'y'.
    AnalyzesOk("create table tab (x int, y string, primary key(x, y)) " + "distribute by hash(x) into 3 buckets, range(y) " + "(partition values < 'aa', partition 'aa' <= values < 'bb', " + "partition 'bb' <= values < 'cc', partition 'cc' <= values) " + "stored as kudu");
    // Key column in upper case
    AnalyzesOk("create table tab (x int, y int, primary key (X)) " + "distribute by hash (x) into 8 buckets stored as kudu");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b) into 8 buckets, hash(c) into 2 buckets stored as " + "kudu");
    // No columns specified in the DISTRIBUTE BY HASH clause
    AnalyzesOk("create table tab (a int primary key, b int, c int, d int) " + "distribute by hash into 8 buckets stored as kudu");
    // Distribute range data types are picked up during analysis and forwarded to Kudu.
    // Column names in distribute params should also be case-insensitive.
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key(a, b, c, d))" + "distribute by hash (a, B, c) into 8 buckets, " + "range (A) (partition values < 1, partition 1 <= values < 2, " + "partition 2 <= values < 3, partition 3 <= values < 4, partition 4 <= values) " + "stored as kudu");
    // Allowing range distribution on a subset of the primary keys
    AnalyzesOk("create table tab (id int, name string, valf float, vali bigint, " + "primary key (id, name)) distribute by range (name) " + "(partition 'aa' < values <= 'bb') stored as kudu");
    // Null values in range partition values
    AnalysisError("create table tab (id int, name string, primary key(id, name)) " + "distribute by hash (id) into 3 buckets, range (name) " + "(partition value = null, partition value = 1) stored as kudu", "Range partition values cannot be NULL. Range partition: 'PARTITION " + "VALUE = NULL'");
    // Primary key specified in tblproperties
    AnalysisError(String.format("create table tab (x int) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('%s' = 'x')", KuduTable.KEY_KEY_COLUMNS), "PRIMARY KEY must be used instead of the table " + "property");
    // Primary key column that doesn't exist
    AnalysisError("create table tab (x int, y int, primary key (z)) " + "distribute by hash (x) into 8 buckets stored as kudu", "PRIMARY KEY column 'z' does not exist in the table");
    // Invalid composite primary key
    AnalysisError("create table tab (x int primary key, primary key(x)) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    AnalysisError("create table tab (x int primary key, y int primary key) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    // Specifying the same primary key column multiple times
    AnalysisError("create table tab (x int, primary key (x, x)) distribute by hash (x) " + "into 8 buckets stored as kudu", "Column 'x' is listed multiple times as a PRIMARY KEY.");
    // Number of range partition boundary values should be equal to the number of range
    // columns.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by range(a) (partition value = (1, 2), " + "partition value = 3, partition value = 4) stored as kudu", "Number of specified range partition values is different than the number of " + "distribution columns: (2 vs 1). Range partition: 'PARTITION VALUE = (1,2)'");
    // Key ranges must match the column types.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by hash (a, b, c) into 8 buckets, range (a) " + "(partition value = 1, partition value = 'abc', partition 3 <= values) " + "stored as kudu", "Range partition value 'abc' (type: STRING) is not type " + "compatible with distribution column 'a' (type: INT).");
    AnalysisError("create table tab (a tinyint primary key) distribute by range (a) " + "(partition value = 128) stored as kudu", "Range partition value 128 " + "(type: SMALLINT) is not type compatible with distribution column 'a' " + "(type: TINYINT)");
    AnalysisError("create table tab (a smallint primary key) distribute by range (a) " + "(partition value = 32768) stored as kudu", "Range partition value 32768 " + "(type: INT) is not type compatible with distribution column 'a' " + "(type: SMALLINT)");
    AnalysisError("create table tab (a int primary key) distribute by range (a) " + "(partition value = 2147483648) stored as kudu", "Range partition value " + "2147483648 (type: BIGINT) is not type compatible with distribution column 'a' " + "(type: INT)");
    AnalysisError("create table tab (a bigint primary key) distribute by range (a) " + "(partition value = 9223372036854775808) stored as kudu", "Range partition " + "value 9223372036854775808 (type: DECIMAL(19,0)) is not type compatible with " + "distribution column 'a' (type: BIGINT)");
    // Test implicit casting/folding of partition values.
    AnalyzesOk("create table tab (a int primary key) distribute by range (a) " + "(partition value = false, partition value = true) stored as kudu");
    // Non-key column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b string, c bigint, primary key (a)) " + "distribute by range (b) (partition value = 'abc') stored as kudu", "Column 'b' in 'RANGE (b) (PARTITION VALUE = 'abc')' is not a key column. " + "Only key columns can be used in DISTRIBUTE BY.");
    // No float range partition values
    AnalysisError("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b, c) into 8 buckets, " + "range (a) (partition value = 1.2, partition value = 2) stored as kudu", "Range partition value 1.2 (type: DECIMAL(2,1)) is not type compatible with " + "distribution column 'a' (type: INT).");
    // Non-existing column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b int, primary key (a, b)) " + "distribute by range(unknown_column) (partition value = 'abc') stored as kudu", "Column 'unknown_column' in 'RANGE (unknown_column) (PARTITION VALUE = 'abc')' " + "is not a key column. Only key columns can be used in DISTRIBUTE BY");
    // Kudu table name is specified in tblproperties
    AnalyzesOk("create table tab (x int primary key) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('kudu.table_name'='tab_1'," + "'kudu.num_tablet_replicas'='1'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081')");
    // No port is specified in kudu master address
    AnalyzesOk("create table tdata_no_port (id int primary key, name string, " + "valf float, vali bigint) distribute by range(id) (partition values <= 10, " + "partition 10 < values <= 30, partition 30 < values) " + "stored as kudu tblproperties('kudu.master_addresses'='127.0.0.1')");
    // Not using the STORED AS KUDU syntax to specify a Kudu table
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    AnalysisError("create table tab (x int primary key) stored as kudu tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    // Invalid value for number of replicas
    AnalysisError("create table t (x int primary key) stored as kudu tblproperties (" + "'kudu.num_tablet_replicas'='1.1')", "Table property 'kudu.num_tablet_replicas' must be an integer.");
    // Don't allow caching
    AnalysisError("create table tab (x int primary key) stored as kudu cached in " + "'testPool'", "A Kudu table cannot be cached in HDFS.");
    // LOCATION cannot be used with Kudu tables
    AnalysisError("create table tab (a int primary key) distribute by hash (a) " + "into 3 buckets stored as kudu location '/test-warehouse/'", "LOCATION cannot be specified for a Kudu table.");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int, primary key (a)) stored as kudu", "Table distribution must be specified for managed Kudu tables.");
    AnalysisError("create table tab (a int) stored as kudu", "A primary key is required for a Kudu table.");
    // Using ROW FORMAT with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "row format delimited escaped by 'X' stored as kudu", "ROW FORMAT cannot be specified for file format KUDU.");
    // Using PARTITIONED BY with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "partitioned by (y int) stored as kudu", "PARTITIONED BY cannot be used " + "in Kudu tables.");
    // Test unsupported Kudu types
    List<String> unsupportedTypes = Lists.newArrayList("DECIMAL(9,0)", "TIMESTAMP", "VARCHAR(20)", "CHAR(20)", "STRUCT<F1:INT,F2:STRING>", "ARRAY<INT>", "MAP<STRING,STRING>");
    for (String t : unsupportedTypes) {
        String expectedError = String.format("Cannot create table 'tab': Type %s is not supported in Kudu", t);
        // Unsupported type is PK and partition col
        String stmt = String.format("create table tab (x %s primary key) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
        // Unsupported type is not PK/partition col
        stmt = String.format("create table tab (x int primary key, y %s) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
    }
    // Test column options
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (Encoding enc : Encoding.values()) {
        for (CompressionAlgorithm comp : CompressionAlgorithm.values()) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        AnalyzesOk(String.format("create table tab (x int primary key " + "not null encoding %s compression %s %s %s, y int encoding %s " + "compression %s %s %s %s) distribute by hash (x) " + "into 3 buckets stored as kudu", enc, comp, def, block, enc, comp, def, block, nul));
                    }
                }
            }
        }
    }
    // Primary keys can't be null
    AnalysisError("create table tab (x int primary key null, y int not null) " + "distribute by hash (x) into 3 buckets stored as kudu", "Primary key columns " + "can't have NULL values: x INT PRIMARY KEY NULL");
    // Unsupported encoding value
    AnalysisError("create table tab (x int primary key, y int encoding invalid_enc) " + "distribute by hash (x) into 3 buckets stored as kudu", "Unsupported encoding " + "value 'INVALID_ENC'. Supported encoding values are: " + Joiner.on(", ").join(Encoding.values()));
    // Unsupported compression algorithm
    AnalysisError("create table tab (x int primary key, y int compression " + "invalid_comp) distribute by hash (x) into 3 buckets stored as kudu", "Unsupported compression algorithm 'INVALID_COMP'. Supported compression " + "algorithms are: " + Joiner.on(", ").join(CompressionAlgorithm.values()));
    // Default values
    AnalyzesOk("create table tab (i1 tinyint default 1, i2 smallint default 10, " + "i3 int default 100, i4 bigint default 1000, vals string default 'test', " + "valf float default cast(1.2 as float), vald double default " + "cast(3.1452 as double), valb boolean default true, " + "primary key (i1, i2, i3, i4, vals)) distribute by hash (i1) into 3 " + "buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key default 1+1+1) " + "distribute by hash (i) into 3 buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key default factorial(5)) " + "distribute by hash (i) into 3 buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key, x int null default " + "isnull(null, null)) distribute by hash (i) into 3 buckets stored as kudu");
    // Invalid default values
    AnalysisError("create table tab (i int primary key default 'string_val') " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value " + "'string_val' (type: STRING) is not compatible with column 'i' (type: INT).");
    AnalysisError("create table tab (i int primary key, x int default 1.1) " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value 1.1 (type: DECIMAL(2,1)) is not compatible with column " + "'x' (type: INT).");
    AnalysisError("create table tab (i tinyint primary key default 128) " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value " + "128 (type: SMALLINT) is not compatible with column 'i' (type: TINYINT).");
    AnalysisError("create table tab (i int primary key default isnull(null, null)) " + "distribute by hash (i) into 3 buckets stored as kudu", "NULL values are not " + "allowed for column 'i': NULL");
    AnalysisError("create table tab (i int primary key, x int not null " + "default isnull(null, null)) distribute by hash (i) into 3 buckets " + "stored as kudu", "NULL values are not allowed for column 'x': NULL");
    // Invalid block_size values
    AnalysisError("create table tab (i int primary key block_size 1.1) " + "distribute by hash (i) into 3 buckets stored as kudu", "Invalid value " + "for BLOCK_SIZE: 1.1. A positive INTEGER value is expected.");
    AnalysisError("create table tab (i int primary key block_size 'val') " + "distribute by hash (i) into 3 buckets stored as kudu", "Invalid value " + "for BLOCK_SIZE: 'val'. A positive INTEGER value is expected.");
}
#method_after
@Test
public void TestCreateManagedKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Test primary keys and distribute by clauses
    AnalyzesOk("create table tab (x int primary key) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, primary key(x)) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) " + "distribute by hash(x, y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x)) " + "distribute by hash(x) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) " + "distribute by hash(y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y string, primary key (x)) distribute by " + "hash (x) into 3 buckets, range (x) (partition values < 1, partition " + "1 <= values < 10, partition 10 <= values < 20, partition value = 30) " + "stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) distribute by " + "range (x, y) (partition value = (2001, 1), partition value = (2002, 1), " + "partition value = (2003, 2)) stored as kudu");
    // Non-literal boundary values in range partitions
    AnalyzesOk("create table tab (x int, y int, primary key (x)) distribute by " + "range (x) (partition values < 1 + 1, partition (1+3) + 2 < values < 10, " + "partition factorial(4) < values < factorial(5), " + "partition value = factorial(6)) stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) distribute by " + "range(x, y) (partition value = (1+1, 2+2), partition value = ((1+1+1)+1, 10), " + "partition value = (cast (30 as int), factorial(5))) stored as kudu");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values < x + 1) stored as kudu", "Only constant values are allowed " + "for range-partition bounds: x + 1");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= isnull(null, null)) stored as kudu", "Range partition " + "values cannot be NULL. Range partition: 'PARTITION VALUES <= " + "isnull(NULL, NULL)'");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= (select count(*) from functional.alltypestiny)) " + "stored as kudu", "Only constant values are allowed for range-partition " + "bounds: (SELECT count(*) FROM functional.alltypestiny)");
    // Multilevel partitioning. Data is split into 3 buckets based on 'x' and each
    // bucket is partitioned into 4 tablets based on the range partitions of 'y'.
    AnalyzesOk("create table tab (x int, y string, primary key(x, y)) " + "distribute by hash(x) into 3 buckets, range(y) " + "(partition values < 'aa', partition 'aa' <= values < 'bb', " + "partition 'bb' <= values < 'cc', partition 'cc' <= values) " + "stored as kudu");
    // Key column in upper case
    AnalyzesOk("create table tab (x int, y int, primary key (X)) " + "distribute by hash (x) into 8 buckets stored as kudu");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b) into 8 buckets, hash(c) into 2 buckets stored as " + "kudu");
    // No columns specified in the DISTRIBUTE BY HASH clause
    AnalyzesOk("create table tab (a int primary key, b int, c int, d int) " + "distribute by hash into 8 buckets stored as kudu");
    // Distribute range data types are picked up during analysis and forwarded to Kudu.
    // Column names in distribute params should also be case-insensitive.
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key(a, b, c, d))" + "distribute by hash (a, B, c) into 8 buckets, " + "range (A) (partition values < 1, partition 1 <= values < 2, " + "partition 2 <= values < 3, partition 3 <= values < 4, partition 4 <= values) " + "stored as kudu");
    // Allowing range distribution on a subset of the primary keys
    AnalyzesOk("create table tab (id int, name string, valf float, vali bigint, " + "primary key (id, name)) distribute by range (name) " + "(partition 'aa' < values <= 'bb') stored as kudu");
    // Null values in range partition values
    AnalysisError("create table tab (id int, name string, primary key(id, name)) " + "distribute by hash (id) into 3 buckets, range (name) " + "(partition value = null, partition value = 1) stored as kudu", "Range partition values cannot be NULL. Range partition: 'PARTITION " + "VALUE = NULL'");
    // Primary key specified in tblproperties
    AnalysisError(String.format("create table tab (x int) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('%s' = 'x')", KuduTable.KEY_KEY_COLUMNS), "PRIMARY KEY must be used instead of the table " + "property");
    // Primary key column that doesn't exist
    AnalysisError("create table tab (x int, y int, primary key (z)) " + "distribute by hash (x) into 8 buckets stored as kudu", "PRIMARY KEY column 'z' does not exist in the table");
    // Invalid composite primary key
    AnalysisError("create table tab (x int primary key, primary key(x)) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    AnalysisError("create table tab (x int primary key, y int primary key) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    // Specifying the same primary key column multiple times
    AnalysisError("create table tab (x int, primary key (x, x)) distribute by hash (x) " + "into 8 buckets stored as kudu", "Column 'x' is listed multiple times as a PRIMARY KEY.");
    // Number of range partition boundary values should be equal to the number of range
    // columns.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by range(a) (partition value = (1, 2), " + "partition value = 3, partition value = 4) stored as kudu", "Number of specified range partition values is different than the number of " + "distribution columns: (2 vs 1). Range partition: 'PARTITION VALUE = (1,2)'");
    // Key ranges must match the column types.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by hash (a, b, c) into 8 buckets, range (a) " + "(partition value = 1, partition value = 'abc', partition 3 <= values) " + "stored as kudu", "Range partition value 'abc' (type: STRING) is not type " + "compatible with distribution column 'a' (type: INT).");
    AnalysisError("create table tab (a tinyint primary key) distribute by range (a) " + "(partition value = 128) stored as kudu", "Range partition value 128 " + "(type: SMALLINT) is not type compatible with distribution column 'a' " + "(type: TINYINT)");
    AnalysisError("create table tab (a smallint primary key) distribute by range (a) " + "(partition value = 32768) stored as kudu", "Range partition value 32768 " + "(type: INT) is not type compatible with distribution column 'a' " + "(type: SMALLINT)");
    AnalysisError("create table tab (a int primary key) distribute by range (a) " + "(partition value = 2147483648) stored as kudu", "Range partition value " + "2147483648 (type: BIGINT) is not type compatible with distribution column 'a' " + "(type: INT)");
    AnalysisError("create table tab (a bigint primary key) distribute by range (a) " + "(partition value = 9223372036854775808) stored as kudu", "Range partition " + "value 9223372036854775808 (type: DECIMAL(19,0)) is not type compatible with " + "distribution column 'a' (type: BIGINT)");
    // Test implicit casting/folding of partition values.
    AnalyzesOk("create table tab (a int primary key) distribute by range (a) " + "(partition value = false, partition value = true) stored as kudu");
    // Non-key column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b string, c bigint, primary key (a)) " + "distribute by range (b) (partition value = 'abc') stored as kudu", "Column 'b' in 'RANGE (b) (PARTITION VALUE = 'abc')' is not a key column. " + "Only key columns can be used in DISTRIBUTE BY.");
    // No float range partition values
    AnalysisError("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b, c) into 8 buckets, " + "range (a) (partition value = 1.2, partition value = 2) stored as kudu", "Range partition value 1.2 (type: DECIMAL(2,1)) is not type compatible with " + "distribution column 'a' (type: INT).");
    // Non-existing column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b int, primary key (a, b)) " + "distribute by range(unknown_column) (partition value = 'abc') stored as kudu", "Column 'unknown_column' in 'RANGE (unknown_column) (PARTITION VALUE = 'abc')' " + "is not a key column. Only key columns can be used in DISTRIBUTE BY");
    // Kudu table name is specified in tblproperties
    AnalyzesOk("create table tab (x int primary key) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('kudu.table_name'='tab_1'," + "'kudu.num_tablet_replicas'='1'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081')");
    // No port is specified in kudu master address
    AnalyzesOk("create table tdata_no_port (id int primary key, name string, " + "valf float, vali bigint) distribute by range(id) (partition values <= 10, " + "partition 10 < values <= 30, partition 30 < values) " + "stored as kudu tblproperties('kudu.master_addresses'='127.0.0.1')");
    // Not using the STORED AS KUDU syntax to specify a Kudu table
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    AnalysisError("create table tab (x int primary key) stored as kudu tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    // Invalid value for number of replicas
    AnalysisError("create table t (x int primary key) stored as kudu tblproperties (" + "'kudu.num_tablet_replicas'='1.1')", "Table property 'kudu.num_tablet_replicas' must be an integer.");
    // Don't allow caching
    AnalysisError("create table tab (x int primary key) stored as kudu cached in " + "'testPool'", "A Kudu table cannot be cached in HDFS.");
    // LOCATION cannot be used with Kudu tables
    AnalysisError("create table tab (a int primary key) distribute by hash (a) " + "into 3 buckets stored as kudu location '/test-warehouse/'", "LOCATION cannot be specified for a Kudu table.");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int, primary key (a)) stored as kudu", "Table distribution must be specified for managed Kudu tables.");
    AnalysisError("create table tab (a int) stored as kudu", "A primary key is required for a Kudu table.");
    // Using ROW FORMAT with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "row format delimited escaped by 'X' stored as kudu", "ROW FORMAT cannot be specified for file format KUDU.");
    // Using PARTITIONED BY with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "partitioned by (y int) stored as kudu", "PARTITIONED BY cannot be used " + "in Kudu tables.");
    // Test unsupported Kudu types
    List<String> unsupportedTypes = Lists.newArrayList("DECIMAL(9,0)", "TIMESTAMP", "VARCHAR(20)", "CHAR(20)", "STRUCT<F1:INT,F2:STRING>", "ARRAY<INT>", "MAP<STRING,STRING>");
    for (String t : unsupportedTypes) {
        String expectedError = String.format("Cannot create table 'tab': Type %s is not supported in Kudu", t);
        // Unsupported type is PK and partition col
        String stmt = String.format("create table tab (x %s primary key) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
        // Unsupported type is not PK/partition col
        stmt = String.format("create table tab (x int primary key, y %s) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
    }
    // Test column options
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (Encoding enc : Encoding.values()) {
        for (CompressionAlgorithm comp : CompressionAlgorithm.values()) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        AnalyzesOk(String.format("create table tab (x int primary key " + "not null encoding %s compression %s %s %s, y int encoding %s " + "compression %s %s %s %s) distribute by hash (x) " + "into 3 buckets stored as kudu", enc, comp, def, block, enc, comp, def, nul, block));
                    }
                }
            }
        }
    }
    // Primary key specified using the PRIMARY KEY clause
    AnalyzesOk("create table tab (x int not null encoding plain_encoding " + "compression snappy block_size 1, y int null encoding rle compression lz4 " + "default 1, primary key(x)) distribute by hash (x) into 3 buckets " + "stored as kudu");
    // Primary keys can't be null
    AnalysisError("create table tab (x int primary key null, y int not null) " + "distribute by hash (x) into 3 buckets stored as kudu", "Primary key columns " + "cannot be nullable: x INT PRIMARY KEY NULL");
    AnalysisError("create table tab (x int not null, y int null, primary key (x, y)) " + "distribute by hash (x) into 3 buckets stored as kudu", "Primary key columns " + "cannot be nullable: y INT NULL");
    // Unsupported encoding value
    AnalysisError("create table tab (x int primary key, y int encoding invalid_enc) " + "distribute by hash (x) into 3 buckets stored as kudu", "Unsupported encoding " + "value 'INVALID_ENC'. Supported encoding values are: " + Joiner.on(", ").join(Encoding.values()));
    // Unsupported compression algorithm
    AnalysisError("create table tab (x int primary key, y int compression " + "invalid_comp) distribute by hash (x) into 3 buckets stored as kudu", "Unsupported compression algorithm 'INVALID_COMP'. Supported compression " + "algorithms are: " + Joiner.on(", ").join(CompressionAlgorithm.values()));
    // Default values
    AnalyzesOk("create table tab (i1 tinyint default 1, i2 smallint default 10, " + "i3 int default 100, i4 bigint default 1000, vals string default 'test', " + "valf float default cast(1.2 as float), vald double default " + "cast(3.1452 as double), valb boolean default true, " + "primary key (i1, i2, i3, i4, vals)) distribute by hash (i1) into 3 " + "buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key default 1+1+1) " + "distribute by hash (i) into 3 buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key default factorial(5)) " + "distribute by hash (i) into 3 buckets stored as kudu");
    AnalyzesOk("create table tab (i int primary key, x int null default " + "isnull(null, null)) distribute by hash (i) into 3 buckets stored as kudu");
    // Invalid default values
    AnalysisError("create table tab (i int primary key default 'string_val') " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value " + "'string_val' (type: STRING) is not compatible with column 'i' (type: INT).");
    AnalysisError("create table tab (i int primary key, x int default 1.1) " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value 1.1 (type: DECIMAL(2,1)) is not compatible with column " + "'x' (type: INT).");
    AnalysisError("create table tab (i tinyint primary key default 128) " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value " + "128 (type: SMALLINT) is not compatible with column 'i' (type: TINYINT).");
    AnalysisError("create table tab (i int primary key default isnull(null, null)) " + "distribute by hash (i) into 3 buckets stored as kudu", "Default value of " + "NULL not allowed on non-nullable column: 'i'");
    AnalysisError("create table tab (i int primary key, x int not null " + "default isnull(null, null)) distribute by hash (i) into 3 buckets " + "stored as kudu", "Default value of NULL not allowed on non-nullable column: " + "'x'");
    // Invalid block_size values
    AnalysisError("create table tab (i int primary key block_size 1.1) " + "distribute by hash (i) into 3 buckets stored as kudu", "Invalid value " + "for BLOCK_SIZE: 1.1. A positive INTEGER value is expected.");
    AnalysisError("create table tab (i int primary key block_size 'val') " + "distribute by hash (i) into 3 buckets stored as kudu", "Invalid value " + "for BLOCK_SIZE: 'val'. A positive INTEGER value is expected.");
}
#end_block

#method_before
@Test
public void TestCreateAvroTest() {
    String alltypesSchemaLoc = "hdfs:///test-warehouse/avro_schemas/functional/alltypes.json";
    // Analysis of Avro schemas. Column definitions match the Avro schema exactly.
    // Note: Avro does not have a tinyint and smallint type.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc));
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc));
    AnalyzesOk("create table foo_avro (string1 string) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}]}')");
    // No column definitions.
    AnalyzesOk(String.format("create table foo_avro with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc));
    AnalyzesOk(String.format("create table foo_avro stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc));
    AnalyzesOk("create table foo_avro stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}]}')");
    // Analysis of Avro schemas. Column definitions do not match Avro schema.
    AnalyzesOk(String.format("create table foo_avro (id int) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc), "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 11 column(s) but 1 column definition(s) were given.");
    AnalyzesOk(String.format("create table foo_avro (bool_col boolean, string_col string) " + "stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc), "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 11 column(s) but 2 column definition(s) were given.");
    AnalyzesOk("create table foo_avro (string1 string) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"string2\", \"type\": \"string\"}]}')", "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 2 column(s) but 1 column definition(s) were given.");
    // Mismatched name.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, bad_int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc), "Resolved the following name and/or type inconsistencies between the column " + "definitions and the Avro schema.\n" + "Column definition at position 4:  bad_int_col INT\n" + "Avro schema column at position 4: int_col INT\n" + "Resolution at position 4: int_col INT\n" + "Column definition at position 10:  timestamp_col TIMESTAMP\n" + "Avro schema column at position 10: timestamp_col STRING\n" + "Resolution at position 10: timestamp_col STRING");
    // Mismatched type.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col bigint, date_string_col string, string_col string, " + "timestamp_col timestamp) stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc), "Resolved the following name and/or type inconsistencies between the column " + "definitions and the Avro schema.\n" + "Column definition at position 7:  double_col BIGINT\n" + "Avro schema column at position 7: double_col DOUBLE\n" + "Resolution at position 7: double_col DOUBLE\n" + "Column definition at position 10:  timestamp_col TIMESTAMP\n" + "Avro schema column at position 10: timestamp_col STRING\n" + "Resolution at position 10: timestamp_col STRING");
    // Avro schema is inferred from column definitions.
    AnalyzesOk("create table foo_avro (c1 tinyint, c2 smallint, c3 int, c4 bigint, " + "c5 float, c6 double, c7 timestamp, c8 string, c9 char(10), c10 varchar(20)," + "c11 decimal(10, 5), c12 struct<f1:int,f2:string>, c13 array<int>," + "c14 map<string,string>) stored as avro");
    AnalyzesOk("create table foo_avro (c1 tinyint, c2 smallint, c3 int, c4 bigint, " + "c5 float, c6 double, c7 timestamp, c8 string, c9 char(10), c10 varchar(20)," + "c11 decimal(10, 5), c12 struct<f1:int,f2:string>, c13 array<int>," + "c14 map<string,string>) partitioned by (year int, month int) stored as avro");
    // Neither Avro schema nor column definitions.
    AnalysisError("create table foo_avro stored as avro tblproperties ('a'='b')", "An Avro table requires column definitions or an Avro schema.");
    // Invalid schema URL
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='')", "Invalid avro.schema.url: . Can not create a Path from an empty string");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='schema.avsc')", "Invalid avro.schema.url: schema.avsc. Path does not exist.");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='hdfs://invalid*host/schema.avsc')", "Failed to read Avro schema at: hdfs://invalid*host/schema.avsc. " + "Incomplete HDFS URI, no host: hdfs://invalid*host/schema.avsc");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='foo://bar/schema.avsc')", "Failed to read Avro schema at: foo://bar/schema.avsc. " + "No FileSystem for scheme: foo");
    // Decimal parsing
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"precision\":5,\"scale\":2}}]}')");
    // Scale not required (defaults to zero).
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"precision\":5}}]}')");
    // Precision is always required
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":5}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "No 'precision' property specified for 'decimal' logicalType");
    // Precision/Scale must be positive integers
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":5, \"precision\":-20}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "Invalid decimal 'precision' property value: -20");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":-1, \"precision\":20}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "Invalid decimal 'scale' property value: -1");
    // Invalid schema (bad JSON - missing opening bracket for "field" array)
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": {\"name\": \"string1\", \"type\": \"string\"}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "org.codehaus.jackson.JsonParseException: Unexpected close marker ']': " + "expected '}'");
    // Map/Array types in Avro schema.
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"list1\", \"type\": {\"type\":\"array\", \"items\": \"int\"}}]}')");
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"map1\", \"type\": {\"type\":\"map\", \"values\": \"int\"}}]}')");
    // Union is not supported
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"union1\", \"type\": [\"float\", \"boolean\"]}]}')", "Unsupported type 'union' of column 'union1'");
    // TODO: Add COLLECTION ITEMS TERMINATED BY and MAP KEYS TERMINATED BY clauses.
    // Test struct complex type.
    AnalyzesOk("create table functional.new_table (" + "a struct<f1: int, f2: string, f3: timestamp, f4: boolean>, " + "b struct<f1: struct<f11: int>, f2: struct<f21: struct<f22: string>>>, " + "c struct<f1: map<int, string>, f2: array<bigint>>," + "d struct<f1: struct<f11: map<int, string>, f12: array<bigint>>>)");
    // Test array complex type.
    AnalyzesOk("create table functional.new_table (" + "a array<int>, b array<timestamp>, c array<string>, d array<boolean>, " + "e array<array<int>>, f array<array<array<string>>>, " + "g array<struct<f1: int, f2: string>>, " + "h array<map<string,int>>)");
    // Test map complex type.
    AnalyzesOk("create table functional.new_table (" + "a map<string, int>, b map<timestamp, boolean>, c map<bigint, float>, " + "d array<array<int>>, e array<array<array<string>>>, " + "f array<struct<f1: int, f2: string>>," + "g array<map<string,int>>)");
    // Cannot partition by a complex column.
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x array<int>)", "Type 'ARRAY<INT>' is not supported as partition-column type in column: x");
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x map<int,int>)", "Type 'MAP<INT,INT>' is not supported as partition-column type in column: x");
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x struct<f1:int>)", "Type 'STRUCT<f1:INT>' is not supported as partition-column type in column: x");
    // Kudu specific clauses used in an Avro table.
    AnalysisError("create table functional.new_table (i int primary key) " + "distribute by hash(i) into 3 buckets stored as avro", "Only Kudu tables can use the DISTRIBUTE BY clause.");
    AnalysisError("create table functional.new_table (i int primary key) " + "stored as avro", "Only Kudu tables can specify a PRIMARY KEY.");
}
#method_after
@Test
public void TestCreateAvroTest() {
    String alltypesSchemaLoc = "hdfs:///test-warehouse/avro_schemas/functional/alltypes.json";
    // Analysis of Avro schemas. Column definitions match the Avro schema exactly.
    // Note: Avro does not have a tinyint and smallint type.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc));
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc));
    AnalyzesOk("create table foo_avro (string1 string) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}]}')");
    // No column definitions.
    AnalyzesOk(String.format("create table foo_avro with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc));
    AnalyzesOk(String.format("create table foo_avro stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc));
    AnalyzesOk("create table foo_avro stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}]}')");
    // Analysis of Avro schemas. Column definitions do not match Avro schema.
    AnalyzesOk(String.format("create table foo_avro (id int) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc), "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 11 column(s) but 1 column definition(s) were given.");
    AnalyzesOk(String.format("create table foo_avro (bool_col boolean, string_col string) " + "stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc), "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 11 column(s) but 2 column definition(s) were given.");
    AnalyzesOk("create table foo_avro (string1 string) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"string2\", \"type\": \"string\"}]}')", "Ignoring column definitions in favor of Avro schema.\n" + "The Avro schema has 2 column(s) but 1 column definition(s) were given.");
    // Mismatched name.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, bad_int_col int, bigint_col bigint, float_col float," + "double_col double, date_string_col string, string_col string, " + "timestamp_col timestamp) with serdeproperties ('avro.schema.url'='%s')" + "stored as avro", alltypesSchemaLoc), "Resolved the following name and/or type inconsistencies between the column " + "definitions and the Avro schema.\n" + "Column definition at position 4:  bad_int_col INT\n" + "Avro schema column at position 4: int_col INT\n" + "Resolution at position 4: int_col INT\n" + "Column definition at position 10:  timestamp_col TIMESTAMP\n" + "Avro schema column at position 10: timestamp_col STRING\n" + "Resolution at position 10: timestamp_col STRING");
    // Mismatched type.
    AnalyzesOk(String.format("create table foo_avro (id int, bool_col boolean, tinyint_col int, " + "smallint_col int, int_col int, bigint_col bigint, float_col float," + "double_col bigint, date_string_col string, string_col string, " + "timestamp_col timestamp) stored as avro tblproperties ('avro.schema.url'='%s')", alltypesSchemaLoc), "Resolved the following name and/or type inconsistencies between the column " + "definitions and the Avro schema.\n" + "Column definition at position 7:  double_col BIGINT\n" + "Avro schema column at position 7: double_col DOUBLE\n" + "Resolution at position 7: double_col DOUBLE\n" + "Column definition at position 10:  timestamp_col TIMESTAMP\n" + "Avro schema column at position 10: timestamp_col STRING\n" + "Resolution at position 10: timestamp_col STRING");
    // Avro schema is inferred from column definitions.
    AnalyzesOk("create table foo_avro (c1 tinyint, c2 smallint, c3 int, c4 bigint, " + "c5 float, c6 double, c7 timestamp, c8 string, c9 char(10), c10 varchar(20)," + "c11 decimal(10, 5), c12 struct<f1:int,f2:string>, c13 array<int>," + "c14 map<string,string>) stored as avro");
    AnalyzesOk("create table foo_avro (c1 tinyint, c2 smallint, c3 int, c4 bigint, " + "c5 float, c6 double, c7 timestamp, c8 string, c9 char(10), c10 varchar(20)," + "c11 decimal(10, 5), c12 struct<f1:int,f2:string>, c13 array<int>," + "c14 map<string,string>) partitioned by (year int, month int) stored as avro");
    // Neither Avro schema nor column definitions.
    AnalysisError("create table foo_avro stored as avro tblproperties ('a'='b')", "An Avro table requires column definitions or an Avro schema.");
    // Invalid schema URL
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='')", "Invalid avro.schema.url: . Can not create a Path from an empty string");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='schema.avsc')", "Invalid avro.schema.url: schema.avsc. Path does not exist.");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='hdfs://invalid*host/schema.avsc')", "Failed to read Avro schema at: hdfs://invalid*host/schema.avsc. " + "Incomplete HDFS URI, no host: hdfs://invalid*host/schema.avsc");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.url'='foo://bar/schema.avsc')", "Failed to read Avro schema at: foo://bar/schema.avsc. " + "No FileSystem for scheme: foo");
    // Decimal parsing
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"precision\":5,\"scale\":2}}]}')");
    // Scale not required (defaults to zero).
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"precision\":5}}]}')");
    // Precision is always required
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":5}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "No 'precision' property specified for 'decimal' logicalType");
    // Precision/Scale must be positive integers
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":5, \"precision\":-20}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "Invalid decimal 'precision' property value: -20");
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\":\"value\",\"type\":{\"type\":\"bytes\", " + "\"logicalType\":\"decimal\",\"scale\":-1, \"precision\":20}}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "Invalid decimal 'scale' property value: -1");
    // Invalid schema (bad JSON - missing opening bracket for "field" array)
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": {\"name\": \"string1\", \"type\": \"string\"}]}')", "Error parsing Avro schema for table 'default.foo_avro': " + "org.codehaus.jackson.JsonParseException: Unexpected close marker ']': " + "expected '}'");
    // Map/Array types in Avro schema.
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"list1\", \"type\": {\"type\":\"array\", \"items\": \"int\"}}]}')");
    AnalyzesOk("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"map1\", \"type\": {\"type\":\"map\", \"values\": \"int\"}}]}')");
    // Union is not supported
    AnalysisError("create table foo_avro (i int) stored as avro tblproperties " + "('avro.schema.literal'='{\"name\": \"my_record\", \"type\": \"record\", " + "\"fields\": [{\"name\": \"string1\", \"type\": \"string\"}," + "{\"name\": \"union1\", \"type\": [\"float\", \"boolean\"]}]}')", "Unsupported type 'union' of column 'union1'");
    // TODO: Add COLLECTION ITEMS TERMINATED BY and MAP KEYS TERMINATED BY clauses.
    // Test struct complex type.
    AnalyzesOk("create table functional.new_table (" + "a struct<f1: int, f2: string, f3: timestamp, f4: boolean>, " + "b struct<f1: struct<f11: int>, f2: struct<f21: struct<f22: string>>>, " + "c struct<f1: map<int, string>, f2: array<bigint>>," + "d struct<f1: struct<f11: map<int, string>, f12: array<bigint>>>)");
    // Test array complex type.
    AnalyzesOk("create table functional.new_table (" + "a array<int>, b array<timestamp>, c array<string>, d array<boolean>, " + "e array<array<int>>, f array<array<array<string>>>, " + "g array<struct<f1: int, f2: string>>, " + "h array<map<string,int>>)");
    // Test map complex type.
    AnalyzesOk("create table functional.new_table (" + "a map<string, int>, b map<timestamp, boolean>, c map<bigint, float>, " + "d array<array<int>>, e array<array<array<string>>>, " + "f array<struct<f1: int, f2: string>>," + "g array<map<string,int>>)");
    // Cannot partition by a complex column.
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x array<int>)", "Type 'ARRAY<INT>' is not supported as partition-column type in column: x");
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x map<int,int>)", "Type 'MAP<INT,INT>' is not supported as partition-column type in column: x");
    AnalysisError("create table functional.new_table (i int) " + "partitioned by (x struct<f1:int>)", "Type 'STRUCT<f1:INT>' is not supported as partition-column type in column: x");
    // Kudu specific clauses used in an Avro table.
    AnalysisError("create table functional.new_table (i int) " + "distribute by hash(i) into 3 buckets stored as avro", "Only Kudu tables can use the DISTRIBUTE BY clause.");
    AnalysisError("create table functional.new_table (i int primary key) " + "stored as avro", "Unsupported column options for file format 'AVRO': " + "'i INT PRIMARY KEY'");
}
#end_block

#method_before
@Test
public void TestShowFiles() throws AnalysisException {
    // Test empty table
    AnalyzesOk(String.format("show files in functional.emptytable"));
    String[] partitions = new String[] { "", "partition(month=10, year=2010)" };
    for (String partition : partitions) {
        AnalyzesOk(String.format("show files in functional.alltypes %s", partition));
        // Database/table doesn't exist.
        AnalysisError(String.format("show files in baddb.alltypes %s", partition), "Database does not exist: baddb");
        AnalysisError(String.format("show files in functional.badtbl %s", partition), "Table does not exist: functional.badtbl");
        // Cannot show files on a non hdfs table.
        AnalysisError(String.format("show files in functional.alltypes_view %s", partition), "SHOW FILES not applicable to a non hdfs table: functional.alltypes_view");
    }
    // Not a partition column.
    AnalysisError("show files in functional.alltypes partition(year=2010,int_col=1)", "Column 'int_col' is not a partition column in table: functional.alltypes");
    // Not a valid column.
    AnalysisError("show files in functional.alltypes partition(year=2010,day=1)", "Partition column 'day' not found in table: functional.alltypes");
    // Table is not partitioned.
    AnalysisError("show files in functional.tinyinttable partition(int_col=1)", "Table is not partitioned: functional.tinyinttable");
    // Partition spec does not exist
    AnalysisError("show files in functional.alltypes partition(year=2010,month=NULL)", "Partition spec does not exist: (year=2010, month=NULL)");
}
#method_after
@Test
public void TestShowFiles() throws AnalysisException {
    // Test empty table
    AnalyzesOk(String.format("show files in functional.emptytable"));
    String[] partitions = new String[] { "", "partition(month=10, year=2010)", "partition(month>10, year<2011, year>2008)" };
    for (String partition : partitions) {
        AnalyzesOk(String.format("show files in functional.alltypes %s", partition));
        // Database/table doesn't exist.
        AnalysisError(String.format("show files in baddb.alltypes %s", partition), "Could not resolve table reference: 'baddb.alltypes'");
        AnalysisError(String.format("show files in functional.badtbl %s", partition), "Could not resolve table reference: 'functional.badtbl'");
        // Cannot show files on a non hdfs table.
        AnalysisError(String.format("show files in functional.alltypes_view %s", partition), "SHOW FILES not applicable to a non hdfs table: functional.alltypes_view");
        AnalysisError(String.format("show files in allcomplextypes.int_array_col %s", partition), createAnalyzer("functional"), "SHOW FILES not applicable to a non hdfs table: allcomplextypes.int_array_col");
    }
    // Not a partition column.
    AnalysisError("show files in functional.alltypes partition(year=2010,int_col=1)", "Partition exprs cannot contain non-partition column(s): int_col = 1.");
    // Not a valid column.
    AnalysisError("show files in functional.alltypes partition(year=2010,day=1)", "Could not resolve column/field reference: 'day'");
    // Table is not partitioned.
    AnalysisError("show files in functional.tinyinttable partition(int_col=1)", "Table is not partitioned: functional.tinyinttable");
    // Partition spec does not exist
    AnalysisError("show files in functional.alltypes partition(year=2010,month=NULL)", "No matching partition(s) found.");
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support the (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE, KUDU"));
    }
    if (createStmt_.getFileFormat() == THdfsFileFormat.KUDU && createStmt_.isExternal()) {
        // TODO: Add support for CTAS on external Kudu tables (see IMPALA-4318)
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT is not " + "supported for external Kudu tables."));
    }
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        // Subqueries need to be rewritten by the StmtRewriter first.
        if (analyzer.containsSubquery())
            return;
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the partition clause to the create statement.
    if (partitionKeys_ != null) {
        int colCnt = tmpQueryStmt.getColLabels().size();
        int partColCnt = partitionKeys_.size();
        if (partColCnt >= colCnt) {
            throw new AnalysisException(String.format("Number of partition columns (%s) " + "must be smaller than the number of columns in the select statement (%s).", partColCnt, colCnt));
        }
        int firstCol = colCnt - partColCnt;
        for (int i = firstCol, j = 0; i < colCnt; ++i, ++j) {
            String partitionLabel = partitionKeys_.get(j);
            String colLabel = tmpQueryStmt.getColLabels().get(i);
            // input column list.
            if (!partitionLabel.equals(colLabel)) {
                throw new AnalysisException(String.format("Partition column name " + "mismatch: %s != %s", partitionLabel, colLabel));
            }
            ColumnDef colDef = new ColumnDef(colLabel, null, Collections.<ColumnDef.Option, Object>emptyMap());
            colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
            createStmt_.getPartitionColumnDefs().add(colDef);
        }
        // Remove partition columns from table column list.
        tmpQueryStmt.getColLabels().subList(firstCol, colCnt).clear();
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    createStmt_.getColumnDefs().clear();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, Collections.<ColumnDef.Option, Object>emptyMap());
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    try (MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient()) {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        Table tmpTable = null;
        if (KuduTable.isKuduTable(msTbl)) {
            tmpTable = KuduTable.createCtasTarget(db, msTbl, createStmt_.getColumnDefs(), createStmt_.getTblPrimaryKeyColumnNames(), createStmt_.getDistributeParams());
        } else {
            // TODO: Creating a tmp table using load() is confusing.
            // Refactor it to use a 'createCtasTarget()' function similar to Kudu table.
            tmpTable = Table.fromMetastoreTable(db, msTbl);
            tmpTable.load(true, client.getHiveClient(), msTbl);
        }
        Preconditions.checkState(tmpTable != null && (tmpTable instanceof HdfsTable || tmpTable instanceof KuduTable));
        insertStmt_.setTargetTable(tmpTable);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support the (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE, KUDU"));
    }
    if (createStmt_.getFileFormat() == THdfsFileFormat.KUDU && createStmt_.isExternal()) {
        // TODO: Add support for CTAS on external Kudu tables (see IMPALA-4318)
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT is not " + "supported for external Kudu tables."));
    }
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        // Subqueries need to be rewritten by the StmtRewriter first.
        if (analyzer.containsSubquery())
            return;
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the partition clause to the create statement.
    if (partitionKeys_ != null) {
        int colCnt = tmpQueryStmt.getColLabels().size();
        int partColCnt = partitionKeys_.size();
        if (partColCnt >= colCnt) {
            throw new AnalysisException(String.format("Number of partition columns (%s) " + "must be smaller than the number of columns in the select statement (%s).", partColCnt, colCnt));
        }
        int firstCol = colCnt - partColCnt;
        for (int i = firstCol, j = 0; i < colCnt; ++i, ++j) {
            String partitionLabel = partitionKeys_.get(j);
            String colLabel = tmpQueryStmt.getColLabels().get(i);
            // input column list.
            if (!partitionLabel.equals(colLabel)) {
                throw new AnalysisException(String.format("Partition column name " + "mismatch: %s != %s", partitionLabel, colLabel));
            }
            ColumnDef colDef = new ColumnDef(colLabel, null);
            colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
            createStmt_.getPartitionColumnDefs().add(colDef);
        }
        // Remove partition columns from table column list.
        tmpQueryStmt.getColLabels().subList(firstCol, colCnt).clear();
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    createStmt_.getColumnDefs().clear();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, Collections.<ColumnDef.Option, Object>emptyMap());
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    try (MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient()) {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        Table tmpTable = null;
        if (KuduTable.isKuduTable(msTbl)) {
            tmpTable = KuduTable.createCtasTarget(db, msTbl, createStmt_.getColumnDefs(), createStmt_.getTblPrimaryKeyColumnNames(), createStmt_.getDistributeParams());
        } else {
            // TODO: Creating a tmp table using load() is confusing.
            // Refactor it to use a 'createCtasTarget()' function similar to Kudu table.
            tmpTable = Table.fromMetastoreTable(db, msTbl);
            tmpTable.load(true, client.getHiveClient(), msTbl);
        }
        Preconditions.checkState(tmpTable != null && (tmpTable instanceof HdfsTable || tmpTable instanceof KuduTable));
        insertStmt_.setTargetTable(tmpTable);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#end_block

#method_before
public static KuduColumn fromColumnSchema(ColumnSchema colSchema, int position) throws ImpalaRuntimeException {
    Type type = KuduUtil.toImpalaType(colSchema.getType());
    Object defaultValue = colSchema.getDefaultValue();
    LiteralExpr defaultValueExpr = null;
    if (defaultValue != null) {
        try {
            defaultValueExpr = LiteralExpr.create(defaultValue.toString(), type);
        } catch (AnalysisException e) {
            throw new ImpalaRuntimeException(String.format("Error parsing default value " + "'%s': %s", defaultValue, e.getMessage()));
        }
        Preconditions.checkNotNull(defaultValueExpr);
    }
    return new KuduColumn(colSchema.getName(), type, colSchema.isKey(), colSchema.isNullable(), colSchema.getEncoding(), colSchema.getCompressionAlgorithm(), defaultValueExpr, colSchema.getDesiredBlockSize(), null, position);
}
#method_after
public static KuduColumn fromColumnSchema(ColumnSchema colSchema, int position) throws ImpalaRuntimeException {
    Type type = KuduUtil.toImpalaType(colSchema.getType());
    Object defaultValue = colSchema.getDefaultValue();
    LiteralExpr defaultValueExpr = null;
    if (defaultValue != null) {
        try {
            defaultValueExpr = LiteralExpr.create(defaultValue.toString(), type);
        } catch (AnalysisException e) {
            throw new ImpalaRuntimeException(String.format("Error parsing default value: " + "'%s'", defaultValue), e);
        }
        Preconditions.checkNotNull(defaultValueExpr);
    }
    return new KuduColumn(colSchema.getName(), type, colSchema.isKey(), colSchema.isNullable(), colSchema.getEncoding(), colSchema.getCompressionAlgorithm(), defaultValueExpr, colSchema.getDesiredBlockSize(), null, position);
}
#end_block

#method_before
@Override
public TColumn toThrift() {
    TColumn colDesc = new TColumn(name_, type_.toThrift());
    KuduUtil.toTKuduColumnHelper(colDesc, isKey_, isNullable_, encoding_, compression_, defaultValue_, blockSize_);
    if (comment_ != null)
        colDesc.setComment(comment_);
    colDesc.setCol_stats(getStats().toThrift());
    colDesc.setPosition(position_);
    colDesc.setIs_kudu_column(true);
    return colDesc;
}
#method_after
@Override
public TColumn toThrift() {
    TColumn colDesc = new TColumn(name_, type_.toThrift());
    KuduUtil.setColumnOptions(colDesc, isKey_, isNullable_, encoding_, compression_, defaultValue_, blockSize_);
    if (comment_ != null)
        colDesc.setComment(comment_);
    colDesc.setCol_stats(getStats().toThrift());
    colDesc.setPosition(position_);
    colDesc.setIs_kudu_column(true);
    return colDesc;
}
#end_block

#method_before
public static KuduClient createKuduClient(String kuduMasters) {
    KuduClientBuilder b = new KuduClient.KuduClientBuilder(kuduMasters);
    b.defaultAdminOperationTimeoutMs(BackendConfig.getKuduClientTimeoutMs());
    b.defaultOperationTimeoutMs(BackendConfig.getKuduClientTimeoutMs());
    return b.build();
}
#method_after
public static KuduClient createKuduClient(String kuduMasters) {
    KuduClientBuilder b = new KuduClient.KuduClientBuilder(kuduMasters);
    b.defaultAdminOperationTimeoutMs(BackendConfig.INSTANCE.getKuduClientTimeoutMs());
    b.defaultOperationTimeoutMs(BackendConfig.INSTANCE.getKuduClientTimeoutMs());
    return b.build();
}
#end_block

#method_before
public static CompressionAlgorithm fromThrift(THdfsCompression compression) throws ImpalaRuntimeException {
    switch(compression) {
        case DEFAULT:
            return CompressionAlgorithm.DEFAULT_COMPRESSION;
        case NONE:
            return CompressionAlgorithm.NO_COMPRESSION;
        case SNAPPY:
            return CompressionAlgorithm.SNAPPY;
        case LZ4:
            return CompressionAlgorithm.LZ4;
        case ZLIB:
            return CompressionAlgorithm.ZLIB;
        default:
            throw new ImpalaRuntimeException("Unsupported compression algorithm: " + compression.toString());
    }
}
#method_after
public static Encoding fromThrift(TColumnEncoding encoding) throws ImpalaRuntimeException {
    switch(encoding) {
        case AUTO:
            return Encoding.AUTO_ENCODING;
        case PLAIN:
            return Encoding.PLAIN_ENCODING;
        case PREFIX:
            return Encoding.PREFIX_ENCODING;
        case GROUP_VARINT:
            return Encoding.GROUP_VARINT;
        case RLE:
            return Encoding.RLE;
        case DICTIONARY:
            return Encoding.DICT_ENCODING;
        case BIT_SHUFFLE:
            return Encoding.BIT_SHUFFLE;
        default:
            throw new ImpalaRuntimeException("Unsupported encoding: " + encoding.toString());
    }
}
#end_block

#method_before
public static TEncoding toThrift(Encoding encoding) throws ImpalaRuntimeException {
    switch(encoding) {
        case AUTO_ENCODING:
            return TEncoding.AUTO;
        case PLAIN_ENCODING:
            return TEncoding.PLAIN;
        case PREFIX_ENCODING:
            return TEncoding.PREFIX;
        case GROUP_VARINT:
            return TEncoding.GROUP_VARINT;
        case RLE:
            return TEncoding.RLE;
        case DICT_ENCODING:
            return TEncoding.DICTIONARY;
        case BIT_SHUFFLE:
            return TEncoding.BIT_SHUFFLE;
        default:
            throw new ImpalaRuntimeException("Unsupported encoding: " + encoding.toString());
    }
}
#method_after
public static TColumnEncoding toThrift(Encoding encoding) throws ImpalaRuntimeException {
    switch(encoding) {
        case AUTO_ENCODING:
            return TColumnEncoding.AUTO;
        case PLAIN_ENCODING:
            return TColumnEncoding.PLAIN;
        case PREFIX_ENCODING:
            return TColumnEncoding.PREFIX;
        case GROUP_VARINT:
            return TColumnEncoding.GROUP_VARINT;
        case RLE:
            return TColumnEncoding.RLE;
        case DICT_ENCODING:
            return TColumnEncoding.DICTIONARY;
        case BIT_SHUFFLE:
            return TColumnEncoding.BIT_SHUFFLE;
        default:
            throw new ImpalaRuntimeException("Unsupported encoding: " + encoding.toString());
    }
}
#end_block

#method_before
private void analyzeColumnDefs(Analyzer analyzer) throws AnalysisException {
    Set<String> colNames = Sets.newHashSet();
    for (ColumnDef colDef : columnDefs_) {
        colDef.analyze(analyzer);
        if (!colNames.add(colDef.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDef.getColName());
        }
        if (getFileFormat() != THdfsFileFormat.KUDU && colDef.hasKuduSpecificOptions()) {
            throw new AnalysisException(String.format("Unsupported column options for " + "file format '%s': '%s'", getFileFormat().name(), colDef.toString()));
        }
    }
    for (ColumnDef colDef : getPartitionColumnDefs()) {
        colDef.analyze(analyzer);
        if (!colDef.getType().supportsTablePartitioning()) {
            throw new AnalysisException(String.format("Type '%s' is not supported as partition-column type " + "in column: %s", colDef.getType().toSql(), colDef.getColName()));
        }
        if (!colNames.add(colDef.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDef.getColName());
        }
    }
}
#method_after
private void analyzeColumnDefs(Analyzer analyzer) throws AnalysisException {
    Set<String> colNames = Sets.newHashSet();
    for (ColumnDef colDef : columnDefs_) {
        colDef.analyze(analyzer);
        if (!colNames.add(colDef.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDef.getColName());
        }
        if (getFileFormat() != THdfsFileFormat.KUDU && colDef.hasKuduOptions()) {
            throw new AnalysisException(String.format("Unsupported column options for " + "file format '%s': '%s'", getFileFormat().name(), colDef.toString()));
        }
    }
    for (ColumnDef colDef : getPartitionColumnDefs()) {
        colDef.analyze(analyzer);
        if (!colDef.getType().supportsTablePartitioning()) {
            throw new AnalysisException(String.format("Type '%s' is not supported as partition-column type " + "in column: %s", colDef.getType().toSql(), colDef.getColName()));
        }
        if (!colNames.add(colDef.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDef.getColName());
        }
    }
}
#end_block

#method_before
private void analyzePrimaryKeys() throws AnalysisException {
    for (ColumnDef colDef : columnDefs_) {
        if (colDef.isPrimaryKey())
            primaryKeyColDefs_.add(colDef);
    }
    if (primaryKeyColDefs_.size() > 1) {
        throw new AnalysisException("Multiple primary keys specified. " + "Composite primary keys can be specified using the " + "PRIMARY KEY (col1, col2, ...) syntax at the end of the column definition.");
    }
    if (primaryKeyColNames_.isEmpty())
        return;
    if (!primaryKeyColDefs_.isEmpty()) {
        throw new AnalysisException("Multiple primary keys specified. " + "Composite primary keys can be specified using the " + "PRIMARY KEY (col1, col2, ...) syntax at the end of the column definition.");
    }
    Map<String, ColumnDef> colDefsByColName = ColumnDef.mapByColumnNames(columnDefs_);
    for (String colName : primaryKeyColNames_) {
        colName = colName.toLowerCase();
        ColumnDef colDef = colDefsByColName.remove(colName);
        if (colDef == null) {
            if (ColumnDef.toColumnNames(primaryKeyColDefs_).contains(colName)) {
                throw new AnalysisException(String.format("Column '%s' is listed multiple " + "times as a PRIMARY KEY.", colName));
            }
            throw new AnalysisException(String.format("PRIMARY KEY column '%s' does not exist in the table", colName));
        }
        primaryKeyColDefs_.add(colDef);
    }
}
#method_after
private void analyzePrimaryKeys() throws AnalysisException {
    for (ColumnDef colDef : columnDefs_) {
        if (colDef.isPrimaryKey())
            primaryKeyColDefs_.add(colDef);
    }
    if (primaryKeyColDefs_.size() > 1) {
        throw new AnalysisException("Multiple primary keys specified. " + "Composite primary keys can be specified using the " + "PRIMARY KEY (col1, col2, ...) syntax at the end of the column definition.");
    }
    if (primaryKeyColNames_.isEmpty())
        return;
    if (!primaryKeyColDefs_.isEmpty()) {
        throw new AnalysisException("Multiple primary keys specified. " + "Composite primary keys can be specified using the " + "PRIMARY KEY (col1, col2, ...) syntax at the end of the column definition.");
    }
    Map<String, ColumnDef> colDefsByColName = ColumnDef.mapByColumnNames(columnDefs_);
    for (String colName : primaryKeyColNames_) {
        colName = colName.toLowerCase();
        ColumnDef colDef = colDefsByColName.remove(colName);
        if (colDef == null) {
            if (ColumnDef.toColumnNames(primaryKeyColDefs_).contains(colName)) {
                throw new AnalysisException(String.format("Column '%s' is listed multiple " + "times as a PRIMARY KEY.", colName));
            }
            throw new AnalysisException(String.format("PRIMARY KEY column '%s' does not exist in the table", colName));
        }
        if (colDef.isNullable()) {
            throw new AnalysisException("Primary key columns cannot be nullable: " + colDef.toString());
        }
        primaryKeyColDefs_.add(colDef);
    }
}
#end_block

#method_before
protected void createColumnAndViewDefs(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(dbName_);
    Preconditions.checkNotNull(owner_);
    // Set the finalColDefs to reflect the given column definitions.
    if (columnDefs_ != null) {
        Preconditions.checkState(!columnDefs_.isEmpty());
        if (columnDefs_.size() != viewDefStmt_.getColLabels().size()) {
            String cmp = (columnDefs_.size() > viewDefStmt_.getColLabels().size()) ? "more" : "fewer";
            throw new AnalysisException(String.format("Column-definition list has " + "%s columns (%s) than the view-definition query statement returns (%s).", cmp, columnDefs_.size(), viewDefStmt_.getColLabels().size()));
        }
        finalColDefs_ = columnDefs_;
        Preconditions.checkState(columnDefs_.size() == viewDefStmt_.getBaseTblResultExprs().size());
        for (int i = 0; i < columnDefs_.size(); ++i) {
            // Set type in the column definition from the view-definition statement.
            columnDefs_.get(i).setType(viewDefStmt_.getBaseTblResultExprs().get(i).getType());
        }
    } else {
        // Create list of column definitions from the view-definition statement.
        finalColDefs_ = Lists.newArrayList();
        List<Expr> exprs = viewDefStmt_.getBaseTblResultExprs();
        List<String> labels = viewDefStmt_.getColLabels();
        Preconditions.checkState(exprs.size() == labels.size());
        for (int i = 0; i < viewDefStmt_.getColLabels().size(); ++i) {
            ColumnDef colDef = new ColumnDef(labels.get(i), null, Collections.<ColumnDef.Option, Object>emptyMap());
            colDef.setType(exprs.get(i).getType());
            finalColDefs_.add(colDef);
        }
    }
    // Check that the column definitions have valid names, and that there are no
    // duplicate column names.
    Set<String> distinctColNames = Sets.newHashSet();
    for (ColumnDef colDesc : finalColDefs_) {
        colDesc.analyze(null);
        if (!distinctColNames.add(colDesc.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDesc.getColName());
        }
    }
    // Set original and expanded view-definition SQL strings.
    originalViewDef_ = viewDefStmt_.toSql();
    // as the original one.
    if (columnDefs_ == null) {
        inlineViewDef_ = originalViewDef_;
        return;
    }
    // Wrap the original view-definition statement into a SELECT to enforce the
    // given column definitions.
    StringBuilder sb = new StringBuilder();
    sb.append("SELECT ");
    for (int i = 0; i < finalColDefs_.size(); ++i) {
        String colRef = ToSqlUtils.getIdentSql(viewDefStmt_.getColLabels().get(i));
        String colAlias = ToSqlUtils.getIdentSql(finalColDefs_.get(i).getColName());
        sb.append(String.format("%s.%s AS %s", tableName_.getTbl(), colRef, colAlias));
        sb.append((i + 1 != finalColDefs_.size()) ? ", " : "");
    }
    // Do not use 'AS' for table aliases because Hive only accepts them without 'AS'.
    sb.append(String.format(" FROM (%s) %s", originalViewDef_, tableName_.getTbl()));
    inlineViewDef_ = sb.toString();
}
#method_after
protected void createColumnAndViewDefs(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkNotNull(dbName_);
    Preconditions.checkNotNull(owner_);
    // Set the finalColDefs to reflect the given column definitions.
    if (columnDefs_ != null) {
        Preconditions.checkState(!columnDefs_.isEmpty());
        if (columnDefs_.size() != viewDefStmt_.getColLabels().size()) {
            String cmp = (columnDefs_.size() > viewDefStmt_.getColLabels().size()) ? "more" : "fewer";
            throw new AnalysisException(String.format("Column-definition list has " + "%s columns (%s) than the view-definition query statement returns (%s).", cmp, columnDefs_.size(), viewDefStmt_.getColLabels().size()));
        }
        finalColDefs_ = columnDefs_;
        Preconditions.checkState(columnDefs_.size() == viewDefStmt_.getBaseTblResultExprs().size());
        for (int i = 0; i < columnDefs_.size(); ++i) {
            // Set type in the column definition from the view-definition statement.
            columnDefs_.get(i).setType(viewDefStmt_.getBaseTblResultExprs().get(i).getType());
        }
    } else {
        // Create list of column definitions from the view-definition statement.
        finalColDefs_ = Lists.newArrayList();
        List<Expr> exprs = viewDefStmt_.getBaseTblResultExprs();
        List<String> labels = viewDefStmt_.getColLabels();
        Preconditions.checkState(exprs.size() == labels.size());
        for (int i = 0; i < viewDefStmt_.getColLabels().size(); ++i) {
            ColumnDef colDef = new ColumnDef(labels.get(i), null);
            colDef.setType(exprs.get(i).getType());
            finalColDefs_.add(colDef);
        }
    }
    // Check that the column definitions have valid names, and that there are no
    // duplicate column names.
    Set<String> distinctColNames = Sets.newHashSet();
    for (ColumnDef colDesc : finalColDefs_) {
        colDesc.analyze(null);
        if (!distinctColNames.add(colDesc.getColName().toLowerCase())) {
            throw new AnalysisException("Duplicate column name: " + colDesc.getColName());
        }
    }
    // Set original and expanded view-definition SQL strings.
    originalViewDef_ = viewDefStmt_.toSql();
    // as the original one.
    if (columnDefs_ == null) {
        inlineViewDef_ = originalViewDef_;
        return;
    }
    // Wrap the original view-definition statement into a SELECT to enforce the
    // given column definitions.
    StringBuilder sb = new StringBuilder();
    sb.append("SELECT ");
    for (int i = 0; i < finalColDefs_.size(); ++i) {
        String colRef = ToSqlUtils.getIdentSql(viewDefStmt_.getColLabels().get(i));
        String colAlias = ToSqlUtils.getIdentSql(finalColDefs_.get(i).getColName());
        sb.append(String.format("%s.%s AS %s", tableName_.getTbl(), colRef, colAlias));
        sb.append((i + 1 != finalColDefs_.size()) ? ", " : "");
    }
    // Do not use 'AS' for table aliases because Hive only accepts them without 'AS'.
    sb.append(String.format(" FROM (%s) %s", originalViewDef_, tableName_.getTbl()));
    inlineViewDef_ = sb.toString();
}
#end_block

#method_before
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    // Finally, 'undo' the permutation so that the selectListExprs are in Hive column
    // order, and add NULL expressions to all missing columns, unless this is an UPSERT.
    ArrayList<Column> columns = table_.getColumnsInHiveOrder();
    for (int col = 0; col < columns.size(); ++col) {
        Column tblColumn = columns.get(col);
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                mentionedColumns_.add(col);
                matchFound = true;
                break;
            }
        }
        // expression if this is an INSERT and the target is not a Kudu table.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                if (table_ instanceof KuduTable) {
                    Preconditions.checkState(tblColumn instanceof KuduColumn);
                    KuduColumn kuduCol = (KuduColumn) tblColumn;
                    if (!kuduCol.hasDefaultValue() && !kuduCol.isNullable()) {
                        throw new AnalysisException("Missing values for column " + kuduCol.getName());
                    }
                } else {
                    // Unmentioned non-clustering columns get NULL literals with the appropriate
                    // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                    resultExprs_.add(NullLiteral.create(tblColumn.getType()));
                }
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && table_ instanceof KuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#method_after
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    boolean isKuduTable = table_ instanceof KuduTable;
    // Finally, 'undo' the permutation so that the selectListExprs are in Hive column
    // order, and add NULL expressions to all missing columns, unless this is an UPSERT.
    ArrayList<Column> columns = table_.getColumnsInHiveOrder();
    for (int col = 0; col < columns.size(); ++col) {
        Column tblColumn = columns.get(col);
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                if (isKuduTable)
                    mentionedColumns_.add(col);
                matchFound = true;
                break;
            }
        }
        // expression if this is an INSERT and the target is not a Kudu table.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                if (isKuduTable) {
                    Preconditions.checkState(tblColumn instanceof KuduColumn);
                    KuduColumn kuduCol = (KuduColumn) tblColumn;
                    if (!kuduCol.hasDefaultValue() && !kuduCol.isNullable()) {
                        throw new AnalysisException("Missing values for column that is not " + "nullable and has no default value " + kuduCol.getName());
                    }
                } else {
                    // Unmentioned non-clustering columns get NULL literals with the appropriate
                    // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                    resultExprs_.add(NullLiteral.create(tblColumn.getType()));
                }
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && isKuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#end_block

#method_before
public TResultSet getTableStats() throws ImpalaRuntimeException {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    resultSchema.addToColumns(new TColumn("# Rows", Type.INT.toThrift()));
    resultSchema.addToColumns(new TColumn("Start Key", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Stop Key", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Leader Replica", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("# Replicas", Type.INT.toThrift()));
    try (KuduClient client = KuduUtil.createKuduClient(getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable kuduTable = client.openTable(kuduTableName_);
        List<LocatedTablet> tablets = kuduTable.getTabletsLocations(BackendConfig.getKuduClientTimeoutMs());
        for (LocatedTablet tab : tablets) {
            TResultRowBuilder builder = new TResultRowBuilder();
            // The Kudu client API doesn't expose tablet row counts.
            builder.add("-1");
            builder.add(DatatypeConverter.printHexBinary(tab.getPartition().getPartitionKeyStart()));
            builder.add(DatatypeConverter.printHexBinary(tab.getPartition().getPartitionKeyEnd()));
            LocatedTablet.Replica leader = tab.getLeaderReplica();
            if (leader == null) {
                // Leader might be null, if it is not yet available (e.g. during
                // leader election in Kudu)
                builder.add("Leader n/a");
            } else {
                builder.add(leader.getRpcHost() + ":" + leader.getRpcPort().toString());
            }
            builder.add(tab.getReplicas().size());
            result.addToRows(builder.get());
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Error accessing Kudu for table stats.", e);
    }
    return result;
}
#method_after
public TResultSet getTableStats() throws ImpalaRuntimeException {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    resultSchema.addToColumns(new TColumn("# Rows", Type.INT.toThrift()));
    resultSchema.addToColumns(new TColumn("Start Key", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Stop Key", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Leader Replica", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("# Replicas", Type.INT.toThrift()));
    try (KuduClient client = KuduUtil.createKuduClient(getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable kuduTable = client.openTable(kuduTableName_);
        List<LocatedTablet> tablets = kuduTable.getTabletsLocations(BackendConfig.INSTANCE.getKuduClientTimeoutMs());
        for (LocatedTablet tab : tablets) {
            TResultRowBuilder builder = new TResultRowBuilder();
            // The Kudu client API doesn't expose tablet row counts.
            builder.add("-1");
            builder.add(DatatypeConverter.printHexBinary(tab.getPartition().getPartitionKeyStart()));
            builder.add(DatatypeConverter.printHexBinary(tab.getPartition().getPartitionKeyEnd()));
            LocatedTablet.Replica leader = tab.getLeaderReplica();
            if (leader == null) {
                // Leader might be null, if it is not yet available (e.g. during
                // leader election in Kudu)
                builder.add("Leader n/a");
            } else {
                builder.add(leader.getRpcHost() + ":" + leader.getRpcPort().toString());
            }
            builder.add(tab.getReplicas().size());
            result.addToRows(builder.get());
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Error accessing Kudu for table stats.", e);
    }
    return result;
}
#end_block

#method_before
@Test
public void TestIdentQuoting() {
    ParsesOk("select a from `t`");
    ParsesOk("select a from `default`.`t`");
    ParsesOk("select a from `default`.t");
    ParsesOk("select a from `default`.`t`");
    ParsesOk("select 01a from `default`.`01_t`");
    ParsesOk("select `a` from `default`.t");
    ParsesOk("select `tbl`.`a` from `default`.t");
    ParsesOk("select `db`.`tbl`.`a` from `default`.t");
    ParsesOk("select `12db`.`tbl`.`12_a` from `default`.t");
    // Make sure quoted float literals are identifiers.
    ParsesOk("select `8e6`", SlotRef.class);
    ParsesOk("select `4.5e2`", SlotRef.class);
    ParsesOk("select `.7e9`", SlotRef.class);
    // Mixed quoting
    ParsesOk("select `db`.tbl.`a` from `default`.t");
    ParsesOk("select `db.table.a` from `default`.t");
    // Identifiers consisting of only whitespace not allowed.
    ParserError("select a from ` `");
    ParserError("select a from `    `");
    // Empty quoted identifier doesn't parse.
    ParserError("select a from ``");
    // Whitespace can be interspersed with other characters.
    // Whitespace is trimmed from the beginning and end of an identifier.
    ParsesOk("select a from `a a a    `");
    ParsesOk("select a from `    a a a`");
    ParsesOk("select a from `    a a a    `");
    // Quoted identifiers can contain any characters except "`".
    ParsesOk("select a from `all types`");
    ParsesOk("select a from `default`.`all types`");
    ParsesOk("select a from `~!@#$%^&*()-_=+|;:'\",<.>/?`");
    // Quoted identifiers do not unescape escape sequences.
    ParsesOk("select a from `ab\rabc`");
    ParsesOk("select a from `ab\tabc`");
    ParsesOk("select a from `ab\fabc`");
    ParsesOk("select a from `ab\babc`");
    ParsesOk("select a from `ab\nabc`");
    // Test non-printable control characters inside quoted identifiers.
    ParsesOk("select a from `abc\u0000abc`");
    ParsesOk("select a from `abc\u0019abc`");
    ParsesOk("select a from `abc\u007fabc`");
    // Quoted identifiers can contain keywords.
    ParsesOk("select `select`, `insert`, `upsert` from `table` where `where` = 10");
    // Quoted identifiers cannot contain "`"
    ParserError("select a from `abcde`abcde`");
    ParserError("select a from `abc\u0060abc`");
    // Wrong quotes
    ParserError("select a from 'default'.'t'");
    // Lots of quoting
    ParsesOk("select `db`.`tbl`.`a` from `default`.`t` `alias` where `alias`.`col` = 'string'" + " group by `alias`.`col`");
}
#method_after
@Test
public void TestIdentQuoting() {
    ParsesOk("select a from `t`");
    ParsesOk("select a from default.`t`");
    ParsesOk("select a from default.t");
    ParsesOk("select a from default.`t`");
    ParsesOk("select 01a from default.`01_t`");
    ParsesOk("select `a` from default.t");
    ParsesOk("select `tbl`.`a` from default.t");
    ParsesOk("select `db`.`tbl`.`a` from default.t");
    ParsesOk("select `12db`.`tbl`.`12_a` from default.t");
    // Make sure quoted float literals are identifiers.
    ParsesOk("select `8e6`", SlotRef.class);
    ParsesOk("select `4.5e2`", SlotRef.class);
    ParsesOk("select `.7e9`", SlotRef.class);
    // Mixed quoting
    ParsesOk("select `db`.tbl.`a` from default.t");
    ParsesOk("select `db.table.a` from default.t");
    // Identifiers consisting of only whitespace not allowed.
    ParserError("select a from ` `");
    ParserError("select a from `    `");
    // Empty quoted identifier doesn't parse.
    ParserError("select a from ``");
    // Whitespace can be interspersed with other characters.
    // Whitespace is trimmed from the beginning and end of an identifier.
    ParsesOk("select a from `a a a    `");
    ParsesOk("select a from `    a a a`");
    ParsesOk("select a from `    a a a    `");
    // Quoted identifiers can contain any characters except "`".
    ParsesOk("select a from `all types`");
    ParsesOk("select a from default.`all types`");
    ParsesOk("select a from `~!@#$%^&*()-_=+|;:'\",<.>/?`");
    // Quoted identifiers do not unescape escape sequences.
    ParsesOk("select a from `ab\rabc`");
    ParsesOk("select a from `ab\tabc`");
    ParsesOk("select a from `ab\fabc`");
    ParsesOk("select a from `ab\babc`");
    ParsesOk("select a from `ab\nabc`");
    // Test non-printable control characters inside quoted identifiers.
    ParsesOk("select a from `abc\u0000abc`");
    ParsesOk("select a from `abc\u0019abc`");
    ParsesOk("select a from `abc\u007fabc`");
    // Quoted identifiers can contain keywords.
    ParsesOk("select `select`, `insert`, `upsert` from `table` where `where` = 10");
    // Quoted identifiers cannot contain "`"
    ParserError("select a from `abcde`abcde`");
    ParserError("select a from `abc\u0060abc`");
    // Wrong quotes
    ParserError("select a from 'default'.'t'");
    // Lots of quoting
    ParsesOk("select `db`.`tbl`.`a` from `default`.`t` `alias` where `alias`.`col` = 'string'" + " group by `alias`.`col`");
}
#end_block

#method_before
@Test
public void TestShow() {
    // Short form ok
    ParsesOk("SHOW TABLES");
    // Well-formed pattern
    ParsesOk("SHOW TABLES 'tablename|othername'");
    // Empty pattern ok
    ParsesOk("SHOW TABLES ''");
    // Databases
    ParsesOk("SHOW DATABASES");
    ParsesOk("SHOW SCHEMAS");
    ParsesOk("SHOW DATABASES LIKE 'pattern'");
    ParsesOk("SHOW SCHEMAS LIKE 'p*ttern'");
    // Data sources
    ParsesOk("SHOW DATA SOURCES");
    ParsesOk("SHOW DATA SOURCES 'pattern'");
    ParsesOk("SHOW DATA SOURCES LIKE 'pattern'");
    ParsesOk("SHOW DATA SOURCES LIKE 'p*ttern'");
    // Functions
    for (String fnType : new String[] { "", "AGGREGATE", "ANALYTIC" }) {
        ParsesOk(String.format("SHOW %s FUNCTIONS", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS LIKE 'pattern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS LIKE 'p*ttern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS in DB LIKE 'pattern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS in DB", fnType));
    }
    // Show table/column stats.
    ParsesOk("SHOW TABLE STATS tbl");
    ParsesOk("SHOW TABLE STATS db.tbl");
    ParsesOk("SHOW TABLE STATS `db`.`tbl`");
    ParsesOk("SHOW COLUMN STATS tbl");
    ParsesOk("SHOW COLUMN STATS db.tbl");
    ParsesOk("SHOW COLUMN STATS `db`.`tbl`");
    // Show partitions
    ParsesOk("SHOW PARTITIONS tbl");
    ParsesOk("SHOW PARTITIONS db.tbl");
    ParsesOk("SHOW PARTITIONS `db`.`tbl`");
    // Show files of table
    ParsesOk("SHOW FILES IN tbl");
    ParsesOk("SHOW FILES IN db.tbl");
    ParsesOk("SHOW FILES IN `db`.`tbl`");
    ParsesOk("SHOW FILES IN db.tbl PARTITION(x='a',y='b')");
    // Missing arguments
    ParserError("SHOW");
    // Malformed pattern (no quotes)
    ParserError("SHOW TABLES tablename");
    // Invalid SHOW DATA SOURCE statements
    ParserError("SHOW DATA");
    ParserError("SHOW SOURCE");
    ParserError("SHOW DATA SOURCE LIKE NotStrLiteral");
    ParserError("SHOW UNKNOWN FUNCTIONS");
    // Missing table/column qualifier.
    ParserError("SHOW STATS tbl");
    // Missing table.
    ParserError("SHOW TABLE STATS");
    ParserError("SHOW COLUMN STATS");
    // String literal not accepted.
    ParserError("SHOW TABLE STATS 'strlit'");
    // Missing table.
    ParserError("SHOW FILES IN");
    // Invalid partition.
    ParserError("SHOW FILES IN db.tbl PARTITION(p)");
}
#method_after
@Test
public void TestShow() {
    // Short form ok
    ParsesOk("SHOW TABLES");
    // Well-formed pattern
    ParsesOk("SHOW TABLES 'tablename|othername'");
    // Empty pattern ok
    ParsesOk("SHOW TABLES ''");
    // Databases
    ParsesOk("SHOW DATABASES");
    ParsesOk("SHOW SCHEMAS");
    ParsesOk("SHOW DATABASES LIKE 'pattern'");
    ParsesOk("SHOW SCHEMAS LIKE 'p*ttern'");
    // Data sources
    ParsesOk("SHOW DATA SOURCES");
    ParsesOk("SHOW DATA SOURCES 'pattern'");
    ParsesOk("SHOW DATA SOURCES LIKE 'pattern'");
    ParsesOk("SHOW DATA SOURCES LIKE 'p*ttern'");
    // Functions
    for (String fnType : new String[] { "", "AGGREGATE", "ANALYTIC" }) {
        ParsesOk(String.format("SHOW %s FUNCTIONS", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS LIKE 'pattern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS LIKE 'p*ttern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS in DB LIKE 'pattern'", fnType));
        ParsesOk(String.format("SHOW %s FUNCTIONS in DB", fnType));
    }
    // Show table/column stats.
    ParsesOk("SHOW TABLE STATS tbl");
    ParsesOk("SHOW TABLE STATS db.tbl");
    ParsesOk("SHOW TABLE STATS `db`.`tbl`");
    ParsesOk("SHOW COLUMN STATS tbl");
    ParsesOk("SHOW COLUMN STATS db.tbl");
    ParsesOk("SHOW COLUMN STATS `db`.`tbl`");
    // Show partitions
    ParsesOk("SHOW PARTITIONS tbl");
    ParsesOk("SHOW PARTITIONS db.tbl");
    ParsesOk("SHOW PARTITIONS `db`.`tbl`");
    // Show files of table
    ParsesOk("SHOW FILES IN tbl");
    ParsesOk("SHOW FILES IN db.tbl");
    ParsesOk("SHOW FILES IN `db`.`tbl`");
    ParsesOk("SHOW FILES IN db.tbl PARTITION(x='a',y='b')");
    // Missing arguments
    ParserError("SHOW");
    // Malformed pattern (no quotes)
    ParserError("SHOW TABLES tablename");
    // Invalid SHOW DATA SOURCE statements
    ParserError("SHOW DATA");
    ParserError("SHOW SOURCE");
    ParserError("SHOW DATA SOURCE LIKE NotStrLiteral");
    ParserError("SHOW UNKNOWN FUNCTIONS");
    // Missing table/column qualifier.
    ParserError("SHOW STATS tbl");
    // Missing table.
    ParserError("SHOW TABLE STATS");
    ParserError("SHOW COLUMN STATS");
    // String literal not accepted.
    ParserError("SHOW TABLE STATS 'strlit'");
    // Missing table.
    ParserError("SHOW FILES IN");
    ParsesOk("SHOW FILES IN db.tbl PARTITION(p)");
}
#end_block

#method_before
@Test
public void TestAlterTableAddPartition() {
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=1)");
    ParsesOk("ALTER TABLE TestDb.Foo ADD IF NOT EXISTS PARTITION (i=1, s='Hello')");
    ParsesOk("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, s='Hello') LOCATION '/a/b'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=NULL)");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=NULL, j=2, k=NULL)");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=abc, j=(5*8+10), k=!true and false)");
    // Cannot use dynamic partition syntax
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION (partcol)");
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, partcol)");
    // Location needs to be a string literal
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, s='Hello') LOCATION a/b");
    // Caching ops
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED 'pool'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' WITH replication = 3");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' " + "with replication = -1");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) UNCACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' UNCACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' CACHED IN 'pool'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' CACHED IN 'pool' " + "with replication = 3");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' LOCATION 'a/b'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) UNCACHED LOCATION 'a/b'");
    ParserError("ALTER TABLE Foo ADD IF EXISTS PARTITION (i=1, s='Hello')");
    ParserError("ALTER TABLE TestDb.Foo ADD (i=1, s='Hello')");
    ParserError("ALTER TABLE TestDb.Foo ADD (i=1)");
    ParserError("ALTER TABLE Foo (i=1)");
    ParserError("ALTER TABLE TestDb.Foo PARTITION (i=1)");
    ParserError("ALTER TABLE Foo ADD PARTITION");
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION ()");
    ParserError("ALTER Foo ADD PARTITION (i=1)");
    ParserError("ALTER TABLE ADD PARTITION (i=1)");
    ParserError("ALTER TABLE ADD");
    ParserError("ALTER TABLE DROP");
}
#method_after
@Test
public void TestAlterTableAddPartition() {
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=1)");
    ParsesOk("ALTER TABLE TestDb.Foo ADD IF NOT EXISTS PARTITION (i=1, s='Hello')");
    ParsesOk("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, s='Hello') LOCATION '/a/b'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=NULL)");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=NULL, j=2, k=NULL)");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (i=abc, j=(5*8+10), k=!true and false)");
    // Location needs to be a string literal
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION (i=1, s='Hello') LOCATION a/b");
    // Caching ops
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED 'pool'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' WITH replication = 3");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' " + "with replication = -1");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) UNCACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' UNCACHED");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' CACHED IN 'pool'");
    ParsesOk("ALTER TABLE Foo ADD PARTITION (j=2) LOCATION 'a/b' CACHED IN 'pool' " + "with replication = 3");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) CACHED IN 'pool' LOCATION 'a/b'");
    ParserError("ALTER TABLE Foo ADD PARTITION (j=2) UNCACHED LOCATION 'a/b'");
    ParserError("ALTER TABLE Foo ADD IF EXISTS PARTITION (i=1, s='Hello')");
    ParserError("ALTER TABLE TestDb.Foo ADD (i=1, s='Hello')");
    ParserError("ALTER TABLE TestDb.Foo ADD (i=1)");
    ParserError("ALTER TABLE Foo (i=1)");
    ParserError("ALTER TABLE TestDb.Foo PARTITION (i=1)");
    ParserError("ALTER TABLE Foo ADD PARTITION");
    ParserError("ALTER TABLE TestDb.Foo ADD PARTITION ()");
    ParserError("ALTER Foo ADD PARTITION (i=1)");
    ParserError("ALTER TABLE ADD PARTITION (i=1)");
    ParserError("ALTER TABLE ADD");
    ParserError("ALTER TABLE DROP");
}
#end_block

#method_before
@Test
public void TestAlterTableDropPartition() {
    // PURGE is optional
    String[] purgeKw = { "PURGE", "" };
    for (String kw : purgeKw) {
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=1) %s", kw));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo DROP IF EXISTS " + "PARTITION (i=1, s='Hello') %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=NULL) %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=NULL, " + "j=2, k=NULL) %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=abc, " + "j=(5*8+10), k=!true and false) %s", kw));
        // Cannot use dynamic partition syntax
        ParserError(String.format("ALTER TABLE Foo DROP PARTITION (partcol) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP PARTITION (i=1, j) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP IF NOT EXISTS " + "PARTITION (i=1, s='Hello') %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP (i=1, s='Hello') %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE Foo (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo PARTITION (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP PARTITION %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP PARTITION () %s", kw));
        ParserError(String.format("ALTER Foo DROP PARTITION (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE DROP PARTITION (i=1) %s", kw));
    }
}
#method_after
@Test
public void TestAlterTableDropPartition() {
    // PURGE is optional
    String[] purgeKw = { "PURGE", "" };
    for (String kw : purgeKw) {
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=1) %s", kw));
        ParsesOk(String.format("ALTER TABLE TestDb.Foo DROP IF EXISTS " + "PARTITION (i=1, s='Hello') %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=NULL) %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=NULL, " + "j=2, k=NULL) %s", kw));
        ParsesOk(String.format("ALTER TABLE Foo DROP PARTITION (i=abc, " + "j=(5*8+10), k=!true and false) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP IF NOT EXISTS " + "PARTITION (i=1, s='Hello') %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP (i=1, s='Hello') %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE Foo (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo PARTITION (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE Foo DROP PARTITION %s", kw));
        ParserError(String.format("ALTER TABLE TestDb.Foo DROP PARTITION () %s", kw));
        ParserError(String.format("ALTER Foo DROP PARTITION (i=1) %s", kw));
        ParserError(String.format("ALTER TABLE DROP PARTITION (i=1) %s", kw));
    }
}
#end_block

#method_before
@Test
public void TestAlterTableSet() {
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("ALTER TABLE Foo SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE TestDb.Foo SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE TestDb.Foo PARTITION (a=1) SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE Foo PARTITION (s='str') SET FILEFORMAT " + format);
        ParserError("ALTER TABLE TestDb.Foo PARTITION (i=5) SET " + format);
        ParserError("ALTER TABLE TestDb.Foo SET " + format);
        ParserError("ALTER TABLE TestDb.Foo " + format);
    }
    ParserError("ALTER TABLE TestDb.Foo SET FILEFORMAT");
    ParsesOk("ALTER TABLE Foo SET LOCATION '/a/b/c'");
    ParsesOk("ALTER TABLE TestDb.Foo SET LOCATION '/a/b/c'");
    ParsesOk("ALTER TABLE Foo PARTITION (i=1,s='str') SET LOCATION '/a/i=1/s=str'");
    ParsesOk("ALTER TABLE Foo PARTITION (s='str') SET LOCATION '/a/i=1/s=str'");
    ParserError("ALTER TABLE Foo PARTITION (s) SET LOCATION '/a'");
    ParserError("ALTER TABLE Foo PARTITION () SET LOCATION '/a'");
    ParserError("ALTER TABLE Foo PARTITION ('str') SET FILEFORMAT TEXTFILE");
    ParserError("ALTER TABLE Foo PARTITION (a=1, 5) SET FILEFORMAT TEXTFILE");
    ParserError("ALTER TABLE Foo PARTITION () SET FILEFORMAT PARQUETFILE");
    ParserError("ALTER TABLE Foo PARTITION (,) SET FILEFORMAT PARQUET");
    ParserError("ALTER TABLE Foo PARTITION (a=1) SET FILEFORMAT");
    ParserError("ALTER TABLE Foo PARTITION (a=1) SET LOCATION");
    ParserError("ALTER TABLE TestDb.Foo SET LOCATION abc");
    ParserError("ALTER TABLE TestDb.Foo SET LOCATION");
    ParserError("ALTER TABLE TestDb.Foo SET");
    String[] tblPropTypes = { "TBLPROPERTIES", "SERDEPROPERTIES" };
    String[] partClauses = { "", "PARTITION(k1=10, k2=20)" };
    for (String propType : tblPropTypes) {
        for (String part : partClauses) {
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('a'='b')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('abc'='123')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('abc'='123', 'a'='1')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('a'='1', 'b'='2', 'c'='3')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ( )", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a', 'b')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a'='b',)", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a'=b)", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s (a='b')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s (a=b)", part, propType));
        }
    }
    // Test SET COLUMN STATS.
    ParsesOk("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'='10')");
    ParsesOk("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'='10','maxSize'='20')");
    ParsesOk("ALTER TABLE TestDb.Foo SET COLUMN STATS col ('avgSize'='20')");
    ParserError("ALTER TABLE SET COLUMN STATS col ('numDVs'='10'");
    ParserError("ALTER TABLE Foo SET COLUMN STATS ('numDVs'='10'");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col ()");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col (numDVs='10')");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'=10)");
    ParserError("ALTER TABLE Foo PARTITION (p=1) SET COLUMN STATS col ('avgSize'='20')");
    for (String cacheClause : Lists.newArrayList("UNCACHED", "CACHED in 'pool'", "CACHED in 'pool' with replication = 4")) {
        ParsesOk("ALTER TABLE Foo SET " + cacheClause);
        ParsesOk("ALTER TABLE Foo PARTITION(j=0) SET " + cacheClause);
        ParserError("ALTER TABLE Foo PARTITION(j=0) " + cacheClause);
    }
}
#method_after
@Test
public void TestAlterTableSet() {
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("ALTER TABLE Foo SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE TestDb.Foo SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE TestDb.Foo PARTITION (a=1) SET FILEFORMAT " + format);
        ParsesOk("ALTER TABLE Foo PARTITION (s='str') SET FILEFORMAT " + format);
        ParserError("ALTER TABLE TestDb.Foo PARTITION (i=5) SET " + format);
        ParserError("ALTER TABLE TestDb.Foo SET " + format);
        ParserError("ALTER TABLE TestDb.Foo " + format);
    }
    ParserError("ALTER TABLE TestDb.Foo SET FILEFORMAT");
    ParsesOk("ALTER TABLE Foo SET LOCATION '/a/b/c'");
    ParsesOk("ALTER TABLE TestDb.Foo SET LOCATION '/a/b/c'");
    ParsesOk("ALTER TABLE Foo PARTITION (i=1,s='str') SET LOCATION '/a/i=1/s=str'");
    ParsesOk("ALTER TABLE Foo PARTITION (s='str') SET LOCATION '/a/i=1/s=str'");
    ParserError("ALTER TABLE Foo PARTITION () SET LOCATION '/a'");
    ParserError("ALTER TABLE Foo PARTITION () SET FILEFORMAT PARQUETFILE");
    ParserError("ALTER TABLE Foo PARTITION (,) SET FILEFORMAT PARQUET");
    ParserError("ALTER TABLE Foo PARTITION (a=1) SET FILEFORMAT");
    ParserError("ALTER TABLE Foo PARTITION (a=1) SET LOCATION");
    ParserError("ALTER TABLE TestDb.Foo SET LOCATION abc");
    ParserError("ALTER TABLE TestDb.Foo SET LOCATION");
    ParserError("ALTER TABLE TestDb.Foo SET");
    String[] tblPropTypes = { "TBLPROPERTIES", "SERDEPROPERTIES" };
    String[] partClauses = { "", "PARTITION(k1=10, k2=20)" };
    for (String propType : tblPropTypes) {
        for (String part : partClauses) {
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('a'='b')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('abc'='123')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('abc'='123', 'a'='1')", part, propType));
            ParsesOk(String.format("ALTER TABLE Foo %s SET %s ('a'='1', 'b'='2', 'c'='3')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ( )", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a', 'b')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a'='b',)", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s ('a'=b)", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s (a='b')", part, propType));
            ParserError(String.format("ALTER TABLE Foo %s SET %s (a=b)", part, propType));
        }
    }
    // Test SET COLUMN STATS.
    ParsesOk("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'='10')");
    ParsesOk("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'='10','maxSize'='20')");
    ParsesOk("ALTER TABLE TestDb.Foo SET COLUMN STATS col ('avgSize'='20')");
    ParserError("ALTER TABLE SET COLUMN STATS col ('numDVs'='10'");
    ParserError("ALTER TABLE Foo SET COLUMN STATS ('numDVs'='10'");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col ()");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col (numDVs='10')");
    ParserError("ALTER TABLE Foo SET COLUMN STATS col ('numDVs'=10)");
    ParserError("ALTER TABLE Foo PARTITION (p=1) SET COLUMN STATS col ('avgSize'='20')");
    for (String cacheClause : Lists.newArrayList("UNCACHED", "CACHED in 'pool'", "CACHED in 'pool' with replication = 4")) {
        ParsesOk("ALTER TABLE Foo SET " + cacheClause);
        ParsesOk("ALTER TABLE Foo PARTITION(j=0) SET " + cacheClause);
        ParserError("ALTER TABLE Foo PARTITION(j=0) " + cacheClause);
    }
}
#end_block

#method_before
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo (i int,)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, " + "HASH(a) INTO 2 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int, k int) DISTRIBUTE BY HASH INTO 4 BUCKETS," + " HASH(k) INTO 4 BUCKETS");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo DISTRIBUTE BY HASH INTO 4 BUCKETS");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) DISTRIBUTE BY " + "HASH (a) INTO 3 BUCKETS, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 1 + 1 < VALUES) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY HASH (a) INTO 4 BUCKETS, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) INTO 3 BUCKETS");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUES = 10) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 10 < VALUE < 20) STORED AS KUDU");
    // Column options for Kudu tables
    String[] encodings = { "encoding auto_encoding", "encoding plain_encoding", "encoding prefix_encoding", "encoding group_varint", "encoding rle", "encoding dict_encoding", "encoding bit_shuffle", "encoding unknown", "" };
    String[] compression = { "compression default_compression", "compression no_compression", "compression snappy", "compression lz4", "compression zlib", "compression unknown", "" };
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (String enc : encodings) {
        for (String comp : compression) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", nul, enc, comp, def, block));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, def, block, nul));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", comp, def, block, nul, enc));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", def, block, nul, enc, comp));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", block, nul, enc, comp, def));
                    }
                }
            }
        }
    }
    // Column option is specified multiple times for the same column
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY ENCODING RLE ENCODING PLAIN) " + "STORED AS KUDU");
    // Constant expr used in DEFAULT
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b int DEFAULT 1+1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b float DEFAULT cast(1.1 as float)) " + "STORED AS KUDU");
    // Non-literal value used in BLOCK_SIZE
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY, b int BLOCK_SIZE 1+1) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY BLOCK_SIZE -1) STORED AS KUDU");
}
#method_after
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo (i int,)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, " + "HASH(a) INTO 2 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int, k int) DISTRIBUTE BY HASH INTO 4 BUCKETS," + " HASH(k) INTO 4 BUCKETS");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo DISTRIBUTE BY HASH INTO 4 BUCKETS");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) DISTRIBUTE BY " + "HASH (a) INTO 3 BUCKETS, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 1 + 1 < VALUES) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY HASH (a) INTO 4 BUCKETS, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) INTO 3 BUCKETS");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUES = 10) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 10 < VALUE < 20) STORED AS KUDU");
    // Column options for Kudu tables
    String[] encodings = { "encoding auto_encoding", "encoding plain_encoding", "encoding prefix_encoding", "encoding group_varint", "encoding rle", "encoding dict_encoding", "encoding bit_shuffle", "encoding unknown", "" };
    String[] compression = { "compression default_compression", "compression no_compression", "compression snappy", "compression lz4", "compression zlib", "compression unknown", "" };
    String[] nullability = { "not null", "null", "" };
    String[] defaultVal = { "default 10", "" };
    String[] blockSize = { "block_size 4096", "" };
    for (String enc : encodings) {
        for (String comp : compression) {
            for (String nul : nullability) {
                for (String def : defaultVal) {
                    for (String block : blockSize) {
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", nul, enc, comp, def, block));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", block, nul, enc, comp, def));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", def, block, nul, enc, comp));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", comp, def, block, nul, enc));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, def, block, nul));
                        ParsesOk(String.format("CREATE TABLE Foo (i int PRIMARY KEY " + "%s %s %s %s %s) STORED AS KUDU", enc, comp, block, def, nul));
                    }
                }
            }
        }
    }
    // Column option is specified multiple times for the same column
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY ENCODING RLE ENCODING PLAIN) " + "STORED AS KUDU");
    // Constant expr used in DEFAULT
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b int DEFAULT 1+1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo(a int PRIMARY KEY, b float DEFAULT cast(1.1 as float)) " + "STORED AS KUDU");
    // Non-literal value used in BLOCK_SIZE
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY, b int BLOCK_SIZE 1+1) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo(a int PRIMARY KEY BLOCK_SIZE -1) STORED AS KUDU");
}
#end_block

#method_before
@Test
public void TestGetErrorMsg() {
    // missing select
    ParserError("c, b, c from t", "Syntax error in line 1:\n" + "c, b, c from t\n" + "^\n" + "Encountered: IDENTIFIER\n" + "Expected: ALTER, COMPUTE, CREATE, DELETE, DESCRIBE, DROP, EXPLAIN, GRANT, " + "INSERT, INVALIDATE, LOAD, REFRESH, REVOKE, SELECT, SET, SHOW, TRUNCATE, " + "UPDATE, UPSERT, USE, VALUES, WITH\n");
    // missing select list
    ParserError("select from t", "Syntax error in line 1:\n" + "select from t\n" + "       ^\n" + "Encountered: FROM\n" + "Expected: ALL, CASE, CAST, DISTINCT, EXISTS, " + "FALSE, IF, INTERVAL, NOT, NULL, " + "STRAIGHT_JOIN, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing from
    ParserError("select c, b, c where a = 5", "Syntax error in line 1:\n" + "select c, b, c where a = 5\n" + "               ^\n" + "Encountered: WHERE\n" + "Expected: AND, AS, BETWEEN, DIV, FROM, ILIKE, IN, IREGEXP, IS, LIKE, LIMIT, NOT, OR, " + "ORDER, REGEXP, RLIKE, UNION, COMMA, IDENTIFIER\n");
    // missing table list
    ParserError("select c, b, c from where a = 5", "Syntax error in line 1:\n" + "select c, b, c from where a = 5\n" + "                    ^\n" + "Encountered: WHERE\n" + "Expected: IDENTIFIER\n");
    // missing predicate in where clause (no group by)
    ParserError("select c, b, c from t where", "Syntax error in line 1:\n" + "select c, b, c from t where\n" + "                           ^\n" + "Encountered: EOF\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing predicate in where clause (group by)
    ParserError("select c, b, c from t where group by a, b", "Syntax error in line 1:\n" + "select c, b, c from t where group by a, b\n" + "                            ^\n" + "Encountered: GROUP\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // unmatched string literal starting with "
    ParserError("select c, \"b, c from t", "Unmatched string literal in line 1:\n" + "select c, \"b, c from t\n" + "           ^\n");
    // unmatched string literal starting with '
    ParserError("select c, 'b, c from t", "Unmatched string literal in line 1:\n" + "select c, 'b, c from t\n" + "           ^\n");
    // test placement of error indicator ^ on queries with multiple lines
    ParserError("select (i + 5)(1 - i) from t", "Syntax error in line 1:\n" + "select (i + 5)(1 - i) from t\n" + "              ^\n" + "Encountered: (\n" + "Expected:");
    ParserError("select (i + 5)\n(1 - i) from t", "Syntax error in line 2:\n" + "(1 - i) from t\n" + "^\n" + "Encountered: (\n" + "Expected");
    ParserError("select (i + 5)\n(1 - i)\nfrom t", "Syntax error in line 2:\n" + "(1 - i)\n" + "^\n" + "Encountered: (\n" + "Expected");
    // Long line: error in the middle
    ParserError("select c, b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "... b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c,...\n" + "                             ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the start
    ParserError("select a a a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "select a a a, b, c,c,c,c,c,c,c,c,c,c,c,...\n" + "           ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the end
    ParserError("select a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t", "Syntax error in line 1:\n" + "...c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t\n" + "                             ^\n" + "Encountered: COMMA\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // Parsing identifiers that have different names printed as EXPECTED
    ParserError("DROP DATA SRC foo", "Syntax error in line 1:\n" + "DROP DATA SRC foo\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCE\n");
    ParserError("SHOW DATA SRCS", "Syntax error in line 1:\n" + "SHOW DATA SRCS\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCES\n");
    ParserError("USE ` `", "Syntax error in line 1:\n" + "USE ` `\n" + "    ^\n" + "Encountered: EMPTY IDENTIFIER\n" + "Expected: IDENTIFIER\n");
    // Expecting = token
    ParserError("SET foo", "Syntax error in line 1:\n" + "SET foo\n" + "       ^\n" + "Encountered: EOF\n" + "Expected: =\n");
}
#method_after
@Test
public void TestGetErrorMsg() {
    // missing select
    ParserError("c, b, c from t", "Syntax error in line 1:\n" + "c, b, c from t\n" + "^\n" + "Encountered: IDENTIFIER\n" + "Expected: ALTER, COMPUTE, CREATE, DELETE, DESCRIBE, DROP, EXPLAIN, GRANT, " + "INSERT, INVALIDATE, LOAD, REFRESH, REVOKE, SELECT, SET, SHOW, TRUNCATE, " + "UPDATE, UPSERT, USE, VALUES, WITH\n");
    // missing select list
    ParserError("select from t", "Syntax error in line 1:\n" + "select from t\n" + "       ^\n" + "Encountered: FROM\n" + "Expected: ALL, CASE, CAST, DEFAULT, DISTINCT, EXISTS, " + "FALSE, IF, INTERVAL, NOT, NULL, " + "STRAIGHT_JOIN, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing from
    ParserError("select c, b, c where a = 5", "Syntax error in line 1:\n" + "select c, b, c where a = 5\n" + "               ^\n" + "Encountered: WHERE\n" + "Expected: AND, AS, BETWEEN, DEFAULT, DIV, FROM, ILIKE, IN, IREGEXP, IS, LIKE, " + "LIMIT, NOT, OR, ORDER, REGEXP, RLIKE, UNION, COMMA, IDENTIFIER\n");
    // missing table list
    ParserError("select c, b, c from where a = 5", "Syntax error in line 1:\n" + "select c, b, c from where a = 5\n" + "                    ^\n" + "Encountered: WHERE\n" + "Expected: DEFAULT, IDENTIFIER\n");
    // missing predicate in where clause (no group by)
    ParserError("select c, b, c from t where", "Syntax error in line 1:\n" + "select c, b, c from t where\n" + "                           ^\n" + "Encountered: EOF\n" + "Expected: CASE, CAST, DEFAULT, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing predicate in where clause (group by)
    ParserError("select c, b, c from t where group by a, b", "Syntax error in line 1:\n" + "select c, b, c from t where group by a, b\n" + "                            ^\n" + "Encountered: GROUP\n" + "Expected: CASE, CAST, DEFAULT, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // unmatched string literal starting with "
    ParserError("select c, \"b, c from t", "Unmatched string literal in line 1:\n" + "select c, \"b, c from t\n" + "           ^\n");
    // unmatched string literal starting with '
    ParserError("select c, 'b, c from t", "Unmatched string literal in line 1:\n" + "select c, 'b, c from t\n" + "           ^\n");
    // test placement of error indicator ^ on queries with multiple lines
    ParserError("select (i + 5)(1 - i) from t", "Syntax error in line 1:\n" + "select (i + 5)(1 - i) from t\n" + "              ^\n" + "Encountered: (\n" + "Expected:");
    ParserError("select (i + 5)\n(1 - i) from t", "Syntax error in line 2:\n" + "(1 - i) from t\n" + "^\n" + "Encountered: (\n" + "Expected");
    ParserError("select (i + 5)\n(1 - i)\nfrom t", "Syntax error in line 2:\n" + "(1 - i)\n" + "^\n" + "Encountered: (\n" + "Expected");
    // Long line: error in the middle
    ParserError("select c, b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "... b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c,...\n" + "                             ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the start
    ParserError("select a a a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "select a a a, b, c,c,c,c,c,c,c,c,c,c,c,...\n" + "           ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the end
    ParserError("select a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t", "Syntax error in line 1:\n" + "...c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t\n" + "                             ^\n" + "Encountered: COMMA\n" + "Expected: CASE, CAST, DEFAULT, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // Parsing identifiers that have different names printed as EXPECTED
    ParserError("DROP DATA SRC foo", "Syntax error in line 1:\n" + "DROP DATA SRC foo\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCE\n");
    ParserError("SHOW DATA SRCS", "Syntax error in line 1:\n" + "SHOW DATA SRCS\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCES\n");
    ParserError("USE ` `", "Syntax error in line 1:\n" + "USE ` `\n" + "    ^\n" + "Encountered: EMPTY IDENTIFIER\n" + "Expected: DEFAULT, IDENTIFIER\n");
    // Expecting = token
    ParserError("SET foo", "Syntax error in line 1:\n" + "SET foo\n" + "       ^\n" + "Encountered: EOF\n" + "Expected: =\n");
}
#end_block

#method_before
@Test
public void TestComputeStatsIncremental() {
    ParsesOk("COMPUTE INCREMENTAL STATS functional.alltypes");
    ParserError("COMPUTE INCREMENTAL functional.alltypes");
    ParsesOk("COMPUTE INCREMENTAL STATS functional.alltypes PARTITION(month=10, year=2010)");
    // No dynamic partition specs
    ParserError("COMPUTE INCREMENTAL STATS functional.alltypes PARTITION(month, year)");
    ParserError("COMPUTE INCREMENTAL STATS");
    ParsesOk("DROP INCREMENTAL STATS functional.alltypes PARTITION(month=10, year=2010)");
    ParserError("DROP INCREMENTAL STATS functional.alltypes PARTITION(month, year)");
    ParserError("DROP INCREMENTAL STATS functional.alltypes");
}
#method_after
@Test
public void TestComputeStatsIncremental() {
    ParsesOk("COMPUTE INCREMENTAL STATS functional.alltypes");
    ParserError("COMPUTE INCREMENTAL functional.alltypes");
    ParsesOk("COMPUTE INCREMENTAL STATS functional.alltypes PARTITION(month=10, year=2010)");
    ParserError("COMPUTE INCREMENTAL STATS");
    ParsesOk("DROP INCREMENTAL STATS functional.alltypes PARTITION(month=10, year=2010)");
    ParserError("DROP INCREMENTAL STATS functional.alltypes");
}
#end_block

#method_before
public void initClients(int numClients, int initialCnxnTimeoutSec) {
    Preconditions.checkState(clientPool_.size() == 0);
    if (numClients > 0) {
        addClients(1, initialCnxnTimeoutSec);
        addClients(numClients - 1, 0);
    }
}
#method_after
public void initClients(int numClients, int initialCnxnTimeoutSec) {
    Preconditions.checkState(clientPool_.size() == 0);
    if (numClients > 0) {
        clientPool_.add(new MetaStoreClient(hiveConf_, initialCnxnTimeoutSec));
        for (int i = 0; i < numClients - 1; ++i) {
            clientPool_.add(new MetaStoreClient(hiveConf_, 0));
        }
    }
}
#end_block

#method_before
@Test
public void TestUnsupportedTypes() {
    // Select supported types from a table with mixed supported/unsupported types.
    AnalyzesOk("select int_col, str_col, bigint_col from functional.unsupported_types");
    // Select supported types from a table with mixed supported/unsupported types.
    AnalyzesOk("select int_col, str_col, bigint_col from functional.unsupported_types");
    // Unsupported type binary.
    AnalysisError("select bin_col from functional.unsupported_types", "Unsupported type 'BINARY' in 'bin_col'.");
    // Unsupported type binary in a star expansion.
    AnalysisError("select * from functional.unsupported_types", "Unsupported type 'DATE' in 'functional.unsupported_types.date_col'.");
    // Mixed supported/unsupported types.
    AnalysisError("select int_col, str_col, bin_col " + "from functional.unsupported_types", "Unsupported type 'BINARY' in 'bin_col'.");
    AnalysisError("create table tmp as select * from functional.unsupported_types", "Unsupported type 'BINARY' in 'functional.unsupported_types.bin_col'.");
    // Unsupported type in the target insert table.
    AnalysisError("insert into functional.unsupported_types " + "values(null, null, null, null, null, null)", "Unable to INSERT into target table (functional.unsupported_types) because " + "the column 'date_col' has an unsupported type 'DATE'");
    // Unsupported partition-column type.
    AnalysisError("select * from functional.unsupported_partition_types", "Failed to load metadata for table: 'functional.unsupported_partition_types'");
    // Try with hbase
    AnalyzesOk("describe functional_hbase.allcomplextypes");
    for (ScalarType t : Type.getUnsupportedTypes()) {
        // Create/Alter table.
        AnalysisError(String.format("create table new_table (new_col %s)", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("create table new_table (new_col int) PARTITIONED BY (p_col %s)", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("alter table functional.alltypes add columns (new_col %s)", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("alter table functional.alltypes change column int_col new_col %s", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        // UDFs.
        final String udfSuffix = " LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'";
        AnalysisError(String.format("create function foo(VARCHAR(5)) RETURNS %s" + udfSuffix, t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("create function foo(%s) RETURNS int" + udfSuffix, t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        // UDAs.
        final String udaSuffix = " LOCATION '/test-warehouse/libTestUdas.so' " + "UPDATE_FN='AggUpdate'";
        AnalysisError(String.format("create aggregate function foo(string, double) " + "RETURNS %s" + udaSuffix, t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("create aggregate function foo(%s, double) " + "RETURNS int" + udaSuffix, t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        // Cast,
        AnalysisError(String.format("select cast('abc' as %s)", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
    }
}
#method_after
@Test
public void TestUnsupportedTypes() {
    // Select supported types from a table with mixed supported/unsupported types.
    AnalyzesOk("select int_col, str_col, bigint_col from functional.unsupported_types");
    // Select supported types from a table with mixed supported/unsupported types.
    AnalyzesOk("select int_col, str_col, bigint_col from functional.unsupported_types");
    // Unsupported type binary.
    AnalysisError("select bin_col from functional.unsupported_types", "Unsupported type 'BINARY' in 'bin_col'.");
    // Unsupported type date in a star expansion.
    AnalysisError("select * from functional.unsupported_types", "Unsupported type 'DATE' in 'functional.unsupported_types.date_col'.");
    // Mixed supported/unsupported types.
    AnalysisError("select int_col, str_col, bin_col " + "from functional.unsupported_types", "Unsupported type 'BINARY' in 'bin_col'.");
    AnalysisError("create table tmp as select * from functional.unsupported_types", "Unsupported type 'DATE' in 'functional.unsupported_types.date_col'.");
    // Unsupported type in the target insert table.
    AnalysisError("insert into functional.unsupported_types " + "values(null, null, null, null, null, null)", "Unable to INSERT into target table (functional.unsupported_types) because " + "the column 'date_col' has an unsupported type 'DATE'");
    // Unsupported partition-column type.
    AnalysisError("select * from functional.unsupported_partition_types", "Failed to load metadata for table: 'functional.unsupported_partition_types'");
    // Try with hbase
    AnalyzesOk("describe functional_hbase.allcomplextypes");
    for (ScalarType t : Type.getUnsupportedTypes()) {
        // Create/Alter table.
        AnalysisError(String.format("create table new_table (new_col %s)", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("create table new_table (new_col int) PARTITIONED BY (p_col %s)", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("alter table functional.alltypes add columns (new_col %s)", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("alter table functional.alltypes change column int_col new_col %s", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        // UDFs.
        final String udfSuffix = " LOCATION '/test-warehouse/libTestUdfs.so' " + "SYMBOL='_Z8IdentityPN10impala_udf15FunctionContextERKNS_10BooleanValE'";
        AnalysisError(String.format("create function foo(VARCHAR(5)) RETURNS %s" + udfSuffix, t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("create function foo(%s) RETURNS int" + udfSuffix, t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        // UDAs.
        final String udaSuffix = " LOCATION '/test-warehouse/libTestUdas.so' " + "UPDATE_FN='AggUpdate'";
        AnalysisError(String.format("create aggregate function foo(string, double) " + "RETURNS %s" + udaSuffix, t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        AnalysisError(String.format("create aggregate function foo(%s, double) " + "RETURNS int" + udaSuffix, t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
        // Cast,
        AnalysisError(String.format("select cast('abc' as %s)", t.toSql()), String.format("Unsupported data type: %s", t.toSql()));
    }
}
#end_block

#method_before
@Test
public void TestExplain() {
    // Analysis error from explain insert: too many partitioning columns.
    AnalysisError("explain insert into table functional.alltypessmall " + "partition (year=2009, month=4, year=10)" + "select id, bool_col, tinyint_col, smallint_col, int_col, bigint_col, " + "float_col, double_col, date_string_col, string_col, timestamp_col " + "from functional.alltypes", "Duplicate column 'year' in partition clause");
    // Analysis error from explain query
    AnalysisError("explain " + "select id from (select id+2 from functional_hbase.alltypessmall) a", "Could not resolve column/field reference: 'id'");
    // Positive test for explain query
    AnalyzesOk("explain select * from functional.AllTypes");
    // Positive test for explain insert
    AnalyzesOk("explain insert into table functional.alltypessmall " + "partition (year=2009, month=4)" + "select id, bool_col, tinyint_col, smallint_col, int_col, int_col, " + "float_col, float_col, date_string_col, string_col, timestamp_col " + "from functional.alltypes");
}
#method_after
@Test
public void TestExplain() {
    // Analysis error from explain insert: too many partitioning columns.
    AnalysisError("explain insert into table functional.alltypessmall " + "partition (year=2009, month=4, year=10)" + "select id, bool_col, tinyint_col, smallint_col, int_col, bigint_col, " + "float_col, double_col, date_string_col, string_col, timestamp_col " + "from functional.alltypes", "Duplicate column 'year' in partition clause");
    // Analysis error from explain query
    AnalysisError("explain " + "select id from (select id+2 from functional_hbase.alltypessmall) a", "Could not resolve column/field reference: 'id'");
    // Analysis error from explain upsert
    AnalysisError("explain upsert into table functional.alltypes select * from " + "functional.alltypes", "UPSERT is only supported for Kudu tables");
    // Positive test for explain query
    AnalyzesOk("explain select * from functional.AllTypes");
    // Positive test for explain insert
    AnalyzesOk("explain insert into table functional.alltypessmall " + "partition (year=2009, month=4)" + "select id, bool_col, tinyint_col, smallint_col, int_col, int_col, " + "float_col, float_col, date_string_col, string_col, timestamp_col " + "from functional.alltypes");
    // Positive test for explain upsert
    AnalyzesOk("explain upsert into table functional_kudu.testtbl select * from " + "functional_kudu.testtbl");
}
#end_block

#method_before
public static LiteralExpr create(Expr constExpr, TQueryCtx queryCtx) throws AnalysisException {
    Preconditions.checkState(constExpr.isConstant());
    Preconditions.checkState(constExpr.getType().isValid());
    if (constExpr instanceof LiteralExpr)
        return (LiteralExpr) constExpr;
    TColumnValue val = null;
    try {
        val = FeSupport.EvalConstExpr(constExpr, queryCtx);
    } catch (InternalException e) {
        throw new AnalysisException(String.format("Failed to evaluate expr '%s'", constExpr.toSql()), e);
    }
    LiteralExpr result = null;
    switch(constExpr.getType().getPrimitiveType()) {
        case NULL_TYPE:
            result = new NullLiteral();
            break;
        case BOOLEAN:
            if (val.isBool_val())
                result = new BoolLiteral(val.bool_val);
            break;
        case TINYINT:
            if (val.isSetByte_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.byte_val));
            }
            break;
        case SMALLINT:
            if (val.isSetShort_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.short_val));
            }
            break;
        case INT:
            if (val.isSetInt_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.int_val));
            }
            break;
        case BIGINT:
            if (val.isSetLong_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.long_val));
            }
            break;
        case FLOAT:
        case DOUBLE:
            if (val.isSetDouble_val()) {
                if (Double.isNaN(val.double_val) || Double.isInfinite(val.double_val)) {
                    // NumericLiteral cannot represent NaN or infinity.
                    return null;
                }
                result = new NumericLiteral(new BigDecimal(val.double_val), constExpr.getType());
            }
            break;
        case DECIMAL:
            if (val.isSetString_val()) {
                result = new NumericLiteral(new BigDecimal(val.string_val), constExpr.getType());
            }
            break;
        case STRING:
        case VARCHAR:
        case CHAR:
            if (val.isSetString_val())
                result = new StringLiteral(val.string_val);
            break;
        case DATE:
        case DATETIME:
        case TIMESTAMP:
            return null;
        default:
            Preconditions.checkState(false, String.format("Literals of type '%s' not supported.", constExpr.getType().toSql()));
    }
    // None of the fields in the thrift struct were set indicating a NULL.
    if (result == null)
        result = new NullLiteral();
    result.analyze(null);
    return (LiteralExpr) result;
}
#method_after
public static LiteralExpr create(Expr constExpr, TQueryCtx queryCtx) throws AnalysisException {
    Preconditions.checkState(constExpr.isConstant());
    Preconditions.checkState(constExpr.getType().isValid());
    if (constExpr instanceof LiteralExpr)
        return (LiteralExpr) constExpr;
    TColumnValue val = null;
    try {
        val = FeSupport.EvalConstExpr(constExpr, queryCtx);
    } catch (InternalException e) {
        throw new AnalysisException(String.format("Failed to evaluate expr '%s'", constExpr.toSql()), e);
    }
    LiteralExpr result = null;
    switch(constExpr.getType().getPrimitiveType()) {
        case NULL_TYPE:
            result = new NullLiteral();
            break;
        case BOOLEAN:
            if (val.isSetBool_val())
                result = new BoolLiteral(val.bool_val);
            break;
        case TINYINT:
            if (val.isSetByte_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.byte_val));
            }
            break;
        case SMALLINT:
            if (val.isSetShort_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.short_val));
            }
            break;
        case INT:
            if (val.isSetInt_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.int_val));
            }
            break;
        case BIGINT:
            if (val.isSetLong_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.long_val));
            }
            break;
        case FLOAT:
        case DOUBLE:
            if (val.isSetDouble_val()) {
                // A NumericLiteral cannot represent NaN, infinity or negative zero.
                if (!NumericLiteral.isValidLiteral(val.double_val))
                    return null;
                result = new NumericLiteral(new BigDecimal(val.double_val), constExpr.getType());
            }
            break;
        case DECIMAL:
            if (val.isSetString_val()) {
                result = new NumericLiteral(new BigDecimal(val.string_val), constExpr.getType());
            }
            break;
        case STRING:
        case VARCHAR:
        case CHAR:
            if (val.isSetString_val())
                result = new StringLiteral(val.string_val);
            break;
        case DATE:
        case DATETIME:
        case TIMESTAMP:
            return null;
        default:
            Preconditions.checkState(false, String.format("Literals of type '%s' not supported.", constExpr.getType().toSql()));
    }
    // None of the fields in the thrift struct were set indicating a NULL.
    if (result == null)
        result = new NullLiteral();
    result.analyze(null);
    return (LiteralExpr) result;
}
#end_block

#method_before
@Test(timeout = 100000)
public void testAlterTableNonCoveringRange() throws Exception {
    String tableName = name.getMethodName() + System.currentTimeMillis();
    syncClient.createTable(tableName, basicSchema, getBasicTableOptionsWithNonCoveredRange());
    KuduTable table = syncClient.openTable(tableName);
    KuduSession session = syncClient.newSession();
    AlterTableOptions ato = new AlterTableOptions();
    PartialRow bLowerBound = schema.newPartialRow();
    bLowerBound.addInt("key", 300);
    PartialRow bUpperBound = schema.newPartialRow();
    bUpperBound.addInt("key", 400);
    ato.addRangePartition(bLowerBound, bUpperBound);
    syncClient.alterTable(tableName, ato);
    Insert insert = createBasicSchemaInsert(table, 301);
    session.apply(insert);
    List<LocatedTablet> tablets;
    // all tablets
    tablets = table.getTabletsLocations(getKeyInBytes(300), null, 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(300), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(400), tablets.get(0).getPartition().getPartitionKeyEnd());
    insert = createBasicSchemaInsert(table, 201);
    session.apply(insert);
    ato = new AlterTableOptions();
    bLowerBound = schema.newPartialRow();
    bLowerBound.addInt("key", 200);
    bUpperBound = schema.newPartialRow();
    bUpperBound.addInt("key", 300);
    ato.dropRangePartition(bLowerBound, bUpperBound);
    syncClient.alterTable(tableName, ato);
    insert = createBasicSchemaInsert(table, 202);
    try {
        session.apply(insert);
        fail("Should get a non-recoverable");
    } catch (NonCoveredRangeException e) {
    // Expected.
    }
}
#method_after
@Test(timeout = 100000)
public void testAlterTableNonCoveringRange() throws Exception {
    String tableName = name.getMethodName() + System.currentTimeMillis();
    syncClient.createTable(tableName, basicSchema, getBasicTableOptionsWithNonCoveredRange());
    KuduTable table = syncClient.openTable(tableName);
    KuduSession session = syncClient.newSession();
    AlterTableOptions ato = new AlterTableOptions();
    PartialRow bLowerBound = schema.newPartialRow();
    bLowerBound.addInt("key", 300);
    PartialRow bUpperBound = schema.newPartialRow();
    bUpperBound.addInt("key", 400);
    ato.addRangePartition(bLowerBound, bUpperBound);
    syncClient.alterTable(tableName, ato);
    Insert insert = createBasicSchemaInsert(table, 301);
    session.apply(insert);
    List<LocatedTablet> tablets;
    // all tablets
    tablets = table.getTabletsLocations(getKeyInBytes(300), null, 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(300), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(400), tablets.get(0).getPartition().getPartitionKeyEnd());
    insert = createBasicSchemaInsert(table, 201);
    session.apply(insert);
    ato = new AlterTableOptions();
    bLowerBound = schema.newPartialRow();
    bLowerBound.addInt("key", 200);
    bUpperBound = schema.newPartialRow();
    bUpperBound.addInt("key", 300);
    ato.dropRangePartition(bLowerBound, bUpperBound);
    syncClient.alterTable(tableName, ato);
    insert = createBasicSchemaInsert(table, 202);
    OperationResponse response = session.apply(insert);
    assertTrue(response.hasRowError());
    assertTrue(response.getRowError().getErrorStatus().isNotFound());
}
#end_block

#method_before
@Override
public boolean equals(Object o) {
    if (this == o)
        return true;
    if (o == null || getClass() != o.getClass())
        return false;
    Partition partition = (Partition) o;
    return Arrays.equals(partitionKeyStart, partition.partitionKeyStart) && Arrays.equals(partitionKeyEnd, partition.partitionKeyEnd);
}
#method_after
@Override
public boolean equals(Object o) {
    if (this == o) {
        return true;
    }
    if (o == null || getClass() != o.getClass()) {
        return false;
    }
    Partition partition = (Partition) o;
    return Arrays.equals(partitionKeyStart, partition.partitionKeyStart) && Arrays.equals(partitionKeyEnd, partition.partitionKeyEnd);
}
#end_block

#method_before
String formatRangePartition(KuduTable table) {
    Schema schema = table.getSchema();
    PartitionSchema partitionSchema = table.getPartitionSchema();
    PartitionSchema.RangeSchema rangeSchema = partitionSchema.getRangeSchema();
    if (rangeSchema.getColumns().isEmpty()) {
        return "";
    }
    if (rangeKeyStart.length == 0 && rangeKeyEnd.length == 0) {
        return "UNBOUNDED";
    }
    List<Integer> idxs = new ArrayList<>();
    for (int id : partitionSchema.getRangeSchema().getColumns()) {
        idxs.add(schema.getColumnIndex(id));
    }
    int numColumns = rangeSchema.getColumns().size();
    StringBuilder sb = new StringBuilder();
    if (rangeKeyEnd.length == 0) {
        sb.append("VALUES >= ");
        if (numColumns > 1)
            sb.append('(');
        KeyEncoder.decodeRangePartitionKey(schema, partitionSchema, rangeKeyStart).appendShortDebugString(idxs, sb);
        if (numColumns > 1)
            sb.append(')');
    } else if (rangeKeyStart.length == 0) {
        sb.append("VALUES < ");
        if (numColumns > 1)
            sb.append('(');
        KeyEncoder.decodeRangePartitionKey(schema, partitionSchema, rangeKeyEnd).appendShortDebugString(idxs, sb);
        if (numColumns > 1)
            sb.append(')');
    } else {
        PartialRow lowerBound = KeyEncoder.decodeRangePartitionKey(schema, partitionSchema, rangeKeyStart);
        PartialRow upperBound = KeyEncoder.decodeRangePartitionKey(schema, partitionSchema, rangeKeyEnd);
        if (PartialRow.isIncremented(lowerBound, upperBound, idxs)) {
            sb.append("VALUES = ");
            if (numColumns > 1)
                sb.append('(');
            lowerBound.appendShortDebugString(idxs, sb);
            if (numColumns > 1)
                sb.append(')');
        } else {
            if (numColumns > 1)
                sb.append('(');
            lowerBound.appendShortDebugString(idxs, sb);
            if (numColumns > 1)
                sb.append(')');
            sb.append(" <= VALUES < ");
            if (numColumns > 1)
                sb.append('(');
            upperBound.appendShortDebugString(idxs, sb);
            if (numColumns > 1)
                sb.append(')');
        }
    }
    return sb.toString();
}
#method_after
String formatRangePartition(KuduTable table) {
    Schema schema = table.getSchema();
    PartitionSchema partitionSchema = table.getPartitionSchema();
    PartitionSchema.RangeSchema rangeSchema = partitionSchema.getRangeSchema();
    if (rangeSchema.getColumns().isEmpty()) {
        return "";
    }
    if (rangeKeyStart.length == 0 && rangeKeyEnd.length == 0) {
        return "UNBOUNDED";
    }
    List<Integer> idxs = new ArrayList<>();
    for (int id : partitionSchema.getRangeSchema().getColumns()) {
        idxs.add(schema.getColumnIndex(id));
    }
    int numColumns = rangeSchema.getColumns().size();
    StringBuilder sb = new StringBuilder();
    if (rangeKeyEnd.length == 0) {
        sb.append("VALUES >= ");
        if (numColumns > 1) {
            sb.append('(');
        }
        KeyEncoder.decodeRangePartitionKey(schema, partitionSchema, rangeKeyStart).appendShortDebugString(idxs, sb);
        if (numColumns > 1) {
            sb.append(')');
        }
    } else if (rangeKeyStart.length == 0) {
        sb.append("VALUES < ");
        if (numColumns > 1) {
            sb.append('(');
        }
        KeyEncoder.decodeRangePartitionKey(schema, partitionSchema, rangeKeyEnd).appendShortDebugString(idxs, sb);
        if (numColumns > 1) {
            sb.append(')');
        }
    } else {
        PartialRow lowerBound = KeyEncoder.decodeRangePartitionKey(schema, partitionSchema, rangeKeyStart);
        PartialRow upperBound = KeyEncoder.decodeRangePartitionKey(schema, partitionSchema, rangeKeyEnd);
        if (PartialRow.isIncremented(lowerBound, upperBound, idxs)) {
            sb.append("VALUES = ");
            if (numColumns > 1) {
                sb.append('(');
            }
            lowerBound.appendShortDebugString(idxs, sb);
            if (numColumns > 1) {
                sb.append(')');
            }
        } else {
            if (numColumns > 1) {
                sb.append('(');
            }
            lowerBound.appendShortDebugString(idxs, sb);
            if (numColumns > 1) {
                sb.append(')');
            }
            sb.append(" <= VALUES < ");
            if (numColumns > 1) {
                sb.append('(');
            }
            upperBound.appendShortDebugString(idxs, sb);
            if (numColumns > 1) {
                sb.append(')');
            }
        }
    }
    return sb.toString();
}
#end_block

#method_before
public static void appendEscapedSQLString(String s, StringBuilder sb) {
    for (int i = 0; i < s.length(); i++) {
        char currentChar = s.charAt(i);
        switch(currentChar) {
            case '\0':
                sb.append("\\0");
                break;
            case '\'':
                sb.append("\\'");
                break;
            case '\"':
                sb.append("\\\"");
                break;
            case '\b':
                sb.append("\\b");
                break;
            case '\n':
                sb.append("\\n");
                break;
            case '\r':
                sb.append("\\r");
                break;
            case '\t':
                sb.append("\\t");
                break;
            case '\\':
                sb.append("\\\\");
                break;
            case '\u001A':
                sb.append("\\Z");
                break;
            default:
                {
                    if (currentChar < ' ') {
                        sb.append("\\u");
                        String hex = Integer.toHexString(currentChar);
                        for (int j = 4; j > hex.length(); --j) {
                            sb.append('0');
                        }
                        sb.append(hex);
                    } else {
                        sb.append(currentChar);
                    }
                }
        }
    }
}
#method_after
public static void appendEscapedSQLString(String s, StringBuilder sb) {
    for (int i = 0; i < s.length(); i++) {
        char currentChar = s.charAt(i);
        switch(currentChar) {
            case '\0':
                {
                    sb.append("\\0");
                    break;
                }
            case '\'':
                {
                    sb.append("\\'");
                    break;
                }
            case '\"':
                {
                    sb.append("\\\"");
                    break;
                }
            case '\b':
                {
                    sb.append("\\b");
                    break;
                }
            case '\n':
                {
                    sb.append("\\n");
                    break;
                }
            case '\r':
                {
                    sb.append("\\r");
                    break;
                }
            case '\t':
                {
                    sb.append("\\t");
                    break;
                }
            case '\\':
                {
                    sb.append("\\\\");
                    break;
                }
            case '\u001A':
                {
                    sb.append("\\Z");
                    break;
                }
            default:
                {
                    if (currentChar < ' ') {
                        sb.append("\\u");
                        String hex = Integer.toHexString(currentChar);
                        for (int j = 4; j > hex.length(); --j) {
                            sb.append('0');
                        }
                        sb.append(hex);
                    } else {
                        sb.append(currentChar);
                    }
                }
        }
    }
}
#end_block

#method_before
public static PartialRow decodeRangePartitionKey(Schema schema, PartitionSchema partitionSchema, byte[] key) {
    ByteBuffer buf = ByteBuffer.wrap(key);
    buf.order(ByteOrder.BIG_ENDIAN);
    return decodeRangePartitionKey(schema, partitionSchema, ByteBuffer.wrap(key));
}
#method_after
public static PartialRow decodeRangePartitionKey(Schema schema, PartitionSchema partitionSchema, byte[] key) {
    ByteBuffer buf = ByteBuffer.wrap(key);
    buf.order(ByteOrder.BIG_ENDIAN);
    return decodeRangePartitionKey(schema, partitionSchema, buf);
}
#end_block

#method_before
@InterfaceAudience.LimitedPrivate("Impala")
@InterfaceStability.Unstable
public List<String> getFormattedRangePartitions(long deadline) throws Exception {
    List<String> rangePartitions = new ArrayList<>();
    for (LocatedTablet tablet : getTabletsLocations(deadline)) {
        Partition partition = tablet.getPartition();
        // partitions are all 0s.
        if (!Iterators.all(partition.getHashBuckets().iterator(), Predicates.equalTo(0)))
            continue;
        rangePartitions.add(partition.formatRangePartition(this));
    }
    return rangePartitions;
}
#method_after
@InterfaceAudience.LimitedPrivate("Impala")
@InterfaceStability.Unstable
public List<String> getFormattedRangePartitions(long deadline) throws Exception {
    List<String> rangePartitions = new ArrayList<>();
    for (LocatedTablet tablet : getTabletsLocations(deadline)) {
        Partition partition = tablet.getPartition();
        // partitions are all 0s.
        if (!Iterators.all(partition.getHashBuckets().iterator(), Predicates.equalTo(0))) {
            continue;
        }
        rangePartitions.add(partition.formatRangePartition(this));
    }
    return rangePartitions;
}
#end_block

#method_before
private void checkColumn(ColumnSchema column, Type... types) {
    checkNotFrozen();
    checkColumnExists(column);
    for (Type type : types) {
        if (column.getType().equals(type))
            return;
    }
    throw new IllegalArgumentException(String.format("%s isn't %s, it's %s", column.getName(), Arrays.toString(types), column.getType().getName()));
}
#method_after
private void checkColumn(ColumnSchema column, Type... types) {
    checkNotFrozen();
    checkColumnExists(column);
    for (Type type : types) {
        if (column.getType().equals(type)) {
            return;
        }
    }
    throw new IllegalArgumentException(String.format("%s isn't %s, it's %s", column.getName(), Arrays.toString(types), column.getType().getName()));
}
#end_block

#method_before
private void checkColumnExists(ColumnSchema column) {
    if (column == null)
        throw new IllegalArgumentException("Column name isn't present in the table's schema");
}
#method_after
private void checkColumnExists(ColumnSchema column) {
    if (column == null) {
        throw new IllegalArgumentException("Column name isn't present in the table's schema");
    }
}
#end_block

#method_before
void appendCellValueDebugString(Integer idx, StringBuilder sb) {
    ColumnSchema col = schema.getColumnByIndex(idx);
    Preconditions.checkState(columnsBitSet.get(idx), "Column %s is not set", col.getName());
    if (nullsBitSet != null && nullsBitSet.get(idx)) {
        sb.append("NULL");
        return;
    }
    switch(col.getType()) {
        case BOOL:
            sb.append(Bytes.getBoolean(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT8:
            sb.append(Bytes.getByte(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT16:
            sb.append(Bytes.getShort(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT32:
            sb.append(Bytes.getInt(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT64:
            sb.append(Bytes.getLong(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case UNIXTIME_MICROS:
            sb.append(RowResult.timestampToString(Bytes.getLong(rowAlloc, schema.getColumnOffset(idx))));
            return;
        case FLOAT:
            sb.append(Bytes.getFloat(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case DOUBLE:
            sb.append(Bytes.getDouble(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case BINARY:
        case STRING:
            ByteBuffer value = getVarLengthData().get(idx).duplicate();
            // Make sure we start at the beginning.
            value.reset();
            byte[] data = new byte[value.limit()];
            value.get(data);
            if (col.getType() == Type.STRING) {
                sb.append('"');
                StringUtil.appendEscapedSQLString(Bytes.getString(data), sb);
                sb.append('"');
            } else {
                sb.append(Bytes.pretty(data));
            }
    }
}
#method_after
void appendCellValueDebugString(Integer idx, StringBuilder sb) {
    ColumnSchema col = schema.getColumnByIndex(idx);
    Preconditions.checkState(columnsBitSet.get(idx), "Column %s is not set", col.getName());
    if (nullsBitSet != null && nullsBitSet.get(idx)) {
        sb.append("NULL");
        return;
    }
    switch(col.getType()) {
        case BOOL:
            sb.append(Bytes.getBoolean(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT8:
            sb.append(Bytes.getByte(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT16:
            sb.append(Bytes.getShort(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT32:
            sb.append(Bytes.getInt(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case INT64:
            sb.append(Bytes.getLong(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case UNIXTIME_MICROS:
            sb.append(RowResult.timestampToString(Bytes.getLong(rowAlloc, schema.getColumnOffset(idx))));
            return;
        case FLOAT:
            sb.append(Bytes.getFloat(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case DOUBLE:
            sb.append(Bytes.getDouble(rowAlloc, schema.getColumnOffset(idx)));
            return;
        case BINARY:
        case STRING:
            ByteBuffer value = getVarLengthData().get(idx).duplicate();
            // Make sure we start at the beginning.
            value.reset();
            byte[] data = new byte[value.limit()];
            value.get(data);
            if (col.getType() == Type.STRING) {
                sb.append('"');
                StringUtil.appendEscapedSQLString(Bytes.getString(data), sb);
                sb.append('"');
            } else {
                sb.append(Bytes.pretty(data));
            }
            return;
        default:
            throw new RuntimeException("unreachable");
    }
}
#end_block

#method_before
void setMin(int index) {
    Type type = schema.getColumnByIndex(index).getType();
    switch(type) {
        case BOOL:
            addBoolean(index, false);
            break;
        case INT8:
            addByte(index, Byte.MIN_VALUE);
            break;
        case INT16:
            addShort(index, Short.MIN_VALUE);
            break;
        case INT32:
            addInt(index, Integer.MIN_VALUE);
            break;
        case INT64:
        case UNIXTIME_MICROS:
            addLong(index, Integer.MIN_VALUE);
            break;
        case FLOAT:
            addFloat(index, -Float.MAX_VALUE);
            break;
        case DOUBLE:
            addDouble(index, -Double.MAX_VALUE);
            break;
        case STRING:
            addStringUtf8(index, AsyncKuduClient.EMPTY_ARRAY);
            break;
        case BINARY:
            addBinary(index, AsyncKuduClient.EMPTY_ARRAY);
            break;
    }
}
#method_after
void setMin(int index) {
    Type type = schema.getColumnByIndex(index).getType();
    switch(type) {
        case BOOL:
            addBoolean(index, false);
            break;
        case INT8:
            addByte(index, Byte.MIN_VALUE);
            break;
        case INT16:
            addShort(index, Short.MIN_VALUE);
            break;
        case INT32:
            addInt(index, Integer.MIN_VALUE);
            break;
        case INT64:
        case UNIXTIME_MICROS:
            addLong(index, Integer.MIN_VALUE);
            break;
        case FLOAT:
            addFloat(index, -Float.MAX_VALUE);
            break;
        case DOUBLE:
            addDouble(index, -Double.MAX_VALUE);
            break;
        case STRING:
            addStringUtf8(index, AsyncKuduClient.EMPTY_ARRAY);
            break;
        case BINARY:
            addBinary(index, AsyncKuduClient.EMPTY_ARRAY);
            break;
        default:
            throw new RuntimeException("unreachable");
    }
}
#end_block

#method_before
void setRaw(int index, byte[] value) {
    Type type = schema.getColumnByIndex(index).getType();
    switch(type) {
        case BOOL:
        case INT8:
        case INT16:
        case INT32:
        case INT64:
        case UNIXTIME_MICROS:
        case FLOAT:
        case DOUBLE:
            {
                Preconditions.checkArgument(value.length == type.getSize());
                System.arraycopy(value, 0, rowAlloc, getPositionInRowAllocAndSetBitSet(index), value.length);
                break;
            }
        case STRING:
        case BINARY:
            {
                addVarLengthData(index, value);
                break;
            }
    }
}
#method_after
void setRaw(int index, byte[] value) {
    Type type = schema.getColumnByIndex(index).getType();
    switch(type) {
        case BOOL:
        case INT8:
        case INT16:
        case INT32:
        case INT64:
        case UNIXTIME_MICROS:
        case FLOAT:
        case DOUBLE:
            {
                Preconditions.checkArgument(value.length == type.getSize());
                System.arraycopy(value, 0, rowAlloc, getPositionInRowAllocAndSetBitSet(index), value.length);
                break;
            }
        case STRING:
        case BINARY:
            {
                addVarLengthData(index, value);
                break;
            }
        default:
            throw new RuntimeException("unreachable");
    }
}
#end_block

#method_before
boolean incrementColumn(int index) {
    Type type = schema.getColumnByIndex(index).getType();
    Preconditions.checkState(isSet(index));
    int offset = schema.getColumnOffset(index);
    switch(type) {
        case BOOL:
            {
                boolean isFalse = rowAlloc[offset] == 0;
                rowAlloc[offset] = 1;
                return isFalse;
            }
        case INT8:
            {
                byte existing = rowAlloc[offset];
                if (existing == Byte.MAX_VALUE)
                    return false;
                rowAlloc[offset] = (byte) (existing + 1);
                return true;
            }
        case INT16:
            {
                short existing = Bytes.getShort(rowAlloc, offset);
                if (existing == Short.MAX_VALUE)
                    return false;
                Bytes.setShort(rowAlloc, (short) (existing + 1), offset);
                return true;
            }
        case INT32:
            {
                int existing = Bytes.getInt(rowAlloc, offset);
                if (existing == Integer.MAX_VALUE)
                    return false;
                Bytes.setInt(rowAlloc, existing + 1, offset);
                return true;
            }
        case INT64:
        case UNIXTIME_MICROS:
            {
                long existing = Bytes.getLong(rowAlloc, offset);
                if (existing == Long.MAX_VALUE)
                    return false;
                Bytes.setLong(rowAlloc, existing + 1, offset);
                return true;
            }
        case FLOAT:
            {
                float existing = Bytes.getFloat(rowAlloc, offset);
                float incremented = Math.nextAfter(existing, Float.POSITIVE_INFINITY);
                if (existing == incremented)
                    return false;
                Bytes.setFloat(rowAlloc, incremented, offset);
                return true;
            }
        case DOUBLE:
            {
                double existing = Bytes.getFloat(rowAlloc, offset);
                double incremented = Math.nextAfter(existing, Double.POSITIVE_INFINITY);
                if (existing == incremented)
                    return false;
                Bytes.setDouble(rowAlloc, incremented, offset);
                return true;
            }
        case STRING:
        case BINARY:
            {
                ByteBuffer data = varLengthData.get(index);
                data.reset();
                int len = data.limit() - data.position();
                byte[] incremented = new byte[len + 1];
                System.arraycopy(data.array(), data.arrayOffset() + data.position(), incremented, 0, len);
                addVarLengthData(index, incremented);
                return true;
            }
    }
    throw new RuntimeException("unreachable");
}
#method_after
boolean incrementColumn(int index) {
    Type type = schema.getColumnByIndex(index).getType();
    Preconditions.checkState(isSet(index));
    int offset = schema.getColumnOffset(index);
    switch(type) {
        case BOOL:
            {
                boolean isFalse = rowAlloc[offset] == 0;
                rowAlloc[offset] = 1;
                return isFalse;
            }
        case INT8:
            {
                byte existing = rowAlloc[offset];
                if (existing == Byte.MAX_VALUE) {
                    return false;
                }
                rowAlloc[offset] = (byte) (existing + 1);
                return true;
            }
        case INT16:
            {
                short existing = Bytes.getShort(rowAlloc, offset);
                if (existing == Short.MAX_VALUE) {
                    return false;
                }
                Bytes.setShort(rowAlloc, (short) (existing + 1), offset);
                return true;
            }
        case INT32:
            {
                int existing = Bytes.getInt(rowAlloc, offset);
                if (existing == Integer.MAX_VALUE) {
                    return false;
                }
                Bytes.setInt(rowAlloc, existing + 1, offset);
                return true;
            }
        case INT64:
        case UNIXTIME_MICROS:
            {
                long existing = Bytes.getLong(rowAlloc, offset);
                if (existing == Long.MAX_VALUE) {
                    return false;
                }
                Bytes.setLong(rowAlloc, existing + 1, offset);
                return true;
            }
        case FLOAT:
            {
                float existing = Bytes.getFloat(rowAlloc, offset);
                float incremented = Math.nextAfter(existing, Float.POSITIVE_INFINITY);
                if (existing == incremented) {
                    return false;
                }
                Bytes.setFloat(rowAlloc, incremented, offset);
                return true;
            }
        case DOUBLE:
            {
                double existing = Bytes.getFloat(rowAlloc, offset);
                double incremented = Math.nextAfter(existing, Double.POSITIVE_INFINITY);
                if (existing == incremented) {
                    return false;
                }
                Bytes.setDouble(rowAlloc, incremented, offset);
                return true;
            }
        case STRING:
        case BINARY:
            {
                ByteBuffer data = varLengthData.get(index);
                data.reset();
                int len = data.limit() - data.position();
                byte[] incremented = new byte[len + 1];
                System.arraycopy(data.array(), data.arrayOffset() + data.position(), incremented, 0, len);
                addVarLengthData(index, incremented);
                return true;
            }
        default:
            throw new RuntimeException("unreachable");
    }
}
#end_block

#method_before
static boolean isIncremented(PartialRow lower, PartialRow upper, List<Integer> indexes) {
    boolean equals = false;
    ListIterator<Integer> iter = indexes.listIterator(indexes.size());
    while (iter.hasPrevious()) {
        int index = iter.previous();
        if (equals) {
            if (isCellEqual(lower, upper, index))
                continue;
            return false;
        }
        if (!lower.isSet(index) && !upper.isSet(index))
            continue;
        if (!isCellIncremented(lower, upper, index))
            return false;
        equals = true;
    }
    return equals;
}
#method_after
static boolean isIncremented(PartialRow lower, PartialRow upper, List<Integer> indexes) {
    boolean equals = false;
    ListIterator<Integer> iter = indexes.listIterator(indexes.size());
    while (iter.hasPrevious()) {
        int index = iter.previous();
        if (equals) {
            if (isCellEqual(lower, upper, index)) {
                continue;
            }
            return false;
        }
        if (!lower.isSet(index) && !upper.isSet(index)) {
            continue;
        }
        if (!isCellIncremented(lower, upper, index)) {
            return false;
        }
        equals = true;
    }
    return equals;
}
#end_block

#method_before
private static boolean isCellEqual(PartialRow a, PartialRow b, int index) {
    // These checks are perhaps overly restrictive, but right now we only use
    // this method for checking fully-set keys.
    Preconditions.checkArgument(a.getSchema().equals(b.getSchema()));
    Preconditions.checkArgument(a.getSchema().getColumnByIndex(index).isKey());
    Preconditions.checkArgument(a.isSet(index));
    Preconditions.checkArgument(b.isSet(index));
    Type type = a.getSchema().getColumnByIndex(index).getType();
    int offset = a.getSchema().getColumnOffset(index);
    switch(type) {
        case BOOL:
            return a.rowAlloc[offset] + 1 == b.rowAlloc[offset];
        case INT8:
            return a.rowAlloc[offset] == b.rowAlloc[offset];
        case INT16:
            return Bytes.getShort(a.rowAlloc, offset) == Bytes.getShort(b.rowAlloc, offset);
        case INT32:
            return Bytes.getInt(a.rowAlloc, offset) == Bytes.getInt(b.rowAlloc, offset);
        case INT64:
        case UNIXTIME_MICROS:
            return Bytes.getLong(a.rowAlloc, offset) == Bytes.getLong(b.rowAlloc, offset);
        case FLOAT:
            return Bytes.getFloat(a.rowAlloc, offset) == Bytes.getFloat(b.rowAlloc, offset);
        case DOUBLE:
            return Bytes.getDouble(a.rowAlloc, offset) == Bytes.getDouble(b.rowAlloc, offset);
        case STRING:
        case BINARY:
            {
                // Check that b is 1 byte longer than a, the extra byte is 0, and all other bytes are equal.
                ByteBuffer aData = a.varLengthData.get(index).duplicate();
                ByteBuffer bData = b.varLengthData.get(index).duplicate();
                aData.reset();
                bData.reset();
                int aLen = aData.limit() - aData.position();
                int bLen = bData.limit() - bData.position();
                if (aLen != bLen)
                    return false;
                for (int i = 0; i < aLen; i++) {
                    if (aData.get(aData.position() + i) != bData.get(bData.position() + i))
                        return false;
                }
                return true;
            }
    }
    throw new RuntimeException("unreachable");
}
#method_after
private static boolean isCellEqual(PartialRow a, PartialRow b, int index) {
    // These checks are perhaps overly restrictive, but right now we only use
    // this method for checking fully-set keys.
    Preconditions.checkArgument(a.getSchema().equals(b.getSchema()));
    Preconditions.checkArgument(a.getSchema().getColumnByIndex(index).isKey());
    Preconditions.checkArgument(a.isSet(index));
    Preconditions.checkArgument(b.isSet(index));
    Type type = a.getSchema().getColumnByIndex(index).getType();
    int offset = a.getSchema().getColumnOffset(index);
    switch(type) {
        case BOOL:
            return a.rowAlloc[offset] == b.rowAlloc[offset];
        case INT8:
            return a.rowAlloc[offset] == b.rowAlloc[offset];
        case INT16:
            return Bytes.getShort(a.rowAlloc, offset) == Bytes.getShort(b.rowAlloc, offset);
        case INT32:
            return Bytes.getInt(a.rowAlloc, offset) == Bytes.getInt(b.rowAlloc, offset);
        case INT64:
        case UNIXTIME_MICROS:
            return Bytes.getLong(a.rowAlloc, offset) == Bytes.getLong(b.rowAlloc, offset);
        case FLOAT:
            return Bytes.getFloat(a.rowAlloc, offset) == Bytes.getFloat(b.rowAlloc, offset);
        case DOUBLE:
            return Bytes.getDouble(a.rowAlloc, offset) == Bytes.getDouble(b.rowAlloc, offset);
        case STRING:
        case BINARY:
            {
                ByteBuffer aData = a.varLengthData.get(index).duplicate();
                ByteBuffer bData = b.varLengthData.get(index).duplicate();
                aData.reset();
                bData.reset();
                int aLen = aData.limit() - aData.position();
                int bLen = bData.limit() - bData.position();
                if (aLen != bLen) {
                    return false;
                }
                for (int i = 0; i < aLen; i++) {
                    if (aData.get(aData.position() + i) != bData.get(bData.position() + i)) {
                        return false;
                    }
                }
                return true;
            }
        default:
            throw new RuntimeException("unreachable");
    }
}
#end_block

#method_before
private static boolean isCellIncremented(PartialRow lower, PartialRow upper, int index) {
    // These checks are perhaps overly restrictive, but right now we only use
    // this method for checking fully-set keys.
    Preconditions.checkArgument(lower.getSchema().equals(upper.getSchema()));
    Preconditions.checkArgument(lower.getSchema().getColumnByIndex(index).isKey());
    Preconditions.checkArgument(lower.isSet(index));
    Preconditions.checkArgument(upper.isSet(index));
    Type type = lower.getSchema().getColumnByIndex(index).getType();
    int offset = lower.getSchema().getColumnOffset(index);
    switch(type) {
        case BOOL:
            return lower.rowAlloc[offset] + 1 == upper.rowAlloc[offset];
        case INT8:
            {
                byte val = lower.rowAlloc[offset];
                return val != Byte.MAX_VALUE && val + 1 == upper.rowAlloc[offset];
            }
        case INT16:
            {
                short val = Bytes.getShort(lower.rowAlloc, offset);
                return val != Short.MAX_VALUE && val + 1 == Bytes.getShort(upper.rowAlloc, offset);
            }
        case INT32:
            {
                int val = Bytes.getInt(lower.rowAlloc, offset);
                return val != Integer.MAX_VALUE && val + 1 == Bytes.getInt(upper.rowAlloc, offset);
            }
        case INT64:
        case UNIXTIME_MICROS:
            {
                long val = Bytes.getLong(lower.rowAlloc, offset);
                return val != Long.MAX_VALUE && val + 1 == Bytes.getLong(upper.rowAlloc, offset);
            }
        case FLOAT:
            {
                float val = Bytes.getFloat(lower.rowAlloc, offset);
                return val != Float.POSITIVE_INFINITY && Math.nextAfter(val, Float.POSITIVE_INFINITY) == Bytes.getFloat(upper.rowAlloc, offset);
            }
        case DOUBLE:
            {
                double val = Bytes.getDouble(lower.rowAlloc, offset);
                return val != Double.POSITIVE_INFINITY && Math.nextAfter(val, Double.POSITIVE_INFINITY) == Bytes.getDouble(upper.rowAlloc, offset);
            }
        case STRING:
        case BINARY:
            {
                // Check that b is 1 byte bigger than a, the extra byte is 0, and the other bytes are equal.
                ByteBuffer aData = lower.varLengthData.get(index).duplicate();
                ByteBuffer bData = upper.varLengthData.get(index).duplicate();
                aData.reset();
                bData.reset();
                int aLen = aData.limit() - aData.position();
                int bLen = bData.limit() - bData.position();
                if (aLen == Integer.MAX_VALUE || aLen + 1 != bLen || bData.get(bData.limit() - 1) != 0) {
                    return false;
                }
                for (int i = 0; i < aLen; i++) {
                    if (aData.get(aData.position() + i) != bData.get(bData.position() + i))
                        return false;
                }
                return true;
            }
    }
    throw new RuntimeException("unreachable");
}
#method_after
private static boolean isCellIncremented(PartialRow lower, PartialRow upper, int index) {
    // These checks are perhaps overly restrictive, but right now we only use
    // this method for checking fully-set keys.
    Preconditions.checkArgument(lower.getSchema().equals(upper.getSchema()));
    Preconditions.checkArgument(lower.getSchema().getColumnByIndex(index).isKey());
    Preconditions.checkArgument(lower.isSet(index));
    Preconditions.checkArgument(upper.isSet(index));
    Type type = lower.getSchema().getColumnByIndex(index).getType();
    int offset = lower.getSchema().getColumnOffset(index);
    switch(type) {
        case BOOL:
            return lower.rowAlloc[offset] + 1 == upper.rowAlloc[offset];
        case INT8:
            {
                byte val = lower.rowAlloc[offset];
                return val != Byte.MAX_VALUE && val + 1 == upper.rowAlloc[offset];
            }
        case INT16:
            {
                short val = Bytes.getShort(lower.rowAlloc, offset);
                return val != Short.MAX_VALUE && val + 1 == Bytes.getShort(upper.rowAlloc, offset);
            }
        case INT32:
            {
                int val = Bytes.getInt(lower.rowAlloc, offset);
                return val != Integer.MAX_VALUE && val + 1 == Bytes.getInt(upper.rowAlloc, offset);
            }
        case INT64:
        case UNIXTIME_MICROS:
            {
                long val = Bytes.getLong(lower.rowAlloc, offset);
                return val != Long.MAX_VALUE && val + 1 == Bytes.getLong(upper.rowAlloc, offset);
            }
        case FLOAT:
            {
                float val = Bytes.getFloat(lower.rowAlloc, offset);
                return val != Float.POSITIVE_INFINITY && Math.nextAfter(val, Float.POSITIVE_INFINITY) == Bytes.getFloat(upper.rowAlloc, offset);
            }
        case DOUBLE:
            {
                double val = Bytes.getDouble(lower.rowAlloc, offset);
                return val != Double.POSITIVE_INFINITY && Math.nextAfter(val, Double.POSITIVE_INFINITY) == Bytes.getDouble(upper.rowAlloc, offset);
            }
        case STRING:
        case BINARY:
            {
                // Check that b is 1 byte bigger than a, the extra byte is 0, and the other bytes are equal.
                ByteBuffer aData = lower.varLengthData.get(index).duplicate();
                ByteBuffer bData = upper.varLengthData.get(index).duplicate();
                aData.reset();
                bData.reset();
                int aLen = aData.limit() - aData.position();
                int bLen = bData.limit() - bData.position();
                if (aLen == Integer.MAX_VALUE || aLen + 1 != bLen || bData.get(bData.limit() - 1) != 0) {
                    return false;
                }
                for (int i = 0; i < aLen; i++) {
                    if (aData.get(aData.position() + i) != bData.get(bData.position() + i)) {
                        return false;
                    }
                }
                return true;
            }
        default:
            throw new RuntimeException("unreachable");
    }
}
#end_block

#method_before
@Override
public void configure(Context context) {
    String regexp = context.getString(PATTERN_PROP);
    Preconditions.checkArgument(regexp != null, "Required parameter %s is not specified", PATTERN_PROP);
    try {
        pattern = Pattern.compile(regexp);
    } catch (PatternSyntaxException e) {
        throw new IllegalArgumentException(String.format("The pattern '%s' is invalid", PATTERN_PROP, regexp), e);
    }
    String charsetName = context.getString(ENCODING_PROP, DEFAULT_ENCODING);
    try {
        charset = Charset.forName(charsetName);
    } catch (IllegalArgumentException e) {
        throw new FlumeException(String.format("Invalid or unsupported charset %s", charsetName), e);
    }
    operation = context.getString(OPERATION_PROP, DEFAULT_OPERATION);
    Preconditions.checkArgument(validOperations.contains(operation.toLowerCase()), "Unrecognized operation '%s'", operation);
    skipMissingColumn = context.getBoolean(SKIP_MISSING_COLUMN_PROP, DEFAULT_SKIP_MISSING_COLUMN);
    skipBadColumnValue = context.getBoolean(SKIP_BAD_COLUMN_VALUE_PROP, DEFAULT_SKIP_BAD_COLUMN_VALUE);
    warnUnmatchedRows = context.getBoolean(WARN_UNMATCHED_ROWS_PROP, DEFAULT_WARN_UNMATCHED_ROWS);
}
#method_after
@Override
public void configure(Context context) {
    String regexp = context.getString(PATTERN_PROP);
    Preconditions.checkArgument(regexp != null, "Required parameter %s is not specified", PATTERN_PROP);
    try {
        pattern = Pattern.compile(regexp);
    } catch (PatternSyntaxException e) {
        throw new IllegalArgumentException(String.format("The pattern '%s' is invalid", regexp), e);
    }
    String charsetName = context.getString(ENCODING_PROP, DEFAULT_ENCODING);
    try {
        charset = Charset.forName(charsetName);
    } catch (IllegalArgumentException e) {
        throw new FlumeException(String.format("Invalid or unsupported charset %s", charsetName), e);
    }
    operation = context.getString(OPERATION_PROP, DEFAULT_OPERATION);
    Preconditions.checkArgument(validOperations.contains(operation.toLowerCase()), "Unrecognized operation '%s'", operation);
    skipMissingColumn = context.getBoolean(SKIP_MISSING_COLUMN_PROP, DEFAULT_SKIP_MISSING_COLUMN);
    skipBadColumnValue = context.getBoolean(SKIP_BAD_COLUMN_VALUE_PROP, DEFAULT_SKIP_BAD_COLUMN_VALUE);
    warnUnmatchedRows = context.getBoolean(WARN_UNMATCHED_ROWS_PROP, DEFAULT_WARN_UNMATCHED_ROWS);
}
#end_block

#method_before
public static void main(String[] args) throws Exception {
    if (args.length != 1) {
        System.err.println("Usage: " + "TestDataGenerator BaseOutputDirectory");
    }
    TimeZone.setDefault(TimeZone.getTimeZone("America/Los_Angeles"));
    // Generate AllTypes
    String dirName = args[0] + "/AllTypes";
    File dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesData(dirName, DEFAULT_NUM_PARTITIONS, DEFAULT_MAX_TUPLES_PER_PARTITION);
    // Generate AllTypesSmall
    dirName = args[0] + "/AllTypesSmall";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesData(dirName, 4, 25);
    // Generate AllTypesSmall
    dirName = args[0] + "/AllTypesTiny";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesData(dirName, 4, 2);
    // Generate AllTypesAgg
    dirName = args[0] + "/AllTypesAgg";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesAggData(dirName, true);
    // Generate AllTypesAgg w/o nulls
    dirName = args[0] + "/AllTypesAggNoNulls";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesAggData(dirName, false);
    // Generate Decimal data
    dirName = args[0] + "/DecimalTiny";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateDecimalData(dirName, 100);
}
#method_after
public static void main(String[] args) throws Exception {
    if (args.length != 1) {
        System.err.println("Usage: " + "TestDataGenerator BaseOutputDirectory");
    }
    // The TimeZone should be the same no matter what the TimeZone is of the computer
    // running this code, in order to ensure the generated data is always the same.
    TimeZone.setDefault(TimeZone.getTimeZone("America/Los_Angeles"));
    // Generate AllTypes
    String dirName = args[0] + "/AllTypes";
    File dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesData(dirName, DEFAULT_NUM_PARTITIONS, DEFAULT_MAX_TUPLES_PER_PARTITION);
    // Generate AllTypesSmall
    dirName = args[0] + "/AllTypesSmall";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesData(dirName, 4, 25);
    // Generate AllTypesSmall
    dirName = args[0] + "/AllTypesTiny";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesData(dirName, 4, 2);
    // Generate AllTypesAgg
    dirName = args[0] + "/AllTypesAgg";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesAggData(dirName, true);
    // Generate AllTypesAgg w/o nulls
    dirName = args[0] + "/AllTypesAggNoNulls";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateAllTypesAggData(dirName, false);
    // Generate Decimal data
    dirName = args[0] + "/DecimalTiny";
    dir = new File(dirName);
    dir.mkdirs();
    GenerateDecimalData(dirName, 100);
}
#end_block

#method_before
public long getReadSize() {
    return READ_SIZE;
}
#method_after
public long getReadSize() {
    return backendCfg_.read_size;
}
#end_block

#method_before
public static boolean isAuthToLocalEnabled() {
    return allowAuthToLocalRules_;
}
#method_after
public boolean isAuthToLocalEnabled() {
    return backendCfg_.load_auth_to_local_rules && !Strings.isNullOrEmpty(backendCfg_.principal);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    table_ = analyzer.getTable(tableName_, Privilege.ALTER);
    String sqlTableName = table_.getTableName().toSql();
    if (table_ instanceof View) {
        throw new AnalysisException(String.format("COMPUTE STATS not supported for view %s", sqlTableName));
    }
    if (!(table_ instanceof HdfsTable)) {
        if (partitionSpec_ != null) {
            throw new AnalysisException("COMPUTE INCREMENTAL ... PARTITION not supported " + "for non-HDFS table " + table_.getTableName());
        }
        isIncremental_ = false;
    }
    // Ensure that we write an entry for every partition if this isn't incremental
    if (!isIncremental_)
        expectAllPartitions_ = true;
    HdfsTable hdfsTable = null;
    if (table_ instanceof HdfsTable) {
        hdfsTable = (HdfsTable) table_;
        if (isIncremental_ && hdfsTable.getNumClusteringCols() == 0 && partitionSpec_ != null) {
            throw new AnalysisException(String.format("Can't compute PARTITION stats on an unpartitioned table: %s", sqlTableName));
        } else if (partitionSpec_ != null) {
            partitionSpec_.setPartitionShouldExist();
            partitionSpec_.analyze(analyzer);
            for (PartitionKeyValue kv : partitionSpec_.getPartitionSpecKeyValues()) {
                // partition spec.
                if (!kv.isStatic()) {
                    throw new AnalysisException("All partition keys must have values: " + kv.toString());
                }
            }
        }
        // error if the estimate is greater than incStatMaxSize in bytes
        if (isIncremental_) {
            long incStatMaxSize = BackendConfig.INSTANCE.getIncStatMaxSize();
            long statsSizeEstimate = hdfsTable.getColumns().size() * hdfsTable.getPartitions().size() * HdfsTable.STATS_SIZE_PER_COLUMN_BYTES;
            if (statsSizeEstimate > incStatMaxSize) {
                LOG.error("Incremental stats size estimate for table " + hdfsTable.getName() + " exceeded " + incStatMaxSize + ", estimate = " + statsSizeEstimate);
                throw new AnalysisException("Incremental stats size estimate exceeds " + PrintUtils.printBytes(incStatMaxSize) + ". Please try COMPUTE STATS instead.");
            }
        }
    }
    // Build partition filters that only select partitions without valid statistics for
    // incremental computation.
    List<String> filterPreds = Lists.newArrayList();
    if (isIncremental_) {
        if (partitionSpec_ == null) {
            // If any column does not have stats, we recompute statistics for all partitions
            // TODO: need a better way to invalidate stats for all partitions, so that we can
            // use this logic to only recompute new / changed columns.
            boolean tableIsMissingColStats = false;
            // We'll warn the user if a column is missing stats (and therefore we rescan the
            // whole table), but if all columns are missing stats, the table just doesn't have
            // any stats and there's no need to warn.
            boolean allColumnsMissingStats = true;
            String exampleColumnMissingStats = null;
            // Partition columns always have stats, so exclude them from this search
            for (Column col : table_.getNonClusteringColumns()) {
                if (!col.getStats().hasStats()) {
                    if (!tableIsMissingColStats) {
                        tableIsMissingColStats = true;
                        exampleColumnMissingStats = col.getName();
                    }
                } else {
                    allColumnsMissingStats = false;
                }
            }
            if (tableIsMissingColStats && !allColumnsMissingStats) {
                analyzer.addWarning("Column " + exampleColumnMissingStats + " does not have statistics, recomputing stats for the whole table");
            }
            for (HdfsPartition p : hdfsTable.getPartitions()) {
                if (p.isDefaultPartition())
                    continue;
                TPartitionStats partStats = p.getPartitionStats();
                if (!p.hasIncrementalStats() || tableIsMissingColStats) {
                    if (partStats == null)
                        LOG.trace(p.toString() + " does not have stats");
                    if (!tableIsMissingColStats)
                        filterPreds.add(p.getConjunctSql());
                    List<String> partValues = Lists.newArrayList();
                    for (LiteralExpr partValue : p.getPartitionValues()) {
                        partValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, "NULL"));
                    }
                    expectedPartitions_.add(partValues);
                } else {
                    LOG.trace(p.toString() + " does have statistics");
                    validPartStats_.add(partStats);
                }
            }
            if (expectedPartitions_.size() == hdfsTable.getPartitions().size() - 1) {
                expectedPartitions_.clear();
                expectAllPartitions_ = true;
            }
        } else {
            // Always compute stats on a particular partition when told to.
            List<String> partitionConjuncts = Lists.newArrayList();
            for (PartitionKeyValue kv : partitionSpec_.getPartitionSpecKeyValues()) {
                partitionConjuncts.add(kv.toPredicateSql());
            }
            filterPreds.add("(" + Joiner.on(" AND ").join(partitionConjuncts) + ")");
            HdfsPartition targetPartition = hdfsTable.getPartition(partitionSpec_.getPartitionSpecKeyValues());
            List<String> partValues = Lists.newArrayList();
            for (LiteralExpr partValue : targetPartition.getPartitionValues()) {
                partValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, "NULL"));
            }
            expectedPartitions_.add(partValues);
            for (HdfsPartition p : hdfsTable.getPartitions()) {
                if (p.isDefaultPartition())
                    continue;
                if (p == targetPartition)
                    continue;
                TPartitionStats partStats = p.getPartitionStats();
                if (partStats != null)
                    validPartStats_.add(partStats);
            }
        }
        if (filterPreds.size() == 0 && validPartStats_.size() != 0) {
            LOG.info("No partitions selected for incremental stats update");
            analyzer.addWarning("No partitions selected for incremental stats update");
            return;
        }
    }
    if (filterPreds.size() > MAX_INCREMENTAL_PARTITIONS) {
        // TODO: Consider simply running for MAX_INCREMENTAL_PARTITIONS partitions, and then
        // advising the user to iterate.
        analyzer.addWarning("Too many partitions selected, doing full recomputation of incremental stats");
        filterPreds.clear();
        validPartStats_.clear();
    }
    List<String> groupByCols = Lists.newArrayList();
    List<String> partitionColsSelectList = Lists.newArrayList();
    // Only add group by clause for HdfsTables.
    if (hdfsTable != null) {
        if (hdfsTable.isAvroTable())
            checkIncompleteAvroSchema(hdfsTable);
        addPartitionCols(hdfsTable, partitionColsSelectList, groupByCols);
    }
    // Query for getting the per-partition row count and the total row count.
    StringBuilder tableStatsQueryBuilder = new StringBuilder("SELECT ");
    List<String> tableStatsSelectList = Lists.newArrayList();
    tableStatsSelectList.add("COUNT(*)");
    tableStatsSelectList.addAll(partitionColsSelectList);
    tableStatsQueryBuilder.append(Joiner.on(", ").join(tableStatsSelectList));
    tableStatsQueryBuilder.append(" FROM " + sqlTableName);
    // Query for getting the per-column NDVs and number of NULLs.
    List<String> columnStatsSelectList = getBaseColumnStatsQuerySelectList(analyzer);
    if (isIncremental_)
        columnStatsSelectList.addAll(partitionColsSelectList);
    StringBuilder columnStatsQueryBuilder = new StringBuilder("SELECT ");
    columnStatsQueryBuilder.append(Joiner.on(", ").join(columnStatsSelectList));
    columnStatsQueryBuilder.append(" FROM " + sqlTableName);
    // selected in).
    if (filterPreds.size() > 0 && (validPartStats_.size() > 0 || partitionSpec_ != null)) {
        String filterClause = " WHERE " + Joiner.on(" OR ").join(filterPreds);
        columnStatsQueryBuilder.append(filterClause);
        tableStatsQueryBuilder.append(filterClause);
    }
    if (groupByCols.size() > 0) {
        String groupBy = " GROUP BY " + Joiner.on(", ").join(groupByCols);
        if (isIncremental_)
            columnStatsQueryBuilder.append(groupBy);
        tableStatsQueryBuilder.append(groupBy);
    }
    tableStatsQueryStr_ = tableStatsQueryBuilder.toString();
    LOG.debug("Table stats query: " + tableStatsQueryStr_);
    if (columnStatsSelectList.isEmpty()) {
        // Table doesn't have any columns that we can compute stats for.
        LOG.info("No supported column types in table " + table_.getTableName() + ", no column statistics will be gathered.");
        columnStatsQueryStr_ = null;
        return;
    }
    columnStatsQueryStr_ = columnStatsQueryBuilder.toString();
    LOG.debug("Column stats query: " + columnStatsQueryStr_);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    table_ = analyzer.getTable(tableName_, Privilege.ALTER);
    String sqlTableName = table_.getTableName().toSql();
    if (table_ instanceof View) {
        throw new AnalysisException(String.format("COMPUTE STATS not supported for view %s", sqlTableName));
    }
    if (!(table_ instanceof HdfsTable)) {
        if (partitionSpec_ != null) {
            throw new AnalysisException("COMPUTE INCREMENTAL ... PARTITION not supported " + "for non-HDFS table " + table_.getTableName());
        }
        isIncremental_ = false;
    }
    // Ensure that we write an entry for every partition if this isn't incremental
    if (!isIncremental_)
        expectAllPartitions_ = true;
    HdfsTable hdfsTable = null;
    if (table_ instanceof HdfsTable) {
        hdfsTable = (HdfsTable) table_;
        if (isIncremental_ && hdfsTable.getNumClusteringCols() == 0 && partitionSpec_ != null) {
            throw new AnalysisException(String.format("Can't compute PARTITION stats on an unpartitioned table: %s", sqlTableName));
        } else if (partitionSpec_ != null) {
            partitionSpec_.setPartitionShouldExist();
            partitionSpec_.analyze(analyzer);
            for (PartitionKeyValue kv : partitionSpec_.getPartitionSpecKeyValues()) {
                // partition spec.
                if (!kv.isStatic()) {
                    throw new AnalysisException("All partition keys must have values: " + kv.toString());
                }
            }
        }
        // error if the estimate is greater than --inc_stats_size_limit_bytes in bytes
        if (isIncremental_) {
            long incStatMaxSize = BackendConfig.INSTANCE.getIncStatsMaxSize();
            long statsSizeEstimate = hdfsTable.getColumns().size() * hdfsTable.getPartitions().size() * HdfsTable.STATS_SIZE_PER_COLUMN_BYTES;
            if (statsSizeEstimate > incStatMaxSize) {
                LOG.error("Incremental stats size estimate for table " + hdfsTable.getName() + " exceeded " + incStatMaxSize + ", estimate = " + statsSizeEstimate);
                throw new AnalysisException("Incremental stats size estimate exceeds " + PrintUtils.printBytes(incStatMaxSize) + ". Please try COMPUTE STATS instead.");
            }
        }
    }
    // Build partition filters that only select partitions without valid statistics for
    // incremental computation.
    List<String> filterPreds = Lists.newArrayList();
    if (isIncremental_) {
        if (partitionSpec_ == null) {
            // If any column does not have stats, we recompute statistics for all partitions
            // TODO: need a better way to invalidate stats for all partitions, so that we can
            // use this logic to only recompute new / changed columns.
            boolean tableIsMissingColStats = false;
            // We'll warn the user if a column is missing stats (and therefore we rescan the
            // whole table), but if all columns are missing stats, the table just doesn't have
            // any stats and there's no need to warn.
            boolean allColumnsMissingStats = true;
            String exampleColumnMissingStats = null;
            // Partition columns always have stats, so exclude them from this search
            for (Column col : table_.getNonClusteringColumns()) {
                if (!col.getStats().hasStats()) {
                    if (!tableIsMissingColStats) {
                        tableIsMissingColStats = true;
                        exampleColumnMissingStats = col.getName();
                    }
                } else {
                    allColumnsMissingStats = false;
                }
            }
            if (tableIsMissingColStats && !allColumnsMissingStats) {
                analyzer.addWarning("Column " + exampleColumnMissingStats + " does not have statistics, recomputing stats for the whole table");
            }
            for (HdfsPartition p : hdfsTable.getPartitions()) {
                if (p.isDefaultPartition())
                    continue;
                TPartitionStats partStats = p.getPartitionStats();
                if (!p.hasIncrementalStats() || tableIsMissingColStats) {
                    if (partStats == null)
                        LOG.trace(p.toString() + " does not have stats");
                    if (!tableIsMissingColStats)
                        filterPreds.add(p.getConjunctSql());
                    List<String> partValues = Lists.newArrayList();
                    for (LiteralExpr partValue : p.getPartitionValues()) {
                        partValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, "NULL"));
                    }
                    expectedPartitions_.add(partValues);
                } else {
                    LOG.trace(p.toString() + " does have statistics");
                    validPartStats_.add(partStats);
                }
            }
            if (expectedPartitions_.size() == hdfsTable.getPartitions().size() - 1) {
                expectedPartitions_.clear();
                expectAllPartitions_ = true;
            }
        } else {
            // Always compute stats on a particular partition when told to.
            List<String> partitionConjuncts = Lists.newArrayList();
            for (PartitionKeyValue kv : partitionSpec_.getPartitionSpecKeyValues()) {
                partitionConjuncts.add(kv.toPredicateSql());
            }
            filterPreds.add("(" + Joiner.on(" AND ").join(partitionConjuncts) + ")");
            HdfsPartition targetPartition = hdfsTable.getPartition(partitionSpec_.getPartitionSpecKeyValues());
            List<String> partValues = Lists.newArrayList();
            for (LiteralExpr partValue : targetPartition.getPartitionValues()) {
                partValues.add(PartitionKeyValue.getPartitionKeyValueString(partValue, "NULL"));
            }
            expectedPartitions_.add(partValues);
            for (HdfsPartition p : hdfsTable.getPartitions()) {
                if (p.isDefaultPartition())
                    continue;
                if (p == targetPartition)
                    continue;
                TPartitionStats partStats = p.getPartitionStats();
                if (partStats != null)
                    validPartStats_.add(partStats);
            }
        }
        if (filterPreds.size() == 0 && validPartStats_.size() != 0) {
            LOG.info("No partitions selected for incremental stats update");
            analyzer.addWarning("No partitions selected for incremental stats update");
            return;
        }
    }
    if (filterPreds.size() > MAX_INCREMENTAL_PARTITIONS) {
        // TODO: Consider simply running for MAX_INCREMENTAL_PARTITIONS partitions, and then
        // advising the user to iterate.
        analyzer.addWarning("Too many partitions selected, doing full recomputation of incremental stats");
        filterPreds.clear();
        validPartStats_.clear();
    }
    List<String> groupByCols = Lists.newArrayList();
    List<String> partitionColsSelectList = Lists.newArrayList();
    // Only add group by clause for HdfsTables.
    if (hdfsTable != null) {
        if (hdfsTable.isAvroTable())
            checkIncompleteAvroSchema(hdfsTable);
        addPartitionCols(hdfsTable, partitionColsSelectList, groupByCols);
    }
    // Query for getting the per-partition row count and the total row count.
    StringBuilder tableStatsQueryBuilder = new StringBuilder("SELECT ");
    List<String> tableStatsSelectList = Lists.newArrayList();
    tableStatsSelectList.add("COUNT(*)");
    tableStatsSelectList.addAll(partitionColsSelectList);
    tableStatsQueryBuilder.append(Joiner.on(", ").join(tableStatsSelectList));
    tableStatsQueryBuilder.append(" FROM " + sqlTableName);
    // Query for getting the per-column NDVs and number of NULLs.
    List<String> columnStatsSelectList = getBaseColumnStatsQuerySelectList(analyzer);
    if (isIncremental_)
        columnStatsSelectList.addAll(partitionColsSelectList);
    StringBuilder columnStatsQueryBuilder = new StringBuilder("SELECT ");
    columnStatsQueryBuilder.append(Joiner.on(", ").join(columnStatsSelectList));
    columnStatsQueryBuilder.append(" FROM " + sqlTableName);
    // selected in).
    if (filterPreds.size() > 0 && (validPartStats_.size() > 0 || partitionSpec_ != null)) {
        String filterClause = " WHERE " + Joiner.on(" OR ").join(filterPreds);
        columnStatsQueryBuilder.append(filterClause);
        tableStatsQueryBuilder.append(filterClause);
    }
    if (groupByCols.size() > 0) {
        String groupBy = " GROUP BY " + Joiner.on(", ").join(groupByCols);
        if (isIncremental_)
            columnStatsQueryBuilder.append(groupBy);
        tableStatsQueryBuilder.append(groupBy);
    }
    tableStatsQueryStr_ = tableStatsQueryBuilder.toString();
    LOG.debug("Table stats query: " + tableStatsQueryStr_);
    if (columnStatsSelectList.isEmpty()) {
        // Table doesn't have any columns that we can compute stats for.
        LOG.info("No supported column types in table " + table_.getTableName() + ", no column statistics will be gathered.");
        columnStatsQueryStr_ = null;
        return;
    }
    columnStatsQueryStr_ = columnStatsQueryBuilder.toString();
    LOG.debug("Column stats query: " + columnStatsQueryStr_);
}
#end_block

#method_before
private THdfsTable getTHdfsTable(boolean includeFileDesc, Set<Long> refPartitions) {
    // includeFileDesc implies all partitions should be included (refPartitions == null).
    Preconditions.checkState(!includeFileDesc || refPartitions == null);
    int numPartitions = (refPartitions == null) ? partitionMap_.values().size() : refPartitions.size();
    long statsSizeEstimate = numPartitions * getColumns().size() * STATS_SIZE_PER_COLUMN_BYTES;
    boolean includeIncrementalStats = (statsSizeEstimate < BackendConfig.INSTANCE.getIncStatMaxSize());
    Map<Long, THdfsPartition> idToPartition = Maps.newHashMap();
    for (HdfsPartition partition : partitionMap_.values()) {
        long id = partition.getId();
        if (refPartitions == null || refPartitions.contains(id)) {
            idToPartition.put(id, partition.toThrift(includeFileDesc, includeIncrementalStats));
        }
    }
    THdfsTable hdfsTable = new THdfsTable(hdfsBaseDir_, getColumnNames(), nullPartitionKeyValue_, nullColumnValue_, idToPartition);
    hdfsTable.setAvroSchema(avroSchema_);
    hdfsTable.setMultiple_filesystems(multipleFileSystems_);
    if (includeFileDesc) {
        // Network addresses are used only by THdfsFileBlocks which are inside
        // THdfsFileDesc, so include network addreses only when including THdfsFileDesc.
        hdfsTable.setNetwork_addresses(hostIndex_.getList());
    }
    hdfsTable.setPartition_prefixes(partitionLocationCompressor_.getPrefixes());
    return hdfsTable;
}
#method_after
private THdfsTable getTHdfsTable(boolean includeFileDesc, Set<Long> refPartitions) {
    // includeFileDesc implies all partitions should be included (refPartitions == null).
    Preconditions.checkState(!includeFileDesc || refPartitions == null);
    int numPartitions = (refPartitions == null) ? partitionMap_.values().size() : refPartitions.size();
    long statsSizeEstimate = numPartitions * getColumns().size() * STATS_SIZE_PER_COLUMN_BYTES;
    boolean includeIncrementalStats = (statsSizeEstimate < BackendConfig.INSTANCE.getIncStatsMaxSize());
    Map<Long, THdfsPartition> idToPartition = Maps.newHashMap();
    for (HdfsPartition partition : partitionMap_.values()) {
        long id = partition.getId();
        if (refPartitions == null || refPartitions.contains(id)) {
            idToPartition.put(id, partition.toThrift(includeFileDesc, includeIncrementalStats));
        }
    }
    THdfsTable hdfsTable = new THdfsTable(hdfsBaseDir_, getColumnNames(), nullPartitionKeyValue_, nullColumnValue_, idToPartition);
    hdfsTable.setAvroSchema(avroSchema_);
    hdfsTable.setMultiple_filesystems(multipleFileSystems_);
    if (includeFileDesc) {
        // Network addresses are used only by THdfsFileBlocks which are inside
        // THdfsFileDesc, so include network addreses only when including THdfsFileDesc.
        hdfsTable.setNetwork_addresses(hostIndex_.getList());
    }
    hdfsTable.setPartition_prefixes(partitionLocationCompressor_.getPrefixes());
    return hdfsTable;
}
#end_block

#method_before
@Test
public void TestExtractCommonConjunctsRule() throws AnalysisException {
    ExprRewriteRule rule = ExtractCommonConjunctRule.INSTANCE;
    // One common conjunct: int_col < 10
    RewritesOk("(int_col < 10 and bigint_col < 10) or " + "(string_col = '10' and int_col < 10)", rule, "int_col < 10 AND ((bigint_col < 10) OR (string_col = '10'))");
    // One common conjunct in multiple disjuncts: int_col < 10
    RewritesOk("(int_col < 10 and bigint_col < 10) or " + "(string_col = '10' and int_col < 10) or " + "(id < 20 and int_col < 10) or " + "(int_col < 10 and float_col > 3.14)", rule, "int_col < 10 AND " + "((bigint_col < 10) OR (string_col = '10') OR " + "(id < 20) OR (float_col > 3.14))");
    // Same as above but with a bushy OR tree.
    RewritesOk("((int_col < 10 and bigint_col < 10) or " + " (string_col = '10' and int_col < 10)) or " + "((id < 20 and int_col < 10) or " + " (int_col < 10 and float_col > 3.14))", rule, "int_col < 10 AND " + "((bigint_col < 10) OR (string_col = '10') OR " + "(id < 20) OR (float_col > 3.14))");
    // Multiple common conjuncts: int_col < 10, bool_col is null
    RewritesOk("(int_col < 10 and bigint_col < 10 and bool_col is null) or " + "(bool_col is null and string_col = '10' and int_col < 10)", rule, "int_col < 10 AND bool_col IS NULL AND " + "((bigint_col < 10) OR (string_col = '10'))");
    // Negated common conjunct: !(int_col=5 or tinyint_col > 9)
    RewritesOk("(!(int_col=5 or tinyint_col > 9) and double_col = 7) or " + "(!(int_col=5 or tinyint_col > 9) and double_col = 8)", rule, "NOT (int_col = 5 OR tinyint_col > 9) AND " + "((double_col = 7) OR (double_col = 8))");
    // All conjuncts are common.
    RewritesOk("(int_col < 10 and id between 5 and 6) or " + "(id between 5 and 6 and int_col < 10) or " + "(int_col < 10 and id between 5 and 6)", rule, "int_col < 10 AND id BETWEEN 5 AND 6");
    // Complex disjuncts are redundant.
    RewritesOk("(int_col < 10) or " + "(int_col < 10 and bigint_col < 10 and bool_col is null) or " + "(int_col < 10) or " + "(bool_col is null and int_col < 10)", rule, "int_col < 10");
    // Due to the shape of the original OR tree we are left with redundant
    // disjuncts after the extraction.
    RewritesOk("(int_col < 10 and bigint_col < 10) or " + "(string_col = '10' and int_col < 10) or " + "(id < 20 and int_col < 10) or " + "(int_col < 10 and id < 20)", rule, "int_col < 10 AND " + "((bigint_col < 10) OR (string_col = '10') OR (id < 20) OR (id < 20))");
}
#method_after
@Test
public void TestExtractCommonConjunctsRule() throws AnalysisException {
    ExprRewriteRule rule = ExtractCommonConjunctRule.INSTANCE;
    // One common conjunct: int_col < 10
    RewritesOk("(int_col < 10 and bigint_col < 10) or " + "(string_col = '10' and int_col < 10)", rule, "int_col < 10 AND ((bigint_col < 10) OR (string_col = '10'))");
    // One common conjunct in multiple disjuncts: int_col < 10
    RewritesOk("(int_col < 10 and bigint_col < 10) or " + "(string_col = '10' and int_col < 10) or " + "(id < 20 and int_col < 10) or " + "(int_col < 10 and float_col > 3.14)", rule, "int_col < 10 AND " + "((bigint_col < 10) OR (string_col = '10') OR " + "(id < 20) OR (float_col > 3.14))");
    // Same as above but with a bushy OR tree.
    RewritesOk("((int_col < 10 and bigint_col < 10) or " + " (string_col = '10' and int_col < 10)) or " + "((id < 20 and int_col < 10) or " + " (int_col < 10 and float_col > 3.14))", rule, "int_col < 10 AND " + "((bigint_col < 10) OR (string_col = '10') OR " + "(id < 20) OR (float_col > 3.14))");
    // Multiple common conjuncts: int_col < 10, bool_col is null
    RewritesOk("(int_col < 10 and bigint_col < 10 and bool_col is null) or " + "(bool_col is null and string_col = '10' and int_col < 10)", rule, "int_col < 10 AND bool_col IS NULL AND " + "((bigint_col < 10) OR (string_col = '10'))");
    // Negated common conjunct: !(int_col=5 or tinyint_col > 9)
    RewritesOk("(!(int_col=5 or tinyint_col > 9) and double_col = 7) or " + "(!(int_col=5 or tinyint_col > 9) and double_col = 8)", rule, "NOT (int_col = 5 OR tinyint_col > 9) AND " + "((double_col = 7) OR (double_col = 8))");
    // Test common BetweenPredicate: int_col between 10 and 30
    RewritesOk("(int_col between 10 and 30 and bigint_col < 10) or " + "(string_col = '10' and int_col between 10 and 30) or " + "(id < 20 and int_col between 10 and 30) or " + "(int_col between 10 and 30 and float_col > 3.14)", rule, "int_col BETWEEN 10 AND 30 AND " + "((bigint_col < 10) OR (string_col = '10') OR " + "(id < 20) OR (float_col > 3.14))");
    // Test common NOT BetweenPredicate: int_col not between 10 and 30
    RewritesOk("(int_col not between 10 and 30 and bigint_col < 10) or " + "(string_col = '10' and int_col not between 10 and 30) or " + "(id < 20 and int_col not between 10 and 30) or " + "(int_col not between 10 and 30 and float_col > 3.14)", rule, "int_col NOT BETWEEN 10 AND 30 AND " + "((bigint_col < 10) OR (string_col = '10') OR " + "(id < 20) OR (float_col > 3.14))");
    // Test mixed BetweenPredicates are not common.
    RewritesOk("(int_col not between 10 and 30 and bigint_col < 10) or " + "(string_col = '10' and int_col between 10 and 30) or " + "(id < 20 and int_col not between 10 and 30) or " + "(int_col between 10 and 30 and float_col > 3.14)", rule, null);
    // All conjuncts are common.
    RewritesOk("(int_col < 10 and id between 5 and 6) or " + "(id between 5 and 6 and int_col < 10) or " + "(int_col < 10 and id between 5 and 6)", rule, "int_col < 10 AND id BETWEEN 5 AND 6");
    // Complex disjuncts are redundant.
    RewritesOk("(int_col < 10) or " + "(int_col < 10 and bigint_col < 10 and bool_col is null) or " + "(int_col < 10) or " + "(bool_col is null and int_col < 10)", rule, "int_col < 10");
    // Due to the shape of the original OR tree we are left with redundant
    // disjuncts after the extraction.
    RewritesOk("(int_col < 10 and bigint_col < 10) or " + "(string_col = '10' and int_col < 10) or " + "(id < 20 and int_col < 10) or " + "(int_col < 10 and id < 20)", rule, "int_col < 10 AND " + "((bigint_col < 10) OR (string_col = '10') OR (id < 20) OR (id < 20))");
}
#end_block

#method_before
@Override
public Expr apply(Expr expr, Analyzer analyzer) throws AnalysisException {
    if (!Expr.IS_OR_PREDICATE.apply(expr))
        return expr;
    // Find common conjuncts.
    List<Expr> child0Conjuncts = expr.getChild(0).getConjuncts();
    List<Expr> child1Conjuncts = expr.getChild(1).getConjuncts();
    List<Expr> commonConjuncts = Lists.newArrayList();
    for (Expr conjunct : child0Conjuncts) {
        if (child1Conjuncts.contains(conjunct)) {
            conjunct.setPrintSqlInParens(false);
            commonConjuncts.add(conjunct);
        }
    }
    if (commonConjuncts.isEmpty())
        return expr;
    // Remove common conjuncts.
    child0Conjuncts.removeAll(commonConjuncts);
    child1Conjuncts.removeAll(commonConjuncts);
    // (a AND b AND c) OR (c) ==> c
    if (child0Conjuncts.isEmpty() || child1Conjuncts.isEmpty()) {
        Preconditions.checkState(!commonConjuncts.isEmpty());
        Expr result = CompoundPredicate.createConjunctivePredicate(commonConjuncts);
        result.analyze(analyzer);
        return result;
    }
    // Re-assemble disjunctive predicate.
    List<Expr> newDisjuncts = Lists.newArrayList();
    if (!child0Conjuncts.isEmpty()) {
        Expr newDisjunct = CompoundPredicate.createConjunctivePredicate(child0Conjuncts);
        newDisjunct.setPrintSqlInParens(expr.getChild(0).getPrintSqlInParens());
        newDisjuncts.add(newDisjunct);
    }
    if (!child1Conjuncts.isEmpty()) {
        Expr newDisjunct = CompoundPredicate.createConjunctivePredicate(child1Conjuncts);
        newDisjunct.setPrintSqlInParens(expr.getChild(1).getPrintSqlInParens());
        newDisjuncts.add(newDisjunct);
    }
    Expr newDisjunction = CompoundPredicate.createDisjunctivePredicate(newDisjuncts);
    newDisjunction.setPrintSqlInParens(newDisjuncts.size() > 1);
    Expr result = CompoundPredicate.createConjunction(newDisjunction, CompoundPredicate.createConjunctivePredicate(commonConjuncts));
    result.analyze(analyzer);
    return result;
}
#method_after
@Override
public Expr apply(Expr expr, Analyzer analyzer) throws AnalysisException {
    if (!Expr.IS_OR_PREDICATE.apply(expr))
        return expr;
    // Get childrens' conjuncts and check
    List<Expr> child0Conjuncts = expr.getChild(0).getConjuncts();
    List<Expr> child1Conjuncts = expr.getChild(1).getConjuncts();
    Preconditions.checkState(!child0Conjuncts.isEmpty() && !child1Conjuncts.isEmpty());
    // Impose cost bound.
    if (child0Conjuncts.size() * child1Conjuncts.size() > MAX_EQUALS_COMPARISONS) {
        return expr;
    }
    // Find common conjuncts.
    List<Expr> commonConjuncts = Lists.newArrayList();
    for (Expr conjunct : child0Conjuncts) {
        if (child1Conjuncts.contains(conjunct)) {
            // The conjunct may have parenthesis but there's no need to preserve them.
            // Removing them makes the toSql() easier to read.
            conjunct.setPrintSqlInParens(false);
            commonConjuncts.add(conjunct);
        }
    }
    if (commonConjuncts.isEmpty())
        return expr;
    // Remove common conjuncts.
    child0Conjuncts.removeAll(commonConjuncts);
    child1Conjuncts.removeAll(commonConjuncts);
    // (a AND b AND c) OR (c) ==> c
    if (child0Conjuncts.isEmpty() || child1Conjuncts.isEmpty()) {
        Preconditions.checkState(!commonConjuncts.isEmpty());
        Expr result = CompoundPredicate.createConjunctivePredicate(commonConjuncts);
        result.analyze(analyzer);
        return result;
    }
    // Re-assemble disjunctive predicate.
    Expr child0Disjunct = CompoundPredicate.createConjunctivePredicate(child0Conjuncts);
    child0Disjunct.setPrintSqlInParens(expr.getChild(0).getPrintSqlInParens());
    Expr child1Disjunct = CompoundPredicate.createConjunctivePredicate(child1Conjuncts);
    child1Disjunct.setPrintSqlInParens(expr.getChild(1).getPrintSqlInParens());
    List<Expr> newDisjuncts = Lists.newArrayList(child0Disjunct, child1Disjunct);
    Expr newDisjunction = CompoundPredicate.createDisjunctivePredicate(newDisjuncts);
    newDisjunction.setPrintSqlInParens(true);
    Expr result = CompoundPredicate.createConjunction(newDisjunction, CompoundPredicate.createConjunctivePredicate(commonConjuncts));
    result.analyze(analyzer);
    return result;
}
#end_block

#method_before
@Override
public String getExplainString(String prefix, String detailPrefix, TExplainLevel explainLevel) {
    StringBuilder output = new StringBuilder();
    output.append(prefix + sinkOp_.toExplainString());
    output.append(" KUDU [" + targetTable_.getFullName() + "]\n");
    output.append(detailPrefix);
    if (sinkOp_ == Op.INSERT) {
        output.append("check unique keys: ");
    } else {
        output.append("check keys exist: ");
    }
    output.append("\n");
    if (explainLevel.ordinal() >= TExplainLevel.EXTENDED.ordinal()) {
        output.append(PrintUtils.printHosts(detailPrefix, fragment_.getNumNodes()));
        output.append(PrintUtils.printMemCost(" ", perHostMemCost_));
        output.append("\n");
    }
    return output.toString();
}
#method_after
@Override
public String getExplainString(String prefix, String detailPrefix, TExplainLevel explainLevel) {
    StringBuilder output = new StringBuilder();
    output.append(prefix + sinkOp_.toExplainString());
    output.append(" KUDU [" + targetTable_.getFullName() + "]\n");
    if (explainLevel.ordinal() >= TExplainLevel.EXTENDED.ordinal()) {
        output.append(PrintUtils.printHosts(detailPrefix, fragment_.getNumNodes()));
        output.append(PrintUtils.printMemCost(" ", perHostMemCost_));
        output.append("\n");
    }
    return output.toString();
}
#end_block

#method_before
@Test
public void TestPlanHints() {
    // All plan-hint styles embed a comma-separated list of hints.
    String[][] hintStyles = new String[][] { // traditional commented hint
    new String[] { "/* +", "*/" }, // eol commented hint
    new String[] { "-- +", "\n" }, // eol commented hint
    new String[] { "\n-- +", "\n" }, // legacy style
    new String[] { "[", "]" } };
    String[][] commentStyles = new String[][] { // traditional comment
    new String[] { "/*", "*/" }, // eol comment
    new String[] { "--", "\n" } };
    for (String[] hintStyle : hintStyles) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test join hints.
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b on(a.id = b.id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a cross join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        // Multiple comma-separated hints.
        TestJoinHints(String.format("select * from functional.alltypes a join " + "%sbroadcast,shuffle,foo,bar%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast", "shuffle", "foo", "bar");
        // Test hints in a multi-way join.
        TestJoinHints(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, prefix, suffix, prefix, suffix, prefix, suffix), "broadcast", "shuffle", "broadcast", "shuffle");
        // Test hints in a multi-way join (flipped prefix/suffix -> bad hint start/ends).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, suffix, prefix, prefix, suffix, suffix, prefix));
        // Test hints in a multi-way join (missing prefixes/suffixes).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", suffix, suffix, suffix, suffix, prefix, "", "", ""));
        // Test insert hints.
        TestInsertHints(String.format("insert into t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert overwrite t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t partition(x, y) %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t(a, b) partition(x, y) %sshuffle%s select * from t", prefix, suffix), "shuffle");
        TestInsertHints(String.format("insert overwrite t(a, b) partition(x, y) %sfoo,bar,baz%s select * from t", prefix, suffix), "foo", "bar", "baz");
        // Test TableRef hints.
        TestTableHints(String.format("select * from functional.alltypes %sschedule_disk_local%s", prefix, suffix), "schedule_disk_local");
        TestTableHints(String.format("select * from functional.alltypes %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s" + ", functional.alltypes b %sschedule_remote%s", prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "schedule_remote");
        // Test both TableRef and join hints.
        TestTableAndJoinHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s join %sbroadcast%s functional.alltypes b " + "%sschedule_remote%s using(id)", prefix, suffix, prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "broadcast", "schedule_remote");
        // prefix and suffix.
        if (prefix.contains("[")) {
            prefix = "";
            suffix = "";
        }
        TestSelectListHints(String.format("select %sstraight_join%s * from functional.alltypes a", prefix, suffix), "straight_join");
        // Only the new hint-style is recognized
        if (!prefix.equals("")) {
            TestSelectListHints(String.format("select %sfoo,bar,baz%s * from functional.alltypes a", prefix, suffix), "foo", "bar", "baz");
        }
        if (prefix.isEmpty())
            continue;
        // Test mixing commented hints and comments.
        for (String[] commentStyle : commentStyles) {
            String commentPrefix = commentStyle[0];
            String commentSuffix = commentStyle[1];
            String queryTemplate = "$1comment$2 select $1comment$2 $3straight_join$4 $1comment$2 * " + "from $1comment$2 functional.alltypes a join $1comment$2 $3shuffle$4 " + "$1comment$2 functional.alltypes b $1comment$2 on $1comment$2 " + "(a.id = b.id)";
            String query = queryTemplate.replaceAll("\\$1", commentPrefix).replaceAll("\\$2", commentSuffix).replaceAll("\\$3", prefix).replaceAll("\\$4", suffix);
            TestSelectListHints(query, "straight_join");
            TestJoinHints(query, "shuffle");
        }
    }
    // No "+" at the beginning so the comment is not recognized as a hint.
    TestJoinHints("select * from functional.alltypes a join /* comment */" + "functional.alltypes b using (int_col)", (String) null);
    TestSelectListHints("select /* comment */ * from functional.alltypes", (String) null);
    TestInsertHints("insert into t(a, b) partition(x, y) /* comment */ select 1", (String) null);
    TestSelectListHints("select /* -- +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* abcdef +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- abcdef +straight_join\n * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- /*+straight_join\n * from functional.alltypes", (String) null);
    // Commented hints cannot span lines (recognized as comments instead).
    TestSelectListHints("select /*\n +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_join \n*/ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_\njoin */ * from functional.alltypes", (String) null);
    ParserError("select -- +straight_join * from functional.alltypes");
    ParserError("select \n-- +straight_join * from functional.alltypes");
    // Missing "/*" or "/*"
    ParserError("select * from functional.alltypes a join + */" + "functional.alltypes b using (int_col)");
    ParserError("select * from functional.alltypes a join /* + " + "functional.alltypes b using (int_col)");
    // Test empty hint tokens.
    TestSelectListHints("select /* +straight_join, ,, */ * from functional.alltypes", "straight_join");
    // Traditional commented hints are not parsed inside a comment.
    ParserError("select /* /* +straight_join */ */ * from functional.alltypes");
}
#method_after
@Test
public void TestPlanHints() {
    // All plan-hint styles embed a comma-separated list of hints.
    String[][] hintStyles = new String[][] { // traditional commented hint
    new String[] { "/* +", "*/" }, // eol commented hint
    new String[] { "-- +", "\n" }, // eol commented hint
    new String[] { "\n-- +", "\n" }, // legacy style
    new String[] { "[", "]" } };
    String[][] commentStyles = new String[][] { // traditional comment
    new String[] { "/*", "*/" }, // eol comment
    new String[] { "--", "\n" } };
    for (String[] hintStyle : hintStyles) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test join hints.
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b on(a.id = b.id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a cross join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        // Multiple comma-separated hints.
        TestJoinHints(String.format("select * from functional.alltypes a join " + "%sbroadcast,shuffle,foo,bar%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast", "shuffle", "foo", "bar");
        // Test hints in a multi-way join.
        TestJoinHints(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, prefix, suffix, prefix, suffix, prefix, suffix), "broadcast", "shuffle", "broadcast", "shuffle");
        // Test hints in a multi-way join (flipped prefix/suffix -> bad hint start/ends).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, suffix, prefix, prefix, suffix, suffix, prefix));
        // Test hints in a multi-way join (missing prefixes/suffixes).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", suffix, suffix, suffix, suffix, prefix, "", "", ""));
        // Test insert hints.
        TestInsertHints(String.format("insert into t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert overwrite t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t partition(x, y) %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t(a, b) partition(x, y) %sshuffle%s select * from t", prefix, suffix), "shuffle");
        TestInsertHints(String.format("insert overwrite t(a, b) partition(x, y) %sfoo,bar,baz%s select * from t", prefix, suffix), "foo", "bar", "baz");
        // Test upsert hints.
        ParsesOk(String.format("upsert into t %sshuffle%s select * from t", prefix, suffix));
        ParsesOk(String.format("upsert into t (x, y) %sshuffle%s select * from t", prefix, suffix));
        // Test TableRef hints.
        TestTableHints(String.format("select * from functional.alltypes %sschedule_disk_local%s", prefix, suffix), "schedule_disk_local");
        TestTableHints(String.format("select * from functional.alltypes %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s" + ", functional.alltypes b %sschedule_remote%s", prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "schedule_remote");
        // Test both TableRef and join hints.
        TestTableAndJoinHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s join %sbroadcast%s functional.alltypes b " + "%sschedule_remote%s using(id)", prefix, suffix, prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "broadcast", "schedule_remote");
        // prefix and suffix.
        if (prefix.contains("[")) {
            prefix = "";
            suffix = "";
        }
        TestSelectListHints(String.format("select %sstraight_join%s * from functional.alltypes a", prefix, suffix), "straight_join");
        // Only the new hint-style is recognized
        if (!prefix.equals("")) {
            TestSelectListHints(String.format("select %sfoo,bar,baz%s * from functional.alltypes a", prefix, suffix), "foo", "bar", "baz");
        }
        if (prefix.isEmpty())
            continue;
        // Test mixing commented hints and comments.
        for (String[] commentStyle : commentStyles) {
            String commentPrefix = commentStyle[0];
            String commentSuffix = commentStyle[1];
            String queryTemplate = "$1comment$2 select $1comment$2 $3straight_join$4 $1comment$2 * " + "from $1comment$2 functional.alltypes a join $1comment$2 $3shuffle$4 " + "$1comment$2 functional.alltypes b $1comment$2 on $1comment$2 " + "(a.id = b.id)";
            String query = queryTemplate.replaceAll("\\$1", commentPrefix).replaceAll("\\$2", commentSuffix).replaceAll("\\$3", prefix).replaceAll("\\$4", suffix);
            TestSelectListHints(query, "straight_join");
            TestJoinHints(query, "shuffle");
        }
    }
    // No "+" at the beginning so the comment is not recognized as a hint.
    TestJoinHints("select * from functional.alltypes a join /* comment */" + "functional.alltypes b using (int_col)", (String) null);
    TestSelectListHints("select /* comment */ * from functional.alltypes", (String) null);
    TestInsertHints("insert into t(a, b) partition(x, y) /* comment */ select 1", (String) null);
    TestSelectListHints("select /* -- +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* abcdef +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- abcdef +straight_join\n * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- /*+straight_join\n * from functional.alltypes", (String) null);
    // Commented hints cannot span lines (recognized as comments instead).
    TestSelectListHints("select /*\n +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_join \n*/ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_\njoin */ * from functional.alltypes", (String) null);
    ParserError("select -- +straight_join * from functional.alltypes");
    ParserError("select \n-- +straight_join * from functional.alltypes");
    // Missing "/*" or "/*"
    ParserError("select * from functional.alltypes a join + */" + "functional.alltypes b using (int_col)");
    ParserError("select * from functional.alltypes a join /* + " + "functional.alltypes b using (int_col)");
    // Test empty hint tokens.
    TestSelectListHints("select /* +straight_join, ,, */ * from functional.alltypes", "straight_join");
    // Traditional commented hints are not parsed inside a comment.
    ParserError("select /* /* +straight_join */ */ * from functional.alltypes");
}
#end_block

#method_before
@Test
public void TestUnion() {
    // Single union test.
    ParsesOk("select a from test union select a from test");
    ParsesOk("select a from test union all select a from test");
    ParsesOk("select a from test union distinct select a from test");
    // Chained union test.
    ParsesOk("select a from test union select a from test " + "union select a from test union select a from test");
    ParsesOk("select a from test union all select a from test " + "union all select a from test union all select a from test");
    ParsesOk("select a from test union distinct select a from test " + "union distinct select a from test union distinct select a from test ");
    // Mixed union with all and distinct.
    ParsesOk("select a from test union select a from test " + "union all select a from test union distinct select a from test");
    // No from clause.
    ParsesOk("select sin() union select cos()");
    ParsesOk("select sin() union all select cos()");
    ParsesOk("select sin() union distinct select cos()");
    // All select blocks in parenthesis.
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test)");
    // Union with order by,
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a");
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a nulls first");
    // Union with limit.
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) limit 10");
    // Union with order by, offset and limit.
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a limit 10");
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a " + "nulls first limit 10");
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a " + "nulls first offset 10");
    ParserError("select a from test union (select a from test) " + "union (select a from test) union (select a from test) offset 10");
    // Union with some select blocks in parenthesis, and others not.
    ParsesOk("(select a from test) union select a from test " + "union (select a from test) union select a from test");
    ParsesOk("select a from test union (select a from test) " + "union select a from test union (select a from test)");
    // Union with order by, offset and limit binding to last select.
    ParsesOk("(select a from test) union (select a from test) " + "union select a from test union select a from test order by a limit 10");
    ParsesOk("(select a from test) union (select a from test) " + "union select a from test union select a from test order by a offset 10");
    ParsesOk("(select a from test) union (select a from test) " + "union select a from test union select a from test order by a");
    // Union with order by and limit.
    // Last select with order by and limit is in parenthesis.
    ParsesOk("select a from test union (select a from test) " + "union select a from test union (select a from test order by a limit 10) " + "order by a limit 1");
    ParsesOk("select a from test union (select a from test) " + "union select a from test union (select a from test order by a offset 10) " + "order by a limit 1");
    ParsesOk("select a from test union (select a from test) " + "union select a from test union (select a from test order by a) " + "order by a limit 1");
    // Union with order by, offset in first operand.
    ParsesOk("select a from test order by a union select a from test");
    ParsesOk("select a from test order by a offset 5 union select a from test");
    ParsesOk("select a from test offset 5 union select a from test");
    // Union with order by and limit.
    // Last select with order by and limit is not in parenthesis.
    ParsesOk("select a from test union select a from test " + "union select a from test union select a from test order by a limit 10 " + "order by a limit 1");
    // Nested unions with order by and limit.
    ParsesOk("select a union " + "((select b) union (select c) order by 1 limit 1)");
    ParsesOk("select a union " + "((select b) union " + "  ((select c) union (select d) " + "   order by 1 limit 1) " + " order by 1 limit 1)");
    // Union in insert query.
    ParsesOk("insert into table t select a from test union select a from test");
    ParsesOk("insert into table t select a from test union select a from test " + "union select a from test union select a from test");
    ParsesOk("insert overwrite table t select a from test union select a from test");
    ParsesOk("insert overwrite table t select a from test union select a from test " + "union select a from test union select a from test");
    // No complete select statement on lhs.
    ParserError("a from test union select a from test");
    // No complete select statement on rhs.
    ParserError("select a from test union a from test");
    // Union cannot be a column or table since it's a keyword.
    ParserError("select union from test");
    ParserError("select a from union");
}
#method_after
@Test
public void TestUnion() {
    // Single union test.
    ParsesOk("select a from test union select a from test");
    ParsesOk("select a from test union all select a from test");
    ParsesOk("select a from test union distinct select a from test");
    // Chained union test.
    ParsesOk("select a from test union select a from test " + "union select a from test union select a from test");
    ParsesOk("select a from test union all select a from test " + "union all select a from test union all select a from test");
    ParsesOk("select a from test union distinct select a from test " + "union distinct select a from test union distinct select a from test ");
    // Mixed union with all and distinct.
    ParsesOk("select a from test union select a from test " + "union all select a from test union distinct select a from test");
    // No from clause.
    ParsesOk("select sin() union select cos()");
    ParsesOk("select sin() union all select cos()");
    ParsesOk("select sin() union distinct select cos()");
    // All select blocks in parenthesis.
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test)");
    // Union with order by,
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a");
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a nulls first");
    // Union with limit.
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) limit 10");
    // Union with order by, offset and limit.
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a limit 10");
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a " + "nulls first limit 10");
    ParsesOk("(select a from test) union (select a from test) " + "union (select a from test) union (select a from test) order by a " + "nulls first offset 10");
    ParserError("select a from test union (select a from test) " + "union (select a from test) union (select a from test) offset 10");
    // Union with some select blocks in parenthesis, and others not.
    ParsesOk("(select a from test) union select a from test " + "union (select a from test) union select a from test");
    ParsesOk("select a from test union (select a from test) " + "union select a from test union (select a from test)");
    // Union with order by, offset and limit binding to last select.
    ParsesOk("(select a from test) union (select a from test) " + "union select a from test union select a from test order by a limit 10");
    ParsesOk("(select a from test) union (select a from test) " + "union select a from test union select a from test order by a offset 10");
    ParsesOk("(select a from test) union (select a from test) " + "union select a from test union select a from test order by a");
    // Union with order by and limit.
    // Last select with order by and limit is in parenthesis.
    ParsesOk("select a from test union (select a from test) " + "union select a from test union (select a from test order by a limit 10) " + "order by a limit 1");
    ParsesOk("select a from test union (select a from test) " + "union select a from test union (select a from test order by a offset 10) " + "order by a limit 1");
    ParsesOk("select a from test union (select a from test) " + "union select a from test union (select a from test order by a) " + "order by a limit 1");
    // Union with order by, offset in first operand.
    ParsesOk("select a from test order by a union select a from test");
    ParsesOk("select a from test order by a offset 5 union select a from test");
    ParsesOk("select a from test offset 5 union select a from test");
    // Union with order by and limit.
    // Last select with order by and limit is not in parenthesis.
    ParsesOk("select a from test union select a from test " + "union select a from test union select a from test order by a limit 10 " + "order by a limit 1");
    // Nested unions with order by and limit.
    ParsesOk("select a union " + "((select b) union (select c) order by 1 limit 1)");
    ParsesOk("select a union " + "((select b) union " + "  ((select c) union (select d) " + "   order by 1 limit 1) " + " order by 1 limit 1)");
    // Union in insert query.
    ParsesOk("insert into table t select a from test union select a from test");
    ParsesOk("insert into table t select a from test union select a from test " + "union select a from test union select a from test");
    ParsesOk("insert overwrite table t select a from test union select a from test");
    ParsesOk("insert overwrite table t select a from test union select a from test " + "union select a from test union select a from test");
    // Union in upsert query.
    ParsesOk("upsert into table t select a from test union select a from test");
    ParsesOk("upsert into table t select a from test union select a from test " + "union select a from test union select a from test");
    // No complete select statement on lhs.
    ParserError("a from test union select a from test");
    // No complete select statement on rhs.
    ParserError("select a from test union a from test");
    // Union cannot be a column or table since it's a keyword.
    ParserError("select union from test");
    ParserError("select a from union");
}
#end_block

#method_before
@Test
public void TestValuesStmt() throws AnalysisException {
    // Values stmt with a single row.
    ParsesOk("values(1, 'a', abc, 1.0, *)");
    ParsesOk("select * from (values(1, 'a', abc, 1.0, *)) as t");
    ParsesOk("values(1, 'a', abc, 1.0, *) union all values(1, 'a', abc, 1.0, *)");
    ParsesOk("insert into t values(1, 'a', abc, 1.0, *)");
    // Values stmt with multiple rows.
    ParsesOk("values(1, abc), ('x', cde), (2), (efg, fgh, ghi)");
    ParsesOk("select * from (values(1, abc), ('x', cde), (2), (efg, fgh, ghi)) as t");
    ParsesOk("values(1, abc), ('x', cde), (2), (efg, fgh, ghi) " + "union all values(1, abc), ('x', cde), (2), (efg, fgh, ghi)");
    ParsesOk("insert into t values(1, abc), ('x', cde), (2), (efg, fgh, ghi)");
    // Test additional parenthesis.
    ParsesOk("(values(1, abc), ('x', cde), (2), (efg, fgh, ghi))");
    ParsesOk("values((1, abc), ('x', cde), (2), (efg, fgh, ghi))");
    ParsesOk("(values((1, abc), ('x', cde), (2), (efg, fgh, ghi)))");
    // Test alias inside select list to assign column names.
    ParsesOk("values(1 as x, 2 as y, 3 as z)");
    // Test order by and limit.
    ParsesOk("values(1, 'a') limit 10");
    ParsesOk("values(1, 'a') order by 1");
    ParsesOk("values(1, 'a') order by 1 limit 10");
    ParsesOk("values(1, 'a') order by 1 offset 10");
    ParsesOk("values(1, 'a') offset 10");
    ParsesOk("values(1, 'a'), (2, 'b') order by 1 limit 10");
    ParsesOk("values((1, 'a'), (2, 'b')) order by 1 limit 10");
    ParserError("values()");
    ParserError("values 1, 'a', abc, 1.0");
    ParserError("values(1, 'a') values(1, 'a')");
    ParserError("select values(1, 'a')");
    ParserError("select * from values(1, 'a', abc, 1.0) as t");
    ParserError("values((1, 2, 3), values(1, 2, 3))");
    ParserError("values((1, 'a'), (1, 'a') order by 1)");
    ParserError("values((1, 'a'), (1, 'a') limit 10)");
}
#method_after
@Test
public void TestValuesStmt() throws AnalysisException {
    // Values stmt with a single row.
    ParsesOk("values(1, 'a', abc, 1.0, *)");
    ParsesOk("select * from (values(1, 'a', abc, 1.0, *)) as t");
    ParsesOk("values(1, 'a', abc, 1.0, *) union all values(1, 'a', abc, 1.0, *)");
    ParsesOk("insert into t values(1, 'a', abc, 1.0, *)");
    ParsesOk("upsert into t values(1, 'a', abc, 1.0, *)");
    // Values stmt with multiple rows.
    ParsesOk("values(1, abc), ('x', cde), (2), (efg, fgh, ghi)");
    ParsesOk("select * from (values(1, abc), ('x', cde), (2), (efg, fgh, ghi)) as t");
    ParsesOk("values(1, abc), ('x', cde), (2), (efg, fgh, ghi) " + "union all values(1, abc), ('x', cde), (2), (efg, fgh, ghi)");
    ParsesOk("insert into t values(1, abc), ('x', cde), (2), (efg, fgh, ghi)");
    ParsesOk("upsert into t values(1, abc), ('x', cde), (2), (efg, fgh, ghi)");
    // Test additional parenthesis.
    ParsesOk("(values(1, abc), ('x', cde), (2), (efg, fgh, ghi))");
    ParsesOk("values((1, abc), ('x', cde), (2), (efg, fgh, ghi))");
    ParsesOk("(values((1, abc), ('x', cde), (2), (efg, fgh, ghi)))");
    // Test alias inside select list to assign column names.
    ParsesOk("values(1 as x, 2 as y, 3 as z)");
    // Test order by and limit.
    ParsesOk("values(1, 'a') limit 10");
    ParsesOk("values(1, 'a') order by 1");
    ParsesOk("values(1, 'a') order by 1 limit 10");
    ParsesOk("values(1, 'a') order by 1 offset 10");
    ParsesOk("values(1, 'a') offset 10");
    ParsesOk("values(1, 'a'), (2, 'b') order by 1 limit 10");
    ParsesOk("values((1, 'a'), (2, 'b')) order by 1 limit 10");
    ParserError("values()");
    ParserError("values 1, 'a', abc, 1.0");
    ParserError("values(1, 'a') values(1, 'a')");
    ParserError("select values(1, 'a')");
    ParserError("select * from values(1, 'a', abc, 1.0) as t");
    ParserError("values((1, 2, 3), values(1, 2, 3))");
    ParserError("values((1, 'a'), (1, 'a') order by 1)");
    ParserError("values((1, 'a'), (1, 'a') limit 10)");
}
#end_block

#method_before
@Test
public void TestWithClause() throws AnalysisException {
    ParsesOk("with t as (select 1 as a) select a from t");
    ParsesOk("with t(x) as (select 1 as a) select x from t");
    ParsesOk("with t as (select c from tab) select * from t");
    ParsesOk("with t(x, y) as (select * from tab) select * from t");
    ParsesOk("with t as (values(1, 2, 3), (4, 5, 6)) select * from t");
    ParsesOk("with t(x, y, z) as (values(1, 2, 3), (4, 5, 6)) select * from t");
    ParsesOk("with t1 as (select 1 as a), t2 as (select 2 as a) select a from t1");
    ParsesOk("with t1 as (select c from tab), t2 as (select c from tab)" + "select c from t2");
    ParsesOk("with t1(x) as (select c from tab), t2(x) as (select c from tab)" + "select x from t2");
    // With clause and union statement.
    ParsesOk("with t1 as (select 1 as a), t2 as (select 2 as a)" + "select a from t1 union all select a from t2");
    // With clause and join.
    ParsesOk("with t1 as (select 1 as a), t2 as (select 2 as a)" + "select a from t1 inner join t2 on t1.a = t2.a");
    // With clause in inline view.
    ParsesOk("select * from (with t as (select 1 as a) select * from t) as a");
    ParsesOk("select * from (with t(x) as (select 1 as a) select * from t) as a");
    // With clause in query statement of insert statement.
    ParsesOk("insert into x with t as (select * from tab) select * from t");
    ParsesOk("insert into x with t(x, y) as (select * from tab) select * from t");
    ParsesOk("insert into x with t as (values(1, 2, 3)) select * from t");
    ParsesOk("insert into x with t(x, y) as (values(1, 2, 3)) select * from t");
    // With clause before insert statement.
    ParsesOk("with t as (select 1) insert into x select * from t");
    ParsesOk("with t(x) as (select 1) insert into x select * from t");
    // Test quoted identifier or string literal as table alias.
    ParsesOk("with `t1` as (select 1 a), 't2' as (select 2 a), \"t3\" as (select 3 a)" + "select a from t1 union all select a from t2 union all select a from t3");
    // Multiple with clauses. Operands must be in parenthesis to
    // have their own with clause.
    ParsesOk("with t as (select 1) " + "(with t as (select 2) select * from t) union all " + "(with t as (select 3) select * from t)");
    ParsesOk("with t as (select 1) " + "(with t as (select 2) select * from t) union all " + "(with t as (select 3) select * from t) order by 1 limit 1");
    // Multiple with clauses. One before the insert and one inside the query statement.
    ParsesOk("with t as (select 1) insert into x with t as (select 2) select * from t");
    ParsesOk("with t(c1) as (select 1) " + "insert into x with t(c2) as (select 2) select * from t");
    // Empty with clause.
    ParserError("with t as () select 1");
    ParserError("with t(x) as () select 1");
    // No labels inside parenthesis.
    ParserError("with t() as (select 1 as a) select a from t");
    // Missing select, union or insert statement after with clause.
    ParserError("select * from (with t as (select 1 as a)) as a");
    ParserError("with t as (select 1)");
    // Missing parenthesis around with query statement.
    ParserError("with t as select 1 as a select a from t");
    ParserError("with t as select 1 as a union all select a from t");
    ParserError("with t1 as (select 1 as a), t2 as select 2 as a select a from t");
    ParserError("with t as select 1 as a select a from t");
    // Missing parenthesis around column labels.
    ParserError("with t c1 as (select 1 as a) select c1 from t");
    // Insert in with clause is not valid.
    ParserError("with t as (insert into x select * from tab) select * from t");
    ParserError("with t(c1) as (insert into x select * from tab) select * from t");
    // Union operands need to be parenthesized to have their own with clause.
    ParserError("select * from t union all with t as (select 2) select * from t");
}
#method_after
@Test
public void TestWithClause() throws AnalysisException {
    ParsesOk("with t as (select 1 as a) select a from t");
    ParsesOk("with t(x) as (select 1 as a) select x from t");
    ParsesOk("with t as (select c from tab) select * from t");
    ParsesOk("with t(x, y) as (select * from tab) select * from t");
    ParsesOk("with t as (values(1, 2, 3), (4, 5, 6)) select * from t");
    ParsesOk("with t(x, y, z) as (values(1, 2, 3), (4, 5, 6)) select * from t");
    ParsesOk("with t1 as (select 1 as a), t2 as (select 2 as a) select a from t1");
    ParsesOk("with t1 as (select c from tab), t2 as (select c from tab)" + "select c from t2");
    ParsesOk("with t1(x) as (select c from tab), t2(x) as (select c from tab)" + "select x from t2");
    // With clause and union statement.
    ParsesOk("with t1 as (select 1 as a), t2 as (select 2 as a)" + "select a from t1 union all select a from t2");
    // With clause and join.
    ParsesOk("with t1 as (select 1 as a), t2 as (select 2 as a)" + "select a from t1 inner join t2 on t1.a = t2.a");
    // With clause in inline view.
    ParsesOk("select * from (with t as (select 1 as a) select * from t) as a");
    ParsesOk("select * from (with t(x) as (select 1 as a) select * from t) as a");
    // With clause in query statement of insert statement.
    ParsesOk("insert into x with t as (select * from tab) select * from t");
    ParsesOk("insert into x with t(x, y) as (select * from tab) select * from t");
    ParsesOk("insert into x with t as (values(1, 2, 3)) select * from t");
    ParsesOk("insert into x with t(x, y) as (values(1, 2, 3)) select * from t");
    // With clause before insert statement.
    ParsesOk("with t as (select 1) insert into x select * from t");
    ParsesOk("with t(x) as (select 1) insert into x select * from t");
    // With clause in query statement of upsert statement.
    ParsesOk("upsert into x with t as (select * from tab) select * from t");
    ParsesOk("upsert into x with t(x, y) as (select * from tab) select * from t");
    ParsesOk("upsert into x with t as (values(1, 2, 3)) select * from t");
    ParsesOk("upsert into x with t(x, y) as (values(1, 2, 3)) select * from t");
    // With clause before upsert statement.
    ParsesOk("with t as (select 1) upsert into x select * from t");
    ParsesOk("with t(x) as (select 1) upsert into x select * from t");
    // Test quoted identifier or string literal as table alias.
    ParsesOk("with `t1` as (select 1 a), 't2' as (select 2 a), \"t3\" as (select 3 a)" + "select a from t1 union all select a from t2 union all select a from t3");
    // Multiple with clauses. Operands must be in parenthesis to
    // have their own with clause.
    ParsesOk("with t as (select 1) " + "(with t as (select 2) select * from t) union all " + "(with t as (select 3) select * from t)");
    ParsesOk("with t as (select 1) " + "(with t as (select 2) select * from t) union all " + "(with t as (select 3) select * from t) order by 1 limit 1");
    // Multiple with clauses. One before the insert and one inside the query statement.
    ParsesOk("with t as (select 1) insert into x with t as (select 2) select * from t");
    ParsesOk("with t(c1) as (select 1) " + "insert into x with t(c2) as (select 2) select * from t");
    // Multiple with clauses. One before the upsert and one inside the query statement.
    ParsesOk("with t as (select 1) upsert into x with t as (select 2) select * from t");
    ParsesOk("with t(c1) as (select 1) " + "upsert into x with t(c2) as (select 2) select * from t");
    // Empty with clause.
    ParserError("with t as () select 1");
    ParserError("with t(x) as () select 1");
    // No labels inside parenthesis.
    ParserError("with t() as (select 1 as a) select a from t");
    // Missing select, union or insert statement after with clause.
    ParserError("select * from (with t as (select 1 as a)) as a");
    ParserError("with t as (select 1)");
    // Missing parenthesis around with query statement.
    ParserError("with t as select 1 as a select a from t");
    ParserError("with t as select 1 as a union all select a from t");
    ParserError("with t1 as (select 1 as a), t2 as select 2 as a select a from t");
    ParserError("with t as select 1 as a select a from t");
    // Missing parenthesis around column labels.
    ParserError("with t c1 as (select 1 as a) select c1 from t");
    // Insert in with clause is not valid.
    ParserError("with t as (insert into x select * from tab) select * from t");
    ParserError("with t(c1) as (insert into x select * from tab) select * from t");
    // Upsert in with clause is not valid.
    ParserError("with t as (upsert into x select * from tab) select * from t");
    ParserError("with t(c1) as (upsert into x select * from tab) select * from t");
    // Union operands need to be parenthesized to have their own with clause.
    ParserError("select * from t union all with t as (select 2) select * from t");
}
#end_block

#method_before
@Test
public void TestIdentQuoting() {
    ParsesOk("select a from `t`");
    ParsesOk("select a from `default`.`t`");
    ParsesOk("select a from `default`.t");
    ParsesOk("select a from default.`t`");
    ParsesOk("select 01a from default.`01_t`");
    ParsesOk("select `a` from default.t");
    ParsesOk("select `tbl`.`a` from default.t");
    ParsesOk("select `db`.`tbl`.`a` from default.t");
    ParsesOk("select `12db`.`tbl`.`12_a` from default.t");
    // Make sure quoted float literals are identifiers.
    ParsesOk("select `8e6`", SlotRef.class);
    ParsesOk("select `4.5e2`", SlotRef.class);
    ParsesOk("select `.7e9`", SlotRef.class);
    // Mixed quoting
    ParsesOk("select `db`.tbl.`a` from default.t");
    ParsesOk("select `db.table.a` from default.t");
    // Identifiers consisting of only whitespace not allowed.
    ParserError("select a from ` `");
    ParserError("select a from `    `");
    // Empty quoted identifier doesn't parse.
    ParserError("select a from ``");
    // Whitespace can be interspersed with other characters.
    // Whitespace is trimmed from the beginning and end of an identifier.
    ParsesOk("select a from `a a a    `");
    ParsesOk("select a from `    a a a`");
    ParsesOk("select a from `    a a a    `");
    // Quoted identifiers can contain any characters except "`".
    ParsesOk("select a from `all types`");
    ParsesOk("select a from `default`.`all types`");
    ParsesOk("select a from `~!@#$%^&*()-_=+|;:'\",<.>/?`");
    // Quoted identifiers do not unescape escape sequences.
    ParsesOk("select a from `ab\rabc`");
    ParsesOk("select a from `ab\tabc`");
    ParsesOk("select a from `ab\fabc`");
    ParsesOk("select a from `ab\babc`");
    ParsesOk("select a from `ab\nabc`");
    // Test non-printable control characters inside quoted identifiers.
    ParsesOk("select a from `abc\u0000abc`");
    ParsesOk("select a from `abc\u0019abc`");
    ParsesOk("select a from `abc\u007fabc`");
    // Quoted identifiers can contain keywords.
    ParsesOk("select `select`, `insert` from `table` where `where` = 10");
    // Quoted identifiers cannot contain "`"
    ParserError("select a from `abcde`abcde`");
    ParserError("select a from `abc\u0060abc`");
    // Wrong quotes
    ParserError("select a from 'default'.'t'");
    // Lots of quoting
    ParsesOk("select `db`.`tbl`.`a` from `default`.`t` `alias` where `alias`.`col` = 'string'" + " group by `alias`.`col`");
}
#method_after
@Test
public void TestIdentQuoting() {
    ParsesOk("select a from `t`");
    ParsesOk("select a from `default`.`t`");
    ParsesOk("select a from `default`.t");
    ParsesOk("select a from default.`t`");
    ParsesOk("select 01a from default.`01_t`");
    ParsesOk("select `a` from default.t");
    ParsesOk("select `tbl`.`a` from default.t");
    ParsesOk("select `db`.`tbl`.`a` from default.t");
    ParsesOk("select `12db`.`tbl`.`12_a` from default.t");
    // Make sure quoted float literals are identifiers.
    ParsesOk("select `8e6`", SlotRef.class);
    ParsesOk("select `4.5e2`", SlotRef.class);
    ParsesOk("select `.7e9`", SlotRef.class);
    // Mixed quoting
    ParsesOk("select `db`.tbl.`a` from default.t");
    ParsesOk("select `db.table.a` from default.t");
    // Identifiers consisting of only whitespace not allowed.
    ParserError("select a from ` `");
    ParserError("select a from `    `");
    // Empty quoted identifier doesn't parse.
    ParserError("select a from ``");
    // Whitespace can be interspersed with other characters.
    // Whitespace is trimmed from the beginning and end of an identifier.
    ParsesOk("select a from `a a a    `");
    ParsesOk("select a from `    a a a`");
    ParsesOk("select a from `    a a a    `");
    // Quoted identifiers can contain any characters except "`".
    ParsesOk("select a from `all types`");
    ParsesOk("select a from `default`.`all types`");
    ParsesOk("select a from `~!@#$%^&*()-_=+|;:'\",<.>/?`");
    // Quoted identifiers do not unescape escape sequences.
    ParsesOk("select a from `ab\rabc`");
    ParsesOk("select a from `ab\tabc`");
    ParsesOk("select a from `ab\fabc`");
    ParsesOk("select a from `ab\babc`");
    ParsesOk("select a from `ab\nabc`");
    // Test non-printable control characters inside quoted identifiers.
    ParsesOk("select a from `abc\u0000abc`");
    ParsesOk("select a from `abc\u0019abc`");
    ParsesOk("select a from `abc\u007fabc`");
    // Quoted identifiers can contain keywords.
    ParsesOk("select `select`, `insert`, `upsert` from `table` where `where` = 10");
    // Quoted identifiers cannot contain "`"
    ParserError("select a from `abcde`abcde`");
    ParserError("select a from `abc\u0060abc`");
    // Wrong quotes
    ParserError("select a from 'default'.'t'");
    // Lots of quoting
    ParsesOk("select `db`.`tbl`.`a` from `default`.`t` `alias` where `alias`.`col` = 'string'" + " group by `alias`.`col`");
}
#end_block

#method_before
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, " + "HASH(a) INTO 2 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int, k int) DISTRIBUTE BY HASH INTO 4 BUCKETS," + " HASH(k) INTO 4 BUCKETS");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i)");
    // Range partitioning, the split rows are not validated in the parser
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "SPLIT ROWS ((1, 2.0, 'asdas'))");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE " + "SPLIT ROWS ((1, 2.0, 'asdas'))");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "SPLIT ROWS (('asdas'))");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "SPLIT ROWS ((1, 2.0, 'asdas'), (2,3.0, 'adas'))");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "SPLIT ROWS ()");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i)");
    ParserError("CREATE EXTERNAL TABLE Foo DISTRIBUTE BY HASH INTO 4 BUCKETS");
    // Combine both
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, RANGE(i) " + "SPLIT ROWS ((1, 2.0, 'asdas'))");
    // Can only have one range clause
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, RANGE(i) " + "SPLIT ROWS ((1, 2.0, 'asdas')), RANGE(i) SPLIT ROWS ((1, 2.0, 'asdas'))");
    // Range needs to be the last DISTRIBUTE BY clause
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) SPLIT ROWS ((1),(2)), " + "HASH (i) INTO 3 BUCKETS");
}
#method_after
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo (i int,)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, " + "HASH(a) INTO 2 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int, k int) DISTRIBUTE BY HASH INTO 4 BUCKETS," + " HASH(k) INTO 4 BUCKETS");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo DISTRIBUTE BY HASH INTO 4 BUCKETS");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) DISTRIBUTE BY " + "HASH (a) INTO 3 BUCKETS, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 1 + 1 < VALUES) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY HASH (a) INTO 4 BUCKETS, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) INTO 3 BUCKETS");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUES = 10) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 10 < VALUE < 20) STORED AS KUDU");
}
#end_block

#method_before
@Test
public void TestCreateView() {
    ParsesOk("CREATE VIEW Bar AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Bar COMMENT 'test' AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Bar (x, y, z) AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Bar (x, y COMMENT 'foo', z) AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Bar (x, y, z) COMMENT 'test' AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW IF NOT EXISTS Bar AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar COMMENT 'test' AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar (x, y, z) AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar (x, y, z COMMENT 'foo') AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar (x, y, z) COMMENT 'test' AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW IF NOT EXISTS Foo.Bar AS SELECT a, b, c from t");
    // Test all valid query statements as view definitions.
    ParsesOk("CREATE VIEW Bar AS SELECT 1, 2, 3");
    ParsesOk("CREATE VIEW Bar AS VALUES(1, 2, 3)");
    ParsesOk("CREATE VIEW Bar AS SELECT 1, 2, 3 UNION ALL select 4, 5, 6");
    ParsesOk("CREATE VIEW Bar AS WITH t AS (SELECT 1, 2, 3) SELECT * FROM t");
    // Mismatched number of columns in column definition and view definition parses ok.
    ParsesOk("CREATE VIEW Bar (x, y) AS SELECT 1, 2, 3");
    // No view name.
    ParserError("CREATE VIEW AS SELECT c FROM t");
    // Missing AS keyword
    ParserError("CREATE VIEW Bar SELECT c FROM t");
    // Empty column definition not allowed.
    ParserError("CREATE VIEW Foo.Bar () AS SELECT c FROM t");
    // Column definitions cannot include types.
    ParserError("CREATE VIEW Foo.Bar (x int) AS SELECT c FROM t");
    ParserError("CREATE VIEW Foo.Bar (x int COMMENT 'x') AS SELECT c FROM t");
    // A type does not parse as an identifier.
    ParserError("CREATE VIEW Foo.Bar (int COMMENT 'x') AS SELECT c FROM t");
    // Missing view definition.
    ParserError("CREATE VIEW Foo.Bar (x) AS");
    // Invalid view definitions. A view definition must be a query statement.
    ParserError("CREATE VIEW Foo.Bar (x) AS INSERT INTO t select * from t");
    ParserError("CREATE VIEW Foo.Bar (x) AS CREATE TABLE Wrong (i int)");
    ParserError("CREATE VIEW Foo.Bar (x) AS ALTER TABLE Foo COLUMNS (i int, s string)");
    ParserError("CREATE VIEW Foo.Bar (x) AS CREATE VIEW Foo.Bar AS SELECT 1");
    ParserError("CREATE VIEW Foo.Bar (x) AS ALTER VIEW Foo.Bar AS SELECT 1");
}
#method_after
@Test
public void TestCreateView() {
    ParsesOk("CREATE VIEW Bar AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Bar COMMENT 'test' AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Bar (x, y, z) AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Bar (x, y COMMENT 'foo', z) AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Bar (x, y, z) COMMENT 'test' AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW IF NOT EXISTS Bar AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar COMMENT 'test' AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar (x, y, z) AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar (x, y, z COMMENT 'foo') AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW Foo.Bar (x, y, z) COMMENT 'test' AS SELECT a, b, c from t");
    ParsesOk("CREATE VIEW IF NOT EXISTS Foo.Bar AS SELECT a, b, c from t");
    // Test all valid query statements as view definitions.
    ParsesOk("CREATE VIEW Bar AS SELECT 1, 2, 3");
    ParsesOk("CREATE VIEW Bar AS VALUES(1, 2, 3)");
    ParsesOk("CREATE VIEW Bar AS SELECT 1, 2, 3 UNION ALL select 4, 5, 6");
    ParsesOk("CREATE VIEW Bar AS WITH t AS (SELECT 1, 2, 3) SELECT * FROM t");
    // Mismatched number of columns in column definition and view definition parses ok.
    ParsesOk("CREATE VIEW Bar (x, y) AS SELECT 1, 2, 3");
    // No view name.
    ParserError("CREATE VIEW AS SELECT c FROM t");
    // Missing AS keyword
    ParserError("CREATE VIEW Bar SELECT c FROM t");
    // Empty column definition not allowed.
    ParserError("CREATE VIEW Foo.Bar () AS SELECT c FROM t");
    // Column definitions cannot include types.
    ParserError("CREATE VIEW Foo.Bar (x int) AS SELECT c FROM t");
    ParserError("CREATE VIEW Foo.Bar (x int COMMENT 'x') AS SELECT c FROM t");
    // A type does not parse as an identifier.
    ParserError("CREATE VIEW Foo.Bar (int COMMENT 'x') AS SELECT c FROM t");
    // Missing view definition.
    ParserError("CREATE VIEW Foo.Bar (x) AS");
    // Invalid view definitions. A view definition must be a query statement.
    ParserError("CREATE VIEW Foo.Bar (x) AS INSERT INTO t select * from t");
    ParserError("CREATE VIEW Foo.Bar (x) AS UPSERT INTO t select * from t");
    ParserError("CREATE VIEW Foo.Bar (x) AS CREATE TABLE Wrong (i int)");
    ParserError("CREATE VIEW Foo.Bar (x) AS ALTER TABLE Foo COLUMNS (i int, s string)");
    ParserError("CREATE VIEW Foo.Bar (x) AS CREATE VIEW Foo.Bar AS SELECT 1");
    ParserError("CREATE VIEW Foo.Bar (x) AS ALTER VIEW Foo.Bar AS SELECT 1");
}
#end_block

#method_before
@Test
public void TestAlterView() {
    ParsesOk("ALTER VIEW Bar AS SELECT 1, 2, 3");
    ParsesOk("ALTER VIEW Bar AS SELECT a, b, c FROM t");
    ParsesOk("ALTER VIEW Bar AS VALUES(1, 2, 3)");
    ParsesOk("ALTER VIEW Bar AS SELECT 1, 2, 3 UNION ALL select 4, 5, 6");
    ParsesOk("ALTER VIEW Foo.Bar AS SELECT 1, 2, 3");
    ParsesOk("ALTER VIEW Foo.Bar AS SELECT a, b, c FROM t");
    ParsesOk("ALTER VIEW Foo.Bar AS VALUES(1, 2, 3)");
    ParsesOk("ALTER VIEW Foo.Bar AS SELECT 1, 2, 3 UNION ALL select 4, 5, 6");
    ParsesOk("ALTER VIEW Foo.Bar AS WITH t AS (SELECT 1, 2, 3) SELECT * FROM t");
    // Must be ALTER VIEW not ALTER TABLE.
    ParserError("ALTER TABLE Foo.Bar AS SELECT 1, 2, 3");
    // Missing view name.
    ParserError("ALTER VIEW AS SELECT 1, 2, 3");
    // Missing AS name.
    ParserError("ALTER VIEW Foo.Bar SELECT 1, 2, 3");
    // Missing view definition.
    ParserError("ALTER VIEW Foo.Bar AS");
    // Invalid view definitions. A view definition must be a query statement.
    ParserError("ALTER VIEW Foo.Bar AS INSERT INTO t select * from t");
    ParserError("ALTER VIEW Foo.Bar AS CREATE TABLE Wrong (i int)");
    ParserError("ALTER VIEW Foo.Bar AS ALTER TABLE Foo COLUMNS (i int, s string)");
    ParserError("ALTER VIEW Foo.Bar AS CREATE VIEW Foo.Bar AS SELECT 1, 2, 3");
    ParserError("ALTER VIEW Foo.Bar AS ALTER VIEW Foo.Bar AS SELECT 1, 2, 3");
}
#method_after
@Test
public void TestAlterView() {
    ParsesOk("ALTER VIEW Bar AS SELECT 1, 2, 3");
    ParsesOk("ALTER VIEW Bar AS SELECT a, b, c FROM t");
    ParsesOk("ALTER VIEW Bar AS VALUES(1, 2, 3)");
    ParsesOk("ALTER VIEW Bar AS SELECT 1, 2, 3 UNION ALL select 4, 5, 6");
    ParsesOk("ALTER VIEW Foo.Bar AS SELECT 1, 2, 3");
    ParsesOk("ALTER VIEW Foo.Bar AS SELECT a, b, c FROM t");
    ParsesOk("ALTER VIEW Foo.Bar AS VALUES(1, 2, 3)");
    ParsesOk("ALTER VIEW Foo.Bar AS SELECT 1, 2, 3 UNION ALL select 4, 5, 6");
    ParsesOk("ALTER VIEW Foo.Bar AS WITH t AS (SELECT 1, 2, 3) SELECT * FROM t");
    // Must be ALTER VIEW not ALTER TABLE.
    ParserError("ALTER TABLE Foo.Bar AS SELECT 1, 2, 3");
    // Missing view name.
    ParserError("ALTER VIEW AS SELECT 1, 2, 3");
    // Missing AS name.
    ParserError("ALTER VIEW Foo.Bar SELECT 1, 2, 3");
    // Missing view definition.
    ParserError("ALTER VIEW Foo.Bar AS");
    // Invalid view definitions. A view definition must be a query statement.
    ParserError("ALTER VIEW Foo.Bar AS INSERT INTO t select * from t");
    ParserError("ALTER VIEW Foo.Bar AS UPSERT INTO t select * from t");
    ParserError("ALTER VIEW Foo.Bar AS CREATE TABLE Wrong (i int)");
    ParserError("ALTER VIEW Foo.Bar AS ALTER TABLE Foo COLUMNS (i int, s string)");
    ParserError("ALTER VIEW Foo.Bar AS CREATE VIEW Foo.Bar AS SELECT 1, 2, 3");
    ParserError("ALTER VIEW Foo.Bar AS ALTER VIEW Foo.Bar AS SELECT 1, 2, 3");
}
#end_block

#method_before
@Test
public void TestCreateTableAsSelect() {
    ParsesOk("CREATE TABLE Foo AS SELECT 1, 2, 3");
    ParsesOk("CREATE TABLE Foo AS SELECT * from foo.bar");
    ParsesOk("CREATE TABLE Foo.Bar AS SELECT int_col, bool_col from tbl limit 10");
    ParsesOk("CREATE TABLE Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE Foo STORED AS PARQUET AS SELECT 1");
    ParsesOk("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH INTO 2 BUCKETS " + "AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH (b) INTO 2 " + "BUCKETS AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY RANGE (b) SPLIT ROWS " + "(('foo'), ('bar')) STORED AS KUDU AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY RANGE SPLIT ROWS " + "(('foo'), ('bar')) STORED AS KUDU AS SELECT * from bar");
    // With clause works
    ParsesOk("CREATE TABLE Foo AS with t1 as (select 1) select * from t1");
    // Incomplete AS SELECT statement
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS SELECT");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS WITH");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS");
    // INSERT statements are not allowed
    ParserError("CREATE TABLE Foo AS INSERT INTO Foo SELECT 1");
    // Column and partition definitions not allowed
    ParserError("CREATE TABLE Foo(i int) AS SELECT 1");
    ParserError("CREATE TABLE Foo PARTITIONED BY(i int) AS SELECT 1");
    // Partitioned by syntax following insert into syntax
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) ROW FORMAT DELIMITED STORED AS " + "PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1, 2");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT * from Bar");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a=2, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a, b=2) AS SELECT * from Bar");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (i) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS " + "SELECT 1");
    ParserError("CREATE TABLE Foo DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY HASH(a) INTO 4 BUCKETS " + "TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
}
#method_after
@Test
public void TestCreateTableAsSelect() {
    ParsesOk("CREATE TABLE Foo AS SELECT 1, 2, 3");
    ParsesOk("CREATE TABLE Foo AS SELECT * from foo.bar");
    ParsesOk("CREATE TABLE Foo.Bar AS SELECT int_col, bool_col from tbl limit 10");
    ParsesOk("CREATE TABLE Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE Foo STORED AS PARQUET AS SELECT 1");
    ParsesOk("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH INTO 2 BUCKETS " + "AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH (b) INTO 2 " + "BUCKETS AS SELECT * from bar");
    // With clause works
    ParsesOk("CREATE TABLE Foo AS with t1 as (select 1) select * from t1");
    // Incomplete AS SELECT statement
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS SELECT");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS WITH");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS");
    // INSERT/UPSERT statements are not allowed
    ParserError("CREATE TABLE Foo AS INSERT INTO Foo SELECT 1");
    ParserError("CREATE TABLE Foo AS UPSERT INTO Foo SELECT 1");
    // Column and partition definitions not allowed
    ParserError("CREATE TABLE Foo(i int) AS SELECT 1");
    ParserError("CREATE TABLE Foo PARTITIONED BY(i int) AS SELECT 1");
    // Partitioned by syntax following insert into syntax
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) ROW FORMAT DELIMITED STORED AS " + "PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1, 2");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT * from Bar");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a=2, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a, b=2) AS SELECT * from Bar");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (i) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS " + "SELECT 1");
    ParserError("CREATE TABLE Foo DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY HASH(a) INTO 4 BUCKETS " + "TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY RANGE(a) " + "(PARTITION 1 < VALUES < 10, PARTITION 10 <= VALUES < 20, PARTITION VALUE = 30) " + "STORED AS KUDU AS SELECT * FROM Bar");
}
#end_block

#method_before
@Test
public void TestGetErrorMsg() {
    // missing select
    ParserError("c, b, c from t", "Syntax error in line 1:\n" + "c, b, c from t\n" + "^\n" + "Encountered: IDENTIFIER\n" + "Expected: ALTER, COMPUTE, CREATE, DELETE, DESCRIBE, DROP, EXPLAIN, GRANT, " + "INSERT, INVALIDATE, LOAD, REFRESH, REVOKE, SELECT, SET, SHOW, TRUNCATE, " + "UPDATE, USE, VALUES, WITH\n");
    // missing select list
    ParserError("select from t", "Syntax error in line 1:\n" + "select from t\n" + "       ^\n" + "Encountered: FROM\n" + "Expected: ALL, CASE, CAST, DISTINCT, EXISTS, " + "FALSE, IF, INTERVAL, NOT, NULL, " + "STRAIGHT_JOIN, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing from
    ParserError("select c, b, c where a = 5", "Syntax error in line 1:\n" + "select c, b, c where a = 5\n" + "               ^\n" + "Encountered: WHERE\n" + "Expected: AND, AS, BETWEEN, DIV, FROM, ILIKE, IN, IREGEXP, IS, LIKE, LIMIT, NOT, OR, " + "ORDER, REGEXP, RLIKE, UNION, COMMA, IDENTIFIER\n");
    // missing table list
    ParserError("select c, b, c from where a = 5", "Syntax error in line 1:\n" + "select c, b, c from where a = 5\n" + "                    ^\n" + "Encountered: WHERE\n" + "Expected: IDENTIFIER\n");
    // missing predicate in where clause (no group by)
    ParserError("select c, b, c from t where", "Syntax error in line 1:\n" + "select c, b, c from t where\n" + "                           ^\n" + "Encountered: EOF\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing predicate in where clause (group by)
    ParserError("select c, b, c from t where group by a, b", "Syntax error in line 1:\n" + "select c, b, c from t where group by a, b\n" + "                            ^\n" + "Encountered: GROUP\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // unmatched string literal starting with "
    ParserError("select c, \"b, c from t", "Unmatched string literal in line 1:\n" + "select c, \"b, c from t\n" + "           ^\n");
    // unmatched string literal starting with '
    ParserError("select c, 'b, c from t", "Unmatched string literal in line 1:\n" + "select c, 'b, c from t\n" + "           ^\n");
    // test placement of error indicator ^ on queries with multiple lines
    ParserError("select (i + 5)(1 - i) from t", "Syntax error in line 1:\n" + "select (i + 5)(1 - i) from t\n" + "              ^\n" + "Encountered: (\n" + "Expected:");
    ParserError("select (i + 5)\n(1 - i) from t", "Syntax error in line 2:\n" + "(1 - i) from t\n" + "^\n" + "Encountered: (\n" + "Expected");
    ParserError("select (i + 5)\n(1 - i)\nfrom t", "Syntax error in line 2:\n" + "(1 - i)\n" + "^\n" + "Encountered: (\n" + "Expected");
    // Long line: error in the middle
    ParserError("select c, b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "... b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c,...\n" + "                             ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the start
    ParserError("select a a a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "select a a a, b, c,c,c,c,c,c,c,c,c,c,c,...\n" + "           ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the end
    ParserError("select a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t", "Syntax error in line 1:\n" + "...c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t\n" + "                             ^\n" + "Encountered: COMMA\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // Parsing identifiers that have different names printed as EXPECTED
    ParserError("DROP DATA SRC foo", "Syntax error in line 1:\n" + "DROP DATA SRC foo\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCE\n");
    ParserError("SHOW DATA SRCS", "Syntax error in line 1:\n" + "SHOW DATA SRCS\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCES\n");
    ParserError("USE ` `", "Syntax error in line 1:\n" + "USE ` `\n" + "    ^\n" + "Encountered: EMPTY IDENTIFIER\n" + "Expected: IDENTIFIER\n");
    // Expecting = token
    ParserError("SET foo", "Syntax error in line 1:\n" + "SET foo\n" + "       ^\n" + "Encountered: EOF\n" + "Expected: =\n");
}
#method_after
@Test
public void TestGetErrorMsg() {
    // missing select
    ParserError("c, b, c from t", "Syntax error in line 1:\n" + "c, b, c from t\n" + "^\n" + "Encountered: IDENTIFIER\n" + "Expected: ALTER, COMPUTE, CREATE, DELETE, DESCRIBE, DROP, EXPLAIN, GRANT, " + "INSERT, INVALIDATE, LOAD, REFRESH, REVOKE, SELECT, SET, SHOW, TRUNCATE, " + "UPDATE, UPSERT, USE, VALUES, WITH\n");
    // missing select list
    ParserError("select from t", "Syntax error in line 1:\n" + "select from t\n" + "       ^\n" + "Encountered: FROM\n" + "Expected: ALL, CASE, CAST, DISTINCT, EXISTS, " + "FALSE, IF, INTERVAL, NOT, NULL, " + "STRAIGHT_JOIN, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing from
    ParserError("select c, b, c where a = 5", "Syntax error in line 1:\n" + "select c, b, c where a = 5\n" + "               ^\n" + "Encountered: WHERE\n" + "Expected: AND, AS, BETWEEN, DIV, FROM, ILIKE, IN, IREGEXP, IS, LIKE, LIMIT, NOT, OR, " + "ORDER, REGEXP, RLIKE, UNION, COMMA, IDENTIFIER\n");
    // missing table list
    ParserError("select c, b, c from where a = 5", "Syntax error in line 1:\n" + "select c, b, c from where a = 5\n" + "                    ^\n" + "Encountered: WHERE\n" + "Expected: IDENTIFIER\n");
    // missing predicate in where clause (no group by)
    ParserError("select c, b, c from t where", "Syntax error in line 1:\n" + "select c, b, c from t where\n" + "                           ^\n" + "Encountered: EOF\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // missing predicate in where clause (group by)
    ParserError("select c, b, c from t where group by a, b", "Syntax error in line 1:\n" + "select c, b, c from t where group by a, b\n" + "                            ^\n" + "Encountered: GROUP\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // unmatched string literal starting with "
    ParserError("select c, \"b, c from t", "Unmatched string literal in line 1:\n" + "select c, \"b, c from t\n" + "           ^\n");
    // unmatched string literal starting with '
    ParserError("select c, 'b, c from t", "Unmatched string literal in line 1:\n" + "select c, 'b, c from t\n" + "           ^\n");
    // test placement of error indicator ^ on queries with multiple lines
    ParserError("select (i + 5)(1 - i) from t", "Syntax error in line 1:\n" + "select (i + 5)(1 - i) from t\n" + "              ^\n" + "Encountered: (\n" + "Expected:");
    ParserError("select (i + 5)\n(1 - i) from t", "Syntax error in line 2:\n" + "(1 - i) from t\n" + "^\n" + "Encountered: (\n" + "Expected");
    ParserError("select (i + 5)\n(1 - i)\nfrom t", "Syntax error in line 2:\n" + "(1 - i)\n" + "^\n" + "Encountered: (\n" + "Expected");
    // Long line: error in the middle
    ParserError("select c, b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "... b, c,c,c,c,c,c,c,c,c,a a a,c,c,c,c,c,c,c,cd,c,d,d,,c,...\n" + "                             ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the start
    ParserError("select a a a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d,,c, from t", "Syntax error in line 1:\n" + "select a a a, b, c,c,c,c,c,c,c,c,c,c,c,...\n" + "           ^\n" + "Encountered: IDENTIFIER\n" + "Expected: CROSS, FROM, FULL, GROUP, HAVING, INNER, JOIN, LEFT, LIMIT, OFFSET, " + "ON, ORDER, RIGHT, STRAIGHT_JOIN, UNION, USING, WHERE, COMMA\n");
    // Long line: error close to the end
    ParserError("select a, b, c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t", "Syntax error in line 1:\n" + "...c,c,c,c,c,c,c,c,cd,c,d,d, ,c, from t\n" + "                             ^\n" + "Encountered: COMMA\n" + "Expected: CASE, CAST, EXISTS, FALSE, " + "IF, INTERVAL, NOT, NULL, TRUNCATE, TRUE, IDENTIFIER\n");
    // Parsing identifiers that have different names printed as EXPECTED
    ParserError("DROP DATA SRC foo", "Syntax error in line 1:\n" + "DROP DATA SRC foo\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCE\n");
    ParserError("SHOW DATA SRCS", "Syntax error in line 1:\n" + "SHOW DATA SRCS\n" + "          ^\n" + "Encountered: IDENTIFIER\n" + "Expected: SOURCES\n");
    ParserError("USE ` `", "Syntax error in line 1:\n" + "USE ` `\n" + "    ^\n" + "Encountered: EMPTY IDENTIFIER\n" + "Expected: IDENTIFIER\n");
    // Expecting = token
    ParserError("SET foo", "Syntax error in line 1:\n" + "SET foo\n" + "       ^\n" + "Encountered: EOF\n" + "Expected: =\n");
}
#end_block

#method_before
@Test
public void TestExplain() {
    ParsesOk("explain select a from tbl");
    ParsesOk("explain insert into tbl select a, b, c, d from tbl");
    ParserError("explain");
    // cannot EXPLAIN an explain stmt
    ParserError("explain explain select a from tbl");
    // cannot EXPLAIN DDL stmt
    ParserError("explain CREATE TABLE Foo (i int)");
}
#method_after
@Test
public void TestExplain() {
    ParsesOk("explain select a from tbl");
    ParsesOk("explain insert into tbl select a, b, c, d from tbl");
    ParsesOk("explain upsert into tbl select a, b, c, d from tbl");
    ParserError("explain");
    // cannot EXPLAIN an explain stmt
    ParserError("explain explain select a from tbl");
    // cannot EXPLAIN DDL stmt
    ParserError("explain CREATE TABLE Foo (i int)");
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    hasClusteredHint_ = false;
    resultExprs_.clear();
    primaryKeyExprs_.clear();
}
#method_after
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    hasClusteredHint_ = false;
    resultExprs_.clear();
    mentionedUpsertColumns_.clear();
    primaryKeyExprs_.clear();
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Subqueries need to be rewritten by the StmtRewriter first.
            if (analyzer.containsSubquery())
                return;
            // Use getResultExprs() and not getBaseTblResultExprs() here because the final
            // substitution with TupleIsNullPredicate() wrapping happens in planning.
            selectListExprs = Expr.cloneList(queryStmt_.getResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    setTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    try {
        if (withClause_ != null)
            withClause_.analyze(analyzer);
    } catch (AnalysisException e) {
        // of missing tables can be collected before failing analyze().
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    List<Expr> selectListExprs = null;
    if (!needsGeneratedQueryStatement_) {
        try {
            // Use a child analyzer for the query stmt to properly scope WITH-clause
            // views and to ignore irrelevant ORDER BYs.
            Analyzer queryStmtAnalyzer = new Analyzer(analyzer);
            queryStmt_.analyze(queryStmtAnalyzer);
            // Subqueries need to be rewritten by the StmtRewriter first.
            if (analyzer.containsSubquery())
                return;
            // Use getResultExprs() and not getBaseTblResultExprs() here because the final
            // substitution with TupleIsNullPredicate() wrapping happens in planning.
            selectListExprs = Expr.cloneList(queryStmt_.getResultExprs());
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    } else {
        selectListExprs = Lists.newArrayList();
    }
    // Set target table and perform table-type specific analysis and auth checking.
    // Also checks if the target table is missing.
    analyzeTargetTable(analyzer);
    // Abort analysis if there are any missing tables beyond this point.
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Analysis of the INSERT statement from this point is basically the act of matching
    // the set of output columns (which come from a column permutation, perhaps
    // implicitly, and the PARTITION clause) to the set of input columns (which come from
    // the select-list and any statically-valued columns in the PARTITION clause).
    // 
    // First, we compute the set of mentioned columns, and reject statements that refer to
    // non-existent columns, or duplicates (we must check both the column permutation, and
    // the set of partition keys). Next, we check that all partition columns are
    // mentioned. During this process we build the map from select-list expr index to
    // column in the targeted table.
    // 
    // Then we check that the select-list contains exactly the right number of expressions
    // for all mentioned columns which are not statically-valued partition columns (which
    // get their expressions from partitionKeyValues).
    // 
    // Finally, prepareExpressions analyzes the expressions themselves, and confirms that
    // they are type-compatible with the target columns. Where columns are not mentioned
    // (and by this point, we know that missing columns are not partition columns),
    // prepareExpressions assigns them a NULL literal expressions, unless this is an
    // UPSERT, in which case we don't want to overwrite unmentioned columns with NULL.
    // An null permutation clause is the same as listing all non-partition columns in
    // order.
    List<String> analysisColumnPermutation = columnPermutation_;
    if (analysisColumnPermutation == null) {
        analysisColumnPermutation = Lists.newArrayList();
        ArrayList<Column> tableColumns = table_.getColumns();
        for (int i = numClusteringCols; i < tableColumns.size(); ++i) {
            analysisColumnPermutation.add(tableColumns.get(i).getName());
        }
    }
    // selectExprTargetColumns maps from select expression index to a column in the target
    // table. It will eventually include all mentioned columns that aren't static-valued
    // partition columns.
    ArrayList<Column> selectExprTargetColumns = Lists.newArrayList();
    // Tracks the name of all columns encountered in either the permutation clause or the
    // partition clause to detect duplicates.
    Set<String> mentionedColumnNames = Sets.newHashSet();
    for (String columnName : analysisColumnPermutation) {
        Column column = table_.getColumn(columnName);
        if (column == null) {
            throw new AnalysisException("Unknown column '" + columnName + "' in column permutation");
        }
        if (!mentionedColumnNames.add(columnName)) {
            throw new AnalysisException("Duplicate column '" + columnName + "' in column permutation");
        }
        selectExprTargetColumns.add(column);
    }
    int numStaticPartitionExprs = 0;
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            Column column = table_.getColumn(pkv.getColName());
            if (column == null) {
                throw new AnalysisException("Unknown column '" + pkv.getColName() + "' in partition clause");
            }
            if (column.getPosition() >= numClusteringCols) {
                throw new AnalysisException("Column '" + pkv.getColName() + "' is not a partition column");
            }
            if (!mentionedColumnNames.add(pkv.getColName())) {
                throw new AnalysisException("Duplicate column '" + pkv.getColName() + "' in partition clause");
            }
            if (!pkv.isDynamic()) {
                numStaticPartitionExprs++;
            } else {
                selectExprTargetColumns.add(column);
            }
        }
    }
    // Checks that exactly all columns in the target table are assigned an expr.
    checkColumnCoverage(selectExprTargetColumns, mentionedColumnNames, selectListExprs.size(), numStaticPartitionExprs);
    // Make sure static partition key values only contain const exprs.
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue kv : partitionKeyValues_) {
            kv.analyze(analyzer);
        }
    }
    // Populate partitionKeyExprs from partitionKeyValues and selectExprTargetColumns
    prepareExpressions(selectExprTargetColumns, selectListExprs, table_, analyzer);
    // Analyze plan hints at the end to prefer reporting other error messages first
    // (e.g., the PARTITION clause is not applicable to unpartitioned and HBase tables).
    analyzePlanHints(analyzer);
}
#end_block

#method_before
private void checkColumnCoverage(ArrayList<Column> selectExprTargetColumns, Set<String> mentionedColumnNames, int numSelectListExprs, int numStaticPartitionExprs) throws AnalysisException {
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    // Check that all columns are mentioned by the permutation and partition clauses
    if (selectExprTargetColumns.size() + numStaticPartitionExprs != table_.getColumns().size()) {
        // We've already ruled out too many columns in the permutation and partition clauses
        // by checking that there are no duplicates and that every column mentioned actually
        // exists. So all columns aren't mentioned in the query. If the unmentioned columns
        // include partition columns, this is an error.
        List<String> missingColumnNames = Lists.newArrayList();
        for (Column column : table_.getColumns()) {
            if (!mentionedColumnNames.contains(column.getName())) {
                // otherwise happen by default).
                if (isHBaseTable && column.getPosition() == 0) {
                    throw new AnalysisException("Row-key column '" + column.getName() + "' must be explicitly mentioned in column permutation.");
                }
                if (column.getPosition() < numClusteringCols) {
                    missingColumnNames.add(column.getName());
                }
            }
        }
        if (!missingColumnNames.isEmpty()) {
            throw new AnalysisException("Not enough partition columns mentioned in query. Missing columns are: " + Joiner.on(", ").join(missingColumnNames));
        }
    }
    // Expect the selectListExpr to have entries for every target column
    if (selectExprTargetColumns.size() != numSelectListExprs) {
        String comparator = (selectExprTargetColumns.size() < numSelectListExprs) ? "fewer" : "more";
        String partitionClause = (partitionKeyValues_ == null) ? "returns" : "and PARTITION clause return";
        // select-list and the permutation itself.
        if (columnPermutation_ == null) {
            int totalColumnsMentioned = numSelectListExprs + numStaticPartitionExprs;
            throw new AnalysisException(String.format("Target table '%s' has %s columns (%s) than the SELECT / VALUES clause %s" + " (%s)", table_.getFullName(), comparator, table_.getColumns().size(), partitionClause, totalColumnsMentioned));
        } else {
            String partitionPrefix = (partitionKeyValues_ == null) ? "mentions" : "and PARTITION clause mention";
            throw new AnalysisException(String.format("Column permutation %s %s columns (%s) than " + "the SELECT / VALUES clause %s (%s)", partitionPrefix, comparator, selectExprTargetColumns.size(), partitionClause, numSelectListExprs));
        }
    }
}
#method_after
private void checkColumnCoverage(ArrayList<Column> selectExprTargetColumns, Set<String> mentionedColumnNames, int numSelectListExprs, int numStaticPartitionExprs) throws AnalysisException {
    // Check that all required cols are mentioned by the permutation and partition clauses
    if (selectExprTargetColumns.size() + numStaticPartitionExprs != table_.getColumns().size()) {
        // exists. So all columns aren't mentioned in the query.
        if (table_ instanceof KuduTable) {
            checkRequiredKuduColumns(mentionedColumnNames);
        } else if (table_ instanceof HBaseTable) {
            checkRequiredHBaseColumns(mentionedColumnNames);
        } else if (table_.getNumClusteringCols() > 0) {
            checkRequiredPartitionedColumns(mentionedColumnNames);
        }
    }
    // Expect the selectListExpr to have entries for every target column
    if (selectExprTargetColumns.size() != numSelectListExprs) {
        String comparator = (selectExprTargetColumns.size() < numSelectListExprs) ? "fewer" : "more";
        String partitionClause = (partitionKeyValues_ == null) ? "returns" : "and PARTITION clause return";
        // select-list and the permutation itself.
        if (columnPermutation_ == null) {
            int totalColumnsMentioned = numSelectListExprs + numStaticPartitionExprs;
            throw new AnalysisException(String.format("Target table '%s' has %s columns (%s) than the SELECT / VALUES clause %s" + " (%s)", table_.getFullName(), comparator, table_.getColumns().size(), partitionClause, totalColumnsMentioned));
        } else {
            String partitionPrefix = (partitionKeyValues_ == null) ? "mentions" : "and PARTITION clause mention";
            throw new AnalysisException(String.format("Column permutation %s %s columns (%s) than " + "the SELECT / VALUES clause %s (%s)", partitionPrefix, comparator, selectExprTargetColumns.size(), partitionClause, numSelectListExprs));
        }
    }
}
#end_block

#method_before
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    // order, and add NULL expressions to all missing columns.
    for (Column tblColumn : table_.getColumnsInHiveOrder()) {
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                matchFound = true;
                break;
            }
        }
        // expression.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                // Unmentioned non-clustering columns get NULL literals with the appropriate
                // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                resultExprs_.add(NullLiteral.create(tblColumn.getType()));
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && table_ instanceof KuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#method_after
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    // Finally, 'undo' the permutation so that the selectListExprs are in Hive column
    // order, and add NULL expressions to all missing columns, unless this is an UPSERT.
    ArrayList<Column> columns = table_.getColumnsInHiveOrder();
    for (int col = 0; col < columns.size(); ++col) {
        Column tblColumn = columns.get(col);
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                if (isUpsert_)
                    mentionedUpsertColumns_.add(col);
                matchFound = true;
                break;
            }
        }
        // expression if this is an INSERT.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols && !isUpsert_) {
                // Unmentioned non-clustering columns get NULL literals with the appropriate
                // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                resultExprs_.add(NullLiteral.create(tblColumn.getType()));
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && table_ instanceof KuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#end_block

#method_before
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    return TableSink.create(table_, TableSink.Op.INSERT, partitionKeyExprs_, ImmutableList.<Integer>of(), overwrite_);
}
#method_after
public DataSink createDataSink() {
    // analyze() must have been called before.
    Preconditions.checkState(table_ != null);
    Preconditions.checkState(isUpsert_ || mentionedUpsertColumns_.isEmpty());
    return TableSink.create(table_, isUpsert_ ? TableSink.Op.UPSERT : TableSink.Op.INSERT, partitionKeyExprs_, mentionedUpsertColumns_, overwrite_);
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    if (withClause_ != null)
        strBuilder.append(withClause_.toSql() + " ");
    strBuilder.append("INSERT ");
    if (overwrite_) {
        strBuilder.append("OVERWRITE ");
    } else {
        strBuilder.append("INTO ");
    }
    strBuilder.append("TABLE " + originalTableName_);
    if (columnPermutation_ != null) {
        strBuilder.append("(");
        strBuilder.append(Joiner.on(", ").join(columnPermutation_));
        strBuilder.append(")");
    }
    if (partitionKeyValues_ != null) {
        List<String> values = Lists.newArrayList();
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            values.add(pkv.getColName() + (pkv.getValue() != null ? ("=" + pkv.getValue().toSql()) : ""));
        }
        strBuilder.append(" PARTITION (" + Joiner.on(", ").join(values) + ")");
    }
    if (planHints_ != null) {
        strBuilder.append(" " + ToSqlUtils.getPlanHintsSql(planHints_));
    }
    if (!needsGeneratedQueryStatement_) {
        strBuilder.append(" " + queryStmt_.toSql());
    }
    return strBuilder.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder strBuilder = new StringBuilder();
    if (withClause_ != null)
        strBuilder.append(withClause_.toSql() + " ");
    strBuilder.append(getOpName() + " ");
    if (overwrite_) {
        strBuilder.append("OVERWRITE ");
    } else {
        strBuilder.append("INTO ");
    }
    strBuilder.append("TABLE " + originalTableName_);
    if (columnPermutation_ != null) {
        strBuilder.append("(");
        strBuilder.append(Joiner.on(", ").join(columnPermutation_));
        strBuilder.append(")");
    }
    if (partitionKeyValues_ != null) {
        List<String> values = Lists.newArrayList();
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            values.add(pkv.getColName() + (pkv.getValue() != null ? ("=" + pkv.getValue().toSql()) : ""));
        }
        strBuilder.append(" PARTITION (" + Joiner.on(", ").join(values) + ")");
    }
    if (planHints_ != null) {
        strBuilder.append(" " + ToSqlUtils.getPlanHintsSql(planHints_));
    }
    if (!needsGeneratedQueryStatement_) {
        strBuilder.append(" " + queryStmt_.toSql());
    }
    return strBuilder.toString();
}
#end_block

#method_before
public Deferred<OperationResponse> apply(final Operation operation) throws KuduException {
    Preconditions.checkNotNull(operation, "Can not apply a null operation");
    // Freeze the row so that the client can not concurrently modify it while it is in flight.
    operation.getRow().freeze();
    // If immediate flush mode, send the operation directly.
    if (flushMode == FlushMode.AUTO_FLUSH_SYNC) {
        if (timeoutMs != 0) {
            operation.setTimeoutMillis(timeoutMs);
        }
        operation.setExternalConsistencyMode(this.consistencyMode);
        operation.setIgnoreAllDuplicateRows(ignoreAllDuplicateRows);
        return client.sendRpcToTablet(operation);
    }
    // Kick off a location lookup.
    Deferred<LocatedTablet> tablet = client.getTabletLocation(operation.getTable(), operation.partitionKey(), timeoutMs);
    // Holds a buffer that should be flushed outside the synchronized block, if necessary.
    Buffer fullBuffer = null;
    try {
        synchronized (monitor) {
            if (activeBuffer == null) {
                // is an inactive buffer available to replace as the active.
                if (inactiveBufferAvailable()) {
                    refreshActiveBuffer();
                } else {
                    Status statusServiceUnavailable = Status.ServiceUnavailable("All buffers are currently flushing");
                    // into the second, flushes it, and immediately tries to write again.
                    throw new PleaseThrottleException(statusServiceUnavailable, null, operation, flushNotification.get());
                }
            }
            if (flushMode == FlushMode.MANUAL_FLUSH) {
                if (activeBuffer.getOperations().size() < mutationBufferSpace) {
                    activeBuffer.getOperations().add(new BufferedOperation(tablet, operation));
                } else {
                    Status statusIllegalState = Status.IllegalState("MANUAL_FLUSH is enabled but the buffer is too big");
                    throw new NonRecoverableException(statusIllegalState);
                }
            } else {
                assert flushMode == FlushMode.AUTO_FLUSH_BACKGROUND;
                int activeBufferSize = activeBuffer.getOperations().size();
                if (activeBufferSize >= mutationBufferSpace) {
                    // Save the active buffer into fullBuffer so that it gets flushed when we leave this
                    // synchronized block.
                    fullBuffer = activeBuffer;
                    activeBuffer = null;
                    activeBufferSize = 0;
                    if (inactiveBufferAvailable()) {
                        refreshActiveBuffer();
                    } else {
                        Status statusServiceUnavailable = Status.ServiceUnavailable("All buffers are currently flushing");
                        throw new PleaseThrottleException(statusServiceUnavailable, null, operation, flushNotification.get());
                    }
                }
                if (// low watermark is enabled
                mutationBufferLowWatermark < mutationBufferSpace && // buffer is over low water mark
                activeBufferSize >= mutationBufferLowWatermark && !inactiveBufferAvailable()) {
                    // no inactive buffers
                    // Check if we are over the low water mark.
                    int randomWatermark = activeBufferSize + 1 + randomizer.nextInt(mutationBufferSpace - mutationBufferLowWatermark);
                    if (randomWatermark > mutationBufferSpace) {
                        Status statusServiceUnavailable = Status.ServiceUnavailable("The previous buffer hasn't been flushed and the " + "current buffer is over the low watermark, please retry later");
                        throw new PleaseThrottleException(statusServiceUnavailable, null, operation, flushNotification.get());
                    }
                }
                activeBuffer.getOperations().add(new BufferedOperation(tablet, operation));
                if (activeBufferSize + 1 >= mutationBufferSpace && inactiveBufferAvailable()) {
                    // If the operation filled the buffer, then flush it.
                    Preconditions.checkState(fullBuffer == null);
                    fullBuffer = activeBuffer;
                    activeBuffer = null;
                    activeBufferSize = 0;
                } else if (activeBufferSize == 0) {
                    // If this is the first operation in the buffer, start a background flush timer.
                    client.newTimeout(activeBuffer.getFlusherTask(), interval);
                }
            }
        }
    } finally {
        // Flush the buffer outside of the synchronized block, if required.
        if (fullBuffer != null) {
            doFlush(fullBuffer);
        }
    }
    return operation.getDeferred();
}
#method_after
public Deferred<OperationResponse> apply(final Operation operation) throws KuduException {
    Preconditions.checkNotNull(operation, "Can not apply a null operation");
    // Freeze the row so that the client can not concurrently modify it while it is in flight.
    operation.getRow().freeze();
    // If immediate flush mode, send the operation directly.
    if (flushMode == FlushMode.AUTO_FLUSH_SYNC) {
        if (timeoutMs != 0) {
            operation.setTimeoutMillis(timeoutMs);
        }
        operation.setExternalConsistencyMode(this.consistencyMode);
        operation.setIgnoreAllDuplicateRows(ignoreAllDuplicateRows);
        return client.sendRpcToTablet(operation).addErrback(new SingleOperationErrCallback(operation));
    }
    // Kick off a location lookup.
    Deferred<LocatedTablet> tablet = client.getTabletLocation(operation.getTable(), operation.partitionKey(), timeoutMs);
    // Holds a buffer that should be flushed outside the synchronized block, if necessary.
    Buffer fullBuffer = null;
    try {
        synchronized (monitor) {
            if (activeBuffer == null) {
                // is an inactive buffer available to replace as the active.
                if (inactiveBufferAvailable()) {
                    refreshActiveBuffer();
                } else {
                    Status statusServiceUnavailable = Status.ServiceUnavailable("All buffers are currently flushing");
                    // into the second, flushes it, and immediately tries to write again.
                    throw new PleaseThrottleException(statusServiceUnavailable, null, operation, flushNotification.get());
                }
            }
            if (flushMode == FlushMode.MANUAL_FLUSH) {
                if (activeBuffer.getOperations().size() < mutationBufferSpace) {
                    activeBuffer.getOperations().add(new BufferedOperation(tablet, operation));
                } else {
                    Status statusIllegalState = Status.IllegalState("MANUAL_FLUSH is enabled but the buffer is too big");
                    throw new NonRecoverableException(statusIllegalState);
                }
            } else {
                assert flushMode == FlushMode.AUTO_FLUSH_BACKGROUND;
                int activeBufferSize = activeBuffer.getOperations().size();
                if (activeBufferSize >= mutationBufferSpace) {
                    // Save the active buffer into fullBuffer so that it gets flushed when we leave this
                    // synchronized block.
                    fullBuffer = activeBuffer;
                    activeBuffer = null;
                    activeBufferSize = 0;
                    if (inactiveBufferAvailable()) {
                        refreshActiveBuffer();
                    } else {
                        Status statusServiceUnavailable = Status.ServiceUnavailable("All buffers are currently flushing");
                        throw new PleaseThrottleException(statusServiceUnavailable, null, operation, flushNotification.get());
                    }
                }
                if (// low watermark is enabled
                mutationBufferLowWatermark < mutationBufferSpace && // buffer is over low water mark
                activeBufferSize >= mutationBufferLowWatermark && !inactiveBufferAvailable()) {
                    // no inactive buffers
                    // Check if we are over the low water mark.
                    int randomWatermark = activeBufferSize + 1 + randomizer.nextInt(mutationBufferSpace - mutationBufferLowWatermark);
                    if (randomWatermark > mutationBufferSpace) {
                        Status statusServiceUnavailable = Status.ServiceUnavailable("The previous buffer hasn't been flushed and the " + "current buffer is over the low watermark, please retry later");
                        throw new PleaseThrottleException(statusServiceUnavailable, null, operation, flushNotification.get());
                    }
                }
                activeBuffer.getOperations().add(new BufferedOperation(tablet, operation));
                if (activeBufferSize + 1 >= mutationBufferSpace && inactiveBufferAvailable()) {
                    // If the operation filled the buffer, then flush it.
                    Preconditions.checkState(fullBuffer == null);
                    fullBuffer = activeBuffer;
                    activeBuffer = null;
                    activeBufferSize = 0;
                } else if (activeBufferSize == 0) {
                    // If this is the first operation in the buffer, start a background flush timer.
                    client.newTimeout(activeBuffer.getFlusherTask(), interval);
                }
            }
        }
    } finally {
        // Flush the buffer outside of the synchronized block, if required.
        if (fullBuffer != null) {
            doFlush(fullBuffer);
        }
    }
    return operation.getDeferred();
}
#end_block

#method_before
private void addBatchCallbacks(final Batch request) {
    final class BatchCallback implements Callback<BatchResponse, BatchResponse> {

        public BatchResponse call(final BatchResponse response) {
            LOG.trace("Got a Batch response for {} rows", request.operations.size());
            if (response.getWriteTimestamp() != 0) {
                AsyncKuduSession.this.client.updateLastPropagatedTimestamp(response.getWriteTimestamp());
            }
            // Send individualized responses to all the operations in this batch.
            for (OperationResponse operationResponse : response.getIndividualResponses()) {
                if (flushMode == FlushMode.AUTO_FLUSH_BACKGROUND && operationResponse.hasRowError()) {
                    errorCollector.addError(operationResponse.getRowError());
                }
                // We send the callback second so that the error collector's count becomes visible first,
                // since calling callback will wake up whoever is waiting right away and having the error
                // missing in the collector could be confusing.
                operationResponse.getOperation().callback(operationResponse);
            }
            return response;
        }

        @Override
        public String toString() {
            return "apply batch response";
        }
    }
    final class BatchErrCallback implements Callback<Object, Exception> {

        @Override
        public Object call(Exception e) {
            // If the exception we receive is a KuduException we're going to build OperationResponses.
            Status status = null;
            List<OperationResponse> responses = null;
            if (e instanceof KuduException) {
                status = ((KuduException) e).getStatus();
                responses = new ArrayList<>(request.operations.size());
            }
            for (Operation operation : request.operations) {
                // Same comment as in BatchCallback regarding the ordering of when to callback.
                if (e instanceof KuduException) {
                    RowError rowError = new RowError(status, operation);
                    OperationResponse response = new OperationResponse(0, null, 0, operation, rowError);
                    errorCollector.addError(rowError);
                    responses.add(response);
                    operation.callback(response);
                } else {
                    // We have no idea what the exception is so we'll just send it up.
                    operation.errback(e);
                }
            }
            // passed to ConvertBatchToListOfResponsesCB.
            return e instanceof KuduException ? new BatchResponse(responses) : e;
        }

        @Override
        public String toString() {
            return "apply batch error response";
        }
    }
    request.getDeferred().addCallbacks(new BatchCallback(), new BatchErrCallback());
}
#method_after
private void addBatchCallbacks(final Batch request) {
    final class BatchCallback implements Callback<BatchResponse, BatchResponse> {

        public BatchResponse call(final BatchResponse response) {
            LOG.trace("Got a Batch response for {} rows", request.operations.size());
            if (response.getWriteTimestamp() != 0) {
                AsyncKuduSession.this.client.updateLastPropagatedTimestamp(response.getWriteTimestamp());
            }
            // Send individualized responses to all the operations in this batch.
            for (OperationResponse operationResponse : response.getIndividualResponses()) {
                if (flushMode == FlushMode.AUTO_FLUSH_BACKGROUND && operationResponse.hasRowError()) {
                    errorCollector.addError(operationResponse.getRowError());
                }
                // Fire the callback after collecting the error so that the error is visible should the
                // callback interrogate the error collector.
                operationResponse.getOperation().callback(operationResponse);
            }
            return response;
        }

        @Override
        public String toString() {
            return "apply batch response";
        }
    }
    final class BatchErrCallback implements Callback<Object, Exception> {

        @Override
        public Object call(Exception e) {
            // If the exception we receive is a KuduException we're going to build OperationResponses.
            Status status = null;
            List<OperationResponse> responses = null;
            boolean handleKuduException = e instanceof KuduException;
            if (handleKuduException) {
                status = ((KuduException) e).getStatus();
                responses = new ArrayList<>(request.operations.size());
            }
            for (Operation operation : request.operations) {
                // Same comment as in BatchCallback regarding the ordering of when to callback.
                if (handleKuduException) {
                    RowError rowError = new RowError(status, operation);
                    OperationResponse response = new OperationResponse(0, null, 0, operation, rowError);
                    errorCollector.addError(rowError);
                    responses.add(response);
                    operation.callback(response);
                } else {
                    // We have no idea what the exception is so we'll just send it up.
                    operation.errback(e);
                }
            }
            // passed to ConvertBatchToListOfResponsesCB.
            return handleKuduException ? new BatchResponse(responses) : e;
        }

        @Override
        public String toString() {
            return "apply batch error response";
        }
    }
    request.getDeferred().addCallbacks(new BatchCallback(), new BatchErrCallback());
}
#end_block

#method_before
@Test(timeout = 100000)
public void testBackgroundErrors() throws Exception {
    try {
        AsyncKuduSession session = client.newSession();
        session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_BACKGROUND);
        session.setFlushInterval(10);
        Batch.injectTabletServerErrorAndLatency(getTabletServerError(), 0);
        try {
            OperationResponse resp = session.apply(createInsert(1)).join(DEFAULT_SLEEP);
            assertTrue(resp.hasRowError());
            assertTrue(resp.getRowError().getErrorStatus().getMessage().contains(getTabletServerErrorMessage()));
        } catch (Exception e) {
            fail("Should not throw");
        }
        assertEquals(1, session.countPendingErrors());
    } finally {
        Batch.injectTabletServerErrorAndLatency(null, 0);
    }
}
#method_after
@Test(timeout = 100000)
public void testBackgroundErrors() throws Exception {
    try {
        AsyncKuduSession session = client.newSession();
        session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_BACKGROUND);
        session.setFlushInterval(10);
        Batch.injectTabletServerErrorAndLatency(makeTabletServerError(), 0);
        try {
            OperationResponse resp = session.apply(createInsert(1)).join(DEFAULT_SLEEP);
            assertTrue(resp.hasRowError());
            assertTrue(resp.getRowError().getErrorStatus().getMessage().contains(getTabletServerErrorMessage()));
        } catch (Exception e) {
            fail("Should not throw");
        }
        assertEquals(1, session.countPendingErrors());
    } finally {
        Batch.injectTabletServerErrorAndLatency(null, 0);
    }
}
#end_block

#method_before
@Test(timeout = 100000)
public void testBatchErrorCauseSessionStuck() throws Exception {
    try {
        AsyncKuduSession session = client.newSession();
        session.setFlushMode(AsyncKuduSession.FlushMode.AUTO_FLUSH_BACKGROUND);
        session.setFlushInterval(100);
        Batch.injectTabletServerErrorAndLatency(getTabletServerError(), 200);
        // 0ms: insert first row, which will be the first batch.
        Deferred<OperationResponse> resp1 = session.apply(createInsert(1));
        Thread.sleep(120);
        // 100ms: start to send first batch.
        // 100ms+: first batch got response from ts,
        // will wait 200s and throw error.
        // 120ms: insert another row, which will be the second batch.
        Deferred<OperationResponse> resp2 = session.apply(createInsert(2));
        // 300ms: first batch's callback handles error, retry second batch.
        try {
            OperationResponse resp = resp1.join(DEFAULT_SLEEP);
            assertTrue(resp.hasRowError());
            assertTrue(resp.getRowError().getErrorStatus().getMessage().contains(getTabletServerErrorMessage()));
        } catch (Exception e) {
            fail("Should not throw");
        }
        try {
            OperationResponse resp = resp2.join(DEFAULT_SLEEP);
            assertTrue(resp.hasRowError());
            assertTrue(resp.getRowError().getErrorStatus().getMessage().contains(getTabletServerErrorMessage()));
        } catch (Exception e) {
            fail("Should not throw");
        }
        assertFalse(session.hasPendingOperations());
    } finally {
        Batch.injectTabletServerErrorAndLatency(null, 0);
    }
}
#method_after
@Test(timeout = 100000)
public void testBatchErrorCauseSessionStuck() throws Exception {
    try {
        AsyncKuduSession session = client.newSession();
        session.setFlushMode(AsyncKuduSession.FlushMode.AUTO_FLUSH_BACKGROUND);
        session.setFlushInterval(100);
        Batch.injectTabletServerErrorAndLatency(makeTabletServerError(), 200);
        // 0ms: insert first row, which will be the first batch.
        Deferred<OperationResponse> resp1 = session.apply(createInsert(1));
        Thread.sleep(120);
        // 100ms: start to send first batch.
        // 100ms+: first batch got response from ts,
        // will wait 200s and throw error.
        // 120ms: insert another row, which will be the second batch.
        Deferred<OperationResponse> resp2 = session.apply(createInsert(2));
        // 300ms: first batch's callback handles error, retry second batch.
        try {
            OperationResponse resp = resp1.join(DEFAULT_SLEEP);
            assertTrue(resp.hasRowError());
            assertTrue(resp.getRowError().getErrorStatus().getMessage().contains(getTabletServerErrorMessage()));
        } catch (Exception e) {
            fail("Should not throw");
        }
        try {
            OperationResponse resp = resp2.join(DEFAULT_SLEEP);
            assertTrue(resp.hasRowError());
            assertTrue(resp.getRowError().getErrorStatus().getMessage().contains(getTabletServerErrorMessage()));
        } catch (Exception e) {
            fail("Should not throw");
        }
        assertFalse(session.hasPendingOperations());
    } finally {
        Batch.injectTabletServerErrorAndLatency(null, 0);
    }
}
#end_block

#method_before
@Test(timeout = 100000)
public void testGetTableLocationsErrorCauseSessionStuck() throws Exception {
    AsyncKuduSession session = client.newSession();
    // Make sure tablet locations is cached.
    Insert insert = createInsert(1);
    session.apply(insert).join(DEFAULT_SLEEP);
    RemoteTablet rt = client.getTableLocationEntry(table.getTableId(), insert.partitionKey()).getTablet();
    String tabletId = rt.getTabletId();
    TabletClient tc = client.getTabletClient(rt.getLeaderUUID());
    try {
        // Delete table so we get table not found error.
        client.deleteTable(TABLE_NAME).join();
        // Wait until tablet is deleted on TS.
        while (true) {
            ListTabletsRequest req = new ListTabletsRequest();
            tc.sendRpc(req);
            ListTabletsResponse resp = req.getDeferred().join();
            if (!resp.getTabletsList().contains(tabletId)) {
                break;
            }
            Thread.sleep(100);
        }
        try {
            session.apply(createInsert(1)).join(DEFAULT_SLEEP);
            fail("Insert should not succeed");
        } catch (KuduException e) {
            assertTrue(e.getStatus().isNotFound());
        } catch (Throwable e) {
            fail("Should not throw other error: " + e);
        }
    } finally {
        table = createTable(TABLE_NAME, schema, getBasicCreateTableOptions());
    }
}
#method_after
@Test(timeout = 100000)
public void testGetTableLocationsErrorCauseSessionStuck() throws Exception {
    AsyncKuduSession session = client.newSession();
    // Make sure tablet locations is cached.
    Insert insert = createInsert(1);
    session.apply(insert).join(DEFAULT_SLEEP);
    RemoteTablet rt = client.getTableLocationEntry(table.getTableId(), insert.partitionKey()).getTablet();
    String tabletId = rt.getTabletId();
    TabletClient tc = client.getTabletClient(rt.getLeaderUUID());
    try {
        // Delete table so we get table not found error.
        client.deleteTable(TABLE_NAME).join();
        // Wait until tablet is deleted on TS.
        while (true) {
            ListTabletsRequest req = new ListTabletsRequest();
            tc.sendRpc(req);
            ListTabletsResponse resp = req.getDeferred().join();
            if (!resp.getTabletsList().contains(tabletId)) {
                break;
            }
            Thread.sleep(100);
        }
        OperationResponse response = session.apply(createInsert(1)).join(DEFAULT_SLEEP);
        assertTrue(response.hasRowError());
        assertTrue(response.getRowError().getErrorStatus().isNotFound());
    } finally {
        table = createTable(TABLE_NAME, schema, getBasicCreateTableOptions());
    }
}
#end_block

#method_before
@Test
public void testInsertIntoUnavailableTablet() throws Exception {
    killTabletServers();
    try {
        AsyncKuduSession session = client.newSession();
        session.setTimeoutMillis(1);
        session.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH);
        Insert insert = createInsert(1);
        session.apply(insert);
        try {
            List<OperationResponse> responses = session.flush().join();
            assertEquals(1, responses.size());
            assertTrue(responses.get(0).getRowError().getErrorStatus().isTimedOut());
        } catch (Exception e) {
            fail("Didn't expect an exception " + e);
        }
    } finally {
        restartTabletServers();
    }
}
#method_after
@Test
public void testInsertIntoUnavailableTablet() throws Exception {
    killTabletServers();
    try {
        AsyncKuduSession session = client.newSession();
        session.setTimeoutMillis(1);
        OperationResponse response = session.apply(createInsert(1)).join();
        assertTrue(response.hasRowError());
        assertTrue(response.getRowError().getErrorStatus().isTimedOut());
        session.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH);
        Insert insert = createInsert(1);
        session.apply(insert);
        List<OperationResponse> responses = session.flush().join();
        assertEquals(1, responses.size());
        assertTrue(responses.get(0).getRowError().getErrorStatus().isTimedOut());
    } finally {
        restartTabletServers();
    }
}
#end_block

#method_before
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the distribution schemes
    List<TDistributeParam> distributeParams = params.getDistribute_by();
    if (distributeParams != null) {
        boolean hasRangePartitioning = false;
        for (TDistributeParam distParam : distributeParams) {
            if (distParam.isSetBy_hash_param()) {
                Preconditions.checkState(!distParam.isSetBy_range_param());
                tableOpts.addHashPartitions(distParam.getBy_hash_param().getColumns(), distParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(distParam.isSetBy_range_param());
                hasRangePartitioning = true;
                tableOpts.setRangePartitionColumns(distParam.getBy_range_param().getColumns());
                for (PartialRow partialRow : KuduUtil.parseSplits(schema, distParam.getBy_range_param())) {
                    tableOpts.addSplitRow(partialRow);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        int parsedReplicas = -1;
        try {
            parsedReplicas = Integer.parseInt(replication);
            Preconditions.checkState(parsedReplicas > 0);
        } catch (Exception e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication));
        }
        tableOpts.setNumReplicas(parsedReplicas);
    }
    return tableOpts;
}
#method_after
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the distribution schemes
    List<TDistributeParam> distributeParams = params.getDistribute_by();
    if (distributeParams != null) {
        boolean hasRangePartitioning = false;
        for (TDistributeParam distParam : distributeParams) {
            if (distParam.isSetBy_hash_param()) {
                Preconditions.checkState(!distParam.isSetBy_range_param());
                tableOpts.addHashPartitions(distParam.getBy_hash_param().getColumns(), distParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(distParam.isSetBy_range_param());
                hasRangePartitioning = true;
                List<String> rangePartitionColumns = distParam.getBy_range_param().getColumns();
                tableOpts.setRangePartitionColumns(rangePartitionColumns);
                for (TRangePartition rangePartition : distParam.getBy_range_param().getRange_partitions()) {
                    Preconditions.checkState(rangePartition.isSetLower_bound_values() || rangePartition.isSetUpper_bound_values());
                    Pair<PartialRow, RangePartitionBound> lowerBound = KuduUtil.buildRangePartitionBound(schema, rangePartitionColumns, rangePartition.getLower_bound_values(), rangePartition.isIs_lower_bound_inclusive());
                    Pair<PartialRow, RangePartitionBound> upperBound = KuduUtil.buildRangePartitionBound(schema, rangePartitionColumns, rangePartition.getUpper_bound_values(), rangePartition.isIs_upper_bound_inclusive());
                    tableOpts.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        int parsedReplicas = -1;
        try {
            parsedReplicas = Integer.parseInt(replication);
            Preconditions.checkState(parsedReplicas > 0, "Invalid number of replicas table property:" + replication);
        } catch (Exception e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication));
        }
        tableOpts.setNumReplicas(parsedReplicas);
    }
    return tableOpts;
}
#end_block

#method_before
public static void validateKuduTblExists(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    Preconditions.checkState(!Strings.isNullOrEmpty(masterHosts));
    String kuduTableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        kudu.tableExists(kuduTableName);
    } catch (Exception e) {
        // TODO: This is misleading when there are other errors, e.g. timeouts.
        throw new ImpalaRuntimeException(String.format("Kudu table '%s' does not exist " + "on master '%s'", kuduTableName, masterHosts), e);
    }
}
#method_after
public static void validateKuduTblExists(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    Preconditions.checkArgument(KuduTable.isKuduTable(msTbl));
    Map<String, String> properties = msTbl.getParameters();
    String masterHosts = properties.get(KuduTable.KEY_MASTER_HOSTS);
    Preconditions.checkState(!Strings.isNullOrEmpty(masterHosts));
    String kuduTableName = properties.get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        kudu.tableExists(kuduTableName);
    } catch (Exception e) {
        // TODO: This is misleading when there are other errors, e.g. timeouts.
        throw new ImpalaRuntimeException(String.format("Kudu table '%s' does not exist " + "on master '%s'", kuduTableName, masterHosts), e);
    }
}
#end_block

#method_before
public static KuduClient createKuduClient(String kuduMasters) {
    KuduClientBuilder b = new KuduClient.KuduClientBuilder(kuduMasters);
    b.defaultAdminOperationTimeoutMs(BackendConfig.getKuduClientTimeoutMs());
    b.defaultOperationTimeoutMs(BackendConfig.getKuduClientTimeoutMs());
    b.defaultSocketReadTimeoutMs(BackendConfig.getKuduClientTimeoutMs());
    return b.build();
}
#method_after
public static KuduClient createKuduClient(String kuduMasters) {
    KuduClientBuilder b = new KuduClient.KuduClientBuilder(kuduMasters);
    b.defaultAdminOperationTimeoutMs(BackendConfig.getKuduClientTimeoutMs());
    b.defaultOperationTimeoutMs(BackendConfig.getKuduClientTimeoutMs());
    return b.build();
}
#end_block

#method_before
private static void setKey(org.apache.kudu.Type type, TRangeLiteral literal, int pos, String colName, PartialRow key) throws ImpalaRuntimeException {
    switch(type) {
        case INT8:
            checkCorrectType(literal.isSetInt_literal(), type, colName, literal);
            key.addByte(pos, (byte) literal.getInt_literal());
            break;
        case INT16:
            checkCorrectType(literal.isSetInt_literal(), type, colName, literal);
            key.addShort(pos, (short) literal.getInt_literal());
            break;
        case INT32:
            checkCorrectType(literal.isSetInt_literal(), type, colName, literal);
            key.addInt(pos, (int) literal.getInt_literal());
            break;
        case INT64:
            checkCorrectType(literal.isSetInt_literal(), type, colName, literal);
            key.addLong(pos, literal.getInt_literal());
            break;
        case STRING:
            checkCorrectType(literal.isSetString_literal(), type, colName, literal);
            key.addString(pos, literal.getString_literal());
            break;
        default:
            throw new ImpalaRuntimeException("Key columns not supported for type: " + type.toString());
    }
}
#method_after
private static void setKey(org.apache.kudu.Type type, TExpr boundaryVal, int pos, String colName, PartialRow key) throws ImpalaRuntimeException {
    Preconditions.checkState(boundaryVal.getNodes().size() == 1);
    TExprNode literal = boundaryVal.getNodes().get(0);
    switch(type) {
        case INT8:
            checkCorrectType(literal.isSetInt_literal(), type, colName, literal);
            key.addByte(pos, (byte) literal.getInt_literal().getValue());
            break;
        case INT16:
            checkCorrectType(literal.isSetInt_literal(), type, colName, literal);
            key.addShort(pos, (short) literal.getInt_literal().getValue());
            break;
        case INT32:
            checkCorrectType(literal.isSetInt_literal(), type, colName, literal);
            key.addInt(pos, (int) literal.getInt_literal().getValue());
            break;
        case INT64:
            checkCorrectType(literal.isSetInt_literal(), type, colName, literal);
            key.addLong(pos, literal.getInt_literal().getValue());
            break;
        case STRING:
            checkCorrectType(literal.isSetString_literal(), type, colName, literal);
            key.addString(pos, literal.getString_literal().getValue());
            break;
        default:
            throw new ImpalaRuntimeException("Key columns not supported for type: " + type.toString());
    }
}
#end_block

#method_before
private static void checkCorrectType(boolean correctType, org.apache.kudu.Type t, String colName, TRangeLiteral literal) throws ImpalaRuntimeException {
    if (correctType)
        return;
    throw new ImpalaRuntimeException(format("Expected %s literal for column '%s' got '%s'", t.getName(), colName, toString(literal)));
}
#method_after
private static void checkCorrectType(boolean correctType, org.apache.kudu.Type t, String colName, TExprNode boundaryVal) throws ImpalaRuntimeException {
    if (correctType)
        return;
    throw new ImpalaRuntimeException(format("Expected '%s' literal for column '%s' got '%s'", t.getName(), colName, Type.fromThrift(boundaryVal.getType()).toSql()));
}
#end_block

#method_before
public static org.apache.kudu.Type fromImpalaType(Type t) throws ImpalaRuntimeException {
    if (!t.isScalarType()) {
        throw new ImpalaRuntimeException(format("Non-scalar type %s is not supported in Kudu", t.toSql()));
    }
    ScalarType s = (ScalarType) t;
    switch(s.getPrimitiveType()) {
        case TINYINT:
            return org.apache.kudu.Type.INT8;
        case SMALLINT:
            return org.apache.kudu.Type.INT16;
        case INT:
            return org.apache.kudu.Type.INT32;
        case BIGINT:
            return org.apache.kudu.Type.INT64;
        case BOOLEAN:
            return org.apache.kudu.Type.BOOL;
        case CHAR:
            return org.apache.kudu.Type.STRING;
        case STRING:
            return org.apache.kudu.Type.STRING;
        case VARCHAR:
            return org.apache.kudu.Type.STRING;
        case DOUBLE:
            return org.apache.kudu.Type.DOUBLE;
        case FLOAT:
            return org.apache.kudu.Type.FLOAT;
        /* Fall through below */
        case INVALID_TYPE:
        case NULL_TYPE:
        case TIMESTAMP:
        case BINARY:
        case DATE:
        case DATETIME:
        case DECIMAL:
        default:
            throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", s.toSql()));
    }
}
#method_after
public static org.apache.kudu.Type fromImpalaType(Type t) throws ImpalaRuntimeException {
    if (!t.isScalarType()) {
        throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", t.toSql()));
    }
    ScalarType s = (ScalarType) t;
    switch(s.getPrimitiveType()) {
        case TINYINT:
            return org.apache.kudu.Type.INT8;
        case SMALLINT:
            return org.apache.kudu.Type.INT16;
        case INT:
            return org.apache.kudu.Type.INT32;
        case BIGINT:
            return org.apache.kudu.Type.INT64;
        case BOOLEAN:
            return org.apache.kudu.Type.BOOL;
        case STRING:
            return org.apache.kudu.Type.STRING;
        case DOUBLE:
            return org.apache.kudu.Type.DOUBLE;
        case FLOAT:
            return org.apache.kudu.Type.FLOAT;
        /* Fall through below */
        case INVALID_TYPE:
        case NULL_TYPE:
        case TIMESTAMP:
        case BINARY:
        case DATE:
        case DATETIME:
        case DECIMAL:
        case CHAR:
        case VARCHAR:
        default:
            throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", s.toSql()));
    }
}
#end_block

#method_before
public static Type toImpalaType(org.apache.kudu.Type t) throws ImpalaRuntimeException {
    switch(t) {
        case BOOL:
            return Type.BOOLEAN;
        case DOUBLE:
            return Type.DOUBLE;
        case FLOAT:
            return Type.FLOAT;
        case INT8:
            return Type.TINYINT;
        case INT16:
            return Type.SMALLINT;
        case INT32:
            return Type.INT;
        case INT64:
            return Type.BIGINT;
        case STRING:
            return Type.STRING;
        default:
            throw new ImpalaRuntimeException(String.format("Kudu type %s is not supported in Impala", t));
    }
}
#method_after
public static Type toImpalaType(org.apache.kudu.Type t) throws ImpalaRuntimeException {
    switch(t) {
        case BOOL:
            return Type.BOOLEAN;
        case DOUBLE:
            return Type.DOUBLE;
        case FLOAT:
            return Type.FLOAT;
        case INT8:
            return Type.TINYINT;
        case INT16:
            return Type.SMALLINT;
        case INT32:
            return Type.INT;
        case INT64:
            return Type.BIGINT;
        case STRING:
            return Type.STRING;
        default:
            throw new ImpalaRuntimeException(String.format("Kudu type '%s' is not supported in Impala", t.getName()));
    }
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = KuduUtil.createKuduClient(kuduTable_.getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeMemLayout(analyzer);
    computeStats(analyzer);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = KuduUtil.createKuduClient(kuduTable_.getKuduMasterHosts())) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Compute mem layout before the scan range locations because creation of the Kudu
        // scan tokens depends on having a mem layout.
        computeMemLayout(analyzer);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeStats(analyzer);
}
#end_block

#method_before
private void computeScanRangeLocations(Analyzer analyzer, KuduClient client, org.apache.kudu.client.KuduTable rpcTable) throws ImpalaRuntimeException {
    scanRanges_ = Lists.newArrayList();
    List<KuduScanToken> scanTokens = createScanTokens(client, rpcTable);
    for (KuduScanToken token : scanTokens) {
        LocatedTablet tablet = token.getTablet();
        List<TScanRangeLocation> locations = Lists.newArrayList();
        if (tablet.getReplicas().isEmpty()) {
            throw new ImpalaRuntimeException(String.format("At least one tablet does not have any replicas. Tablet ID: %s", new String(tablet.getTabletId(), Charsets.UTF_8)));
        }
        for (LocatedTablet.Replica replica : tablet.getReplicas()) {
            TNetworkAddress address = new TNetworkAddress(replica.getRpcHost(), replica.getRpcPort());
            // Use the network address to look up the host in the global list
            Integer hostIndex = analyzer.getHostIndex().getIndex(address);
            locations.add(new TScanRangeLocation(hostIndex));
            hostIndexSet_.add(hostIndex);
        }
        TScanRange scanRange = new TScanRange();
        try {
            scanRange.setKudu_scan_token(token.serialize());
        } catch (IOException e) {
            throw new ImpalaRuntimeException("Unable to serialize Kudu scan token=" + token.toString(), e);
        }
        TScanRangeLocations locs = new TScanRangeLocations();
        locs.setScan_range(scanRange);
        locs.locations = locations;
        scanRanges_.add(locs);
    }
}
#method_after
private void computeScanRangeLocations(Analyzer analyzer, KuduClient client, org.apache.kudu.client.KuduTable rpcTable) throws ImpalaRuntimeException {
    scanRanges_ = Lists.newArrayList();
    List<KuduScanToken> scanTokens = createScanTokens(client, rpcTable);
    for (KuduScanToken token : scanTokens) {
        LocatedTablet tablet = token.getTablet();
        List<TScanRangeLocation> locations = Lists.newArrayList();
        if (tablet.getReplicas().isEmpty()) {
            throw new ImpalaRuntimeException(String.format("At least one tablet does not have any replicas. Tablet ID: %s", new String(tablet.getTabletId(), Charsets.UTF_8)));
        }
        for (LocatedTablet.Replica replica : tablet.getReplicas()) {
            TNetworkAddress address = new TNetworkAddress(replica.getRpcHost(), replica.getRpcPort());
            // Use the network address to look up the host in the global list
            Integer hostIndex = analyzer.getHostIndex().getIndex(address);
            locations.add(new TScanRangeLocation(hostIndex));
            hostIndexSet_.add(hostIndex);
        }
        TScanRange scanRange = new TScanRange();
        try {
            scanRange.setKudu_scan_token(token.serialize());
        } catch (IOException e) {
            throw new ImpalaRuntimeException("Unable to serialize Kudu scan token=" + token.toString(), e);
        }
        TScanRangeLocationList locs = new TScanRangeLocationList();
        locs.setScan_range(scanRange);
        locs.locations = locations;
        scanRanges_.add(locs);
    }
}
#end_block

#method_before
private List<KuduScanToken> createScanTokens(KuduClient client, org.apache.kudu.client.KuduTable rpcTable) {
    List<String> projectedCols = Lists.newArrayList();
    for (SlotDescriptor desc : getTupleDesc().getSlots()) {
        if (desc.isMaterialized())
            projectedCols.add(desc.getColumn().getName());
    }
    KuduScanTokenBuilder tokenBuilder = client.newScanTokenBuilder(rpcTable);
    tokenBuilder.setProjectedColumnNames(projectedCols);
    for (KuduPredicate predicate : kuduPredicates_) tokenBuilder.addPredicate(predicate);
    return tokenBuilder.build();
}
#method_after
private List<KuduScanToken> createScanTokens(KuduClient client, org.apache.kudu.client.KuduTable rpcTable) {
    List<String> projectedCols = Lists.newArrayList();
    for (SlotDescriptor desc : getTupleDesc().getSlotsOrderedByOffset()) {
        projectedCols.add(desc.getColumn().getName());
    }
    KuduScanTokenBuilder tokenBuilder = client.newScanTokenBuilder(rpcTable);
    tokenBuilder.setProjectedColumnNames(projectedCols);
    for (KuduPredicate predicate : kuduPredicates_) tokenBuilder.addPredicate(predicate);
    return tokenBuilder.build();
}
#end_block

#method_before
public static int getKuduClientTimeoutMs() {
    return kuduClientTimeoutMs_;
}
#method_after
public static int getKuduClientTimeoutMs() {
    return kuduOperationTimeoutMs_;
}
#end_block

#method_before
public static void setKuduClientTimeoutMs(int kuduClientTimeoutMs) {
    Preconditions.checkArgument(kuduClientTimeoutMs > 0);
    BackendConfig.kuduClientTimeoutMs_ = kuduClientTimeoutMs;
}
#method_after
public static void setKuduClientTimeoutMs(int kuduOperationTimeoutMs) {
    Preconditions.checkArgument(kuduOperationTimeoutMs > 0);
    BackendConfig.kuduOperationTimeoutMs_ = kuduOperationTimeoutMs;
}
#end_block

#method_before
void addTrace(RpcTraceObject rpcTraceObject) {
    if (parentRpc != null) {
        parentRpc.addTrace(rpcTraceObject);
    }
    if (traces.size() == MAX_TRACES_SIZE) {
        // Add a last trace that indicates that we've reached the max size.
        traces.add(new RpcTraceObject.RpcTraceObjectBuilder(this.method(), RpcTraceObject.Action.TRACE_TRUNCATED).build());
    } else if (traces.size() < MAX_TRACES_SIZE) {
        traces.add(rpcTraceObject);
    }
}
#method_after
void addTrace(RpcTraceFrame rpcTraceFrame) {
    if (parentRpc != null) {
        parentRpc.addTrace(rpcTraceFrame);
    }
    if (traces.size() == MAX_TRACES_SIZE) {
        // Add a last trace that indicates that we've reached the max size.
        traces.add(new RpcTraceFrame.RpcTraceFrameBuilder(this.method(), RpcTraceFrame.Action.TRACE_TRUNCATED).build());
    } else if (traces.size() < MAX_TRACES_SIZE) {
        traces.add(rpcTraceFrame);
    }
}
#end_block

#method_before
List<RpcTraceObject> getImmutableTraces() {
    return ImmutableList.copyOf(traces);
}
#method_after
List<RpcTraceFrame> getImmutableTraces() {
    return ImmutableList.copyOf(traces);
}
#end_block

#method_before
public String toString() {
    final StringBuilder buf = new StringBuilder();
    buf.append("KuduRpc(method=");
    buf.append(method());
    buf.append(", tablet=");
    if (tablet == null) {
        buf.append("null");
    } else {
        buf.append(tablet.getTabletId());
    }
    buf.append(", attempt=").append(attempt);
    buf.append(", ").append(deadlineTracker);
    buf.append(", ").append(RpcTraceObject.getHumanReadableStringForTraces(traces));
    // this method if DEBUG is enabled.
    if (LOG.isDebugEnabled()) {
        buf.append(", ").append(deferred);
    }
    buf.append(')');
    return buf.toString();
}
#method_after
public String toString() {
    final StringBuilder buf = new StringBuilder();
    buf.append("KuduRpc(method=");
    buf.append(method());
    buf.append(", tablet=");
    if (tablet == null) {
        buf.append("null");
    } else {
        buf.append(tablet.getTabletId());
    }
    buf.append(", attempt=").append(attempt);
    buf.append(", ").append(deadlineTracker);
    buf.append(", ").append(RpcTraceFrame.getHumanReadableStringForTraces(traces));
    // this method if DEBUG is enabled.
    if (LOG.isDebugEnabled()) {
        buf.append(", ").append(deferred);
    }
    buf.append(')');
    return buf.toString();
}
#end_block

#method_before
Deferred<AsyncKuduScanner.Response> scanNextRows(final AsyncKuduScanner scanner) {
    RemoteTablet tablet = scanner.currentTablet();
    assert (tablet != null);
    TabletClient client = connectionCache.getClient(tablet.getLeaderUUID());
    KuduRpc<AsyncKuduScanner.Response> nextRequest = scanner.getNextRowsRequest();
    Deferred<AsyncKuduScanner.Response> d = nextRequest.getDeferred();
    // Important to increment the attempts before the next if statement since
    // getSleepTimeForRpc() relies on it if the client is null or dead.
    nextRequest.attempt++;
    if (client == null || !client.isAlive()) {
        // We'll first delay the RPC in case things take some time to settle down, then retry.
        return delayedSendRpcToTablet(nextRequest, null);
    }
    client.sendRpc(nextRequest);
    return d;
}
#method_after
Deferred<AsyncKuduScanner.Response> scanNextRows(final AsyncKuduScanner scanner) {
    RemoteTablet tablet = scanner.currentTablet();
    assert (tablet != null);
    KuduRpc<AsyncKuduScanner.Response> nextRequest = scanner.getNextRowsRequest();
    String uuid = tablet.getReplicaSelectedUUID(nextRequest.getReplicaSelection());
    TabletClient client = connectionCache.getClient(uuid);
    Deferred<AsyncKuduScanner.Response> d = nextRequest.getDeferred();
    // Important to increment the attempts before the next if statement since
    // getSleepTimeForRpc() relies on it if the client is null or dead.
    nextRequest.attempt++;
    if (client == null || !client.isAlive()) {
        // A null client means we either don't know about this tablet anymore (unlikely) or we
        // couldn't find a leader (which could be triggered by a read timeout).
        // We'll first delay the RPC in case things take some time to settle down, then retry.
        Status statusRemoteError = Status.RemoteError("Not connected to server " + uuid + " will retry after a delay");
        return delayedSendRpcToTablet(nextRequest, new RecoverableException(statusRemoteError));
    }
    client.sendRpc(nextRequest);
    return d;
}
#end_block

#method_before
Deferred<AsyncKuduScanner.Response> closeScanner(final AsyncKuduScanner scanner) {
    final RemoteTablet tablet = scanner.currentTablet();
    // Getting a null tablet here without being in a closed state means we were in between tablets.
    if (tablet == null) {
        return Deferred.fromResult(null);
    }
    final TabletClient client = connectionCache.getClient(tablet.getLeaderUUID());
    if (client == null || !client.isAlive()) {
        // Oops, we couldn't find a tablet server that hosts this tablet. Our
        // cache was probably invalidated while the client was scanning. So
        // we can't close this scanner properly.
        LOG.warn("Cannot close {} properly, no connection open for {}", scanner, tablet);
        return Deferred.fromResult(null);
    }
    final KuduRpc<AsyncKuduScanner.Response> close_request = scanner.getCloseRequest();
    final Deferred<AsyncKuduScanner.Response> d = close_request.getDeferred();
    close_request.attempt++;
    client.sendRpc(close_request);
    return d;
}
#method_after
Deferred<AsyncKuduScanner.Response> closeScanner(final AsyncKuduScanner scanner) {
    final RemoteTablet tablet = scanner.currentTablet();
    // Getting a null tablet here without being in a closed state means we were in between tablets.
    if (tablet == null) {
        return Deferred.fromResult(null);
    }
    final KuduRpc<AsyncKuduScanner.Response> closeRequest = scanner.getCloseRequest();
    final TabletClient client = connectionCache.getClient(tablet.getReplicaSelectedUUID(closeRequest.getReplicaSelection()));
    if (client == null || !client.isAlive()) {
        // Oops, we couldn't find a tablet server that hosts this tablet. Our
        // cache was probably invalidated while the client was scanning. So
        // we can't close this scanner properly.
        LOG.warn("Cannot close {} properly, no connection open for {}", scanner, tablet);
        return Deferred.fromResult(null);
    }
    final Deferred<AsyncKuduScanner.Response> d = closeRequest.getDeferred();
    closeRequest.attempt++;
    client.sendRpc(closeRequest);
    return d;
}
#end_block

#method_before
<R> Deferred<R> sendRpcToTablet(final KuduRpc<R> request) {
    if (cannotRetryRequest(request)) {
        return tooManyAttemptsOrTimeout(request, null);
    }
    request.attempt++;
    final String tableId = request.getTable().getTableId();
    byte[] partitionKey = request.partitionKey();
    TableLocationsCache.Entry entry = getTableLocationEntry(tableId, partitionKey);
    if (entry != null && entry.isNonCoveredRange()) {
        Exception e = new NonCoveredRangeException(entry.getLowerBoundPartitionKey(), entry.getUpperBoundPartitionKey());
        // Sending both as an errback and returning fromError because sendRpcToTablet might be
        // called via a callback that won't care about the returned Deferred.
        request.errback(e);
        return Deferred.fromError(e);
    }
    // Set the propagated timestamp so that the next time we send a message to
    // the server the message includes the last propagated timestamp.
    long lastPropagatedTs = getLastPropagatedTimestamp();
    if (request.getExternalConsistencyMode() == CLIENT_PROPAGATED && lastPropagatedTs != NO_TIMESTAMP) {
        request.setPropagatedTimestamp(lastPropagatedTs);
    }
    // If we found a tablet, we'll try to find the TS to talk to.
    if (entry != null) {
        RemoteTablet tablet = entry.getTablet();
        String uuid = tablet.getLeaderUUID();
        if (uuid != null) {
            request.addTrace(new RpcTraceObject.RpcTraceObjectBuilder(request.method()).action(RpcTraceObject.Action.PICKED_REPLICA).build());
            Deferred<R> d = request.getDeferred();
            request.setTablet(tablet);
            TabletClient client = connectionCache.getLiveClient(uuid);
            if (client != null) {
                client.sendRpc(request);
                return d;
            }
        }
    }
    request.addTrace(new RpcTraceObject.RpcTraceObjectBuilder(request.method()).action(RpcTraceObject.Action.QUERY_MASTER).build());
    // leader replica.
    if (tablesNotServed.contains(tableId)) {
        return delayedIsCreateTableDone(request.getTable(), request, new RetryRpcCB<R, Master.IsCreateTableDoneResponsePB>(request), getDelayedIsCreateTableDoneErrback(request));
    }
    Callback<Deferred<R>, Master.GetTableLocationsResponsePB> cb = new RetryRpcCB<>(request);
    Callback<Deferred<R>, Exception> eb = new RetryRpcErrback<>(request);
    Deferred<Master.GetTableLocationsResponsePB> returnedD = locateTablet(request.getTable(), partitionKey, request);
    return AsyncUtil.addCallbacksDeferring(returnedD, cb, eb);
}
#method_after
<R> Deferred<R> sendRpcToTablet(final KuduRpc<R> request) {
    if (cannotRetryRequest(request)) {
        return tooManyAttemptsOrTimeout(request, null);
    }
    request.attempt++;
    final String tableId = request.getTable().getTableId();
    byte[] partitionKey = request.partitionKey();
    TableLocationsCache.Entry entry = getTableLocationEntry(tableId, partitionKey);
    if (entry != null && entry.isNonCoveredRange()) {
        Exception e = new NonCoveredRangeException(entry.getLowerBoundPartitionKey(), entry.getUpperBoundPartitionKey());
        // Sending both as an errback and returning fromError because sendRpcToTablet might be
        // called via a callback that won't care about the returned Deferred.
        request.errback(e);
        return Deferred.fromError(e);
    }
    // Set the propagated timestamp so that the next time we send a message to
    // the server the message includes the last propagated timestamp.
    long lastPropagatedTs = getLastPropagatedTimestamp();
    if (request.getExternalConsistencyMode() == CLIENT_PROPAGATED && lastPropagatedTs != NO_TIMESTAMP) {
        request.setPropagatedTimestamp(lastPropagatedTs);
    }
    // If we found a tablet, we'll try to find the TS to talk to.
    if (entry != null) {
        RemoteTablet tablet = entry.getTablet();
        String uuid = tablet.getReplicaSelectedUUID(request.getReplicaSelection());
        if (uuid != null) {
            Deferred<R> d = request.getDeferred();
            request.setTablet(tablet);
            TabletClient client = connectionCache.getLiveClient(uuid);
            if (client != null) {
                client.sendRpc(request);
                return d;
            }
        }
    }
    request.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(request.method(), RpcTraceFrame.Action.QUERY_MASTER).build());
    // leader replica.
    if (tablesNotServed.contains(tableId)) {
        return delayedIsCreateTableDone(request.getTable(), request, new RetryRpcCB<R, Master.IsCreateTableDoneResponsePB>(request), getDelayedIsCreateTableDoneErrback(request));
    }
    Callback<Deferred<R>, Master.GetTableLocationsResponsePB> cb = new RetryRpcCB<>(request);
    Callback<Deferred<R>, Exception> eb = new RetryRpcErrback<>(request);
    Deferred<Master.GetTableLocationsResponsePB> returnedD = locateTablet(request.getTable(), partitionKey, request);
    return AsyncUtil.addCallbacksDeferring(returnedD, cb, eb);
}
#end_block

#method_before
<R> Deferred<R> delayedIsCreateTableDone(final KuduTable table, final KuduRpc<R> rpc, final Callback<Deferred<R>, Master.IsCreateTableDoneResponsePB> retryCB, final Callback<Exception, Exception> errback) {
    final class RetryTimer implements TimerTask {

        public void run(final Timeout timeout) {
            String tableId = table.getTableId();
            final boolean has_permit = acquireMasterLookupPermit();
            if (!has_permit) {
                // this will save us a Master lookup.
                if (!tablesNotServed.contains(tableId)) {
                    try {
                        retryCB.call(null);
                        return;
                    } catch (Exception e) {
                    // we're calling RetryRpcCB which doesn't throw exceptions, ignore
                    }
                }
            }
            IsCreateTableDoneRequest isCreateTableDoneRequest = new IsCreateTableDoneRequest(masterTable, tableId);
            isCreateTableDoneRequest.setTimeoutMillis(defaultAdminOperationTimeoutMs);
            isCreateTableDoneRequest.setRelatedRpc(rpc);
            final Deferred<Master.IsCreateTableDoneResponsePB> d = sendRpcToTablet(isCreateTableDoneRequest).addCallback(new IsCreateTableDoneCB(tableId));
            if (has_permit) {
                // The errback is needed here to release the lookup permit
                d.addCallbacks(new ReleaseMasterLookupPermit<Master.IsCreateTableDoneResponsePB>(), new ReleaseMasterLookupPermit<Exception>());
            }
            d.addCallbacks(retryCB, errback);
        }
    }
    long sleepTime = getSleepTimeForRpc(rpc);
    if (rpc.deadlineTracker.wouldSleepingTimeout(sleepTime)) {
        return tooManyAttemptsOrTimeout(rpc, null);
    }
    newTimeout(new RetryTimer(), sleepTime);
    return rpc.getDeferred();
}
#method_after
<R> Deferred<R> delayedIsCreateTableDone(final KuduTable table, final KuduRpc<R> rpc, final Callback<Deferred<R>, Master.IsCreateTableDoneResponsePB> retryCB, final Callback<Exception, Exception> errback) {
    final class RetryTimer implements TimerTask {

        public void run(final Timeout timeout) {
            String tableId = table.getTableId();
            final boolean has_permit = acquireMasterLookupPermit();
            if (!has_permit) {
                // this will save us a Master lookup.
                if (!tablesNotServed.contains(tableId)) {
                    try {
                        retryCB.call(null);
                        return;
                    } catch (Exception e) {
                    // we're calling RetryRpcCB which doesn't throw exceptions, ignore
                    }
                }
            }
            IsCreateTableDoneRequest isCreateTableDoneRequest = new IsCreateTableDoneRequest(masterTable, tableId);
            isCreateTableDoneRequest.setTimeoutMillis(defaultAdminOperationTimeoutMs);
            isCreateTableDoneRequest.setParentRpc(rpc);
            final Deferred<Master.IsCreateTableDoneResponsePB> d = sendRpcToTablet(isCreateTableDoneRequest).addCallback(new IsCreateTableDoneCB(tableId));
            if (has_permit) {
                // The errback is needed here to release the lookup permit
                d.addCallbacks(new ReleaseMasterLookupPermit<Master.IsCreateTableDoneResponsePB>(), new ReleaseMasterLookupPermit<Exception>());
            }
            d.addCallbacks(retryCB, errback);
        }
    }
    long sleepTime = getSleepTimeForRpc(rpc);
    if (rpc.deadlineTracker.wouldSleepingTimeout(sleepTime)) {
        return tooManyAttemptsOrTimeout(rpc, null);
    }
    newTimeout(new RetryTimer(), sleepTime);
    return rpc.getDeferred();
}
#end_block

#method_before
static <R> Deferred<R> tooManyAttemptsOrTimeout(final KuduRpc<R> request, final KuduException cause) {
    String message;
    if (request.attempt > MAX_RPC_ATTEMPTS) {
        message = "Too many attempts: ";
    } else {
        message = "RPC can not complete before timeout: ";
    }
    Status statusTimedOut = Status.TimedOut(message + request);
    final Exception e = new NonRecoverableException(statusTimedOut, cause);
    request.errback(e);
    LOG.debug("Cannot continue with this RPC: {} because of: {}", request, message, e);
    return Deferred.fromError(e);
}
#method_after
static <R> Deferred<R> tooManyAttemptsOrTimeout(final KuduRpc<R> request, final KuduException cause) {
    String message;
    if (request.attempt > MAX_RPC_ATTEMPTS) {
        message = "Too many attempts: ";
    } else {
        message = "RPC can not complete before timeout: ";
    }
    Status statusTimedOut = Status.TimedOut(message + request);
    final Exception e = new NonRecoverableException(statusTimedOut, cause);
    LOG.debug("Cannot continue with this RPC: {} because of: {}", request, message, e);
    request.errback(e);
    return Deferred.fromError(e);
}
#end_block

#method_before
private Deferred<Master.GetTableLocationsResponsePB> locateTablet(KuduTable table, byte[] partitionKey, KuduRpc<?> relatedRpc) {
    final boolean has_permit = acquireMasterLookupPermit();
    String tableId = table.getTableId();
    if (!has_permit) {
        // If we failed to acquire a permit, it's worth checking if someone
        // looked up the tablet we're interested in.  Every once in a while
        // this will save us a Master lookup.
        TableLocationsCache.Entry entry = getTableLocationEntry(tableId, partitionKey);
        if (entry != null && !entry.isNonCoveredRange() && entry.getTablet().getLeaderUUID() != null) {
            // Looks like no lookup needed.
            return Deferred.fromResult(null);
        }
    }
    // Leave the end of the partition key range empty in order to pre-fetch tablet locations.
    GetTableLocationsRequest rpc = new GetTableLocationsRequest(masterTable, partitionKey, null, tableId);
    rpc.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    final Deferred<Master.GetTableLocationsResponsePB> d;
    // cache the current leader.
    if (isMasterTable(tableId)) {
        d = getMasterTableLocationsPB();
    } else {
        rpc.setRelatedRpc(relatedRpc);
        d = sendRpcToTablet(rpc);
    }
    d.addCallback(new MasterLookupCB(table, partitionKey));
    if (has_permit) {
        d.addBoth(new ReleaseMasterLookupPermit<Master.GetTableLocationsResponsePB>());
    }
    return d;
}
#method_after
private Deferred<Master.GetTableLocationsResponsePB> locateTablet(KuduTable table, byte[] partitionKey, KuduRpc<?> parentRpc) {
    boolean hasPermit = acquireMasterLookupPermit();
    String tableId = table.getTableId();
    if (!hasPermit) {
        // If we failed to acquire a permit, it's worth checking if someone
        // looked up the tablet we're interested in.  Every once in a while
        // this will save us a Master lookup.
        TableLocationsCache.Entry entry = getTableLocationEntry(tableId, partitionKey);
        if (entry != null && !entry.isNonCoveredRange() && entry.getTablet().getLeaderUUID() != null) {
            // Looks like no lookup needed.
            return Deferred.fromResult(null);
        }
    }
    // If we know this is going to the master, check the master consensus
    // configuration (as specified by 'masterAddresses' field) to determine and
    // cache the current leader.
    Deferred<Master.GetTableLocationsResponsePB> d;
    if (isMasterTable(tableId)) {
        d = getMasterTableLocationsPB(parentRpc);
    } else {
        // Leave the end of the partition key range empty in order to pre-fetch tablet locations.
        GetTableLocationsRequest rpc = new GetTableLocationsRequest(masterTable, partitionKey, null, tableId);
        if (parentRpc != null) {
            rpc.setTimeoutMillis(parentRpc.deadlineTracker.getMillisBeforeDeadline());
            rpc.setParentRpc(parentRpc);
        } else {
            rpc.setTimeoutMillis(defaultAdminOperationTimeoutMs);
        }
        d = sendRpcToTablet(rpc);
    }
    d.addCallback(new MasterLookupCB(table, partitionKey));
    if (hasPermit) {
        d.addBoth(new ReleaseMasterLookupPermit<Master.GetTableLocationsResponsePB>());
    }
    return d;
}
#end_block

#method_before
Deferred<Master.GetTableLocationsResponsePB> getMasterTableLocationsPB() {
    final Deferred<Master.GetTableLocationsResponsePB> responseD = new Deferred<>();
    final GetMasterRegistrationReceived received = new GetMasterRegistrationReceived(masterAddresses, responseD);
    for (HostAndPort hostAndPort : masterAddresses) {
        Deferred<GetMasterRegistrationResponse> d;
        // Note: we need to create a client for that host first, as there's a
        // chicken and egg problem: since there is no source of truth beyond
        // the master, the only way to get information about a master host is
        // by making an RPC to that host.
        TabletClient clientForHostAndPort = newMasterClient(hostAndPort);
        if (clientForHostAndPort == null) {
            String message = "Couldn't resolve this master's address " + hostAndPort.toString();
            LOG.warn(message);
            Status statusIOE = Status.IOError(message);
            d = Deferred.fromError(new NonRecoverableException(statusIOE));
        } else {
            d = getMasterRegistration(clientForHostAndPort);
        }
        d.addCallbacks(received.callbackForNode(hostAndPort), received.errbackForNode(hostAndPort));
    }
    return responseD;
}
#method_after
Deferred<Master.GetTableLocationsResponsePB> getMasterTableLocationsPB(KuduRpc<?> parentRpc) {
    final Deferred<Master.GetTableLocationsResponsePB> responseD = new Deferred<>();
    final GetMasterRegistrationReceived received = new GetMasterRegistrationReceived(masterAddresses, responseD);
    for (HostAndPort hostAndPort : masterAddresses) {
        Deferred<GetMasterRegistrationResponse> d;
        // Note: we need to create a client for that host first, as there's a
        // chicken and egg problem: since there is no source of truth beyond
        // the master, the only way to get information about a master host is
        // by making an RPC to that host.
        TabletClient clientForHostAndPort = newMasterClient(hostAndPort);
        if (clientForHostAndPort == null) {
            String message = "Couldn't resolve this master's address " + hostAndPort.toString();
            LOG.warn(message);
            Status statusIOE = Status.IOError(message);
            d = Deferred.fromError(new NonRecoverableException(statusIOE));
        } else {
            d = getMasterRegistration(clientForHostAndPort, parentRpc);
        }
        d.addCallbacks(received.callbackForNode(hostAndPort), received.errbackForNode(hostAndPort));
    }
    return responseD;
}
#end_block

#method_before
<R> void handleNotLeader(final KuduRpc<R> rpc, KuduException ex, TabletClient server) {
    rpc.getTablet().demoteLeader(server.getUuid());
    handleRetryableError(rpc, ex);
}
#method_after
<R> void handleNotLeader(final KuduRpc<R> rpc, KuduException ex, TabletClient server) {
    rpc.getTablet().demoteLeader(server.getServerInfo().getUuid());
    handleRetryableError(rpc, ex);
}
#end_block

#method_before
private <R> Deferred<R> delayedSendRpcToTablet(final KuduRpc<R> rpc, KuduException ex) {
    // we're not expecting this in Kudu.
    final class RetryTimer implements TimerTask {

        public void run(final Timeout timeout) {
            sendRpcToTablet(rpc);
        }
    }
    Status reasonForRetry = ex == null ? null : ex.getStatus();
    rpc.addTrace(new RpcTraceObject.RpcTraceObjectBuilder(rpc.method()).action(RpcTraceObject.Action.SLEEP_THEN_RETRY).callStatus(reasonForRetry).build());
    long sleepTime = getSleepTimeForRpc(rpc);
    if (cannotRetryRequest(rpc) || rpc.deadlineTracker.wouldSleepingTimeout(sleepTime)) {
        // Don't let it retry.
        return tooManyAttemptsOrTimeout(rpc, ex);
    }
    newTimeout(new RetryTimer(), sleepTime);
    return rpc.getDeferred();
}
#method_after
private <R> Deferred<R> delayedSendRpcToTablet(final KuduRpc<R> rpc, KuduException ex) {
    // we're not expecting this in Kudu.
    final class RetryTimer implements TimerTask {

        public void run(final Timeout timeout) {
            sendRpcToTablet(rpc);
        }
    }
    assert (ex != null);
    Status reasonForRetry = ex.getStatus();
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.SLEEP_THEN_RETRY).callStatus(reasonForRetry).build());
    long sleepTime = getSleepTimeForRpc(rpc);
    if (cannotRetryRequest(rpc) || rpc.deadlineTracker.wouldSleepingTimeout(sleepTime)) {
        // Don't let it retry.
        return tooManyAttemptsOrTimeout(rpc, ex);
    }
    newTimeout(new RetryTimer(), sleepTime);
    return rpc.getDeferred();
}
#end_block

#method_before
private void invalidateTabletCache(RemoteTablet tablet, TabletClient server) {
    LOG.info("Removing server " + server.getUuid() + " from this tablet's cache " + tablet.getTabletId());
    tablet.removeTabletClient(server.getUuid());
}
#method_after
private void invalidateTabletCache(RemoteTablet tablet, TabletClient server) {
    String uuid = server.getServerInfo().getUuid();
    LOG.info("Removing server {} from this tablet's cache {}", uuid, tablet.getTabletId());
    tablet.removeTabletClient(uuid);
}
#end_block

#method_before
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws KuduException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        String tabletId = tabletPb.getTabletId().toStringUtf8();
        List<UnknownHostException> lookupExceptions = new ArrayList<>(tabletPb.getReplicasCount());
        for (Master.TabletLocationsPB.ReplicaPB replica : tabletPb.getReplicasList()) {
            try {
                connectionCache.connectTS(replica.getTsInfo());
            } catch (UnknownHostException ex) {
                lookupExceptions.add(ex);
            }
        }
        if (!lookupExceptions.isEmpty() && lookupExceptions.size() == tabletPb.getReplicasCount()) {
            Status statusIOE = Status.IOError("Couldn't find any valid locations, exceptions: " + lookupExceptions);
            throw new NonRecoverableException(statusIOE);
        }
        Partition partition = ProtobufHelper.pbToPartition(tabletPb.getPartition());
        RemoteTablet rt = new RemoteTablet(tableId, tabletId, partition, tabletPb);
        LOG.info("Learned about tablet {} for table '{}' with partition {}", rt.getTabletId(), tableName, rt.getPartition());
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
    // Now test if we found the tablet we were looking for. If so, RetryRpcCB will retry the RPC
    // right away. If not, we throw an exception that RetryRpcErrback will understand as needing to
    // sleep before retrying.
    TableLocationsCache.Entry entry = locationsCache.get(requestPartitionKey);
    if (!entry.isNonCoveredRange() && entry.getTablet().getLeaderUUID() == null) {
        throw new NoLeaderFoundException(Status.NotFound("Tablet " + entry.toString() + " doesn't have a leader"));
    }
}
#method_after
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws KuduException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        List<UnknownHostException> lookupExceptions = new ArrayList<>(tabletPb.getReplicasCount());
        List<ServerInfo> servers = new ArrayList<>(tabletPb.getReplicasCount());
        for (Master.TabletLocationsPB.ReplicaPB replica : tabletPb.getReplicasList()) {
            try {
                ServerInfo serverInfo = connectionCache.connectTS(replica.getTsInfo());
                if (serverInfo != null) {
                    servers.add(serverInfo);
                }
            } catch (UnknownHostException ex) {
                lookupExceptions.add(ex);
            }
        }
        if (!lookupExceptions.isEmpty() && lookupExceptions.size() == tabletPb.getReplicasCount()) {
            Status statusIOE = Status.IOError("Couldn't find any valid locations, exceptions: " + lookupExceptions);
            throw new NonRecoverableException(statusIOE);
        }
        RemoteTablet rt = new RemoteTablet(tableId, tabletPb, servers);
        LOG.info("Learned about tablet {} for table '{}' with partition {}", rt.getTabletId(), tableName, rt.getPartition());
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
    // Now test if we found the tablet we were looking for. If so, RetryRpcCB will retry the RPC
    // right away. If not, we throw an exception that RetryRpcErrback will understand as needing to
    // sleep before retrying.
    TableLocationsCache.Entry entry = locationsCache.get(requestPartitionKey);
    if (!entry.isNonCoveredRange() && entry.getTablet().getLeaderUUID() == null) {
        throw new NoLeaderFoundException(Status.NotFound("Tablet " + entry.toString() + " doesn't have a leader"));
    }
}
#end_block

#method_before
Deferred<GetMasterRegistrationResponse> getMasterRegistration(TabletClient masterClient) {
    // TODO: Handle the situation when multiple in-flight RPCs all want to query the masters,
    // basically reuse in some way the master permits.
    GetMasterRegistrationRequest rpc = new GetMasterRegistrationRequest(masterTable);
    rpc.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    Deferred<GetMasterRegistrationResponse> d = rpc.getDeferred();
    rpc.attempt++;
    masterClient.sendRpc(rpc);
    return d;
}
#method_after
Deferred<GetMasterRegistrationResponse> getMasterRegistration(TabletClient masterClient, KuduRpc<?> parentRpc) {
    // TODO: Handle the situation when multiple in-flight RPCs all want to query the masters,
    // basically reuse in some way the master permits.
    GetMasterRegistrationRequest rpc = new GetMasterRegistrationRequest(masterTable);
    if (parentRpc != null) {
        rpc.setTimeoutMillis(parentRpc.deadlineTracker.getMillisBeforeDeadline());
        rpc.setParentRpc(parentRpc);
    } else {
        rpc.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    }
    Deferred<GetMasterRegistrationResponse> d = rpc.getDeferred();
    rpc.attempt++;
    masterClient.sendRpc(rpc);
    return d;
}
#end_block

#method_before
TabletClient newMasterClient(HostAndPort masterHostPort) {
    String ip = ConnectionCache.getIP(masterHostPort.getHostText());
    if (ip == null) {
        return null;
    }
    // host and port which is enough to identify the node we're connecting to.
    return connectionCache.newClient(MASTER_TABLE_NAME_PLACEHOLDER + " - " + masterHostPort.toString(), ip, masterHostPort.getPort());
}
#method_after
TabletClient newMasterClient(HostAndPort masterHostPort) {
    InetAddress inetAddress = NetUtil.getInetAddress((masterHostPort.getHostText()));
    if (inetAddress == null) {
        return null;
    }
    // host and port which is enough to identify the node we're connecting to.
    return connectionCache.newClient(MASTER_TABLE_NAME_PLACEHOLDER + " - " + masterHostPort.toString(), inetAddress, masterHostPort.getPort());
}
#end_block

#method_before
@Override
public void close() throws Exception {
    shutdown().join(defaultAdminOperationTimeoutMs);
}
#method_after
@Override
public void close() throws Exception {
    shutdown().join();
}
#end_block

#method_before
private void handleCallback(final Object result) {
    final Deferred<R> d = deferred;
    if (d == null) {
        return;
    }
    deferred = null;
    attempt = 0;
    if (isRequestTracked()) {
        table.getAsyncClient().getRequestTracker().rpcCompleted(sequenceId);
        sequenceId = RequestTracker.NO_SEQ_NO;
    }
    deadlineTracker.reset();
    traces.clear();
    relatedRpc = null;
    d.callback(result);
}
#method_after
private void handleCallback(final Object result) {
    final Deferred<R> d = deferred;
    if (d == null) {
        return;
    }
    deferred = null;
    attempt = 0;
    if (isRequestTracked()) {
        table.getAsyncClient().getRequestTracker().rpcCompleted(sequenceId);
        sequenceId = RequestTracker.NO_SEQ_NO;
    }
    deadlineTracker.reset();
    traces.clear();
    parentRpc = null;
    d.callback(result);
}
#end_block

#method_before
void addTrace(RpcTraceObject rpcTraceObject) {
    if (relatedRpc != null) {
        relatedRpc.addTrace(rpcTraceObject);
    }
    traces.add(rpcTraceObject);
}
#method_after
void addTrace(RpcTraceFrame rpcTraceFrame) {
    if (parentRpc != null) {
        parentRpc.addTrace(rpcTraceFrame);
    }
    if (traces.size() == MAX_TRACES_SIZE) {
        // Add a last trace that indicates that we've reached the max size.
        traces.add(new RpcTraceFrame.RpcTraceFrameBuilder(this.method(), RpcTraceFrame.Action.TRACE_TRUNCATED).build());
    } else if (traces.size() < MAX_TRACES_SIZE) {
        traces.add(rpcTraceFrame);
    }
}
#end_block

#method_before
<R> void sendRpc(KuduRpc<R> rpc) {
    rpc.addTrace(new RpcTraceObject.RpcTraceObjectBuilder(rpc.method()).server(this).action(RpcTraceObject.Action.SEND_TO_SERVER).build());
    if (!rpc.deadlineTracker.hasDeadline()) {
        LOG.warn(getPeerUuidLoggingString() + " sending an rpc without a timeout " + rpc);
    }
    Pair<ChannelBuffer, Integer> encodedRpcAndId = null;
    if (chan != null) {
        if (!rpc.getRequiredFeatures().isEmpty() && !secureRpcHelper.getServerFeatures().contains(RpcHeader.RpcFeatureFlag.APPLICATION_FEATURE_FLAGS)) {
            Status statusNotSupported = Status.NotSupported("the server does not support the" + "APPLICATION_FEATURE_FLAGS RPC feature");
            rpc.errback(new NonRecoverableException(statusNotSupported));
        }
        encodedRpcAndId = encode(rpc);
        if (encodedRpcAndId == null) {
            // Stop here.  RPC has been failed already.
            return;
        }
        // Volatile read.
        final Channel chan = this.chan;
        if (chan != null) {
            // Double check if we disconnected during encode().
            Channels.write(chan, encodedRpcAndId.getFirst());
            return;
        }
    }
    // True when we notice we are about to get connected to the TS.
    boolean tryAgain = false;
    // True when the connection was closed while encoding.
    boolean failRpc = false;
    synchronized (this) {
        // Check if we got connected while entering this synchronized block.
        if (chan != null) {
            tryAgain = true;
        // Check if we got disconnected.
        } else if (dead) {
            // `encodedRpcAndId` is null iff `chan` is null.
            if (encodedRpcAndId == null || rpcs_inflight.containsKey(encodedRpcAndId.getSecond())) {
                failRpc = true;
            }
        } else {
            if (pending_rpcs == null) {
                pending_rpcs = new ArrayList<>();
            }
            pending_rpcs.add(rpc);
        }
    }
    if (failRpc) {
        Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset on " + chan);
        failOrRetryRpc(rpc, new RecoverableException(statusNetworkError));
    } else if (tryAgain) {
        // This recursion will not lead to a loop because we only get here if we
        // connected while entering the synchronized block above. So when trying
        // a second time,  we will either succeed to send the RPC if we're still
        // connected, or fail through to the code below if we got disconnected
        // in the mean time.
        sendRpc(rpc);
    }
}
#method_after
<R> void sendRpc(KuduRpc<R> rpc) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.SEND_TO_SERVER).serverInfo(serverInfo).build());
    if (!rpc.deadlineTracker.hasDeadline()) {
        LOG.warn(getPeerUuidLoggingString() + " sending an rpc without a timeout " + rpc);
    }
    Pair<ChannelBuffer, Integer> encodedRpcAndId = null;
    if (chan != null) {
        if (!rpc.getRequiredFeatures().isEmpty() && !secureRpcHelper.getServerFeatures().contains(RpcHeader.RpcFeatureFlag.APPLICATION_FEATURE_FLAGS)) {
            Status statusNotSupported = Status.NotSupported("the server does not support the" + "APPLICATION_FEATURE_FLAGS RPC feature");
            rpc.errback(new NonRecoverableException(statusNotSupported));
        }
        encodedRpcAndId = encode(rpc);
        if (encodedRpcAndId == null) {
            // Stop here.  RPC has been failed already.
            return;
        }
        // Volatile read.
        final Channel chan = this.chan;
        if (chan != null) {
            // Double check if we disconnected during encode().
            Channels.write(chan, encodedRpcAndId.getFirst());
            return;
        }
    }
    // True when we notice we are about to get connected to the TS.
    boolean tryAgain = false;
    // True when the connection was closed while encoding.
    boolean failRpc = false;
    synchronized (this) {
        // Check if we got connected while entering this synchronized block.
        if (chan != null) {
            tryAgain = true;
        // Check if we got disconnected.
        } else if (dead) {
            // `encodedRpcAndId` is null iff `chan` is null.
            if (encodedRpcAndId == null || rpcs_inflight.containsKey(encodedRpcAndId.getSecond())) {
                failRpc = true;
            }
        } else {
            if (pending_rpcs == null) {
                pending_rpcs = new ArrayList<>();
            }
            pending_rpcs.add(rpc);
        }
    }
    if (failRpc) {
        Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset");
        failOrRetryRpc(rpc, new RecoverableException(statusNetworkError));
    } else if (tryAgain) {
        // This recursion will not lead to a loop because we only get here if we
        // connected while entering the synchronized block above. So when trying
        // a second time,  we will either succeed to send the RPC if we're still
        // connected, or fail through to the code below if we got disconnected
        // in the mean time.
        sendRpc(rpc);
    }
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
protected Object decode(ChannelHandlerContext ctx, Channel chan, ChannelBuffer buf, VoidEnum voidEnum) throws NonRecoverableException {
    final long start = System.nanoTime();
    final int rdx = buf.readerIndex();
    LOG.debug("------------------>> ENTERING DECODE >>------------------");
    try {
        buf = secureRpcHelper.handleResponse(buf, chan);
    } catch (SaslException e) {
        String message = getPeerUuidLoggingString() + "Couldn't complete the SASL handshake";
        LOG.error(message);
        Status statusIOE = Status.IOError(message);
        throw new NonRecoverableException(statusIOE, e);
    }
    if (buf == null) {
        return null;
    }
    CallResponse response = new CallResponse(buf);
    RpcHeader.ResponseHeader header = response.getHeader();
    if (!header.hasCallId()) {
        final int size = response.getTotalResponseSize();
        final String msg = getPeerUuidLoggingString() + "RPC response (size: " + size + ") doesn't" + " have a call ID: " + header + ", buf=" + Bytes.pretty(buf);
        LOG.error(msg);
        Status statusIncomplete = Status.Incomplete(msg);
        throw new NonRecoverableException(statusIncomplete);
    }
    final int rpcid = header.getCallId();
    @SuppressWarnings("rawtypes")
    final KuduRpc rpc = rpcs_inflight.get(rpcid);
    if (rpc == null) {
        final String msg = getPeerUuidLoggingString() + "Invalid rpcid: " + rpcid + " found in " + buf + '=' + Bytes.pretty(buf);
        LOG.error(msg);
        Status statusIllegalState = Status.IllegalState(msg);
        // all RPCs in flight to be failed.
        throw new NonRecoverableException(statusIllegalState);
    }
    // Start building the trace, we'll finish it as we parse the response.
    RpcTraceObject.RpcTraceObjectBuilder traceBuilder = new RpcTraceObject.RpcTraceObjectBuilder(rpc.method()).server(this).action(RpcTraceObject.Action.RECEIVE_FROM_SERVER);
    Pair<Object, Object> decoded = null;
    KuduException exception = null;
    Status retryableHeaderError = Status.OK();
    if (header.hasIsError() && header.getIsError()) {
        RpcHeader.ErrorStatusPB.Builder errorBuilder = RpcHeader.ErrorStatusPB.newBuilder();
        KuduRpc.readProtobuf(response.getPBMessage(), errorBuilder);
        RpcHeader.ErrorStatusPB error = errorBuilder.build();
        if (error.getCode().equals(RpcHeader.ErrorStatusPB.RpcErrorCodePB.ERROR_SERVER_TOO_BUSY)) {
            // We can't return right away, we still need to remove ourselves from 'rpcs_inflight', so we
            // populate 'retryableHeaderError'.
            retryableHeaderError = Status.ServiceUnavailable(error.getMessage());
        } else {
            String message = getPeerUuidLoggingString() + "Tablet server sent error " + error.getMessage();
            Status status = Status.RemoteError(message);
            exception = new NonRecoverableException(status);
            // can be useful
            LOG.error(message);
        }
    } else {
        try {
            decoded = rpc.deserialize(response, this.uuid);
        } catch (KuduException ex) {
            exception = ex;
        }
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug(getPeerUuidLoggingString() + "rpcid=" + rpcid + ", response size=" + (buf.readerIndex() - rdx) + " bytes" + ", " + actualReadableBytes() + " readable bytes left" + ", rpc=" + rpc);
    }
    {
        final KuduRpc<?> removed = rpcs_inflight.remove(rpcid);
        if (removed == null) {
            // The RPC we were decoding was cleaned up already, give up.
            Status statusIllegalState = Status.IllegalState("RPC not found");
            throw new NonRecoverableException(statusIllegalState);
        }
    }
    // This check is specifically for the ERROR_SERVER_TOO_BUSY case above.
    if (!retryableHeaderError.ok()) {
        rpc.addTrace(traceBuilder.callStatus(retryableHeaderError).build());
        kuduClient.handleRetryableError(rpc, new RecoverableException(retryableHeaderError));
        return null;
    }
    // Have to do it for both TS and Master errors.
    if (decoded != null) {
        if (decoded.getSecond() instanceof Tserver.TabletServerErrorPB) {
            Tserver.TabletServerErrorPB error = (Tserver.TabletServerErrorPB) decoded.getSecond();
            exception = dispatchTSErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // It was taken care of.
                return null;
            } else {
                // We're going to errback.
                decoded = null;
            }
        } else if (decoded.getSecond() instanceof Master.MasterErrorPB) {
            Master.MasterErrorPB error = (Master.MasterErrorPB) decoded.getSecond();
            exception = dispatchMasterErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // Exception was taken care of.
                return null;
            } else {
                decoded = null;
            }
        }
    }
    try {
        if (decoded != null) {
            assert !(decoded.getFirst() instanceof Exception);
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), decoded.getFirst());
            }
            rpc.addTrace(traceBuilder.callStatus(Status.OK()).build());
            rpc.callback(decoded.getFirst());
        } else {
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), null);
            }
            rpc.addTrace(traceBuilder.callStatus(exception.getStatus()).build());
            rpc.errback(exception);
        }
    } catch (Exception e) {
        LOG.debug(getPeerUuidLoggingString() + "Unexpected exception while handling RPC #" + rpcid + ", rpc=" + rpc + ", buf=" + Bytes.pretty(buf), e);
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug("------------------<< LEAVING  DECODE <<------------------" + " time elapsed: " + ((System.nanoTime() - start) / 1000) + "us");
    }
    // Stop processing here.  The Deferred does everything else.
    return null;
}
#method_after
@Override
@SuppressWarnings("unchecked")
protected Object decode(ChannelHandlerContext ctx, Channel chan, ChannelBuffer buf, VoidEnum voidEnum) throws NonRecoverableException {
    final long start = System.nanoTime();
    final int rdx = buf.readerIndex();
    LOG.debug("------------------>> ENTERING DECODE >>------------------");
    try {
        buf = secureRpcHelper.handleResponse(buf, chan);
    } catch (SaslException e) {
        String message = getPeerUuidLoggingString() + "Couldn't complete the SASL handshake";
        LOG.error(message);
        Status statusIOE = Status.IOError(message);
        throw new NonRecoverableException(statusIOE, e);
    }
    if (buf == null) {
        return null;
    }
    CallResponse response = new CallResponse(buf);
    RpcHeader.ResponseHeader header = response.getHeader();
    if (!header.hasCallId()) {
        final int size = response.getTotalResponseSize();
        final String msg = getPeerUuidLoggingString() + "RPC response (size: " + size + ") doesn't" + " have a call ID: " + header + ", buf=" + Bytes.pretty(buf);
        LOG.error(msg);
        Status statusIncomplete = Status.Incomplete(msg);
        throw new NonRecoverableException(statusIncomplete);
    }
    final int rpcid = header.getCallId();
    @SuppressWarnings("rawtypes")
    final KuduRpc rpc = rpcs_inflight.get(rpcid);
    if (rpc == null) {
        final String msg = getPeerUuidLoggingString() + "Invalid rpcid: " + rpcid + " found in " + buf + '=' + Bytes.pretty(buf);
        LOG.error(msg);
        Status statusIllegalState = Status.IllegalState(msg);
        // all RPCs in flight to be failed.
        throw new NonRecoverableException(statusIllegalState);
    }
    // Start building the trace, we'll finish it as we parse the response.
    RpcTraceFrame.RpcTraceFrameBuilder traceBuilder = new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo);
    Pair<Object, Object> decoded = null;
    KuduException exception = null;
    Status retryableHeaderError = Status.OK();
    if (header.hasIsError() && header.getIsError()) {
        RpcHeader.ErrorStatusPB.Builder errorBuilder = RpcHeader.ErrorStatusPB.newBuilder();
        KuduRpc.readProtobuf(response.getPBMessage(), errorBuilder);
        RpcHeader.ErrorStatusPB error = errorBuilder.build();
        if (error.getCode().equals(RpcHeader.ErrorStatusPB.RpcErrorCodePB.ERROR_SERVER_TOO_BUSY)) {
            // We can't return right away, we still need to remove ourselves from 'rpcs_inflight', so we
            // populate 'retryableHeaderError'.
            retryableHeaderError = Status.ServiceUnavailable(error.getMessage());
        } else {
            String message = getPeerUuidLoggingString() + "Tablet server sent error " + error.getMessage();
            Status status = Status.RemoteError(message);
            exception = new NonRecoverableException(status);
            // can be useful
            LOG.error(message);
        }
    } else {
        try {
            decoded = rpc.deserialize(response, this.serverInfo.getUuid());
        } catch (KuduException ex) {
            exception = ex;
        }
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug(getPeerUuidLoggingString() + "rpcid=" + rpcid + ", response size=" + (buf.readerIndex() - rdx) + " bytes" + ", " + actualReadableBytes() + " readable bytes left" + ", rpc=" + rpc);
    }
    {
        final KuduRpc<?> removed = rpcs_inflight.remove(rpcid);
        if (removed == null) {
            // The RPC we were decoding was cleaned up already, give up.
            Status statusIllegalState = Status.IllegalState("RPC not found");
            throw new NonRecoverableException(statusIllegalState);
        }
    }
    // This check is specifically for the ERROR_SERVER_TOO_BUSY case above.
    if (!retryableHeaderError.ok()) {
        rpc.addTrace(traceBuilder.callStatus(retryableHeaderError).build());
        kuduClient.handleRetryableError(rpc, new RecoverableException(retryableHeaderError));
        return null;
    }
    // Have to do it for both TS and Master errors.
    if (decoded != null) {
        if (decoded.getSecond() instanceof Tserver.TabletServerErrorPB) {
            Tserver.TabletServerErrorPB error = (Tserver.TabletServerErrorPB) decoded.getSecond();
            exception = dispatchTSErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // It was taken care of.
                return null;
            } else {
                // We're going to errback.
                decoded = null;
            }
        } else if (decoded.getSecond() instanceof Master.MasterErrorPB) {
            Master.MasterErrorPB error = (Master.MasterErrorPB) decoded.getSecond();
            exception = dispatchMasterErrorOrReturnException(rpc, error, traceBuilder);
            if (exception == null) {
                // Exception was taken care of.
                return null;
            } else {
                decoded = null;
            }
        }
    }
    try {
        if (decoded != null) {
            assert !(decoded.getFirst() instanceof Exception);
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), decoded.getFirst());
            }
            rpc.addTrace(traceBuilder.callStatus(Status.OK()).build());
            rpc.callback(decoded.getFirst());
        } else {
            if (kuduClient.isStatisticsEnabled()) {
                rpc.updateStatistics(kuduClient.getStatistics(), null);
            }
            rpc.addTrace(traceBuilder.callStatus(exception.getStatus()).build());
            rpc.errback(exception);
        }
    } catch (Exception e) {
        LOG.debug(getPeerUuidLoggingString() + "Unexpected exception while handling RPC #" + rpcid + ", rpc=" + rpc + ", buf=" + Bytes.pretty(buf), e);
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug("------------------<< LEAVING  DECODE <<------------------" + " time elapsed: " + ((System.nanoTime() - start) / 1000) + "us");
    }
    // Stop processing here.  The Deferred does everything else.
    return null;
}
#end_block

#method_before
private KuduException dispatchTSErrorOrReturnException(KuduRpc rpc, Tserver.TabletServerErrorPB error, RpcTraceObject.RpcTraceObjectBuilder traceBuilder) {
    WireProtocol.AppStatusPB.ErrorCode code = error.getStatus().getCode();
    Status status = Status.fromTabletServerErrorPB(error);
    if (error.getCode() == Tserver.TabletServerErrorPB.Code.TABLET_NOT_FOUND) {
        kuduClient.handleTabletNotFound(rpc, new RecoverableException(status), this);
    // we're not calling rpc.callback() so we rely on the client to retry that RPC
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.SERVICE_UNAVAILABLE) {
        kuduClient.handleRetryableError(rpc, new RecoverableException(status));
    // The following two error codes are an indication that the tablet isn't a leader.
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.ILLEGAL_STATE || code == WireProtocol.AppStatusPB.ErrorCode.ABORTED) {
        kuduClient.handleNotLeader(rpc, new RecoverableException(status), this);
    } else {
        return new NonRecoverableException(status);
    }
    rpc.addTrace(traceBuilder.callStatus(status).build());
    return null;
}
#method_after
private KuduException dispatchTSErrorOrReturnException(KuduRpc rpc, Tserver.TabletServerErrorPB error, RpcTraceFrame.RpcTraceFrameBuilder traceBuilder) {
    WireProtocol.AppStatusPB.ErrorCode code = error.getStatus().getCode();
    Status status = Status.fromTabletServerErrorPB(error);
    if (error.getCode() == Tserver.TabletServerErrorPB.Code.TABLET_NOT_FOUND) {
        kuduClient.handleTabletNotFound(rpc, new RecoverableException(status), this);
    // we're not calling rpc.callback() so we rely on the client to retry that RPC
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.SERVICE_UNAVAILABLE) {
        kuduClient.handleRetryableError(rpc, new RecoverableException(status));
    // The following two error codes are an indication that the tablet isn't a leader.
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.ILLEGAL_STATE || code == WireProtocol.AppStatusPB.ErrorCode.ABORTED) {
        kuduClient.handleNotLeader(rpc, new RecoverableException(status), this);
    } else {
        return new NonRecoverableException(status);
    }
    rpc.addTrace(traceBuilder.callStatus(status).build());
    return null;
}
#end_block

#method_before
private KuduException dispatchMasterErrorOrReturnException(KuduRpc rpc, Master.MasterErrorPB error, RpcTraceObject.RpcTraceObjectBuilder traceBuilder) {
    WireProtocol.AppStatusPB.ErrorCode code = error.getStatus().getCode();
    Status status = Status.fromMasterErrorPB(error);
    if (error.getCode() == Master.MasterErrorPB.Code.NOT_THE_LEADER) {
        kuduClient.handleNotLeader(rpc, new RecoverableException(status), this);
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.SERVICE_UNAVAILABLE) {
        if (rpc instanceof GetMasterRegistrationRequest) {
            // is. If the error is truly non recoverable, it'll be handled later.
            return new RecoverableException(status);
        } else {
            // TODO: This is a crutch until we either don't have to retry RPCs going to the
            // same server or use retry policies.
            kuduClient.handleRetryableError(rpc, new RecoverableException(status));
        }
    } else {
        return new NonRecoverableException(status);
    }
    rpc.addTrace(traceBuilder.callStatus(status).build());
    return null;
}
#method_after
private KuduException dispatchMasterErrorOrReturnException(KuduRpc rpc, Master.MasterErrorPB error, RpcTraceFrame.RpcTraceFrameBuilder traceBuilder) {
    WireProtocol.AppStatusPB.ErrorCode code = error.getStatus().getCode();
    Status status = Status.fromMasterErrorPB(error);
    if (error.getCode() == Master.MasterErrorPB.Code.NOT_THE_LEADER) {
        kuduClient.handleNotLeader(rpc, new RecoverableException(status), this);
    } else if (code == WireProtocol.AppStatusPB.ErrorCode.SERVICE_UNAVAILABLE) {
        if (rpc instanceof GetMasterRegistrationRequest) {
            // is. If the error is truly non recoverable, it'll be handled later.
            return new RecoverableException(status);
        } else {
            // TODO: This is a crutch until we either don't have to retry RPCs going to the
            // same server or use retry policies.
            kuduClient.handleRetryableError(rpc, new RecoverableException(status));
        }
    } else {
        return new NonRecoverableException(status);
    }
    rpc.addTrace(traceBuilder.callStatus(status).build());
    return null;
}
#end_block

#method_before
private void cleanup(final Channel chan) {
    final ArrayList<KuduRpc<?>> rpcs;
    LOG.info(getPeerUuidLoggingString() + "before cleanup");
    // to be sent to failOrRetryRpc.
    synchronized (this) {
        // clear up rpcs_inflight multiple times.
        if (dead) {
            return;
        }
        dead = true;
        rpcs = pending_rpcs == null ? new ArrayList<KuduRpc<?>>(rpcs_inflight.size()) : pending_rpcs;
        for (Iterator<KuduRpc<?>> iterator = rpcs_inflight.values().iterator(); iterator.hasNext(); ) {
            KuduRpc<?> rpc = iterator.next();
            rpcs.add(rpc);
            iterator.remove();
        }
        // After this, rpcs_inflight might still have entries since they could have been added
        // concurrently, and those RPCs will be handled by their caller in sendRpc.
        pending_rpcs = null;
    }
    Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset on " + chan);
    RecoverableException exception = new RecoverableException(statusNetworkError);
    failOrRetryRpcs(rpcs, exception);
}
#method_after
private void cleanup(final Channel chan) {
    final ArrayList<KuduRpc<?>> rpcs;
    // to be sent to failOrRetryRpc.
    synchronized (this) {
        // clear up rpcs_inflight multiple times.
        if (dead) {
            return;
        }
        dead = true;
        rpcs = pending_rpcs == null ? new ArrayList<KuduRpc<?>>(rpcs_inflight.size()) : pending_rpcs;
        for (Iterator<KuduRpc<?>> iterator = rpcs_inflight.values().iterator(); iterator.hasNext(); ) {
            KuduRpc<?> rpc = iterator.next();
            rpcs.add(rpc);
            iterator.remove();
        }
        // After this, rpcs_inflight might still have entries since they could have been added
        // concurrently, and those RPCs will be handled by their caller in sendRpc.
        pending_rpcs = null;
    }
    Status statusNetworkError = Status.NetworkError(getPeerUuidLoggingString() + "Connection reset");
    RecoverableException exception = new RecoverableException(statusNetworkError);
    failOrRetryRpcs(rpcs, exception);
}
#end_block

#method_before
private void failOrRetryRpc(final KuduRpc<?> rpc, final RecoverableException exception) {
    rpc.addTrace(new RpcTraceObject.RpcTraceObjectBuilder(rpc.method()).action(RpcTraceObject.Action.RECEIVE_FROM_SERVER).server(this).callStatus(exception.getStatus()).build());
    RemoteTablet tablet = rpc.getTablet();
    // tablet it's because we didn't set it properly before calling sendRpc().
    if (tablet == null) {
        // Can't retry, dunno where this RPC should go.
        rpc.errback(exception);
    } else {
        if (gotUncaughtException) {
            // This will remove this TabletClient from this RPC's cache since there's something wrong
            // about it.
            kuduClient.handleTabletNotFound(rpc, exception, this);
        } else {
            kuduClient.handleRetryableError(rpc, exception);
        }
    }
}
#method_after
private void failOrRetryRpc(final KuduRpc<?> rpc, final RecoverableException exception) {
    rpc.addTrace(new RpcTraceFrame.RpcTraceFrameBuilder(rpc.method(), RpcTraceFrame.Action.RECEIVE_FROM_SERVER).serverInfo(serverInfo).callStatus(exception.getStatus()).build());
    RemoteTablet tablet = rpc.getTablet();
    // tablet it's because we didn't set it properly before calling sendRpc().
    if (tablet == null) {
        // Can't retry, dunno where this RPC should go.
        rpc.errback(exception);
    } else {
        if (gotUncaughtException) {
            // This will remove this TabletClient from this RPC's cache since there's something wrong
            // about it.
            kuduClient.handleTabletNotFound(rpc, exception, this);
        } else {
            kuduClient.handleRetryableError(rpc, exception);
        }
    }
}
#end_block

#method_before
private ChannelBuffer header() {
    RpcHeader.ConnectionContextPB.Builder builder = RpcHeader.ConnectionContextPB.newBuilder();
    RpcHeader.UserInformationPB.Builder userBuilder = RpcHeader.UserInformationPB.newBuilder();
    // TODO set real user
    userBuilder.setEffectiveUser(SecureRpcHelper.USER_AND_PASSWORD);
    userBuilder.setRealUser(SecureRpcHelper.USER_AND_PASSWORD);
    builder.setUserInfo(userBuilder.build());
    RpcHeader.ConnectionContextPB pb = builder.build();
    RpcHeader.RequestHeader header = RpcHeader.RequestHeader.newBuilder().setCallId(CONNECTION_CTX_CALL_ID).build();
    return KuduRpc.toChannelBuffer(header, pb);
}
#method_after
private ChannelBuffer header() {
    RpcHeader.ConnectionContextPB.Builder builder = RpcHeader.ConnectionContextPB.newBuilder();
    // The UserInformationPB is deprecated, but used by servers prior to Kudu 1.1.
    RpcHeader.UserInformationPB.Builder userBuilder = RpcHeader.UserInformationPB.newBuilder();
    userBuilder.setEffectiveUser(SecureRpcHelper.USER_AND_PASSWORD);
    userBuilder.setRealUser(SecureRpcHelper.USER_AND_PASSWORD);
    builder.setDEPRECATEDUserInfo(userBuilder.build());
    RpcHeader.ConnectionContextPB pb = builder.build();
    RpcHeader.RequestHeader header = RpcHeader.RequestHeader.newBuilder().setCallId(CONNECTION_CTX_CALL_ID).build();
    return KuduRpc.toChannelBuffer(header, pb);
}
#end_block

#method_before
private String getPeerUuidLoggingString() {
    return "[Peer " + uuid + "] ";
}
#method_after
private String getPeerUuidLoggingString() {
    return "[Peer " + serverInfo.getUuid() + "] ";
}
#end_block

#method_before
public String toString() {
    final StringBuilder buf = new StringBuilder(13 + 10 + 6 + 64 + 7 + 32 + 16 + 1 + 17 + 2 + 1);
    // =13
    buf.append("TabletClient@").append(// ~10
    hashCode()).append(// = 6
    "(chan=").append(// ~64 (up to 66 when using IPv4)
    chan).append(// = 7
    ", uuid=").append(// = 32
    uuid).append(// =16
    ", #pending_rpcs=");
    int npending_rpcs;
    synchronized (this) {
        npending_rpcs = pending_rpcs == null ? 0 : pending_rpcs.size();
    }
    // = 1
    buf.append(npending_rpcs);
    // =17
    buf.append(", #rpcs_inflight=").append(// ~ 2
    rpcs_inflight.size()).append(// = 1
    ')');
    return buf.toString();
}
#method_after
public String toString() {
    final StringBuilder buf = new StringBuilder(13 + 10 + 6 + 64 + 7 + 32 + 16 + 1 + 17 + 2 + 1);
    // =13
    buf.append("TabletClient@").append(// ~10
    hashCode()).append(// = 6
    "(chan=").append(// ~64 (up to 66 when using IPv4)
    chan).append(// = 7
    ", uuid=").append(// = 32
    serverInfo.getUuid()).append(// =16
    ", #pending_rpcs=");
    int npending_rpcs;
    synchronized (this) {
        npending_rpcs = pending_rpcs == null ? 0 : pending_rpcs.size();
    }
    // = 1
    buf.append(npending_rpcs);
    // =17
    buf.append(", #rpcs_inflight=").append(// ~ 2
    rpcs_inflight.size()).append(// = 1
    ')');
    return buf.toString();
}
#end_block

#method_before
KuduRpc<Response> getOpenRequest() {
    checkScanningNotStarted();
    // should be fully configured
    if (this.inFirstTablet) {
        this.inFirstTablet = false;
    }
    return new ScanRequest(table, State.OPENING);
}
#method_after
KuduRpc<Response> getOpenRequest() {
    checkScanningNotStarted();
    return new ScanRequest(table, State.OPENING);
}
#end_block

#method_before
public AsyncKuduScanner build() {
    return new AsyncKuduScanner(client, table, projectedColumnNames, projectedColumnIndexes, readMode, orderMode, scanRequestTimeout, predicates, limit, cacheBlocks, prefetching, lowerBoundPrimaryKey, upperBoundPrimaryKey, htTimestamp, batchSizeBytes, PartitionPruner.create(this));
}
#method_after
public AsyncKuduScanner build() {
    return new AsyncKuduScanner(client, table, projectedColumnNames, projectedColumnIndexes, readMode, orderMode, scanRequestTimeout, predicates, limit, cacheBlocks, prefetching, lowerBoundPrimaryKey, upperBoundPrimaryKey, htTimestamp, batchSizeBytes, PartitionPruner.create(this), replicaSelection);
}
#end_block

#method_before
@Test
public void TestKuduUpdate() {
    // TestUtils.assumeKuduIsSupported();
    ParserError("update (select * from functional_kudu.testtbl) a set name = '10'");
}
#method_after
@Test
public void TestKuduUpdate() {
    TestUtils.assumeKuduIsSupported();
    ParserError("update (select * from functional_kudu.testtbl) a set name = '10'");
}
#end_block

#method_before
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, " + "HASH(a) INTO 2 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int, k int) DISTRIBUTE BY HASH INTO 4 BUCKETS," + " HASH(k) INTO 4 BUCKETS");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo DISTRIBUTE BY HASH INTO 4 BUCKETS");
    // The SPLIT ROWS clause is no longer supported
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) SPLIT ROWS ((1),(2))");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) DISTRIBUTE BY " + "HASH (a) INTO 3 BUCKETS, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY HASH (a) INTO 4 BUCKETS, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) INTO 3 BUCKETS");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITITON 1 + 1 < VALUES) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
}
#method_after
@Test
public void TestCreateTable() {
    // Support unqualified and fully-qualified table names
    ParsesOk("CREATE TABLE Foo (i int)");
    ParsesOk("CREATE TABLE Foo.Bar (i int)");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar (i int)");
    ParsesOk("CREATE TABLE Foo.Bar2 LIKE Foo.Bar1");
    ParsesOk("CREATE TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1");
    ParsesOk("CREATE EXTERNAL TABLE IF NOT EXISTS Bar2 LIKE Bar1 LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'sdafsdf'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT ''");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS PARQUETFILE");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo COMMENT 'tbl' " + "STORED AS PARQUETFILE LOCATION '/a/b'");
    ParsesOk("CREATE TABLE Foo2 LIKE Foo STORED AS TEXTFILE LOCATION '/a/b'");
    // Table and column names starting with digits.
    ParsesOk("CREATE TABLE 01_Foo (01_i int, 02_j string)");
    // Unpartitioned tables
    ParsesOk("CREATE TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string)");
    ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) LOCATION '/test-warehouse/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/'");
    ParsesOk("CREATE TABLE Foo (i int, s string) COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo COMMENT 'hello' LOCATION '/a/b/' " + "TBLPROPERTIES ('123'='1234')");
    // Partitioned tables
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (j string)");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY (s string, d double)");
    ParsesOk("CREATE TABLE Foo (i int, s string) PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    // No column definitions.
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (s string, d double)" + " COMMENT 'hello' LOCATION '/a/b/'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (int)");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY ()");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY");
    // Column comments
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string)");
    ParsesOk("CREATE TABLE Foo (i int COMMENT 'hello', s string COMMENT 'hi')");
    ParsesOk("CREATE TABLE T (i int COMMENT 'hi') PARTITIONED BY (j int COMMENT 'bye')");
    // Supported file formats
    String[] supportedFileFormats = { "TEXTFILE", "SEQUENCEFILE", "PARQUET", "PARQUETFILE", "RCFILE", "AVRO" };
    for (String format : supportedFileFormats) {
        ParsesOk("CREATE TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk("CREATE EXTERNAL TABLE Foo (i int, s string) STORED AS " + format);
        ParsesOk(String.format("CREATE TABLE Foo (i int, s string) STORED AS %s LOCATION '/b'", format));
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo (f float) COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        // No column definitions.
        ParsesOk(String.format("CREATE EXTERNAL TABLE Foo COMMENT 'c' STORED AS %s LOCATION '/b'", format));
        ParserError(String.format("CREATE EXTERNAL TABLE t PRIMARY KEYS (i) STORED AS " + "%s", format));
    }
    ParsesOk("CREATE TABLE foo (i INT) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (i, j)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT, j INT, PRIMARY KEY (j, i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, PRIMARY KEY(i)) STORED AS KUDU");
    ParsesOk("CREATE TABLE foo (i INT PRIMARY KEY, j INT PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT, PRIMARY KEY) STORED AS KUDU");
    ParserError("CREATE TABLE foo (PRIMARY KEY(a), a INT) STORED AS KUDU");
    ParserError("CREATE TABLE foo (i INT) PRIMARY KEY (i) STORED AS KUDU");
    // Table Properties
    String[] tblPropTypes = { "TBLPROPERTIES", "WITH SERDEPROPERTIES" };
    for (String propType : tblPropTypes) {
        ParsesOk(String.format("CREATE TABLE Foo (i int) %s ('a'='b', 'c'='d', 'e'='f')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ()", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a')", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s ('a'=c)", propType));
        ParserError(String.format("CREATE TABLE Foo (i int) %s (a='c')", propType));
    }
    ParsesOk("CREATE TABLE Foo (i int) WITH SERDEPROPERTIES ('a'='b') " + "TBLPROPERTIES ('c'='d', 'e'='f')");
    // TBLPROPERTIES must go after SERDEPROPERTIES
    ParserError("CREATE TABLE Foo (i int) TBLPROPERTIES ('c'='d', 'e'='f') " + "WITH SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int) SERDEPROPERTIES ('a'='b')");
    ParserError("CREATE TABLE Foo (i int, s string) STORED AS SEQFILE");
    ParserError("CREATE TABLE Foo (i int, s string) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS TEXT");
    ParserError("CREATE TABLE Foo LIKE Bar COMMENT");
    ParserError("CREATE TABLE Foo LIKE Bar STORED TEXTFILE");
    ParserError("CREATE TABLE Foo LIKE Bar STORED AS");
    ParserError("CREATE TABLE Foo LIKE Bar LOCATION");
    // Row format syntax
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY '|'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY '\'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " ESCAPED BY '\3' LINES TERMINATED BY '\1'");
    ParsesOk("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\0'" + " LINES TERMINATED BY '\1' STORED AS TEXTFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hi' ROW FORMAT DELIMITED STORED AS RCFILE");
    ParsesOk("CREATE TABLE T (i int) COMMENT 'hello' ROW FORMAT DELIMITED FIELDS " + "TERMINATED BY '\0' LINES TERMINATED BY '\1' STORED AS TEXTFILE LOCATION '/a'");
    // Negative row format syntax
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES TERMINATED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED ESCAPED BY");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED '|'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS TERMINATED BY |");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED FIELDS BY '\0'");
    ParserError("CREATE TABLE T (i int) ROW FORMAT DELIMITED LINES BY '\n'");
    ParserError("CREATE TABLE T (i int) FIELDS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ROWS TERMINATED BY '\0'");
    ParserError("CREATE TABLE T (i int) ESCAPED BY '\0'");
    // Order should be: [comment] [partition by cols] [row format] [serdeproperties (..)]
    // [stored as FILEFORMAT] [location] [cache spec] [tblproperties (...)]
    ParserError("CREATE TABLE Foo (d double) COMMENT 'c' PARTITIONED BY (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) STORED AS TEXTFILE ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (d double) ROW FORMAT DELIMITED COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c'");
    ParserError("CREATE TABLE Foo (d double) UNCACHED LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) CACHED IN 'pool' REPLICATION = 8 " + "LOCATION '/a/b'");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' COMMENT 'c' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) LOCATION 'a' STORED AS RCFILE");
    ParserError("CREATE TABLE Foo (d double) TBLPROPERTIES('a'='b') LOCATION 'a'");
    ParserError("CREATE TABLE Foo (i int) LOCATION 'a' WITH SERDEPROPERTIES('a'='b')");
    // Location and comment need to be string literals, file format is not
    ParserError("CREATE TABLE Foo (d double) LOCATION a");
    ParserError("CREATE TABLE Foo (d double) COMMENT c");
    ParserError("CREATE TABLE Foo (d double COMMENT c)");
    ParserError("CREATE TABLE Foo (d double COMMENT 'c') PARTITIONED BY (j COMMENT hi)");
    ParserError("CREATE TABLE Foo (d double) STORED AS 'TEXTFILE'");
    // Caching
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'" + " WITH REPLICATION = 4");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool'");
    ParsesOk("CREATE TABLE Foo (i int) PARTITIONED BY(j int) LOCATION '/a' " + "CACHED IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN myPool");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN");
    ParserError("CREATE TABLE Foo (i int) CACHED 'myPool'");
    ParserError("CREATE TABLE Foo (i int) IN 'myPool'");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY(j int) CACHED IN 'myPool' " + "LOCATION '/a'");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = -1");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' WITH REPLICATION = 1.0");
    ParserError("CREATE TABLE Foo (i int) CACHED IN 'myPool' " + "WITH REPLICATION = cast(1 as double)");
    // Invalid syntax
    ParserError("CREATE TABLE IF EXISTS Foo.Bar (i int)");
    ParserError("CREATE TABLE Bar LIKE Bar2 (i int)");
    ParserError("CREATE IF NOT EXISTS TABLE Foo.Bar (i int)");
    ParserError("CREATE TABLE Foo (d double) STORED TEXTFILE");
    ParserError("CREATE TABLE Foo (d double) AS TEXTFILE");
    ParserError("CREATE TABLE Foo i int");
    ParserError("CREATE TABLE Foo (i intt)");
    ParserError("CREATE TABLE Foo (int i)");
    ParserError("CREATE TABLE Foo (i int,)");
    ParserError("CREATE TABLE Foo ()");
    ParserError("CREATE TABLE");
    ParserError("CREATE EXTERNAL");
    ParserError("CREATE");
    // Valid syntax for tables PRODUCED BY DATA SOURCE
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar");
    ParsesOk("CREATE TABLE Foo (i int, s string) PRODUCED BY DATA SOURCE Bar(\"\")");
    ParsesOk("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE " + "Bar(\"Foo \\!@#$%^&*()\")");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo (i int) PRODUCED BY DATA SOURCE Bar(\"\")");
    // Invalid syntax for tables PRODUCED BY DATA SOURCE
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SRC Foo");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo.Bar");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo()");
    ParserError("CREATE EXTERNAL TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\")");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "LOCATION 'x'");
    ParserError("CREATE TABLE Foo (i int) PRODUCED BY DATA SOURCE Foo(\"\") " + "ROW FORMAT DELIMITED");
    ParserError("CREATE TABLE Foo (i int) PARTITIONED BY (j string) PRODUCED BY DATA " + "SOURCE Foo(\"\")");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS, " + "HASH(a) INTO 2 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH INTO 4 BUCKETS");
    ParsesOk("CREATE TABLE Foo (i int, k int) DISTRIBUTE BY HASH INTO 4 BUCKETS," + " HASH(k) INTO 4 BUCKETS");
    ParserError("CREATE TABLE Foo (i int) DISTRIBUTE BY HASH(i)");
    ParserError("CREATE EXTERNAL TABLE Foo DISTRIBUTE BY HASH INTO 4 BUCKETS");
    // Range partitioning
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE (PARTITION VALUE = 10)");
    ParsesOk("CREATE TABLE Foo (i int) DISTRIBUTE BY RANGE(i) " + "(PARTITION 1 <= VALUES < 10, PARTITION 10 <= VALUES < 20, " + "PARTITION 21 < VALUES <= 30, PARTITION VALUE = 50)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION 10 <= VALUES)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES < 10)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE(a) " + "(PARTITION VALUES <= 10, PARTITION VALUE = 20)");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE(a, b) " + "(PARTITION VALUE = (2001, 1), PARTITION VALUE = (2001, 2), " + "PARTITION VALUE = (2002, 1))");
    ParsesOk("CREATE TABLE Foo (a int, b string) DISTRIBUTE BY " + "HASH (a) INTO 3 BUCKETS, RANGE (a, b) (PARTITION VALUE = (1, 'abc'), " + "PARTITION VALUE = (2, 'def'))");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 1 + 1) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 1 + 1 < VALUES) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int, b int) DISTRIBUTE BY RANGE (a) " + "(PARTITION b < VALUES <= a) STORED AS KUDU");
    ParsesOk("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION now() <= VALUES, PARTITION VALUE = add_months(now(), 2)) " + "STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) ()");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY HASH (a) INTO 4 BUCKETS, " + "RANGE (a) (PARTITION VALUE = 10), RANGE (a) (PARTITION VALUES < 10)");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUE = 10), HASH (a) INTO 3 BUCKETS");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION VALUES = 10) STORED AS KUDU");
    ParserError("CREATE TABLE Foo (a int) DISTRIBUTE BY RANGE (a) " + "(PARTITION 10 < VALUE < 20) STORED AS KUDU");
}
#end_block

#method_before
@Test
public void TestCreateTableAsSelect() {
    ParsesOk("CREATE TABLE Foo AS SELECT 1, 2, 3");
    ParsesOk("CREATE TABLE Foo AS SELECT * from foo.bar");
    ParsesOk("CREATE TABLE Foo.Bar AS SELECT int_col, bool_col from tbl limit 10");
    ParsesOk("CREATE TABLE Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE Foo STORED AS PARQUET AS SELECT 1");
    ParsesOk("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH INTO 2 BUCKETS " + "AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH (b) INTO 2 " + "BUCKETS AS SELECT * from bar");
    // With clause works
    ParsesOk("CREATE TABLE Foo AS with t1 as (select 1) select * from t1");
    // Incomplete AS SELECT statement
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS SELECT");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS WITH");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS");
    // INSERT statements are not allowed
    ParserError("CREATE TABLE Foo AS INSERT INTO Foo SELECT 1");
    // Column and partition definitions not allowed
    ParserError("CREATE TABLE Foo(i int) AS SELECT 1");
    ParserError("CREATE TABLE Foo PARTITIONED BY(i int) AS SELECT 1");
    // Partitioned by syntax following insert into syntax
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) ROW FORMAT DELIMITED STORED AS " + "PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1, 2");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT * from Bar");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a=2, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a, b=2) AS SELECT * from Bar");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (i) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS " + "SELECT 1");
    ParserError("CREATE TABLE Foo DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY HASH(a) INTO 4 BUCKETS " + "TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
}
#method_after
@Test
public void TestCreateTableAsSelect() {
    ParsesOk("CREATE TABLE Foo AS SELECT 1, 2, 3");
    ParsesOk("CREATE TABLE Foo AS SELECT * from foo.bar");
    ParsesOk("CREATE TABLE Foo.Bar AS SELECT int_col, bool_col from tbl limit 10");
    ParsesOk("CREATE TABLE Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE IF NOT EXISTS Foo.Bar LOCATION '/a/b' AS SELECT * from foo");
    ParsesOk("CREATE TABLE Foo STORED AS PARQUET AS SELECT 1");
    ParsesOk("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH INTO 2 BUCKETS " + "AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a, b) DISTRIBUTE BY HASH (b) INTO 2 " + "BUCKETS AS SELECT * from bar");
    // With clause works
    ParsesOk("CREATE TABLE Foo AS with t1 as (select 1) select * from t1");
    // Incomplete AS SELECT statement
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS SELECT");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS WITH");
    ParserError("CREATE TABLE Foo ROW FORMAT DELIMITED STORED AS PARQUET AS");
    // INSERT statements are not allowed
    ParserError("CREATE TABLE Foo AS INSERT INTO Foo SELECT 1");
    // Column and partition definitions not allowed
    ParserError("CREATE TABLE Foo(i int) AS SELECT 1");
    ParserError("CREATE TABLE Foo PARTITIONED BY(i int) AS SELECT 1");
    // Partitioned by syntax following insert into syntax
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) ROW FORMAT DELIMITED STORED AS " + "PARQUETFILE AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT 1, 2");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a) AS SELECT * from Bar");
    ParsesOk("CREATE TABLE Foo PARTITIONED BY (a, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a=2, b) AS SELECT * from Bar");
    ParserError("CREATE TABLE Foo PARTITIONED BY (a, b=2) AS SELECT * from Bar");
    // Flexible partitioning
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (i) DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS " + "SELECT 1");
    ParserError("CREATE TABLE Foo DISTRIBUTE BY HASH(i) INTO 4 BUCKETS AS SELECT 1");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY HASH(a) INTO 4 BUCKETS " + "TBLPROPERTIES ('a'='b', 'c'='d') AS SELECT * from bar");
    ParsesOk("CREATE TABLE Foo PRIMARY KEY (a) DISTRIBUTE BY RANGE(a) " + "(PARTITION 1 < VALUES < 10, PARTITION 10 <= VALUES < 20, PARTITION VALUE = 30) " + "STORED AS KUDU AS SELECT * FROM Bar");
}
#end_block

#method_before
@Test
public void TestAlterTableSet() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes set serdeproperties('a'='2')");
    AnalyzesOk("alter table functional.alltypes PARTITION (Year=2010, month=11) " + "set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes PARTITION (month=11, year=2010) " + "set fileformat parquetfile");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition1') set fileformat parquet");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='PaRtiTion1') set location '/a/b/c'");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set serdeproperties ('a'='2')");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("alter table functional.alltypes " + "set serdeproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        AnalyzesOk("alter table functional.alltypes " + "set tblproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
    }
    // Arbitrary exprs as partition key values. Constant exprs are ok.
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set location '/a/b'");
    // Arbitrary exprs as partition key values. Non-constant exprs should fail.
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set fileformat sequencefile", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set location '/a/b'", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    // Partition spec does not exist
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set location '/a/b'", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set tblproperties('a'='1')", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010) " + "set tblproperties('a'='1')", "Items in partition spec must exactly match the partition columns " + "in the table definition: functional.alltypes (1 vs 2)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010, year=2010) " + "set location '/a/b'", "Duplicate partition key name: year");
    AnalysisError("alter table functional.alltypes PARTITION (month=11, year=2014) " + "set fileformat sequencefile", "Partition spec does not exist: (month=11, year=2014)");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set fileformat sequencefile", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set location '/a/b/c'", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set location '/a/b'", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set fileformat sequencefile", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set location '/a/b'", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set fileformat sequencefile", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes set fileformat sequencefile", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set fileformat rcfile", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table db_does_not_exist.alltypes set location '/a/b'", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set location '/a/b'", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table functional.no_tbl partition(i=1) set location '/a/b'", "Table does not exist: functional.no_tbl");
    AnalysisError("alter table no_db.alltypes partition(i=1) set fileformat textfile", "Database does not exist: no_db");
    // Valid location
    AnalyzesOk("alter table functional.alltypes set location " + "'hdfs://localhost:20500/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'s3n://bucket/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'file:///test-warehouse/a/b'");
    // Invalid location
    AnalysisError("alter table functional.alltypes set location 'test/warehouse'", "URI path must be absolute: test/warehouse");
    AnalysisError("alter table functional.alltypes set location 'blah:///warehouse/'", "No FileSystem for scheme: blah");
    AnalysisError("alter table functional.alltypes set location ''", "URI path cannot be empty.");
    AnalysisError("alter table functional.alltypes set location '      '", "URI path cannot be empty.");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view set fileformat sequencefile", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource set fileformat parquet", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE SET on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes set tblproperties('a'='b')", "ALTER TABLE SET not currently supported on HBase tables.");
}
#method_after
@Test
public void TestAlterTableSet() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes set serdeproperties('a'='2')");
    AnalyzesOk("alter table functional.alltypes PARTITION (Year=2010, month=11) " + "set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes PARTITION (month=11, year=2010) " + "set fileformat parquetfile");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition1') set fileformat parquet");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='PaRtiTion1') set location '/a/b/c'");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set serdeproperties ('a'='2')");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("alter table functional.alltypes " + "set serdeproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        AnalyzesOk("alter table functional.alltypes " + "set tblproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set tblproperties('storage_handler'='1')", "Changing the 'storage_handler' table property is not supported to protect " + "against metadata corruption.");
    }
    // Arbitrary exprs as partition key values. Constant exprs are ok.
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set location '/a/b'");
    // Arbitrary exprs as partition key values. Non-constant exprs should fail.
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set fileformat sequencefile", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set location '/a/b'", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    // Partition spec does not exist
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set location '/a/b'", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set tblproperties('a'='1')", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010) " + "set tblproperties('a'='1')", "Items in partition spec must exactly match the partition columns " + "in the table definition: functional.alltypes (1 vs 2)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010, year=2010) " + "set location '/a/b'", "Duplicate partition key name: year");
    AnalysisError("alter table functional.alltypes PARTITION (month=11, year=2014) " + "set fileformat sequencefile", "Partition spec does not exist: (month=11, year=2014)");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set fileformat sequencefile", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set location '/a/b/c'", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set location '/a/b'", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set fileformat sequencefile", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set location '/a/b'", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set fileformat sequencefile", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes set fileformat sequencefile", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set fileformat rcfile", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table db_does_not_exist.alltypes set location '/a/b'", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set location '/a/b'", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table functional.no_tbl partition(i=1) set location '/a/b'", "Table does not exist: functional.no_tbl");
    AnalysisError("alter table no_db.alltypes partition(i=1) set fileformat textfile", "Database does not exist: no_db");
    // Valid location
    AnalyzesOk("alter table functional.alltypes set location " + "'hdfs://localhost:20500/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'s3n://bucket/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'file:///test-warehouse/a/b'");
    // Invalid location
    AnalysisError("alter table functional.alltypes set location 'test/warehouse'", "URI path must be absolute: test/warehouse");
    AnalysisError("alter table functional.alltypes set location 'blah:///warehouse/'", "No FileSystem for scheme: blah");
    AnalysisError("alter table functional.alltypes set location ''", "URI path cannot be empty.");
    AnalysisError("alter table functional.alltypes set location '      '", "URI path cannot be empty.");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view set fileformat sequencefile", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource set fileformat parquet", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE SET on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes set tblproperties('a'='b')", "ALTER TABLE SET not currently supported on HBase tables.");
}
#end_block

#method_before
@Test
public void TestCreateTableAsSelect() throws AnalysisException {
    // Constant select.
    AnalyzesOk("create table newtbl as select 1+2, 'abc'");
    // Select from partitioned and unpartitioned tables using different
    // queries.
    AnalyzesOk("create table newtbl stored as textfile " + "as select * from functional.jointbl");
    AnalyzesOk("create table newtbl stored as parquetfile " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl stored as parquet " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl as select int_col from functional.alltypes");
    AnalyzesOk("create table functional.newtbl " + "as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.tbl as select a.* from functional.alltypes a " + "join functional.alltypes b on (a.int_col=b.int_col) limit 1000");
    // Caching operations
    AnalyzesOk("create table functional.newtbl cached in 'testPool'" + " as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.newtbl uncached" + " as select count(*) as CNT from functional.alltypes");
    // Table already exists with and without IF NOT EXISTS
    AnalysisError("create table functional.alltypes as select 1", "Table already exists: functional.alltypes");
    AnalyzesOk("create table if not exists functional.alltypes as select 1");
    // Database does not exist
    AnalysisError("create table db_does_not_exist.new_table as select 1", "Database does not exist: db_does_not_exist");
    // Analysis errors in the SELECT statement
    AnalysisError("create table newtbl as select * from tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    AnalysisError("create table newtbl as select 1 as c1, 2 as c1", "Duplicate column name: c1");
    // Unsupported file formats
    AnalysisError("create table foo stored as sequencefile as select 1", "CREATE TABLE AS SELECT does not support the (SEQUENCEFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE, KUDU)");
    AnalysisError("create table foo stored as RCFILE as select 1", "CREATE TABLE AS SELECT does not support the (RCFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE, KUDU)");
    // CTAS with a WITH clause and inline view (IMPALA-1100)
    AnalyzesOk("create table test_with as with with_1 as (select 1 as int_col from " + "functional.alltypes as t1 right join (select 1 as int_col from " + "functional.alltypestiny as t1) as t2 on t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
    // CTAS with a correlated inline view.
    AnalyzesOk("create table test as select id, item " + "from functional.allcomplextypes b, (select item from b.int_array_col) v1");
    // Correlated inline view in WITH clause.
    AnalyzesOk("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col) v1) select * from w");
    // CTAS with illegal correlated inline views.
    AnalysisError("create table test as select id, item " + "from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    AnalysisError("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1) select * from w", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    // CTAS into partitioned table.
    AnalyzesOk("create table p partitioned by (int_col) as " + "select double_col, int_col from functional.alltypes");
    AnalyzesOk("create table p partitioned by (int_col) as " + "select sum(double_col), int_col from functional.alltypes group by int_col");
    // At least one non-partition column must be specified.
    AnalysisError("create table p partitioned by (int_col, tinyint_col) as " + "select int_col, tinyint_col from functional.alltypes", "Number of partition columns (2) must be smaller than the number of columns in " + "the select statement (2).");
    // Order of the columns is important and not automatically corrected.
    AnalysisError("create table p partitioned by (int_col) as " + "select double_col, int_col, tinyint_col from functional.alltypes", "Partition column name mismatch: int_col != tinyint_col");
    AnalysisError("create table p partitioned by (tinyint_col, int_col) as " + "select double_col, int_col, tinyint_col from functional.alltypes", "Partition column name mismatch: tinyint_col != int_col");
    // CTAS into managed Kudu tables
    AnalyzesOk("create table t primary key (id) distribute by hash (id) into 3 buckets" + " stored as kudu as select id, bool_col, tinyint_col, smallint_col, int_col, " + "bigint_col, float_col, double_col, date_string_col, string_col " + "from functional.alltypestiny");
    // CTAS in an external Kudu table
    AnalysisError("create external table t stored as kudu " + "tblproperties('kudu.table_name'='t') as select id, int_col from " + "functional.alltypestiny", "CREATE TABLE AS SELECT is not supported for " + "external Kudu tables.");
}
#method_after
@Test
public void TestCreateTableAsSelect() throws AnalysisException {
    // Constant select.
    AnalyzesOk("create table newtbl as select 1+2, 'abc'");
    // Select from partitioned and unpartitioned tables using different
    // queries.
    AnalyzesOk("create table newtbl stored as textfile " + "as select * from functional.jointbl");
    AnalyzesOk("create table newtbl stored as parquetfile " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl stored as parquet " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl as select int_col from functional.alltypes");
    AnalyzesOk("create table functional.newtbl " + "as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.tbl as select a.* from functional.alltypes a " + "join functional.alltypes b on (a.int_col=b.int_col) limit 1000");
    // Caching operations
    AnalyzesOk("create table functional.newtbl cached in 'testPool'" + " as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.newtbl uncached" + " as select count(*) as CNT from functional.alltypes");
    // Table already exists with and without IF NOT EXISTS
    AnalysisError("create table functional.alltypes as select 1", "Table already exists: functional.alltypes");
    AnalyzesOk("create table if not exists functional.alltypes as select 1");
    // Database does not exist
    AnalysisError("create table db_does_not_exist.new_table as select 1", "Database does not exist: db_does_not_exist");
    // Analysis errors in the SELECT statement
    AnalysisError("create table newtbl as select * from tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    AnalysisError("create table newtbl as select 1 as c1, 2 as c1", "Duplicate column name: c1");
    // Unsupported file formats
    AnalysisError("create table foo stored as sequencefile as select 1", "CREATE TABLE AS SELECT does not support the (SEQUENCEFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE, KUDU)");
    AnalysisError("create table foo stored as RCFILE as select 1", "CREATE TABLE AS SELECT does not support the (RCFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE, KUDU)");
    // CTAS with a WITH clause and inline view (IMPALA-1100)
    AnalyzesOk("create table test_with as with with_1 as (select 1 as int_col from " + "functional.alltypes as t1 right join (select 1 as int_col from " + "functional.alltypestiny as t1) as t2 on t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
    // CTAS with a correlated inline view.
    AnalyzesOk("create table test as select id, item " + "from functional.allcomplextypes b, (select item from b.int_array_col) v1");
    // Correlated inline view in WITH clause.
    AnalyzesOk("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col) v1) select * from w");
    // CTAS with illegal correlated inline views.
    AnalysisError("create table test as select id, item " + "from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    AnalysisError("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1) select * from w", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    // CTAS into partitioned table.
    AnalyzesOk("create table p partitioned by (int_col) as " + "select double_col, int_col from functional.alltypes");
    AnalyzesOk("create table p partitioned by (int_col) as " + "select sum(double_col), int_col from functional.alltypes group by int_col");
    // At least one non-partition column must be specified.
    AnalysisError("create table p partitioned by (int_col, tinyint_col) as " + "select int_col, tinyint_col from functional.alltypes", "Number of partition columns (2) must be smaller than the number of columns in " + "the select statement (2).");
    // Order of the columns is important and not automatically corrected.
    AnalysisError("create table p partitioned by (int_col) as " + "select double_col, int_col, tinyint_col from functional.alltypes", "Partition column name mismatch: int_col != tinyint_col");
    AnalysisError("create table p partitioned by (tinyint_col, int_col) as " + "select double_col, int_col, tinyint_col from functional.alltypes", "Partition column name mismatch: tinyint_col != int_col");
    // CTAS into managed Kudu tables
    AnalyzesOk("create table t primary key (id) distribute by hash (id) into 3 buckets" + " stored as kudu as select id, bool_col, tinyint_col, smallint_col, int_col, " + "bigint_col, float_col, double_col, date_string_col, string_col " + "from functional.alltypestiny");
    AnalyzesOk("create table t primary key (id) distribute by range (id) " + "(partition values < 10, partition 20 <= values < 30, partition value = 50) " + "stored as kudu as select id, bool_col, tinyint_col, smallint_col, int_col, " + "bigint_col, float_col, double_col, date_string_col, string_col " + "from functional.alltypestiny");
    AnalyzesOk("create table t primary key (id) distribute by hash (id) into 3 buckets, " + "range (id) (partition values < 10, partition 10 <= values < 20, " + "partition value = 30) stored as kudu as select id, bool_col, tinyint_col, " + "smallint_col, int_col, bigint_col, float_col, double_col, date_string_col, " + "string_col from functional.alltypestiny");
    // CTAS in an external Kudu table
    AnalysisError("create external table t stored as kudu " + "tblproperties('kudu.table_name'='t') as select id, int_col from " + "functional.alltypestiny", "CREATE TABLE AS SELECT is not supported for " + "external Kudu tables.");
    // CTAS into Kudu tables with unsupported types
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, timestamp_col from functional.alltypestiny", "Cannot create table 't': Type TIMESTAMP is not supported in Kudu");
    AnalysisError("create table t primary key (cs) distribute by hash into 3 buckets" + " stored as kudu as select cs from functional.chars_tiny", "Cannot create table 't': Type CHAR(5) is not supported in Kudu");
    AnalysisError("create table t primary key (vc) distribute by hash into 3 buckets" + " stored as kudu as select vc from functional.chars_tiny", "Cannot create table 't': Type VARCHAR(32) is not supported in Kudu");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select c1 as id from functional.decimal_tiny", "Cannot create table 't': Type DECIMAL(10,4) is not supported in Kudu");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, s from functional.complextypes_fileformat", "Expr 's' in select list returns a complex type 'STRUCT<f1:STRING,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, m from functional.complextypes_fileformat", "Expr 'm' in select list returns a complex type 'MAP<STRING,BIGINT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, a from functional.complextypes_fileformat", "Expr 'a' in select list returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
}
#end_block

#method_before
@Test
public void TestCreateManagedKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Test primary keys and distribute by clauses
    AnalyzesOk("create table tab (x int primary key) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, primary key(x)) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) " + "distribute by hash(x, y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x)) " + "distribute by hash(x) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) " + "distribute by hash(y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y string, primary key (x)) distribute by " + "hash (x) into 3 buckets, range (x) (partition values < 1, partition " + "1 <= values < 10, partition 10 <= values < 20, partition value = 30) " + "stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) distribute by " + "range (x, y) (partition value = (2001, 1), partition value = (2002, 1), " + "partition value = (2003, 2)) stored as kudu");
    // Multilevel partitioning. Data is split into 3 buckets based on 'x' and each
    // bucket is partitioned into 4 tablets based on the range partitions of 'y'.
    AnalyzesOk("create table tab (x int, y string, primary key(x, y)) " + "distribute by hash(x) into 3 buckets, range(y) " + "(partition values < 'aa', partition 'aa' <= values < 'bb', " + "partition 'bb' <= values < 'cc', partition 'cc' <= values) " + "stored as kudu");
    // Key column in upper case
    AnalyzesOk("create table tab (x int, y int, primary key (X)) " + "distribute by hash (x) into 8 buckets stored as kudu");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b) into 8 buckets, hash(c) into 2 buckets stored as " + "kudu");
    // No columns specified in the DISTRIBUTE BY HASH clause
    AnalyzesOk("create table tab (a int primary key, b int, c int, d int) " + "distribute by hash into 8 buckets stored as kudu");
    // Distribute range data types are picked up during analysis and forwarded to Kudu.
    // Column names in distribute params should also be case-insensitive.
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key(a, b, c, d))" + "distribute by hash (a, B, c) into 8 buckets, " + "range (A) (partition values < 1, partition 1 <= values < 2, " + "partition 2 <= values < 3, partition 3 <= values < 4, partition 4 <= values) " + "stored as kudu");
    // Allowing range distribution on a subset of the primary keys
    AnalyzesOk("create table tab (id int, name string, valf float, vali bigint, " + "primary key (id, name)) distribute by range (name) " + "(partition 'aa' < values <= 'bb') stored as kudu");
    // Null values in range partition values
    AnalysisError("create table tab (id int, name string, primary key(id, name)) " + "distribute by hash (id) into 3 buckets, range (name) " + "(partition value = null, partition value = 1) stored as kudu", "Range partition values cannot be NULL. Range partition: 'PARTITION " + "VALUE = NULL'");
    // Primary key specified in tblproperties
    AnalysisError(String.format("create table tab (x int) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('%s' = 'x')", KuduTable.KEY_KEY_COLUMNS), "PRIMARY KEY must be used instead of the table " + "property");
    // Primary key column that doesn't exist
    AnalysisError("create table tab (x int, y int, primary key (z)) " + "distribute by hash (x) into 8 buckets stored as kudu", "PRIMARY KEY column 'z' does not exist in the table");
    // Invalid composite primary key
    AnalysisError("create table tab (x int primary key, primary key(x)) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    AnalysisError("create table tab (x int primary key, y int primary key) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    // Specifying the same primary key column multiple times
    AnalysisError("create table tab (x int, primary key (x, x)) distribute by hash (x) " + "into 8 buckets stored as kudu", "Column 'x' is listed multiple times as a PRIMARY KEY.");
    // Number of range partition boundary values should be equal to the number of range
    // columns.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by range(a) (partition value = (1, 2), " + "partition value = 3, partition value = 4) stored as kudu", "Number of specified range partition values is different than the number of " + "projected key columns: (2 vs 1). Range partition: 'PARTITION VALUE = (1,2)'");
    // Key ranges must match the column types.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by hash (a, b, c) into 8 buckets, range (a) " + "(partition value = 1, partition value = 'abc', partition 3 <= values) " + "stored as kudu", "Range partition value 'abc' (type: STRING) is not type " + "compatible with column 'a' (type: INT).");
    // Non-key column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b string, c bigint, primary key (a)) " + "distribute by range (b) (partition value = 'abc') stored as kudu", "Column 'b' in 'RANGE (b) (PARTITION VALUE = 'abc')' is not a key column. " + "Only key columns can be used in DISTRIBUTE BY.");
    // No float range partition values
    AnalysisError("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b, c) into 8 buckets, " + "range (a) (partition value = 1.2, partition value = 2) stored as kudu", "Range partition value 1.2 (type: DECIMAL(2,1)) is not type compatible with " + "column 'a' (type: INT).");
    // Non-existing column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b int, primary key (a, b)) " + "distribute by range(unknown_column) (partition value = 'abc') stored as kudu", "Column 'unknown_column' in 'RANGE (unknown_column) (PARTITION VALUE = 'abc')' " + "is not a key column. Only key columns can be used in DISTRIBUTE BY");
    // Kudu table name is specified in tblproperties
    AnalyzesOk("create table tab (x int primary key) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('kudu.table_name'='tab_1'," + "'kudu.num_tablet_replicas'='1'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081')");
    // No port is specified in kudu master address
    AnalyzesOk("create table tdata_no_port (id int primary key, name string, " + "valf float, vali bigint) distribute by range(id) (partition values <= 10, " + "partition 10 < values <= 30, partition 30 < values) " + "stored as kudu tblproperties('kudu.master_addresses'='127.0.0.1')");
    // Not using the STORED AS KUDU syntax to specify a Kudu table
    AnalysisError("create table tab (x int primary key) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    AnalysisError("create table tab (x int primary key) stored as kudu tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    // Invalid value for number of replicas
    AnalysisError("create table t (x int primary key) stored as kudu tblproperties (" + "'kudu.num_tablet_replicas'='1.1')", "Table property 'kudu.num_tablet_replicas' must be an integer.");
    // Don't allow caching
    AnalysisError("create table tab (x int primary key) stored as kudu cached in " + "'testPool'", "A Kudu table cannot be cached in HDFS.");
    // LOCATION cannot be used with Kudu tables
    AnalysisError("create table tab (a int primary key) distribute by hash (a) " + "into 3 buckets stored as kudu location '/test-warehouse/'", "LOCATION cannot be specified for a Kudu table.");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int, primary key (a)) stored as kudu", "Table distribution must be specified for managed Kudu tables.");
    AnalysisError("create table tab (a int) stored as kudu", "A primary key is required for a Kudu table.");
    // Using ROW FORMAT with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "row format delimited escaped by 'X' stored as kudu", "ROW FORMAT cannot be specified for file format KUDU.");
    // Using PARTITIONED BY with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "partitioned by (y int) stored as kudu", "PARTITIONED BY cannot be used " + "in Kudu tables.");
}
#method_after
@Test
public void TestCreateManagedKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Test primary keys and distribute by clauses
    AnalyzesOk("create table tab (x int primary key) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, primary key(x)) distribute by hash(x) " + "into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) " + "distribute by hash(x, y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x)) " + "distribute by hash(x) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) " + "distribute by hash(y) into 8 buckets stored as kudu");
    AnalyzesOk("create table tab (x int, y string, primary key (x)) distribute by " + "hash (x) into 3 buckets, range (x) (partition values < 1, partition " + "1 <= values < 10, partition 10 <= values < 20, partition value = 30) " + "stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key (x, y)) distribute by " + "range (x, y) (partition value = (2001, 1), partition value = (2002, 1), " + "partition value = (2003, 2)) stored as kudu");
    // Non-literal boundary values in range partitions
    AnalyzesOk("create table tab (x int, y int, primary key (x)) distribute by " + "range (x) (partition values < 1 + 1, partition (1+3) + 2 < values < 10, " + "partition factorial(4) < values < factorial(5), " + "partition value = factorial(6)) stored as kudu");
    AnalyzesOk("create table tab (x int, y int, primary key(x, y)) distribute by " + "range(x, y) (partition value = (1+1, 2+2), partition value = ((1+1+1)+1, 10), " + "partition value = (cast (30 as int), factorial(5))) stored as kudu");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values < x + 1) stored as kudu", "Only constant values are allowed " + "for range-partition bounds: x + 1");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= isnull(null, null)) stored as kudu", "Range partition " + "values cannot be NULL. Range partition: 'PARTITION VALUES <= " + "isnull(NULL, NULL)'");
    AnalysisError("create table tab (x int primary key) distribute by range (x) " + "(partition values <= (select count(*) from functional.alltypestiny)) " + "stored as kudu", "Only constant values are allowed for range-partition " + "bounds: (SELECT count(*) FROM functional.alltypestiny)");
    // Multilevel partitioning. Data is split into 3 buckets based on 'x' and each
    // bucket is partitioned into 4 tablets based on the range partitions of 'y'.
    AnalyzesOk("create table tab (x int, y string, primary key(x, y)) " + "distribute by hash(x) into 3 buckets, range(y) " + "(partition values < 'aa', partition 'aa' <= values < 'bb', " + "partition 'bb' <= values < 'cc', partition 'cc' <= values) " + "stored as kudu");
    // Key column in upper case
    AnalyzesOk("create table tab (x int, y int, primary key (X)) " + "distribute by hash (x) into 8 buckets stored as kudu");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b) into 8 buckets, hash(c) into 2 buckets stored as " + "kudu");
    // No columns specified in the DISTRIBUTE BY HASH clause
    AnalyzesOk("create table tab (a int primary key, b int, c int, d int) " + "distribute by hash into 8 buckets stored as kudu");
    // Distribute range data types are picked up during analysis and forwarded to Kudu.
    // Column names in distribute params should also be case-insensitive.
    AnalyzesOk("create table tab (a int, b int, c int, d int, primary key(a, b, c, d))" + "distribute by hash (a, B, c) into 8 buckets, " + "range (A) (partition values < 1, partition 1 <= values < 2, " + "partition 2 <= values < 3, partition 3 <= values < 4, partition 4 <= values) " + "stored as kudu");
    // Allowing range distribution on a subset of the primary keys
    AnalyzesOk("create table tab (id int, name string, valf float, vali bigint, " + "primary key (id, name)) distribute by range (name) " + "(partition 'aa' < values <= 'bb') stored as kudu");
    // Null values in range partition values
    AnalysisError("create table tab (id int, name string, primary key(id, name)) " + "distribute by hash (id) into 3 buckets, range (name) " + "(partition value = null, partition value = 1) stored as kudu", "Range partition values cannot be NULL. Range partition: 'PARTITION " + "VALUE = NULL'");
    // Primary key specified in tblproperties
    AnalysisError(String.format("create table tab (x int) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('%s' = 'x')", KuduTable.KEY_KEY_COLUMNS), "PRIMARY KEY must be used instead of the table " + "property");
    // Primary key column that doesn't exist
    AnalysisError("create table tab (x int, y int, primary key (z)) " + "distribute by hash (x) into 8 buckets stored as kudu", "PRIMARY KEY column 'z' does not exist in the table");
    // Invalid composite primary key
    AnalysisError("create table tab (x int primary key, primary key(x)) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    AnalysisError("create table tab (x int primary key, y int primary key) stored " + "as kudu", "Multiple primary keys specified. Composite primary keys can " + "be specified using the PRIMARY KEY (col1, col2, ...) syntax at the end " + "of the column definition.");
    // Specifying the same primary key column multiple times
    AnalysisError("create table tab (x int, primary key (x, x)) distribute by hash (x) " + "into 8 buckets stored as kudu", "Column 'x' is listed multiple times as a PRIMARY KEY.");
    // Number of range partition boundary values should be equal to the number of range
    // columns.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by range(a) (partition value = (1, 2), " + "partition value = 3, partition value = 4) stored as kudu", "Number of specified range partition values is different than the number of " + "distribution columns: (2 vs 1). Range partition: 'PARTITION VALUE = (1,2)'");
    // Key ranges must match the column types.
    AnalysisError("create table tab (a int, b int, c int, d int, primary key(a, b, c)) " + "distribute by hash (a, b, c) into 8 buckets, range (a) " + "(partition value = 1, partition value = 'abc', partition 3 <= values) " + "stored as kudu", "Range partition value 'abc' (type: STRING) is not type " + "compatible with distribution column 'a' (type: INT).");
    AnalysisError("create table tab (a tinyint primary key) distribute by range (a) " + "(partition value = 128) stored as kudu", "Range partition value 128 " + "(type: SMALLINT) is not type compatible with distribution column 'a' " + "(type: TINYINT)");
    AnalysisError("create table tab (a smallint primary key) distribute by range (a) " + "(partition value = 32768) stored as kudu", "Range partition value 32768 " + "(type: INT) is not type compatible with distribution column 'a' " + "(type: SMALLINT)");
    AnalysisError("create table tab (a int primary key) distribute by range (a) " + "(partition value = 2147483648) stored as kudu", "Range partition value " + "2147483648 (type: BIGINT) is not type compatible with distribution column 'a' " + "(type: INT)");
    AnalysisError("create table tab (a bigint primary key) distribute by range (a) " + "(partition value = 9223372036854775808) stored as kudu", "Range partition " + "value 9223372036854775808 (type: DECIMAL(19,0)) is not type compatible with " + "distribution column 'a' (type: BIGINT)");
    // Test implicit casting/folding of partition values.
    AnalyzesOk("create table tab (a int primary key) distribute by range (a) " + "(partition value = false, partition value = true) stored as kudu");
    // Non-key column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b string, c bigint, primary key (a)) " + "distribute by range (b) (partition value = 'abc') stored as kudu", "Column 'b' in 'RANGE (b) (PARTITION VALUE = 'abc')' is not a key column. " + "Only key columns can be used in DISTRIBUTE BY.");
    // No float range partition values
    AnalysisError("create table tab (a int, b int, c int, d int, primary key (a, b, c))" + "distribute by hash (a, b, c) into 8 buckets, " + "range (a) (partition value = 1.2, partition value = 2) stored as kudu", "Range partition value 1.2 (type: DECIMAL(2,1)) is not type compatible with " + "distribution column 'a' (type: INT).");
    // Non-existing column used in DISTRIBUTE BY
    AnalysisError("create table tab (a int, b int, primary key (a, b)) " + "distribute by range(unknown_column) (partition value = 'abc') stored as kudu", "Column 'unknown_column' in 'RANGE (unknown_column) (PARTITION VALUE = 'abc')' " + "is not a key column. Only key columns can be used in DISTRIBUTE BY");
    // Kudu table name is specified in tblproperties
    AnalyzesOk("create table tab (x int primary key) distribute by hash (x) " + "into 8 buckets stored as kudu tblproperties ('kudu.table_name'='tab_1'," + "'kudu.num_tablet_replicas'='1'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081')");
    // No port is specified in kudu master address
    AnalyzesOk("create table tdata_no_port (id int primary key, name string, " + "valf float, vali bigint) distribute by range(id) (partition values <= 10, " + "partition 10 < values <= 30, partition 30 < values) " + "stored as kudu tblproperties('kudu.master_addresses'='127.0.0.1')");
    // Not using the STORED AS KUDU syntax to specify a Kudu table
    AnalysisError("create table tab (x int primary key) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    AnalysisError("create table tab (x int primary key) stored as kudu tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler')", CreateTableStmt.KUDU_STORAGE_HANDLER_ERROR_MESSAGE);
    // Invalid value for number of replicas
    AnalysisError("create table t (x int primary key) stored as kudu tblproperties (" + "'kudu.num_tablet_replicas'='1.1')", "Table property 'kudu.num_tablet_replicas' must be an integer.");
    // Don't allow caching
    AnalysisError("create table tab (x int primary key) stored as kudu cached in " + "'testPool'", "A Kudu table cannot be cached in HDFS.");
    // LOCATION cannot be used with Kudu tables
    AnalysisError("create table tab (a int primary key) distribute by hash (a) " + "into 3 buckets stored as kudu location '/test-warehouse/'", "LOCATION cannot be specified for a Kudu table.");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int, primary key (a)) stored as kudu", "Table distribution must be specified for managed Kudu tables.");
    AnalysisError("create table tab (a int) stored as kudu", "A primary key is required for a Kudu table.");
    // Using ROW FORMAT with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "row format delimited escaped by 'X' stored as kudu", "ROW FORMAT cannot be specified for file format KUDU.");
    // Using PARTITIONED BY with a Kudu table
    AnalysisError("create table tab (x int primary key) " + "partitioned by (y int) stored as kudu", "PARTITIONED BY cannot be used " + "in Kudu tables.");
    // Test unsupported Kudu types
    List<String> unsupportedTypes = Lists.newArrayList("DECIMAL(9,0)", "TIMESTAMP", "VARCHAR(20)", "CHAR(20)", "STRUCT<F1:INT,F2:STRING>", "ARRAY<INT>", "MAP<STRING,STRING>");
    for (String t : unsupportedTypes) {
        String expectedError = String.format("Cannot create table 'tab': Type %s is not supported in Kudu", t);
        // Unsupported type is PK and partition col
        String stmt = String.format("create table tab (x %s primary key) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
        // Unsupported type is not PK/partition col
        stmt = String.format("create table tab (x int primary key, y %s) " + "distribute by hash(x) into 3 buckets stored as kudu", t);
        AnalysisError(stmt, expectedError);
    }
}
#end_block

#method_before
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the distribution schemes
    List<TDistributeParam> distributeParams = params.getDistribute_by();
    if (distributeParams != null) {
        boolean hasRangePartitioning = false;
        for (TDistributeParam distParam : distributeParams) {
            if (distParam.isSetBy_hash_param()) {
                Preconditions.checkState(!distParam.isSetBy_range_param());
                tableOpts.addHashPartitions(distParam.getBy_hash_param().getColumns(), distParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(distParam.isSetBy_range_param());
                hasRangePartitioning = true;
                List<String> rangePartitionColumns = distParam.getBy_range_param().getColumns();
                tableOpts.setRangePartitionColumns(rangePartitionColumns);
                for (TRangePartition rangePartition : distParam.getBy_range_param().getRange_partitions()) {
                    Preconditions.checkState(rangePartition.isSetLower_bound_values() || rangePartition.isSetUpper_bound_values());
                    Pair<PartialRow, RangePartitionBound> lowerBound = KuduUtil.buildRangePartitionBound(schema, rangePartitionColumns, rangePartition.getLower_bound_values(), rangePartition.getLower_bound_op());
                    Pair<PartialRow, RangePartitionBound> upperBound = KuduUtil.buildRangePartitionBound(schema, rangePartitionColumns, rangePartition.getUpper_bound_values(), rangePartition.getUpper_bound_op());
                    tableOpts.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        try {
            int r = Integer.parseInt(replication);
            Preconditions.checkState(r > 0);
            tableOpts.setNumReplicas(r);
        } catch (NumberFormatException e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication), e);
        }
    }
    return tableOpts;
}
#method_after
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the distribution schemes
    List<TDistributeParam> distributeParams = params.getDistribute_by();
    if (distributeParams != null) {
        boolean hasRangePartitioning = false;
        for (TDistributeParam distParam : distributeParams) {
            if (distParam.isSetBy_hash_param()) {
                Preconditions.checkState(!distParam.isSetBy_range_param());
                tableOpts.addHashPartitions(distParam.getBy_hash_param().getColumns(), distParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(distParam.isSetBy_range_param());
                hasRangePartitioning = true;
                List<String> rangePartitionColumns = distParam.getBy_range_param().getColumns();
                tableOpts.setRangePartitionColumns(rangePartitionColumns);
                for (TRangePartition rangePartition : distParam.getBy_range_param().getRange_partitions()) {
                    Preconditions.checkState(rangePartition.isSetLower_bound_values() || rangePartition.isSetUpper_bound_values());
                    Pair<PartialRow, RangePartitionBound> lowerBound = KuduUtil.buildRangePartitionBound(schema, rangePartitionColumns, rangePartition.getLower_bound_values(), rangePartition.isIs_lower_bound_inclusive());
                    Pair<PartialRow, RangePartitionBound> upperBound = KuduUtil.buildRangePartitionBound(schema, rangePartitionColumns, rangePartition.getUpper_bound_values(), rangePartition.isIs_upper_bound_inclusive());
                    tableOpts.addRangePartition(lowerBound.first, upperBound.first, lowerBound.second, upperBound.second);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        try {
            int r = Integer.parseInt(replication);
            Preconditions.checkState(r > 0);
            tableOpts.setNumReplicas(r);
        } catch (NumberFormatException e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication), e);
        }
    }
    return tableOpts;
}
#end_block

#method_before
public static void validateKuduTblExists(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    Preconditions.checkState(!Strings.isNullOrEmpty(masterHosts));
    String kuduTableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    try (KuduClient kudu = new KuduClient.KuduClientBuilder(masterHosts).build()) {
        kudu.tableExists(kuduTableName);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Kudu table '%s' does not exist " + "on master '%s'", kuduTableName, masterHosts), e);
    }
}
#method_after
public static void validateKuduTblExists(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    Preconditions.checkArgument(KuduTable.isKuduTable(msTbl));
    Map<String, String> properties = msTbl.getParameters();
    String masterHosts = properties.get(KuduTable.KEY_MASTER_HOSTS);
    Preconditions.checkState(!Strings.isNullOrEmpty(masterHosts));
    String kuduTableName = properties.get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    try (KuduClient kudu = new KuduClient.KuduClientBuilder(masterHosts).build()) {
        kudu.tableExists(kuduTableName);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Kudu table '%s' does not exist " + "on master '%s'", kuduTableName, masterHosts), e);
    }
}
#end_block

#method_before
public static Pair<PartialRow, RangePartitionBound> buildRangePartitionBound(Schema schema, List<String> rangePartitionColumns, List<TExpr> boundaryValues, TComparisonOp op) throws ImpalaRuntimeException {
    if (boundaryValues == null || boundaryValues.isEmpty()) {
        // TODO: Do we need to set the bound type?
        return new Pair<PartialRow, RangePartitionBound>(new PartialRow(schema), RangePartitionBound.INCLUSIVE_BOUND);
    }
    Preconditions.checkNotNull(op);
    PartialRow bound = parseRangePartitionBoundaryValues(schema, rangePartitionColumns, boundaryValues);
    RangePartitionBound boundType = null;
    if (op == TComparisonOp.EQ || op == TComparisonOp.LE) {
        boundType = RangePartitionBound.INCLUSIVE_BOUND;
    } else {
        boundType = RangePartitionBound.EXCLUSIVE_BOUND;
    }
    return new Pair<PartialRow, RangePartitionBound>(bound, boundType);
}
#method_after
public static Pair<PartialRow, RangePartitionBound> buildRangePartitionBound(Schema schema, List<String> rangePartitionColumns, List<TExpr> boundaryValues, boolean isInclusiveBound) throws ImpalaRuntimeException {
    if (boundaryValues == null || boundaryValues.isEmpty()) {
        // TODO: Do we need to set the bound type?
        return new Pair<PartialRow, RangePartitionBound>(new PartialRow(schema), RangePartitionBound.INCLUSIVE_BOUND);
    }
    PartialRow bound = parseRangePartitionBoundaryValues(schema, rangePartitionColumns, boundaryValues);
    RangePartitionBound boundType = null;
    if (isInclusiveBound) {
        boundType = RangePartitionBound.INCLUSIVE_BOUND;
    } else {
        boundType = RangePartitionBound.EXCLUSIVE_BOUND;
    }
    return new Pair<PartialRow, RangePartitionBound>(bound, boundType);
}
#end_block

#method_before
public static org.apache.kudu.Type fromImpalaType(Type t) throws ImpalaRuntimeException {
    if (!t.isScalarType()) {
        throw new ImpalaRuntimeException(format("Non-scalar type %s is not supported in Kudu", t.toSql()));
    }
    ScalarType s = (ScalarType) t;
    switch(s.getPrimitiveType()) {
        case TINYINT:
            return org.apache.kudu.Type.INT8;
        case SMALLINT:
            return org.apache.kudu.Type.INT16;
        case INT:
            return org.apache.kudu.Type.INT32;
        case BIGINT:
            return org.apache.kudu.Type.INT64;
        case BOOLEAN:
            return org.apache.kudu.Type.BOOL;
        case CHAR:
            return org.apache.kudu.Type.STRING;
        case STRING:
            return org.apache.kudu.Type.STRING;
        case VARCHAR:
            return org.apache.kudu.Type.STRING;
        case DOUBLE:
            return org.apache.kudu.Type.DOUBLE;
        case FLOAT:
            return org.apache.kudu.Type.FLOAT;
        /* Fall through below */
        case INVALID_TYPE:
        case NULL_TYPE:
        case TIMESTAMP:
        case BINARY:
        case DATE:
        case DATETIME:
        case DECIMAL:
        default:
            throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", s.toSql()));
    }
}
#method_after
public static org.apache.kudu.Type fromImpalaType(Type t) throws ImpalaRuntimeException {
    if (!t.isScalarType()) {
        throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", t.toSql()));
    }
    ScalarType s = (ScalarType) t;
    switch(s.getPrimitiveType()) {
        case TINYINT:
            return org.apache.kudu.Type.INT8;
        case SMALLINT:
            return org.apache.kudu.Type.INT16;
        case INT:
            return org.apache.kudu.Type.INT32;
        case BIGINT:
            return org.apache.kudu.Type.INT64;
        case BOOLEAN:
            return org.apache.kudu.Type.BOOL;
        case STRING:
            return org.apache.kudu.Type.STRING;
        case DOUBLE:
            return org.apache.kudu.Type.DOUBLE;
        case FLOAT:
            return org.apache.kudu.Type.FLOAT;
        /* Fall through below */
        case INVALID_TYPE:
        case NULL_TYPE:
        case TIMESTAMP:
        case BINARY:
        case DATE:
        case DATETIME:
        case DECIMAL:
        case CHAR:
        case VARCHAR:
        default:
            throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", s.toSql()));
    }
}
#end_block

#method_before
public static Type toImpalaType(org.apache.kudu.Type t) throws ImpalaRuntimeException {
    switch(t) {
        case BOOL:
            return Type.BOOLEAN;
        case DOUBLE:
            return Type.DOUBLE;
        case FLOAT:
            return Type.FLOAT;
        case INT8:
            return Type.TINYINT;
        case INT16:
            return Type.SMALLINT;
        case INT32:
            return Type.INT;
        case INT64:
            return Type.BIGINT;
        case STRING:
            return Type.STRING;
        default:
            throw new ImpalaRuntimeException(String.format("Kudu type %s is not supported in Impala", t));
    }
}
#method_after
public static Type toImpalaType(org.apache.kudu.Type t) throws ImpalaRuntimeException {
    switch(t) {
        case BOOL:
            return Type.BOOLEAN;
        case DOUBLE:
            return Type.DOUBLE;
        case FLOAT:
            return Type.FLOAT;
        case INT8:
            return Type.TINYINT;
        case INT16:
            return Type.SMALLINT;
        case INT32:
            return Type.INT;
        case INT64:
            return Type.BIGINT;
        case STRING:
            return Type.STRING;
        default:
            throw new ImpalaRuntimeException(String.format("Kudu type '%s' is not supported in Impala", t.getName()));
    }
}
#end_block

#method_before
public void analyzeRangeParam(Analyzer analyzer) throws AnalysisException {
    for (RangePartition rangePartition : rangePartitions_) {
        rangePartition.analyze(analyzer);
        List<LiteralExpr> lowerBound = rangePartition.getLowerBound();
        List<LiteralExpr> upperBound = rangePartition.getUpperBound();
        if (!lowerBound.isEmpty() && lowerBound.size() != colNames_.size()) {
            throw new AnalysisException(String.format("Number of specified range " + "partition values is different than the number of projected key " + "columns: (%d vs %d). Range partition: '%s'", lowerBound.size(), colNames_.size(), rangePartition.toSql()));
        }
        if (!upperBound.isEmpty() && upperBound.size() != colNames_.size()) {
            throw new AnalysisException(String.format("Number of specified range " + "partition values is different than the number of projected key " + "columns: (%d vs %d). Range partition: '%s'", upperBound.size(), colNames_.size(), rangePartition.toSql()));
        }
        for (int i = 0; i < lowerBound.size(); ++i) {
            analyzeRangePartitionValue(lowerBound.get(i), pkColumnDefByName_.get(colNames_.get(i)), analyzer);
        }
        for (int i = 0; i < upperBound.size(); ++i) {
            analyzeRangePartitionValue(upperBound.get(i), pkColumnDefByName_.get(colNames_.get(i)), analyzer);
        }
    }
}
#method_after
public void analyzeRangeParam(Analyzer analyzer) throws AnalysisException {
    List<ColumnDef> pkColDefs = Lists.newArrayListWithCapacity(colNames_.size());
    for (String colName : colNames_) pkColDefs.add(pkColumnDefByName_.get(colName));
    for (RangePartition rangePartition : rangePartitions_) {
        rangePartition.analyze(analyzer, pkColDefs);
    }
}
#end_block

#method_before
public static RangePartition createFromRange(Pair<LiteralExpr, BinaryPredicate.Operator> lower, Pair<LiteralExpr, BinaryPredicate.Operator> upper) {
    List<LiteralExpr> lowerBoundExprs = Lists.newArrayListWithCapacity(1);
    BinaryPredicate.Operator lowerBoundOp = null;
    List<LiteralExpr> upperBoundExprs = Lists.newArrayListWithCapacity(1);
    BinaryPredicate.Operator upperBoundOp = null;
    if (lower != null) {
        lowerBoundExprs.add(lower.first);
        lowerBoundOp = lower.second;
    }
    if (upper != null) {
        upperBoundExprs.add(upper.first);
        upperBoundOp = upper.second;
    }
    return new RangePartition(lowerBoundExprs, lowerBoundOp, upperBoundExprs, upperBoundOp);
}
#method_after
public static RangePartition createFromRange(Pair<Expr, Boolean> lower, Pair<Expr, Boolean> upper) {
    List<Expr> lowerBoundExprs = Lists.newArrayListWithCapacity(1);
    boolean lowerBoundInclusive = false;
    List<Expr> upperBoundExprs = Lists.newArrayListWithCapacity(1);
    boolean upperBoundInclusive = false;
    if (lower != null) {
        lowerBoundExprs.add(lower.first);
        lowerBoundInclusive = lower.second;
    }
    if (upper != null) {
        upperBoundExprs.add(upper.first);
        upperBoundInclusive = upper.second;
    }
    return new RangePartition(lowerBoundExprs, lowerBoundInclusive, upperBoundExprs, upperBoundInclusive);
}
#end_block

#method_before
public static RangePartition createFromValues(List<LiteralExpr> values) {
    return new RangePartition(values, BinaryPredicate.Operator.EQ, values, BinaryPredicate.Operator.EQ);
}
#method_after
public static RangePartition createFromValues(List<Expr> values) {
    return new RangePartition(values, true, values, true);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    List<LiteralExpr> boundaryValues = Lists.newArrayList(lowerBound_);
    boundaryValues.addAll(upperBound_);
    for (LiteralExpr literal : boundaryValues) {
        literal.analyze(analyzer);
        if (literal.getType().isNull()) {
            throw new AnalysisException(String.format("Range partition values cannot be " + "NULL. Range partition: '%s'", toSql()));
        }
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    throw new IllegalStateException("Not implemented");
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    List<LiteralExpr> boundaryValues = Lists.newArrayList(lowerBound_);
    boundaryValues.addAll(upperBound_);
    for (LiteralExpr literal : boundaryValues) {
        literal.analyze(analyzer);
        if (literal.getType().isNull()) {
            throw new AnalysisException(String.format("Range partition values cannot be " + "NULL. Range partition: '%s'", toSql()));
        }
    }
}
#method_after
public void analyze(Analyzer analyzer, List<ColumnDef> distributionColDefs) throws AnalysisException {
    analyzeBoundaryValues(lowerBound_, distributionColDefs, analyzer);
    if (!isSingletonRange_) {
        analyzeBoundaryValues(upperBound_, distributionColDefs, analyzer);
    }
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder output = new StringBuilder();
    output.append("PARTITION ");
    if (lowerBoundOperator_ == BinaryPredicate.Operator.EQ) {
        output.append("VALUE = ");
        if (lowerBound_.size() > 1)
            output.append("(");
        List<String> literals = Lists.newArrayList();
        for (LiteralExpr literal : lowerBound_) literals.add(literal.toSql());
        output.append(Joiner.on(",").join(literals));
        if (lowerBound_.size() > 1)
            output.append(")");
    } else {
        if (lowerBoundOperator_ != null) {
            Preconditions.checkState(lowerBound_.size() == 1);
            output.append(lowerBound_.get(0).toSql() + " " + lowerBoundOperator_.toString());
            output.append(" ");
        }
        output.append("VALUES");
        if (upperBoundOperator_ != null) {
            Preconditions.checkState(upperBound_.size() == 1);
            output.append(" ");
            output.append(upperBound_.get(0).toSql() + " " + upperBoundOperator_.toString());
        }
    }
    return output.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder output = new StringBuilder();
    output.append("PARTITION ");
    if (isSingletonRange_) {
        output.append("VALUE = ");
        if (lowerBound_.size() > 1)
            output.append("(");
        List<String> literals = Lists.newArrayList();
        for (Expr literal : lowerBound_) literals.add(literal.toSql());
        output.append(Joiner.on(",").join(literals));
        if (lowerBound_.size() > 1)
            output.append(")");
    } else {
        if (!lowerBound_.isEmpty()) {
            Preconditions.checkState(lowerBound_.size() == 1);
            output.append(lowerBound_.get(0).toSql() + " " + (lowerBoundInclusive_ ? "<=" : "<"));
            output.append(" ");
        }
        output.append("VALUES");
        if (!upperBound_.isEmpty()) {
            Preconditions.checkState(upperBound_.size() == 1);
            output.append(" ");
            output.append((upperBoundInclusive_ ? "<=" : "<") + " " + upperBound_.get(0).toSql());
        }
    }
    return output.toString();
}
#end_block

#method_before
public TRangePartition toThrift() {
    TRangePartition tRangePartition = new TRangePartition();
    for (LiteralExpr literal : lowerBound_) {
        tRangePartition.addToLower_bound_values(literal.treeToThrift());
    }
    if (lowerBoundOperator_ != null) {
        tRangePartition.setLower_bound_op(lowerBoundOperator_.getThriftOp());
    }
    for (LiteralExpr literal : upperBound_) {
        tRangePartition.addToUpper_bound_values(literal.treeToThrift());
    }
    if (upperBoundOperator_ != null) {
        tRangePartition.setUpper_bound_op(upperBoundOperator_.getThriftOp());
    }
    Preconditions.checkState(tRangePartition.isSetLower_bound_values() || tRangePartition.isSetUpper_bound_values());
    return tRangePartition;
}
#method_after
public TRangePartition toThrift() {
    TRangePartition tRangePartition = new TRangePartition();
    for (Expr literal : lowerBound_) {
        tRangePartition.addToLower_bound_values(literal.treeToThrift());
    }
    if (!lowerBound_.isEmpty()) {
        tRangePartition.setIs_lower_bound_inclusive(lowerBoundInclusive_);
    }
    for (Expr literal : upperBound_) {
        tRangePartition.addToUpper_bound_values(literal.treeToThrift());
    }
    if (!upperBound_.isEmpty()) {
        tRangePartition.setIs_upper_bound_inclusive(upperBoundInclusive_);
    }
    Preconditions.checkState(tRangePartition.isSetLower_bound_values() || tRangePartition.isSetUpper_bound_values());
    return tRangePartition;
}
#end_block

#method_before
public List<LiteralExpr> getLowerBound() {
    return ImmutableList.copyOf(lowerBound_);
}
#method_after
public List<Expr> getLowerBound() {
    return ImmutableList.copyOf(lowerBound_);
}
#end_block

#method_before
public List<LiteralExpr> getUpperBound() {
    return ImmutableList.copyOf(upperBound_);
}
#method_after
public List<Expr> getUpperBound() {
    return ImmutableList.copyOf(upperBound_);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    MetaStoreUtil.checkShortPropertyMap("Property", tblProperties_);
    if (tblProperties_.containsKey(hive_metastoreConstants.META_TABLE_STORAGE)) {
        throw new AnalysisException(String.format("Setting the '%s' table property is " + "not supported.", hive_metastoreConstants.META_TABLE_STORAGE));
    }
    // avro.schema.url.
    if (tblProperties_.containsKey(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName()) || tblProperties_.containsKey(AvroSerdeUtils.AvroTableProperties.SCHEMA_URL.getPropName())) {
        analyzeAvroSchema(analyzer);
    }
    // Analyze 'skip.header.line.format' property.
    analyzeSkipHeaderLineCount(getTargetTable(), tblProperties_);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    super.analyze(analyzer);
    MetaStoreUtil.checkShortPropertyMap("Property", tblProperties_);
    if (tblProperties_.containsKey(hive_metastoreConstants.META_TABLE_STORAGE)) {
        throw new AnalysisException(String.format("Changing the '%s' table property is " + "not supported to protect against metadata corruption.", hive_metastoreConstants.META_TABLE_STORAGE));
    }
    // avro.schema.url.
    if (tblProperties_.containsKey(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName()) || tblProperties_.containsKey(AvroSerdeUtils.AvroTableProperties.SCHEMA_URL.getPropName())) {
        analyzeAvroSchema(analyzer);
    }
    // Analyze 'skip.header.line.format' property.
    analyzeSkipHeaderLineCount(getTargetTable(), tblProperties_);
}
#end_block

#method_before
@Test
public void TestAlterTableSet() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes set serdeproperties('a'='2')");
    AnalyzesOk("alter table functional.alltypes PARTITION (Year=2010, month=11) " + "set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes PARTITION (month=11, year=2010) " + "set fileformat parquetfile");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition1') set fileformat parquet");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='PaRtiTion1') set location '/a/b/c'");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set serdeproperties ('a'='2')");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("alter table functional.alltypes " + "set serdeproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        AnalyzesOk("alter table functional.alltypes " + "set tblproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set tblproperties('storage_handler'='1')", "Setting the 'storage_handler' table property is not supported.");
    }
    // Arbitrary exprs as partition key values. Constant exprs are ok.
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set location '/a/b'");
    // Arbitrary exprs as partition key values. Non-constant exprs should fail.
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set fileformat sequencefile", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set location '/a/b'", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    // Partition spec does not exist
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set location '/a/b'", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set tblproperties('a'='1')", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010) " + "set tblproperties('a'='1')", "Items in partition spec must exactly match the partition columns " + "in the table definition: functional.alltypes (1 vs 2)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010, year=2010) " + "set location '/a/b'", "Duplicate partition key name: year");
    AnalysisError("alter table functional.alltypes PARTITION (month=11, year=2014) " + "set fileformat sequencefile", "Partition spec does not exist: (month=11, year=2014)");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set fileformat sequencefile", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set location '/a/b/c'", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set location '/a/b'", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set fileformat sequencefile", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set location '/a/b'", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set fileformat sequencefile", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes set fileformat sequencefile", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set fileformat rcfile", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table db_does_not_exist.alltypes set location '/a/b'", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set location '/a/b'", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table functional.no_tbl partition(i=1) set location '/a/b'", "Table does not exist: functional.no_tbl");
    AnalysisError("alter table no_db.alltypes partition(i=1) set fileformat textfile", "Database does not exist: no_db");
    // Valid location
    AnalyzesOk("alter table functional.alltypes set location " + "'hdfs://localhost:20500/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'s3n://bucket/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'file:///test-warehouse/a/b'");
    // Invalid location
    AnalysisError("alter table functional.alltypes set location 'test/warehouse'", "URI path must be absolute: test/warehouse");
    AnalysisError("alter table functional.alltypes set location 'blah:///warehouse/'", "No FileSystem for scheme: blah");
    AnalysisError("alter table functional.alltypes set location ''", "URI path cannot be empty.");
    AnalysisError("alter table functional.alltypes set location '      '", "URI path cannot be empty.");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view set fileformat sequencefile", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource set fileformat parquet", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE SET on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes set tblproperties('a'='b')", "ALTER TABLE SET not currently supported on HBase tables.");
}
#method_after
@Test
public void TestAlterTableSet() throws AnalysisException {
    AnalyzesOk("alter table functional.alltypes set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes set serdeproperties('a'='2')");
    AnalyzesOk("alter table functional.alltypes PARTITION (Year=2010, month=11) " + "set location '/a/b'");
    AnalyzesOk("alter table functional.alltypes PARTITION (month=11, year=2010) " + "set fileformat parquetfile");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition1') set fileformat parquet");
    AnalyzesOk("alter table functional.stringpartitionkey PARTITION " + "(string_col='PaRtiTion1') set location '/a/b/c'");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set tblproperties('a'='1')");
    AnalyzesOk("alter table functional.alltypes PARTITION (year=2010, month=11) " + "set serdeproperties ('a'='2')");
    {
        // Check that long_properties fail at the analysis layer
        String long_property_key = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH; ++i) {
            long_property_key += 'k';
        }
        String long_property_value = "";
        for (int i = 0; i < MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH; ++i) {
            long_property_value += 'v';
        }
        // At this point long_property_{key_value} are actually not quite long enough to
        // cause analysis to fail.
        AnalyzesOk("alter table functional.alltypes " + "set serdeproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        AnalyzesOk("alter table functional.alltypes " + "set tblproperties ('" + long_property_key + "'='" + long_property_value + "') ");
        long_property_key += 'X';
        long_property_value += 'X';
        // Now that long_property_{key,value} are one character longer, they are too long
        // for the analyzer.
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "tblproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('" + long_property_key + "'='value')", "Property key length must be <= " + MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_KEY_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set " + "serdeproperties ('key'='" + long_property_value + "')", "Property value length must be <= " + MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + ": " + (MetaStoreUtil.MAX_PROPERTY_VALUE_LENGTH + 1));
        AnalysisError("alter table functional.alltypes set tblproperties('storage_handler'='1')", "Changing the 'storage_handler' table property is not supported to protect " + "against metadata corruption.");
    }
    // Arbitrary exprs as partition key values. Constant exprs are ok.
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set fileformat sequencefile");
    AnalyzesOk("alter table functional.alltypes PARTITION " + "(year=cast(100*20+10 as INT), month=cast(2+9 as INT)) " + "set location '/a/b'");
    // Arbitrary exprs as partition key values. Non-constant exprs should fail.
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set fileformat sequencefile", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    AnalysisError("alter table functional.alltypes PARTITION " + "(Year=2050, month=int_col) set location '/a/b'", "Non-constant expressions are not supported as static partition-key " + "values in 'month=int_col'.");
    // Partition spec does not exist
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set location '/a/b'", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2014, month=11) " + "set tblproperties('a'='1')", "Partition spec does not exist: (year=2014, month=11)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010) " + "set tblproperties('a'='1')", "Items in partition spec must exactly match the partition columns " + "in the table definition: functional.alltypes (1 vs 2)");
    AnalysisError("alter table functional.alltypes PARTITION (year=2010, year=2010) " + "set location '/a/b'", "Duplicate partition key name: year");
    AnalysisError("alter table functional.alltypes PARTITION (month=11, year=2014) " + "set fileformat sequencefile", "Partition spec does not exist: (month=11, year=2014)");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set fileformat sequencefile", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.alltypesnopart PARTITION (month=1) " + "set location '/a/b/c'", "Table is not partitioned: functional.alltypesnopart");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set location '/a/b'", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.stringpartitionkey PARTITION " + "(string_col='partition2') set fileformat sequencefile", "Partition spec does not exist: (string_col='partition2')");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set location '/a/b'", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    AnalysisError("alter table functional.alltypes PARTITION " + "(year=cast(10*20+10 as INT), month=cast(5*3 as INT)) " + "set fileformat sequencefile", "Partition spec does not exist: " + "(year=CAST(10 * 20 + 10 AS INT), month=CAST(5 * 3 AS INT))");
    // Table/Db does not exist
    AnalysisError("alter table db_does_not_exist.alltypes set fileformat sequencefile", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set fileformat rcfile", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table db_does_not_exist.alltypes set location '/a/b'", "Database does not exist: db_does_not_exist");
    AnalysisError("alter table functional.table_does_not_exist set location '/a/b'", "Table does not exist: functional.table_does_not_exist");
    AnalysisError("alter table functional.no_tbl partition(i=1) set location '/a/b'", "Table does not exist: functional.no_tbl");
    AnalysisError("alter table no_db.alltypes partition(i=1) set fileformat textfile", "Database does not exist: no_db");
    // Valid location
    AnalyzesOk("alter table functional.alltypes set location " + "'hdfs://localhost:20500/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'s3n://bucket/test-warehouse/a/b'");
    AnalyzesOk("alter table functional.alltypes set location " + "'file:///test-warehouse/a/b'");
    // Invalid location
    AnalysisError("alter table functional.alltypes set location 'test/warehouse'", "URI path must be absolute: test/warehouse");
    AnalysisError("alter table functional.alltypes set location 'blah:///warehouse/'", "No FileSystem for scheme: blah");
    AnalysisError("alter table functional.alltypes set location ''", "URI path cannot be empty.");
    AnalysisError("alter table functional.alltypes set location '      '", "URI path cannot be empty.");
    // Cannot ALTER TABLE a view.
    AnalysisError("alter table functional.alltypes_view set fileformat sequencefile", "ALTER TABLE not allowed on a view: functional.alltypes_view");
    // Cannot ALTER TABLE produced by a data source.
    AnalysisError("alter table functional.alltypes_datasource set fileformat parquet", "ALTER TABLE not allowed on a table produced by a data source: " + "functional.alltypes_datasource");
    // Cannot ALTER TABLE SET on an HBase table.
    AnalysisError("alter table functional_hbase.alltypes set tblproperties('a'='b')", "ALTER TABLE SET not currently supported on HBase tables.");
}
#end_block

#method_before
@Test
public void TestCreateTableAsSelect() throws AnalysisException {
    // Constant select.
    AnalyzesOk("create table newtbl as select 1+2, 'abc'");
    // Select from partitioned and unpartitioned tables using different
    // queries.
    AnalyzesOk("create table newtbl stored as textfile " + "as select * from functional.jointbl");
    AnalyzesOk("create table newtbl stored as parquetfile " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl stored as parquet " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl as select int_col from functional.alltypes");
    AnalyzesOk("create table functional.newtbl " + "as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.tbl as select a.* from functional.alltypes a " + "join functional.alltypes b on (a.int_col=b.int_col) limit 1000");
    // Caching operations
    AnalyzesOk("create table functional.newtbl cached in 'testPool'" + " as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.newtbl uncached" + " as select count(*) as CNT from functional.alltypes");
    // Table already exists with and without IF NOT EXISTS
    AnalysisError("create table functional.alltypes as select 1", "Table already exists: functional.alltypes");
    AnalyzesOk("create table if not exists functional.alltypes as select 1");
    // Database does not exist
    AnalysisError("create table db_does_not_exist.new_table as select 1", "Database does not exist: db_does_not_exist");
    // Analysis errors in the SELECT statement
    AnalysisError("create table newtbl as select * from tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    AnalysisError("create table newtbl as select 1 as c1, 2 as c1", "Duplicate column name: c1");
    // Unsupported file formats
    AnalysisError("create table foo stored as sequencefile as select 1", "CREATE TABLE AS SELECT does not support the (SEQUENCEFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE, KUDU)");
    AnalysisError("create table foo stored as RCFILE as select 1", "CREATE TABLE AS SELECT does not support the (RCFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE, KUDU)");
    // CTAS with a WITH clause and inline view (IMPALA-1100)
    AnalyzesOk("create table test_with as with with_1 as (select 1 as int_col from " + "functional.alltypes as t1 right join (select 1 as int_col from " + "functional.alltypestiny as t1) as t2 on t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
    // CTAS with a correlated inline view.
    AnalyzesOk("create table test as select id, item " + "from functional.allcomplextypes b, (select item from b.int_array_col) v1");
    // Correlated inline view in WITH clause.
    AnalyzesOk("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col) v1) select * from w");
    // CTAS with illegal correlated inline views.
    AnalysisError("create table test as select id, item " + "from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    AnalysisError("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1) select * from w", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    // CTAS into partitioned table.
    AnalyzesOk("create table p partitioned by (int_col) as " + "select double_col, int_col from functional.alltypes");
    AnalyzesOk("create table p partitioned by (int_col) as " + "select sum(double_col), int_col from functional.alltypes group by int_col");
    // At least one non-partition column must be specified.
    AnalysisError("create table p partitioned by (int_col, tinyint_col) as " + "select int_col, tinyint_col from functional.alltypes", "Number of partition columns (2) must be smaller than the number of columns in " + "the select statement (2).");
    // Order of the columns is important and not automatically corrected.
    AnalysisError("create table p partitioned by (int_col) as " + "select double_col, int_col, tinyint_col from functional.alltypes", "Partition column name mismatch: int_col != tinyint_col");
    AnalysisError("create table p partitioned by (tinyint_col, int_col) as " + "select double_col, int_col, tinyint_col from functional.alltypes", "Partition column name mismatch: tinyint_col != int_col");
    // CTAS into managed Kudu tables
    AnalyzesOk("create table t primary key (id) distribute by hash (id) into 3 buckets" + " stored as kudu as select id, bool_col, tinyint_col, smallint_col, int_col, " + "bigint_col, float_col, double_col, date_string_col, string_col " + "from functional.alltypestiny");
    // CTAS in an external Kudu table
    AnalysisError("create external table t stored as kudu " + "tblproperties('kudu.table_name'='t') as select id, int_col from " + "functional.alltypestiny", "CREATE TABLE AS SELECT is not supported for " + "external Kudu tables.");
    // CTAS into Kudu tables with unsupported types
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, timestamp_col from functional.alltypestiny", "Cannot create table 't': Type TIMESTAMP is not supported in Kudu");
    AnalysisError("create table t primary key (cs) distribute by hash into 3 buckets" + " stored as kudu as select cs from functional.chars_tiny", "Cannot create table 't': Type CHAR(5) is not supported in Kudu");
    AnalysisError("create table t primary key (vc) distribute by hash into 3 buckets" + " stored as kudu as select vc from functional.chars_tiny", "Cannot create table 't': Type VARCHAR(32) is not supported in Kudu");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, s from functional.complextypes_fileformat", "Expr 's' in select list returns a complex type 'STRUCT<f1:STRING,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, m from functional.complextypes_fileformat", "Expr 'm' in select list returns a complex type 'MAP<STRING,BIGINT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, a from functional.complextypes_fileformat", "Expr 'a' in select list returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
}
#method_after
@Test
public void TestCreateTableAsSelect() throws AnalysisException {
    // Constant select.
    AnalyzesOk("create table newtbl as select 1+2, 'abc'");
    // Select from partitioned and unpartitioned tables using different
    // queries.
    AnalyzesOk("create table newtbl stored as textfile " + "as select * from functional.jointbl");
    AnalyzesOk("create table newtbl stored as parquetfile " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl stored as parquet " + "as select * from functional.alltypes");
    AnalyzesOk("create table newtbl as select int_col from functional.alltypes");
    AnalyzesOk("create table functional.newtbl " + "as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.tbl as select a.* from functional.alltypes a " + "join functional.alltypes b on (a.int_col=b.int_col) limit 1000");
    // Caching operations
    AnalyzesOk("create table functional.newtbl cached in 'testPool'" + " as select count(*) as CNT from functional.alltypes");
    AnalyzesOk("create table functional.newtbl uncached" + " as select count(*) as CNT from functional.alltypes");
    // Table already exists with and without IF NOT EXISTS
    AnalysisError("create table functional.alltypes as select 1", "Table already exists: functional.alltypes");
    AnalyzesOk("create table if not exists functional.alltypes as select 1");
    // Database does not exist
    AnalysisError("create table db_does_not_exist.new_table as select 1", "Database does not exist: db_does_not_exist");
    // Analysis errors in the SELECT statement
    AnalysisError("create table newtbl as select * from tbl_does_not_exist", "Could not resolve table reference: 'tbl_does_not_exist'");
    AnalysisError("create table newtbl as select 1 as c1, 2 as c1", "Duplicate column name: c1");
    // Unsupported file formats
    AnalysisError("create table foo stored as sequencefile as select 1", "CREATE TABLE AS SELECT does not support the (SEQUENCEFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE, KUDU)");
    AnalysisError("create table foo stored as RCFILE as select 1", "CREATE TABLE AS SELECT does not support the (RCFILE) file format. " + "Supported formats are: (PARQUET, TEXTFILE, KUDU)");
    // CTAS with a WITH clause and inline view (IMPALA-1100)
    AnalyzesOk("create table test_with as with with_1 as (select 1 as int_col from " + "functional.alltypes as t1 right join (select 1 as int_col from " + "functional.alltypestiny as t1) as t2 on t2.int_col = t1.int_col) " + "select * from with_1 limit 10");
    // CTAS with a correlated inline view.
    AnalyzesOk("create table test as select id, item " + "from functional.allcomplextypes b, (select item from b.int_array_col) v1");
    // Correlated inline view in WITH clause.
    AnalyzesOk("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col) v1) select * from w");
    // CTAS with illegal correlated inline views.
    AnalysisError("create table test as select id, item " + "from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    AnalysisError("create table test as " + "with w as (select id, item from functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypes) v1) select * from w", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypes':\n" + "SELECT item FROM b.int_array_col, functional.alltypes");
    // CTAS into partitioned table.
    AnalyzesOk("create table p partitioned by (int_col) as " + "select double_col, int_col from functional.alltypes");
    AnalyzesOk("create table p partitioned by (int_col) as " + "select sum(double_col), int_col from functional.alltypes group by int_col");
    // At least one non-partition column must be specified.
    AnalysisError("create table p partitioned by (int_col, tinyint_col) as " + "select int_col, tinyint_col from functional.alltypes", "Number of partition columns (2) must be smaller than the number of columns in " + "the select statement (2).");
    // Order of the columns is important and not automatically corrected.
    AnalysisError("create table p partitioned by (int_col) as " + "select double_col, int_col, tinyint_col from functional.alltypes", "Partition column name mismatch: int_col != tinyint_col");
    AnalysisError("create table p partitioned by (tinyint_col, int_col) as " + "select double_col, int_col, tinyint_col from functional.alltypes", "Partition column name mismatch: tinyint_col != int_col");
    // CTAS into managed Kudu tables
    AnalyzesOk("create table t primary key (id) distribute by hash (id) into 3 buckets" + " stored as kudu as select id, bool_col, tinyint_col, smallint_col, int_col, " + "bigint_col, float_col, double_col, date_string_col, string_col " + "from functional.alltypestiny");
    // CTAS in an external Kudu table
    AnalysisError("create external table t stored as kudu " + "tblproperties('kudu.table_name'='t') as select id, int_col from " + "functional.alltypestiny", "CREATE TABLE AS SELECT is not supported for " + "external Kudu tables.");
    // CTAS into Kudu tables with unsupported types
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, timestamp_col from functional.alltypestiny", "Cannot create table 't': Type TIMESTAMP is not supported in Kudu");
    AnalysisError("create table t primary key (cs) distribute by hash into 3 buckets" + " stored as kudu as select cs from functional.chars_tiny", "Cannot create table 't': Type CHAR(5) is not supported in Kudu");
    AnalysisError("create table t primary key (vc) distribute by hash into 3 buckets" + " stored as kudu as select vc from functional.chars_tiny", "Cannot create table 't': Type VARCHAR(32) is not supported in Kudu");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select c1 as id from functional.decimal_tiny", "Cannot create table 't': Type DECIMAL(10,4) is not supported in Kudu");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, s from functional.complextypes_fileformat", "Expr 's' in select list returns a complex type 'STRUCT<f1:STRING,f2:INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, m from functional.complextypes_fileformat", "Expr 'm' in select list returns a complex type 'MAP<STRING,BIGINT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("create table t primary key (id) distribute by hash into 3 buckets" + " stored as kudu as select id, a from functional.complextypes_fileformat", "Expr 'a' in select list returns a complex type 'ARRAY<INT>'.\n" + "Only scalar types are allowed in the select list.");
}
#end_block

#method_before
public static List<Function> extractFunctions(String db, org.apache.hadoop.hive.metastore.api.Function function) throws ImpalaRuntimeException {
    List<Function> result = Lists.newArrayList();
    List<String> addedSignatures = Lists.newArrayList();
    StringBuilder warnMessage = new StringBuilder();
    if (!isFunctionCompatible(function, warnMessage)) {
        LOG.warn("Skipping load of incompatible function: " + function.getFunctionName() + ". " + warnMessage.toString());
        return result;
    }
    String jarUri = function.getResourceUris().get(0).getUri();
    Class<?> udfClass = null;
    Path localJarPath = null;
    try {
        localJarPath = new Path(LOCAL_LIBRARY_PATH, UUID.randomUUID().toString() + ".jar");
        try {
            FileSystemUtil.copyToLocal(new Path(jarUri), localJarPath);
        } catch (IOException e) {
            String errorMsg = "Error loading Java function: " + db + "." + function.getFunctionName() + ". Couldn't copy " + jarUri + " to local path: " + localJarPath.toString();
            LOG.error(errorMsg, e);
            throw new ImpalaRuntimeException(errorMsg);
        }
        URL[] classLoaderUrls = new URL[] { new URL(localJarPath.toString()) };
        URLClassLoader urlClassLoader = new URLClassLoader(classLoaderUrls);
        udfClass = urlClassLoader.loadClass(function.getClassName());
        // TODO: Remove this once we support Java UDAF/UDTF
        if (FunctionUtils.getUDFClassType(udfClass) != FunctionUtils.UDFClassType.UDF) {
            LOG.warn("Ignoring load of incompatible Java function: " + function.getFunctionName() + " as " + FunctionUtils.getUDFClassType(udfClass) + " is not a supported type. Only UDFs are supported");
            return result;
        }
        // object.
        for (Method m : udfClass.getMethods()) {
            if (!m.getName().equals(UdfExecutor.UDF_FUNCTION_NAME))
                continue;
            Function fn = ScalarFunction.fromHiveFunction(db, function.getFunctionName(), function.getClassName(), m.getParameterTypes(), m.getReturnType(), jarUri);
            if (fn == null) {
                LOG.warn("Ignoring incompatible method: " + m.toString() + " during load of " + "Hive UDF:" + function.getFunctionName() + " from " + udfClass);
                continue;
            }
            if (!addedSignatures.contains(fn.signatureString())) {
                result.add(fn);
                addedSignatures.add(fn.signatureString());
            }
        }
    } catch (ClassNotFoundException c) {
        String errorMsg = "Error loading Java function: " + db + "." + function.getFunctionName() + ". Symbol class " + udfClass + "not found in Jar: " + jarUri;
        LOG.error(errorMsg);
        throw new ImpalaRuntimeException(errorMsg, c);
    } catch (Exception e) {
        LOG.error("Skipping function load: " + function.getFunctionName(), e);
        throw new ImpalaRuntimeException("Error extracting functions", e);
    } catch (LinkageError e) {
        String errorMsg = "Error resolving dependencies for Java function: " + db + "." + function.getFunctionName();
        LOG.error(errorMsg);
        throw new ImpalaRuntimeException(errorMsg, e);
    } finally {
        if (localJarPath != null)
            FileSystemUtil.deleteIfExists(localJarPath);
    }
    return result;
}
#method_after
public static List<Function> extractFunctions(String db, org.apache.hadoop.hive.metastore.api.Function function) throws ImpalaRuntimeException {
    List<Function> result = Lists.newArrayList();
    List<String> addedSignatures = Lists.newArrayList();
    StringBuilder warnMessage = new StringBuilder();
    if (!isFunctionCompatible(function, warnMessage)) {
        LOG.warn("Skipping load of incompatible function: " + function.getFunctionName() + ". " + warnMessage.toString());
        return result;
    }
    String jarUri = function.getResourceUris().get(0).getUri();
    Class<?> udfClass = null;
    Path localJarPath = null;
    try {
        localJarPath = new Path(localLibraryPath_, UUID.randomUUID().toString() + ".jar");
        try {
            FileSystemUtil.copyToLocal(new Path(jarUri), localJarPath);
        } catch (IOException e) {
            String errorMsg = "Error loading Java function: " + db + "." + function.getFunctionName() + ". Couldn't copy " + jarUri + " to local path: " + localJarPath.toString();
            LOG.error(errorMsg, e);
            throw new ImpalaRuntimeException(errorMsg);
        }
        URL[] classLoaderUrls = new URL[] { new URL(localJarPath.toString()) };
        URLClassLoader urlClassLoader = new URLClassLoader(classLoaderUrls);
        udfClass = urlClassLoader.loadClass(function.getClassName());
        // TODO: Remove this once we support Java UDAF/UDTF
        if (FunctionUtils.getUDFClassType(udfClass) != FunctionUtils.UDFClassType.UDF) {
            LOG.warn("Ignoring load of incompatible Java function: " + function.getFunctionName() + " as " + FunctionUtils.getUDFClassType(udfClass) + " is not a supported type. Only UDFs are supported");
            return result;
        }
        // object.
        for (Method m : udfClass.getMethods()) {
            if (!m.getName().equals(UdfExecutor.UDF_FUNCTION_NAME))
                continue;
            Function fn = ScalarFunction.fromHiveFunction(db, function.getFunctionName(), function.getClassName(), m.getParameterTypes(), m.getReturnType(), jarUri);
            if (fn == null) {
                LOG.warn("Ignoring incompatible method: " + m.toString() + " during load of " + "Hive UDF:" + function.getFunctionName() + " from " + udfClass);
                continue;
            }
            if (!addedSignatures.contains(fn.signatureString())) {
                result.add(fn);
                addedSignatures.add(fn.signatureString());
            }
        }
    } catch (ClassNotFoundException c) {
        String errorMsg = "Error loading Java function: " + db + "." + function.getFunctionName() + ". Symbol class " + udfClass + "not found in Jar: " + jarUri;
        LOG.error(errorMsg);
        throw new ImpalaRuntimeException(errorMsg, c);
    } catch (Exception e) {
        LOG.error("Skipping function load: " + function.getFunctionName(), e);
        throw new ImpalaRuntimeException("Error extracting functions", e);
    } catch (LinkageError e) {
        String errorMsg = "Error resolving dependencies for Java function: " + db + "." + function.getFunctionName();
        LOG.error(errorMsg);
        throw new ImpalaRuntimeException(errorMsg, e);
    } finally {
        if (localJarPath != null)
            FileSystemUtil.deleteIfExists(localJarPath);
    }
    return result;
}
#end_block

#method_before
private Pair<Db, List<TTableName>> invalidateDb(MetaStoreClient msClient, String dbName, Db existingDb) {
    try {
        List<org.apache.hadoop.hive.metastore.api.Function> javaFns = Lists.newArrayList();
        for (String javaFn : msClient.getHiveClient().getFunctions(dbName, "*")) {
            javaFns.add(msClient.getHiveClient().getFunction(dbName, javaFn));
        }
        org.apache.hadoop.hive.metastore.api.Database msDb = msClient.getHiveClient().getDatabase(dbName);
        Db newDb = new Db(dbName, this, msDb);
        // In that case we needn't restore any transient functions.
        if (existingDb != null) {
            // Catalog restart.
            for (Function fn : existingDb.getTransientFunctions()) {
                newDb.addFunction(fn);
                fn.setCatalogVersion(incrementAndGetCatalogVersion());
            }
        }
        // Reload native UDFs.
        loadFunctionsFromDbParams(newDb, msDb);
        // Reload Java UDFs from HMS.
        loadJavaFunctions(newDb, javaFns);
        newDb.setCatalogVersion(incrementAndGetCatalogVersion());
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        for (String tableName : msClient.getHiveClient().getAllTables(dbName)) {
            Table incompleteTbl = IncompleteTable.createUninitializedTable(getNextTableId(), newDb, tableName);
            incompleteTbl.setCatalogVersion(incrementAndGetCatalogVersion());
            newDb.addTable(incompleteTbl);
            if (loadInBackground_) {
                tblsToBackgroundLoad.add(new TTableName(dbName, tableName.toLowerCase()));
            }
        }
        return Pair.create(newDb, tblsToBackgroundLoad);
    } catch (Exception e) {
        LOG.warn("Encountered an exception while invalidating database: " + dbName + ". Ignoring further load of this db.", e);
    }
    return null;
}
#method_after
private Pair<Db, List<TTableName>> invalidateDb(MetaStoreClient msClient, String dbName, Db existingDb) {
    try {
        List<org.apache.hadoop.hive.metastore.api.Function> javaFns = Lists.newArrayList();
        for (String javaFn : msClient.getHiveClient().getFunctions(dbName, "*")) {
            javaFns.add(msClient.getHiveClient().getFunction(dbName, javaFn));
        }
        org.apache.hadoop.hive.metastore.api.Database msDb = msClient.getHiveClient().getDatabase(dbName);
        Db newDb = new Db(dbName, this, msDb);
        // In that case we needn't restore any transient functions.
        if (existingDb != null) {
            // Catalog restart.
            for (Function fn : existingDb.getTransientFunctions()) {
                newDb.addFunction(fn);
                fn.setCatalogVersion(incrementAndGetCatalogVersion());
            }
        }
        // Reload native UDFs.
        loadFunctionsFromDbParams(newDb, msDb);
        // Reload Java UDFs from HMS.
        loadJavaFunctions(newDb, javaFns);
        newDb.setCatalogVersion(incrementAndGetCatalogVersion());
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        for (String tableName : msClient.getHiveClient().getAllTables(dbName)) {
            Table incompleteTbl = IncompleteTable.createUninitializedTable(newDb, tableName);
            incompleteTbl.setCatalogVersion(incrementAndGetCatalogVersion());
            newDb.addTable(incompleteTbl);
            if (loadInBackground_) {
                tblsToBackgroundLoad.add(new TTableName(dbName, tableName.toLowerCase()));
            }
        }
        return Pair.create(newDb, tblsToBackgroundLoad);
    } catch (Exception e) {
        LOG.warn("Encountered an exception while invalidating database: " + dbName + ". Ignoring further load of this db.", e);
    }
    return null;
}
#end_block

#method_before
public void reset() throws CatalogException {
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        nextTableId_.set(0);
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                dbName = dbName.toLowerCase();
                Db oldDb = oldDbCache.get(dbName);
                Pair<Db, List<TTableName>> invalidatedDb = invalidateDb(msClient, dbName, oldDb);
                if (invalidatedDb == null)
                    continue;
                newDbCache.put(dbName, invalidatedDb.first);
                tblsToBackgroundLoad.addAll(invalidatedDb.second);
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#method_after
public void reset() throws CatalogException {
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                dbName = dbName.toLowerCase();
                Db oldDb = oldDbCache.get(dbName);
                Pair<Db, List<TTableName>> invalidatedDb = invalidateDb(msClient, dbName, oldDb);
                if (invalidatedDb == null)
                    continue;
                newDbCache.put(dbName, invalidatedDb.first);
                tblsToBackgroundLoad.addAll(invalidatedDb.second);
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#end_block

#method_before
public Table addTable(String dbName, String tblName) throws TableNotFoundException {
    Db db = getDb(dbName);
    if (db == null)
        return null;
    Table incompleteTable = IncompleteTable.createUninitializedTable(getNextTableId(), db, tblName);
    incompleteTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(incompleteTable);
    return db.getTable(tblName);
}
#method_after
public Table addTable(String dbName, String tblName) {
    Db db = getDb(dbName);
    if (db == null)
        return null;
    Table incompleteTable = IncompleteTable.createUninitializedTable(db, tblName);
    incompleteTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(incompleteTable);
    return db.getTable(tblName);
}
#end_block

#method_before
public boolean invalidateTable(TTableName tableName, Pair<Db, Table> updatedObjects) {
    Preconditions.checkNotNull(updatedObjects);
    updatedObjects.first = null;
    updatedObjects.second = null;
    LOG.debug(String.format("Invalidating table metadata: %s.%s", tableName.getDb_name(), tableName.getTable_name()));
    String dbName = tableName.getDb_name();
    String tblName = tableName.getTable_name();
    // Stores whether the table exists in the metastore. Can have three states:
    // 1) true - Table exists in metastore.
    // 2) false - Table does not exist in metastore.
    // 3) unknown (null) - There was exception thrown by the metastore client.
    Boolean tableExistsInMetaStore;
    Db db = null;
    try (MetaStoreClient msClient = getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Database msDb = null;
        try {
            tableExistsInMetaStore = msClient.getHiveClient().tableExists(dbName, tblName);
        } catch (UnknownDBException e) {
            // The parent database does not exist in the metastore. Treat this the same
            // as if the table does not exist.
            tableExistsInMetaStore = false;
        } catch (TException e) {
            LOG.error("Error executing tableExists() metastore call: " + tblName, e);
            tableExistsInMetaStore = null;
        }
        if (tableExistsInMetaStore != null && !tableExistsInMetaStore) {
            updatedObjects.second = removeTable(dbName, tblName);
            return true;
        }
        db = getDb(dbName);
        if ((db == null || !db.containsTable(tblName)) && tableExistsInMetaStore == null) {
            // table exists in the metastore. Do nothing.
            return false;
        } else if (db == null && tableExistsInMetaStore) {
            // must be valid since tableExistsInMetaStore is true.
            try {
                msDb = msClient.getHiveClient().getDatabase(dbName);
                Preconditions.checkNotNull(msDb);
                db = new Db(dbName, this, msDb);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                addDb(db);
                updatedObjects.first = db;
            } catch (TException e) {
                // The metastore database cannot be get. Log the error and return.
                LOG.error("Error executing getDatabase() metastore call: " + dbName, e);
                return false;
            }
        }
    }
    // Add a new uninitialized table to the table cache, effectively invalidating
    // any existing entry. The metadata for the table will be loaded lazily, on the
    // on the next access to the table.
    Table newTable = IncompleteTable.createUninitializedTable(getNextTableId(), db, tblName);
    newTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(newTable);
    if (loadInBackground_) {
        tableLoadingMgr_.backgroundLoad(new TTableName(dbName.toLowerCase(), tblName.toLowerCase()));
    }
    updatedObjects.second = newTable;
    return false;
}
#method_after
public boolean invalidateTable(TTableName tableName, Pair<Db, Table> updatedObjects) {
    Preconditions.checkNotNull(updatedObjects);
    updatedObjects.first = null;
    updatedObjects.second = null;
    LOG.debug(String.format("Invalidating table metadata: %s.%s", tableName.getDb_name(), tableName.getTable_name()));
    String dbName = tableName.getDb_name();
    String tblName = tableName.getTable_name();
    // Stores whether the table exists in the metastore. Can have three states:
    // 1) true - Table exists in metastore.
    // 2) false - Table does not exist in metastore.
    // 3) unknown (null) - There was exception thrown by the metastore client.
    Boolean tableExistsInMetaStore;
    Db db = null;
    try (MetaStoreClient msClient = getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Database msDb = null;
        try {
            tableExistsInMetaStore = msClient.getHiveClient().tableExists(dbName, tblName);
        } catch (UnknownDBException e) {
            // The parent database does not exist in the metastore. Treat this the same
            // as if the table does not exist.
            tableExistsInMetaStore = false;
        } catch (TException e) {
            LOG.error("Error executing tableExists() metastore call: " + tblName, e);
            tableExistsInMetaStore = null;
        }
        if (tableExistsInMetaStore != null && !tableExistsInMetaStore) {
            updatedObjects.second = removeTable(dbName, tblName);
            return true;
        }
        db = getDb(dbName);
        if ((db == null || !db.containsTable(tblName)) && tableExistsInMetaStore == null) {
            // table exists in the metastore. Do nothing.
            return false;
        } else if (db == null && tableExistsInMetaStore) {
            // must be valid since tableExistsInMetaStore is true.
            try {
                msDb = msClient.getHiveClient().getDatabase(dbName);
                Preconditions.checkNotNull(msDb);
                db = new Db(dbName, this, msDb);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                addDb(db);
                updatedObjects.first = db;
            } catch (TException e) {
                // The metastore database cannot be get. Log the error and return.
                LOG.error("Error executing getDatabase() metastore call: " + dbName, e);
                return false;
            }
        }
    }
    // Add a new uninitialized table to the table cache, effectively invalidating
    // any existing entry. The metadata for the table will be loaded lazily, on the
    // on the next access to the table.
    Table newTable = IncompleteTable.createUninitializedTable(db, tblName);
    newTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(newTable);
    if (loadInBackground_) {
        tableLoadingMgr_.backgroundLoad(new TTableName(dbName.toLowerCase(), tblName.toLowerCase()));
    }
    updatedObjects.second = newTable;
    return false;
}
#end_block

#method_before
public static boolean hasGetFileBlockLocations(FileSystem fs) {
    // Common case.
    if (isDistributedFileSystem(fs))
        return true;
    // Blacklist FileSystems that are known to not implement getFileBlockLocations().
    return !(fs instanceof S3AFileSystem || fs instanceof NativeS3FileSystem || fs instanceof S3FileSystem || fs instanceof LocalFileSystem);
}
#method_after
public static boolean hasGetFileBlockLocations(FileSystem fs) {
    // Common case.
    if (isDistributedFileSystem(fs))
        return true;
    // Blacklist FileSystems that are known to not implement getFileBlockLocations().
    return !(fs instanceof S3AFileSystem || fs instanceof LocalFileSystem);
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    try {
        super.analyze(analyzer);
    } catch (AnalysisException e) {
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    Preconditions.checkState(operands_.size() > 0);
    // Propagates DISTINCT from right to left.
    propagateDistinct();
    // Analyze all operands and make sure they return an equal number of exprs.
    for (int i = 0; i < operands_.size(); ++i) {
        try {
            operands_.get(i).analyze(analyzer);
            QueryStmt firstQuery = operands_.get(0).getQueryStmt();
            List<Expr> firstExprs = operands_.get(0).getQueryStmt().getResultExprs();
            QueryStmt query = operands_.get(i).getQueryStmt();
            List<Expr> exprs = query.getResultExprs();
            if (firstExprs.size() != exprs.size()) {
                throw new AnalysisException("Operands have unequal number of columns:\n" + "'" + queryStmtToSql(firstQuery) + "' has " + firstExprs.size() + " column(s)\n" + "'" + queryStmtToSql(query) + "' has " + exprs.size() + " column(s)");
            }
        } catch (AnalysisException e) {
            if (analyzer.getMissingTbls().isEmpty())
                throw e;
        }
    }
    if (!analyzer.getMissingTbls().isEmpty()) {
        throw new AnalysisException("Found missing tables. Aborting analysis.");
    }
    // Remember SQL string before unnesting operands.
    toSqlString_ = toSql();
    // Unnest the operands before casting the result exprs. Unnesting may add
    // additional entries to operands_ and the result exprs of those unnested
    // operands must also be cast properly.
    unnestOperands(analyzer);
    // Compute hasAnalyticExprs_
    hasAnalyticExprs_ = false;
    for (UnionOperand op : operands_) {
        if (op.hasAnalyticExprs()) {
            hasAnalyticExprs_ = true;
            break;
        }
    }
    // Cast all result exprs to a compatible type.
    List<List<Expr>> resultExprLists = Lists.newArrayList();
    for (UnionOperand op : operands_) {
        resultExprLists.add(op.getQueryStmt().getResultExprs());
    }
    analyzer.castToUnionCompatibleTypes(resultExprLists);
    // Create tuple descriptor materialized by this UnionStmt, its resultExprs, and
    // its sortInfo if necessary.
    createMetadata(analyzer);
    createSortInfo(analyzer);
    // Create unnested operands' smaps.
    for (UnionOperand operand : operands_) setOperandSmap(operand, analyzer);
    // Create distinctAggInfo, if necessary.
    if (!distinctOperands_.isEmpty()) {
        // Aggregate produces exactly the same tuple as the original union stmt.
        ArrayList<Expr> groupingExprs = Expr.cloneList(resultExprs_);
        try {
            distinctAggInfo_ = AggregateInfo.create(groupingExprs, null, analyzer.getDescTbl().getTupleDesc(tupleId_), analyzer);
        } catch (AnalysisException e) {
            // Should never happen
            throw new AnalysisException("Error creating agg info in UnionStmt.analyze()");
        }
    }
    if (evaluateOrderBy_)
        createSortTupleInfo(analyzer);
    baseTblResultExprs_ = resultExprs_;
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    try {
        super.analyze(analyzer);
    } catch (AnalysisException e) {
        if (analyzer.getMissingTbls().isEmpty())
            throw e;
    }
    Preconditions.checkState(operands_.size() > 0);
    // Propagates DISTINCT from right to left.
    propagateDistinct();
    // Analyze all operands and make sure they return an equal number of exprs.
    analyzeOperands(analyzer);
    // Remember the SQL string before unnesting operands.
    toSqlString_ = toSql();
    // Unnest the operands before casting the result exprs. Unnesting may add
    // additional entries to operands_ and the result exprs of those unnested
    // operands must also be cast properly.
    unnestOperands(analyzer);
    // Compute hasAnalyticExprs_
    hasAnalyticExprs_ = false;
    for (UnionOperand op : operands_) {
        if (op.hasAnalyticExprs()) {
            hasAnalyticExprs_ = true;
            break;
        }
    }
    // Collect all result expr lists and cast the exprs as necessary.
    List<List<Expr>> resultExprLists = Lists.newArrayList();
    for (UnionOperand op : operands_) {
        resultExprLists.add(op.getQueryStmt().getResultExprs());
    }
    analyzer.castToUnionCompatibleTypes(resultExprLists);
    // Create tuple descriptor materialized by this UnionStmt, its resultExprs, and
    // its sortInfo if necessary.
    createMetadata(analyzer);
    createSortInfo(analyzer);
    // Create unnested operands' smaps.
    for (UnionOperand operand : operands_) setOperandSmap(operand, analyzer);
    // Create distinctAggInfo, if necessary.
    if (!distinctOperands_.isEmpty()) {
        // Aggregate produces exactly the same tuple as the original union stmt.
        ArrayList<Expr> groupingExprs = Expr.cloneList(resultExprs_);
        try {
            distinctAggInfo_ = AggregateInfo.create(groupingExprs, null, analyzer.getDescTbl().getTupleDesc(tupleId_), analyzer);
        } catch (AnalysisException e) {
            // Should never happen.
            throw new IllegalStateException("Error creating agg info in UnionStmt.analyze()", e);
        }
    }
    if (evaluateOrderBy_)
        createSortTupleInfo(analyzer);
    baseTblResultExprs_ = resultExprs_;
}
#end_block

#method_before
private boolean hasNullableKuduScanSlots() {
    if (!(getTable() instanceof KuduTable))
        return false;
    for (SlotDescriptor d : slots_) {
        if (!d.isMaterialized())
            continue;
        if (d.isKuduScanSlot() && d.getIsNullable())
            return true;
    }
    return false;
}
#method_after
private boolean hasNullableKuduScanSlots() {
    if (!(getTable() instanceof KuduTable))
        return false;
    for (SlotDescriptor d : slots_) {
        if (d.isMaterialized() && d.getIsNullable())
            return true;
    }
    return false;
}
#end_block

#method_before
public static org.apache.kudu.Type fromImpalaType(Type t) throws ImpalaRuntimeException {
    if (!t.isScalarType()) {
        throw new ImpalaRuntimeException(format("Non-scalar type %s is not supported in Kudu", t.toSql()));
    }
    ScalarType s = (ScalarType) t;
    switch(s.getPrimitiveType()) {
        case TINYINT:
            return org.apache.kudu.Type.INT8;
        case SMALLINT:
            return org.apache.kudu.Type.INT16;
        case INT:
            return org.apache.kudu.Type.INT32;
        case BIGINT:
            return org.apache.kudu.Type.INT64;
        case BOOLEAN:
            return org.apache.kudu.Type.BOOL;
        case STRING:
            return org.apache.kudu.Type.STRING;
        case DOUBLE:
            return org.apache.kudu.Type.DOUBLE;
        case FLOAT:
            return org.apache.kudu.Type.FLOAT;
        /* Fall through below */
        case INVALID_TYPE:
        case NULL_TYPE:
        case TIMESTAMP:
        case BINARY:
        case DATE:
        case DATETIME:
        case DECIMAL:
        case CHAR:
        case VARCHAR:
        default:
            throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", s.toSql()));
    }
}
#method_after
public static org.apache.kudu.Type fromImpalaType(Type t) throws ImpalaRuntimeException {
    if (!t.isScalarType()) {
        throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", t.toSql()));
    }
    ScalarType s = (ScalarType) t;
    switch(s.getPrimitiveType()) {
        case TINYINT:
            return org.apache.kudu.Type.INT8;
        case SMALLINT:
            return org.apache.kudu.Type.INT16;
        case INT:
            return org.apache.kudu.Type.INT32;
        case BIGINT:
            return org.apache.kudu.Type.INT64;
        case BOOLEAN:
            return org.apache.kudu.Type.BOOL;
        case STRING:
            return org.apache.kudu.Type.STRING;
        case DOUBLE:
            return org.apache.kudu.Type.DOUBLE;
        case FLOAT:
            return org.apache.kudu.Type.FLOAT;
        /* Fall through below */
        case INVALID_TYPE:
        case NULL_TYPE:
        case TIMESTAMP:
        case BINARY:
        case DATE:
        case DATETIME:
        case DECIMAL:
        case CHAR:
        case VARCHAR:
        default:
            throw new ImpalaRuntimeException(format("Type %s is not supported in Kudu", s.toSql()));
    }
}
#end_block

#method_before
public static Type toImpalaType(org.apache.kudu.Type t) throws ImpalaRuntimeException {
    switch(t) {
        case BOOL:
            return Type.BOOLEAN;
        case DOUBLE:
            return Type.DOUBLE;
        case FLOAT:
            return Type.FLOAT;
        case INT8:
            return Type.TINYINT;
        case INT16:
            return Type.SMALLINT;
        case INT32:
            return Type.INT;
        case INT64:
            return Type.BIGINT;
        case STRING:
            return Type.STRING;
        default:
            throw new ImpalaRuntimeException(String.format("Kudu type %s is not supported in Impala", t));
    }
}
#method_after
public static Type toImpalaType(org.apache.kudu.Type t) throws ImpalaRuntimeException {
    switch(t) {
        case BOOL:
            return Type.BOOLEAN;
        case DOUBLE:
            return Type.DOUBLE;
        case FLOAT:
            return Type.FLOAT;
        case INT8:
            return Type.TINYINT;
        case INT16:
            return Type.SMALLINT;
        case INT32:
            return Type.INT;
        case INT64:
            return Type.BIGINT;
        case STRING:
            return Type.STRING;
        default:
            throw new ImpalaRuntimeException(String.format("Kudu type '%s' is not supported in Impala", t.getName()));
    }
}
#end_block

#method_before
static void createManagedTable(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params) throws ImpalaRuntimeException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String kuduTableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Creating table '%s' in master '%s'", kuduTableName, masterHosts));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        // (see KUDU-1710).
        if (kudu.tableExists(kuduTableName)) {
            if (params.if_not_exists)
                return;
            throw new ImpalaRuntimeException(String.format("Table '%s' already exists in Kudu.", kuduTableName));
        }
        Schema schema = createTableSchema(msTbl, params);
        CreateTableOptions tableOpts = buildTableOptions(msTbl, params, schema);
        kudu.createTable(kuduTableName, schema, tableOpts);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error creating Kudu table '%s'", kuduTableName), e);
    }
}
#method_after
static void createManagedTable(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params) throws ImpalaRuntimeException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String kuduTableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Creating table '%s' in master '%s'", kuduTableName, masterHosts));
    try (KuduClient kudu = new KuduClient.KuduClientBuilder(masterHosts).build()) {
        // (see KUDU-1710).
        if (kudu.tableExists(kuduTableName)) {
            if (params.if_not_exists)
                return;
            throw new ImpalaRuntimeException(String.format("Table '%s' already exists in Kudu.", kuduTableName));
        }
        Schema schema = createTableSchema(msTbl, params);
        CreateTableOptions tableOpts = buildTableOptions(msTbl, params, schema);
        kudu.createTable(kuduTableName, schema, tableOpts);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error creating table '%s'", kuduTableName), e);
    }
}
#end_block

#method_before
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the distribution schemes
    List<TDistributeParam> distributeParams = params.getDistribute_by();
    if (distributeParams != null) {
        boolean hasRangePartitioning = false;
        for (TDistributeParam distParam : distributeParams) {
            if (distParam.isSetBy_hash_param()) {
                Preconditions.checkState(!distParam.isSetBy_range_param());
                tableOpts.addHashPartitions(distParam.getBy_hash_param().getColumns(), distParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(distParam.isSetBy_range_param());
                hasRangePartitioning = true;
                tableOpts.setRangePartitionColumns(distParam.getBy_range_param().getColumns());
                for (PartialRow partialRow : KuduUtil.parseSplits(schema, distParam.getBy_range_param())) {
                    tableOpts.addSplitRow(partialRow);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        int parsedReplicas = -1;
        try {
            parsedReplicas = Integer.parseInt(replication);
            Preconditions.checkState(parsedReplicas > 0);
        } catch (Exception e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication));
        }
        tableOpts.setNumReplicas(parsedReplicas);
    }
    return tableOpts;
}
#method_after
private static CreateTableOptions buildTableOptions(org.apache.hadoop.hive.metastore.api.Table msTbl, TCreateTableParams params, Schema schema) throws ImpalaRuntimeException {
    CreateTableOptions tableOpts = new CreateTableOptions();
    // Set the distribution schemes
    List<TDistributeParam> distributeParams = params.getDistribute_by();
    if (distributeParams != null) {
        boolean hasRangePartitioning = false;
        for (TDistributeParam distParam : distributeParams) {
            if (distParam.isSetBy_hash_param()) {
                Preconditions.checkState(!distParam.isSetBy_range_param());
                tableOpts.addHashPartitions(distParam.getBy_hash_param().getColumns(), distParam.getBy_hash_param().getNum_buckets());
            } else {
                Preconditions.checkState(distParam.isSetBy_range_param());
                hasRangePartitioning = true;
                tableOpts.setRangePartitionColumns(distParam.getBy_range_param().getColumns());
                for (PartialRow partialRow : KuduUtil.parseSplits(schema, distParam.getBy_range_param())) {
                    tableOpts.addSplitRow(partialRow);
                }
            }
        }
        // an empty list.
        if (!hasRangePartitioning) {
            tableOpts.setRangePartitionColumns(Collections.<String>emptyList());
        }
    }
    // Set the number of table replicas, if specified.
    String replication = msTbl.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    if (!Strings.isNullOrEmpty(replication)) {
        try {
            int r = Integer.parseInt(replication);
            Preconditions.checkState(r > 0);
            tableOpts.setNumReplicas(r);
        } catch (NumberFormatException e) {
            throw new ImpalaRuntimeException(String.format("Invalid number of table " + "replicas specified: '%s'", replication), e);
        }
    }
    return tableOpts;
}
#end_block

#method_before
static void dropTable(org.apache.hadoop.hive.metastore.api.Table msTbl, boolean ifExists) throws ImpalaRuntimeException, TableNotFoundException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String tableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Dropping table '%s' from master '%s'", tableName, masterHosts));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        Preconditions.checkState(!Strings.isNullOrEmpty(tableName));
        // (see KUDU-1710).
        if (kudu.tableExists(tableName)) {
            kudu.deleteTable(tableName);
        } else if (!ifExists) {
            throw new TableNotFoundException(String.format("Table '%s' does not exist in Kudu master(s) '%s'.", tableName, masterHosts));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error dropping table '%s'", tableName), e);
    }
}
#method_after
static void dropTable(org.apache.hadoop.hive.metastore.api.Table msTbl, boolean ifExists) throws ImpalaRuntimeException, TableNotFoundException {
    Preconditions.checkState(!Table.isExternalTable(msTbl));
    String tableName = msTbl.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String masterHosts = msTbl.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Dropping table '%s' from master '%s'", tableName, masterHosts));
    try (KuduClient kudu = new KuduClient.KuduClientBuilder(masterHosts).build()) {
        Preconditions.checkState(!Strings.isNullOrEmpty(tableName));
        // (see KUDU-1710).
        if (kudu.tableExists(tableName)) {
            kudu.deleteTable(tableName);
        } else if (!ifExists) {
            throw new TableNotFoundException(String.format("Table '%s' does not exist in Kudu master(s) '%s'.", tableName, masterHosts));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error dropping table '%s'", tableName), e);
    }
}
#end_block

#method_before
public static void populateColumnsFromKudu(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    org.apache.hadoop.hive.metastore.api.Table msTblCopy = msTbl.deepCopy();
    List<FieldSchema> cols = msTblCopy.getSd().getCols();
    String kuduTableName = msTblCopy.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    String masterHosts = msTblCopy.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Loading schema of table '%s' from master '%s'", kuduTableName, masterHosts));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        if (!kudu.tableExists(kuduTableName)) {
            throw new ImpalaRuntimeException(String.format("Table does not exist in Kudu: " + "'%s'", kuduTableName));
        }
        org.apache.kudu.client.KuduTable kuduTable = kudu.openTable(kuduTableName);
        // Replace the columns in the Metastore table with the columns from the recently
        // accessed Kudu schema.
        cols.clear();
        for (ColumnSchema colSchema : kuduTable.getSchema().getColumns()) {
            Type type = KuduUtil.toImpalaType(colSchema.getType());
            cols.add(new FieldSchema(colSchema.getName(), type.toSql().toLowerCase(), null));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error loading schema of table " + "'%s'", kuduTableName), e);
    }
    List<FieldSchema> newCols = msTbl.getSd().getCols();
    newCols.clear();
    newCols.addAll(cols);
}
#method_after
public static void populateColumnsFromKudu(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    org.apache.hadoop.hive.metastore.api.Table msTblCopy = msTbl.deepCopy();
    List<FieldSchema> cols = msTblCopy.getSd().getCols();
    String kuduTableName = msTblCopy.getParameters().get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    String masterHosts = msTblCopy.getParameters().get(KuduTable.KEY_MASTER_HOSTS);
    LOG.debug(String.format("Loading schema of table '%s' from master '%s'", kuduTableName, masterHosts));
    try (KuduClient kudu = new KuduClient.KuduClientBuilder(masterHosts).build()) {
        if (!kudu.tableExists(kuduTableName)) {
            throw new ImpalaRuntimeException(String.format("Table does not exist in Kudu: " + "'%s'", kuduTableName));
        }
        org.apache.kudu.client.KuduTable kuduTable = kudu.openTable(kuduTableName);
        // Replace the columns in the Metastore table with the columns from the recently
        // accessed Kudu schema.
        cols.clear();
        for (ColumnSchema colSchema : kuduTable.getSchema().getColumns()) {
            Type type = KuduUtil.toImpalaType(colSchema.getType());
            cols.add(new FieldSchema(colSchema.getName(), type.toSql().toLowerCase(), null));
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Error loading schema of table " + "'%s'", kuduTableName), e);
    }
    List<FieldSchema> newCols = msTbl.getSd().getCols();
    newCols.clear();
    newCols.addAll(cols);
}
#end_block

#method_before
public static void validateKuduTblExists(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    Map<String, String> properties = msTbl.getParameters();
    String storageHandler = properties.get(KuduTable.KEY_STORAGE_HANDLER);
    Preconditions.checkState(!Strings.isNullOrEmpty(storageHandler));
    if (!storageHandler.equals(KuduTable.KUDU_STORAGE_HANDLER)) {
        throw new ImpalaRuntimeException(String.format("Table '%s' does not represent a " + "Kudu table. Expected storage_handler '%s' but found '%s'", msTbl.getTableName(), KuduTable.KUDU_STORAGE_HANDLER, storageHandler));
    }
    String masterHosts = properties.get(KuduTable.KEY_MASTER_HOSTS);
    Preconditions.checkState(!Strings.isNullOrEmpty(masterHosts));
    String kuduTableName = properties.get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    try (KuduClient kudu = KuduUtil.createKuduClient(masterHosts)) {
        kudu.tableExists(kuduTableName);
    } catch (Exception e) {
        // TODO: This is misleading when there are other errors, e.g. timeouts.
        throw new ImpalaRuntimeException(String.format("Kudu table '%s' does not exist " + "on master '%s'", kuduTableName, masterHosts), e);
    }
}
#method_after
public static void validateKuduTblExists(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    Map<String, String> properties = msTbl.getParameters();
    if (!KuduTable.isKuduTable(msTbl)) {
        throw new ImpalaRuntimeException(String.format("Table '%s' does not represent a " + "Kudu table. Expected storage_handler '%s' but found '%s'", msTbl.getTableName(), KuduTable.KUDU_STORAGE_HANDLER, properties.get(KuduTable.KEY_STORAGE_HANDLER)));
    }
    String masterHosts = properties.get(KuduTable.KEY_MASTER_HOSTS);
    Preconditions.checkState(!Strings.isNullOrEmpty(masterHosts));
    String kuduTableName = properties.get(KuduTable.KEY_TABLE_NAME);
    Preconditions.checkState(!Strings.isNullOrEmpty(kuduTableName));
    try (KuduClient kudu = new KuduClient.KuduClientBuilder(masterHosts).build()) {
        kudu.tableExists(kuduTableName);
    } catch (Exception e) {
        throw new ImpalaRuntimeException(String.format("Kudu table '%s' does not exist " + "on master '%s'", kuduTableName, masterHosts), e);
    }
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = new KuduClientBuilder(kuduTable_.getKuduMasterHosts()).build()) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeMemLayout(analyzer);
    computeStats(analyzer);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = new KuduClientBuilder(kuduTable_.getKuduMasterHosts()).build()) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Compute mem layout before the scan range locations because creation of the Kudu
        // scan tokens depends on having a mem layout.
        computeMemLayout(analyzer);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeStats(analyzer);
}
#end_block

#method_before
private List<KuduScanToken> createScanTokens(KuduClient client, org.apache.kudu.client.KuduTable rpcTable) {
    List<String> projectedCols = Lists.newArrayList();
    for (SlotDescriptor desc : getTupleDesc().getSlots()) {
        if (desc.isMaterialized())
            projectedCols.add(desc.getColumn().getName());
    }
    KuduScanTokenBuilder tokenBuilder = client.newScanTokenBuilder(rpcTable);
    tokenBuilder.setProjectedColumnNames(projectedCols);
    for (KuduPredicate predicate : kuduPredicates_) tokenBuilder.addPredicate(predicate);
    return tokenBuilder.build();
}
#method_after
private List<KuduScanToken> createScanTokens(KuduClient client, org.apache.kudu.client.KuduTable rpcTable) {
    List<String> projectedCols = Lists.newArrayList();
    for (SlotDescriptor desc : getTupleDesc().getSlotsOrderedByOffset()) {
        projectedCols.add(desc.getColumn().getName());
    }
    KuduScanTokenBuilder tokenBuilder = client.newScanTokenBuilder(rpcTable);
    tokenBuilder.setProjectedColumnNames(projectedCols);
    for (KuduPredicate predicate : kuduPredicates_) tokenBuilder.addPredicate(predicate);
    return tokenBuilder.build();
}
#end_block

#method_before
static KuduException transformException(Exception e) {
    // The message may be null.
    String message = e.getMessage() == null ? "" : e.getMessage();
    if (e instanceof KuduException) {
        return (KuduException) e;
    } else if (e instanceof DeferredGroupException) {
        // The cause of a DeferredGroupException is the first exception it sees, we're just going to
        // use it as our main exception. DGE doesn't let us see the others exception anyways.
        Throwable cause = e.getCause();
        if (cause instanceof Exception) {
            return transformException((Exception) cause);
        }
    // Else fall down into a generic exception at the end.
    } else if (e instanceof TimeoutException) {
        Status statusTimeout = Status.TimedOut(message);
        return new NonRecoverableException(statusTimeout, e);
    } else if (e instanceof InterruptedException) {
        // Need to reset the interrupt flag since we caught it but aren't handling it.
        Thread.currentThread().interrupt();
        Status statusAborted = Status.Aborted(message);
        return new NonRecoverableException(statusAborted, e);
    }
    Status status = Status.IOError(message);
    return new NonRecoverableException(status, e);
}
#method_after
static KuduException transformException(Exception e) {
    // The message may be null.
    String message = e.getMessage() == null ? "" : e.getMessage();
    if (e instanceof KuduException) {
        return (KuduException) e;
    } else if (e instanceof DeferredGroupException) {
        // The cause of a DeferredGroupException is the first exception it sees, we're just going to
        // use it as our main exception. DGE doesn't let us see the other exceptions anyways.
        Throwable cause = e.getCause();
        if (cause instanceof Exception) {
            return transformException((Exception) cause);
        }
    // Else fall down into a generic exception at the end.
    } else if (e instanceof TimeoutException) {
        Status statusTimeout = Status.TimedOut(message);
        return new NonRecoverableException(statusTimeout, e);
    } else if (e instanceof InterruptedException) {
        // Need to reset the interrupt flag since we caught it but aren't handling it.
        Thread.currentThread().interrupt();
        Status statusAborted = Status.Aborted(message);
        return new NonRecoverableException(statusAborted, e);
    }
    Status status = Status.IOError(message);
    return new NonRecoverableException(status, e);
}
#end_block

#method_before
@Test
public void testNoLocalReplica() {
    RemoteTablet tablet = getTablet(0, -1);
    // We currently just send the last one in the list back.
    assertEquals("2", tablet.getClosestUUID());
}
#method_after
@Test
public void testNoLocalReplica() {
    RemoteTablet tablet = getTablet(0, -1);
    // We just care about getting one back.
    assertNotNull(tablet.getClosestUUID());
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    hasClusteredHint_ = false;
    hasNoClusteredHint_ = false;
    resultExprs_.clear();
}
#method_after
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    hasClusteredHint_ = false;
    resultExprs_.clear();
    primaryKeyExprs_.clear();
}
#end_block

#method_before
private void setTargetTable(Analyzer analyzer) throws AnalysisException {
    // INSERT before the table has actually been created.
    if (table_ == null) {
        if (!targetTableName_.isFullyQualified()) {
            targetTableName_ = new TableName(analyzer.getDefaultDb(), targetTableName_.getTbl());
        }
        table_ = analyzer.getTable(targetTableName_, Privilege.INSERT);
    } else {
        targetTableName_ = new TableName(table_.getDb().getName(), table_.getName());
        PrivilegeRequestBuilder pb = new PrivilegeRequestBuilder();
        analyzer.registerPrivReq(pb.onTable(table_.getDb().getName(), table_.getName()).allOf(Privilege.INSERT).toRequest());
    }
    // We do not support inserting into views.
    if (table_ instanceof View) {
        throw new AnalysisException(String.format("Impala does not support inserting into views: %s", table_.getFullName()));
    }
    for (Column c : table_.getColumns()) {
        if (!c.getType().isSupported()) {
            throw new AnalysisException(String.format("Unable to INSERT into target table " + "(%s) because the column '%s' has an unsupported type '%s'.", targetTableName_, c.getName(), c.getType().toSql()));
        }
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    if (partitionKeyValues_ != null && numClusteringCols == 0) {
        if (isHBaseTable) {
            throw new AnalysisException("PARTITION clause is not valid for INSERT into " + "HBase tables. '" + targetTableName_ + "' is an HBase table");
        } else {
            // Unpartitioned table, but INSERT has PARTITION clause
            throw new AnalysisException("PARTITION clause is only valid for INSERT into " + "partitioned table. '" + targetTableName_ + "' is not partitioned");
        }
    }
    if (table_ instanceof HdfsTable) {
        HdfsTable hdfsTable = (HdfsTable) table_;
        if (!hdfsTable.hasWriteAccess()) {
            throw new AnalysisException(String.format("Unable to INSERT into target table " + "(%s) because Impala does not have WRITE access to at least one HDFS path" + ": %s", targetTableName_, hdfsTable.getFirstLocationWithoutWriteAccess()));
        }
        StringBuilder error = new StringBuilder();
        hdfsTable.parseSkipHeaderLineCount(error);
        if (error.length() > 0)
            throw new AnalysisException(error.toString());
        try {
            if (!FileSystemUtil.isImpalaWritableFilesystem(hdfsTable.getLocation())) {
                throw new AnalysisException(String.format("Unable to INSERT into target " + "table (%s) because %s is not a supported filesystem.", targetTableName_, hdfsTable.getLocation()));
            }
        } catch (IOException e) {
            throw new AnalysisException(String.format("Unable to INSERT into target " + "table (%s): %s.", targetTableName_, e.getMessage()), e);
        }
        for (int colIdx = 0; colIdx < numClusteringCols; ++colIdx) {
            Column col = hdfsTable.getColumns().get(colIdx);
            // analysis check.
            if (col.getType() == Type.BOOLEAN) {
                throw new AnalysisException(String.format("INSERT into table with BOOLEAN " + "partition column (%s) is not supported: %s", col.getName(), targetTableName_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        if (overwrite_) {
            throw new AnalysisException("INSERT OVERWRITE not supported for Kudu tables.");
        }
        if (partitionKeyValues_ != null && !partitionKeyValues_.isEmpty()) {
            throw new AnalysisException("Partition specifications are not supported for Kudu tables.");
        }
    }
    if (isHBaseTable && overwrite_) {
        throw new AnalysisException("HBase doesn't have a way to perform INSERT OVERWRITE");
    }
    // Add target table to descriptor table.
    analyzer.getDescTbl().addReferencedTable(table_);
}
#method_after
private void setTargetTable(Analyzer analyzer) throws AnalysisException {
    // INSERT before the table has actually been created.
    if (table_ == null) {
        if (!targetTableName_.isFullyQualified()) {
            targetTableName_ = new TableName(analyzer.getDefaultDb(), targetTableName_.getTbl());
        }
        table_ = analyzer.getTable(targetTableName_, Privilege.INSERT);
    } else {
        targetTableName_ = new TableName(table_.getDb().getName(), table_.getName());
        PrivilegeRequestBuilder pb = new PrivilegeRequestBuilder();
        analyzer.registerPrivReq(pb.onTable(table_.getDb().getName(), table_.getName()).allOf(Privilege.INSERT).toRequest());
    }
    // We do not support inserting into views.
    if (table_ instanceof View) {
        throw new AnalysisException(String.format("Impala does not support inserting into views: %s", table_.getFullName()));
    }
    for (Column c : table_.getColumns()) {
        if (!c.getType().isSupported()) {
            throw new AnalysisException(String.format("Unable to INSERT into target table " + "(%s) because the column '%s' has an unsupported type '%s'.", targetTableName_, c.getName(), c.getType().toSql()));
        }
    }
    boolean isHBaseTable = (table_ instanceof HBaseTable);
    int numClusteringCols = isHBaseTable ? 0 : table_.getNumClusteringCols();
    if (partitionKeyValues_ != null && numClusteringCols == 0) {
        if (isHBaseTable) {
            throw new AnalysisException("PARTITION clause is not valid for INSERT into " + "HBase tables. '" + targetTableName_ + "' is an HBase table");
        } else {
            // Unpartitioned table, but INSERT has PARTITION clause
            throw new AnalysisException("PARTITION clause is only valid for INSERT into " + "partitioned table. '" + targetTableName_ + "' is not partitioned");
        }
    }
    if (table_ instanceof HdfsTable) {
        HdfsTable hdfsTable = (HdfsTable) table_;
        if (!hdfsTable.hasWriteAccess()) {
            throw new AnalysisException(String.format("Unable to INSERT into target table " + "(%s) because Impala does not have WRITE access to at least one HDFS path" + ": %s", targetTableName_, hdfsTable.getFirstLocationWithoutWriteAccess()));
        }
        StringBuilder error = new StringBuilder();
        hdfsTable.parseSkipHeaderLineCount(error);
        if (error.length() > 0)
            throw new AnalysisException(error.toString());
        try {
            if (!FileSystemUtil.isImpalaWritableFilesystem(hdfsTable.getLocation())) {
                throw new AnalysisException(String.format("Unable to INSERT into target " + "table (%s) because %s is not a supported filesystem.", targetTableName_, hdfsTable.getLocation()));
            }
        } catch (IOException e) {
            throw new AnalysisException(String.format("Unable to INSERT into target " + "table (%s): %s.", targetTableName_, e.getMessage()), e);
        }
        for (int colIdx = 0; colIdx < numClusteringCols; ++colIdx) {
            Column col = hdfsTable.getColumns().get(colIdx);
            // analysis check.
            if (col.getType() == Type.BOOLEAN) {
                throw new AnalysisException(String.format("INSERT into table with BOOLEAN " + "partition column (%s) is not supported: %s", col.getName(), targetTableName_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        if (overwrite_) {
            throw new AnalysisException("INSERT OVERWRITE not supported for Kudu tables.");
        }
        if (partitionKeyValues_ != null && !partitionKeyValues_.isEmpty()) {
            throw new AnalysisException("Partition specifications are not supported for Kudu tables.");
        }
    }
    if (isHBaseTable && overwrite_) {
        throw new AnalysisException("HBase doesn't have a way to perform INSERT OVERWRITE");
    }
    // Add target table to descriptor table.
    analyzer.getDescTbl().setTargetTable(table_);
}
#end_block

#method_before
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    // order, and add NULL expressions to all missing columns.
    for (Column tblColumn : table_.getColumnsInHiveOrder()) {
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                matchFound = true;
                break;
            }
        }
        // expression.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                // Unmentioned non-clustering columns get NULL literals with the appropriate
                // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                resultExprs_.add(NullLiteral.create(tblColumn.getType()));
            }
        }
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#method_after
private void prepareExpressions(List<Column> selectExprTargetColumns, List<Expr> selectListExprs, Table tbl, Analyzer analyzer) throws AnalysisException {
    // Temporary lists of partition key exprs and names in an arbitrary order.
    List<Expr> tmpPartitionKeyExprs = new ArrayList<Expr>();
    List<String> tmpPartitionKeyNames = new ArrayList<String>();
    int numClusteringCols = (tbl instanceof HBaseTable) ? 0 : tbl.getNumClusteringCols();
    // Check dynamic partition columns for type compatibility.
    for (int i = 0; i < selectListExprs.size(); ++i) {
        Column targetColumn = selectExprTargetColumns.get(i);
        Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), targetColumn, selectListExprs.get(i));
        if (targetColumn.getPosition() < numClusteringCols) {
            // This is a dynamic clustering column
            tmpPartitionKeyExprs.add(compatibleExpr);
            tmpPartitionKeyNames.add(targetColumn.getName());
        }
        selectListExprs.set(i, compatibleExpr);
    }
    // be in selectExprTargetColumns and therefore are ignored in this loop
    if (partitionKeyValues_ != null) {
        for (PartitionKeyValue pkv : partitionKeyValues_) {
            if (pkv.isStatic()) {
                // tableColumns is guaranteed to exist after the earlier analysis checks
                Column tableColumn = table_.getColumn(pkv.getColName());
                Expr compatibleExpr = checkTypeCompatibility(targetTableName_.toString(), tableColumn, pkv.getValue());
                tmpPartitionKeyExprs.add(compatibleExpr);
                tmpPartitionKeyNames.add(pkv.getColName());
            }
        }
    }
    // Hdfs folder structure correctly.
    for (Column c : table_.getColumns()) {
        for (int j = 0; j < tmpPartitionKeyNames.size(); ++j) {
            if (c.getName().equals(tmpPartitionKeyNames.get(j))) {
                partitionKeyExprs_.add(tmpPartitionKeyExprs.get(j));
                break;
            }
        }
    }
    Preconditions.checkState(partitionKeyExprs_.size() == numClusteringCols);
    // Make sure we have stats for partitionKeyExprs
    for (Expr expr : partitionKeyExprs_) {
        expr.analyze(analyzer);
    }
    // order, and add NULL expressions to all missing columns.
    for (Column tblColumn : table_.getColumnsInHiveOrder()) {
        boolean matchFound = false;
        for (int i = 0; i < selectListExprs.size(); ++i) {
            if (selectExprTargetColumns.get(i).getName().equals(tblColumn.getName())) {
                resultExprs_.add(selectListExprs.get(i));
                matchFound = true;
                break;
            }
        }
        // expression.
        if (!matchFound) {
            if (tblColumn.getPosition() >= numClusteringCols) {
                // Unmentioned non-clustering columns get NULL literals with the appropriate
                // target type because Parquet cannot handle NULL_TYPE (IMPALA-617).
                resultExprs_.add(NullLiteral.create(tblColumn.getType()));
            }
        }
        // Store exprs for Kudu key columns.
        if (matchFound && table_ instanceof KuduTable) {
            KuduTable kuduTable = (KuduTable) table_;
            if (kuduTable.isPrimaryKeyColumn(tblColumn.getName())) {
                primaryKeyExprs_.add(Iterables.getLast(resultExprs_));
            }
        }
    }
    if (table_ instanceof KuduTable) {
        Preconditions.checkState(!primaryKeyExprs_.isEmpty());
    }
    // TODO: Check that HBase row-key columns are not NULL? See IMPALA-406
    if (needsGeneratedQueryStatement_) {
        // Build a query statement that returns NULL for every column
        List<SelectListItem> selectListItems = Lists.newArrayList();
        for (Expr e : resultExprs_) {
            selectListItems.add(new SelectListItem(e, null));
        }
        SelectList selectList = new SelectList(selectListItems);
        queryStmt_ = new SelectStmt(selectList, null, null, null, null, null, null);
        queryStmt_.analyze(analyzer);
    }
}
#end_block

#method_before
private void analyzePlanHints(Analyzer analyzer) throws AnalysisException {
    if (planHints_ == null)
        return;
    if (!planHints_.isEmpty() && table_ instanceof HBaseTable) {
        throw new AnalysisException("INSERT hints are only supported for inserting into " + "Hdfs and Kudu tables.");
    }
    for (String hint : planHints_) {
        if (hint.equalsIgnoreCase("SHUFFLE")) {
            if (hasNoShuffleHint_) {
                throw new AnalysisException("Conflicting INSERT hint: " + hint);
            }
            hasShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.equalsIgnoreCase("NOSHUFFLE")) {
            if (hasShuffleHint_) {
                throw new AnalysisException("Conflicting INSERT hint: " + hint);
            }
            hasNoShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.equalsIgnoreCase("CLUSTERED")) {
            if (hasNoShuffleHint_) {
                throw new AnalysisException("Conflicting INSERT hint: " + hint);
            }
            hasClusteredHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.equalsIgnoreCase("NOCLUSTERED")) {
            if (hasClusteredHint_) {
                throw new AnalysisException("Conflicting INSERT hint: " + hint);
            }
            hasNoClusteredHint_ = true;
            analyzer.setHasPlanHints();
        } else {
            analyzer.addWarning("INSERT hint not recognized: " + hint);
        }
    }
    // Both flags may be false or one of them may be true, but not both.
    Preconditions.checkState((!hasShuffleHint_ && !hasNoShuffleHint_) || (hasShuffleHint_ ^ hasNoShuffleHint_));
    Preconditions.checkState((!hasClusteredHint_ && !hasNoClusteredHint_) || (hasClusteredHint_ ^ hasNoClusteredHint_));
}
#method_after
private void analyzePlanHints(Analyzer analyzer) throws AnalysisException {
    if (planHints_ == null)
        return;
    if (!planHints_.isEmpty() && table_ instanceof HBaseTable) {
        throw new AnalysisException("INSERT hints are only supported for inserting into " + "Hdfs and Kudu tables.");
    }
    boolean hasNoClusteredHint = false;
    for (String hint : planHints_) {
        if (hint.equalsIgnoreCase("SHUFFLE")) {
            hasShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.equalsIgnoreCase("NOSHUFFLE")) {
            hasNoShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.equalsIgnoreCase("CLUSTERED")) {
            hasClusteredHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.equalsIgnoreCase("NOCLUSTERED")) {
            hasNoClusteredHint = true;
            analyzer.setHasPlanHints();
        } else {
            analyzer.addWarning("INSERT hint not recognized: " + hint);
        }
    }
    // Both flags may be false or one of them may be true, but not both.
    if (hasShuffleHint_ && hasNoShuffleHint_) {
        throw new AnalysisException("Conflicting INSERT hints: shuffle and noshuffle");
    }
    if (hasClusteredHint_ && hasNoClusteredHint) {
        throw new AnalysisException("Conflicting INSERT hints: clustered and noclustered");
    }
}
#end_block

#method_before
public void substituteResultExprs(ExprSubstitutionMap smap, Analyzer analyzer) {
    resultExprs_ = Expr.substituteList(resultExprs_, smap, analyzer, true);
    partitionKeyExprs_ = Expr.substituteList(partitionKeyExprs_, smap, analyzer, true);
}
#method_after
public void substituteResultExprs(ExprSubstitutionMap smap, Analyzer analyzer) {
    resultExprs_ = Expr.substituteList(resultExprs_, smap, analyzer, true);
    partitionKeyExprs_ = Expr.substituteList(partitionKeyExprs_, smap, analyzer, true);
    primaryKeyExprs_ = Expr.substituteList(primaryKeyExprs_, smap, analyzer, true);
}
#end_block

#method_before
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
            // Add optional clustering node to the plan, based on clustered/noclustered plan
            // hint.
            rootFragment = distributedPlanner.addClusteringFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer());
        }
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (RuntimeEnv.INSTANCE.computeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#method_after
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // Add optional sort node to the plan, based on clustered/noclustered plan hint.
        createClusteringSort(insertStmt, rootFragment, ctx_.getRootAnalyzer());
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (RuntimeEnv.INSTANCE.computeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#end_block

#method_before
@Test
public void testBasicFunctionality() throws Exception {
    try (MiniKdc kdc = MiniKdc.withDefaults()) {
        kdc.start();
        kdc.createUserPrincipal("alice");
        kdc.kinit("alice");
        kdc.stop();
        kdc.start();
        kdc.createUserPrincipal("bob");
        kdc.kinit("bob");
        kdc.createUserPrincipal("kudu/KRBTEST.COM");
        String klist = kdc.klist();
        assertTrue(klist.contains("alice@KRBTEST.COM"));
        assertTrue(klist.contains("bob@KRBTEST.COM"));
        assertTrue(klist.contains("krbtgt/KRBTEST.COM@KRBTEST.COM"));
    }
}
#method_after
@Test
public void testBasicFunctionality() throws Exception {
    try (MiniKdc kdc = MiniKdc.withDefaults()) {
        kdc.start();
        kdc.createUserPrincipal("alice");
        kdc.kinit("alice");
        kdc.stop();
        kdc.start();
        kdc.createUserPrincipal("bob");
        kdc.kinit("bob");
        kdc.createServiceKeytab("kudu/KRBTEST.COM");
        String klist = kdc.klist();
        assertTrue(klist.contains("alice@KRBTEST.COM"));
        assertTrue(klist.contains("bob@KRBTEST.COM"));
        assertTrue(klist.contains("krbtgt/KRBTEST.COM@KRBTEST.COM"));
    }
}
#end_block

#method_before
public void start() throws IOException {
    Preconditions.checkState(kdcProcess == null);
    LOG.debug("starting KDC {}", options);
    File dataRootDir = options.dataRoot.toFile();
    if (!dataRootDir.exists()) {
        if (!dataRootDir.mkdir()) {
            throw new RuntimeException(String.format("unable to create krb5 state directory: %s", dataRootDir));
        }
        createKdcConf();
        createKrb5Conf();
        // Create the KDC database using the kdb5_util tool.
        checkReturnCode(startProcessWithKrbEnv(getBinaryPath("kdb5_util"), "create", // Stash the master password.
        "-s", // Set a password.
        "-P", // Set a password.
        "masterpw", // Use weak entropy (since we don't need real security).
        "-W"), "kdb5_util");
    }
    kdcProcess = startProcessWithKrbEnv(getBinaryPath("krb5kdc"), // Do not daemonize.
    "-n");
// The C++ MiniKdc defaults to binding the KDC to an ephemeral port, which
// it then finds using lsof at this point. Java is unable to do that since
// the Process API does not expose the subprocess PID. As a result, this
// MiniKdc doesn't support binding to an ephemeral port, and we use the
// potentially race TestUtils.findFreePort instead. The upside is that we
// don't have to rewrite the config files.
}
#method_after
public void start() throws IOException {
    Preconditions.checkState(kdcProcess == null);
    LOG.debug("starting KDC {}", options);
    File dataRootDir = options.dataRoot.toFile();
    if (!dataRootDir.exists()) {
        if (!dataRootDir.mkdir()) {
            throw new RuntimeException(String.format("unable to create krb5 state directory: %s", dataRootDir));
        }
        File credentialCacheDir = options.dataRoot.resolve("krb5cc").toFile();
        if (!credentialCacheDir.mkdir()) {
            throw new RuntimeException(String.format("unable to create credential cache directory: %s", credentialCacheDir));
        }
        createKdcConf();
        createKrb5Conf();
        // Create the KDC database using the kdb5_util tool.
        checkReturnCode(startProcessWithKrbEnv(getBinaryPath("kdb5_util"), "create", // Stash the master password.
        "-s", // Set a password.
        "-P", // Set a password.
        "masterpw", // Use weak entropy (since we don't need real security).
        "-W"), "kdb5_util");
    }
    kdcProcess = startProcessWithKrbEnv(getBinaryPath("krb5kdc"), // Do not daemonize.
    "-n");
// The C++ MiniKdc defaults to binding the KDC to an ephemeral port, which
// it then finds using lsof at this point. Java is unable to do that since
// the Process API does not expose the subprocess PID. As a result, this
// MiniKdc doesn't support binding to an ephemeral port, and we use the
// race-prone TestUtils.findFreePort instead. The upside is that we
// don't have to rewrite the config files.
}
#end_block

#method_before
@Override
public void close() throws IOException {
    LOG.debug("closing KDC {}", options);
    try {
        if (kdcProcess != null) {
            kdcProcess.destroy();
            kdcProcess.waitFor();
        }
        FileUtils.deleteDirectory(options.dataRoot.toFile());
    } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
    } finally {
        kdcProcess = null;
    }
}
#method_after
@Override
public void close() throws IOException {
    LOG.debug("closing KDC {}", options);
    try {
        if (kdcProcess != null) {
            stop();
        }
    } finally {
        FileUtils.deleteDirectory(options.dataRoot.toFile());
    }
}
#end_block

#method_before
private static String getBinaryPath(String executable, List<String> searchPaths) throws IOException {
    for (String path : searchPaths) {
        File f = new File(path + File.separatorChar + executable);
        if (f.exists() && f.canExecute()) {
            return f.getPath();
        }
    }
    Process which = new ProcessBuilder().command("which", executable).start();
    checkReturnCode(which, "which");
    return CharStreams.toString(new InputStreamReader(which.getInputStream()));
}
#method_after
private static String getBinaryPath(String executable, List<String> searchPaths) throws IOException {
    for (String path : searchPaths) {
        File f = Paths.get(path).resolve(executable).toFile();
        if (f.exists() && f.canExecute()) {
            return f.getPath();
        }
    }
    Process which = new ProcessBuilder().command("which", executable).start();
    checkReturnCode(which, "which");
    return CharStreams.toString(new InputStreamReader(which.getInputStream())).trim();
}
#end_block

#method_before
public void addDefaultPartition(StorageDescriptor storageDescriptor) throws CatalogException {
    // Default partition has no files and is not referred to by scan nodes. Data sinks
    // refer to this to understand how to create new partitions. If this method is called
    // on a table that already has a default partition, it will be overwritten.
    HdfsStorageDescriptor hdfsStorageDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    HdfsPartition partition = HdfsPartition.defaultPartition(this, hdfsStorageDescriptor);
    partitionMap_.put(partition.getId(), partition);
}
#method_after
public void addDefaultPartition(StorageDescriptor storageDescriptor) throws CatalogException {
    // Default partition has no files and is not referred to by scan nodes. Data sinks
    // refer to this to understand how to create new partitions.
    HdfsStorageDescriptor hdfsStorageDescriptor = HdfsStorageDescriptor.fromStorageDescriptor(this.name_, storageDescriptor);
    HdfsPartition partition = HdfsPartition.defaultPartition(this, hdfsStorageDescriptor);
    partitionMap_.put(partition.getId(), partition);
}
#end_block

#method_before
private boolean alterTableSetFileFormat(Table tbl, List<TPartitionKeyValue> partitionSpec, THdfsFileFormat fileFormat) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    Preconditions.checkState(partitionSpec == null || !partitionSpec.isEmpty());
    boolean reloadFileMetadata = false;
    if (partitionSpec == null) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        setStorageDescriptorFileFormat(msTbl.getSd(), fileFormat);
        // The default partition must be updated if the file format is changed.
        if (tbl instanceof HdfsTable)
            ((HdfsTable) tbl).addDefaultPartition(msTbl.getSd());
        applyAlterTable(msTbl);
        reloadFileMetadata = true;
    } else {
        TableName tableName = tbl.getTableName();
        HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
        Preconditions.checkNotNull(partition);
        partition.setFileFormat(HdfsFileFormat.fromThrift(fileFormat));
        try {
            applyAlterPartition(tbl, partition);
        } finally {
            partition.markDirty();
        }
    }
    return reloadFileMetadata;
}
#method_after
private boolean alterTableSetFileFormat(Table tbl, List<TPartitionKeyValue> partitionSpec, THdfsFileFormat fileFormat) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    Preconditions.checkState(partitionSpec == null || !partitionSpec.isEmpty());
    boolean reloadFileMetadata = false;
    if (partitionSpec == null) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        setStorageDescriptorFileFormat(msTbl.getSd(), fileFormat);
        // partitions are created with the new file format.
        if (tbl instanceof HdfsTable)
            ((HdfsTable) tbl).addDefaultPartition(msTbl.getSd());
        applyAlterTable(msTbl);
        reloadFileMetadata = true;
    } else {
        TableName tableName = tbl.getTableName();
        HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
        Preconditions.checkNotNull(partition);
        partition.setFileFormat(HdfsFileFormat.fromThrift(fileFormat));
        try {
            applyAlterPartition(tbl, partition);
        } finally {
            partition.markDirty();
        }
    }
    return reloadFileMetadata;
}
#end_block

#method_before
private boolean alterTableSetLocation(Table tbl, List<TPartitionKeyValue> partitionSpec, String location) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    Preconditions.checkState(partitionSpec == null || !partitionSpec.isEmpty());
    boolean reloadFileMetadata = false;
    if (partitionSpec == null) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        if (msTbl.getPartitionKeysSize() == 0)
            reloadFileMetadata = true;
        msTbl.getSd().setLocation(location);
        if (tbl instanceof HdfsTable)
            ((HdfsTable) tbl).addDefaultPartition(msTbl.getSd());
        applyAlterTable(msTbl);
    } else {
        TableName tableName = tbl.getTableName();
        HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
        partition.setLocation(location);
        try {
            applyAlterPartition(tbl, partition);
        } finally {
            partition.markDirty();
        }
    }
    return reloadFileMetadata;
}
#method_after
private boolean alterTableSetLocation(Table tbl, List<TPartitionKeyValue> partitionSpec, String location) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    Preconditions.checkState(partitionSpec == null || !partitionSpec.isEmpty());
    boolean reloadFileMetadata = false;
    if (partitionSpec == null) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        if (msTbl.getPartitionKeysSize() == 0)
            reloadFileMetadata = true;
        msTbl.getSd().setLocation(location);
        applyAlterTable(msTbl);
    } else {
        TableName tableName = tbl.getTableName();
        HdfsPartition partition = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
        partition.setLocation(location);
        try {
            applyAlterPartition(tbl, partition);
        } finally {
            partition.markDirty();
        }
    }
    return reloadFileMetadata;
}
#end_block

#method_before
@VisibleForTesting
List<TabletClient> getTabletClients() {
    return connectionCache.getTabletClients();
}
#method_after
@VisibleForTesting
List<TabletClient> getTabletClients() {
    return connectionCache.getImmutableTabletClientsList();
}
#end_block

#method_before
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws KuduException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        String tabletId = tabletPb.getTabletId().toStringUtf8();
        List<UnknownHostException> lookupExceptions = new ArrayList<>(tabletPb.getReplicasCount());
        for (Master.TabletLocationsPB.ReplicaPB replica : tabletPb.getReplicasList()) {
            try {
                connectionCache.connectTS(replica.getTsInfo());
            } catch (UnknownHostException ex) {
                lookupExceptions.add(ex);
            }
            if (!lookupExceptions.isEmpty() && lookupExceptions.size() == tabletPb.getReplicasCount()) {
                Status statusIOE = Status.IOError("Couldn't find any valid locations, exceptions: " + lookupExceptions);
                throw new NonRecoverableException(statusIOE);
            }
        }
        Partition partition = ProtobufHelper.pbToPartition(tabletPb.getPartition());
        RemoteTablet rt = new RemoteTablet(tableId, tabletId, partition, tabletPb);
        LOG.info("Learned about tablet {} for table '{}' with partition {}", rt.getTabletId(), tableName, rt.getPartition());
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
    // Now test if we found the tablet we were looking for. If so, RetryRpcCB will retry the RPC
    // right away. If not, we throw an exception that RetryRpcErrback will understand as needing to
    // sleep before retrying.
    TableLocationsCache.Entry entry = locationsCache.get(requestPartitionKey);
    if (!entry.isNonCoveredRange() && entry.getTablet().getLeaderUUID() == null) {
        throw new NoLeaderFoundException(Status.NotFound("Tablet " + entry.toString() + " doesn't have a leader"));
    }
}
#method_after
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws KuduException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        String tabletId = tabletPb.getTabletId().toStringUtf8();
        List<UnknownHostException> lookupExceptions = new ArrayList<>(tabletPb.getReplicasCount());
        for (Master.TabletLocationsPB.ReplicaPB replica : tabletPb.getReplicasList()) {
            try {
                connectionCache.connectTS(replica.getTsInfo());
            } catch (UnknownHostException ex) {
                lookupExceptions.add(ex);
            }
        }
        if (!lookupExceptions.isEmpty() && lookupExceptions.size() == tabletPb.getReplicasCount()) {
            Status statusIOE = Status.IOError("Couldn't find any valid locations, exceptions: " + lookupExceptions);
            throw new NonRecoverableException(statusIOE);
        }
        Partition partition = ProtobufHelper.pbToPartition(tabletPb.getPartition());
        RemoteTablet rt = new RemoteTablet(tableId, tabletId, partition, tabletPb);
        LOG.info("Learned about tablet {} for table '{}' with partition {}", rt.getTabletId(), tableName, rt.getPartition());
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
    // Now test if we found the tablet we were looking for. If so, RetryRpcCB will retry the RPC
    // right away. If not, we throw an exception that RetryRpcErrback will understand as needing to
    // sleep before retrying.
    TableLocationsCache.Entry entry = locationsCache.get(requestPartitionKey);
    if (!entry.isNonCoveredRange() && entry.getTablet().getLeaderUUID() == null) {
        throw new NoLeaderFoundException(Status.NotFound("Tablet " + entry.toString() + " doesn't have a leader"));
    }
}
#end_block

#method_before
private void disconnectAndWait() throws InterruptedException {
    for (TabletClient tabletClient : client.getTabletClients()) {
        tabletClient.disconnect();
    }
    Stopwatch sw = Stopwatch.createStarted();
    boolean allDead = false;
    while (sw.elapsed(TimeUnit.MILLISECONDS) < DEFAULT_SLEEP) {
        boolean sleep = false;
        if (!client.getTabletClients().isEmpty()) {
            for (TabletClient tserver : client.getTabletClients()) {
                if (tserver.isAlive()) {
                    sleep = true;
                    break;
                }
            }
            allDead = true;
        }
        if (sleep) {
            Thread.sleep(50);
        } else {
            break;
        }
    }
    assertTrue(allDead);
}
#method_after
private void disconnectAndWait() throws InterruptedException {
    for (TabletClient tabletClient : client.getTabletClients()) {
        tabletClient.disconnect();
    }
    Stopwatch sw = Stopwatch.createStarted();
    boolean allDead = false;
    while (sw.elapsed(TimeUnit.MILLISECONDS) < DEFAULT_SLEEP) {
        boolean sleep = false;
        if (!client.getTabletClients().isEmpty()) {
            for (TabletClient tserver : client.getTabletClients()) {
                if (tserver.isAlive()) {
                    sleep = true;
                    break;
                }
            }
        }
        if (sleep) {
            Thread.sleep(50);
        } else {
            allDead = true;
            break;
        }
    }
    assertTrue(allDead);
}
#end_block

#method_before
@Test
public void testLeaderFirst() {
    RemoteTablet tablet = getTablet(0);
    // Test we can remove it.
    assertTrue(tablet.removeTabletClient("0"));
    assertEquals(null, tablet.getLeaderUUID());
    // Test demoting it doesn't break anything.
    tablet.demoteLeader("0");
    assertEquals(null, tablet.getLeaderUUID());
}
#method_after
@Test
public void testLeaderFirst() {
    RemoteTablet tablet = getTablet(0);
    // Test we can remove it.
    assertTrue(tablet.removeTabletClient("0"));
    assertEquals(null, tablet.getLeaderUUID());
    // Test demoting it doesn't break anything.
    tablet.demoteLeader("0");
    assertEquals(null, tablet.getLeaderUUID());
    // Test removing a server with no leader doesn't break.
    assertTrue(tablet.removeTabletClient("2"));
}
#end_block

#method_before
@Test(timeout = 50000)
public void test() throws Exception {
    try (MiniKuduCluster cluster = new MiniKuduCluster.MiniKuduClusterBuilder().numMasters(3).build()) {
        AsyncKuduClient client = new AsyncKuduClient.AsyncKuduClientBuilder(cluster.getMasterAddresses()).build();
        List<HostAndPort> addresses = cluster.getMasterHostPorts();
        ConnectionCache cache = new ConnectionCache(client);
        int i = 0;
        for (HostAndPort hp : addresses) {
            TabletClient conn = cache.newClient(i + "", hp.getHostText(), hp.getPort());
            // Ping the process so we go through the whole connection process.
            pingConnection(conn);
            i++;
        }
        assertEquals(3, cache.getTabletClients().size());
        assertFalse(cache.allConnectionsAreDead());
        TabletClient conn = cache.getClient("0");
        // Kill the connection.
        conn.shutdown().join();
        waitForConnectionToDie(conn);
        assertFalse(conn.isAlive());
        // Make sure the cache also knows it's dead, but that not all the connections are.
        assertFalse(cache.getClient("0").isAlive());
        assertFalse(cache.allConnectionsAreDead());
        // Test reconnecting with only the UUID.
        TabletClient newConn = cache.getLiveClient("0");
        assertFalse(conn == newConn);
        pingConnection(newConn);
        // Test disconnecting and make sure we cleaned up all the connections.
        cache.disconnectEverything().join();
        waitForConnectionToDie(cache.getClient("0"));
        waitForConnectionToDie(cache.getClient("1"));
        waitForConnectionToDie(cache.getClient("2"));
        assertTrue(cache.allConnectionsAreDead());
    }
}
#method_after
@Test(timeout = 50000)
public void test() throws Exception {
    try (MiniKuduCluster cluster = new MiniKuduCluster.MiniKuduClusterBuilder().numMasters(3).build()) {
        AsyncKuduClient client = new AsyncKuduClient.AsyncKuduClientBuilder(cluster.getMasterAddresses()).build();
        List<HostAndPort> addresses = cluster.getMasterHostPorts();
        ConnectionCache cache = new ConnectionCache(client);
        int i = 0;
        for (HostAndPort hp : addresses) {
            TabletClient conn = cache.newClient(i + "", hp.getHostText(), hp.getPort());
            // Ping the process so we go through the whole connection process.
            pingConnection(conn);
            i++;
        }
        assertEquals(3, cache.getImmutableTabletClientsList().size());
        assertFalse(cache.allConnectionsAreDead());
        TabletClient conn = cache.getClient("0");
        // Kill the connection.
        conn.shutdown().join();
        waitForConnectionToDie(conn);
        assertFalse(conn.isAlive());
        // Make sure the cache also knows it's dead, but that not all the connections are.
        assertFalse(cache.getClient("0").isAlive());
        assertFalse(cache.allConnectionsAreDead());
        // Test reconnecting with only the UUID.
        TabletClient newConn = cache.getLiveClient("0");
        assertFalse(conn == newConn);
        pingConnection(newConn);
        // Test disconnecting and make sure we cleaned up all the connections.
        cache.disconnectEverything().join();
        waitForConnectionToDie(cache.getClient("0"));
        waitForConnectionToDie(cache.getClient("1"));
        waitForConnectionToDie(cache.getClient("2"));
        assertTrue(cache.allConnectionsAreDead());
    }
}
#end_block

#method_before
boolean removeTabletClient(String uuid) {
    synchronized (tabletServers) {
        int index = tabletServers.indexOf(uuid);
        if (index == -1) {
            LOG.debug("tablet {} already removed ts {}, size left is {}", getTabletId(), uuid, tabletServers.size());
            // we removed it already
            return false;
        }
        tabletServers.remove(index);
        if (leaderIndex == index) {
            leaderIndex = NO_LEADER_INDEX;
        } else if (leaderIndex > index) {
            // leader moved down the list
            leaderIndex--;
            LOG.debug("tablet {} removed ts {}, size left is {}, leader is at {}", getTabletId(), uuid, tabletServers.size(), leaderIndex);
        }
        return true;
    }
}
#method_after
boolean removeTabletClient(String uuid) {
    synchronized (tabletServers) {
        if (leaderUuid != null && leaderUuid.equals(uuid)) {
            leaderUuid = null;
        }
        if (tabletServers.remove(uuid)) {
            return true;
        }
        LOG.debug("tablet {} already removed ts {}, size left is {}", getTabletId(), uuid, tabletServers.size());
        return false;
    }
}
#end_block

#method_before
void demoteLeader(String uuid) {
    synchronized (tabletServers) {
        int index = tabletServers.indexOf(uuid);
        // else beat us to it), then we just noop.
        if (index == -1 || leaderIndex == NO_LEADER_INDEX) {
            LOG.debug("{} couldn't be demoted as the leader for {}", uuid, getTabletId());
            return;
        }
        if (leaderIndex == index) {
            leaderIndex = NO_LEADER_INDEX;
            LOG.debug("{} was demoted as the leader for {}", uuid, getTabletId());
        } else {
            LOG.debug("{} wasn't the leader for {}, current leader is at index {}", uuid, getTabletId(), leaderIndex);
        }
    }
}
#method_after
void demoteLeader(String uuid) {
    synchronized (tabletServers) {
        if (leaderUuid == null) {
            LOG.debug("{} couldn't be demoted as the leader for {}, there is no known leader", uuid, getTabletId());
            return;
        }
        if (leaderUuid.equals(uuid)) {
            leaderUuid = null;
            LOG.debug("{} was demoted as the leader for {}", uuid, getTabletId());
        } else {
            LOG.debug("{} wasn't the leader for {}, current leader is {}", uuid, getTabletId(), leaderUuid);
        }
    }
}
#end_block

#method_before
String getLeaderUUID() {
    synchronized (tabletServers) {
        if (tabletServers.isEmpty()) {
            return null;
        }
        if (leaderIndex == NO_LEADER_INDEX) {
            return null;
        } else {
            return tabletServers.get(leaderIndex);
        }
    }
}
#method_after
String getLeaderUUID() {
    synchronized (tabletServers) {
        return leaderUuid;
    }
}
#end_block

#method_before
void connectTS(Master.TSInfoPB tsInfoPB) throws UnknownHostException {
    List<Common.HostPortPB> addresses = tsInfoPB.getRpcAddressesList();
    String uuid = tsInfoPB.getPermanentUuid().toStringUtf8();
    if (addresses.isEmpty()) {
        LOG.warn("Received a tablet server with no addresses, UUID: {uuid}", uuid);
        return;
    }
    // from meta_cache.cc
    // TODO: if the TS advertises multiple host/ports, pick the right one
    // based on some kind of policy. For now just use the first always.
    String ip = getIP(addresses.get(0).getHost());
    if (ip == null) {
        throw new UnknownHostException("Failed to resolve the IP of `" + addresses.get(0).getHost() + "'");
    }
    newClient(uuid, ip, addresses.get(0).getPort());
}
#method_after
void connectTS(Master.TSInfoPB tsInfoPB) throws UnknownHostException {
    List<Common.HostPortPB> addresses = tsInfoPB.getRpcAddressesList();
    String uuid = tsInfoPB.getPermanentUuid().toStringUtf8();
    if (addresses.isEmpty()) {
        LOG.warn("Received a tablet server with no addresses, UUID: {}", uuid);
        return;
    }
    // from meta_cache.cc
    // TODO: if the TS advertises multiple host/ports, pick the right one
    // based on some kind of policy. For now just use the first always.
    String ip = getIP(addresses.get(0).getHost());
    if (ip == null) {
        throw new UnknownHostException("Failed to resolve the IP of `" + addresses.get(0).getHost() + "'");
    }
    newClient(uuid, ip, addresses.get(0).getPort());
}
#end_block

#method_before
TabletClient getLiveClient(String uuid) {
    TabletClient client;
    readLock.lock();
    try {
        client = uuid2client.get(uuid);
    } finally {
        readLock.unlock();
    }
    if (client == null) {
        return null;
    } else if (client.isAlive()) {
        return client;
    } else {
        return newClient(uuid, client.getHost(), client.getPort());
    }
}
#method_after
TabletClient getLiveClient(String uuid) {
    TabletClient client = getClient(uuid);
    if (client == null) {
        return null;
    } else if (client.isAlive()) {
        return client;
    } else {
        return newClient(uuid, client.getHost(), client.getPort());
    }
}
#end_block

#method_before
Deferred<ArrayList<Void>> disconnectEverything() {
    ArrayList<Deferred<Void>> deferreds = new ArrayList<>();
    readLock.lock();
    try {
        for (TabletClient ts : uuid2client.values()) {
            deferreds.add(ts.shutdown());
        }
    } finally {
        readLock.unlock();
    }
    return Deferred.group(deferreds);
}
#method_after
Deferred<ArrayList<Void>> disconnectEverything() {
    readLock.lock();
    try {
        ArrayList<Deferred<Void>> deferreds = new ArrayList<>(uuid2client.size());
        for (TabletClient ts : uuid2client.values()) {
            deferreds.add(ts.shutdown());
        }
        return Deferred.group(deferreds);
    } finally {
        readLock.unlock();
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    fnCall_.analyze(analyzer);
    super.analyze(analyzer);
    type_ = getFnCall().getType();
    for (Expr e : partitionExprs_) {
        if (e.isConstant()) {
            throw new AnalysisException("Expressions in the PARTITION BY clause must not be constant: " + e.toSql() + " (in " + toSql() + ")");
        } else if (e.getType().isComplexType()) {
            throw new AnalysisException(String.format("PARTITION BY expression '%s' with " + "complex type '%s' is not supported.", e.toSql(), e.getType().toSql()));
        }
    }
    for (OrderByElement e : orderByElements_) {
        if (e.getExpr().isConstant()) {
            throw new AnalysisException("Expressions in the ORDER BY clause must not be constant: " + e.getExpr().toSql() + " (in " + toSql() + ")");
        } else if (e.getExpr().getType().isComplexType()) {
            throw new AnalysisException(String.format("ORDER BY expression '%s' with " + "complex type '%s' is not supported.", e.getExpr().toSql(), e.getExpr().getType().toSql()));
        }
    }
    if (getFnCall().getParams().isDistinct()) {
        throw new AnalysisException("DISTINCT not allowed in analytic function: " + getFnCall().toSql());
    }
    if (getFnCall().getParams().isIgnoreNulls()) {
        String fnName = getFnCall().getFnName().getFunction();
        // to allow statement rewriting for subqueries.
        if (!fnName.equals(LAST_VALUE) && !fnName.equals(FIRST_VALUE) && !fnName.equals(LAST_VALUE_IGNORE_NULLS) && !fnName.equals(FIRST_VALUE_IGNORE_NULLS)) {
            throw new AnalysisException("Function " + fnName.toUpperCase() + " does not accept the keyword IGNORE NULLS.");
        }
    }
    // check for correct composition of analytic expr
    Function fn = getFnCall().getFn();
    if (!(fn instanceof AggregateFunction)) {
        throw new AnalysisException("OVER clause requires aggregate or analytic function: " + getFnCall().toSql());
    }
    // check for non-analytic aggregate functions
    if (!isAnalyticFn(fn)) {
        throw new AnalysisException(String.format("Aggregate function '%s' not supported with OVER clause.", getFnCall().toSql()));
    }
    if (isAnalyticFn(fn) && !isAggregateFn(fn)) {
        if (orderByElements_.isEmpty()) {
            throw new AnalysisException("'" + getFnCall().toSql() + "' requires an ORDER BY clause");
        }
        if ((isRankingFn(fn) || isOffsetFn(fn)) && window_ != null) {
            throw new AnalysisException("Windowing clause not allowed with '" + getFnCall().toSql() + "'");
        }
        if (isOffsetFn(fn) && getFnCall().getChildren().size() > 1) {
            checkOffset(analyzer);
            // TODO: remove this check when the backend can handle non-constants
            if (getFnCall().getChildren().size() > 2) {
                if (!getFnCall().getChild(2).isConstant()) {
                    throw new AnalysisException("The default parameter (parameter 3) of LEAD/LAG must be a constant: " + getFnCall().toSql());
                }
            }
        }
        if (isNtileFn(fn)) {
            // TODO: IMPALA-2171:Remove this when ntile() can handle a non-constant argument.
            if (!getFnCall().getChild(0).isConstant()) {
                throw new AnalysisException("NTILE() requires a constant argument");
            }
            // Check if argument value is zero or negative and throw an exception if found.
            try {
                TColumnValue bucketValue = FeSupport.EvalConstExpr(getFnCall().getChild(0), analyzer.getQueryCtx());
                Long arg = bucketValue.getLong_val();
                if (arg <= 0) {
                    throw new AnalysisException("NTILE() requires a positive argument: " + arg);
                }
            } catch (InternalException e) {
                throw new AnalysisException(e.toString());
            }
        }
    }
    if (window_ != null) {
        if (orderByElements_.isEmpty()) {
            throw new AnalysisException("Windowing clause requires ORDER BY clause: " + toSql());
        }
        window_.analyze(analyzer);
        if (!orderByElements_.isEmpty() && window_.getType() == AnalyticWindow.Type.RANGE) {
            // check that preceding/following ranges match ordering
            if (window_.getLeftBoundary().getType().isOffset()) {
                checkRangeOffsetBoundaryExpr(window_.getLeftBoundary());
            }
            if (window_.getRightBoundary() != null && window_.getRightBoundary().getType().isOffset()) {
                checkRangeOffsetBoundaryExpr(window_.getRightBoundary());
            }
        }
    }
    // check nesting
    if (TreeNode.contains(getChildren(), AnalyticExpr.class)) {
        throw new AnalysisException("Nesting of analytic expressions is not allowed: " + toSql());
    }
    sqlString_ = toSql();
    standardize(analyzer);
    // unbounded).
    if (window_ != null && isMinMax(fn) && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING) {
        throw new AnalysisException("'" + getFnCall().toSql() + "' is only supported with an " + "UNBOUNDED PRECEDING start bound.");
    }
    setChildren();
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    fnCall_.analyze(analyzer);
    super.analyze(analyzer);
    type_ = getFnCall().getType();
    for (Expr e : partitionExprs_) {
        if (e.isConstant()) {
            throw new AnalysisException("Expressions in the PARTITION BY clause must not be constant: " + e.toSql() + " (in " + toSql() + ")");
        } else if (e.getType().isComplexType()) {
            throw new AnalysisException(String.format("PARTITION BY expression '%s' with " + "complex type '%s' is not supported.", e.toSql(), e.getType().toSql()));
        }
    }
    for (OrderByElement e : orderByElements_) {
        if (e.getExpr().isConstant()) {
            throw new AnalysisException("Expressions in the ORDER BY clause must not be constant: " + e.getExpr().toSql() + " (in " + toSql() + ")");
        } else if (e.getExpr().getType().isComplexType()) {
            throw new AnalysisException(String.format("ORDER BY expression '%s' with " + "complex type '%s' is not supported.", e.getExpr().toSql(), e.getExpr().getType().toSql()));
        }
    }
    if (getFnCall().getParams().isDistinct()) {
        throw new AnalysisException("DISTINCT not allowed in analytic function: " + getFnCall().toSql());
    }
    if (getFnCall().getParams().isIgnoreNulls()) {
        String fnName = getFnCall().getFnName().getFunction();
        if (!fnName.equals(LAST_VALUE) && !fnName.equals(FIRST_VALUE)) {
            throw new AnalysisException("Function " + fnName.toUpperCase() + " does not accept the keyword IGNORE NULLS.");
        }
    }
    // check for correct composition of analytic expr
    Function fn = getFnCall().getFn();
    if (!(fn instanceof AggregateFunction)) {
        throw new AnalysisException("OVER clause requires aggregate or analytic function: " + getFnCall().toSql());
    }
    // check for non-analytic aggregate functions
    if (!isAnalyticFn(fn)) {
        throw new AnalysisException(String.format("Aggregate function '%s' not supported with OVER clause.", getFnCall().toSql()));
    }
    if (isAnalyticFn(fn) && !isAggregateFn(fn)) {
        if (orderByElements_.isEmpty()) {
            throw new AnalysisException("'" + getFnCall().toSql() + "' requires an ORDER BY clause");
        }
        if ((isRankingFn(fn) || isOffsetFn(fn)) && window_ != null) {
            throw new AnalysisException("Windowing clause not allowed with '" + getFnCall().toSql() + "'");
        }
        if (isOffsetFn(fn) && getFnCall().getChildren().size() > 1) {
            checkOffset(analyzer);
            // TODO: remove this check when the backend can handle non-constants
            if (getFnCall().getChildren().size() > 2) {
                if (!getFnCall().getChild(2).isConstant()) {
                    throw new AnalysisException("The default parameter (parameter 3) of LEAD/LAG must be a constant: " + getFnCall().toSql());
                }
            }
        }
        if (isNtileFn(fn)) {
            // TODO: IMPALA-2171:Remove this when ntile() can handle a non-constant argument.
            if (!getFnCall().getChild(0).isConstant()) {
                throw new AnalysisException("NTILE() requires a constant argument");
            }
            // Check if argument value is zero or negative and throw an exception if found.
            try {
                TColumnValue bucketValue = FeSupport.EvalConstExpr(getFnCall().getChild(0), analyzer.getQueryCtx());
                Long arg = bucketValue.getLong_val();
                if (arg <= 0) {
                    throw new AnalysisException("NTILE() requires a positive argument: " + arg);
                }
            } catch (InternalException e) {
                throw new AnalysisException(e.toString());
            }
        }
    }
    if (window_ != null) {
        if (orderByElements_.isEmpty()) {
            throw new AnalysisException("Windowing clause requires ORDER BY clause: " + toSql());
        }
        window_.analyze(analyzer);
        if (!orderByElements_.isEmpty() && window_.getType() == AnalyticWindow.Type.RANGE) {
            // check that preceding/following ranges match ordering
            if (window_.getLeftBoundary().getType().isOffset()) {
                checkRangeOffsetBoundaryExpr(window_.getLeftBoundary());
            }
            if (window_.getRightBoundary() != null && window_.getRightBoundary().getType().isOffset()) {
                checkRangeOffsetBoundaryExpr(window_.getRightBoundary());
            }
        }
    }
    // check nesting
    if (TreeNode.contains(getChildren(), AnalyticExpr.class)) {
        throw new AnalysisException("Nesting of analytic expressions is not allowed: " + toSql());
    }
    sqlString_ = toSql();
    standardize(analyzer);
    // unbounded).
    if (window_ != null && isMinMax(fn) && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING) {
        throw new AnalysisException("'" + getFnCall().toSql() + "' is only supported with an " + "UNBOUNDED PRECEDING start bound.");
    }
    setChildren();
}
#end_block

#method_before
private void standardize(Analyzer analyzer) {
    FunctionName analyticFnName = getFnCall().getFnName();
    // 1. Set a window from UNBOUNDED PRECEDING to CURRENT_ROW for row_number().
    if (analyticFnName.getFunction().equals(ROWNUMBER)) {
        Preconditions.checkState(window_ == null, "Unexpected window set for row_numer()");
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), new Boundary(BoundaryType.CURRENT_ROW, null));
        resetWindow_ = true;
        return;
    }
    // Set a window for lag(): UNBOUNDED PRECEDING to OFFSET PRECEDING.
    if (isOffsetFn(getFnCall().getFn())) {
        Preconditions.checkState(window_ == null);
        // If necessary, create a new fn call with the default args explicitly set.
        List<Expr> newExprParams = null;
        if (getFnCall().getChildren().size() == 1) {
            newExprParams = Lists.newArrayListWithExpectedSize(3);
            newExprParams.addAll(getFnCall().getChildren());
            // Default offset is 1.
            newExprParams.add(new NumericLiteral(BigDecimal.valueOf(1)));
            // Default default value is NULL.
            newExprParams.add(new NullLiteral());
        } else if (getFnCall().getChildren().size() == 2) {
            newExprParams = Lists.newArrayListWithExpectedSize(3);
            newExprParams.addAll(getFnCall().getChildren());
            // Default default value is NULL.
            newExprParams.add(new NullLiteral());
        } else {
            Preconditions.checkState(getFnCall().getChildren().size() == 3);
        }
        if (newExprParams != null) {
            fnCall_ = new FunctionCallExpr(getFnCall().getFnName(), new FunctionParams(newExprParams));
            fnCall_.setIsAnalyticFnCall(true);
            fnCall_.analyzeNoThrow(analyzer);
        }
        // Set the window.
        BoundaryType rightBoundaryType = BoundaryType.FOLLOWING;
        if (analyticFnName.getFunction().equals(LAG)) {
            rightBoundaryType = BoundaryType.PRECEDING;
        }
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), new Boundary(rightBoundaryType, getOffsetExpr(getFnCall())));
        try {
            window_.analyze(analyzer);
        } catch (AnalysisException e) {
            throw new IllegalStateException(e);
        }
        resetWindow_ = true;
        return;
    }
    // 3.
    if (analyticFnName.getFunction().equals(FIRST_VALUE) && window_ != null && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING && !getFnCall().getParams().isIgnoreNulls()) {
        if (window_.getLeftBoundary().getType() != BoundaryType.PRECEDING) {
            window_ = new AnalyticWindow(window_.getType(), window_.getLeftBoundary(), window_.getLeftBoundary());
            fnCall_ = new FunctionCallExpr(new FunctionName(LAST_VALUE), getFnCall().getParams());
        } else {
            List<Expr> paramExprs = Expr.cloneList(getFnCall().getParams().exprs());
            if (window_.getRightBoundary().getType() == BoundaryType.PRECEDING) {
                // The number of rows preceding for the end bound determines the number of
                // rows at the beginning of each partition that should have a NULL value.
                paramExprs.add(new NumericLiteral(window_.getRightBoundary().getOffsetValue(), Type.BIGINT));
            } else {
                // -1 indicates that no NULL values are inserted even though we set the end
                // bound to the start bound (which is PRECEDING) below; this is different from
                // the default behavior of windows with an end bound PRECEDING.
                paramExprs.add(new NumericLiteral(BigInteger.valueOf(-1), Type.BIGINT));
            }
            window_ = new AnalyticWindow(window_.getType(), new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), window_.getLeftBoundary());
            fnCall_ = new FunctionCallExpr(new FunctionName(FIRST_VALUE_REWRITE), new FunctionParams(paramExprs));
            fnCall_.setIsInternalFnCall(true);
        }
        fnCall_.setIsAnalyticFnCall(true);
        fnCall_.analyzeNoThrow(analyzer);
        // Use getType() instead if getReturnType() because wildcard decimals
        // have only been resolved in the former.
        type_ = fnCall_.getType();
        analyticFnName = getFnCall().getFnName();
    }
    // first_value(... ignore nulls)
    if (window_ != null && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING && (window_.getRightBoundary().getType() == BoundaryType.UNBOUNDED_FOLLOWING || (analyticFnName.getFunction().equals(FIRST_VALUE) && getFnCall().getParams().isIgnoreNulls()))) {
        orderByElements_ = OrderByElement.reverse(orderByElements_);
        window_ = window_.reverse();
        // Also flip first_value()/last_value(). For other analytic functions there is no
        // need to also change the function.
        FunctionName reversedFnName = null;
        if (analyticFnName.getFunction().equals(FIRST_VALUE)) {
            reversedFnName = new FunctionName(LAST_VALUE);
        } else if (analyticFnName.getFunction().equals(LAST_VALUE)) {
            reversedFnName = new FunctionName(FIRST_VALUE);
        }
        if (reversedFnName != null) {
            fnCall_ = new FunctionCallExpr(reversedFnName, getFnCall().getParams());
            fnCall_.setIsAnalyticFnCall(true);
            fnCall_.analyzeNoThrow(analyzer);
        }
        analyticFnName = getFnCall().getFnName();
    }
    // is UNBOUNDED_PRECEDING and IGNORE NULLS is not set.
    if (analyticFnName.getFunction().equals(FIRST_VALUE) && window_ != null && window_.getLeftBoundary().getType() == BoundaryType.UNBOUNDED_PRECEDING && window_.getRightBoundary().getType() != BoundaryType.PRECEDING && !getFnCall().getParams().isIgnoreNulls()) {
        window_.setRightBoundary(new Boundary(BoundaryType.CURRENT_ROW, null));
    }
    // 6. Set the default window.
    if (!orderByElements_.isEmpty() && window_ == null) {
        window_ = AnalyticWindow.DEFAULT_WINDOW;
        resetWindow_ = true;
    }
    // 7. Change first_value/last_value RANGE windows to ROWS.
    if ((analyticFnName.getFunction().equals(FIRST_VALUE) || analyticFnName.getFunction().equals(LAST_VALUE)) && window_ != null && window_.getType() == AnalyticWindow.Type.RANGE) {
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, window_.getLeftBoundary(), window_.getRightBoundary());
    }
    // 8. Append IGNORE NULLS to fn name if applicable.
    if (getFnCall().getParams().isIgnoreNulls()) {
        String fnName = analyticFnName.getFunction();
        Preconditions.checkState(fnName.equals(FIRST_VALUE) || fnName.equals(LAST_VALUE) || fnName.equals(FIRST_VALUE_IGNORE_NULLS) || fnName.equals(LAST_VALUE_IGNORE_NULLS));
        if (analyticFnName.getFunction().equals(LAST_VALUE)) {
            fnCall_ = new FunctionCallExpr(new FunctionName(LAST_VALUE_IGNORE_NULLS), getFnCall().getParams());
        } else if (analyticFnName.getFunction().equals(FIRST_VALUE)) {
            fnCall_ = new FunctionCallExpr(new FunctionName(FIRST_VALUE_IGNORE_NULLS), getFnCall().getParams());
        }
        fnCall_.setIsAnalyticFnCall(true);
        fnCall_.setIsInternalFnCall(true);
        fnCall_.analyzeNoThrow(analyzer);
        analyticFnName = getFnCall().getFnName();
        Preconditions.checkState(type_.equals(fnCall_.getType()));
    }
}
#method_after
private void standardize(Analyzer analyzer) {
    FunctionName analyticFnName = getFnCall().getFnName();
    // 1. Set a window from UNBOUNDED PRECEDING to CURRENT_ROW for row_number().
    if (analyticFnName.getFunction().equals(ROWNUMBER)) {
        Preconditions.checkState(window_ == null, "Unexpected window set for row_numer()");
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), new Boundary(BoundaryType.CURRENT_ROW, null));
        resetWindow_ = true;
        return;
    }
    // Set a window for lag(): UNBOUNDED PRECEDING to OFFSET PRECEDING.
    if (isOffsetFn(getFnCall().getFn())) {
        Preconditions.checkState(window_ == null);
        // If necessary, create a new fn call with the default args explicitly set.
        List<Expr> newExprParams = null;
        if (getFnCall().getChildren().size() == 1) {
            newExprParams = Lists.newArrayListWithExpectedSize(3);
            newExprParams.addAll(getFnCall().getChildren());
            // Default offset is 1.
            newExprParams.add(new NumericLiteral(BigDecimal.valueOf(1)));
            // Default default value is NULL.
            newExprParams.add(new NullLiteral());
        } else if (getFnCall().getChildren().size() == 2) {
            newExprParams = Lists.newArrayListWithExpectedSize(3);
            newExprParams.addAll(getFnCall().getChildren());
            // Default default value is NULL.
            newExprParams.add(new NullLiteral());
        } else {
            Preconditions.checkState(getFnCall().getChildren().size() == 3);
        }
        if (newExprParams != null) {
            fnCall_ = new FunctionCallExpr(getFnCall().getFnName(), new FunctionParams(newExprParams));
            fnCall_.setIsAnalyticFnCall(true);
            fnCall_.analyzeNoThrow(analyzer);
        }
        // Set the window.
        BoundaryType rightBoundaryType = BoundaryType.FOLLOWING;
        if (analyticFnName.getFunction().equals(LAG)) {
            rightBoundaryType = BoundaryType.PRECEDING;
        }
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), new Boundary(rightBoundaryType, getOffsetExpr(getFnCall())));
        try {
            window_.analyze(analyzer);
        } catch (AnalysisException e) {
            throw new IllegalStateException(e);
        }
        resetWindow_ = true;
        return;
    }
    // 3.
    if (analyticFnName.getFunction().equals(FIRST_VALUE) && window_ != null && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING && !getFnCall().getParams().isIgnoreNulls()) {
        if (window_.getLeftBoundary().getType() != BoundaryType.PRECEDING) {
            window_ = new AnalyticWindow(window_.getType(), window_.getLeftBoundary(), window_.getLeftBoundary());
            fnCall_ = new FunctionCallExpr(new FunctionName(LAST_VALUE), getFnCall().getParams());
        } else {
            List<Expr> paramExprs = Expr.cloneList(getFnCall().getParams().exprs());
            if (window_.getRightBoundary().getType() == BoundaryType.PRECEDING) {
                // The number of rows preceding for the end bound determines the number of
                // rows at the beginning of each partition that should have a NULL value.
                paramExprs.add(new NumericLiteral(window_.getRightBoundary().getOffsetValue(), Type.BIGINT));
            } else {
                // -1 indicates that no NULL values are inserted even though we set the end
                // bound to the start bound (which is PRECEDING) below; this is different from
                // the default behavior of windows with an end bound PRECEDING.
                paramExprs.add(new NumericLiteral(BigInteger.valueOf(-1), Type.BIGINT));
            }
            window_ = new AnalyticWindow(window_.getType(), new Boundary(BoundaryType.UNBOUNDED_PRECEDING, null), window_.getLeftBoundary());
            fnCall_ = new FunctionCallExpr(new FunctionName(FIRST_VALUE_REWRITE), new FunctionParams(paramExprs));
            fnCall_.setIsInternalFnCall(true);
        }
        fnCall_.setIsAnalyticFnCall(true);
        fnCall_.analyzeNoThrow(analyzer);
        // Use getType() instead if getReturnType() because wildcard decimals
        // have only been resolved in the former.
        type_ = fnCall_.getType();
        analyticFnName = getFnCall().getFnName();
    }
    // first_value(... ignore nulls)
    if (window_ != null && window_.getLeftBoundary().getType() != BoundaryType.UNBOUNDED_PRECEDING && (window_.getRightBoundary().getType() == BoundaryType.UNBOUNDED_FOLLOWING || (analyticFnName.getFunction().equals(FIRST_VALUE) && getFnCall().getParams().isIgnoreNulls()))) {
        orderByElements_ = OrderByElement.reverse(orderByElements_);
        window_ = window_.reverse();
        // Also flip first_value()/last_value(). For other analytic functions there is no
        // need to also change the function.
        FunctionName reversedFnName = null;
        if (analyticFnName.getFunction().equals(FIRST_VALUE)) {
            reversedFnName = new FunctionName(LAST_VALUE);
        } else if (analyticFnName.getFunction().equals(LAST_VALUE)) {
            reversedFnName = new FunctionName(FIRST_VALUE);
        }
        if (reversedFnName != null) {
            fnCall_ = new FunctionCallExpr(reversedFnName, getFnCall().getParams());
            fnCall_.setIsAnalyticFnCall(true);
            fnCall_.analyzeNoThrow(analyzer);
        }
        analyticFnName = getFnCall().getFnName();
    }
    // is UNBOUNDED_PRECEDING and IGNORE NULLS is not set.
    if (analyticFnName.getFunction().equals(FIRST_VALUE) && window_ != null && window_.getLeftBoundary().getType() == BoundaryType.UNBOUNDED_PRECEDING && window_.getRightBoundary().getType() != BoundaryType.PRECEDING && !getFnCall().getParams().isIgnoreNulls()) {
        window_.setRightBoundary(new Boundary(BoundaryType.CURRENT_ROW, null));
    }
    // 6. Set the default window.
    if (!orderByElements_.isEmpty() && window_ == null) {
        window_ = AnalyticWindow.DEFAULT_WINDOW;
        resetWindow_ = true;
    }
    // 7. Change first_value/last_value RANGE windows to ROWS.
    if ((analyticFnName.getFunction().equals(FIRST_VALUE) || analyticFnName.getFunction().equals(LAST_VALUE)) && window_ != null && window_.getType() == AnalyticWindow.Type.RANGE) {
        window_ = new AnalyticWindow(AnalyticWindow.Type.ROWS, window_.getLeftBoundary(), window_.getRightBoundary());
    }
    // to allow statement rewriting for subqueries.
    if (getFnCall().getParams().isIgnoreNulls()) {
        if (analyticFnName.getFunction().equals(LAST_VALUE)) {
            fnCall_ = new FunctionCallExpr(new FunctionName(LAST_VALUE_IGNORE_NULLS), getFnCall().getParams());
        } else {
            Preconditions.checkState(analyticFnName.getFunction().equals(FIRST_VALUE));
            fnCall_ = new FunctionCallExpr(new FunctionName(FIRST_VALUE_IGNORE_NULLS), getFnCall().getParams());
        }
        getFnCall().getParams().setIsIgnoreNulls(false);
        fnCall_.setIsAnalyticFnCall(true);
        fnCall_.setIsInternalFnCall(true);
        fnCall_.analyzeNoThrow(analyzer);
        analyticFnName = getFnCall().getFnName();
        Preconditions.checkState(type_.equals(fnCall_.getType()));
    }
}
#end_block

#method_before
@Test
public void TestFunctionCallExpr() throws AnalysisException {
    AnalyzesOk("select pi()");
    AnalyzesOk("select _impala_builtins.pi()");
    AnalyzesOk("select _impala_builtins.decode(1, 2, 3)");
    AnalyzesOk("select _impala_builtins.DECODE(1, 2, 3)");
    AnalyzesOk("select sin(pi())");
    AnalyzesOk("select sin(cos(pi()))");
    AnalyzesOk("select sin(cos(tan(e())))");
    AnalysisError("select pi(*)", "Cannot pass '*' to scalar function.");
    AnalysisError("select sin(DISTINCT 1)", "Cannot pass 'DISTINCT' to scalar function.");
    AnalysisError("select * from functional.alltypes where pi(*) = 5", "Cannot pass '*' to scalar function.");
    // Invalid function name.
    AnalysisError("select a.b.sin()", "Invalid function name: 'a.b.sin'. Expected [dbname].funcname");
    // Call function that only accepts decimal
    AnalyzesOk("select precision(1)");
    AnalyzesOk("select precision(cast('1.1' as decimal))");
    AnalyzesOk("select scale(1.1)");
    AnalysisError("select scale('1.1')", "No matching function with signature: scale(STRING).");
    AnalyzesOk("select round(cast('1.1' as decimal), cast(1 as int))");
    // 1 is a tinyint, so the function is not a perfect match
    AnalyzesOk("select round(cast('1.1' as decimal), 1)");
    // No matching signature for complex type.
    AnalysisError("select lower(int_struct_col) from functional.allcomplextypes", "No matching function with signature: lower(STRUCT<f1:INT,f2:INT>).");
    // Special cases for FROM in function call
    AnalyzesOk("select extract(year from now())");
    AnalysisError("select extract(foo from now())", "Time unit 'foo' in expression 'EXTRACT(foo FROM now())' is invalid. Expected " + "one of YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, MILLISECOND, EPOCH.");
    AnalysisError("select extract(year from 0)", "Expression '0' in 'EXTRACT(year FROM 0)' has a return type of TINYINT but a " + "TIMESTAMP is required.");
    AnalysisError("select functional.extract(year from now())", "Function functional.extract conflicts with the EXTRACT builtin");
    AnalysisError("select date_part(year from now())", "Function DATE_PART does not accept the keyword FROM");
}
#method_after
@Test
public void TestFunctionCallExpr() throws AnalysisException {
    AnalyzesOk("select pi()");
    AnalyzesOk("select _impala_builtins.pi()");
    AnalyzesOk("select _impala_builtins.decode(1, 2, 3)");
    AnalyzesOk("select _impala_builtins.DECODE(1, 2, 3)");
    AnalyzesOk("select sin(pi())");
    AnalyzesOk("select sin(cos(pi()))");
    AnalyzesOk("select sin(cos(tan(e())))");
    AnalysisError("select pi(*)", "Cannot pass '*' to scalar function.");
    AnalysisError("select sin(DISTINCT 1)", "Cannot pass 'DISTINCT' to scalar function.");
    AnalysisError("select * from functional.alltypes where pi(*) = 5", "Cannot pass '*' to scalar function.");
    // Invalid function name.
    AnalysisError("select a.b.sin()", "Invalid function name: 'a.b.sin'. Expected [dbname].funcname");
    // Call function that only accepts decimal
    AnalyzesOk("select precision(1)");
    AnalyzesOk("select precision(cast('1.1' as decimal))");
    AnalyzesOk("select scale(1.1)");
    AnalysisError("select scale('1.1')", "No matching function with signature: scale(STRING).");
    AnalyzesOk("select round(cast('1.1' as decimal), cast(1 as int))");
    // 1 is a tinyint, so the function is not a perfect match
    AnalyzesOk("select round(cast('1.1' as decimal), 1)");
    // No matching signature for complex type.
    AnalysisError("select lower(int_struct_col) from functional.allcomplextypes", "No matching function with signature: lower(STRUCT<f1:INT,f2:INT>).");
    // Special cases for FROM in function call
    AnalyzesOk("select extract(year from now())");
    AnalysisError("select extract(foo from now())", "Time unit 'foo' in expression 'EXTRACT(foo FROM now())' is invalid. Expected " + "one of YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, MILLISECOND, EPOCH.");
    AnalysisError("select extract(year from 0)", "Expression '0' in 'EXTRACT(year FROM 0)' has a return type of TINYINT but a " + "TIMESTAMP is required.");
    AnalysisError("select functional.extract(year from now())", "Function functional.extract conflicts with the EXTRACT builtin");
    AnalysisError("select date_part(year from now())", "Function DATE_PART does not accept the keyword FROM");
    // IGNORE NULLS may only be used with first_value/last_value
    AnalysisError("select lower('FOO' ignore nulls)", "Function LOWER does not accept the keyword IGNORE NULLS.");
}
#end_block

#method_before
Deferred<AsyncKuduScanner.Response> scanNextRows(final AsyncKuduScanner scanner) {
    final RemoteTablet tablet = scanner.currentTablet();
    final TabletClient client = tablet == null ? null : tablet.getLeaderConnection();
    final KuduRpc<AsyncKuduScanner.Response> next_request = scanner.getNextRowsRequest();
    final Deferred<AsyncKuduScanner.Response> d = next_request.getDeferred();
    // Important to increment the attempts before the next if statement since
    // getSleepTimeForRpc() relies on it if the client is null or dead.
    next_request.attempt++;
    if (client == null || !client.isAlive()) {
        // We'll first delay the RPC in case things take some time to settle down, then retry.
        return delayedSendRpcToTablet(next_request, null);
    }
    client.sendRpc(next_request);
    return d;
}
#method_after
Deferred<AsyncKuduScanner.Response> scanNextRows(final AsyncKuduScanner scanner) {
    final RemoteTablet tablet = scanner.currentTablet();
    assert (tablet != null);
    final TabletClient client = tablet.getLeaderConnection();
    final KuduRpc<AsyncKuduScanner.Response> next_request = scanner.getNextRowsRequest();
    final Deferred<AsyncKuduScanner.Response> d = next_request.getDeferred();
    // Important to increment the attempts before the next if statement since
    // getSleepTimeForRpc() relies on it if the client is null or dead.
    next_request.attempt++;
    if (client == null || !client.isAlive()) {
        // We'll first delay the RPC in case things take some time to settle down, then retry.
        return delayedSendRpcToTablet(next_request, null);
    }
    client.sendRpc(next_request);
    return d;
}
#end_block

#method_before
<R> Deferred<R> sendRpcToTablet(final KuduRpc<R> request) {
    if (cannotRetryRequest(request)) {
        return tooManyAttemptsOrTimeout(request, null);
    }
    request.attempt++;
    final String tableId = request.getTable().getTableId();
    byte[] partitionKey = request.partitionKey();
    TableLocationsCache.Entry entry = getTableLocationEntry(tableId, partitionKey);
    if (entry != null && entry.isNonCoveredRange()) {
        Exception e = new NonCoveredRangeException(entry.getLowerBoundPartitionKey(), entry.getUpperBoundPartitionKey());
        // Sending both as an errback and returning fromError because sendRpcToTablet might be
        // called via a callback that won't care about the returned Deferred.
        request.errback(e);
        return Deferred.fromError(e);
    }
    // Set the propagated timestamp so that the next time we send a message to
    // the server the message includes the last propagated timestamp.
    long lastPropagatedTs = getLastPropagatedTimestamp();
    if (request.getExternalConsistencyMode() == CLIENT_PROPAGATED && lastPropagatedTs != NO_TIMESTAMP) {
        request.setPropagatedTimestamp(lastPropagatedTs);
    }
    // block that queries the master.
    if (entry != null) {
        RemoteTablet tablet = entry.getTablet();
        TabletClient tabletClient = tablet.getLeaderConnection();
        if (tabletClient != null) {
            final Deferred<R> d = request.getDeferred();
            if (tabletClient.isAlive()) {
                request.setTablet(tablet);
                tabletClient.sendRpc(request);
                return d;
            }
            try {
                tablet.reconnectTabletClient(tabletClient);
            } catch (UnknownHostException e) {
                LOG.error("Cached tablet server {}'s host cannot be resolved, will query the master", tabletClient.getUuid(), e);
            // Because of this exception, getConnectionToLeader() below won't be able to find a newTabletClient
            // and we'll delay the RPC.
            }
            TabletClient newTabletClient = tablet.getLeaderConnection();
            assert (tabletClient != newTabletClient);
            if (newTabletClient == null) {
                // Wait a little bit before hitting the master.
                return delayedSendRpcToTablet(request, null);
            }
            if (!newTabletClient.isAlive()) {
                LOG.debug("Tried reconnecting to tablet server {} but failed, " + "will query the master", tabletClient.getUuid());
            // Let fall through.
            } else {
                request.setTablet(tablet);
                newTabletClient.sendRpc(request);
                return d;
            }
        }
    }
    // leader replica.
    if (tablesNotServed.contains(tableId)) {
        return delayedIsCreateTableDone(request.getTable(), request, new RetryRpcCB<R, Master.IsCreateTableDoneResponsePB>(request), getDelayedIsCreateTableDoneErrback(request));
    }
    Callback<Deferred<R>, Master.GetTableLocationsResponsePB> cb = new RetryRpcCB<>(request);
    Callback<Deferred<R>, Exception> eb = new RetryRpcErrback<>(request);
    Deferred<Master.GetTableLocationsResponsePB> returnedD = locateTablet(request.getTable(), partitionKey);
    return AsyncUtil.addCallbacksDeferring(returnedD, cb, eb);
}
#method_after
<R> Deferred<R> sendRpcToTablet(final KuduRpc<R> request) {
    if (cannotRetryRequest(request)) {
        return tooManyAttemptsOrTimeout(request, null);
    }
    request.attempt++;
    final String tableId = request.getTable().getTableId();
    byte[] partitionKey = request.partitionKey();
    TableLocationsCache.Entry entry = getTableLocationEntry(tableId, partitionKey);
    if (entry != null && entry.isNonCoveredRange()) {
        Exception e = new NonCoveredRangeException(entry.getLowerBoundPartitionKey(), entry.getUpperBoundPartitionKey());
        // Sending both as an errback and returning fromError because sendRpcToTablet might be
        // called via a callback that won't care about the returned Deferred.
        request.errback(e);
        return Deferred.fromError(e);
    }
    // Set the propagated timestamp so that the next time we send a message to
    // the server the message includes the last propagated timestamp.
    long lastPropagatedTs = getLastPropagatedTimestamp();
    if (request.getExternalConsistencyMode() == CLIENT_PROPAGATED && lastPropagatedTs != NO_TIMESTAMP) {
        request.setPropagatedTimestamp(lastPropagatedTs);
    }
    // block that queries the master.
    if (entry != null) {
        RemoteTablet tablet = entry.getTablet();
        TabletClient tabletClient = tablet.getLeaderConnection();
        if (tabletClient != null) {
            final Deferred<R> d = request.getDeferred();
            if (tabletClient.isAlive()) {
                request.setTablet(tablet);
                tabletClient.sendRpc(request);
                return d;
            }
            try {
                tablet.reconnectTabletClient(tabletClient);
            } catch (UnknownHostException e) {
                LOG.error("Cached tablet server {}'s host cannot be resolved, will query the master", tabletClient.getUuid(), e);
            // Because of this exception, getLeaderConnection() below won't be able to find a newTabletClient
            // and we'll delay the RPC.
            }
            TabletClient newTabletClient = tablet.getLeaderConnection();
            assert (tabletClient != newTabletClient);
            if (newTabletClient == null) {
                // Wait a little bit before hitting the master.
                return delayedSendRpcToTablet(request, null);
            }
            if (!newTabletClient.isAlive()) {
                LOG.debug("Tried reconnecting to tablet server {} but failed, " + "will query the master", tabletClient.getUuid());
            // Let fall through.
            } else {
                request.setTablet(tablet);
                newTabletClient.sendRpc(request);
                return d;
            }
        }
    }
    // leader replica.
    if (tablesNotServed.contains(tableId)) {
        return delayedIsCreateTableDone(request.getTable(), request, new RetryRpcCB<R, Master.IsCreateTableDoneResponsePB>(request), getDelayedIsCreateTableDoneErrback(request));
    }
    Callback<Deferred<R>, Master.GetTableLocationsResponsePB> cb = new RetryRpcCB<>(request);
    Callback<Deferred<R>, Exception> eb = new RetryRpcErrback<>(request);
    Deferred<Master.GetTableLocationsResponsePB> returnedD = locateTablet(request.getTable(), partitionKey);
    return AsyncUtil.addCallbacksDeferring(returnedD, cb, eb);
}
#end_block

#method_before
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws KuduException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        RemoteTablet rt = createTabletFromPb(tableId, tabletPb);
        Slice tabletId = rt.getTabletId();
        LOG.info("Learned about tablet {} for table '{}' with partition {}", tabletId.toString(Charset.defaultCharset()), tableName, rt.getPartition());
        rt.init(tabletPb);
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
    // Now test if we found the tablet we were looking for. If so, RetryRpcCB will retry the RPC
    // right away. If not, we throw an exception that RetryRpcErrback will understand as needing to
    // sleep before retrying.
    TableLocationsCache.Entry entry = locationsCache.get(requestPartitionKey);
    if (!entry.isNonCoveredRange() && entry.getTablet().getLeaderConnection() == null) {
        throw new NoLeaderFoundException(Status.NotFound("Tablet " + entry.toString() + " doesn't have a leader"));
    }
}
#method_after
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws KuduException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        RemoteTablet rt = RemoteTablet.createTabletFromPb(tableId, tabletPb, connectionCache);
        LOG.info("Learned about tablet {} for table '{}' with partition {}", rt.getTabletIdAsString(), tableName, rt.getPartition());
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
    // Now test if we found the tablet we were looking for. If so, RetryRpcCB will retry the RPC
    // right away. If not, we throw an exception that RetryRpcErrback will understand as needing to
    // sleep before retrying.
    TableLocationsCache.Entry entry = locationsCache.get(requestPartitionKey);
    if (!entry.isNonCoveredRange() && entry.getTablet().getLeaderConnection() == null) {
        throw new NoLeaderFoundException(Status.NotFound("Tablet " + entry.toString() + " doesn't have a leader"));
    }
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    checkForSupportedFileFormats();
    assignCollectionConjuncts(analyzer);
    computeMemLayout(analyzer);
    // compute scan range locations
    computeScanRangeLocations(analyzer);
    // do this at the end so it can take all conjuncts and scan ranges into account
    computeStats(analyzer);
    // TODO: do we need this?
    assignedConjuncts_ = analyzer.getAssignedConjuncts();
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    checkForSupportedFileFormats();
    assignCollectionConjuncts(analyzer);
    computeMemLayout(analyzer);
    // compute scan range locations
    Set<HdfsFileFormat> fileFormats = computeScanRangeLocations(analyzer);
    // is currently only supported for Parquet.
    if (analyzer.getQueryOptions().mt_dop > 0 && fileFormats.size() == 1 && fileFormats.contains(HdfsFileFormat.PARQUET)) {
        useMtScanNode_ = true;
    } else {
        useMtScanNode_ = false;
    }
    // do this at the end so it can take all conjuncts and scan ranges into account
    computeStats(analyzer);
    // TODO: do we need this?
    assignedConjuncts_ = analyzer.getAssignedConjuncts();
}
#end_block

#method_before
private void computeScanRangeLocations(Analyzer analyzer) {
    long maxScanRangeLength = analyzer.getQueryCtx().getRequest().getQuery_options().getMax_scan_range_length();
    scanRanges_ = Lists.newArrayList();
    for (HdfsPartition partition : partitions_) {
        Preconditions.checkState(partition.getId() >= 0);
        for (HdfsPartition.FileDescriptor fileDesc : partition.getFileDescriptors()) {
            for (THdfsFileBlock thriftBlock : fileDesc.getFileBlocks()) {
                HdfsPartition.FileBlock block = FileBlock.fromThrift(thriftBlock);
                List<Integer> replicaHostIdxs = block.getReplicaHostIdxs();
                if (replicaHostIdxs.size() == 0) {
                    // TODO: do something meaningful with that
                    continue;
                }
                // Collect the network address and volume ID of all replicas of this block.
                List<TScanRangeLocation> locations = Lists.newArrayList();
                for (int i = 0; i < replicaHostIdxs.size(); ++i) {
                    TScanRangeLocation location = new TScanRangeLocation();
                    // Translate from the host index (local to the HdfsTable) to network address.
                    Integer tableHostIdx = replicaHostIdxs.get(i);
                    TNetworkAddress networkAddress = partition.getTable().getHostIndex().getEntry(tableHostIdx);
                    Preconditions.checkNotNull(networkAddress);
                    // Translate from network address to the global (to this request) host index.
                    Integer globalHostIdx = analyzer.getHostIndex().getIndex(networkAddress);
                    location.setHost_idx(globalHostIdx);
                    location.setVolume_id(block.getDiskId(i));
                    location.setIs_cached(block.isCached(i));
                    locations.add(location);
                }
                // create scan ranges, taking into account maxScanRangeLength
                long currentOffset = block.getOffset();
                long remainingLength = block.getLength();
                while (remainingLength > 0) {
                    long currentLength = remainingLength;
                    if (maxScanRangeLength > 0 && remainingLength > maxScanRangeLength) {
                        currentLength = maxScanRangeLength;
                    }
                    TScanRange scanRange = new TScanRange();
                    scanRange.setHdfs_file_split(new THdfsFileSplit(fileDesc.getFileName(), currentOffset, currentLength, partition.getId(), fileDesc.getFileLength(), fileDesc.getFileCompression(), fileDesc.getModificationTime()));
                    TScanRangeLocations scanRangeLocations = new TScanRangeLocations();
                    scanRangeLocations.scan_range = scanRange;
                    scanRangeLocations.locations = locations;
                    scanRanges_.add(scanRangeLocations);
                    remainingLength -= currentLength;
                    currentOffset += currentLength;
                }
            }
        }
    }
}
#method_after
private Set<HdfsFileFormat> computeScanRangeLocations(Analyzer analyzer) {
    long maxScanRangeLength = analyzer.getQueryCtx().getRequest().getQuery_options().getMax_scan_range_length();
    scanRanges_ = Lists.newArrayList();
    Set<HdfsFileFormat> fileFormats = Sets.newHashSet();
    for (HdfsPartition partition : partitions_) {
        fileFormats.add(partition.getFileFormat());
        Preconditions.checkState(partition.getId() >= 0);
        for (HdfsPartition.FileDescriptor fileDesc : partition.getFileDescriptors()) {
            for (THdfsFileBlock thriftBlock : fileDesc.getFileBlocks()) {
                HdfsPartition.FileBlock block = FileBlock.fromThrift(thriftBlock);
                List<Integer> replicaHostIdxs = block.getReplicaHostIdxs();
                if (replicaHostIdxs.size() == 0) {
                    // TODO: do something meaningful with that
                    continue;
                }
                // Collect the network address and volume ID of all replicas of this block.
                List<TScanRangeLocation> locations = Lists.newArrayList();
                for (int i = 0; i < replicaHostIdxs.size(); ++i) {
                    TScanRangeLocation location = new TScanRangeLocation();
                    // Translate from the host index (local to the HdfsTable) to network address.
                    Integer tableHostIdx = replicaHostIdxs.get(i);
                    TNetworkAddress networkAddress = partition.getTable().getHostIndex().getEntry(tableHostIdx);
                    Preconditions.checkNotNull(networkAddress);
                    // Translate from network address to the global (to this request) host index.
                    Integer globalHostIdx = analyzer.getHostIndex().getIndex(networkAddress);
                    location.setHost_idx(globalHostIdx);
                    location.setVolume_id(block.getDiskId(i));
                    location.setIs_cached(block.isCached(i));
                    locations.add(location);
                }
                // create scan ranges, taking into account maxScanRangeLength
                long currentOffset = block.getOffset();
                long remainingLength = block.getLength();
                while (remainingLength > 0) {
                    long currentLength = remainingLength;
                    if (maxScanRangeLength > 0 && remainingLength > maxScanRangeLength) {
                        currentLength = maxScanRangeLength;
                    }
                    TScanRange scanRange = new TScanRange();
                    scanRange.setHdfs_file_split(new THdfsFileSplit(fileDesc.getFileName(), currentOffset, currentLength, partition.getId(), fileDesc.getFileLength(), fileDesc.getFileCompression(), fileDesc.getModificationTime()));
                    TScanRangeLocations scanRangeLocations = new TScanRangeLocations();
                    scanRangeLocations.scan_range = scanRange;
                    scanRangeLocations.locations = locations;
                    scanRanges_.add(scanRangeLocations);
                    remainingLength -= currentLength;
                    currentOffset += currentLength;
                }
            }
        }
    }
    return fileFormats;
}
#end_block

#method_before
@Override
protected void toThrift(TPlanNode msg) {
    msg.hdfs_scan_node = new THdfsScanNode(desc_.getId().asInt());
    if (replicaPreference_ != null) {
        msg.hdfs_scan_node.setReplica_preference(replicaPreference_);
    }
    msg.hdfs_scan_node.setRandom_replica(randomReplica_);
    msg.node_type = TPlanNodeType.HDFS_SCAN_NODE;
    if (!collectionConjuncts_.isEmpty()) {
        Map<Integer, List<TExpr>> tcollectionConjuncts = Maps.newLinkedHashMap();
        for (Map.Entry<TupleDescriptor, List<Expr>> entry : collectionConjuncts_.entrySet()) {
            tcollectionConjuncts.put(entry.getKey().getId().asInt(), Expr.treesToThrift(entry.getValue()));
        }
        msg.hdfs_scan_node.setCollection_conjuncts(tcollectionConjuncts);
    }
    if (skipHeaderLineCount_ > 0) {
        msg.hdfs_scan_node.setSkip_header_line_count(skipHeaderLineCount_);
    }
}
#method_after
@Override
protected void toThrift(TPlanNode msg) {
    msg.hdfs_scan_node = new THdfsScanNode(desc_.getId().asInt());
    if (replicaPreference_ != null) {
        msg.hdfs_scan_node.setReplica_preference(replicaPreference_);
    }
    msg.hdfs_scan_node.setRandom_replica(randomReplica_);
    msg.node_type = TPlanNodeType.HDFS_SCAN_NODE;
    if (!collectionConjuncts_.isEmpty()) {
        Map<Integer, List<TExpr>> tcollectionConjuncts = Maps.newLinkedHashMap();
        for (Map.Entry<TupleDescriptor, List<Expr>> entry : collectionConjuncts_.entrySet()) {
            tcollectionConjuncts.put(entry.getKey().getId().asInt(), Expr.treesToThrift(entry.getValue()));
        }
        msg.hdfs_scan_node.setCollection_conjuncts(tcollectionConjuncts);
    }
    if (skipHeaderLineCount_ > 0) {
        msg.hdfs_scan_node.setSkip_header_line_count(skipHeaderLineCount_);
    }
    msg.hdfs_scan_node.setUse_mt_scan_node(useMtScanNode_);
}
#end_block

#method_before
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        singleNodePlanner.validatePlan(singleNodePlan, ctx_.getQueryOptions());
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (RuntimeEnv.INSTANCE.computeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#method_after
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    singleNodePlanner.validatePlan(singleNodePlan);
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        } else if (ctx_.isQuery()) {
            rootFragment.setSink(ctx_.getAnalysisResult().getQueryStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (RuntimeEnv.INSTANCE.computeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#end_block

#method_before
public List<PlanFragment> createParallelPlans() throws ImpalaException {
    ArrayList<PlanFragment> distrPlan = createPlan();
    Preconditions.checkNotNull(distrPlan);
    ParallelPlanner planner = new ParallelPlanner(ctx_);
    List<PlanFragment> parallelPlans = planner.createPlans(distrPlan.get(0));
    ctx_.getRootAnalyzer().getTimeline().markEvent("Parallel plans created");
    return parallelPlans;
}
#method_after
public List<PlanFragment> createParallelPlans() throws ImpalaException {
    Preconditions.checkState(ctx_.getQueryOptions().mt_dop > 0);
    ArrayList<PlanFragment> distrPlan = createPlan();
    Preconditions.checkNotNull(distrPlan);
    ParallelPlanner planner = new ParallelPlanner(ctx_);
    List<PlanFragment> parallelPlans = planner.createPlans(distrPlan.get(0));
    // Only use one scanner thread per scan-node instance since intra-node
    // parallelism is achieved via multiple fragment instances.
    ctx_.getQueryOptions().setNum_scanner_threads(1);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Parallel plans created");
    return parallelPlans;
}
#end_block

#method_before
public void validatePlan(PlanNode planNode, TQueryOptions queryOpts) throws NotImplementedException {
    if (planNode instanceof JoinNode && queryOpts.mt_dop > 0 && !RuntimeEnv.INSTANCE.isTestEnv()) {
        throw new NotImplementedException("MT_DOP not supported for plans with joins.");
    }
    if (planNode instanceof NestedLoopJoinNode) {
        JoinNode joinNode = (JoinNode) planNode;
        JoinOperator joinOp = joinNode.getJoinOp();
        if ((joinOp.isRightSemiJoin() || joinOp.isFullOuterJoin() || joinOp == JoinOperator.RIGHT_OUTER_JOIN) && joinNode.getEqJoinConjuncts().isEmpty()) {
            throw new NotImplementedException(String.format("Error generating a valid " + "execution plan for this query. A %s type with no equi-join " + "predicates can only be executed with a single node plan.", joinOp.toString()));
        }
    }
    if (planNode instanceof SubplanNode) {
        // Right and full outer joins with no equi-join conjuncts are ok in the right
        // child of a SubplanNode.
        validatePlan(planNode.getChild(0), queryOpts);
    } else {
        for (PlanNode child : planNode.getChildren()) {
            validatePlan(child, queryOpts);
        }
    }
}
#method_after
public void validatePlan(PlanNode planNode) throws NotImplementedException {
    if (ctx_.getQueryOptions().mt_dop > 0 && !RuntimeEnv.INSTANCE.isTestEnv() && (planNode instanceof JoinNode || ctx_.hasTableSink())) {
        throw new NotImplementedException("MT_DOP not supported for plans with base table joins or table sinks.");
    }
    // As long as MT_DOP == 0 any join can run in a single-node plan.
    if (ctx_.isSingleNodeExec() && ctx_.getQueryOptions().mt_dop == 0)
        return;
    if (planNode instanceof NestedLoopJoinNode) {
        JoinNode joinNode = (JoinNode) planNode;
        JoinOperator joinOp = joinNode.getJoinOp();
        if ((joinOp.isRightSemiJoin() || joinOp.isFullOuterJoin() || joinOp == JoinOperator.RIGHT_OUTER_JOIN) && joinNode.getEqJoinConjuncts().isEmpty()) {
            throw new NotImplementedException(String.format("Error generating a valid " + "execution plan for this query. A %s type with no equi-join " + "predicates can only be executed with a single node plan.", joinOp.toString()));
        }
    }
    if (planNode instanceof SubplanNode) {
        // Right and full outer joins with no equi-join conjuncts are ok in the right
        // child of a SubplanNode.
        validatePlan(planNode.getChild(0));
    } else {
        for (PlanNode child : planNode.getChildren()) {
            validatePlan(child);
        }
    }
}
#end_block

#method_before
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws KuduException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        // Early creating the tablet so that it parses out the pb.
        RemoteTablet rt = createTabletFromPb(tableId, tabletPb);
        Slice tabletId = rt.tabletId;
        LOG.info("Discovered tablet {} for table '{}' with partition {}", tabletId.toString(Charset.defaultCharset()), tableName, rt.getPartition());
        rt.refreshTabletClients(tabletPb);
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
    // Now test if we found the tablet we were looking for. If so, RetryRpcCB will retry the RPC
    // right away. If not, we throw an exception that RetryRpcErrback will understand as needing to
    // sleep before retrying.
    TableLocationsCache.Entry entry = locationsCache.get(requestPartitionKey);
    if (!entry.isNonCoveredRange() && clientFor(entry.getTablet()) == null) {
        throw new NoLeaderFoundException(Status.NotFound("Tablet " + entry.toString() + " doesn't have a leader"));
    }
}
#method_after
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws KuduException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        RemoteTablet rt = createTabletFromPb(tableId, tabletPb);
        Slice tabletId = rt.tabletId;
        LOG.info("Learned about tablet {} for table '{}' with partition {}", tabletId.toString(Charset.defaultCharset()), tableName, rt.getPartition());
        rt.refreshTabletClients(tabletPb);
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
    // Now test if we found the tablet we were looking for. If so, RetryRpcCB will retry the RPC
    // right away. If not, we throw an exception that RetryRpcErrback will understand as needing to
    // sleep before retrying.
    TableLocationsCache.Entry entry = locationsCache.get(requestPartitionKey);
    if (!entry.isNonCoveredRange() && clientFor(entry.getTablet()) == null) {
        throw new NoLeaderFoundException(Status.NotFound("Tablet " + entry.toString() + " doesn't have a leader"));
    }
}
#end_block

#method_before
void addTabletClient(String uuid, String host, int port, boolean isLeader) throws UnknownHostException {
    String ip = getIP(host);
    if (ip == null) {
        throw new UnknownHostException("Failed to resolve the IP of `" + host + "'");
    }
    TabletClient client = newClient(uuid, ip, port);
    tabletServers.add(client);
    if (isLeader) {
        leaderIndex = tabletServers.size() - 1;
    }
}
#method_after
@GuardedBy("tabletServers")
private void addTabletClient(String uuid, String host, int port, boolean isLeader) throws UnknownHostException {
    String ip = getIP(host);
    if (ip == null) {
        throw new UnknownHostException("Failed to resolve the IP of `" + host + "'");
    }
    TabletClient client = newClient(uuid, ip, port);
    tabletServers.add(client);
    if (isLeader) {
        leaderIndex = tabletServers.size() - 1;
    }
}
#end_block

#method_before
public void shutdown() {
    try {
        for (Iterator<Process> masterIter = masterProcesses.values().iterator(); masterIter.hasNext(); ) {
            destroyAndWaitForProcess(masterIter.next());
            masterIter.remove();
        }
        for (Iterator<Process> tsIter = tserverProcesses.values().iterator(); tsIter.hasNext(); ) {
            destroyAndWaitForProcess(tsIter.next());
            tsIter.remove();
        }
    } catch (InterruptedException ex) {
    // We still need to finish cleaning up.
    }
    for (Thread thread : PROCESS_INPUT_PRINTERS) {
        thread.interrupt();
    }
    for (String path : pathsToDelete) {
        try {
            File f = new File(path);
            if (f.isDirectory()) {
                FileUtils.deleteDirectory(f);
            } else {
                f.delete();
            }
        } catch (Exception e) {
            LOG.warn("Could not delete path {}", path, e);
        }
    }
}
#method_after
public void shutdown() {
    for (Iterator<Process> masterIter = masterProcesses.values().iterator(); masterIter.hasNext(); ) {
        try {
            destroyAndWaitForProcess(masterIter.next());
        } catch (InterruptedException e) {
        // Need to continue cleaning up.
        }
        masterIter.remove();
    }
    for (Iterator<Process> tsIter = tserverProcesses.values().iterator(); tsIter.hasNext(); ) {
        try {
            destroyAndWaitForProcess(tsIter.next());
        } catch (InterruptedException e) {
        // Need to continue cleaning up.
        }
        tsIter.remove();
    }
    // printers will hit EOFs and stop.
    for (Thread thread : PROCESS_INPUT_PRINTERS) {
        try {
            thread.join();
        } catch (InterruptedException e) {
        // Need to continue cleaning up.
        }
    }
    for (String path : pathsToDelete) {
        try {
            File f = new File(path);
            if (f.isDirectory()) {
                FileUtils.deleteDirectory(f);
            } else {
                f.delete();
            }
        } catch (Exception e) {
            LOG.warn("Could not delete path {}", path, e);
        }
    }
}
#end_block

#method_before
private boolean fullScan() {
    KuduScanner scanner = getScannerBuilder().build();
    try {
        int rowCount = countRowsInScan(scanner);
        if (rowCount < lastRowCount) {
            // TODO uncomment the line below when KUDU-798 is solved and remove the WARN.
            // reportError("Row count regressed: " + rowCount + " < " + lastRowCount, null);
            LOG.warn("Row count regressed: " + rowCount + " < " + lastRowCount);
            return false;
        }
        if (rowCount > lastRowCount) {
            lastRowCount = rowCount;
            LOG.info("New row count {}", lastRowCount);
        }
    } catch (KuduException e) {
        return checkAndReportError("Got error while row counting", e);
    }
    return true;
}
#method_after
private boolean fullScan() {
    int rowCount;
    DeadlineTracker deadlineTracker = new DeadlineTracker();
    deadlineTracker.setDeadline(DEFAULT_SLEEP);
    while (KEEP_RUNNING_LATCH.getCount() > 0 && !deadlineTracker.timedOut()) {
        KuduScanner scanner = getScannerBuilder().build();
        try {
            rowCount = countRowsInScan(scanner);
        } catch (KuduException e) {
            return checkAndReportError("Got error while row counting", e);
        }
        if (rowCount >= lastRowCount) {
            if (rowCount > lastRowCount) {
                lastRowCount = rowCount;
                LOG.info("New row count {}", lastRowCount);
            }
            return true;
        }
        // Due to the lack of KUDU-430, we need to loop until the row count stops regressing.
        try {
            KEEP_RUNNING_LATCH.await(50, TimeUnit.MILLISECONDS);
        } catch (InterruptedException e) {
        // No need to do anything, we'll exit the loop once we test getCount() in the condition.
        }
    }
    return !deadlineTracker.timedOut();
}
#end_block

#method_before
public List<BinaryPredicate> getEqJoinConjuncts() {
    return eqJoinConjuncts_;
}
#method_after
@Override
public List<BinaryPredicate> getEqJoinConjuncts() {
    return eqJoinConjuncts_;
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    super.init(analyzer);
    List<BinaryPredicate> newEqJoinConjuncts = Lists.newArrayList();
    ExprSubstitutionMap combinedChildSmap = getCombinedChildSmap();
    for (Expr c : eqJoinConjuncts_) {
        BinaryPredicate eqPred = (BinaryPredicate) c.substitute(combinedChildSmap, analyzer, false);
        Type t0 = eqPred.getChild(0).getType();
        Type t1 = eqPred.getChild(1).getType();
        if (!t0.matchesType(t1)) {
            // With decimal and char types, the child types do not have to match because
            // the equality builtin handles it. However, they will not hash correctly so
            // insert a cast.
            boolean bothDecimal = t0.isDecimal() && t1.isDecimal();
            boolean bothString = t0.isStringType() && t1.isStringType();
            if (!bothDecimal && !bothString) {
                throw new InternalException("Cannot compare " + t0.toSql() + " to " + t1.toSql() + " in join predicate.");
            }
            Type compatibleType = Type.getAssignmentCompatibleType(t0, t1, false);
            Preconditions.checkState(compatibleType.isDecimal() || compatibleType.isStringType());
            try {
                if (!t0.equals(compatibleType)) {
                    eqPred.setChild(0, eqPred.getChild(0).castTo(compatibleType));
                }
                if (!t1.equals(compatibleType)) {
                    eqPred.setChild(1, eqPred.getChild(1).castTo(compatibleType));
                }
            } catch (AnalysisException e) {
                throw new InternalException("Should not happen", e);
            }
        }
        Preconditions.checkState(eqPred.getChild(0).getType().matchesType(eqPred.getChild(1).getType()));
        BinaryPredicate newEqPred = new BinaryPredicate(eqPred.getOp(), eqPred.getChild(0), eqPred.getChild(1));
        newEqPred.analyze(analyzer);
        newEqJoinConjuncts.add(newEqPred);
    }
    eqJoinConjuncts_ = newEqJoinConjuncts;
    orderJoinConjunctsByCost();
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    super.init(analyzer);
    List<BinaryPredicate> newEqJoinConjuncts = Lists.newArrayList();
    ExprSubstitutionMap combinedChildSmap = getCombinedChildSmap();
    for (Expr c : eqJoinConjuncts_) {
        BinaryPredicate eqPred = (BinaryPredicate) c.substitute(combinedChildSmap, analyzer, false);
        Type t0 = eqPred.getChild(0).getType();
        Type t1 = eqPred.getChild(1).getType();
        if (!t0.matchesType(t1)) {
            // With decimal and char types, the child types do not have to match because
            // the equality builtin handles it. However, they will not hash correctly so
            // insert a cast.
            boolean bothDecimal = t0.isDecimal() && t1.isDecimal();
            boolean bothString = t0.isStringType() && t1.isStringType();
            if (!bothDecimal && !bothString) {
                throw new InternalException("Cannot compare " + t0.toSql() + " to " + t1.toSql() + " in join predicate.");
            }
            Type compatibleType = Type.getAssignmentCompatibleType(t0, t1, false);
            Preconditions.checkState(compatibleType.isDecimal() || compatibleType.isStringType());
            try {
                if (!t0.equals(compatibleType)) {
                    eqPred.setChild(0, eqPred.getChild(0).castTo(compatibleType));
                }
                if (!t1.equals(compatibleType)) {
                    eqPred.setChild(1, eqPred.getChild(1).castTo(compatibleType));
                }
            } catch (AnalysisException e) {
                throw new InternalException("Should not happen", e);
            }
        }
        Preconditions.checkState(eqPred.getChild(0).getType().matchesType(eqPred.getChild(1).getType()));
        BinaryPredicate newEqPred = new BinaryPredicate(eqPred.getOp(), eqPred.getChild(0), eqPred.getChild(1));
        newEqPred.analyze(analyzer);
        newEqJoinConjuncts.add(newEqPred);
    }
    eqJoinConjuncts_ = newEqJoinConjuncts;
    orderJoinConjunctsByCost();
    computeStats(analyzer);
}
#end_block

#method_before
@BeforeClass
public static void setUp() throws Exception {
    // Use 8 cores for resource estimation.
    RuntimeEnv.INSTANCE.setNumCores(8);
    // Set test env to control the explain level.
    RuntimeEnv.INSTANCE.setTestEnv(true);
    // Mimic the 3 node test mini-cluster.
    TUpdateMembershipRequest updateReq = new TUpdateMembershipRequest();
    updateReq.setIp_addresses(Sets.newHashSet("127.0.0.1"));
    updateReq.setHostnames(Sets.newHashSet("localhost"));
    updateReq.setNum_nodes(3);
    MembershipSnapshot.update(updateReq);
}
#method_after
@BeforeClass
public static void setUp() throws Exception {
    // Use 8 cores for resource estimation.
    RuntimeEnv.INSTANCE.setNumCores(8);
    // Set test env to control the explain level.
    RuntimeEnv.INSTANCE.setTestEnv(true);
    // Mimic the 3 node test mini-cluster.
    TUpdateMembershipRequest updateReq = new TUpdateMembershipRequest();
    updateReq.setIp_addresses(Sets.newHashSet("127.0.0.1"));
    updateReq.setHostnames(Sets.newHashSet("localhost"));
    updateReq.setNum_nodes(3);
    MembershipSnapshot.update(updateReq);
    if (RuntimeEnv.INSTANCE.isKuduSupported()) {
        kuduClient_ = new KuduClient.KuduClientBuilder("127.0.0.1:7051").build();
    }
}
#end_block

#method_before
@AfterClass
public static void cleanUp() {
    RuntimeEnv.INSTANCE.reset();
}
#method_after
@AfterClass
public static void cleanUp() throws Exception {
    RuntimeEnv.INSTANCE.reset();
    if (kuduClient_ != null) {
        kuduClient_.close();
        kuduClient_ = null;
    }
}
#end_block

#method_before
private StringBuilder printScanRangeLocations(TQueryExecRequest execRequest) {
    StringBuilder result = new StringBuilder();
    if (execRequest.per_node_scan_ranges == null) {
        return result;
    }
    for (Map.Entry<Integer, List<TScanRangeLocations>> entry : execRequest.per_node_scan_ranges.entrySet()) {
        result.append("NODE " + entry.getKey().toString() + ":\n");
        if (entry.getValue() == null) {
            continue;
        }
        for (TScanRangeLocations locations : entry.getValue()) {
            // print scan range
            result.append("  ");
            if (locations.scan_range.isSetHdfs_file_split()) {
                THdfsFileSplit split = locations.scan_range.getHdfs_file_split();
                THdfsTable table = findTable(entry.getKey());
                THdfsPartition partition = table.getPartitions().get(split.partition_id);
                THdfsPartitionLocation location = partition.getLocation();
                String file_location = location.getSuffix();
                if (location.prefix_index != -1) {
                    file_location = table.getPartition_prefixes().get(location.prefix_index) + file_location;
                }
                Path filePath = new Path(file_location, split.file_name);
                filePath = cleanseFilePath(filePath);
                result.append("HDFS SPLIT " + filePath.toString() + " " + Long.toString(split.offset) + ":" + Long.toString(split.length));
            }
            if (locations.scan_range.isSetHbase_key_range()) {
                THBaseKeyRange keyRange = locations.scan_range.getHbase_key_range();
                Integer hostIdx = locations.locations.get(0).host_idx;
                TNetworkAddress networkAddress = execRequest.getHost_list().get(hostIdx);
                result.append("HBASE KEYRANGE ");
                result.append("port=" + networkAddress.port + " ");
                if (keyRange.isSetStartKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStartKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
                result.append(":");
                if (keyRange.isSetStopKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStopKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
            }
            if (locations.scan_range.isSetKudu_key_range()) {
                TKuduKeyRange kr = locations.scan_range.getKudu_key_range();
                Integer hostIdx = locations.locations.get(0).host_idx;
                TNetworkAddress networkAddress = execRequest.getHost_list().get(hostIdx);
                result.append("KUDU KEYRANGE ");
                // TODO Enable the lines below once we have better testing for
                // non-local key-ranges
                // result.append("host=" + networkAddress.hostname + ":" +
                // networkAddress.port + " ");
                result.append(Arrays.toString(kr.getRange_start_key()));
                result.append(":");
                result.append(Arrays.toString(kr.getRange_stop_key()));
            }
            result.append("\n");
        }
    }
    return result;
}
#method_after
private StringBuilder printScanRangeLocations(TQueryExecRequest execRequest) {
    StringBuilder result = new StringBuilder();
    if (execRequest.per_node_scan_ranges == null) {
        return result;
    }
    for (Map.Entry<Integer, List<TScanRangeLocations>> entry : execRequest.per_node_scan_ranges.entrySet()) {
        result.append("NODE " + entry.getKey().toString() + ":\n");
        if (entry.getValue() == null) {
            continue;
        }
        for (TScanRangeLocations locations : entry.getValue()) {
            // print scan range
            result.append("  ");
            if (locations.scan_range.isSetHdfs_file_split()) {
                THdfsFileSplit split = locations.scan_range.getHdfs_file_split();
                THdfsTable table = findTable(entry.getKey());
                THdfsPartition partition = table.getPartitions().get(split.partition_id);
                THdfsPartitionLocation location = partition.getLocation();
                String file_location = location.getSuffix();
                if (location.prefix_index != -1) {
                    file_location = table.getPartition_prefixes().get(location.prefix_index) + file_location;
                }
                Path filePath = new Path(file_location, split.file_name);
                filePath = cleanseFilePath(filePath);
                result.append("HDFS SPLIT " + filePath.toString() + " " + Long.toString(split.offset) + ":" + Long.toString(split.length));
            }
            if (locations.scan_range.isSetHbase_key_range()) {
                THBaseKeyRange keyRange = locations.scan_range.getHbase_key_range();
                Integer hostIdx = locations.locations.get(0).host_idx;
                TNetworkAddress networkAddress = execRequest.getHost_list().get(hostIdx);
                result.append("HBASE KEYRANGE ");
                result.append("port=" + networkAddress.port + " ");
                if (keyRange.isSetStartKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStartKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
                result.append(":");
                if (keyRange.isSetStopKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStopKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
            }
            if (locations.scan_range.isSetKudu_scan_token()) {
                Preconditions.checkNotNull(kuduClient_, "Test should not be invoked on platforms that do not support Kudu.");
                try {
                    result.append(KuduScanToken.stringifySerializedToken(locations.scan_range.kudu_scan_token.array(), kuduClient_));
                } catch (IOException e) {
                    throw new IllegalStateException("Unable to parse Kudu scan token", e);
                }
            }
            result.append("\n");
        }
    }
    return result;
}
#end_block

#method_before
public static String getAvroSchema(List<Map<String, String>> schemaSearchLocations) throws AnalysisException {
    String url = null;
    // Search all locations and break out on the first valid schema found.
    for (Map<String, String> schemaLocation : schemaSearchLocations) {
        if (schemaLocation == null)
            continue;
        String literal = schemaLocation.get(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName());
        if (literal != null && !literal.equals(AvroSerdeUtils.SCHEMA_NONE))
            return literal;
        url = schemaLocation.get(AvroSerdeUtils.AvroTableProperties.SCHEMA_URL.getPropName());
        if (url != null && !url.equals(AvroSerdeUtils.SCHEMA_NONE)) {
            url = url.trim();
            break;
        }
    }
    if (url == null)
        return null;
    String schema = null;
    InputStream urlStream = null;
    try {
        // TODO: Add support for https:// here.
        if (url.toLowerCase().startsWith("http://")) {
            urlStream = new URL(url).openStream();
            schema = IOUtils.toString(urlStream);
        } else {
            Path path = new Path(url);
            FileSystem fs = null;
            fs = path.getFileSystem(FileSystemUtil.getConfiguration());
            StringBuilder errorMsg = new StringBuilder();
            if (!FileSystemUtil.isPathReachable(path, fs, errorMsg)) {
                throw new AnalysisException(String.format("Invalid avro.schema.url: %s. %s", url, errorMsg));
            }
            schema = FileSystemUtil.readFile(path);
        }
    } catch (AnalysisException e) {
        throw e;
    } catch (IOException e) {
        throw new AnalysisException(String.format("Failed to read Avro schema at: %s. %s ", url, e.getMessage()));
    } catch (Exception e) {
        throw new AnalysisException(String.format("Invalid avro.schema.url: %s. %s", url, e.getMessage()));
    } finally {
        if (urlStream != null)
            IOUtils.closeQuietly(urlStream);
    }
    return schema;
}
#method_after
public static String getAvroSchema(List<Map<String, String>> schemaSearchLocations) throws AnalysisException {
    String url = null;
    // Search all locations and break out on the first valid schema found.
    for (Map<String, String> schemaLocation : schemaSearchLocations) {
        if (schemaLocation == null)
            continue;
        String literal = schemaLocation.get(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName());
        if (literal != null && !literal.equals(AvroSerdeUtils.SCHEMA_NONE))
            return literal;
        url = schemaLocation.get(AvroSerdeUtils.AvroTableProperties.SCHEMA_URL.getPropName());
        if (url != null && !url.equals(AvroSerdeUtils.SCHEMA_NONE)) {
            url = url.trim();
            break;
        }
    }
    if (url == null)
        return null;
    String schema = null;
    InputStream urlStream = null;
    try {
        // TODO: Add support for https:// here.
        if (url.toLowerCase().startsWith("http://")) {
            urlStream = new URL(url).openStream();
            schema = IOUtils.toString(urlStream);
        } else {
            Path path = new Path(url);
            FileSystem fs = null;
            fs = path.getFileSystem(FileSystemUtil.getConfiguration());
            if (!fs.exists(path)) {
                throw new AnalysisException(String.format("Invalid avro.schema.url: %s. Path does not exist.", url));
            }
            schema = FileSystemUtil.readFile(path);
        }
    } catch (AnalysisException e) {
        throw e;
    } catch (IOException e) {
        throw new AnalysisException(String.format("Failed to read Avro schema at: %s. %s ", url, e.getMessage()));
    } catch (Exception e) {
        throw new AnalysisException(String.format("Invalid avro.schema.url: %s. %s", url, e.getMessage()));
    } finally {
        if (urlStream != null)
            IOUtils.closeQuietly(urlStream);
    }
    return schema;
}
#end_block

#method_before
public boolean resolve() {
    if (isResolved_)
        return true;
    Preconditions.checkState(rootDesc_ != null || rootTable_ != null);
    Type currentType = null;
    int rawPathIdx = 0;
    if (rootPath_ != null) {
        // Continue resolving this path relative to the rootPath_.
        currentType = rootPath_.destType();
        rawPathIdx = rootPath_.getRawPath().size();
    } else if (rootDesc_ != null) {
        currentType = rootDesc_.getType();
    } else {
        // Directly start from the item type because only implicit paths are allowed.
        currentType = rootTable_.getType().getItemType();
    }
    // True if the next raw-path element must match explicitly.
    boolean expectExplicitMatch = false;
    // Map all remaining raw-path elements to field types and positions.
    while (rawPathIdx < rawPath_.size()) {
        if (!currentType.isComplexType())
            return false;
        StructType structType = getTypeAsStruct(currentType);
        // Resolve explicit path.
        StructField field = structType.getField(rawPath_.get(rawPathIdx));
        if (field == null) {
            // Resolve implicit path.
            if (!expectExplicitMatch && structType instanceof CollectionStructType) {
                field = ((CollectionStructType) structType).getOptionalField();
            } else {
                // Failed to resolve implicit or explicit path.
                return false;
            }
            // Update the physical types/positions.
            matchedTypes_.add(field.getType());
            matchedPositions_.add(field.getPosition());
            currentType = field.getType();
            // After an implicit match there must be an explicit match.
            expectExplicitMatch = true;
            // Do not consume a raw-path element.
            continue;
        }
        // After an explicit match we could have an implicit match again.
        expectExplicitMatch = false;
        matchedTypes_.add(field.getType());
        matchedPositions_.add(field.getPosition());
        if (field.getType().isCollectionType() && firstCollectionPathIdx_ == -1) {
            Preconditions.checkState(firstCollectionTypeIdx_ == -1);
            firstCollectionPathIdx_ = rawPathIdx;
            firstCollectionTypeIdx_ = matchedTypes_.size() - 1;
        }
        currentType = field.getType();
        ++rawPathIdx;
    }
    Preconditions.checkState(matchedTypes_.size() == matchedPositions_.size());
    Preconditions.checkState(matchedTypes_.size() >= rawPath_.size());
    isResolved_ = true;
    return true;
}
#method_after
public boolean resolve() {
    if (isResolved_)
        return true;
    Preconditions.checkState(rootDesc_ != null || rootTable_ != null);
    Type currentType = null;
    int rawPathIdx = 0;
    if (rootPath_ != null) {
        // Continue resolving this path relative to the rootPath_.
        currentType = rootPath_.destType();
        rawPathIdx = rootPath_.getRawPath().size();
    } else if (rootDesc_ != null) {
        currentType = rootDesc_.getType();
    } else {
        // Directly start from the item type because only implicit paths are allowed.
        currentType = rootTable_.getType().getItemType();
    }
    // Map all remaining raw-path elements to field types and positions.
    while (rawPathIdx < rawPath_.size()) {
        if (!currentType.isComplexType())
            return false;
        StructType structType = getTypeAsStruct(currentType);
        // Resolve explicit path.
        StructField field = structType.getField(rawPath_.get(rawPathIdx));
        if (field == null) {
            // Resolve implicit path.
            if (structType instanceof CollectionStructType) {
                field = ((CollectionStructType) structType).getOptionalField();
                // Collections must be matched explicitly.
                if (field.getType().isCollectionType())
                    return false;
            } else {
                // Failed to resolve implicit or explicit path.
                return false;
            }
            // Update the physical types/positions.
            matchedTypes_.add(field.getType());
            matchedPositions_.add(field.getPosition());
            currentType = field.getType();
            // Do not consume a raw-path element.
            continue;
        }
        matchedTypes_.add(field.getType());
        matchedPositions_.add(field.getPosition());
        if (field.getType().isCollectionType() && firstCollectionPathIdx_ == -1) {
            Preconditions.checkState(firstCollectionTypeIdx_ == -1);
            firstCollectionPathIdx_ = rawPathIdx;
            firstCollectionTypeIdx_ = matchedTypes_.size() - 1;
        }
        currentType = field.getType();
        ++rawPathIdx;
    }
    Preconditions.checkState(matchedTypes_.size() == matchedPositions_.size());
    Preconditions.checkState(matchedTypes_.size() >= rawPath_.size());
    isResolved_ = true;
    return true;
}
#end_block

#method_before
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    LOG.debug("collecting partitions for table " + tbl_.getName());
    numPartitionsMissingStats_ = 0;
    if (tbl_.getPartitions().isEmpty()) {
        cardinality_ = tbl_.getNumRows();
        if ((cardinality_ < -1 || cardinality_ == 0) && tbl_.getTotalHdfsBytes() > 0) {
            hasCorruptTableStats_ = true;
        }
    } else {
        cardinality_ = 0;
        totalFiles_ = 0;
        totalBytes_ = 0;
        boolean hasValidPartitionCardinality = false;
        for (HdfsPartition p : partitions_) {
            // Check for corrupt table stats
            if ((p.getNumRows() == 0 || p.getNumRows() < -1) && p.getSize() > 0) {
                hasCorruptTableStats_ = true;
            }
            // enough to change the planning outcome
            if (p.getNumRows() > -1) {
                cardinality_ = addCardinalities(cardinality_, p.getNumRows());
                hasValidPartitionCardinality = true;
            } else {
                ++numPartitionsMissingStats_;
            }
            totalFiles_ += p.getFileDescriptors().size();
            totalBytes_ += p.getSize();
        }
        if (!partitions_.isEmpty() && !hasValidPartitionCardinality) {
            // if none of the partitions knew its number of rows, we fall back on
            // the table stats
            cardinality_ = tbl_.getNumRows();
        }
    }
    // Adjust cardinality for all collections referenced along the tuple's path.
    if (cardinality_ != -1) {
        for (Type t : desc_.getPath().getMatchedTypes()) {
            if (t.isCollectionType())
                cardinality_ *= PlannerContext.AVG_COLLECTION_SIZE;
        }
    }
    inputCardinality_ = cardinality_;
    Preconditions.checkState(cardinality_ >= 0 || cardinality_ == -1, "Internal error: invalid scan node cardinality: " + cardinality_);
    if (cardinality_ > 0) {
        LOG.debug("cardinality_=" + Long.toString(cardinality_) + " sel=" + Double.toString(computeSelectivity()));
        cardinality_ = Math.round(cardinality_ * computeSelectivity());
        // IMPALA-2165: Avoid setting the cardinality to 0 after rounding.
        cardinality_ = Math.max(cardinality_, 1);
    }
    cardinality_ = capAtLimit(cardinality_);
    LOG.debug("computeStats HdfsScan: cardinality_=" + Long.toString(cardinality_));
    computeNumNodes(analyzer, cardinality_);
    LOG.debug("computeStats HdfsScan: #nodes=" + Integer.toString(numNodes_));
}
#method_after
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    LOG.debug("collecting partitions for table " + tbl_.getName());
    numPartitionsMissingStats_ = 0;
    totalFiles_ = 0;
    totalBytes_ = 0;
    if (tbl_.getNumClusteringCols() == 0) {
        cardinality_ = tbl_.getNumRows();
        if (cardinality_ < -1 || (cardinality_ == 0 && tbl_.getTotalHdfsBytes() > 0)) {
            hasCorruptTableStats_ = true;
        }
        if (partitions_.isEmpty()) {
            // Nothing to scan. Definitely a cardinality of 0 even if we have no stats.
            cardinality_ = 0;
        } else {
            Preconditions.checkState(partitions_.size() == 1);
            totalFiles_ += partitions_.get(0).getFileDescriptors().size();
            totalBytes_ += partitions_.get(0).getSize();
        }
    } else {
        cardinality_ = 0;
        boolean hasValidPartitionCardinality = false;
        for (HdfsPartition p : partitions_) {
            // Check for corrupt table stats
            if (p.getNumRows() < -1 || (p.getNumRows() == 0 && p.getSize() > 0)) {
                hasCorruptTableStats_ = true;
            }
            // enough to change the planning outcome
            if (p.getNumRows() > -1) {
                cardinality_ = addCardinalities(cardinality_, p.getNumRows());
                hasValidPartitionCardinality = true;
            } else {
                ++numPartitionsMissingStats_;
            }
            totalFiles_ += p.getFileDescriptors().size();
            totalBytes_ += p.getSize();
        }
        if (!partitions_.isEmpty() && !hasValidPartitionCardinality) {
            // if none of the partitions knew its number of rows, we fall back on
            // the table stats
            cardinality_ = tbl_.getNumRows();
        }
    }
    // Adjust cardinality for all collections referenced along the tuple's path.
    if (cardinality_ != -1) {
        for (Type t : desc_.getPath().getMatchedTypes()) {
            if (t.isCollectionType())
                cardinality_ *= PlannerContext.AVG_COLLECTION_SIZE;
        }
    }
    inputCardinality_ = cardinality_;
    // Sanity check scan node cardinality.
    if (cardinality_ < -1) {
        hasCorruptTableStats_ = true;
        cardinality_ = -1;
    }
    if (cardinality_ > 0) {
        LOG.debug("cardinality_=" + Long.toString(cardinality_) + " sel=" + Double.toString(computeSelectivity()));
        cardinality_ = Math.round(cardinality_ * computeSelectivity());
        // IMPALA-2165: Avoid setting the cardinality to 0 after rounding.
        cardinality_ = Math.max(cardinality_, 1);
    }
    cardinality_ = capAtLimit(cardinality_);
    LOG.debug("computeStats HdfsScan: cardinality_=" + Long.toString(cardinality_));
    computeNumNodes(analyzer, cardinality_);
    LOG.debug("computeStats HdfsScan: #nodes=" + Integer.toString(numNodes_));
}
#end_block

#method_before
private void loadTableMetadata(Table tbl, long newCatalogVersion, boolean reloadFileMetadata, boolean reloadTableSchema, Set<String> partitionsToUpdate) throws CatalogException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(msClient, tbl);
        if (tbl instanceof HdfsTable) {
            ((HdfsTable) tbl).load(true, msClient.getHiveClient(), msTbl, reloadFileMetadata, reloadTableSchema, partitionsToUpdate);
        } else {
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
    } finally {
        msClient.release();
    }
    tbl.setCatalogVersion(newCatalogVersion);
}
#method_after
private void loadTableMetadata(Table tbl, long newCatalogVersion, boolean reloadFileMetadata, boolean reloadTableSchema, Set<String> partitionsToUpdate) throws CatalogException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable(msClient, tbl);
        if (tbl instanceof HdfsTable) {
            ((HdfsTable) tbl).load(true, msClient.getHiveClient(), msTbl, reloadFileMetadata, reloadTableSchema, partitionsToUpdate);
        } else {
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
    }
    tbl.setCatalogVersion(newCatalogVersion);
}
#end_block

#method_before
private void alterView(TCreateOrAlterViewParams params, TDdlExecResponse resp) throws ImpalaException {
    TableName tableName = TableName.fromThrift(params.getView_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    Preconditions.checkState(params.getColumns() != null && params.getColumns().size() > 0, "Null or empty column list given as argument to DdlExecutor.alterView");
    Table tbl = catalog_.getTable(tableName.getDb(), tableName.getTbl());
    Preconditions.checkState(tbl instanceof View);
    catalog_.getLock().writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        catalog_.getLock().writeLock().unlock();
        // Operate on a copy of the metastore table to avoid prematurely applying the
        // alteration to our cached table in case the actual alteration fails.
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        if (!msTbl.getTableType().equalsIgnoreCase((TableType.VIRTUAL_VIEW.toString()))) {
            throw new ImpalaRuntimeException(String.format("ALTER VIEW not allowed on a table: %s", tableName.toString()));
        }
        // Set the altered view attributes and update the metastore.
        setViewAttributes(params, msTbl);
        LOG.debug(String.format("Altering view %s", tableName));
        applyAlterTable(msTbl);
        tbl.load(true, catalog_.getMetaStoreClient().getHiveClient(), msTbl);
        tbl.setCatalogVersion(newCatalogVersion);
        addTableToCatalogUpdate(tbl, resp.result);
    }
}
#method_after
private void alterView(TCreateOrAlterViewParams params, TDdlExecResponse resp) throws ImpalaException {
    TableName tableName = TableName.fromThrift(params.getView_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    Preconditions.checkState(params.getColumns() != null && params.getColumns().size() > 0, "Null or empty column list given as argument to DdlExecutor.alterView");
    Table tbl = catalog_.getTable(tableName.getDb(), tableName.getTbl());
    Preconditions.checkState(tbl instanceof View);
    catalog_.getLock().writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        catalog_.getLock().writeLock().unlock();
        // Operate on a copy of the metastore table to avoid prematurely applying the
        // alteration to our cached table in case the actual alteration fails.
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        if (!msTbl.getTableType().equalsIgnoreCase((TableType.VIRTUAL_VIEW.toString()))) {
            throw new ImpalaRuntimeException(String.format("ALTER VIEW not allowed on a table: %s", tableName.toString()));
        }
        // Set the altered view attributes and update the metastore.
        setViewAttributes(params, msTbl);
        LOG.debug(String.format("Altering view %s", tableName));
        applyAlterTable(msTbl);
        try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
        tbl.setCatalogVersion(newCatalogVersion);
        addTableToCatalogUpdate(tbl, resp.result);
    }
}
#end_block

#method_before
private void alterTableUpdateStats(Table table, TAlterTableUpdateStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(table));
    if (params.isSetTable_stats()) {
        // Updating table and column stats via COMPUTE STATS.
        Preconditions.checkState(params.isSetPartition_stats() && params.isSetTable_stats());
    } else {
        // Only changing column stats via ALTER TABLE SET COLUMN STATS.
        Preconditions.checkState(params.isSetColumn_stats());
    }
    TableName tableName = table.getTableName();
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    LOG.info(String.format("Updating table stats for: %s", tableName));
    // Deep copy the msTbl to avoid updating our cache before successfully persisting
    // the results to the metastore.
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
    List<HdfsPartition> partitions = Lists.newArrayList();
    if (table instanceof HdfsTable) {
        // Build a list of non-default partitions to update.
        HdfsTable hdfsTable = (HdfsTable) table;
        for (HdfsPartition p : hdfsTable.getPartitions()) {
            if (!p.isDefaultPartition())
                partitions.add(p);
        }
    }
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    int numTargetedPartitions = 0;
    int numUpdatedColumns = 0;
    try {
        // Update the table and partition row counts based on the query results.
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        if (params.isSetTable_stats()) {
            numTargetedPartitions = updateTableStats(table, params, msTbl, partitions, modifiedParts);
        }
        ColumnStatistics colStats = null;
        if (params.isSetColumn_stats()) {
            // Create Hive column stats from the query results.
            colStats = createHiveColStats(params.getColumn_stats(), table);
            numUpdatedColumns = colStats.getStatsObjSize();
        }
        // Update all partitions.
        bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
        if (numUpdatedColumns > 0) {
            Preconditions.checkNotNull(colStats);
            // Update column stats.
            try {
                msClient.getHiveClient().updateTableColumnStatistics(colStats);
            } catch (Exception e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "updateTableColumnStatistics"), e);
            }
        }
        // Update the table stats. Apply the table alteration last to ensure the
        // lastDdlTime is as accurate as possible.
        applyAlterTable(msTbl);
    } finally {
        msClient.release();
    }
    // Set the results to be reported to the client.
    TResultSet resultSet = new TResultSet();
    resultSet.setSchema(new TResultSetMetadata(Lists.newArrayList(new TColumn("summary", Type.STRING.toThrift()))));
    TColumnValue resultColVal = new TColumnValue();
    resultColVal.setString_val("Updated " + numTargetedPartitions + " partition(s) and " + numUpdatedColumns + " column(s).");
    TResultRow resultRow = new TResultRow();
    resultRow.setColVals(Lists.newArrayList(resultColVal));
    resultSet.setRows(Lists.newArrayList(resultRow));
    resp.setResult_set(resultSet);
}
#method_after
private void alterTableUpdateStats(Table table, TAlterTableUpdateStatsParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(table));
    if (params.isSetTable_stats()) {
        // Updating table and column stats via COMPUTE STATS.
        Preconditions.checkState(params.isSetPartition_stats() && params.isSetTable_stats());
    } else {
        // Only changing column stats via ALTER TABLE SET COLUMN STATS.
        Preconditions.checkState(params.isSetColumn_stats());
    }
    TableName tableName = table.getTableName();
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    LOG.info(String.format("Updating table stats for: %s", tableName));
    // Deep copy the msTbl to avoid updating our cache before successfully persisting
    // the results to the metastore.
    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
    List<HdfsPartition> partitions = Lists.newArrayList();
    if (table instanceof HdfsTable) {
        // Build a list of non-default partitions to update.
        HdfsTable hdfsTable = (HdfsTable) table;
        for (HdfsPartition p : hdfsTable.getPartitions()) {
            if (!p.isDefaultPartition())
                partitions.add(p);
        }
    }
    int numTargetedPartitions = 0;
    int numUpdatedColumns = 0;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Update the table and partition row counts based on the query results.
        List<HdfsPartition> modifiedParts = Lists.newArrayList();
        if (params.isSetTable_stats()) {
            numTargetedPartitions = updateTableStats(table, params, msTbl, partitions, modifiedParts);
        }
        ColumnStatistics colStats = null;
        if (params.isSetColumn_stats()) {
            // Create Hive column stats from the query results.
            colStats = createHiveColStats(params.getColumn_stats(), table);
            numUpdatedColumns = colStats.getStatsObjSize();
        }
        // Update all partitions.
        bulkAlterPartitions(table.getDb().getName(), table.getName(), modifiedParts);
        if (numUpdatedColumns > 0) {
            Preconditions.checkNotNull(colStats);
            // Update column stats.
            try {
                msClient.getHiveClient().updateTableColumnStatistics(colStats);
            } catch (Exception e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "updateTableColumnStatistics"), e);
            }
        }
        // Update the table stats. Apply the table alteration last to ensure the
        // lastDdlTime is as accurate as possible.
        applyAlterTable(msTbl);
    }
    // Set the results to be reported to the client.
    TResultSet resultSet = new TResultSet();
    resultSet.setSchema(new TResultSetMetadata(Lists.newArrayList(new TColumn("summary", Type.STRING.toThrift()))));
    TColumnValue resultColVal = new TColumnValue();
    resultColVal.setString_val("Updated " + numTargetedPartitions + " partition(s) and " + numUpdatedColumns + " column(s).");
    TResultRow resultRow = new TResultRow();
    resultRow.setColVals(Lists.newArrayList(resultColVal));
    resultSet.setRows(Lists.newArrayList(resultRow));
    resp.setResult_set(resultSet);
}
#end_block

#method_before
private void createDatabase(TCreateDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    String dbName = params.getDb();
    Preconditions.checkState(dbName != null && !dbName.isEmpty(), "Null or empty database name passed as argument to Catalog.createDatabase");
    if (params.if_not_exists && catalog_.getDb(dbName) != null) {
        LOG.debug("Skipping database creation because " + dbName + " already exists and " + "IF NOT EXISTS was specified.");
        resp.getResult().setVersion(catalog_.getCatalogVersion());
        return;
    }
    org.apache.hadoop.hive.metastore.api.Database db = new org.apache.hadoop.hive.metastore.api.Database();
    db.setName(dbName);
    if (params.getComment() != null) {
        db.setDescription(params.getComment());
    }
    if (params.getLocation() != null) {
        db.setLocationUri(params.getLocation());
    }
    LOG.debug("Creating database " + dbName);
    Db newDb = null;
    synchronized (metastoreDdlLock_) {
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        try {
            msClient.getHiveClient().createDatabase(db);
            newDb = catalog_.addDb(dbName, db);
        } catch (AlreadyExistsException e) {
            if (!params.if_not_exists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating database %s because " + "IF NOT EXISTS was specified.", e, dbName));
            newDb = catalog_.getDb(dbName);
            if (newDb == null) {
                try {
                    org.apache.hadoop.hive.metastore.api.Database msDb = msClient.getHiveClient().getDatabase(dbName);
                    newDb = catalog_.addDb(dbName, msDb);
                } catch (TException e1) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e1);
                }
            }
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
        } finally {
            msClient.release();
        }
        Preconditions.checkNotNull(newDb);
        TCatalogObject thriftDb = new TCatalogObject(TCatalogObjectType.DATABASE, Catalog.INITIAL_CATALOG_VERSION);
        thriftDb.setDb(newDb.toThrift());
        thriftDb.setCatalog_version(newDb.getCatalogVersion());
        resp.result.setUpdated_catalog_object_DEPRECATED(thriftDb);
    }
    resp.result.setVersion(resp.result.getUpdated_catalog_object_DEPRECATED().getCatalog_version());
}
#method_after
private void createDatabase(TCreateDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    String dbName = params.getDb();
    Preconditions.checkState(dbName != null && !dbName.isEmpty(), "Null or empty database name passed as argument to Catalog.createDatabase");
    if (params.if_not_exists && catalog_.getDb(dbName) != null) {
        LOG.debug("Skipping database creation because " + dbName + " already exists and " + "IF NOT EXISTS was specified.");
        resp.getResult().setVersion(catalog_.getCatalogVersion());
        return;
    }
    org.apache.hadoop.hive.metastore.api.Database db = new org.apache.hadoop.hive.metastore.api.Database();
    db.setName(dbName);
    if (params.getComment() != null) {
        db.setDescription(params.getComment());
    }
    if (params.getLocation() != null) {
        db.setLocationUri(params.getLocation());
    }
    LOG.debug("Creating database " + dbName);
    Db newDb = null;
    synchronized (metastoreDdlLock_) {
        try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
            try {
                msClient.getHiveClient().createDatabase(db);
                newDb = catalog_.addDb(dbName, db);
            } catch (AlreadyExistsException e) {
                if (!params.if_not_exists) {
                    throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
                }
                LOG.debug(String.format("Ignoring '%s' when creating database %s because " + "IF NOT EXISTS was specified.", e, dbName));
                newDb = catalog_.getDb(dbName);
                if (newDb == null) {
                    try {
                        org.apache.hadoop.hive.metastore.api.Database msDb = msClient.getHiveClient().getDatabase(dbName);
                        newDb = catalog_.addDb(dbName, msDb);
                    } catch (TException e1) {
                        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e1);
                    }
                }
            } catch (TException e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createDatabase"), e);
            }
        }
        Preconditions.checkNotNull(newDb);
        TCatalogObject thriftDb = new TCatalogObject(TCatalogObjectType.DATABASE, Catalog.INITIAL_CATALOG_VERSION);
        thriftDb.setDb(newDb.toThrift());
        thriftDb.setCatalog_version(newDb.getCatalogVersion());
        resp.result.setUpdated_catalog_object_DEPRECATED(thriftDb);
    }
    resp.result.setVersion(resp.result.getUpdated_catalog_object_DEPRECATED().getCatalog_version());
}
#end_block

#method_before
private int dropColumnStats(Table table) throws ImpalaRuntimeException {
    Preconditions.checkState(Thread.holdsLock(table));
    int numColsUpdated = 0;
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        for (Column col : table.getColumns()) {
            // Skip columns that don't have stats.
            if (!col.getStats().hasStats())
                continue;
            try {
                msClient.getHiveClient().deleteTableColumnStatistics(table.getDb().getName(), table.getName(), col.getName());
                ++numColsUpdated;
            } catch (NoSuchObjectException e) {
            // We don't care if the column stats do not exist, just ignore the exception.
            // We would only expect to make it here if the Impala and HMS metadata
            // diverged.
            } catch (TException e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "delete_table_column_statistics"), e);
            }
        }
    } finally {
        msClient.release();
    }
    return numColsUpdated;
}
#method_after
private int dropColumnStats(Table table) throws ImpalaRuntimeException {
    Preconditions.checkState(Thread.holdsLock(table));
    int numColsUpdated = 0;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        for (Column col : table.getColumns()) {
            // Skip columns that don't have stats.
            if (!col.getStats().hasStats())
                continue;
            try {
                msClient.getHiveClient().deleteTableColumnStatistics(table.getDb().getName(), table.getName(), col.getName());
                ++numColsUpdated;
            } catch (NoSuchObjectException e) {
            // We don't care if the column stats do not exist, just ignore the exception.
            // We would only expect to make it here if the Impala and HMS metadata
            // diverged.
            } catch (TException e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "delete_table_column_statistics"), e);
            }
        }
    }
    return numColsUpdated;
}
#end_block

#method_before
private void dropDatabase(TDropDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    Preconditions.checkState(params.getDb() != null && !params.getDb().isEmpty(), "Null or empty database name passed as argument to Catalog.dropDatabase");
    LOG.debug("Dropping database " + params.getDb());
    Db db = catalog_.getDb(params.db);
    if (db != null && db.numFunctions() > 0 && !params.cascade) {
        throw new CatalogException("Database " + db.getName() + " is not empty");
    }
    TCatalogObject removedObject = new TCatalogObject();
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    synchronized (metastoreDdlLock_) {
        try {
            msClient.getHiveClient().dropDatabase(params.getDb(), true, params.if_exists, params.cascade);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropDatabase"), e);
        } finally {
            msClient.release();
        }
        Db removedDb = catalog_.removeDb(params.getDb());
        // version.
        if (removedDb == null) {
            removedObject.setCatalog_version(catalog_.getCatalogVersion());
        } else {
            removedObject.setCatalog_version(removedDb.getCatalogVersion());
        }
    }
    removedObject.setType(TCatalogObjectType.DATABASE);
    removedObject.setDb(new TDatabase());
    removedObject.getDb().setDb_name(params.getDb());
    resp.result.setVersion(removedObject.getCatalog_version());
    resp.result.setRemoved_catalog_object_DEPRECATED(removedObject);
}
#method_after
private void dropDatabase(TDropDbParams params, TDdlExecResponse resp) throws ImpalaException {
    Preconditions.checkNotNull(params);
    Preconditions.checkState(params.getDb() != null && !params.getDb().isEmpty(), "Null or empty database name passed as argument to Catalog.dropDatabase");
    LOG.debug("Dropping database " + params.getDb());
    Db db = catalog_.getDb(params.db);
    if (db != null && db.numFunctions() > 0 && !params.cascade) {
        throw new CatalogException("Database " + db.getName() + " is not empty");
    }
    TCatalogObject removedObject = new TCatalogObject();
    synchronized (metastoreDdlLock_) {
        try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
            msClient.getHiveClient().dropDatabase(params.getDb(), true, params.if_exists, params.cascade);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropDatabase"), e);
        }
        Db removedDb = catalog_.removeDb(params.getDb());
        // version.
        if (removedDb == null) {
            removedObject.setCatalog_version(catalog_.getCatalogVersion());
        } else {
            removedObject.setCatalog_version(removedDb.getCatalogVersion());
        }
    }
    removedObject.setType(TCatalogObjectType.DATABASE);
    removedObject.setDb(new TDatabase());
    removedObject.getDb().setDb_name(params.getDb());
    resp.result.setVersion(removedObject.getCatalog_version());
    resp.result.setRemoved_catalog_object_DEPRECATED(removedObject);
}
#end_block

#method_before
private void dropTableOrView(TDropTableOrViewParams params, TDdlExecResponse resp) throws ImpalaException {
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    LOG.debug(String.format("Dropping table/view %s", tableName));
    TCatalogObject removedObject = new TCatalogObject();
    synchronized (metastoreDdlLock_) {
        // Forward the DDL operation to the specified storage backend.
        try {
            org.apache.hadoop.hive.metastore.api.Table msTbl = getExistingTable(tableName.getDb(), tableName.getTbl()).getMetaStoreTable();
            DdlDelegate handler = createDdlDelegate(msTbl);
            handler.dropTable();
        } catch (TableNotFoundException | DatabaseNotFoundException e) {
        // Do nothing
        }
        MetaStoreClient msClient = catalog_.getMetaStoreClient();
        Db db = catalog_.getDb(params.getTable_name().db_name);
        if (db == null) {
            if (params.if_exists)
                return;
            throw new CatalogException("Database does not exist: " + params.getTable_name().db_name);
        }
        Table existingTbl = db.getTable(params.getTable_name().table_name);
        if (existingTbl == null) {
            if (params.if_exists)
                return;
            throw new CatalogException("Table/View does not exist: " + tableName);
        }
        // fixed.
        if (params.isSetIs_table() && ((params.is_table && existingTbl instanceof View) || (!params.is_table && !(existingTbl instanceof View)))) {
            if (params.if_exists)
                return;
            String errorMsg = "DROP " + (params.is_table ? "TABLE " : "VIEW ") + "not allowed on a " + (params.is_table ? "view: " : "table: ") + tableName;
            throw new CatalogException(errorMsg);
        }
        try {
            msClient.getHiveClient().dropTable(tableName.getDb(), tableName.getTbl(), true, params.if_exists, params.purge);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropTable"), e);
        } finally {
            msClient.release();
        }
        Table table = catalog_.removeTable(params.getTable_name().db_name, params.getTable_name().table_name);
        if (table != null) {
            resp.result.setVersion(table.getCatalogVersion());
            if (table instanceof HdfsTable) {
                HdfsTable hdfsTable = (HdfsTable) table;
                if (hdfsTable.isMarkedCached()) {
                    try {
                        HdfsCachingUtil.uncacheTbl(table.getMetaStoreTable());
                    } catch (Exception e) {
                        LOG.error("Unable to uncache table: " + table.getFullName(), e);
                    }
                }
                if (table.getNumClusteringCols() > 0) {
                    for (HdfsPartition partition : hdfsTable.getPartitions()) {
                        if (partition.isMarkedCached()) {
                            try {
                                HdfsCachingUtil.uncachePartition(partition);
                            } catch (Exception e) {
                                LOG.error("Unable to uncache partition: " + partition.getPartitionName(), e);
                            }
                        }
                    }
                }
            }
        } else {
            resp.result.setVersion(catalog_.getCatalogVersion());
        }
    }
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(resp.result.getVersion());
    resp.result.setRemoved_catalog_object_DEPRECATED(removedObject);
}
#method_after
private void dropTableOrView(TDropTableOrViewParams params, TDdlExecResponse resp) throws ImpalaException {
    TableName tableName = TableName.fromThrift(params.getTable_name());
    Preconditions.checkState(tableName != null && tableName.isFullyQualified());
    LOG.debug(String.format("Dropping table/view %s", tableName));
    TCatalogObject removedObject = new TCatalogObject();
    synchronized (metastoreDdlLock_) {
        // Forward the DDL operation to the specified storage backend.
        try {
            org.apache.hadoop.hive.metastore.api.Table msTbl = getExistingTable(tableName.getDb(), tableName.getTbl()).getMetaStoreTable();
            DdlDelegate handler = createDdlDelegate(msTbl);
            handler.dropTable();
        } catch (TableNotFoundException | DatabaseNotFoundException e) {
        // Do nothing
        }
        Db db = catalog_.getDb(params.getTable_name().db_name);
        if (db == null) {
            if (params.if_exists)
                return;
            throw new CatalogException("Database does not exist: " + params.getTable_name().db_name);
        }
        Table existingTbl = db.getTable(params.getTable_name().table_name);
        if (existingTbl == null) {
            if (params.if_exists)
                return;
            throw new CatalogException("Table/View does not exist: " + tableName);
        }
        // fixed.
        if (params.isSetIs_table() && ((params.is_table && existingTbl instanceof View) || (!params.is_table && !(existingTbl instanceof View)))) {
            if (params.if_exists)
                return;
            String errorMsg = "DROP " + (params.is_table ? "TABLE " : "VIEW ") + "not allowed on a " + (params.is_table ? "view: " : "table: ") + tableName;
            throw new CatalogException(errorMsg);
        }
        try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
            msClient.getHiveClient().dropTable(tableName.getDb(), tableName.getTbl(), true, params.if_exists, params.purge);
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropTable"), e);
        }
        Table table = catalog_.removeTable(params.getTable_name().db_name, params.getTable_name().table_name);
        if (table != null) {
            resp.result.setVersion(table.getCatalogVersion());
            if (table instanceof HdfsTable) {
                HdfsTable hdfsTable = (HdfsTable) table;
                if (hdfsTable.isMarkedCached()) {
                    try {
                        HdfsCachingUtil.uncacheTbl(table.getMetaStoreTable());
                    } catch (Exception e) {
                        LOG.error("Unable to uncache table: " + table.getFullName(), e);
                    }
                }
                if (table.getNumClusteringCols() > 0) {
                    for (HdfsPartition partition : hdfsTable.getPartitions()) {
                        if (partition.isMarkedCached()) {
                            try {
                                HdfsCachingUtil.uncachePartition(partition);
                            } catch (Exception e) {
                                LOG.error("Unable to uncache partition: " + partition.getPartitionName(), e);
                            }
                        }
                    }
                }
            }
        } else {
            resp.result.setVersion(catalog_.getCatalogVersion());
        }
    }
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(resp.result.getVersion());
    resp.result.setRemoved_catalog_object_DEPRECATED(removedObject);
}
#end_block

#method_before
private boolean createTable(org.apache.hadoop.hive.metastore.api.Table newTable, boolean ifNotExists, THdfsCachingOp cacheOp, List<TDistributeParam> distribute_by, TDdlExecResponse response) throws ImpalaException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    synchronized (metastoreDdlLock_) {
        try {
            msClient.getHiveClient().createTable(newTable);
            // the user, an extra step is needed to read the table to find the location.
            if (cacheOp != null && cacheOp.isSet_cached() && newTable.getSd().getLocation() == null) {
                newTable = msClient.getHiveClient().getTable(newTable.getDbName(), newTable.getTableName());
            }
        } catch (AlreadyExistsException e) {
            if (!ifNotExists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createTable"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating table %s.%s because " + "IF NOT EXISTS was specified.", e, newTable.getDbName(), newTable.getTableName()));
            return false;
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createTable"), e);
        } finally {
            msClient.release();
        }
        // delete the just created hive table to avoid inconsistencies.
        try {
            createDdlDelegate(newTable).setDistributeParams(distribute_by).createTable();
        } catch (ImpalaRuntimeException e) {
            MetaStoreClient c = catalog_.getMetaStoreClient();
            try {
                c.getHiveClient().dropTable(newTable.getDbName(), newTable.getTableName(), false, ifNotExists);
            } catch (Exception hE) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropTable"), hE);
            } finally {
                c.release();
            }
            throw e;
        }
        // Submit the cache request and update the table metadata.
        if (cacheOp != null && cacheOp.isSet_cached()) {
            short replication = cacheOp.isSetReplication() ? cacheOp.getReplication() : JniCatalogConstants.HDFS_DEFAULT_CACHE_REPLICATION_FACTOR;
            long id = HdfsCachingUtil.submitCacheTblDirective(newTable, cacheOp.getCache_pool_name(), replication);
            catalog_.watchCacheDirs(Lists.<Long>newArrayList(id), new TTableName(newTable.getDbName(), newTable.getTableName()));
            applyAlterTable(newTable);
        }
        Table newTbl = catalog_.addTable(newTable.getDbName(), newTable.getTableName());
        addTableToCatalogUpdate(newTbl, response.result);
    }
    return true;
}
#method_after
private boolean createTable(org.apache.hadoop.hive.metastore.api.Table newTable, boolean ifNotExists, THdfsCachingOp cacheOp, List<TDistributeParam> distribute_by, TDdlExecResponse response) throws ImpalaException {
    synchronized (metastoreDdlLock_) {
        try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
            msClient.getHiveClient().createTable(newTable);
            // the user, an extra step is needed to read the table to find the location.
            if (cacheOp != null && cacheOp.isSet_cached() && newTable.getSd().getLocation() == null) {
                newTable = msClient.getHiveClient().getTable(newTable.getDbName(), newTable.getTableName());
            }
        } catch (AlreadyExistsException e) {
            if (!ifNotExists) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createTable"), e);
            }
            LOG.debug(String.format("Ignoring '%s' when creating table %s.%s because " + "IF NOT EXISTS was specified.", e, newTable.getDbName(), newTable.getTableName()));
            return false;
        } catch (TException e) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createTable"), e);
        }
        // delete the just created hive table to avoid inconsistencies.
        try {
            createDdlDelegate(newTable).setDistributeParams(distribute_by).createTable();
        } catch (ImpalaRuntimeException e) {
            try (MetaStoreClient c = catalog_.getMetaStoreClient()) {
                c.getHiveClient().dropTable(newTable.getDbName(), newTable.getTableName(), false, ifNotExists);
            } catch (Exception hE) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropTable"), hE);
            }
            throw e;
        }
        // Submit the cache request and update the table metadata.
        if (cacheOp != null && cacheOp.isSet_cached()) {
            short replication = cacheOp.isSetReplication() ? cacheOp.getReplication() : JniCatalogConstants.HDFS_DEFAULT_CACHE_REPLICATION_FACTOR;
            long id = HdfsCachingUtil.submitCacheTblDirective(newTable, cacheOp.getCache_pool_name(), replication);
            catalog_.watchCacheDirs(Lists.<Long>newArrayList(id), new TTableName(newTable.getDbName(), newTable.getTableName()));
            applyAlterTable(newTable);
        }
        Table newTbl = catalog_.addTable(newTable.getDbName(), newTable.getTableName());
        addTableToCatalogUpdate(newTbl, response.result);
    }
    return true;
}
#end_block

#method_before
private Table alterTableAddPartition(Table tbl, List<TPartitionKeyValue> partitionSpec, boolean ifNotExists, String location, THdfsCachingOp cacheOp) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    TableName tableName = tbl.getTableName();
    if (ifNotExists && catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.debug(String.format("Skipping partition creation because (%s) already exists" + " and ifNotExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    org.apache.hadoop.hive.metastore.api.Partition partition = null;
    Table result = null;
    List<Long> cacheIds = null;
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    partition = createHmsPartition(partitionSpec, msTbl, tableName, location);
    try {
        // Add the new partition.
        partition = msClient.getHiveClient().add_partition(partition);
        String cachePoolName = null;
        Short replication = null;
        if (cacheOp == null && parentTblCacheDirId != null) {
            // The user didn't specify an explicit caching operation, inherit the value
            // from the parent table.
            cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
            Preconditions.checkNotNull(cachePoolName);
            replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            Preconditions.checkNotNull(replication);
        } else if (cacheOp != null && cacheOp.isSet_cached()) {
            // The user explicitly stated that this partition should be cached.
            cachePoolName = cacheOp.getCache_pool_name();
            // explicitly set, use the default value.
            if (!cacheOp.isSetReplication() && parentTblCacheDirId != null) {
                replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            } else {
                replication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
            }
        }
        // If cache pool name is not null, it indicates this partition should be cached.
        if (cachePoolName != null) {
            long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
            cacheIds = Lists.<Long>newArrayList(id);
            // Update the partition metadata to include the cache directive id.
            msClient.getHiveClient().alter_partition(partition.getDbName(), partition.getTableName(), partition);
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (AlreadyExistsException e) {
        if (!ifNotExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
        }
        LOG.debug(String.format("Ignoring '%s' when adding partition to %s because" + " ifNotExists is true.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    } finally {
        msClient.release();
    }
    if (cacheIds != null)
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    // Return the table object with an updated catalog version after creating the
    // partition.
    result = addHdfsPartition(tbl, partition);
    return result;
}
#method_after
private Table alterTableAddPartition(Table tbl, List<TPartitionKeyValue> partitionSpec, boolean ifNotExists, String location, THdfsCachingOp cacheOp) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    TableName tableName = tbl.getTableName();
    if (ifNotExists && catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.debug(String.format("Skipping partition creation because (%s) already exists" + " and ifNotExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    org.apache.hadoop.hive.metastore.api.Partition partition = null;
    Table result = null;
    List<Long> cacheIds = null;
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    partition = createHmsPartition(partitionSpec, msTbl, tableName, location);
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Add the new partition.
        partition = msClient.getHiveClient().add_partition(partition);
        String cachePoolName = null;
        Short replication = null;
        if (cacheOp == null && parentTblCacheDirId != null) {
            // The user didn't specify an explicit caching operation, inherit the value
            // from the parent table.
            cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
            Preconditions.checkNotNull(cachePoolName);
            replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            Preconditions.checkNotNull(replication);
        } else if (cacheOp != null && cacheOp.isSet_cached()) {
            // The user explicitly stated that this partition should be cached.
            cachePoolName = cacheOp.getCache_pool_name();
            // explicitly set, use the default value.
            if (!cacheOp.isSetReplication() && parentTblCacheDirId != null) {
                replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
            } else {
                replication = HdfsCachingUtil.getReplicationOrDefault(cacheOp);
            }
        }
        // If cache pool name is not null, it indicates this partition should be cached.
        if (cachePoolName != null) {
            long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
            cacheIds = Lists.<Long>newArrayList(id);
            // Update the partition metadata to include the cache directive id.
            msClient.getHiveClient().alter_partition(partition.getDbName(), partition.getTableName(), partition);
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (AlreadyExistsException e) {
        if (!ifNotExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
        }
        LOG.debug(String.format("Ignoring '%s' when adding partition to %s because" + " ifNotExists is true.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    }
    if (cacheIds != null)
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    // Return the table object with an updated catalog version after creating the
    // partition.
    result = addHdfsPartition(tbl, partition);
    return result;
}
#end_block

#method_before
private Table alterTableDropPartition(Table tbl, List<TPartitionKeyValue> partitionSpec, boolean ifExists, boolean purge) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    TableName tableName = tbl.getTableName();
    if (ifExists && !catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.debug(String.format("Skipping partition drop because (%s) does not exist " + "and ifExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    HdfsPartition part = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    List<String> values = Lists.newArrayList();
    // Need to add in the values in the same order they are defined in the table.
    for (FieldSchema fs : msTbl.getPartitionKeys()) {
        for (TPartitionKeyValue kv : partitionSpec) {
            if (fs.getName().toLowerCase().equals(kv.getName().toLowerCase())) {
                values.add(kv.getValue());
            }
        }
    }
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    PartitionDropOptions dropOptions = PartitionDropOptions.instance();
    dropOptions.purgeData(purge);
    try {
        msClient.getHiveClient().dropPartition(tableName.getDb(), tableName.getTbl(), values, dropOptions);
        updateLastDdlTime(msTbl, msClient);
        if (part.isMarkedCached()) {
            HdfsCachingUtil.uncachePartition(part);
        }
    } catch (NoSuchObjectException e) {
        if (!ifExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
        }
        LOG.debug(String.format("Ignoring '%s' when dropping partition from %s because" + " ifExists is true.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
    } finally {
        msClient.release();
    }
    return catalog_.dropPartition(tbl, partitionSpec);
}
#method_after
private Table alterTableDropPartition(Table tbl, List<TPartitionKeyValue> partitionSpec, boolean ifExists, boolean purge) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    TableName tableName = tbl.getTableName();
    if (ifExists && !catalog_.containsHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec)) {
        LOG.debug(String.format("Skipping partition drop because (%s) does not exist " + "and ifExists is true.", Joiner.on(", ").join(partitionSpec)));
        return null;
    }
    HdfsPartition part = catalog_.getHdfsPartition(tableName.getDb(), tableName.getTbl(), partitionSpec);
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    List<String> values = Lists.newArrayList();
    // Need to add in the values in the same order they are defined in the table.
    for (FieldSchema fs : msTbl.getPartitionKeys()) {
        for (TPartitionKeyValue kv : partitionSpec) {
            if (fs.getName().toLowerCase().equals(kv.getName().toLowerCase())) {
                values.add(kv.getValue());
            }
        }
    }
    PartitionDropOptions dropOptions = PartitionDropOptions.instance();
    dropOptions.purgeData(purge);
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        msClient.getHiveClient().dropPartition(tableName.getDb(), tableName.getTbl(), values, dropOptions);
        updateLastDdlTime(msTbl, msClient);
        if (part.isMarkedCached()) {
            HdfsCachingUtil.uncachePartition(part);
        }
    } catch (NoSuchObjectException e) {
        if (!ifExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
        }
        LOG.debug(String.format("Ignoring '%s' when dropping partition from %s because" + " ifExists is true.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropPartition"), e);
    }
    return catalog_.dropPartition(tbl, partitionSpec);
}
#end_block

#method_before
private void alterTableOrViewRename(Table oldTbl, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(oldTbl) && catalog_.getLock().isWriteLockedByCurrentThread());
    TableName tableName = oldTbl.getTableName();
    org.apache.hadoop.hive.metastore.api.Table msTbl = oldTbl.getMetaStoreTable().deepCopy();
    msTbl.setDbName(newTableName.getDb());
    msTbl.setTableName(newTableName.getTbl());
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column
        // stats across databases, we save, drop and restore the column stats because
        // the HMS does not properly move them to the new table via alteration.
        ColumnStatistics hmsColStats = null;
        if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
            Map<String, TColumnStats> colStats = Maps.newHashMap();
            for (Column c : oldTbl.getColumns()) {
                colStats.put(c.getName(), c.getStats().toThrift());
            }
            hmsColStats = createHiveColStats(colStats, oldTbl);
            // Set the new db/table.
            hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
            LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            // Delete all column stats of the original table from the HMS.
            msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
        }
        // Perform the table rename in any case.
        msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
        if (hmsColStats != null) {
            LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
        }
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    } finally {
        msClient.release();
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object_DEPRECATED(removedObject);
    response.result.setUpdated_catalog_object_DEPRECATED(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#method_after
private void alterTableOrViewRename(Table oldTbl, TableName newTableName, TDdlExecResponse response) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(oldTbl) && catalog_.getLock().isWriteLockedByCurrentThread());
    TableName tableName = oldTbl.getTableName();
    org.apache.hadoop.hive.metastore.api.Table msTbl = oldTbl.getMetaStoreTable().deepCopy();
    msTbl.setDbName(newTableName.getDb());
    msTbl.setTableName(newTableName.getTbl());
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Workaround for HIVE-9720/IMPALA-1711: When renaming a table with column
        // stats across databases, we save, drop and restore the column stats because
        // the HMS does not properly move them to the new table via alteration.
        ColumnStatistics hmsColStats = null;
        if (!msTbl.getTableType().equalsIgnoreCase(TableType.VIRTUAL_VIEW.toString()) && !tableName.getDb().equalsIgnoreCase(newTableName.getDb())) {
            Map<String, TColumnStats> colStats = Maps.newHashMap();
            for (Column c : oldTbl.getColumns()) {
                colStats.put(c.getName(), c.getStats().toThrift());
            }
            hmsColStats = createHiveColStats(colStats, oldTbl);
            // Set the new db/table.
            hmsColStats.setStatsDesc(new ColumnStatisticsDesc(true, newTableName.getDb(), newTableName.getTbl()));
            LOG.trace(String.format("Dropping column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            // Delete all column stats of the original table from the HMS.
            msClient.getHiveClient().deleteTableColumnStatistics(tableName.getDb(), tableName.getTbl(), null);
        }
        // Perform the table rename in any case.
        msClient.getHiveClient().alter_table(tableName.getDb(), tableName.getTbl(), msTbl);
        if (hmsColStats != null) {
            LOG.trace(String.format("Restoring column stats for table %s being " + "renamed to %s to workaround HIVE-9720.", tableName.toString(), newTableName.toString()));
            msClient.getHiveClient().updateTableColumnStatistics(hmsColStats);
        }
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    }
    // Rename the table in the Catalog and get the resulting catalog object.
    // ALTER TABLE/VIEW RENAME is implemented as an ADD + DROP.
    TCatalogObject newTable = TableToTCatalogObject(catalog_.renameTable(tableName.toThrift(), newTableName.toThrift()));
    TCatalogObject removedObject = new TCatalogObject();
    removedObject.setType(TCatalogObjectType.TABLE);
    removedObject.setTable(new TTable());
    removedObject.getTable().setTbl_name(tableName.getTbl());
    removedObject.getTable().setDb_name(tableName.getDb());
    removedObject.setCatalog_version(newTable.getCatalog_version());
    response.result.setRemoved_catalog_object_DEPRECATED(removedObject);
    response.result.setUpdated_catalog_object_DEPRECATED(newTable);
    response.result.setVersion(newTable.getCatalog_version());
}
#end_block

#method_before
private void alterTableRecoverPartitions(Table tbl) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an HDFS table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    List<List<String>> partitionsNotInHms = hdfsTable.getPathsWithoutPartitions();
    if (partitionsNotInHms.isEmpty())
        return;
    List<Partition> hmsPartitions = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    TableName tableName = tbl.getTableName();
    for (List<String> partitionSpecValues : partitionsNotInHms) {
        hmsPartitions.add(createHmsPartitionFromValues(partitionSpecValues, msTbl, tableName, null));
    }
    String cachePoolName = null;
    Short replication = null;
    List<Long> cacheIds = Lists.newArrayList();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (parentTblCacheDirId != null) {
        // Inherit the HDFS cache value from the parent table.
        cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
        Preconditions.checkNotNull(cachePoolName);
        replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
        Preconditions.checkNotNull(replication);
    }
    // Add partitions to metastore.
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        // ifNotExists and needResults are true.
        hmsPartitions = msClient.getHiveClient().add_partitions(hmsPartitions, true, true);
        for (Partition partition : hmsPartitions) {
            // Create and add the HdfsPartition. Return the table object with an updated
            // catalog version.
            addHdfsPartition(tbl, partition);
        }
        // Handle HDFS cache.
        if (cachePoolName != null) {
            for (Partition partition : hmsPartitions) {
                long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
                cacheIds.add(id);
            }
            // Update the partition metadata to include the cache directive id.
            msClient.getHiveClient().alter_partitions(tableName.getDb(), tableName.getTbl(), hmsPartitions);
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (AlreadyExistsException e) {
        // This may happen when another client of HMS has added the partitions.
        LOG.debug(String.format("Ignoring '%s' when adding partition to %s.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    } finally {
        msClient.release();
    }
    if (!cacheIds.isEmpty()) {
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    }
}
#method_after
private void alterTableRecoverPartitions(Table tbl) throws ImpalaException {
    Preconditions.checkState(Thread.holdsLock(tbl));
    if (!(tbl instanceof HdfsTable)) {
        throw new CatalogException("Table " + tbl.getFullName() + " is not an HDFS table");
    }
    HdfsTable hdfsTable = (HdfsTable) tbl;
    List<List<String>> partitionsNotInHms = hdfsTable.getPathsWithoutPartitions();
    if (partitionsNotInHms.isEmpty())
        return;
    List<Partition> hmsPartitions = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
    TableName tableName = tbl.getTableName();
    for (List<String> partitionSpecValues : partitionsNotInHms) {
        hmsPartitions.add(createHmsPartitionFromValues(partitionSpecValues, msTbl, tableName, null));
    }
    String cachePoolName = null;
    Short replication = null;
    List<Long> cacheIds = Lists.newArrayList();
    Long parentTblCacheDirId = HdfsCachingUtil.getCacheDirectiveId(msTbl.getParameters());
    if (parentTblCacheDirId != null) {
        // Inherit the HDFS cache value from the parent table.
        cachePoolName = HdfsCachingUtil.getCachePool(parentTblCacheDirId);
        Preconditions.checkNotNull(cachePoolName);
        replication = HdfsCachingUtil.getCacheReplication(parentTblCacheDirId);
        Preconditions.checkNotNull(replication);
    }
    // Add partitions to metastore.
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // ifNotExists and needResults are true.
        hmsPartitions = msClient.getHiveClient().add_partitions(hmsPartitions, true, true);
        for (Partition partition : hmsPartitions) {
            // Create and add the HdfsPartition. Return the table object with an updated
            // catalog version.
            addHdfsPartition(tbl, partition);
        }
        // Handle HDFS cache.
        if (cachePoolName != null) {
            for (Partition partition : hmsPartitions) {
                long id = HdfsCachingUtil.submitCachePartitionDirective(partition, cachePoolName, replication);
                cacheIds.add(id);
            }
            // Update the partition metadata to include the cache directive id.
            msClient.getHiveClient().alter_partitions(tableName.getDb(), tableName.getTbl(), hmsPartitions);
        }
        updateLastDdlTime(msTbl, msClient);
    } catch (AlreadyExistsException e) {
        // This may happen when another client of HMS has added the partitions.
        LOG.debug(String.format("Ignoring '%s' when adding partition to %s.", e, tableName));
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "add_partition"), e);
    }
    if (!cacheIds.isEmpty()) {
        catalog_.watchCacheDirs(cacheIds, tableName.toThrift());
    }
}
#end_block

#method_before
public boolean addJavaFunctionToHms(String db, org.apache.hadoop.hive.metastore.api.Function fn, boolean ifNotExists) throws ImpalaRuntimeException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        msClient.getHiveClient().createFunction(fn);
    } catch (AlreadyExistsException e) {
        if (!ifNotExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createFunction"), e);
        }
        return false;
    } catch (Exception e) {
        LOG.error("Error executing createFunction() metastore call: " + fn.getFunctionName(), e);
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createFunction"), e);
    } finally {
        msClient.release();
    }
    return true;
}
#method_after
public boolean addJavaFunctionToHms(String db, org.apache.hadoop.hive.metastore.api.Function fn, boolean ifNotExists) throws ImpalaRuntimeException {
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        msClient.getHiveClient().createFunction(fn);
    } catch (AlreadyExistsException e) {
        if (!ifNotExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createFunction"), e);
        }
        return false;
    } catch (Exception e) {
        LOG.error("Error executing createFunction() metastore call: " + fn.getFunctionName(), e);
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "createFunction"), e);
    }
    return true;
}
#end_block

#method_before
public boolean dropJavaFunctionFromHms(String db, String fn, boolean ifExists) throws ImpalaRuntimeException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        msClient.getHiveClient().dropFunction(db, fn);
    } catch (NoSuchObjectException e) {
        if (!ifExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropFunction"), e);
        }
        return false;
    } catch (TException e) {
        LOG.error("Error executing dropFunction() metastore call: " + fn, e);
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropFunction"), e);
    } finally {
        msClient.release();
    }
    return true;
}
#method_after
public boolean dropJavaFunctionFromHms(String db, String fn, boolean ifExists) throws ImpalaRuntimeException {
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        msClient.getHiveClient().dropFunction(db, fn);
    } catch (NoSuchObjectException e) {
        if (!ifExists) {
            throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropFunction"), e);
        }
        return false;
    } catch (TException e) {
        LOG.error("Error executing dropFunction() metastore call: " + fn, e);
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "dropFunction"), e);
    }
    return true;
}
#end_block

#method_before
private void applyAlterDatabase(Db db) throws ImpalaRuntimeException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        msClient.getHiveClient().alterDatabase(db.getName(), db.getMetaStoreDb());
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alterDatabase"), e);
    } finally {
        msClient.release();
    }
}
#method_after
private void applyAlterDatabase(Db db) throws ImpalaRuntimeException {
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        msClient.getHiveClient().alterDatabase(db.getName(), db.getMetaStoreDb());
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alterDatabase"), e);
    }
}
#end_block

#method_before
private void applyAlterTable(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    long lastDdlTime = -1;
    try {
        lastDdlTime = calculateDdlTime(msTbl);
        msTbl.putToParameters("transient_lastDdlTime", Long.toString(lastDdlTime));
        msClient.getHiveClient().alter_table(msTbl.getDbName(), msTbl.getTableName(), msTbl);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    } finally {
        msClient.release();
        catalog_.updateLastDdlTime(new TTableName(msTbl.getDbName(), msTbl.getTableName()), lastDdlTime);
    }
}
#method_after
private void applyAlterTable(org.apache.hadoop.hive.metastore.api.Table msTbl) throws ImpalaRuntimeException {
    long lastDdlTime = -1;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        lastDdlTime = calculateDdlTime(msTbl);
        msTbl.putToParameters("transient_lastDdlTime", Long.toString(lastDdlTime));
        msClient.getHiveClient().alter_table(msTbl.getDbName(), msTbl.getTableName(), msTbl);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_table"), e);
    } finally {
        catalog_.updateLastDdlTime(new TTableName(msTbl.getDbName(), msTbl.getTableName()), lastDdlTime);
    }
}
#end_block

#method_before
private void applyAlterPartition(Table tbl, HdfsPartition partition) throws ImpalaException {
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        TableName tableName = tbl.getTableName();
        msClient.getHiveClient().alter_partition(tableName.getDb(), tableName.getTbl(), partition.toHmsPartition());
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        updateLastDdlTime(msTbl, msClient);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partition"), e);
    } finally {
        msClient.release();
    }
}
#method_after
private void applyAlterPartition(Table tbl, HdfsPartition partition) throws ImpalaException {
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        TableName tableName = tbl.getTableName();
        msClient.getHiveClient().alter_partition(tableName.getDb(), tableName.getTbl(), partition.toHmsPartition());
        org.apache.hadoop.hive.metastore.api.Table msTbl = tbl.getMetaStoreTable().deepCopy();
        updateLastDdlTime(msTbl, msClient);
    } catch (TException e) {
        throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partition"), e);
    }
}
#end_block

#method_before
private void bulkAlterPartitions(String dbName, String tableName, List<HdfsPartition> modifiedParts) throws ImpalaException {
    List<org.apache.hadoop.hive.metastore.api.Partition> hmsPartitions = Lists.newArrayList();
    for (HdfsPartition p : modifiedParts) {
        org.apache.hadoop.hive.metastore.api.Partition msPart = p.toHmsPartition();
        if (msPart != null)
            hmsPartitions.add(msPart);
    }
    if (hmsPartitions.size() == 0)
        return;
    MetaStoreClient msClient = catalog_.getMetaStoreClient();
    try {
        // Apply the updates in batches of 'MAX_PARTITION_UPDATES_PER_RPC'.
        for (int i = 0; i < hmsPartitions.size(); i += MAX_PARTITION_UPDATES_PER_RPC) {
            int numPartitionsToUpdate = Math.min(i + MAX_PARTITION_UPDATES_PER_RPC, hmsPartitions.size());
            try {
                // Alter partitions in bulk.
                msClient.getHiveClient().alter_partitions(dbName, tableName, hmsPartitions.subList(i, numPartitionsToUpdate));
                // Mark the corresponding HdfsPartition objects as dirty
                for (org.apache.hadoop.hive.metastore.api.Partition msPartition : hmsPartitions.subList(i, numPartitionsToUpdate)) {
                    try {
                        catalog_.getHdfsPartition(dbName, tableName, msPartition).markDirty();
                    } catch (PartitionNotFoundException e) {
                        LOG.error(String.format("Partition of table %s could not be found: %s", tableName, e.getMessage()));
                        continue;
                    }
                }
            } catch (TException e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partitions"), e);
            }
        }
    } finally {
        msClient.release();
    }
}
#method_after
private void bulkAlterPartitions(String dbName, String tableName, List<HdfsPartition> modifiedParts) throws ImpalaException {
    List<org.apache.hadoop.hive.metastore.api.Partition> hmsPartitions = Lists.newArrayList();
    for (HdfsPartition p : modifiedParts) {
        org.apache.hadoop.hive.metastore.api.Partition msPart = p.toHmsPartition();
        if (msPart != null)
            hmsPartitions.add(msPart);
    }
    if (hmsPartitions.size() == 0)
        return;
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        // Apply the updates in batches of 'MAX_PARTITION_UPDATES_PER_RPC'.
        for (int i = 0; i < hmsPartitions.size(); i += MAX_PARTITION_UPDATES_PER_RPC) {
            int numPartitionsToUpdate = Math.min(i + MAX_PARTITION_UPDATES_PER_RPC, hmsPartitions.size());
            try {
                // Alter partitions in bulk.
                msClient.getHiveClient().alter_partitions(dbName, tableName, hmsPartitions.subList(i, numPartitionsToUpdate));
                // Mark the corresponding HdfsPartition objects as dirty
                for (org.apache.hadoop.hive.metastore.api.Partition msPartition : hmsPartitions.subList(i, numPartitionsToUpdate)) {
                    try {
                        catalog_.getHdfsPartition(dbName, tableName, msPartition).markDirty();
                    } catch (PartitionNotFoundException e) {
                        LOG.error(String.format("Partition of table %s could not be found: %s", tableName, e.getMessage()));
                        continue;
                    }
                }
            } catch (TException e) {
                throw new ImpalaRuntimeException(String.format(HMS_RPC_ERROR_FORMAT_STR, "alter_partitions"), e);
            }
        }
    }
}
#end_block

#method_before
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    catalog_.getLock().writeLock().lock();
    synchronized (table) {
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        catalog_.getLock().writeLock().unlock();
        // Collects the cache directive IDs of any cached table/partitions that were
        // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
        // and the table will be refreshed asynchronously after all cache directives
        // complete.
        List<Long> cacheDirIds = Lists.<Long>newArrayList();
        // If the table is cached, get its cache pool name and replication factor. New
        // partitions will inherit this property.
        Pair<String, Short> cacheInfo = table.getTableCacheInfo(cacheDirIds);
        String cachePoolName = cacheInfo.first;
        Short cacheReplication = cacheInfo.second;
        TableName tblName = new TableName(table.getDb().getName(), table.getName());
        List<String> errorMessages = Lists.newArrayList();
        HashSet<String> partsToLoadMetadata = null;
        if (table.getNumClusteringCols() > 0) {
            // Set of all partition names targeted by the insert that need to be created
            // in the Metastore (partitions that do not currently exist in the catalog).
            // In the BE, we don't currently distinguish between which targeted partitions
            // are new and which already exist, so initialize the set with all targeted
            // partition names and remove the ones that are found to exist.
            HashSet<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
            partsToLoadMetadata = Sets.newHashSet(partsToCreate);
            for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
                // Skip dummy default partition.
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                // TODO: In the BE we build partition names without a trailing char. In FE we
                // build partition name with a trailing char. We should make this consistent.
                String partName = partition.getPartitionName() + "/";
                // returns true, it indicates the partition already exists.
                if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                    // The partition was targeted by the insert and is also a cached. Since data
                    // was written to the partition, a watch needs to be placed on the cache
                    // cache directive so the TableLoadingMgr can perform an async refresh once
                    // all data becomes cached.
                    cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
                }
                if (partsToCreate.size() == 0)
                    break;
            }
            if (!partsToCreate.isEmpty()) {
                MetaStoreClient msClient = catalog_.getMetaStoreClient();
                try {
                    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
                    List<org.apache.hadoop.hive.metastore.api.Partition> hmsParts = Lists.newArrayList();
                    HiveConf hiveConf = new HiveConf(this.getClass());
                    Warehouse warehouse = new Warehouse(hiveConf);
                    for (String partName : partsToCreate) {
                        org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
                        hmsParts.add(partition);
                        partition.setDbName(tblName.getDb());
                        partition.setTableName(tblName.getTbl());
                        partition.setValues(getPartValsFromName(msTbl, partName));
                        partition.setParameters(new HashMap<String, String>());
                        partition.setSd(msTbl.getSd().deepCopy());
                        partition.getSd().setSerdeInfo(msTbl.getSd().getSerdeInfo().deepCopy());
                        partition.getSd().setLocation(msTbl.getSd().getLocation() + "/" + partName.substring(0, partName.length() - 1));
                        MetaStoreUtils.updatePartitionStatsFast(partition, warehouse);
                    }
                    // First add_partitions and then alter_partitions the successful ones with
                    // caching directives. The reason is that some partitions could have been
                    // added concurrently, and we want to avoid caching a partition twice and
                    // leaking a caching directive.
                    List<org.apache.hadoop.hive.metastore.api.Partition> addedHmsParts = msClient.getHiveClient().add_partitions(hmsParts, true, true);
                    if (addedHmsParts.size() > 0) {
                        if (cachePoolName != null) {
                            List<org.apache.hadoop.hive.metastore.api.Partition> cachedHmsParts = Lists.newArrayList();
                            // the directive id.
                            for (org.apache.hadoop.hive.metastore.api.Partition part : addedHmsParts) {
                                try {
                                    cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(part, cachePoolName, cacheReplication));
                                    cachedHmsParts.add(part);
                                } catch (ImpalaRuntimeException e) {
                                    String msg = String.format("Partition %s.%s(%s): State: Not " + "cached. Action: Cache manully via 'ALTER TABLE'.", part.getDbName(), part.getTableName(), part.getValues());
                                    LOG.error(msg, e);
                                    errorMessages.add(msg);
                                }
                            }
                            try {
                                msClient.getHiveClient().alter_partitions(tblName.getDb(), tblName.getTbl(), cachedHmsParts);
                            } catch (Exception e) {
                                LOG.error("Failed in alter_partitions: ", e);
                                // Try to uncache the partitions when the alteration in the HMS failed.
                                for (org.apache.hadoop.hive.metastore.api.Partition part : cachedHmsParts) {
                                    try {
                                        HdfsCachingUtil.uncachePartition(part);
                                    } catch (ImpalaException e1) {
                                        String msg = String.format("Partition %s.%s(%s): State: Leaked caching directive. " + "Action: Manually uncache directory %s via hdfs cacheAdmin.", part.getDbName(), part.getTableName(), part.getValues(), part.getSd().getLocation());
                                        LOG.error(msg, e);
                                        errorMessages.add(msg);
                                    }
                                }
                            }
                        }
                        updateLastDdlTime(msTbl, msClient);
                    }
                } catch (AlreadyExistsException e) {
                    throw new InternalException("AlreadyExistsException thrown although ifNotExists given", e);
                } catch (Exception e) {
                    throw new InternalException("Error adding partitions", e);
                } finally {
                    msClient.release();
                }
            }
        }
        // Submit the watch request for the given cache directives.
        if (!cacheDirIds.isEmpty()) {
            catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
        }
        response.setResult(new TCatalogUpdateResult());
        response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
        if (errorMessages.size() > 0) {
            errorMessages.add("Please refer to the catalogd error log for details " + "regarding the failed un/caching operations.");
            response.getResult().setStatus(new TStatus(TErrorCode.INTERNAL_ERROR, errorMessages));
        } else {
            response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
        }
        loadTableMetadata(table, newCatalogVersion, true, false, partsToLoadMetadata);
        addTableToCatalogUpdate(table, response.result);
    }
    // end of synchronized block
    return response;
}
#method_after
public TUpdateCatalogResponse updateCatalog(TUpdateCatalogRequest update) throws ImpalaException {
    TUpdateCatalogResponse response = new TUpdateCatalogResponse();
    // Only update metastore for Hdfs tables.
    Table table = getExistingTable(update.getDb_name(), update.getTarget_table());
    if (!(table instanceof HdfsTable)) {
        throw new InternalException("Unexpected table type: " + update.getTarget_table());
    }
    catalog_.getLock().writeLock().lock();
    synchronized (table) {
        long newCatalogVersion = catalog_.incrementAndGetCatalogVersion();
        catalog_.getLock().writeLock().unlock();
        // Collects the cache directive IDs of any cached table/partitions that were
        // targeted. A watch on these cache directives is submitted to the TableLoadingMgr
        // and the table will be refreshed asynchronously after all cache directives
        // complete.
        List<Long> cacheDirIds = Lists.<Long>newArrayList();
        // If the table is cached, get its cache pool name and replication factor. New
        // partitions will inherit this property.
        Pair<String, Short> cacheInfo = table.getTableCacheInfo(cacheDirIds);
        String cachePoolName = cacheInfo.first;
        Short cacheReplication = cacheInfo.second;
        TableName tblName = new TableName(table.getDb().getName(), table.getName());
        List<String> errorMessages = Lists.newArrayList();
        HashSet<String> partsToLoadMetadata = null;
        if (table.getNumClusteringCols() > 0) {
            // Set of all partition names targeted by the insert that need to be created
            // in the Metastore (partitions that do not currently exist in the catalog).
            // In the BE, we don't currently distinguish between which targeted partitions
            // are new and which already exist, so initialize the set with all targeted
            // partition names and remove the ones that are found to exist.
            HashSet<String> partsToCreate = Sets.newHashSet(update.getCreated_partitions());
            partsToLoadMetadata = Sets.newHashSet(partsToCreate);
            for (HdfsPartition partition : ((HdfsTable) table).getPartitions()) {
                // Skip dummy default partition.
                if (partition.getId() == ImpalaInternalServiceConstants.DEFAULT_PARTITION_ID) {
                    continue;
                }
                // TODO: In the BE we build partition names without a trailing char. In FE we
                // build partition name with a trailing char. We should make this consistent.
                String partName = partition.getPartitionName() + "/";
                // returns true, it indicates the partition already exists.
                if (partsToCreate.remove(partName) && partition.isMarkedCached()) {
                    // The partition was targeted by the insert and is also a cached. Since data
                    // was written to the partition, a watch needs to be placed on the cache
                    // cache directive so the TableLoadingMgr can perform an async refresh once
                    // all data becomes cached.
                    cacheDirIds.add(HdfsCachingUtil.getCacheDirectiveId(partition.getParameters()));
                }
                if (partsToCreate.size() == 0)
                    break;
            }
            if (!partsToCreate.isEmpty()) {
                try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
                    org.apache.hadoop.hive.metastore.api.Table msTbl = table.getMetaStoreTable().deepCopy();
                    List<org.apache.hadoop.hive.metastore.api.Partition> hmsParts = Lists.newArrayList();
                    HiveConf hiveConf = new HiveConf(this.getClass());
                    Warehouse warehouse = new Warehouse(hiveConf);
                    for (String partName : partsToCreate) {
                        org.apache.hadoop.hive.metastore.api.Partition partition = new org.apache.hadoop.hive.metastore.api.Partition();
                        hmsParts.add(partition);
                        partition.setDbName(tblName.getDb());
                        partition.setTableName(tblName.getTbl());
                        partition.setValues(getPartValsFromName(msTbl, partName));
                        partition.setParameters(new HashMap<String, String>());
                        partition.setSd(msTbl.getSd().deepCopy());
                        partition.getSd().setSerdeInfo(msTbl.getSd().getSerdeInfo().deepCopy());
                        partition.getSd().setLocation(msTbl.getSd().getLocation() + "/" + partName.substring(0, partName.length() - 1));
                        MetaStoreUtils.updatePartitionStatsFast(partition, warehouse);
                    }
                    // First add_partitions and then alter_partitions the successful ones with
                    // caching directives. The reason is that some partitions could have been
                    // added concurrently, and we want to avoid caching a partition twice and
                    // leaking a caching directive.
                    List<org.apache.hadoop.hive.metastore.api.Partition> addedHmsParts = msClient.getHiveClient().add_partitions(hmsParts, true, true);
                    if (addedHmsParts.size() > 0) {
                        if (cachePoolName != null) {
                            List<org.apache.hadoop.hive.metastore.api.Partition> cachedHmsParts = Lists.newArrayList();
                            // the directive id.
                            for (org.apache.hadoop.hive.metastore.api.Partition part : addedHmsParts) {
                                try {
                                    cacheDirIds.add(HdfsCachingUtil.submitCachePartitionDirective(part, cachePoolName, cacheReplication));
                                    cachedHmsParts.add(part);
                                } catch (ImpalaRuntimeException e) {
                                    String msg = String.format("Partition %s.%s(%s): State: Not " + "cached. Action: Cache manully via 'ALTER TABLE'.", part.getDbName(), part.getTableName(), part.getValues());
                                    LOG.error(msg, e);
                                    errorMessages.add(msg);
                                }
                            }
                            try {
                                msClient.getHiveClient().alter_partitions(tblName.getDb(), tblName.getTbl(), cachedHmsParts);
                            } catch (Exception e) {
                                LOG.error("Failed in alter_partitions: ", e);
                                // Try to uncache the partitions when the alteration in the HMS failed.
                                for (org.apache.hadoop.hive.metastore.api.Partition part : cachedHmsParts) {
                                    try {
                                        HdfsCachingUtil.uncachePartition(part);
                                    } catch (ImpalaException e1) {
                                        String msg = String.format("Partition %s.%s(%s): State: Leaked caching directive. " + "Action: Manually uncache directory %s via hdfs cacheAdmin.", part.getDbName(), part.getTableName(), part.getValues(), part.getSd().getLocation());
                                        LOG.error(msg, e);
                                        errorMessages.add(msg);
                                    }
                                }
                            }
                        }
                        updateLastDdlTime(msTbl, msClient);
                    }
                } catch (AlreadyExistsException e) {
                    throw new InternalException("AlreadyExistsException thrown although ifNotExists given", e);
                } catch (Exception e) {
                    throw new InternalException("Error adding partitions", e);
                }
            }
        }
        // Submit the watch request for the given cache directives.
        if (!cacheDirIds.isEmpty()) {
            catalog_.watchCacheDirs(cacheDirIds, tblName.toThrift());
        }
        response.setResult(new TCatalogUpdateResult());
        response.getResult().setCatalog_service_id(JniCatalog.getServiceId());
        if (errorMessages.size() > 0) {
            errorMessages.add("Please refer to the catalogd error log for details " + "regarding the failed un/caching operations.");
            response.getResult().setStatus(new TStatus(TErrorCode.INTERNAL_ERROR, errorMessages));
        } else {
            response.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
        }
        loadTableMetadata(table, newCatalogVersion, true, false, partsToLoadMetadata);
        addTableToCatalogUpdate(table, response.result);
    }
    // end of synchronized block
    return response;
}
#end_block

#method_before
public static LiteralExpr create(String value, Type type) throws AnalysisException {
    Preconditions.checkArgument(type.isValid());
    LiteralExpr e = null;
    switch(type.getPrimitiveType()) {
        case NULL_TYPE:
            e = new NullLiteral();
            break;
        case BOOLEAN:
            e = new BoolLiteral(value);
            break;
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case FLOAT:
        case DOUBLE:
        case DECIMAL:
            e = new NumericLiteral(value, type);
            break;
        case STRING:
        case VARCHAR:
        case CHAR:
            e = new StringLiteral(value);
            break;
        case DATE:
        case DATETIME:
        case TIMESTAMP:
            // TODO: we support TIMESTAMP but no way to specify it in SQL.
            throw new AnalysisException("DATE/DATETIME/TIMESTAMP literals not supported: " + value);
        default:
            Preconditions.checkState(false, String.format("Literals of type '%s' not supported.", type.toSql()));
    }
    e.analyze(null);
    // can be parsed as tinyint but we need a bigint.
    return (LiteralExpr) e.uncheckedCastTo(type);
}
#method_after
public static LiteralExpr create(String value, Type type) throws AnalysisException {
    Preconditions.checkArgument(type.isValid());
    LiteralExpr e = null;
    switch(type.getPrimitiveType()) {
        case NULL_TYPE:
            e = new NullLiteral();
            break;
        case BOOLEAN:
            e = new BoolLiteral(value);
            break;
        case TINYINT:
        case SMALLINT:
        case INT:
        case BIGINT:
        case FLOAT:
        case DOUBLE:
        case DECIMAL:
            e = new NumericLiteral(value, type);
            break;
        case STRING:
        case VARCHAR:
        case CHAR:
            e = new StringLiteral(value);
            break;
        case DATE:
        case DATETIME:
        case TIMESTAMP:
            // TODO: we support TIMESTAMP but no way to specify it in SQL.
            return null;
        default:
            Preconditions.checkState(false, String.format("Literals of type '%s' not supported.", type.toSql()));
    }
    e.analyze(null);
    // can be parsed as tinyint but we need a bigint.
    return (LiteralExpr) e.uncheckedCastTo(type);
}
#end_block

#method_before
public static LiteralExpr create(Expr constExpr, TQueryCtx queryCtx) throws AnalysisException {
    Preconditions.checkState(constExpr.isConstant());
    Preconditions.checkState(constExpr.getType().isValid());
    if (constExpr instanceof LiteralExpr)
        return (LiteralExpr) constExpr;
    TColumnValue val = null;
    try {
        val = FeSupport.EvalConstExpr(constExpr, queryCtx);
    } catch (InternalException e) {
        throw new AnalysisException(String.format("Failed to evaluate expr '%s'", constExpr.toSql()), e);
    }
    LiteralExpr result = null;
    switch(constExpr.getType().getPrimitiveType()) {
        case NULL_TYPE:
            result = new NullLiteral();
            break;
        case BOOLEAN:
            if (val.isBool_val())
                result = new BoolLiteral(val.bool_val);
            break;
        case TINYINT:
            if (val.isSetByte_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.byte_val));
            }
            break;
        case SMALLINT:
            if (val.isSetShort_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.short_val));
            }
            break;
        case INT:
            if (val.isSetInt_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.int_val));
            }
            break;
        case BIGINT:
            if (val.isSetLong_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.long_val));
            }
            break;
        case FLOAT:
        case DOUBLE:
            if (val.isSetDouble_val()) {
                result = new NumericLiteral(new BigDecimal(val.double_val), constExpr.getType());
            }
            break;
        case DECIMAL:
            if (val.isSetString_val()) {
                result = new NumericLiteral(new BigDecimal(val.string_val), constExpr.getType());
            }
            break;
        case STRING:
        case VARCHAR:
        case CHAR:
            if (val.isSetString_val())
                result = new StringLiteral(val.string_val);
            break;
        case DATE:
        case DATETIME:
        case TIMESTAMP:
            throw new AnalysisException("DATE/DATETIME/TIMESTAMP literals not supported: " + constExpr.toSql());
        default:
            Preconditions.checkState(false, String.format("Literals of type '%s' not supported.", constExpr.getType().toSql()));
    }
    // None of the fields in the thrift struct were set indicating a NULL.
    if (result == null)
        result = new NullLiteral();
    result.analyze(null);
    return (LiteralExpr) result;
}
#method_after
public static LiteralExpr create(Expr constExpr, TQueryCtx queryCtx) throws AnalysisException {
    Preconditions.checkState(constExpr.isConstant());
    Preconditions.checkState(constExpr.getType().isValid());
    if (constExpr instanceof LiteralExpr)
        return (LiteralExpr) constExpr;
    TColumnValue val = null;
    try {
        val = FeSupport.EvalConstExpr(constExpr, queryCtx);
    } catch (InternalException e) {
        throw new AnalysisException(String.format("Failed to evaluate expr '%s'", constExpr.toSql()), e);
    }
    LiteralExpr result = null;
    switch(constExpr.getType().getPrimitiveType()) {
        case NULL_TYPE:
            result = new NullLiteral();
            break;
        case BOOLEAN:
            if (val.isBool_val())
                result = new BoolLiteral(val.bool_val);
            break;
        case TINYINT:
            if (val.isSetByte_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.byte_val));
            }
            break;
        case SMALLINT:
            if (val.isSetShort_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.short_val));
            }
            break;
        case INT:
            if (val.isSetInt_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.int_val));
            }
            break;
        case BIGINT:
            if (val.isSetLong_val()) {
                result = new NumericLiteral(BigDecimal.valueOf(val.long_val));
            }
            break;
        case FLOAT:
        case DOUBLE:
            if (val.isSetDouble_val()) {
                result = new NumericLiteral(new BigDecimal(val.double_val), constExpr.getType());
            }
            break;
        case DECIMAL:
            if (val.isSetString_val()) {
                result = new NumericLiteral(new BigDecimal(val.string_val), constExpr.getType());
            }
            break;
        case STRING:
        case VARCHAR:
        case CHAR:
            if (val.isSetString_val())
                result = new StringLiteral(val.string_val);
            break;
        case DATE:
        case DATETIME:
        case TIMESTAMP:
            return null;
        default:
            Preconditions.checkState(false, String.format("Literals of type '%s' not supported.", constExpr.getType().toSql()));
    }
    // None of the fields in the thrift struct were set indicating a NULL.
    if (result == null)
        result = new NullLiteral();
    result.analyze(null);
    return (LiteralExpr) result;
}
#end_block

#method_before
@SuppressWarnings("unchecked")
@Test
public void TestImplicitAndExplicitPaths() {
    // Check that there are no implicit field names for base tables.
    String[] implicitFieldNames = new String[] { Path.ARRAY_POS_FIELD_NAME, Path.ARRAY_ITEM_FIELD_NAME, Path.MAP_KEY_FIELD_NAME, Path.MAP_VALUE_FIELD_NAME };
    for (String field : implicitFieldNames) {
        AnalysisError(String.format("select %s from functional.alltypes", field), String.format("Could not resolve column/field reference: '%s'", field));
    }
    addTestDb("d", null);
    // Test array of scalars. Only explicit paths make sense.
    addTestTable("create table d.t1 (c array<int>)");
    testSlotRefPath("select item from d.t1.c", path(0, 0));
    testSlotRefPath("select pos from d.t1.c", path(0, 1));
    AnalysisError("select item.item from d.t1.c", "Could not resolve column/field reference: 'item.item'");
    AnalysisError("select item.pos from d.t1.c", "Could not resolve column/field reference: 'item.pos'");
    // Test star expansion.
    testStarPath("select * from d.t1.c", path(0, 0));
    testStarPath("select c.* from d.t1.c", path(0, 0));
    // Array of structs. No name conflicts with implicit fields. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t2 (c array<struct<f:int>>)");
    testSlotRefPath("select f from d.t2.c", path(0, 0, 0));
    testSlotRefPath("select item.f from d.t2.c", path(0, 0, 0));
    testSlotRefPath("select pos from d.t2.c", path(0, 1));
    AnalysisError("select item from d.t2.c", "Expr 'item' in select list returns a complex type 'STRUCT<f:INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("select item.pos from d.t2.c", "Could not resolve column/field reference: 'item.pos'");
    // Test star expansion.
    testStarPath("select * from d.t2.c", path(0, 0, 0));
    testStarPath("select c.* from d.t2.c", path(0, 0, 0));
    // Array of structs with name conflicts. Both implicit and explicit
    // paths are allowed.
    addTestTable("create table d.t3 (c array<struct<f:int,item:int,pos:int>>)");
    testSlotRefPath("select f from d.t3.c", path(0, 0, 0));
    testSlotRefPath("select item.f from d.t3.c", path(0, 0, 0));
    testSlotRefPath("select item.item from d.t3.c", path(0, 0, 1));
    testSlotRefPath("select item.pos from d.t3.c", path(0, 0, 2));
    testSlotRefPath("select pos from d.t3.c", path(0, 1));
    AnalysisError("select item from d.t3.c", "Expr 'item' in select list returns a complex type " + "'STRUCT<f:INT,item:INT,pos:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t3.c", path(0, 0, 0), path(0, 0, 1), path(0, 0, 2));
    testStarPath("select c.* from d.t3.c", path(0, 0, 0), path(0, 0, 1), path(0, 0, 2));
    // Map with a scalar key and value. Only implicit paths make sense.
    addTestTable("create table d.t4 (c map<int,string>)");
    testSlotRefPath("select key from d.t4.c", path(0, 0));
    testSlotRefPath("select value from d.t4.c", path(0, 1));
    AnalysisError("select value.value from d.t4.c", "Could not resolve column/field reference: 'value.value'");
    // Test star expansion.
    testStarPath("select * from d.t4.c", path(0, 0), path(0, 1));
    testStarPath("select c.* from d.t4.c", path(0, 0), path(0, 1));
    // Map with a scalar key and struct value. No name conflicts. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t5 (c map<int,struct<f:int>>)");
    testSlotRefPath("select key from d.t5.c", path(0, 0));
    testSlotRefPath("select f from d.t5.c", path(0, 1, 0));
    testSlotRefPath("select value.f from d.t5.c", path(0, 1, 0));
    AnalysisError("select value.value from d.t5.c", "Could not resolve column/field reference: 'value.value'");
    AnalysisError("select value from d.t5.c", "Expr 'value' in select list returns a complex type " + "'STRUCT<f:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t5.c", path(0, 0), path(0, 1, 0));
    testStarPath("select c.* from d.t5.c", path(0, 0), path(0, 1, 0));
    // Map with a scalar key and struct value with name conflicts. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t6 (c map<int,struct<f:int,key:int,value:int>>)");
    testSlotRefPath("select key from d.t6.c", path(0, 0));
    testSlotRefPath("select f from d.t6.c", path(0, 1, 0));
    testSlotRefPath("select value.f from d.t6.c", path(0, 1, 0));
    testSlotRefPath("select value.key from d.t6.c", path(0, 1, 1));
    testSlotRefPath("select value.value from d.t6.c", path(0, 1, 2));
    AnalysisError("select value from d.t6.c", "Expr 'value' in select list returns a complex type " + "'STRUCT<f:INT,key:INT,value:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t6.c", path(0, 0), path(0, 1, 0), path(0, 1, 1), path(0, 1, 2));
    testStarPath("select c.* from d.t6.c", path(0, 0), path(0, 1, 0), path(0, 1, 1), path(0, 1, 2));
    // Test implicit/explicit paths on a complicated schema.
    addTestTable("create table d.t7 (" + "c1 int, " + "c2 decimal(10, 4), " + "c3 array<struct<a1:array<int>,a2:array<struct<x:int,y:int,a3:array<int>>>>>, " + "c4 bigint, " + "c5 map<int,struct<m1:map<int,string>," + "                  m2:map<int,struct<x:int,y:int,m3:map<int,int>>>>>)");
    // Test paths with c3.
    testTableRefPath("select 1 from d.t7.c3.a1", path(2, 0, 0), null);
    testTableRefPath("select 1 from d.t7.c3.item.a1", path(2, 0, 0), null);
    testSlotRefPath("select item from d.t7.c3.a1", path(2, 0, 0, 0));
    testSlotRefPath("select item from d.t7.c3.item.a1", path(2, 0, 0, 0));
    testTableRefPath("select 1 from d.t7.c3.a2", path(2, 0, 1), null);
    testTableRefPath("select 1 from d.t7.c3.item.a2", path(2, 0, 1), null);
    testSlotRefPath("select x from d.t7.c3.a2", path(2, 0, 1, 0, 0));
    testSlotRefPath("select x from d.t7.c3.item.a2", path(2, 0, 1, 0, 0));
    testTableRefPath("select 1 from d.t7.c3.a2.a3", path(2, 0, 1, 0, 2), null);
    testTableRefPath("select 1 from d.t7.c3.item.a2.item.a3", path(2, 0, 1, 0, 2), null);
    testSlotRefPath("select item from d.t7.c3.a2.a3", path(2, 0, 1, 0, 2, 0));
    testSlotRefPath("select item from d.t7.c3.item.a2.item.a3", path(2, 0, 1, 0, 2, 0));
    // Test path assembly with multiple tuple descriptors.
    testTableRefPath("select 1 from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 2), path(2, 0, 1, 0, 2));
    testTableRefPath("select 1 from d.t7, t7.c3, c3.item.a2, a2.item.a3", path(2, 0, 1, 0, 2), path(2, 0, 1, 0, 2));
    testSlotRefPath("select y from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 1));
    testSlotRefPath("select y, x from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 0));
    testSlotRefPath("select x, y from d.t7, t7.c3.item.a2, a2.a3", path(2, 0, 1, 0, 1));
    testSlotRefPath("select a1.item from d.t7, t7.c3, c3.a1, c3.a2, a2.a3", path(2, 0, 0, 0));
    // Test materialized path.
    testTableRefPath("select 1 from d.t7, t7.c3.a1", path(2, 0, 0), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3.a2", path(2, 0, 1), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3.a2.a3", path(2, 0, 1, 0, 2), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3, c3.a2.a3", path(2, 0, 1, 0, 2), path(2, 0, 1));
    // Test paths with c5.
    testTableRefPath("select 1 from d.t7.c5.m1", path(4, 1, 0), null);
    testTableRefPath("select 1 from d.t7.c5.value.m1", path(4, 1, 0), null);
    testSlotRefPath("select key from d.t7.c5.m1", path(4, 1, 0, 0));
    testSlotRefPath("select key from d.t7.c5.value.m1", path(4, 1, 0, 0));
    testSlotRefPath("select value from d.t7.c5.m1", path(4, 1, 0, 1));
    testSlotRefPath("select value from d.t7.c5.value.m1", path(4, 1, 0, 1));
    testTableRefPath("select 1 from d.t7.c5.m2", path(4, 1, 1), null);
    testTableRefPath("select 1 from d.t7.c5.value.m2", path(4, 1, 1), null);
    testSlotRefPath("select key from d.t7.c5.m2", path(4, 1, 1, 0));
    testSlotRefPath("select key from d.t7.c5.value.m2", path(4, 1, 1, 0));
    testSlotRefPath("select x from d.t7.c5.m2", path(4, 1, 1, 1, 0));
    testSlotRefPath("select x from d.t7.c5.value.m2", path(4, 1, 1, 1, 0));
    testTableRefPath("select 1 from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2), null);
    testTableRefPath("select 1 from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2), null);
    testSlotRefPath("select key from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2, 0));
    testSlotRefPath("select key from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2, 0));
    testSlotRefPath("select value from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2, 1));
    testSlotRefPath("select value from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2, 1));
    // Test path assembly with multiple tuple descriptors.
    testTableRefPath("select 1 from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 2), path(4, 1, 1, 1, 2));
    testTableRefPath("select 1 from d.t7, t7.c5, c5.value.m2, m2.value.m3", path(4, 1, 1, 1, 2), path(4, 1, 1, 1, 2));
    testSlotRefPath("select y from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 1));
    testSlotRefPath("select y, x from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 0));
    testSlotRefPath("select x, y from d.t7, t7.c5.value.m2, m2.m3", path(4, 1, 1, 1, 1));
    testSlotRefPath("select m1.key from d.t7, t7.c5, c5.m1, c5.m2, m2.m3", path(4, 1, 0, 0));
    // Test materialized path.
    testTableRefPath("select 1 from d.t7, t7.c5.m1", path(4, 1, 0), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5.m2", path(4, 1, 1), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5.m2.m3", path(4, 1, 1, 1, 2), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5, c5.m2.m3", path(4, 1, 1, 1, 2), path(4, 1, 1));
    // Tests that an attempted implicit match must be succeeded by an explicit match.
    addTestTable("create table d.t8 (" + "s1 struct<" + "  s2:struct<" + "    a:array<" + "      array<struct<" + "        e:int,f:string>>>>>)");
    // Explanation of test:
    // - d.t8.s1.s2.a resolves to a CollectionStructType with fields 'item' and 'pos'
    // - we are allowed to implicitly skip the 'item' field
    // - d.t8.s1.s2.a.item again resolves to a CollectionStructType with 'item' and 'pos'
    // - however, we are not allowed to implicitly skip 'item' again, since we have
    // already skipped 'item' previously
    // - the rule is: an implicit match must be followed by an explicit one
    AnalysisError("select f from d.t8.s1.s2.a", "Could not resolve column/field reference: 'f'");
    AnalysisError("select 1 from d.t8.s1.s2.a, a.f", "Could not resolve table reference: 'a.f'");
}
#method_after
@SuppressWarnings("unchecked")
@Test
public void TestImplicitAndExplicitPaths() {
    // Check that there are no implicit field names for base tables.
    String[] implicitFieldNames = new String[] { Path.ARRAY_POS_FIELD_NAME, Path.ARRAY_ITEM_FIELD_NAME, Path.MAP_KEY_FIELD_NAME, Path.MAP_VALUE_FIELD_NAME };
    for (String field : implicitFieldNames) {
        AnalysisError(String.format("select %s from functional.alltypes", field), String.format("Could not resolve column/field reference: '%s'", field));
    }
    addTestDb("d", null);
    // Test array of scalars. Only explicit paths make sense.
    addTestTable("create table d.t1 (c array<int>)");
    testSlotRefPath("select item from d.t1.c", path(0, 0));
    testSlotRefPath("select pos from d.t1.c", path(0, 1));
    AnalysisError("select item.item from d.t1.c", "Could not resolve column/field reference: 'item.item'");
    AnalysisError("select item.pos from d.t1.c", "Could not resolve column/field reference: 'item.pos'");
    // Test star expansion.
    testStarPath("select * from d.t1.c", path(0, 0));
    testStarPath("select c.* from d.t1.c", path(0, 0));
    // Array of structs. No name conflicts with implicit fields. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t2 (c array<struct<f:int>>)");
    testSlotRefPath("select f from d.t2.c", path(0, 0, 0));
    testSlotRefPath("select item.f from d.t2.c", path(0, 0, 0));
    testSlotRefPath("select pos from d.t2.c", path(0, 1));
    AnalysisError("select item from d.t2.c", "Expr 'item' in select list returns a complex type 'STRUCT<f:INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("select item.pos from d.t2.c", "Could not resolve column/field reference: 'item.pos'");
    // Test star expansion.
    testStarPath("select * from d.t2.c", path(0, 0, 0));
    testStarPath("select c.* from d.t2.c", path(0, 0, 0));
    // Array of structs with name conflicts. Both implicit and explicit
    // paths are allowed.
    addTestTable("create table d.t3 (c array<struct<f:int,item:int,pos:int>>)");
    testSlotRefPath("select f from d.t3.c", path(0, 0, 0));
    testSlotRefPath("select item.f from d.t3.c", path(0, 0, 0));
    testSlotRefPath("select item.item from d.t3.c", path(0, 0, 1));
    testSlotRefPath("select item.pos from d.t3.c", path(0, 0, 2));
    testSlotRefPath("select pos from d.t3.c", path(0, 1));
    AnalysisError("select item from d.t3.c", "Expr 'item' in select list returns a complex type " + "'STRUCT<f:INT,item:INT,pos:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t3.c", path(0, 0, 0), path(0, 0, 1), path(0, 0, 2));
    testStarPath("select c.* from d.t3.c", path(0, 0, 0), path(0, 0, 1), path(0, 0, 2));
    // Map with a scalar key and value. Only implicit paths make sense.
    addTestTable("create table d.t4 (c map<int,string>)");
    testSlotRefPath("select key from d.t4.c", path(0, 0));
    testSlotRefPath("select value from d.t4.c", path(0, 1));
    AnalysisError("select value.value from d.t4.c", "Could not resolve column/field reference: 'value.value'");
    // Test star expansion.
    testStarPath("select * from d.t4.c", path(0, 0), path(0, 1));
    testStarPath("select c.* from d.t4.c", path(0, 0), path(0, 1));
    // Map with a scalar key and struct value. No name conflicts. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t5 (c map<int,struct<f:int>>)");
    testSlotRefPath("select key from d.t5.c", path(0, 0));
    testSlotRefPath("select f from d.t5.c", path(0, 1, 0));
    testSlotRefPath("select value.f from d.t5.c", path(0, 1, 0));
    AnalysisError("select value.value from d.t5.c", "Could not resolve column/field reference: 'value.value'");
    AnalysisError("select value from d.t5.c", "Expr 'value' in select list returns a complex type " + "'STRUCT<f:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t5.c", path(0, 0), path(0, 1, 0));
    testStarPath("select c.* from d.t5.c", path(0, 0), path(0, 1, 0));
    // Map with a scalar key and struct value with name conflicts. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t6 (c map<int,struct<f:int,key:int,value:int>>)");
    testSlotRefPath("select key from d.t6.c", path(0, 0));
    testSlotRefPath("select f from d.t6.c", path(0, 1, 0));
    testSlotRefPath("select value.f from d.t6.c", path(0, 1, 0));
    testSlotRefPath("select value.key from d.t6.c", path(0, 1, 1));
    testSlotRefPath("select value.value from d.t6.c", path(0, 1, 2));
    AnalysisError("select value from d.t6.c", "Expr 'value' in select list returns a complex type " + "'STRUCT<f:INT,key:INT,value:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t6.c", path(0, 0), path(0, 1, 0), path(0, 1, 1), path(0, 1, 2));
    testStarPath("select c.* from d.t6.c", path(0, 0), path(0, 1, 0), path(0, 1, 1), path(0, 1, 2));
    // Test implicit/explicit paths on a complicated schema.
    addTestTable("create table d.t7 (" + "c1 int, " + "c2 decimal(10, 4), " + "c3 array<struct<a1:array<int>,a2:array<struct<x:int,y:int,a3:array<int>>>>>, " + "c4 bigint, " + "c5 map<int,struct<m1:map<int,string>," + "                  m2:map<int,struct<x:int,y:int,m3:map<int,int>>>>>)");
    // Test paths with c3.
    testTableRefPath("select 1 from d.t7.c3.a1", path(2, 0, 0), null);
    testTableRefPath("select 1 from d.t7.c3.item.a1", path(2, 0, 0), null);
    testSlotRefPath("select item from d.t7.c3.a1", path(2, 0, 0, 0));
    testSlotRefPath("select item from d.t7.c3.item.a1", path(2, 0, 0, 0));
    testTableRefPath("select 1 from d.t7.c3.a2", path(2, 0, 1), null);
    testTableRefPath("select 1 from d.t7.c3.item.a2", path(2, 0, 1), null);
    testSlotRefPath("select x from d.t7.c3.a2", path(2, 0, 1, 0, 0));
    testSlotRefPath("select x from d.t7.c3.item.a2", path(2, 0, 1, 0, 0));
    testTableRefPath("select 1 from d.t7.c3.a2.a3", path(2, 0, 1, 0, 2), null);
    testTableRefPath("select 1 from d.t7.c3.item.a2.item.a3", path(2, 0, 1, 0, 2), null);
    testSlotRefPath("select item from d.t7.c3.a2.a3", path(2, 0, 1, 0, 2, 0));
    testSlotRefPath("select item from d.t7.c3.item.a2.item.a3", path(2, 0, 1, 0, 2, 0));
    // Test path assembly with multiple tuple descriptors.
    testTableRefPath("select 1 from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 2), path(2, 0, 1, 0, 2));
    testTableRefPath("select 1 from d.t7, t7.c3, c3.item.a2, a2.item.a3", path(2, 0, 1, 0, 2), path(2, 0, 1, 0, 2));
    testSlotRefPath("select y from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 1));
    testSlotRefPath("select y, x from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 0));
    testSlotRefPath("select x, y from d.t7, t7.c3.item.a2, a2.a3", path(2, 0, 1, 0, 1));
    testSlotRefPath("select a1.item from d.t7, t7.c3, c3.a1, c3.a2, a2.a3", path(2, 0, 0, 0));
    // Test materialized path.
    testTableRefPath("select 1 from d.t7, t7.c3.a1", path(2, 0, 0), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3.a2", path(2, 0, 1), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3.a2.a3", path(2, 0, 1, 0, 2), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3, c3.a2.a3", path(2, 0, 1, 0, 2), path(2, 0, 1));
    // Test paths with c5.
    testTableRefPath("select 1 from d.t7.c5.m1", path(4, 1, 0), null);
    testTableRefPath("select 1 from d.t7.c5.value.m1", path(4, 1, 0), null);
    testSlotRefPath("select key from d.t7.c5.m1", path(4, 1, 0, 0));
    testSlotRefPath("select key from d.t7.c5.value.m1", path(4, 1, 0, 0));
    testSlotRefPath("select value from d.t7.c5.m1", path(4, 1, 0, 1));
    testSlotRefPath("select value from d.t7.c5.value.m1", path(4, 1, 0, 1));
    testTableRefPath("select 1 from d.t7.c5.m2", path(4, 1, 1), null);
    testTableRefPath("select 1 from d.t7.c5.value.m2", path(4, 1, 1), null);
    testSlotRefPath("select key from d.t7.c5.m2", path(4, 1, 1, 0));
    testSlotRefPath("select key from d.t7.c5.value.m2", path(4, 1, 1, 0));
    testSlotRefPath("select x from d.t7.c5.m2", path(4, 1, 1, 1, 0));
    testSlotRefPath("select x from d.t7.c5.value.m2", path(4, 1, 1, 1, 0));
    testTableRefPath("select 1 from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2), null);
    testTableRefPath("select 1 from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2), null);
    testSlotRefPath("select key from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2, 0));
    testSlotRefPath("select key from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2, 0));
    testSlotRefPath("select value from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2, 1));
    testSlotRefPath("select value from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2, 1));
    // Test path assembly with multiple tuple descriptors.
    testTableRefPath("select 1 from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 2), path(4, 1, 1, 1, 2));
    testTableRefPath("select 1 from d.t7, t7.c5, c5.value.m2, m2.value.m3", path(4, 1, 1, 1, 2), path(4, 1, 1, 1, 2));
    testSlotRefPath("select y from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 1));
    testSlotRefPath("select y, x from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 0));
    testSlotRefPath("select x, y from d.t7, t7.c5.value.m2, m2.m3", path(4, 1, 1, 1, 1));
    testSlotRefPath("select m1.key from d.t7, t7.c5, c5.m1, c5.m2, m2.m3", path(4, 1, 0, 0));
    // Test materialized path.
    testTableRefPath("select 1 from d.t7, t7.c5.m1", path(4, 1, 0), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5.m2", path(4, 1, 1), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5.m2.m3", path(4, 1, 1, 1, 2), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5, c5.m2.m3", path(4, 1, 1, 1, 2), path(4, 1, 1));
    // Tests that implicit references are not allowed through collection types.
    addTestTable("create table d.t8 (" + "c1 array<map<string, string>>," + "c2 map<string, array<struct<a:int>>>," + "c3 struct<s1:struct<a:array<array<struct<e:int, f:string>>>>>)");
    testImplicitPathFailure("d.t8", true, "c1", "key", "value");
    testImplicitPathFailure("d.t8", true, "c2", "pos");
    testImplicitPathFailure("d.t8.c3.s1", false, "a", "f");
}
#end_block

#method_before
@Test
public void TestInsertHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test plan hints for partitioned Hdfs tables.
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle%s select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into table functional.alltypessmall " + "partition (year, month) %snoshuffle%s select * from functional.alltypes", prefix, suffix));
        // Only warn on unrecognized hints.
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sbadhint%s select * from functional.alltypes", prefix, suffix), "INSERT hint not recognized: badhint");
        // Plan hints require a partition clause.
        AnalysisError(String.format("insert into table functional.alltypesnopart %sshuffle%s " + "select * from functional.alltypesnopart", prefix, suffix), "INSERT hints are only supported for inserting into partitioned Hdfs tables.");
        // Plan hints do not make sense for inserting into HBase tables.
        AnalysisError(String.format("insert into table functional_hbase.alltypes %sshuffle%s " + "select * from functional_hbase.alltypes", prefix, suffix), "INSERT hints are only supported for inserting into partitioned Hdfs tables.");
        // Conflicting plan hints.
        AnalysisError("insert into table functional.alltypessmall " + "partition (year, month) /* +shuffle,noshuffle */ " + "select * from functional.alltypes", "Conflicting INSERT hint: noshuffle");
    }
    // Multiple non-conflicting hints and case insensitivity of hints.
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) /* +shuffle, ShUfFlE */ " + "select * from functional.alltypes");
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) [shuffle, ShUfFlE] " + "select * from functional.alltypes");
}
#method_after
@Test
public void TestInsertHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test plan hints for partitioned Hdfs tables.
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sshuffle%s select * from functional.alltypes", prefix, suffix));
        AnalyzesOk(String.format("insert into table functional.alltypessmall " + "partition (year, month) %snoshuffle%s select * from functional.alltypes", prefix, suffix));
        // Only warn on unrecognized hints.
        AnalyzesOk(String.format("insert into functional.alltypessmall " + "partition (year, month) %sbadhint%s select * from functional.alltypes", prefix, suffix), "INSERT hint not recognized: badhint");
        // Insert hints are ok for unpartitioned tables.
        AnalyzesOk(String.format("insert into table functional.alltypesnopart %sshuffle%s " + "select * from functional.alltypesnopart", prefix, suffix));
        // Plan hints do not make sense for inserting into HBase tables.
        AnalysisError(String.format("insert into table functional_hbase.alltypes %sshuffle%s " + "select * from functional_hbase.alltypes", prefix, suffix), "INSERT hints are only supported for inserting into Hdfs tables.");
        // Conflicting plan hints.
        AnalysisError("insert into table functional.alltypessmall " + "partition (year, month) /* +shuffle,noshuffle */ " + "select * from functional.alltypes", "Conflicting INSERT hint: noshuffle");
    }
    // Multiple non-conflicting hints and case insensitivity of hints.
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) /* +shuffle, ShUfFlE */ " + "select * from functional.alltypes");
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) [shuffle, ShUfFlE] " + "select * from functional.alltypes");
}
#end_block

#method_before
@Test
public void TestInsert() throws AnalysisException {
    for (String qualifier : ImmutableList.of("INTO", "OVERWRITE")) {
        testInsertStatic(qualifier);
        testInsertDynamic(qualifier);
        testInsertUnpartitioned(qualifier);
        testInsertWithPermutation(qualifier);
    }
    // Test INSERT into a table that Impala does not have WRITE access to.
    AnalysisError("insert into functional_seq.alltypes partition(year, month)" + "select * from functional.alltypes", "Unable to INSERT into target table (functional_seq.alltypes) because Impala " + "does not have WRITE access to at least one HDFS path: " + "hdfs://localhost:20500/test-warehouse/alltypes_seq/year=2009/month=");
    // Insert with a correlated inline view.
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month)" + "select a.id, bool_col, tinyint_col, smallint_col, item, bigint_col, " + "float_col, double_col, date_string_col, string_col, timestamp_col, a.year, " + "b.month from functional.alltypes a, functional.allcomplextypes b, " + "(select item from b.int_array_col) v1 " + "where a.id = b.id");
    AnalysisError("insert into table functional.alltypessmall " + "partition (year, month)" + "select a.id, a.bool_col, a.tinyint_col, a.smallint_col, item, a.bigint_col, " + "a.float_col, a.double_col, a.date_string_col, a.string_col, a.timestamp_col, " + "a.year, b.month from functional.alltypes a, functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypestiny) v1 " + "where a.id = b.id", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypestiny':\n" + "SELECT item FROM b.int_array_col, functional.alltypestiny");
    // Test plan hints for partitioned Hdfs tables.
    AnalyzesOk("insert into functional.alltypessmall " + "partition (year, month) [shuffle] select * from functional.alltypes");
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) [noshuffle] select * from functional.alltypes");
    // Multiple non-conflicting hints and case insensitivity of hints.
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month) [shuffle, ShUfFlE] select * from functional.alltypes");
    // Unknown plan hint. Expect a warning but no error.
    AnalyzesOk("insert into functional.alltypessmall " + "partition (year, month) [badhint] select * from functional.alltypes", "INSERT hint not recognized: badhint");
    // Conflicting plan hints.
    AnalysisError("insert into table functional.alltypessmall " + "partition (year, month) [shuffle, noshuffle] select * from functional.alltypes", "Conflicting INSERT hint: noshuffle");
    // Plan hints require a partition clause.
    AnalysisError("insert into table functional.alltypesnopart [shuffle] " + "select * from functional.alltypesnopart", "INSERT hints are only supported for inserting into partitioned Hdfs tables.");
    // Plan hints do not make sense for inserting into HBase tables.
    AnalysisError("insert into table functional_hbase.alltypes [shuffle] " + "select * from functional_hbase.alltypes", "INSERT hints are only supported for inserting into partitioned Hdfs tables.");
}
#method_after
@Test
public void TestInsert() throws AnalysisException {
    for (String qualifier : ImmutableList.of("INTO", "OVERWRITE")) {
        testInsertStatic(qualifier);
        testInsertDynamic(qualifier);
        testInsertUnpartitioned(qualifier);
        testInsertWithPermutation(qualifier);
    }
    // Test INSERT into a table that Impala does not have WRITE access to.
    AnalysisError("insert into functional_seq.alltypes partition(year, month)" + "select * from functional.alltypes", "Unable to INSERT into target table (functional_seq.alltypes) because Impala " + "does not have WRITE access to at least one HDFS path: " + "hdfs://localhost:20500/test-warehouse/alltypes_seq/year=2009/month=");
    // Insert with a correlated inline view.
    AnalyzesOk("insert into table functional.alltypessmall " + "partition (year, month)" + "select a.id, bool_col, tinyint_col, smallint_col, item, bigint_col, " + "float_col, double_col, date_string_col, string_col, timestamp_col, a.year, " + "b.month from functional.alltypes a, functional.allcomplextypes b, " + "(select item from b.int_array_col) v1 " + "where a.id = b.id");
    AnalysisError("insert into table functional.alltypessmall " + "partition (year, month)" + "select a.id, a.bool_col, a.tinyint_col, a.smallint_col, item, a.bigint_col, " + "a.float_col, a.double_col, a.date_string_col, a.string_col, a.timestamp_col, " + "a.year, b.month from functional.alltypes a, functional.allcomplextypes b, " + "(select item from b.int_array_col, functional.alltypestiny) v1 " + "where a.id = b.id", "Nested query is illegal because it contains a table reference " + "'b.int_array_col' correlated with an outer block as well as an " + "uncorrelated one 'functional.alltypestiny':\n" + "SELECT item FROM b.int_array_col, functional.alltypestiny");
}
#end_block

#method_before
public boolean isScanSlot() {
    return path_ != null;
}
#method_after
public boolean isScanSlot() {
    return path_ != null && path_.isRootedAtTable();
}
#end_block

#method_before
/**
 * Verifies that updating column stats data for a type that isn't compatible with
 * the column type results in the stats being set to "unknown". This is a regression
 * test for IMPALA-588, where this used to result in a Preconditions failure.
 */
// TODO: All Hive-stats related tests are temporarily disabled because of an unknown,
// sporadic issue causing stats of some columns to be absent in Jenkins runs.
// Investigate this issue further.
public void testColStatsColTypeMismatch() throws Exception {
    // First load a table that has column stats.
    // catalog_.refreshTable("functional", "alltypesagg", false);
    HdfsTable table = (HdfsTable) catalog_.getOrLoadTable("functional", "alltypesagg");
    // Now attempt to update a column's stats with mismatched stats data and ensure
    // we get the expected results.
    MetaStoreClient client = catalog_.getMetaStoreClient();
    try {
        // Load some string stats data and use it to update the stats of different
        // typed columns.
        ColumnStatisticsData stringColStatsData = client.getHiveClient().getTableColumnStatistics("functional", "alltypesagg", Lists.newArrayList("string_col")).get(0).getStatsData();
        assertTrue(!table.getColumn("int_col").updateStats(stringColStatsData));
        assertStatsUnknown(table.getColumn("int_col"));
        assertTrue(!table.getColumn("double_col").updateStats(stringColStatsData));
        assertStatsUnknown(table.getColumn("double_col"));
        assertTrue(!table.getColumn("bool_col").updateStats(stringColStatsData));
        assertStatsUnknown(table.getColumn("bool_col"));
        // Do the same thing, but apply bigint stats to a string column.
        ColumnStatisticsData bigIntCol = client.getHiveClient().getTableColumnStatistics("functional", "alltypes", Lists.newArrayList("bigint_col")).get(0).getStatsData();
        assertTrue(!table.getColumn("string_col").updateStats(bigIntCol));
        assertStatsUnknown(table.getColumn("string_col"));
        // Now try to apply a matching column stats data and ensure it succeeds.
        assertTrue(table.getColumn("string_col").updateStats(stringColStatsData));
        assertEquals(1178, table.getColumn("string_col").getStats().getNumDistinctValues());
    } finally {
        // Make sure to invalidate the metadata so the next test isn't using bad col stats
        // catalog_.refreshTable("functional", "alltypesagg", false);
        client.release();
    }
}
#method_after
/**
 * Verifies that updating column stats data for a type that isn't compatible with
 * the column type results in the stats being set to "unknown". This is a regression
 * test for IMPALA-588, where this used to result in a Preconditions failure.
 */
// TODO: All Hive-stats related tests are temporarily disabled because of an unknown,
// sporadic issue causing stats of some columns to be absent in Jenkins runs.
// Investigate this issue further.
public void testColStatsColTypeMismatch() throws Exception {
    // First load a table that has column stats.
    // catalog_.refreshTable("functional", "alltypesagg", false);
    HdfsTable table = (HdfsTable) catalog_.getOrLoadTable("functional", "alltypesagg");
    // we get the expected results.
    try (MetaStoreClient client = catalog_.getMetaStoreClient()) {
        // Load some string stats data and use it to update the stats of different
        // typed columns.
        ColumnStatisticsData stringColStatsData = client.getHiveClient().getTableColumnStatistics("functional", "alltypesagg", Lists.newArrayList("string_col")).get(0).getStatsData();
        assertTrue(!table.getColumn("int_col").updateStats(stringColStatsData));
        assertStatsUnknown(table.getColumn("int_col"));
        assertTrue(!table.getColumn("double_col").updateStats(stringColStatsData));
        assertStatsUnknown(table.getColumn("double_col"));
        assertTrue(!table.getColumn("bool_col").updateStats(stringColStatsData));
        assertStatsUnknown(table.getColumn("bool_col"));
        // Do the same thing, but apply bigint stats to a string column.
        ColumnStatisticsData bigIntCol = client.getHiveClient().getTableColumnStatistics("functional", "alltypes", Lists.newArrayList("bigint_col")).get(0).getStatsData();
        assertTrue(!table.getColumn("string_col").updateStats(bigIntCol));
        assertStatsUnknown(table.getColumn("string_col"));
        // Now try to apply a matching column stats data and ensure it succeeds.
        assertTrue(table.getColumn("string_col").updateStats(stringColStatsData));
        assertEquals(1178, table.getColumn("string_col").getStats().getNumDistinctValues());
    }
}
#end_block

#method_before
@Override
public void createTable() throws ImpalaRuntimeException {
    String kuduTableName = msTbl_.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String kuduMasters = msTbl_.getParameters().get(KuduTable.KEY_MASTER_ADDRESSES);
    // Can be optional for un-managed tables
    String kuduKeyCols = msTbl_.getParameters().get(KuduTable.KEY_KEY_COLUMNS);
    String replication = msTbl_.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    try (KuduClient client = new KuduClient.KuduClientBuilder(kuduMasters).build()) {
        // TODO should we throw if the table does not exist when its an external table?
        if (client.tableExists(kuduTableName)) {
            if (msTbl_.getTableType().equals(TableType.MANAGED_TABLE.toString())) {
                throw new ImpalaRuntimeException(String.format("Table %s already exists in Kudu master %s.", kuduTableName, kuduMasters));
            }
            // Check if the external table matches the schema
            org.kududb.client.KuduTable kuduTable = client.openTable(kuduTableName);
            if (!compareSchema(msTbl_, kuduTable)) {
                throw new ImpalaRuntimeException(String.format("Table %s (%s) has a different schema in Kudu than in Hive.", msTbl_.getTableName(), kuduTableName));
            }
            return;
        }
        HashSet<String> keyColNames = parseKeyColumns(kuduKeyCols);
        List<ColumnSchema> keyColSchemas = new ArrayList<>();
        // Create a new Schema and map the types accordingly
        ArrayList<ColumnSchema> columns = Lists.newArrayList();
        for (FieldSchema fieldSchema : msTbl_.getSd().getCols()) {
            org.apache.impala.catalog.Type catalogType = org.apache.impala.catalog.Type.parseColumnType(fieldSchema.getType());
            if (catalogType == null) {
                throw new ImpalaRuntimeException(String.format("Could not parse column type %s.", fieldSchema.getType()));
            }
            Type t = fromImpalaType(catalogType);
            // Create the actual column and check if the column is a key column
            ColumnSchemaBuilder csb = new ColumnSchemaBuilder(fieldSchema.getName(), t);
            boolean isKeyColumn = keyColNames.contains(fieldSchema.getName());
            csb.key(isKeyColumn);
            csb.nullable(!isKeyColumn);
            ColumnSchema cs = csb.build();
            columns.add(cs);
            if (isKeyColumn)
                keyColSchemas.add(cs);
        }
        Schema schema = new Schema(columns);
        CreateTableOptions cto = new CreateTableOptions();
        // Handle auto-partitioning of the Kudu table
        if (distributeParams_ != null) {
            for (TDistributeParam param : distributeParams_) {
                if (param.isSetBy_hash_param()) {
                    Preconditions.checkState(!param.isSetBy_range_param());
                    cto.addHashPartitions(param.getBy_hash_param().getColumns(), param.getBy_hash_param().getNum_buckets());
                } else {
                    Preconditions.checkState(param.isSetBy_range_param());
                    cto.setRangePartitionColumns(param.getBy_range_param().getColumns());
                    for (PartialRow p : KuduUtil.parseSplits(schema, param.getBy_range_param())) {
                        cto.addSplitRow(p);
                    }
                }
            }
        }
        if (!Strings.isNullOrEmpty(replication)) {
            int r = Integer.parseInt(replication);
            if (r <= 0) {
                throw new ImpalaRuntimeException("Number of tablet replicas must be greater than zero. " + "Given number of replicas is: " + Integer.toString(r));
            }
            cto.setNumReplicas(r);
        }
        client.createTable(kuduTableName, schema, cto);
    } catch (ImpalaRuntimeException e) {
        throw e;
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Error creating Kudu table", e);
    }
}
#method_after
@Override
public void createTable() throws ImpalaRuntimeException {
    String kuduTableName = msTbl_.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String kuduMasters = msTbl_.getParameters().get(KuduTable.KEY_MASTER_ADDRESSES);
    // Can be optional for un-managed tables
    String kuduKeyCols = msTbl_.getParameters().get(KuduTable.KEY_KEY_COLUMNS);
    String replication = msTbl_.getParameters().get(KuduTable.KEY_TABLET_REPLICAS);
    try (KuduClient client = new KuduClient.KuduClientBuilder(kuduMasters).build()) {
        // TODO should we throw if the table does not exist when its an external table?
        if (client.tableExists(kuduTableName)) {
            if (msTbl_.getTableType().equals(TableType.MANAGED_TABLE.toString())) {
                throw new ImpalaRuntimeException(String.format("Table %s already exists in Kudu master %s.", kuduTableName, kuduMasters));
            }
            // Check if the external table matches the schema
            org.apache.kudu.client.KuduTable kuduTable = client.openTable(kuduTableName);
            if (!compareSchema(msTbl_, kuduTable)) {
                throw new ImpalaRuntimeException(String.format("Table %s (%s) has a different schema in Kudu than in Hive.", msTbl_.getTableName(), kuduTableName));
            }
            return;
        }
        HashSet<String> keyColNames = parseKeyColumns(kuduKeyCols);
        List<ColumnSchema> keyColSchemas = new ArrayList<>();
        // Create a new Schema and map the types accordingly
        ArrayList<ColumnSchema> columns = Lists.newArrayList();
        for (FieldSchema fieldSchema : msTbl_.getSd().getCols()) {
            org.apache.impala.catalog.Type catalogType = org.apache.impala.catalog.Type.parseColumnType(fieldSchema.getType());
            if (catalogType == null) {
                throw new ImpalaRuntimeException(String.format("Could not parse column type %s.", fieldSchema.getType()));
            }
            Type t = fromImpalaType(catalogType);
            // Create the actual column and check if the column is a key column
            ColumnSchemaBuilder csb = new ColumnSchemaBuilder(fieldSchema.getName(), t);
            boolean isKeyColumn = keyColNames.contains(fieldSchema.getName());
            csb.key(isKeyColumn);
            csb.nullable(!isKeyColumn);
            ColumnSchema cs = csb.build();
            columns.add(cs);
            if (isKeyColumn)
                keyColSchemas.add(cs);
        }
        Schema schema = new Schema(columns);
        CreateTableOptions cto = new CreateTableOptions();
        // Handle auto-partitioning of the Kudu table
        if (distributeParams_ != null) {
            for (TDistributeParam param : distributeParams_) {
                if (param.isSetBy_hash_param()) {
                    Preconditions.checkState(!param.isSetBy_range_param());
                    cto.addHashPartitions(param.getBy_hash_param().getColumns(), param.getBy_hash_param().getNum_buckets());
                } else {
                    Preconditions.checkState(param.isSetBy_range_param());
                    cto.setRangePartitionColumns(param.getBy_range_param().getColumns());
                    for (PartialRow p : KuduUtil.parseSplits(schema, param.getBy_range_param())) {
                        cto.addSplitRow(p);
                    }
                }
            }
        }
        if (!Strings.isNullOrEmpty(replication)) {
            int r = Integer.parseInt(replication);
            if (r <= 0) {
                throw new ImpalaRuntimeException("Number of tablet replicas must be greater than zero. " + "Given number of replicas is: " + Integer.toString(r));
            }
            cto.setNumReplicas(r);
        }
        client.createTable(kuduTableName, schema, cto);
    } catch (ImpalaRuntimeException e) {
        throw e;
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Error creating Kudu table", e);
    }
}
#end_block

#method_before
@Override
public void dropTable() throws ImpalaRuntimeException {
    // If table is an external table, do not delete the data
    if (msTbl_.getTableType().equals(TableType.EXTERNAL_TABLE.toString()))
        return;
    String kuduTableName = msTbl_.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String kuduMasters = msTbl_.getParameters().get(KuduTable.KEY_MASTER_ADDRESSES);
    try (KuduClient client = new KuduClient.KuduClientBuilder(kuduMasters).build()) {
        if (!client.tableExists(kuduTableName)) {
            LOG.warn("Table: %s is in inconsistent state. It does not exist in Kudu master(s)" + " %s, but it exists in Hive metastore. Deleting from metastore only.", kuduTableName, kuduMasters);
            return;
        }
        client.deleteTable(kuduTableName);
        return;
    } catch (ImpalaRuntimeException e) {
        throw e;
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Error dropping Kudu table", e);
    }
}
#method_after
@Override
public void dropTable() throws ImpalaRuntimeException {
    // If table is an external table, do not delete the data
    if (msTbl_.getTableType().equals(TableType.EXTERNAL_TABLE.toString()))
        return;
    String kuduTableName = msTbl_.getParameters().get(KuduTable.KEY_TABLE_NAME);
    String kuduMasters = msTbl_.getParameters().get(KuduTable.KEY_MASTER_ADDRESSES);
    try (KuduClient client = new KuduClient.KuduClientBuilder(kuduMasters).build()) {
        if (!client.tableExists(kuduTableName)) {
            LOG.warn("Table: %s is in inconsistent state. It does not exist in Kudu master(s)" + " %s, but it exists in Hive metastore. Deleting from metastore only.", kuduTableName, kuduMasters);
            return;
        }
        client.deleteTable(kuduTableName);
        return;
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Error dropping Kudu table", e);
    }
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws InternalException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    // Extract predicates that can be evaluated by Kudu.
    try {
        kuduConjuncts_.addAll(extractKuduConjuncts(analyzer));
        // Mark these slots as materialized, otherwise the toThrift() of SlotRefs
        // referencing them will fail. These slots will never be filled with data though as
        // Kudu won't return these columns.
        // TODO KUDU-935 Don't require that slots be materialized in order to serialize
        // SlotRefs.
        analyzer.materializeSlots(kuduConjuncts_);
    } catch (AnalysisException e) {
        throw new InternalException("Error while extracting Kudu conjuncts.", e);
    }
    computeScanRangeLocations(analyzer);
    analyzer.materializeSlots(conjuncts_);
    computeMemLayout(analyzer);
    computeStats(analyzer);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = new KuduClientBuilder(kuduTable_.getKuduMasterAddresses()).build()) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeMemLayout(analyzer);
    computeStats(analyzer);
}
#end_block

#method_before
private void computeScanRangeLocations(Analyzer analyzer) {
    scanRanges_ = Lists.newArrayList();
    try (KuduClient client = new KuduClientBuilder(kuduTable_.getKuduMasterAddresses()).build()) {
        org.kududb.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        List<LocatedTablet> tabletLocations = rpcTable.getTabletsLocations(KuduTable.KUDU_RPC_TIMEOUT_MS);
        for (LocatedTablet tablet : tabletLocations) {
            List<TScanRangeLocation> locations = Lists.newArrayList();
            if (tablet.getReplicas().isEmpty()) {
                throw new ImpalaRuntimeException(String.format("At least one tablet does not have any replicas. Tablet ID: %s", new String(tablet.getTabletId(), Charsets.UTF_8)));
            }
            for (LocatedTablet.Replica replica : tablet.getReplicas()) {
                TNetworkAddress address = new TNetworkAddress(replica.getRpcHost(), replica.getRpcPort());
                // Use the network address to look up the host in the global list
                Integer hostIndex = analyzer.getHostIndex().getIndex(address);
                locations.add(new TScanRangeLocation(hostIndex));
                hostIndexSet_.add(hostIndex);
            }
            TScanRangeLocations locs = new TScanRangeLocations();
            // Now set the scan range of this tablet
            TKuduKeyRange keyRange = new TKuduKeyRange();
            keyRange.setRange_start_key(tablet.getPartition().getPartitionKeyStart());
            keyRange.setRange_stop_key(tablet.getPartition().getPartitionKeyEnd());
            TScanRange scanRange = new TScanRange();
            scanRange.setKudu_key_range(keyRange);
            // Set the scan range for this set of locations
            locs.setScan_range(scanRange);
            locs.locations = locations;
            scanRanges_.add(locs);
        }
    } catch (Exception e) {
        throw new RuntimeException("Loading Kudu Table failed", e);
    }
}
#method_after
private void computeScanRangeLocations(Analyzer analyzer, KuduClient client, org.apache.kudu.client.KuduTable rpcTable) throws ImpalaRuntimeException {
    scanRanges_ = Lists.newArrayList();
    List<KuduScanToken> scanTokens = createScanTokens(client, rpcTable);
    for (KuduScanToken token : scanTokens) {
        LocatedTablet tablet = token.getTablet();
        List<TScanRangeLocation> locations = Lists.newArrayList();
        if (tablet.getReplicas().isEmpty()) {
            throw new ImpalaRuntimeException(String.format("At least one tablet does not have any replicas. Tablet ID: %s", new String(tablet.getTabletId(), Charsets.UTF_8)));
        }
        for (LocatedTablet.Replica replica : tablet.getReplicas()) {
            TNetworkAddress address = new TNetworkAddress(replica.getRpcHost(), replica.getRpcPort());
            // Use the network address to look up the host in the global list
            Integer hostIndex = analyzer.getHostIndex().getIndex(address);
            locations.add(new TScanRangeLocation(hostIndex));
            hostIndexSet_.add(hostIndex);
        }
        TScanRange scanRange = new TScanRange();
        try {
            scanRange.setKudu_scan_token(token.serialize());
        } catch (IOException e) {
            throw new ImpalaRuntimeException("Unable to serialize Kudu scan token=" + token.toString(), e);
        }
        TScanRangeLocations locs = new TScanRangeLocations();
        locs.setScan_range(scanRange);
        locs.locations = locations;
        scanRanges_.add(locs);
    }
}
#end_block

#method_before
@Override
protected void toThrift(TPlanNode node) {
    node.node_type = TPlanNodeType.KUDU_SCAN_NODE;
    node.kudu_scan_node = new TKuduScanNode(desc_.getId().asInt());
    // Thriftify the pushable predicates and set them on the scan node.
    for (Expr predicate : kuduConjuncts_) {
        node.kudu_scan_node.addToKudu_conjuncts(predicate.treeToThrift());
    }
}
#method_after
@Override
protected void toThrift(TPlanNode node) {
    node.node_type = TPlanNodeType.KUDU_SCAN_NODE;
    node.kudu_scan_node = new TKuduScanNode(desc_.getId().asInt());
}
#end_block

#method_before
private List<Expr> extractKuduConjuncts(Analyzer analyzer) throws InternalException, AnalysisException {
    ImmutableList.Builder<Expr> pushableConjunctsBuilder = ImmutableList.builder();
    ListIterator<Expr> i = conjuncts_.listIterator();
    while (i.hasNext()) {
        Expr e = i.next();
        if (!(e instanceof BinaryPredicate))
            continue;
        BinaryPredicate comparisonPred = (BinaryPredicate) e;
        // TODO KUDU-931 look into handling implicit/explicit casts on the SlotRef.
        comparisonPred = BinaryPredicate.normalizeSlotRefComparison(comparisonPred, analyzer);
        if (comparisonPred == null)
            continue;
        // Needs to have a literal on the right.
        if (!comparisonPred.getChild(1).isLiteral())
            continue;
        comparisonPred = normalizeIntLiteralComparison(comparisonPred, analyzer);
        Operator op = comparisonPred.getOp();
        switch(comparisonPred.getOp()) {
            case NE:
                continue;
            // TODO Exclusive predicates are not supported in Kudu yet.
            case GT:
                continue;
            // TODO Exclusive predicates are not supported in Kudu yet.
            case LT:
                continue;
            // Fallthrough intended.
            case GE:
            // Fallthrough intended.
            case LE:
            case EQ:
                {
                    i.remove();
                    pushableConjunctsBuilder.add(comparisonPred);
                    break;
                }
            default:
                Preconditions.checkState(false, "Unexpected BinaryPredicate type: " + op.getName());
        }
    }
    return pushableConjunctsBuilder.build();
}
#method_after
private void extractKuduConjuncts(Analyzer analyzer, KuduClient client, org.apache.kudu.client.KuduTable rpcTable) {
    ListIterator<Expr> it = conjuncts_.listIterator();
    while (it.hasNext()) {
        if (tryConvertKuduPredicate(analyzer, rpcTable, it.next()))
            it.remove();
    }
}
#end_block

#method_before
public Table load(Db db, String tblName) {
    String fullTblName = db.getName() + "." + tblName;
    LOG.info("Loading metadata for: " + fullTblName);
    MetaStoreClient msClient = null;
    Table table;
    // turn all exceptions into TableLoadingException
    try {
        msClient = catalog_.getMetaStoreClient();
        org.apache.hadoop.hive.metastore.api.Table msTbl = null;
        // All calls to getTable() need to be serialized due to HIVE-5457.
        synchronized (metastoreAccessLock_) {
            msTbl = msClient.getHiveClient().getTable(db.getName(), tblName);
        }
        // Check that the Hive TableType is supported
        TableType tableType = TableType.valueOf(msTbl.getTableType());
        if (!SUPPORTED_TABLE_TYPES.contains(tableType)) {
            throw new TableLoadingException(String.format("Unsupported table type '%s' for: %s", tableType, fullTblName));
        }
        // Create a table of appropriate type and have it load itself
        table = Table.fromMetastoreTable(catalog_.getNextTableId(), db, msTbl);
        if (table == null) {
            throw new TableLoadingException("Unrecognized table type for table: " + fullTblName);
        }
        table.load(false, msClient.getHiveClient(), msTbl);
        table.validate();
    } catch (TableLoadingException e) {
        table = IncompleteTable.createFailedMetadataLoadTable(TableId.createInvalidId(), db, tblName, e);
    } catch (NoSuchObjectException e) {
        TableLoadingException tableDoesNotExist = new TableLoadingException("Table " + fullTblName + " no longer exists in the Hive MetaStore. " + "Run 'invalidate metadata " + fullTblName + "' to update the Impala " + "catalog.");
        table = IncompleteTable.createFailedMetadataLoadTable(TableId.createInvalidId(), db, tblName, tableDoesNotExist);
    } catch (Exception e) {
        table = IncompleteTable.createFailedMetadataLoadTable(catalog_.getNextTableId(), db, tblName, new TableLoadingException("Failed to load metadata for table: " + fullTblName + ". Running " + "'invalidate metadata " + fullTblName + "' may resolve this problem.", e));
    } finally {
        if (msClient != null)
            msClient.release();
    }
    return table;
}
#method_after
public Table load(Db db, String tblName) {
    String fullTblName = db.getName() + "." + tblName;
    LOG.info("Loading metadata for: " + fullTblName);
    Table table;
    // turn all exceptions into TableLoadingException
    try (MetaStoreClient msClient = catalog_.getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Table msTbl = null;
        // All calls to getTable() need to be serialized due to HIVE-5457.
        synchronized (metastoreAccessLock_) {
            msTbl = msClient.getHiveClient().getTable(db.getName(), tblName);
        }
        // Check that the Hive TableType is supported
        TableType tableType = TableType.valueOf(msTbl.getTableType());
        if (!SUPPORTED_TABLE_TYPES.contains(tableType)) {
            throw new TableLoadingException(String.format("Unsupported table type '%s' for: %s", tableType, fullTblName));
        }
        // Create a table of appropriate type and have it load itself
        table = Table.fromMetastoreTable(catalog_.getNextTableId(), db, msTbl);
        if (table == null) {
            throw new TableLoadingException("Unrecognized table type for table: " + fullTblName);
        }
        table.load(false, msClient.getHiveClient(), msTbl);
        table.validate();
    } catch (TableLoadingException e) {
        table = IncompleteTable.createFailedMetadataLoadTable(TableId.createInvalidId(), db, tblName, e);
    } catch (NoSuchObjectException e) {
        TableLoadingException tableDoesNotExist = new TableLoadingException("Table " + fullTblName + " no longer exists in the Hive MetaStore. " + "Run 'invalidate metadata " + fullTblName + "' to update the Impala " + "catalog.");
        table = IncompleteTable.createFailedMetadataLoadTable(TableId.createInvalidId(), db, tblName, tableDoesNotExist);
    } catch (Exception e) {
        table = IncompleteTable.createFailedMetadataLoadTable(catalog_.getNextTableId(), db, tblName, new TableLoadingException("Failed to load metadata for table: " + fullTblName + ". Running " + "'invalidate metadata " + fullTblName + "' may resolve this problem.", e));
    }
    return table;
}
#end_block

#method_before
@Test
public void testTpch() {
    runPlannerTestFile("tpch-all");
}
#method_after
@Test
public void testTpch() {
    runPlannerTestFile("tpch-all", "tpch");
}
#end_block

#method_before
@Test
public void TestCreateKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Create Kudu Table with all required properties
    AnalyzesOk("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // Check that all properties are present
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    // Check that properties are not empty
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'=''," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='asd'," + "'kudu.master_addresses' = '', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    // Don't allow caching
    AnalysisError("create table tab (x int) cached in 'testPool' tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "A Kudu table cannot be cached in HDFS.");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b) into 8 buckets, hash(c) into 2 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + " distribute by hash into 8 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // Number of buckets must be larger 1
    AnalysisError("create table tab (a int, b int, c int, d int) " + " distribute by hash(a,b) into 8 buckets, hash(c) into 1 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Number of buckets in DISTRIBUTE BY clause 'HASH(c) INTO 1 BUCKETS' must " + "be larger than 1");
    // Key ranges must match the column types.
    // TODO(kudu-merge) uncomment this when IMPALA-3156 is addressed.
    // AnalysisError("create table tab (a int, b int, c int, d int) " +
    // "distribute by hash(a,b,c) into 8 buckets, " +
    // "range(a) split rows ((1),('abc'),(3)) " +
    // "tblproperties (" +
    // "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " +
    // "'kudu.table_name'='tab'," +
    // "'kudu.master_addresses' = '127.0.0.1:8080', " +
    // "'kudu.key_columns' = 'a,b,c')");
    // Distribute range data types are picked up during analysis and forwarded to Kudu
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b,c) into 8 buckets, " + "range(a) split rows ((1),(2),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c')");
    // No float split keys
    AnalysisError("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b,c) into 8 buckets, " + "range(a) split rows ((1.2),('abc'),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Only integral and string values allowed for split rows.");
}
#method_after
@Test
public void TestCreateKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Create Kudu Table with all required properties
    AnalyzesOk("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // Check that all properties are present
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    // Check that properties are not empty
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'=''," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='asd'," + "'kudu.master_addresses' = '', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    // Don't allow caching
    AnalysisError("create table tab (x int) cached in 'testPool' " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "A Kudu table cannot be cached in HDFS.");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b) into 8 buckets, hash(c) into 2 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash into 8 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a'" + ")", "A data distribution must be specified using the DISTRIBUTE BY clause.");
    // DISTRIBUTE BY is not allowed for external tables.
    AnalysisError("create external table tab (a int) " + "distribute by hash into 3 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a'" + ")", "The DISTRIBUTE BY clause may not be specified for external tables.");
    // Number of buckets must be larger 1
    AnalysisError("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b) into 8 buckets, hash(c) into 1 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Number of buckets in DISTRIBUTE BY clause 'HASH(c) INTO 1 BUCKETS' must " + "be larger than 1");
    // Key ranges must match the column types.
    // TODO(kudu-merge) uncomment this when IMPALA-3156 is addressed.
    // AnalysisError("create table tab (a int, b int, c int, d int) " +
    // "distribute by hash(a,b,c) into 8 buckets, " +
    // "range(a) split rows ((1),('abc'),(3)) " +
    // "tblproperties (" +
    // "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " +
    // "'kudu.table_name'='tab'," +
    // "'kudu.master_addresses' = '127.0.0.1:8080', " +
    // "'kudu.key_columns' = 'a,b,c')");
    // Distribute range data types are picked up during analysis and forwarded to Kudu
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b,c) into 8 buckets, " + "range(a) split rows ((1),(2),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // Each split row size should equals to the number of range columns.
    AnalysisError("create table tab (a int, b int, c int, d int) " + "distribute by range(a) split rows ((1,'extra_val'),(2),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "SPLIT ROWS has different size than number of projected key columns: 1. " + "Split row: (1, 'extra_val')");
    // No float split keys
    AnalysisError("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b,c) into 8 buckets, " + "range(a) split rows ((1.2),('abc'),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Only integral and string values allowed for split rows.");
}
#end_block

#method_before
public static List<org.apache.hadoop.hive.metastore.api.Partition> fetchAllPartitions(HiveMetaStoreClient client, String dbName, String tblName, int numRetries) throws MetaException, TException {
    Preconditions.checkArgument(numRetries >= 0);
    int retryAttempt = 0;
    while (true) {
        try {
            // First, get all partition names that currently exist.
            List<String> partNames = client.listPartitionNames(dbName, tblName, (short) -1);
            return MetaStoreUtil.fetchPartitionsByName(client, partNames, dbName, tblName);
        } catch (MetaException e) {
            // connection which we can't recover from by retrying.
            if (retryAttempt < numRetries) {
                LOG.error(String.format("Error fetching partitions for table: %s.%s. " + "Retry attempt: %d/%d", dbName, tblName, retryAttempt, numRetries), e);
                ++retryAttempt;
            // TODO: Sleep for a bit?
            } else {
                throw e;
            }
        }
    }
}
#method_after
public static List<org.apache.hadoop.hive.metastore.api.Partition> fetchAllPartitions(IMetaStoreClient client, String dbName, String tblName, int numRetries) throws MetaException, TException {
    Preconditions.checkArgument(numRetries >= 0);
    int retryAttempt = 0;
    while (true) {
        try {
            // First, get all partition names that currently exist.
            List<String> partNames = client.listPartitionNames(dbName, tblName, (short) -1);
            return MetaStoreUtil.fetchPartitionsByName(client, partNames, dbName, tblName);
        } catch (MetaException e) {
            // connection which we can't recover from by retrying.
            if (retryAttempt < numRetries) {
                LOG.error(String.format("Error fetching partitions for table: %s.%s. " + "Retry attempt: %d/%d", dbName, tblName, retryAttempt, numRetries), e);
                ++retryAttempt;
            // TODO: Sleep for a bit?
            } else {
                throw e;
            }
        }
    }
}
#end_block

#method_before
public static List<Partition> fetchPartitionsByName(HiveMetaStoreClient client, List<String> partNames, String dbName, String tblName) throws MetaException, TException {
    LOG.trace(String.format("Fetching %d partitions for: %s.%s using partition " + "batch size: %d", partNames.size(), dbName, tblName, maxPartitionsPerRpc_));
    List<org.apache.hadoop.hive.metastore.api.Partition> fetchedPartitions = Lists.newArrayList();
    // Fetch the partitions in batches.
    for (int i = 0; i < partNames.size(); i += maxPartitionsPerRpc_) {
        // Get a subset of partition names to fetch.
        List<String> partsToFetch = partNames.subList(i, Math.min(i + maxPartitionsPerRpc_, partNames.size()));
        // Fetch these partitions from the metastore.
        fetchedPartitions.addAll(client.getPartitionsByNames(dbName, tblName, partsToFetch));
    }
    return fetchedPartitions;
}
#method_after
public static List<Partition> fetchPartitionsByName(IMetaStoreClient client, List<String> partNames, String dbName, String tblName) throws MetaException, TException {
    LOG.trace(String.format("Fetching %d partitions for: %s.%s using partition " + "batch size: %d", partNames.size(), dbName, tblName, maxPartitionsPerRpc_));
    List<org.apache.hadoop.hive.metastore.api.Partition> fetchedPartitions = Lists.newArrayList();
    // Fetch the partitions in batches.
    for (int i = 0; i < partNames.size(); i += maxPartitionsPerRpc_) {
        // Get a subset of partition names to fetch.
        List<String> partsToFetch = partNames.subList(i, Math.min(i + maxPartitionsPerRpc_, partNames.size()));
        // Fetch these partitions from the metastore.
        fetchedPartitions.addAll(client.getPartitionsByNames(dbName, tblName, partsToFetch));
    }
    return fetchedPartitions;
}
#end_block

#method_before
private void testSelectStar() throws AnalysisException {
    AnalyzesOk("select * from functional.AllTypes");
    DescriptorTable descTbl = analyzer_.getDescTbl();
    TupleDescriptor tupleD = descTbl.getTupleDesc(new TupleId(0));
    for (SlotDescriptor slotD : tupleD.getSlots()) {
        slotD.setIsMaterialized(true);
    }
    descTbl.computeMemLayout();
    Assert.assertEquals(97.0f, tupleD.getAvgSerializedSize(), 0.0);
    checkLayoutParams("functional.alltypes.bool_col", 1, 2, 0, 0);
    checkLayoutParams("functional.alltypes.tinyint_col", 1, 3, 0, 1);
    checkLayoutParams("functional.alltypes.smallint_col", 2, 4, 0, 2);
    checkLayoutParams("functional.alltypes.id", 4, 8, 0, 3);
    checkLayoutParams("functional.alltypes.int_col", 4, 12, 0, 4);
    checkLayoutParams("functional.alltypes.float_col", 4, 16, 0, 5);
    checkLayoutParams("functional.alltypes.year", 4, 20, 0, 6);
    checkLayoutParams("functional.alltypes.month", 4, 24, 0, 7);
    checkLayoutParams("functional.alltypes.bigint_col", 8, 32, 1, 0);
    checkLayoutParams("functional.alltypes.double_col", 8, 40, 1, 1);
    int strSlotSize = PrimitiveType.STRING.getSlotSize();
    checkLayoutParams("functional.alltypes.date_string_col", strSlotSize, 48, 1, 2);
    checkLayoutParams("functional.alltypes.string_col", strSlotSize, 48 + strSlotSize, 1, 3);
}
#method_after
private void testSelectStar() throws AnalysisException {
    SelectStmt stmt = (SelectStmt) AnalyzesOk("select * from functional.AllTypes");
    Analyzer analyzer = stmt.getAnalyzer();
    DescriptorTable descTbl = analyzer.getDescTbl();
    TupleDescriptor tupleD = descTbl.getTupleDesc(new TupleId(0));
    for (SlotDescriptor slotD : tupleD.getSlots()) {
        slotD.setIsMaterialized(true);
    }
    descTbl.computeMemLayout();
    Assert.assertEquals(97.0f, tupleD.getAvgSerializedSize(), 0.0);
    checkLayoutParams("functional.alltypes.bool_col", 1, 2, 0, 0, analyzer);
    checkLayoutParams("functional.alltypes.tinyint_col", 1, 3, 0, 1, analyzer);
    checkLayoutParams("functional.alltypes.smallint_col", 2, 4, 0, 2, analyzer);
    checkLayoutParams("functional.alltypes.id", 4, 8, 0, 3, analyzer);
    checkLayoutParams("functional.alltypes.int_col", 4, 12, 0, 4, analyzer);
    checkLayoutParams("functional.alltypes.float_col", 4, 16, 0, 5, analyzer);
    checkLayoutParams("functional.alltypes.year", 4, 20, 0, 6, analyzer);
    checkLayoutParams("functional.alltypes.month", 4, 24, 0, 7, analyzer);
    checkLayoutParams("functional.alltypes.bigint_col", 8, 32, 1, 0, analyzer);
    checkLayoutParams("functional.alltypes.double_col", 8, 40, 1, 1, analyzer);
    int strSlotSize = PrimitiveType.STRING.getSlotSize();
    checkLayoutParams("functional.alltypes.date_string_col", strSlotSize, 48, 1, 2, analyzer);
    checkLayoutParams("functional.alltypes.string_col", strSlotSize, 48 + strSlotSize, 1, 3, analyzer);
}
#end_block

#method_before
private void testNonNullable() throws AnalysisException {
    // both slots are non-nullable bigints. The layout should look like:
    // (byte range : data)
    // 0 - 7: count(int_col)
    // 8 - 15: count(*)
    AnalyzesOk("select count(int_col), count(*) from functional.AllTypes");
    DescriptorTable descTbl = analyzer_.getDescTbl();
    TupleDescriptor aggDesc = descTbl.getTupleDesc(new TupleId(1));
    for (SlotDescriptor slotD : aggDesc.getSlots()) {
        slotD.setIsMaterialized(true);
    }
    descTbl.computeMemLayout();
    Assert.assertEquals(16.0f, aggDesc.getAvgSerializedSize(), 0.0);
    Assert.assertEquals(16, aggDesc.getByteSize());
    checkLayoutParams(aggDesc.getSlots().get(0), 8, 0, 0, -1);
    checkLayoutParams(aggDesc.getSlots().get(1), 8, 8, 0, -1);
}
#method_after
private void testNonNullable() throws AnalysisException {
    // both slots are non-nullable bigints. The layout should look like:
    // (byte range : data)
    // 0 - 7: count(int_col)
    // 8 - 15: count(*)
    SelectStmt stmt = (SelectStmt) AnalyzesOk("select count(int_col), count(*) from functional.AllTypes");
    DescriptorTable descTbl = stmt.getAnalyzer().getDescTbl();
    TupleDescriptor aggDesc = descTbl.getTupleDesc(new TupleId(1));
    for (SlotDescriptor slotD : aggDesc.getSlots()) {
        slotD.setIsMaterialized(true);
    }
    descTbl.computeMemLayout();
    Assert.assertEquals(16.0f, aggDesc.getAvgSerializedSize(), 0.0);
    Assert.assertEquals(16, aggDesc.getByteSize());
    checkLayoutParams(aggDesc.getSlots().get(0), 8, 0, 0, -1);
    checkLayoutParams(aggDesc.getSlots().get(1), 8, 8, 0, -1);
}
#end_block

#method_before
private void testMixedNullable() throws AnalysisException {
    // one slot is nullable, one is not. The layout should look like:
    // (byte range : data)
    // 0 : 1 nullable-byte (only 1 bit used)
    // 1 - 7: padded bytes
    // 8 - 15: sum(int_col)
    // 16 - 23: count(*)
    AnalyzesOk("select sum(int_col), count(*) from functional.AllTypes");
    DescriptorTable descTbl = analyzer_.getDescTbl();
    TupleDescriptor aggDesc = descTbl.getTupleDesc(new TupleId(1));
    for (SlotDescriptor slotD : aggDesc.getSlots()) {
        slotD.setIsMaterialized(true);
    }
    descTbl.computeMemLayout();
    Assert.assertEquals(16.0f, aggDesc.getAvgSerializedSize(), 0.0);
    Assert.assertEquals(24, aggDesc.getByteSize());
    checkLayoutParams(aggDesc.getSlots().get(0), 8, 8, 0, 0);
    checkLayoutParams(aggDesc.getSlots().get(1), 8, 16, 0, -1);
}
#method_after
private void testMixedNullable() throws AnalysisException {
    // one slot is nullable, one is not. The layout should look like:
    // (byte range : data)
    // 0 : 1 nullable-byte (only 1 bit used)
    // 1 - 7: padded bytes
    // 8 - 15: sum(int_col)
    // 16 - 23: count(*)
    SelectStmt stmt = (SelectStmt) AnalyzesOk("select sum(int_col), count(*) from functional.AllTypes");
    DescriptorTable descTbl = stmt.getAnalyzer().getDescTbl();
    TupleDescriptor aggDesc = descTbl.getTupleDesc(new TupleId(1));
    for (SlotDescriptor slotD : aggDesc.getSlots()) {
        slotD.setIsMaterialized(true);
    }
    descTbl.computeMemLayout();
    Assert.assertEquals(16.0f, aggDesc.getAvgSerializedSize(), 0.0);
    Assert.assertEquals(24, aggDesc.getByteSize());
    checkLayoutParams(aggDesc.getSlots().get(0), 8, 8, 0, 0);
    checkLayoutParams(aggDesc.getSlots().get(1), 8, 16, 0, -1);
}
#end_block

#method_before
private void testNonMaterializedSlots() throws AnalysisException {
    AnalyzesOk("select * from functional.alltypes");
    DescriptorTable descTbl = analyzer_.getDescTbl();
    TupleDescriptor tupleD = descTbl.getTupleDesc(new TupleId(0));
    ArrayList<SlotDescriptor> slots = tupleD.getSlots();
    for (SlotDescriptor slotD : slots) {
        slotD.setIsMaterialized(true);
    }
    // Mark slots 0 (id), 7 (double_col), 9 (string_col) as non-materialized.
    slots.get(0).setIsMaterialized(false);
    slots.get(7).setIsMaterialized(false);
    slots.get(9).setIsMaterialized(false);
    descTbl.computeMemLayout();
    Assert.assertEquals(68.0f, tupleD.getAvgSerializedSize(), 0.0);
    // Check non-materialized slots.
    checkLayoutParams("functional.alltypes.id", 0, -1, 0, 0);
    checkLayoutParams("functional.alltypes.double_col", 0, -1, 0, 0);
    checkLayoutParams("functional.alltypes.string_col", 0, -1, 0, 0);
    // Check materialized slots.
    checkLayoutParams("functional.alltypes.bool_col", 1, 2, 0, 0);
    checkLayoutParams("functional.alltypes.tinyint_col", 1, 3, 0, 1);
    checkLayoutParams("functional.alltypes.smallint_col", 2, 4, 0, 2);
    checkLayoutParams("functional.alltypes.int_col", 4, 8, 0, 3);
    checkLayoutParams("functional.alltypes.float_col", 4, 12, 0, 4);
    checkLayoutParams("functional.alltypes.year", 4, 16, 0, 5);
    checkLayoutParams("functional.alltypes.month", 4, 20, 0, 6);
    checkLayoutParams("functional.alltypes.bigint_col", 8, 24, 0, 7);
    int strSlotSize = PrimitiveType.STRING.getSlotSize();
    checkLayoutParams("functional.alltypes.date_string_col", strSlotSize, 32, 1, 0);
}
#method_after
private void testNonMaterializedSlots() throws AnalysisException {
    SelectStmt stmt = (SelectStmt) AnalyzesOk("select * from functional.alltypes");
    Analyzer analyzer = stmt.getAnalyzer();
    DescriptorTable descTbl = analyzer.getDescTbl();
    TupleDescriptor tupleD = descTbl.getTupleDesc(new TupleId(0));
    ArrayList<SlotDescriptor> slots = tupleD.getSlots();
    for (SlotDescriptor slotD : slots) {
        slotD.setIsMaterialized(true);
    }
    // Mark slots 0 (id), 7 (double_col), 9 (string_col) as non-materialized.
    slots.get(0).setIsMaterialized(false);
    slots.get(7).setIsMaterialized(false);
    slots.get(9).setIsMaterialized(false);
    descTbl.computeMemLayout();
    Assert.assertEquals(68.0f, tupleD.getAvgSerializedSize(), 0.0);
    // Check non-materialized slots.
    checkLayoutParams("functional.alltypes.id", 0, -1, 0, 0, analyzer);
    checkLayoutParams("functional.alltypes.double_col", 0, -1, 0, 0, analyzer);
    checkLayoutParams("functional.alltypes.string_col", 0, -1, 0, 0, analyzer);
    // Check materialized slots.
    checkLayoutParams("functional.alltypes.bool_col", 1, 2, 0, 0, analyzer);
    checkLayoutParams("functional.alltypes.tinyint_col", 1, 3, 0, 1, analyzer);
    checkLayoutParams("functional.alltypes.smallint_col", 2, 4, 0, 2, analyzer);
    checkLayoutParams("functional.alltypes.int_col", 4, 8, 0, 3, analyzer);
    checkLayoutParams("functional.alltypes.float_col", 4, 12, 0, 4, analyzer);
    checkLayoutParams("functional.alltypes.year", 4, 16, 0, 5, analyzer);
    checkLayoutParams("functional.alltypes.month", 4, 20, 0, 6, analyzer);
    checkLayoutParams("functional.alltypes.bigint_col", 8, 24, 0, 7, analyzer);
    int strSlotSize = PrimitiveType.STRING.getSlotSize();
    checkLayoutParams("functional.alltypes.date_string_col", strSlotSize, 32, 1, 0, analyzer);
}
#end_block

#method_before
private void checkLayoutParams(String colAlias, int byteSize, int byteOffset, int nullIndicatorByte, int nullIndicatorBit) {
    SlotDescriptor d = analyzer_.getSlotDescriptor(colAlias);
    checkLayoutParams(d, byteSize, byteOffset, nullIndicatorByte, nullIndicatorBit);
}
#method_after
private void checkLayoutParams(String colAlias, int byteSize, int byteOffset, int nullIndicatorByte, int nullIndicatorBit, Analyzer analyzer) {
    SlotDescriptor d = analyzer.getSlotDescriptor(colAlias);
    checkLayoutParams(d, byteSize, byteOffset, nullIndicatorByte, nullIndicatorBit);
}
#end_block

#method_before
public void foldConstantChildren(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(isAnalyzed_);
    Preconditions.checkNotNull(analyzer);
    for (int i = 0; i < children_.size(); ++i) {
        Expr child = getChild(i);
        if (child.isLiteral() || !child.isConstant())
            continue;
        LiteralExpr literalExpr = LiteralExpr.create(child, analyzer.getQueryCtx());
        Preconditions.checkNotNull(literalExpr);
        setChild(i, literalExpr);
    }
    isAnalyzed_ = false;
}
#method_after
public void foldConstantChildren(Analyzer analyzer) throws AnalysisException {
    Preconditions.checkState(isAnalyzed_);
    Preconditions.checkNotNull(analyzer);
    for (int i = 0; i < children_.size(); ++i) {
        Expr child = getChild(i);
        if (child.isLiteral() || !child.isConstant())
            continue;
        LiteralExpr literalExpr = LiteralExpr.create(child, analyzer.getQueryCtx());
        if (literalExpr == null)
            continue;
        setChild(i, literalExpr);
    }
    isAnalyzed_ = false;
}
#end_block

#method_before
private void loadColumns(List<FieldSchema> schema, HiveMetaStoreClient client, Set<String> keyColumns) throws TableLoadingException {
    if (keyColumns.size() == 0 || keyColumns.size() > schema.size()) {
        throw new TableLoadingException(String.format("Kudu tables must have at least one" + "key column (had %d), and no more key columns than there are table columns " + "(had %d).", keyColumns.size(), schema.size()));
    }
    clearColumns();
    Set<String> columnNames = Sets.newHashSet();
    int pos = 0;
    for (FieldSchema field : schema) {
        org.apache.impala.catalog.Type type = parseColumnType(field);
        // TODO(kudu-merge): Check for decimal types?
        boolean isKey = keyColumns.contains(field.getName());
        KuduColumn col = new KuduColumn(field.getName(), isKey, !isKey, type, field.getComment(), pos);
        columnNames.add(col.getName());
        addColumn(col);
        ++pos;
    }
    if (!columnNames.containsAll(keyColumns)) {
        throw new TableLoadingException(String.format("Some key columns were not found in" + " the set of columns. List of column names: %s, List of key column names:" + " %s", Iterables.toString(columnNames), Iterables.toString(keyColumns)));
    }
    kuduKeyColumnNames_ = ImmutableList.copyOf(keyColumns);
    loadAllColumnStats(client);
}
#method_after
private void loadColumns(List<FieldSchema> schema, IMetaStoreClient client, Set<String> keyColumns) throws TableLoadingException {
    if (keyColumns.size() == 0 || keyColumns.size() > schema.size()) {
        throw new TableLoadingException(String.format("Kudu tables must have at least one" + "key column (had %d), and no more key columns than there are table columns " + "(had %d).", keyColumns.size(), schema.size()));
    }
    clearColumns();
    Set<String> columnNames = Sets.newHashSet();
    int pos = 0;
    for (FieldSchema field : schema) {
        org.apache.impala.catalog.Type type = parseColumnType(field);
        // TODO(kudu-merge): Check for decimal types?
        boolean isKey = keyColumns.contains(field.getName());
        KuduColumn col = new KuduColumn(field.getName(), isKey, !isKey, type, field.getComment(), pos);
        columnNames.add(col.getName());
        addColumn(col);
        ++pos;
    }
    if (!columnNames.containsAll(keyColumns)) {
        throw new TableLoadingException(String.format("Some key columns were not found in" + " the set of columns. List of column names: %s, List of key column names:" + " %s", Iterables.toString(columnNames), Iterables.toString(keyColumns)));
    }
    kuduKeyColumnNames_ = ImmutableList.copyOf(keyColumns);
    loadAllColumnStats(client);
}
#end_block

#method_before
@Override
public void load(boolean reuseMetadata, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    // TODO handle 'reuseMetadata'
    if (getMetaStoreTable() == null || !tableParamsAreValid(msTbl.getParameters())) {
        throw new TableLoadingException(String.format("Cannot load Kudu table %s, table is corrupt.", name_));
    }
    msTable_ = msTbl;
    kuduTableName_ = msTbl.getParameters().get(KEY_TABLE_NAME);
    kuduMasters_ = msTbl.getParameters().get(KEY_MASTER_ADDRESSES);
    String keyColumnsProp = Preconditions.checkNotNull(msTbl.getParameters().get(KEY_KEY_COLUMNS).toLowerCase(), "'kudu.key_columns' cannot be null.");
    Set<String> keyColumns = KuduUtil.parseKeyColumns(keyColumnsProp);
    // Load the rest of the data from the table parameters directly
    loadColumns(msTbl.getSd().getCols(), client, keyColumns);
    numClusteringCols_ = 0;
    // Get row count from stats
    numRows_ = getRowCount(getMetaStoreTable().getParameters());
}
#method_after
@Override
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    // TODO handle 'reuseMetadata'
    if (getMetaStoreTable() == null || !tableParamsAreValid(msTbl.getParameters())) {
        throw new TableLoadingException(String.format("Cannot load Kudu table %s, table is corrupt.", name_));
    }
    msTable_ = msTbl;
    kuduTableName_ = msTbl.getParameters().get(KEY_TABLE_NAME);
    kuduMasters_ = msTbl.getParameters().get(KEY_MASTER_ADDRESSES);
    String keyColumnsProp = Preconditions.checkNotNull(msTbl.getParameters().get(KEY_KEY_COLUMNS).toLowerCase(), "'kudu.key_columns' cannot be null.");
    Set<String> keyColumns = KuduUtil.parseKeyColumns(keyColumnsProp);
    // Load the rest of the data from the table parameters directly
    loadColumns(msTbl.getSd().getCols(), client, keyColumns);
    numClusteringCols_ = 0;
    // Get row count from stats
    numRows_ = getRowCount(getMetaStoreTable().getParameters());
}
#end_block

#method_before
public TResultSet getTableStats() throws ImpalaRuntimeException {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    resultSchema.addToColumns(new TColumn("# Rows", Type.INT.toThrift()));
    resultSchema.addToColumns(new TColumn("Start Key", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Stop Key", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Leader Replica", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("# Replicas", Type.INT.toThrift()));
    try (KuduClient client = new KuduClient.KuduClientBuilder(getKuduMasterAddresses()).build()) {
        org.kududb.client.KuduTable kuduTable = client.openTable(kuduTableName_);
        List<LocatedTablet> tablets = kuduTable.getTabletsLocations(KUDU_RPC_TIMEOUT_MS);
        for (LocatedTablet tab : tablets) {
            TResultRowBuilder builder = new TResultRowBuilder();
            // The Kudu client API doesn't expose tablet row counts.
            builder.add("-1");
            builder.add(DatatypeConverter.printHexBinary(tab.getPartition().getPartitionKeyStart()));
            builder.add(DatatypeConverter.printHexBinary(tab.getPartition().getPartitionKeyEnd()));
            LocatedTablet.Replica leader = tab.getLeaderReplica();
            if (leader == null) {
                // Leader might be null, if it is not yet available (e.g. during
                // leader election in Kudu)
                builder.add("Leader n/a");
            } else {
                builder.add(leader.getRpcHost() + ":" + leader.getRpcPort().toString());
            }
            builder.add(tab.getReplicas().size());
            result.addToRows(builder.get());
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Could not communicate with Kudu.", e);
    }
    return result;
}
#method_after
public TResultSet getTableStats() throws ImpalaRuntimeException {
    TResultSet result = new TResultSet();
    TResultSetMetadata resultSchema = new TResultSetMetadata();
    result.setSchema(resultSchema);
    resultSchema.addToColumns(new TColumn("# Rows", Type.INT.toThrift()));
    resultSchema.addToColumns(new TColumn("Start Key", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Stop Key", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("Leader Replica", Type.STRING.toThrift()));
    resultSchema.addToColumns(new TColumn("# Replicas", Type.INT.toThrift()));
    try (KuduClient client = new KuduClient.KuduClientBuilder(getKuduMasterAddresses()).build()) {
        org.apache.kudu.client.KuduTable kuduTable = client.openTable(kuduTableName_);
        List<LocatedTablet> tablets = kuduTable.getTabletsLocations(KUDU_RPC_TIMEOUT_MS);
        for (LocatedTablet tab : tablets) {
            TResultRowBuilder builder = new TResultRowBuilder();
            // The Kudu client API doesn't expose tablet row counts.
            builder.add("-1");
            builder.add(DatatypeConverter.printHexBinary(tab.getPartition().getPartitionKeyStart()));
            builder.add(DatatypeConverter.printHexBinary(tab.getPartition().getPartitionKeyEnd()));
            LocatedTablet.Replica leader = tab.getLeaderReplica();
            if (leader == null) {
                // Leader might be null, if it is not yet available (e.g. during
                // leader election in Kudu)
                builder.add("Leader n/a");
            } else {
                builder.add(leader.getRpcHost() + ":" + leader.getRpcPort().toString());
            }
            builder.add(tab.getReplicas().size());
            result.addToRows(builder.get());
        }
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Could not communicate with Kudu.", e);
    }
    return result;
}
#end_block

#method_before
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
    } else if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        singleNodePlanner.validatePlan(singleNodePlan);
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (RuntimeEnv.INSTANCE.computeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#method_after
public ArrayList<PlanFragment> createPlan() throws ImpalaException {
    SingleNodePlanner singleNodePlanner = new SingleNodePlanner(ctx_);
    DistributedPlanner distributedPlanner = new DistributedPlanner(ctx_);
    PlanNode singleNodePlan = singleNodePlanner.createSingleNodePlan();
    ctx_.getRootAnalyzer().getTimeline().markEvent("Single node plan created");
    ArrayList<PlanFragment> fragments = null;
    // Determine the maximum number of rows processed by any node in the plan tree
    MaxRowsProcessedVisitor visitor = new MaxRowsProcessedVisitor();
    singleNodePlan.accept(visitor);
    long maxRowsProcessed = visitor.get() == -1 ? Long.MAX_VALUE : visitor.get();
    boolean isSmallQuery = maxRowsProcessed < ctx_.getQueryOptions().exec_single_node_rows_threshold;
    if (isSmallQuery) {
        // Execute on a single node and disable codegen for small results
        ctx_.getQueryOptions().setNum_nodes(1);
        ctx_.getQueryOptions().setDisable_codegen(true);
        if (maxRowsProcessed < ctx_.getQueryOptions().batch_size || maxRowsProcessed < 1024 && ctx_.getQueryOptions().batch_size == 0) {
            // Only one scanner thread for small queries
            ctx_.getQueryOptions().setNum_scanner_threads(1);
        }
        // disable runtime filters
        ctx_.getQueryOptions().setRuntime_filter_mode(TRuntimeFilterMode.OFF);
    }
    // Join rewrites.
    invertJoins(singleNodePlan, ctx_.isSingleNodeExec());
    singleNodePlan = useNljForSingularRowBuilds(singleNodePlan, ctx_.getRootAnalyzer());
    // create runtime filters
    if (ctx_.getQueryOptions().getRuntime_filter_mode() != TRuntimeFilterMode.OFF) {
        // Always compute filters, even if the BE won't always use all of them.
        RuntimeFilterGenerator.generateRuntimeFilters(ctx_.getRootAnalyzer(), singleNodePlan, ctx_.getQueryOptions().getMax_num_runtime_filters());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Runtime filters computed");
    }
    if (ctx_.isSingleNodeExec()) {
        // create one fragment containing the entire single-node plan tree
        fragments = Lists.newArrayList(new PlanFragment(ctx_.getNextFragmentId(), singleNodePlan, DataPartition.UNPARTITIONED));
    } else {
        singleNodePlanner.validatePlan(singleNodePlan);
        // create distributed plan
        fragments = distributedPlanner.createPlanFragments(singleNodePlan);
    }
    PlanFragment rootFragment = fragments.get(fragments.size() - 1);
    rootFragment.verifyTree();
    ExprSubstitutionMap rootNodeSmap = rootFragment.getPlanRoot().getOutputSmap();
    List<Expr> resultExprs = null;
    if (ctx_.isInsertOrCtas()) {
        InsertStmt insertStmt = ctx_.getAnalysisResult().getInsertStmt();
        insertStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        if (!ctx_.isSingleNodeExec()) {
            // repartition on partition keys
            rootFragment = distributedPlanner.createInsertFragment(rootFragment, insertStmt, ctx_.getRootAnalyzer(), fragments);
        }
        // set up table sink for root fragment
        rootFragment.setSink(insertStmt.createDataSink());
        resultExprs = insertStmt.getResultExprs();
    } else {
        if (ctx_.isUpdate()) {
            // Set up update sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getUpdateStmt().createDataSink());
        } else if (ctx_.isDelete()) {
            // Set up delete sink for root fragment
            rootFragment.setSink(ctx_.getAnalysisResult().getDeleteStmt().createDataSink());
        }
        QueryStmt queryStmt = ctx_.getQueryStmt();
        queryStmt.substituteResultExprs(rootNodeSmap, ctx_.getRootAnalyzer());
        resultExprs = queryStmt.getResultExprs();
    }
    rootFragment.setOutputExprs(resultExprs);
    LOG.debug("desctbl: " + ctx_.getRootAnalyzer().getDescTbl().debugString());
    LOG.debug("resultexprs: " + Expr.debugString(rootFragment.getOutputExprs()));
    LOG.debug("finalize plan fragments");
    for (PlanFragment fragment : fragments) {
        fragment.finalize(ctx_.getRootAnalyzer());
    }
    Collections.reverse(fragments);
    ctx_.getRootAnalyzer().getTimeline().markEvent("Distributed plan created");
    ColumnLineageGraph graph = ctx_.getRootAnalyzer().getColumnLineageGraph();
    if (RuntimeEnv.INSTANCE.computeLineage() || RuntimeEnv.INSTANCE.isTestEnv()) {
        // Compute the column lineage graph
        if (ctx_.isInsertOrCtas()) {
            Table targetTable = ctx_.getAnalysisResult().getInsertStmt().getTargetTable();
            graph.addTargetColumnLabels(targetTable);
            Preconditions.checkNotNull(targetTable);
            List<Expr> exprs = Lists.newArrayList();
            if (targetTable instanceof HBaseTable) {
                exprs.addAll(resultExprs);
            } else {
                exprs.addAll(ctx_.getAnalysisResult().getInsertStmt().getPartitionKeyExprs());
                exprs.addAll(resultExprs.subList(0, targetTable.getNonClusteringColumns().size()));
            }
            graph.computeLineageGraph(exprs, ctx_.getRootAnalyzer());
        } else {
            graph.addTargetColumnLabels(ctx_.getQueryStmt().getColLabels());
            graph.computeLineageGraph(resultExprs, ctx_.getRootAnalyzer());
        }
        LOG.trace("lineage: " + graph.debugString());
        ctx_.getRootAnalyzer().getTimeline().markEvent("Lineage info computed");
    }
    return fragments;
}
#end_block

#method_before
public String getExplainString(ArrayList<PlanFragment> fragments, TQueryExecRequest request, TExplainLevel explainLevel) {
    StringBuilder str = new StringBuilder();
    boolean hasHeader = false;
    if (request.isSetPer_host_mem_req() && request.isSetPer_host_vcores()) {
        str.append(String.format("Estimated Per-Host Requirements: Memory=%s VCores=%s\n", PrintUtils.printBytes(request.getPer_host_mem_req()), request.per_host_vcores));
        hasHeader = true;
    }
    // child queries of 'compute stats'.
    if (!request.query_ctx.isSetParent_query_id() && request.query_ctx.isSetTables_with_corrupt_stats() && !request.query_ctx.getTables_with_corrupt_stats().isEmpty()) {
        List<String> tableNames = Lists.newArrayList();
        for (TTableName tableName : request.query_ctx.getTables_with_corrupt_stats()) {
            tableNames.add(tableName.db_name + "." + tableName.table_name);
        }
        str.append("WARNING: The following tables have potentially corrupt table\n" + "statistics. Drop and re-compute statistics to resolve this problem.\n" + Joiner.on(", ").join(tableNames) + "\n");
        hasHeader = true;
    }
    // 'compute stats'. The parent_query_id is only set for compute stats child queries.
    if (!request.query_ctx.isSetParent_query_id() && request.query_ctx.isSetTables_missing_stats() && !request.query_ctx.getTables_missing_stats().isEmpty()) {
        List<String> tableNames = Lists.newArrayList();
        for (TTableName tableName : request.query_ctx.getTables_missing_stats()) {
            tableNames.add(tableName.db_name + "." + tableName.table_name);
        }
        str.append("WARNING: The following tables are missing relevant table " + "and/or column statistics.\n" + Joiner.on(", ").join(tableNames) + "\n");
        hasHeader = true;
    }
    if (request.query_ctx.isDisable_spilling()) {
        str.append("WARNING: Spilling is disabled for this query as a safety guard.\n" + "Reason: Query option disable_unsafe_spills is set, at least one table\n" + "is missing relevant stats, and no plan hints were given.\n");
        hasHeader = true;
    }
    if (hasHeader)
        str.append("\n");
    if (explainLevel.ordinal() < TExplainLevel.VERBOSE.ordinal()) {
        // Print the non-fragmented parallel plan.
        str.append(fragments.get(0).getExplainString(explainLevel));
    } else {
        // Print the fragmented parallel plan.
        for (int i = 0; i < fragments.size(); ++i) {
            PlanFragment fragment = fragments.get(i);
            str.append(fragment.getExplainString(explainLevel));
            if (explainLevel == TExplainLevel.VERBOSE && i + 1 != fragments.size()) {
                str.append("\n");
            }
        }
    }
    return str.toString();
}
#method_after
public String getExplainString(ArrayList<PlanFragment> fragments, TQueryExecRequest request, TExplainLevel explainLevel) {
    StringBuilder str = new StringBuilder();
    boolean hasHeader = false;
    if (request.isSetPer_host_mem_req() && request.isSetPer_host_vcores()) {
        str.append(String.format("Estimated Per-Host Requirements: Memory=%s VCores=%s\n", PrintUtils.printBytes(request.getPer_host_mem_req()), request.per_host_vcores));
        hasHeader = true;
    }
    // child queries of 'compute stats'.
    if (!request.query_ctx.isSetParent_query_id() && request.query_ctx.isSetTables_with_corrupt_stats() && !request.query_ctx.getTables_with_corrupt_stats().isEmpty()) {
        List<String> tableNames = Lists.newArrayList();
        for (TTableName tableName : request.query_ctx.getTables_with_corrupt_stats()) {
            tableNames.add(tableName.db_name + "." + tableName.table_name);
        }
        str.append("WARNING: The following tables have potentially corrupt table statistics.\n" + "Drop and re-compute statistics to resolve this problem.\n" + Joiner.on(", ").join(tableNames) + "\n");
        hasHeader = true;
    }
    // 'compute stats'. The parent_query_id is only set for compute stats child queries.
    if (!request.query_ctx.isSetParent_query_id() && request.query_ctx.isSetTables_missing_stats() && !request.query_ctx.getTables_missing_stats().isEmpty()) {
        List<String> tableNames = Lists.newArrayList();
        for (TTableName tableName : request.query_ctx.getTables_missing_stats()) {
            tableNames.add(tableName.db_name + "." + tableName.table_name);
        }
        str.append("WARNING: The following tables are missing relevant table " + "and/or column statistics.\n" + Joiner.on(", ").join(tableNames) + "\n");
        hasHeader = true;
    }
    if (request.query_ctx.isDisable_spilling()) {
        str.append("WARNING: Spilling is disabled for this query as a safety guard.\n" + "Reason: Query option disable_unsafe_spills is set, at least one table\n" + "is missing relevant stats, and no plan hints were given.\n");
        hasHeader = true;
    }
    if (hasHeader)
        str.append("\n");
    if (explainLevel.ordinal() < TExplainLevel.VERBOSE.ordinal()) {
        // Print the non-fragmented parallel plan.
        str.append(fragments.get(0).getExplainString(explainLevel));
    } else {
        // Print the fragmented parallel plan.
        for (int i = 0; i < fragments.size(); ++i) {
            PlanFragment fragment = fragments.get(i);
            str.append(fragment.getExplainString(explainLevel));
            if (explainLevel == TExplainLevel.VERBOSE && i + 1 != fragments.size()) {
                str.append("\n");
            }
        }
    }
    return str.toString();
}
#end_block

#method_before
@Override
public void load(boolean reuseMetadata, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    if (cause_ instanceof TableLoadingException) {
        throw (TableLoadingException) cause_;
    } else {
        throw new TableLoadingException("Table metadata incomplete: ", cause_);
    }
}
#method_after
@Override
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    if (cause_ instanceof TableLoadingException) {
        throw (TableLoadingException) cause_;
    } else {
        throw new TableLoadingException("Table metadata incomplete: ", cause_);
    }
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    super.init(analyzer);
    assignedConjuncts_ = analyzer.getAssignedConjuncts();
    otherJoinConjuncts_ = Expr.substituteList(otherJoinConjuncts_, getCombinedChildSmap(), analyzer, false);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    // Do not call super.init() to defer computeStats() until all conjuncts
    // have been collected.
    assignConjuncts(analyzer);
    createDefaultSmap(analyzer);
    assignedConjuncts_ = analyzer.getAssignedConjuncts();
    otherJoinConjuncts_ = Expr.substituteList(otherJoinConjuncts_, getCombinedChildSmap(), analyzer, false);
}
#end_block

#method_before
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    if (joinOp_.isSemiJoin()) {
        cardinality_ = getSemiJoinCardinality();
    } else if (joinOp_.isInnerJoin() || joinOp_.isOuterJoin()) {
        cardinality_ = getJoinCardinality(analyzer);
    }
    // Impose lower/upper bounds on the cardinality based on the join type.
    long leftCard = getChild(0).cardinality_;
    long rightCard = getChild(1).cardinality_;
    switch(joinOp_) {
        case LEFT_SEMI_JOIN:
            {
                if (leftCard != -1) {
                    cardinality_ = Math.min(leftCard, cardinality_);
                }
                break;
            }
        case RIGHT_SEMI_JOIN:
            {
                if (rightCard != -1) {
                    cardinality_ = Math.min(rightCard, cardinality_);
                }
                break;
            }
        case LEFT_OUTER_JOIN:
            {
                if (leftCard != -1) {
                    cardinality_ = Math.max(leftCard, cardinality_);
                }
                break;
            }
        case RIGHT_OUTER_JOIN:
            {
                if (rightCard != -1) {
                    cardinality_ = Math.max(rightCard, cardinality_);
                }
                break;
            }
        case FULL_OUTER_JOIN:
            {
                if (leftCard != -1 && rightCard != -1) {
                    long cardinalitySum = addCardinalities(leftCard, rightCard);
                    cardinality_ = Math.max(cardinalitySum, cardinality_);
                }
                break;
            }
        case LEFT_ANTI_JOIN:
        case NULL_AWARE_LEFT_ANTI_JOIN:
            {
                if (leftCard != -1) {
                    cardinality_ = Math.min(leftCard, cardinality_);
                }
                break;
            }
        case RIGHT_ANTI_JOIN:
            {
                if (rightCard != -1) {
                    cardinality_ = Math.min(rightCard, cardinality_);
                }
                break;
            }
        case CROSS_JOIN:
            {
                if (getChild(0).cardinality_ == -1 || getChild(1).cardinality_ == -1) {
                    cardinality_ = -1;
                } else {
                    cardinality_ = multiplyCardinalities(getChild(0).cardinality_, getChild(1).cardinality_);
                }
                break;
            }
    }
    cardinality_ = capAtLimit(cardinality_);
    Preconditions.checkState(hasValidStats());
    LOG.debug("stats Join: cardinality=" + Long.toString(cardinality_));
}
#method_after
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    if (joinOp_.isSemiJoin()) {
        cardinality_ = getSemiJoinCardinality();
    } else if (joinOp_.isInnerJoin() || joinOp_.isOuterJoin()) {
        cardinality_ = getJoinCardinality(analyzer);
    } else {
        Preconditions.checkState(joinOp_.isCrossJoin());
        long leftCard = getChild(0).cardinality_;
        long rightCard = getChild(1).cardinality_;
        if (leftCard != -1 && rightCard != -1) {
            cardinality_ = multiplyCardinalities(leftCard, rightCard);
        }
    }
    // Impose lower/upper bounds on the cardinality based on the join type.
    long leftCard = getChild(0).cardinality_;
    long rightCard = getChild(1).cardinality_;
    switch(joinOp_) {
        case LEFT_SEMI_JOIN:
            {
                if (leftCard != -1) {
                    cardinality_ = Math.min(leftCard, cardinality_);
                }
                break;
            }
        case RIGHT_SEMI_JOIN:
            {
                if (rightCard != -1) {
                    cardinality_ = Math.min(rightCard, cardinality_);
                }
                break;
            }
        case LEFT_OUTER_JOIN:
            {
                if (leftCard != -1) {
                    cardinality_ = Math.max(leftCard, cardinality_);
                }
                break;
            }
        case RIGHT_OUTER_JOIN:
            {
                if (rightCard != -1) {
                    cardinality_ = Math.max(rightCard, cardinality_);
                }
                break;
            }
        case FULL_OUTER_JOIN:
            {
                if (leftCard != -1 && rightCard != -1) {
                    long cardinalitySum = addCardinalities(leftCard, rightCard);
                    cardinality_ = Math.max(cardinalitySum, cardinality_);
                }
                break;
            }
        case LEFT_ANTI_JOIN:
        case NULL_AWARE_LEFT_ANTI_JOIN:
            {
                if (leftCard != -1) {
                    cardinality_ = Math.min(leftCard, cardinality_);
                }
                break;
            }
        case RIGHT_ANTI_JOIN:
            {
                if (rightCard != -1) {
                    cardinality_ = Math.min(rightCard, cardinality_);
                }
                break;
            }
        case CROSS_JOIN:
            {
                if (getChild(0).cardinality_ == -1 || getChild(1).cardinality_ == -1) {
                    cardinality_ = -1;
                } else {
                    cardinality_ = multiplyCardinalities(getChild(0).cardinality_, getChild(1).cardinality_);
                }
                break;
            }
    }
    cardinality_ = capAtLimit(cardinality_);
    Preconditions.checkState(hasValidStats());
    LOG.debug("stats Join: cardinality=" + Long.toString(cardinality_));
}
#end_block

#method_before
private void analyzeKuduTable(Analyzer analyzer) throws AnalysisException {
    // Validate that Kudu table is correctly specified.
    if (!KuduTable.tableParamsAreValid(getTblProperties())) {
        throw new AnalysisException("Kudu table is missing parameters " + String.format("in table properties. Please verify if %s, %s, and %s are " + "present and have valid values.", KuduTable.KEY_TABLE_NAME, KuduTable.KEY_MASTER_ADDRESSES, KuduTable.KEY_KEY_COLUMNS));
    }
    // Kudu table cannot be a cached table
    if (cachingOp_ != null) {
        throw new AnalysisException("A Kudu table cannot be cached in HDFS.");
    }
    if (distributeParams_ != null) {
        List<String> keyColumns = KuduUtil.parseKeyColumnsAsList(getTblProperties().get(KuduTable.KEY_KEY_COLUMNS));
        for (DistributeParam d : distributeParams_) {
            // If the columns are not set, default to all key columns
            if (d.getColumns() == null)
                d.setColumns(keyColumns);
            d.analyze(analyzer);
        }
    }
}
#method_after
private void analyzeKuduTable(Analyzer analyzer) throws AnalysisException {
    // Validate that Kudu table is correctly specified.
    if (!KuduTable.tableParamsAreValid(getTblProperties())) {
        throw new AnalysisException("Kudu table is missing parameters " + String.format("in table properties. Please verify if %s, %s, and %s are " + "present and have valid values.", KuduTable.KEY_TABLE_NAME, KuduTable.KEY_MASTER_ADDRESSES, KuduTable.KEY_KEY_COLUMNS));
    }
    // Kudu table cannot be a cached table
    if (cachingOp_ != null) {
        throw new AnalysisException("A Kudu table cannot be cached in HDFS.");
    }
    if (distributeParams_ != null) {
        if (isExternal_) {
            throw new AnalysisException("The DISTRIBUTE BY clause may not be specified for external tables.");
        }
        List<String> keyColumns = KuduUtil.parseKeyColumnsAsList(getTblProperties().get(KuduTable.KEY_KEY_COLUMNS));
        for (DistributeParam d : distributeParams_) {
            // If the columns are not set, default to all key columns
            if (d.getColumns() == null)
                d.setColumns(keyColumns);
            d.analyze(analyzer);
        }
    } else if (!isExternal_) {
        throw new AnalysisException("A data distribution must be specified using the DISTRIBUTE BY clause.");
    }
}
#end_block

#method_before
public void analyze(Analyzer analyzer, Privilege privilege, FsAction perm, boolean registerPrivReq) throws AnalysisException {
    if (location_.isEmpty()) {
        throw new AnalysisException("URI path cannot be empty.");
    }
    uriPath_ = new Path(location_);
    if (!uriPath_.isUriPathAbsolute()) {
        throw new AnalysisException("URI path must be absolute: " + uriPath_);
    }
    uriPath_ = FileSystemUtil.createFullyQualifiedPath(uriPath_);
    // Check if parent path exists and if impala is allowed to access it.
    Path parentPath = uriPath_.getParent();
    try {
        FileSystem fs = uriPath_.getFileSystem(FileSystemUtil.getConfiguration());
        StringBuilder errorMsg = new StringBuilder();
        if (!FileSystemUtil.isPathReachable(parentPath, fs, errorMsg)) {
            analyzer.addWarning(String.format("Path '%s' cannot be reached: %s", parentPath, errorMsg.toString()));
        } else if (perm != FsAction.NONE) {
            FsPermissionChecker checker = FsPermissionChecker.getInstance();
            if (!checker.getPermissions(fs, parentPath).checkPermissions(perm)) {
                analyzer.addWarning(String.format("Impala does not have %s access to path '%s'", perm.toString(), parentPath));
            }
        }
    } catch (IOException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    if (registerPrivReq) {
        analyzer.registerPrivReq(new PrivilegeRequest(new AuthorizeableUri(uriPath_.toString()), privilege));
    }
}
#method_after
public void analyze(Analyzer analyzer, Privilege privilege, FsAction perm, boolean registerPrivReq) throws AnalysisException {
    if (location_.isEmpty()) {
        throw new AnalysisException("URI path cannot be empty.");
    }
    uriPath_ = new Path(location_);
    if (!uriPath_.isUriPathAbsolute()) {
        throw new AnalysisException("URI path must be absolute: " + uriPath_);
    }
    uriPath_ = FileSystemUtil.createFullyQualifiedPath(uriPath_);
    // Check if parent path exists and if impala is allowed to access it.
    Path parentPath = uriPath_.getParent();
    try {
        FileSystem fs = uriPath_.getFileSystem(FileSystemUtil.getConfiguration());
        boolean pathExists = false;
        StringBuilder errorMsg = new StringBuilder();
        try {
            pathExists = fs.exists(parentPath);
            if (!pathExists)
                errorMsg.append("Path does not exist.");
        } catch (Exception e) {
            errorMsg.append(e.getMessage());
        }
        if (!pathExists) {
            analyzer.addWarning(String.format("Path '%s' cannot be reached: %s", parentPath, errorMsg.toString()));
        } else if (perm != FsAction.NONE) {
            FsPermissionChecker checker = FsPermissionChecker.getInstance();
            if (!checker.getPermissions(fs, parentPath).checkPermissions(perm)) {
                analyzer.addWarning(String.format("Impala does not have %s access to path '%s'", perm.toString(), parentPath));
            }
        }
    } catch (IOException e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    if (registerPrivReq) {
        analyzer.registerPrivReq(new PrivilegeRequest(new AuthorizeableUri(uriPath_.toString()), privilege));
    }
}
#end_block

#method_before
@Override
public String toSql() {
    if (num_buckets_ == NO_BUCKETS) {
        StringBuilder builder = new StringBuilder();
        for (ArrayList<LiteralExpr> splitRow : splitRows_) {
            splitRowToString(splitRow);
        }
        return String.format("RANGE(%s) INTO RANGES(%s)", Joiner.on(", ").join(columns_), builder.toString());
    } else {
        return String.format("HASH(%s) INTO %d BUCKETS", Joiner.on(", ").join(columns_), num_buckets_);
    }
}
#method_after
@Override
public String toSql() {
    if (num_buckets_ == NO_BUCKETS) {
        List<String> splitRowStrings = Lists.newArrayList();
        for (ArrayList<LiteralExpr> splitRow : splitRows_) {
            splitRowStrings.add(splitRowToString(splitRow));
        }
        return String.format("RANGE(%s) INTO RANGES(%s)", Joiner.on(", ").join(columns_), Joiner.on(", ").join(splitRowStrings));
    } else {
        return String.format("HASH(%s) INTO %d BUCKETS", Joiner.on(", ").join(columns_), num_buckets_);
    }
}
#end_block

#method_before
private String splitRowToString(ArrayList<LiteralExpr> splitRow) {
    StringBuilder builder = new StringBuilder();
    builder.append("[");
    List<String> rangeElementStrings = Lists.newArrayList();
    for (LiteralExpr rangeElement : splitRow) {
        rangeElementStrings.add(rangeElement.getStringValue());
    }
    builder.append(Joiner.on(",").join(rangeElementStrings));
    builder.append("]");
    return builder.toString();
}
#method_after
private String splitRowToString(ArrayList<LiteralExpr> splitRow) {
    StringBuilder builder = new StringBuilder();
    builder.append("(");
    List<String> rangeElementStrings = Lists.newArrayList();
    for (LiteralExpr rangeElement : splitRow) {
        rangeElementStrings.add(rangeElement.toSql());
    }
    builder.append(Joiner.on(", ").join(rangeElementStrings));
    builder.append(")");
    return builder.toString();
}
#end_block

#method_before
protected void loadAllColumnStats(HiveMetaStoreClient client) {
    LOG.debug("Loading column stats for table: " + name_);
    List<ColumnStatisticsObj> colStats;
    // We need to only query those columns which may have stats; asking HMS for other
    // columns causes loadAllColumnStats() to return nothing.
    List<String> colNames = getColumnNamesWithHmsStats();
    try {
        colStats = client.getTableColumnStatistics(db_.getName(), name_, colNames);
    } catch (Exception e) {
        LOG.warn("Could not load column statistics for: " + getFullName(), e);
        return;
    }
    for (ColumnStatisticsObj stats : colStats) {
        Column col = getColumn(stats.getColName());
        Preconditions.checkNotNull(col);
        if (!ColumnStats.isSupportedColType(col.getType())) {
            LOG.warn(String.format("Statistics for %s, column %s are not supported as " + "column has type %s", getFullName(), col.getName(), col.getType()));
            continue;
        }
        if (!col.updateStats(stats.getStatsData())) {
            LOG.warn(String.format("Failed to load column stats for %s, column %s. Stats " + "may be incompatible with column type %s. Consider regenerating statistics " + "for %s.", getFullName(), col.getName(), col.getType(), getFullName()));
            continue;
        }
    }
}
#method_after
protected void loadAllColumnStats(IMetaStoreClient client) {
    LOG.debug("Loading column stats for table: " + name_);
    List<ColumnStatisticsObj> colStats;
    // We need to only query those columns which may have stats; asking HMS for other
    // columns causes loadAllColumnStats() to return nothing.
    List<String> colNames = getColumnNamesWithHmsStats();
    try {
        colStats = client.getTableColumnStatistics(db_.getName(), name_, colNames);
    } catch (Exception e) {
        LOG.warn("Could not load column statistics for: " + getFullName(), e);
        return;
    }
    for (ColumnStatisticsObj stats : colStats) {
        Column col = getColumn(stats.getColName());
        Preconditions.checkNotNull(col);
        if (!ColumnStats.isSupportedColType(col.getType())) {
            LOG.warn(String.format("Statistics for %s, column %s are not supported as " + "column has type %s", getFullName(), col.getName(), col.getType()));
            continue;
        }
        if (!col.updateStats(stats.getStatsData())) {
            LOG.warn(String.format("Failed to load column stats for %s, column %s. Stats " + "may be incompatible with column type %s. Consider regenerating statistics " + "for %s.", getFullName(), col.getName(), col.getType(), getFullName()));
            continue;
        }
    }
}
#end_block

#method_before
public ArrayList<Column> getColumnsInHiveOrder() {
    ArrayList<Column> columns = Lists.newArrayList(getNonClusteringColumns());
    for (Column column : colsByPos_.subList(0, numClusteringCols_)) {
        columns.add(column);
    }
    return columns;
}
#method_after
public ArrayList<Column> getColumnsInHiveOrder() {
    ArrayList<Column> columns = Lists.newArrayList(getNonClusteringColumns());
    columns.addAll(getClusteringColumns());
    return columns;
}
#end_block

#method_before
private PlanNode createCheapestJoinPlan(Analyzer analyzer, List<Pair<TableRef, PlanNode>> parentRefPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    LOG.trace("createCheapestJoinPlan");
    if (parentRefPlans.size() == 1)
        return parentRefPlans.get(0).second;
    // collect eligible candidates for the leftmost input; list contains
    // (plan, materialized size)
    ArrayList<Pair<TableRef, Long>> candidates = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : parentRefPlans) {
        TableRef ref = entry.first;
        JoinOperator joinOp = ref.getJoinOp();
        // consideration of the joinOps that result from such a re-ordering (IMPALA-1281).
        if (((joinOp.isOuterJoin() || joinOp.isSemiJoin() || joinOp.isCrossJoin()) && ref != parentRefPlans.get(1).first) || joinOp.isNullAwareLeftAntiJoin()) {
            continue;
        }
        PlanNode plan = entry.second;
        if (plan.getCardinality() == -1) {
            // use 0 for the size to avoid it becoming the leftmost input
            // TODO: Consider raw size of scanned partitions in the absence of stats.
            candidates.add(new Pair(ref, new Long(0)));
            LOG.trace("candidate " + ref.getUniqueAlias() + ": 0");
            continue;
        }
        Preconditions.checkState(ref.isAnalyzed());
        long materializedSize = (long) Math.ceil(plan.getAvgRowSize() * (double) plan.getCardinality());
        candidates.add(new Pair(ref, new Long(materializedSize)));
        LOG.trace("candidate " + ref.getUniqueAlias() + ": " + Long.toString(materializedSize));
    }
    if (candidates.isEmpty())
        return null;
    // order candidates by descending materialized size; we want to minimize the memory
    // consumption of the materialized hash tables required for the join sequence
    Collections.sort(candidates, new Comparator<Pair<TableRef, Long>>() {

        public int compare(Pair<TableRef, Long> a, Pair<TableRef, Long> b) {
            long diff = b.second - a.second;
            return (diff < 0 ? -1 : (diff > 0 ? 1 : 0));
        }
    });
    for (Pair<TableRef, Long> candidate : candidates) {
        PlanNode result = createJoinPlan(analyzer, candidate.first, parentRefPlans, subplanRefs);
        if (result != null)
            return result;
    }
    return null;
}
#method_after
private PlanNode createCheapestJoinPlan(Analyzer analyzer, List<Pair<TableRef, PlanNode>> parentRefPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    LOG.trace("createCheapestJoinPlan");
    if (parentRefPlans.size() == 1)
        return parentRefPlans.get(0).second;
    // collect eligible candidates for the leftmost input; list contains
    // (plan, materialized size)
    ArrayList<Pair<TableRef, Long>> candidates = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : parentRefPlans) {
        TableRef ref = entry.first;
        JoinOperator joinOp = ref.getJoinOp();
        // consideration of the joinOps that result from such a re-ordering (IMPALA-1281).
        if (joinOp.isOuterJoin() || joinOp.isSemiJoin() || joinOp.isCrossJoin())
            continue;
        PlanNode plan = entry.second;
        if (plan.getCardinality() == -1) {
            // use 0 for the size to avoid it becoming the leftmost input
            // TODO: Consider raw size of scanned partitions in the absence of stats.
            candidates.add(new Pair(ref, new Long(0)));
            LOG.trace("candidate " + ref.getUniqueAlias() + ": 0");
            continue;
        }
        Preconditions.checkState(ref.isAnalyzed());
        long materializedSize = (long) Math.ceil(plan.getAvgRowSize() * (double) plan.getCardinality());
        candidates.add(new Pair(ref, new Long(materializedSize)));
        LOG.trace("candidate " + ref.getUniqueAlias() + ": " + Long.toString(materializedSize));
    }
    if (candidates.isEmpty())
        return null;
    // order candidates by descending materialized size; we want to minimize the memory
    // consumption of the materialized hash tables required for the join sequence
    Collections.sort(candidates, new Comparator<Pair<TableRef, Long>>() {

        public int compare(Pair<TableRef, Long> a, Pair<TableRef, Long> b) {
            long diff = b.second - a.second;
            return (diff < 0 ? -1 : (diff > 0 ? 1 : 0));
        }
    });
    for (Pair<TableRef, Long> candidate : candidates) {
        PlanNode result = createJoinPlan(analyzer, candidate.first, parentRefPlans, subplanRefs);
        if (result != null)
            return result;
    }
    return null;
}
#end_block

#method_before
private PlanNode createJoinPlan(Analyzer analyzer, TableRef leftmostRef, List<Pair<TableRef, PlanNode>> refPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    LOG.trace("createJoinPlan: " + leftmostRef.getUniqueAlias());
    // the refs that have yet to be joined
    List<Pair<TableRef, PlanNode>> remainingRefs = Lists.newArrayList();
    // root of accumulated join plan
    PlanNode root = null;
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        if (entry.first == leftmostRef) {
            root = entry.second;
        } else {
            remainingRefs.add(entry);
        }
    }
    Preconditions.checkNotNull(root);
    // If the leftmostTblRef is an outer/semi/cross join, we must invert it.
    boolean planHasInvertedJoin = false;
    if (leftmostRef.getJoinOp().isOuterJoin() || leftmostRef.getJoinOp().isSemiJoin() || leftmostRef.getJoinOp().isCrossJoin()) {
        // TODO: Revisit the interaction of join inversion here and the analysis state
        // that is changed in analyzer.invertOuterJoin(). Changing the analysis state
        // should not be necessary because the semantics of an inverted outer join do
        // not change.
        leftmostRef.invertJoin(refPlans, analyzer);
        planHasInvertedJoin = true;
        // Avoid swapping the refPlan elements in-place.
        refPlans = Lists.newArrayList(refPlans);
        Collections.swap(refPlans, 0, 1);
    }
    // Maps from a TableRef in refPlans with an outer/semi join op to the set of
    // TableRefs that precede it refPlans (i.e., in FROM-clause order).
    // The map is used to place outer/semi joins at a fixed position in the plan tree
    // (IMPALA-860), s.t. all the tables appearing to the left/right of an outer/semi
    // join in the original query still remain to the left/right after join ordering.
    // This prevents join re-ordering across outer/semi joins which is generally wrong.
    Map<TableRef, Set<TableRef>> precedingRefs = Maps.newHashMap();
    List<TableRef> tmpTblRefs = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        TableRef tblRef = entry.first;
        if (tblRef.getJoinOp().isOuterJoin() || tblRef.getJoinOp().isSemiJoin()) {
            precedingRefs.put(tblRef, Sets.newHashSet(tmpTblRefs));
        }
        tmpTblRefs.add(tblRef);
    }
    // Refs that have been joined. The union of joinedRefs and the refs in remainingRefs
    // are the set of all table refs.
    Set<TableRef> joinedRefs = Sets.newHashSet(leftmostRef);
    long numOps = 0;
    int i = 0;
    while (!remainingRefs.isEmpty()) {
        // We minimize the resulting cardinality at each step in the join chain,
        // which minimizes the total number of hash table lookups.
        PlanNode newRoot = null;
        Pair<TableRef, PlanNode> minEntry = null;
        for (Pair<TableRef, PlanNode> entry : remainingRefs) {
            TableRef ref = entry.first;
            JoinOperator joinOp = ref.getJoinOp();
            // Place outer/semi joins at a fixed position in the plan tree.
            Set<TableRef> requiredRefs = precedingRefs.get(ref);
            if (requiredRefs != null) {
                Preconditions.checkState(joinOp.isOuterJoin() || joinOp.isSemiJoin());
                // outer/semi joins.
                if (!requiredRefs.equals(joinedRefs))
                    break;
            }
            PlanNode rhsPlan = entry.second;
            boolean invertJoin = false;
            if (joinOp.isOuterJoin() || joinOp.isSemiJoin() || joinOp.isCrossJoin()) {
                // Invert the join if doing so reduces the size of build-side hash table
                // (may also reduce network costs depending on the join strategy).
                // Only consider this optimization if both the lhs/rhs cardinalities are known.
                // The null-aware left anti-join operator is never considered for inversion
                // because we can't execute the null-aware right anti-join efficiently.
                long lhsCard = root.getCardinality();
                long rhsCard = rhsPlan.getCardinality();
                if (lhsCard != -1 && rhsCard != -1 && lhsCard * root.getAvgRowSize() < rhsCard * rhsPlan.getAvgRowSize() && !joinOp.isNullAwareLeftAntiJoin()) {
                    invertJoin = true;
                }
            }
            // null-aware left anti join which cannot be inverted.
            if (root instanceof SingularRowSrcNode && !joinOp.isNullAwareLeftAntiJoin()) {
                invertJoin = true;
            }
            analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
            PlanNode candidate = null;
            if (invertJoin) {
                ref.setJoinOp(ref.getJoinOp().invert());
                candidate = createJoinNode(analyzer, rhsPlan, root, ref, null);
                planHasInvertedJoin = true;
            } else {
                candidate = createJoinNode(analyzer, root, rhsPlan, null, ref);
            }
            if (candidate == null)
                continue;
            LOG.trace("cardinality=" + Long.toString(candidate.getCardinality()));
            // position in the plan.
            if (joinOp.isOuterJoin() || joinOp.isSemiJoin()) {
                newRoot = candidate;
                minEntry = entry;
                break;
            }
            // infrastructure.
            if (newRoot == null || (candidate.getClass().equals(newRoot.getClass()) && candidate.getCardinality() < newRoot.getCardinality()) || (candidate instanceof HashJoinNode && newRoot instanceof NestedLoopJoinNode)) {
                newRoot = candidate;
                minEntry = entry;
            }
        }
        if (newRoot == null) {
            // Currently, it should not be possible to invert a join for a plan that turns
            // out to be non-executable because (1) the joins we consider for inversion are
            // barriers in the join order, and (2) the caller of this function only considers
            // other leftmost table refs if a plan turns out to be non-executable.
            // TODO: This preconditions check will need to be changed to undo the in-place
            // modifications made to table refs for join inversion, if the caller decides to
            // explore more leftmost table refs.
            Preconditions.checkState(!planHasInvertedJoin);
            return null;
        }
        // we need to insert every rhs row into the hash table and then look up
        // every lhs row
        long lhsCardinality = root.getCardinality();
        long rhsCardinality = minEntry.second.getCardinality();
        numOps += lhsCardinality + rhsCardinality;
        LOG.debug(Integer.toString(i) + " chose " + minEntry.first.getUniqueAlias() + " #lhs=" + Long.toString(lhsCardinality) + " #rhs=" + Long.toString(rhsCardinality) + " #ops=" + Long.toString(numOps));
        remainingRefs.remove(minEntry);
        joinedRefs.add(minEntry.first);
        root = newRoot;
        // Create a Subplan on top of the new root for all the subplan refs that can be
        // evaluated at this point.
        // TODO: Once we have stats on nested collections, we should consider the join
        // order in conjunction with the placement of SubplanNodes, i.e., move the creation
        // of SubplanNodes into the join-ordering loop above.
        root = createSubplan(root, subplanRefs, false, false, analyzer);
        // with a dense sequence of node ids
        if (root instanceof SubplanNode)
            root.getChild(0).setId(ctx_.getNextNodeId());
        root.setId(ctx_.getNextNodeId());
        analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
        ++i;
    }
    return root;
}
#method_after
private PlanNode createJoinPlan(Analyzer analyzer, TableRef leftmostRef, List<Pair<TableRef, PlanNode>> refPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    LOG.trace("createJoinPlan: " + leftmostRef.getUniqueAlias());
    // the refs that have yet to be joined
    List<Pair<TableRef, PlanNode>> remainingRefs = Lists.newArrayList();
    // root of accumulated join plan
    PlanNode root = null;
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        if (entry.first == leftmostRef) {
            root = entry.second;
        } else {
            remainingRefs.add(entry);
        }
    }
    Preconditions.checkNotNull(root);
    // Maps from a TableRef in refPlans with an outer/semi join op to the set of
    // TableRefs that precede it refPlans (i.e., in FROM-clause order).
    // The map is used to place outer/semi joins at a fixed position in the plan tree
    // (IMPALA-860), s.t. all the tables appearing to the left/right of an outer/semi
    // join in the original query still remain to the left/right after join ordering.
    // This prevents join re-ordering across outer/semi joins which is generally wrong.
    Map<TableRef, Set<TableRef>> precedingRefs = Maps.newHashMap();
    List<TableRef> tmpTblRefs = Lists.newArrayList();
    for (Pair<TableRef, PlanNode> entry : refPlans) {
        TableRef tblRef = entry.first;
        if (tblRef.getJoinOp().isOuterJoin() || tblRef.getJoinOp().isSemiJoin()) {
            precedingRefs.put(tblRef, Sets.newHashSet(tmpTblRefs));
        }
        tmpTblRefs.add(tblRef);
    }
    // Refs that have been joined. The union of joinedRefs and the refs in remainingRefs
    // are the set of all table refs.
    Set<TableRef> joinedRefs = Sets.newHashSet(leftmostRef);
    long numOps = 0;
    int i = 0;
    while (!remainingRefs.isEmpty()) {
        // We minimize the resulting cardinality at each step in the join chain,
        // which minimizes the total number of hash table lookups.
        PlanNode newRoot = null;
        Pair<TableRef, PlanNode> minEntry = null;
        for (Pair<TableRef, PlanNode> entry : remainingRefs) {
            TableRef ref = entry.first;
            JoinOperator joinOp = ref.getJoinOp();
            // Place outer/semi joins at a fixed position in the plan tree.
            Set<TableRef> requiredRefs = precedingRefs.get(ref);
            if (requiredRefs != null) {
                Preconditions.checkState(joinOp.isOuterJoin() || joinOp.isSemiJoin());
                // outer/semi joins.
                if (!requiredRefs.equals(joinedRefs))
                    break;
            }
            analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
            PlanNode candidate = createJoinNode(root, entry.second, ref, analyzer);
            if (candidate == null)
                continue;
            LOG.trace("cardinality=" + Long.toString(candidate.getCardinality()));
            // position in the plan.
            if (joinOp.isOuterJoin() || joinOp.isSemiJoin()) {
                newRoot = candidate;
                minEntry = entry;
                break;
            }
            // infrastructure.
            if (newRoot == null || (candidate.getClass().equals(newRoot.getClass()) && candidate.getCardinality() < newRoot.getCardinality()) || (candidate instanceof HashJoinNode && newRoot instanceof NestedLoopJoinNode)) {
                newRoot = candidate;
                minEntry = entry;
            }
        }
        if (newRoot == null) {
            // Could not generate a valid plan.
            return null;
        }
        // we need to insert every rhs row into the hash table and then look up
        // every lhs row
        long lhsCardinality = root.getCardinality();
        long rhsCardinality = minEntry.second.getCardinality();
        numOps += lhsCardinality + rhsCardinality;
        LOG.debug(Integer.toString(i) + " chose " + minEntry.first.getUniqueAlias() + " #lhs=" + Long.toString(lhsCardinality) + " #rhs=" + Long.toString(rhsCardinality) + " #ops=" + Long.toString(numOps));
        remainingRefs.remove(minEntry);
        joinedRefs.add(minEntry.first);
        root = newRoot;
        // Create a Subplan on top of the new root for all the subplan refs that can be
        // evaluated at this point.
        // TODO: Once we have stats on nested collections, we should consider the join
        // order in conjunction with the placement of SubplanNodes, i.e., move the creation
        // of SubplanNodes into the join-ordering loop above.
        root = createSubplan(root, subplanRefs, false, analyzer);
        // with a dense sequence of node ids
        if (root instanceof SubplanNode)
            root.getChild(0).setId(ctx_.getNextNodeId());
        root.setId(ctx_.getNextNodeId());
        analyzer.setAssignedConjuncts(root.getAssignedConjuncts());
        ++i;
    }
    return root;
}
#end_block

#method_before
private PlanNode createFromClauseJoinPlan(Analyzer analyzer, List<Pair<TableRef, PlanNode>> parentRefPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    // create left-deep sequence of binary hash joins; assign node ids as we go along
    Preconditions.checkState(!parentRefPlans.isEmpty());
    PlanNode root = parentRefPlans.get(0).second;
    for (int i = 1; i < parentRefPlans.size(); ++i) {
        TableRef innerRef = parentRefPlans.get(i).first;
        PlanNode innerPlan = parentRefPlans.get(i).second;
        root = createJoinNode(analyzer, root, innerPlan, null, innerRef);
        if (root != null)
            root = createSubplan(root, subplanRefs, true, false, analyzer);
        if (root instanceof SubplanNode)
            root.getChild(0).setId(ctx_.getNextNodeId());
        root.setId(ctx_.getNextNodeId());
    }
    return root;
}
#method_after
private PlanNode createFromClauseJoinPlan(Analyzer analyzer, List<Pair<TableRef, PlanNode>> parentRefPlans, List<SubplanRef> subplanRefs) throws ImpalaException {
    // create left-deep sequence of binary hash joins; assign node ids as we go along
    Preconditions.checkState(!parentRefPlans.isEmpty());
    PlanNode root = parentRefPlans.get(0).second;
    for (int i = 1; i < parentRefPlans.size(); ++i) {
        TableRef innerRef = parentRefPlans.get(i).first;
        PlanNode innerPlan = parentRefPlans.get(i).second;
        root = createJoinNode(root, innerPlan, innerRef, analyzer);
        if (root != null)
            root = createSubplan(root, subplanRefs, false, analyzer);
        if (root instanceof SubplanNode)
            root.getChild(0).setId(ctx_.getNextNodeId());
        root.setId(ctx_.getNextNodeId());
    }
    return root;
}
#end_block

#method_before
private PlanNode createSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    // no from clause -> materialize the select's exprs with a UnionNode
    if (selectStmt.getTableRefs().isEmpty()) {
        return createConstantSelectPlan(selectStmt, analyzer);
    }
    // Slot materialization:
    // We need to mark all slots as materialized that are needed during the execution
    // of selectStmt, and we need to do that prior to creating plans for the TableRefs
    // (because createTableRefNode() might end up calling computeMemLayout() on one or
    // more TupleDescriptors, at which point all referenced slots need to be marked).
    // 
    // For non-join predicates, slots are marked as follows:
    // - for base table scan predicates, this is done directly by ScanNode.init(), which
    // can do a better job because it doesn't need to materialize slots that are only
    // referenced for partition pruning, for instance
    // - for inline views, non-join predicates are pushed down, at which point the
    // process repeats itself.
    selectStmt.materializeRequiredSlots(analyzer);
    ArrayList<TupleId> rowTuples = Lists.newArrayList();
    // collect output tuples of subtrees
    for (TableRef tblRef : selectStmt.getTableRefs()) {
        rowTuples.addAll(tblRef.getMaterializedTupleIds());
    }
    // computed.
    if (analyzer.hasEmptySpjResultSet()) {
        unmarkCollectionSlots(selectStmt);
        PlanNode emptySetNode = new EmptySetNode(ctx_.getNextNodeId(), rowTuples);
        emptySetNode.init(analyzer);
        emptySetNode.setOutputSmap(selectStmt.getBaseTblSmap());
        return createAggregationPlan(selectStmt, analyzer, emptySetNode);
    }
    AggregateInfo aggInfo = selectStmt.getAggInfo();
    // For queries which contain partition columns only, we may use the metadata instead
    // of table scans. This is only feasible if all materialized aggregate expressions
    // have distinct semantics. Please see createHdfsScanPlan() for details.
    boolean fastPartitionKeyScans = analyzer.getQueryCtx().getRequest().query_options.optimize_partition_key_scans && aggInfo != null && aggInfo.hasAllDistinctAgg();
    // Separate table refs into parent refs (uncorrelated or absolute) and
    // subplan refs (correlated or relative), and generate their plan.
    boolean isStraightJoin = selectStmt.getSelectList().isStraightJoin();
    List<TableRef> parentRefs = Lists.newArrayList();
    List<SubplanRef> subplanRefs = Lists.newArrayList();
    computeParentAndSubplanRefs(selectStmt.getTableRefs(), isStraightJoin, parentRefs, subplanRefs);
    PlanNode root = createTableRefsPlan(parentRefs, subplanRefs, isStraightJoin, fastPartitionKeyScans, analyzer);
    // add aggregation, if any
    if (aggInfo != null)
        root = createAggregationPlan(selectStmt, analyzer, root);
    // Preconditions.checkState(!analyzer.hasUnassignedConjuncts());
    return root;
}
#method_after
private PlanNode createSelectPlan(SelectStmt selectStmt, Analyzer analyzer) throws ImpalaException {
    // no from clause -> materialize the select's exprs with a UnionNode
    if (selectStmt.getTableRefs().isEmpty()) {
        return createConstantSelectPlan(selectStmt, analyzer);
    }
    // Slot materialization:
    // We need to mark all slots as materialized that are needed during the execution
    // of selectStmt, and we need to do that prior to creating plans for the TableRefs
    // (because createTableRefNode() might end up calling computeMemLayout() on one or
    // more TupleDescriptors, at which point all referenced slots need to be marked).
    // 
    // For non-join predicates, slots are marked as follows:
    // - for base table scan predicates, this is done directly by ScanNode.init(), which
    // can do a better job because it doesn't need to materialize slots that are only
    // referenced for partition pruning, for instance
    // - for inline views, non-join predicates are pushed down, at which point the
    // process repeats itself.
    selectStmt.materializeRequiredSlots(analyzer);
    ArrayList<TupleId> rowTuples = Lists.newArrayList();
    // collect output tuples of subtrees
    for (TableRef tblRef : selectStmt.getTableRefs()) {
        rowTuples.addAll(tblRef.getMaterializedTupleIds());
    }
    // computed.
    if (analyzer.hasEmptySpjResultSet()) {
        unmarkCollectionSlots(selectStmt);
        PlanNode emptySetNode = new EmptySetNode(ctx_.getNextNodeId(), rowTuples);
        emptySetNode.init(analyzer);
        emptySetNode.setOutputSmap(selectStmt.getBaseTblSmap());
        return createAggregationPlan(selectStmt, analyzer, emptySetNode);
    }
    AggregateInfo aggInfo = selectStmt.getAggInfo();
    // For queries which contain partition columns only, we may use the metadata instead
    // of table scans. This is only feasible if all materialized aggregate expressions
    // have distinct semantics. Please see createHdfsScanPlan() for details.
    boolean fastPartitionKeyScans = analyzer.getQueryCtx().getRequest().query_options.optimize_partition_key_scans && aggInfo != null && aggInfo.hasAllDistinctAgg();
    // Separate table refs into parent refs (uncorrelated or absolute) and
    // subplan refs (correlated or relative), and generate their plan.
    List<TableRef> parentRefs = Lists.newArrayList();
    List<SubplanRef> subplanRefs = Lists.newArrayList();
    computeParentAndSubplanRefs(selectStmt.getTableRefs(), analyzer.isStraightJoin(), parentRefs, subplanRefs);
    PlanNode root = createTableRefsPlan(parentRefs, subplanRefs, fastPartitionKeyScans, analyzer);
    // add aggregation, if any
    if (aggInfo != null)
        root = createAggregationPlan(selectStmt, analyzer, root);
    // Preconditions.checkState(!analyzer.hasUnassignedConjuncts());
    return root;
}
#end_block

#method_before
private PlanNode createTableRefsPlan(List<TableRef> parentRefs, List<SubplanRef> subplanRefs, boolean isStraightJoin, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    // create plans for our table refs; use a list here instead of a map to
    // maintain a deterministic order of traversing the TableRefs during join
    // plan generation (helps with tests)
    List<Pair<TableRef, PlanNode>> parentRefPlans = Lists.newArrayList();
    for (TableRef ref : parentRefs) {
        PlanNode root = createTableRefNode(ref, isStraightJoin, fastPartitionKeyScans, analyzer);
        Preconditions.checkNotNull(root);
        root = createSubplan(root, subplanRefs, isStraightJoin, true, analyzer);
        parentRefPlans.add(new Pair<TableRef, PlanNode>(ref, root));
    }
    // save state of conjunct assignment; needed for join plan generation
    for (Pair<TableRef, PlanNode> entry : parentRefPlans) {
        entry.second.setAssignedConjuncts(analyzer.getAssignedConjuncts());
    }
    PlanNode root = null;
    if (!isStraightJoin) {
        Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
        root = createCheapestJoinPlan(analyzer, parentRefPlans, subplanRefs);
        // to not incorrectly miss conjuncts.
        if (root == null)
            analyzer.setAssignedConjuncts(assignedConjuncts);
    }
    if (isStraightJoin || root == null) {
        // we didn't have enough stats to do a cost-based join plan, or the STRAIGHT_JOIN
        // keyword was in the select list: use the FROM clause order instead
        root = createFromClauseJoinPlan(analyzer, parentRefPlans, subplanRefs);
        Preconditions.checkNotNull(root);
    }
    return root;
}
#method_after
private PlanNode createTableRefsPlan(List<TableRef> parentRefs, List<SubplanRef> subplanRefs, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    // create plans for our table refs; use a list here instead of a map to
    // maintain a deterministic order of traversing the TableRefs during join
    // plan generation (helps with tests)
    List<Pair<TableRef, PlanNode>> parentRefPlans = Lists.newArrayList();
    for (TableRef ref : parentRefs) {
        PlanNode root = createTableRefNode(ref, fastPartitionKeyScans, analyzer);
        Preconditions.checkNotNull(root);
        root = createSubplan(root, subplanRefs, true, analyzer);
        parentRefPlans.add(new Pair<TableRef, PlanNode>(ref, root));
    }
    // save state of conjunct assignment; needed for join plan generation
    for (Pair<TableRef, PlanNode> entry : parentRefPlans) {
        entry.second.setAssignedConjuncts(analyzer.getAssignedConjuncts());
    }
    PlanNode root = null;
    if (!analyzer.isStraightJoin()) {
        Set<ExprId> assignedConjuncts = analyzer.getAssignedConjuncts();
        root = createCheapestJoinPlan(analyzer, parentRefPlans, subplanRefs);
        // to not incorrectly miss conjuncts.
        if (root == null)
            analyzer.setAssignedConjuncts(assignedConjuncts);
    }
    if (analyzer.isStraightJoin() || root == null) {
        // we didn't have enough stats to do a cost-based join plan, or the STRAIGHT_JOIN
        // keyword was in the select list: use the FROM clause order instead
        root = createFromClauseJoinPlan(analyzer, parentRefPlans, subplanRefs);
        Preconditions.checkNotNull(root);
    }
    return root;
}
#end_block

#method_before
private PlanNode createSubplan(PlanNode root, List<SubplanRef> subplanRefs, boolean isStraightJoin, boolean assignId, Analyzer analyzer) throws ImpalaException {
    Preconditions.checkNotNull(root);
    List<TableRef> applicableRefs = extractApplicableRefs(root, subplanRefs);
    if (applicableRefs.isEmpty())
        return root;
    // Prepend a SingularRowSrcTableRef representing the current row being processed
    // by the SubplanNode from its input (first child).
    Preconditions.checkState(applicableRefs.get(0).getLeftTblRef() == null);
    applicableRefs.add(0, new SingularRowSrcTableRef(root));
    applicableRefs.get(1).setLeftTblRef(applicableRefs.get(0));
    // Construct an incomplete SubplanNode that only knows its input so we can push it
    // into the planner context. The subplan is set after the subplan tree has been
    // constructed.
    SubplanNode subplanNode = new SubplanNode(root);
    if (assignId)
        subplanNode.setId(ctx_.getNextNodeId());
    // Push the SubplanNode such that UnnestNodes and SingularRowSrcNodes can pick up
    // their containing SubplanNode. Also, further plan generation relies on knowing
    // whether we are in a subplan context or not (see computeParentAndSubplanRefs()).
    ctx_.pushSubplan(subplanNode);
    PlanNode subplan = createTableRefsPlan(applicableRefs, subplanRefs, isStraightJoin, false, analyzer);
    ctx_.popSubplan();
    subplanNode.setSubplan(subplan);
    subplanNode.init(analyzer);
    return subplanNode;
}
#method_after
private PlanNode createSubplan(PlanNode root, List<SubplanRef> subplanRefs, boolean assignId, Analyzer analyzer) throws ImpalaException {
    Preconditions.checkNotNull(root);
    List<TableRef> applicableRefs = extractApplicableRefs(root, subplanRefs);
    if (applicableRefs.isEmpty())
        return root;
    // Prepend a SingularRowSrcTableRef representing the current row being processed
    // by the SubplanNode from its input (first child).
    Preconditions.checkState(applicableRefs.get(0).getLeftTblRef() == null);
    applicableRefs.add(0, new SingularRowSrcTableRef(root));
    applicableRefs.get(1).setLeftTblRef(applicableRefs.get(0));
    // Construct an incomplete SubplanNode that only knows its input so we can push it
    // into the planner context. The subplan is set after the subplan tree has been
    // constructed.
    SubplanNode subplanNode = new SubplanNode(root);
    if (assignId)
        subplanNode.setId(ctx_.getNextNodeId());
    // Push the SubplanNode such that UnnestNodes and SingularRowSrcNodes can pick up
    // their containing SubplanNode. Also, further plan generation relies on knowing
    // whether we are in a subplan context or not (see computeParentAndSubplanRefs()).
    ctx_.pushSubplan(subplanNode);
    PlanNode subplan = createTableRefsPlan(applicableRefs, subplanRefs, false, analyzer);
    ctx_.popSubplan();
    subplanNode.setSubplan(subplan);
    subplanNode.init(analyzer);
    return subplanNode;
}
#end_block

#method_before
private PlanNode createJoinNode(Analyzer analyzer, PlanNode outer, PlanNode inner, TableRef outerRef, TableRef innerRef) throws ImpalaException {
    Preconditions.checkState(innerRef != null ^ outerRef != null);
    TableRef tblRef = (innerRef != null) ? innerRef : outerRef;
    // get eq join predicates for the TableRefs' ids (not the PlanNodes' ids, which
    // are materialized)
    List<BinaryPredicate> eqJoinConjuncts = Collections.emptyList();
    if (innerRef != null) {
        eqJoinConjuncts = getHashLookupJoinConjuncts(outer.getTblRefIds(), inner.getTblRefIds(), analyzer);
        // Outer joins should only use On-clause predicates as eqJoinConjuncts.
        if (!innerRef.getJoinOp().isOuterJoin()) {
            analyzer.createEquivConjuncts(outer.getTblRefIds(), inner.getTblRefIds(), eqJoinConjuncts);
        }
    } else {
        eqJoinConjuncts = getHashLookupJoinConjuncts(inner.getTblRefIds(), outer.getTblRefIds(), analyzer);
        // Outer joins should only use On-clause predicates as eqJoinConjuncts.
        if (!outerRef.getJoinOp().isOuterJoin()) {
            analyzer.createEquivConjuncts(inner.getTblRefIds(), outer.getTblRefIds(), eqJoinConjuncts);
        }
        // Reverse the lhs/rhs of the join conjuncts.
        for (BinaryPredicate eqJoinConjunct : eqJoinConjuncts) {
            Expr swapTmp = eqJoinConjunct.getChild(0);
            eqJoinConjunct.setChild(0, eqJoinConjunct.getChild(1));
            eqJoinConjunct.setChild(1, swapTmp);
        }
    }
    if (!eqJoinConjuncts.isEmpty() && tblRef.getJoinOp() == JoinOperator.CROSS_JOIN) {
        tblRef.setJoinOp(JoinOperator.INNER_JOIN);
    }
    List<Expr> otherJoinConjuncts = Lists.newArrayList();
    if (tblRef.getJoinOp().isOuterJoin()) {
        // Also assign conjuncts from On clause. All remaining unassigned conjuncts
        // that can be evaluated by this join are assigned in createSelectPlan().
        otherJoinConjuncts = analyzer.getUnassignedOjConjuncts(tblRef);
    } else if (tblRef.getJoinOp().isSemiJoin()) {
        // Unassigned conjuncts bound by the invisible tuple id of a semi join must have
        // come from the join's On-clause, and therefore, must be added to the other join
        // conjuncts to produce correct results.
        // TODO This doesn't handle predicates specified in the On clause which are not
        // bound by any tuple id (e.g. ON (true))
        List<TupleId> tblRefIds = Lists.newArrayList(outer.getTblRefIds());
        tblRefIds.addAll(inner.getTblRefIds());
        otherJoinConjuncts = analyzer.getUnassignedConjuncts(tblRefIds, false);
        if (tblRef.getJoinOp().isNullAwareLeftAntiJoin()) {
            boolean hasNullMatchingEqOperator = false;
            // Keep only the null-matching eq conjunct in the eqJoinConjuncts and move
            // all the others in otherJoinConjuncts. The BE relies on this
            // separation for correct execution of the null-aware left anti join.
            Iterator<BinaryPredicate> it = eqJoinConjuncts.iterator();
            while (it.hasNext()) {
                BinaryPredicate conjunct = it.next();
                if (!conjunct.isNullMatchingEq()) {
                    otherJoinConjuncts.add(conjunct);
                    it.remove();
                } else {
                    // Only one null-matching eq conjunct is allowed
                    Preconditions.checkState(!hasNullMatchingEqOperator);
                    hasNullMatchingEqOperator = true;
                }
            }
            Preconditions.checkState(hasNullMatchingEqOperator);
        }
    }
    analyzer.markConjunctsAssigned(otherJoinConjuncts);
    // Use a nested-loop join if there are no equi-join conjuncts, or if the inner
    // (build side) is a singular row src. A singular row src has a cardinality of 1, so
    // a nested-loop join is certainly cheaper than a hash join.
    JoinNode result = null;
    Preconditions.checkState(!tblRef.getJoinOp().isNullAwareLeftAntiJoin() || !(inner instanceof SingularRowSrcNode));
    if (eqJoinConjuncts.isEmpty() || inner instanceof SingularRowSrcNode) {
        otherJoinConjuncts.addAll(eqJoinConjuncts);
        result = new NestedLoopJoinNode(outer, inner, tblRef.getDistributionMode(), tblRef.getJoinOp(), otherJoinConjuncts);
    } else {
        result = new HashJoinNode(outer, inner, tblRef.getDistributionMode(), tblRef.getJoinOp(), eqJoinConjuncts, otherJoinConjuncts);
    }
    result.init(analyzer);
    return result;
}
#method_after
private PlanNode createJoinNode(PlanNode outer, PlanNode inner, TableRef innerRef, Analyzer analyzer) throws ImpalaException {
    // get eq join predicates for the TableRefs' ids (not the PlanNodes' ids, which
    // are materialized)
    List<BinaryPredicate> eqJoinConjuncts = getHashLookupJoinConjuncts(outer.getTblRefIds(), inner.getTblRefIds(), analyzer);
    // Outer joins should only use On-clause predicates as eqJoinConjuncts.
    if (!innerRef.getJoinOp().isOuterJoin()) {
        analyzer.createEquivConjuncts(outer.getTblRefIds(), inner.getTblRefIds(), eqJoinConjuncts);
    }
    if (!eqJoinConjuncts.isEmpty() && innerRef.getJoinOp() == JoinOperator.CROSS_JOIN) {
        innerRef.setJoinOp(JoinOperator.INNER_JOIN);
    }
    List<Expr> otherJoinConjuncts = Lists.newArrayList();
    if (innerRef.getJoinOp().isOuterJoin()) {
        // Also assign conjuncts from On clause. All remaining unassigned conjuncts
        // that can be evaluated by this join are assigned in createSelectPlan().
        otherJoinConjuncts = analyzer.getUnassignedOjConjuncts(innerRef);
    } else if (innerRef.getJoinOp().isSemiJoin()) {
        // Unassigned conjuncts bound by the invisible tuple id of a semi join must have
        // come from the join's On-clause, and therefore, must be added to the other join
        // conjuncts to produce correct results.
        // TODO This doesn't handle predicates specified in the On clause which are not
        // bound by any tuple id (e.g. ON (true))
        List<TupleId> tblRefIds = Lists.newArrayList(outer.getTblRefIds());
        tblRefIds.addAll(inner.getTblRefIds());
        otherJoinConjuncts = analyzer.getUnassignedConjuncts(tblRefIds, false);
        if (innerRef.getJoinOp().isNullAwareLeftAntiJoin()) {
            boolean hasNullMatchingEqOperator = false;
            // Keep only the null-matching eq conjunct in the eqJoinConjuncts and move
            // all the others in otherJoinConjuncts. The BE relies on this
            // separation for correct execution of the null-aware left anti join.
            Iterator<BinaryPredicate> it = eqJoinConjuncts.iterator();
            while (it.hasNext()) {
                BinaryPredicate conjunct = it.next();
                if (!conjunct.isNullMatchingEq()) {
                    otherJoinConjuncts.add(conjunct);
                    it.remove();
                } else {
                    // Only one null-matching eq conjunct is allowed
                    Preconditions.checkState(!hasNullMatchingEqOperator);
                    hasNullMatchingEqOperator = true;
                }
            }
            Preconditions.checkState(hasNullMatchingEqOperator);
        }
    }
    analyzer.markConjunctsAssigned(otherJoinConjuncts);
    // Use a nested-loop join if there are no equi-join conjuncts, or if the inner
    // (build side) is a singular row src. A singular row src has a cardinality of 1, so
    // a nested-loop join is certainly cheaper than a hash join.
    JoinNode result = null;
    Preconditions.checkState(!innerRef.getJoinOp().isNullAwareLeftAntiJoin() || !(inner instanceof SingularRowSrcNode));
    if (eqJoinConjuncts.isEmpty() || inner instanceof SingularRowSrcNode) {
        otherJoinConjuncts.addAll(eqJoinConjuncts);
        result = new NestedLoopJoinNode(outer, inner, analyzer.isStraightJoin(), innerRef.getDistributionMode(), innerRef.getJoinOp(), otherJoinConjuncts);
    } else {
        result = new HashJoinNode(outer, inner, analyzer.isStraightJoin(), innerRef.getDistributionMode(), innerRef.getJoinOp(), eqJoinConjuncts, otherJoinConjuncts);
    }
    result.init(analyzer);
    return result;
}
#end_block

#method_before
private PlanNode createTableRefNode(TableRef tblRef, boolean isStraightJoin, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    PlanNode result = null;
    if (tblRef instanceof BaseTableRef) {
        result = createScanNode(tblRef, fastPartitionKeyScans, analyzer);
    } else if (tblRef instanceof CollectionTableRef) {
        if (tblRef.isRelative()) {
            Preconditions.checkState(ctx_.hasSubplan());
            result = new UnnestNode(ctx_.getNextNodeId(), ctx_.getSubplan(), (CollectionTableRef) tblRef);
            result.init(analyzer);
        } else {
            result = createScanNode(tblRef, false, analyzer);
        }
    } else if (tblRef instanceof InlineViewRef) {
        result = createInlineViewPlan(analyzer, (InlineViewRef) tblRef);
    } else if (tblRef instanceof SingularRowSrcTableRef) {
        Preconditions.checkState(ctx_.hasSubplan());
        result = new SingularRowSrcNode(ctx_.getNextNodeId(), ctx_.getSubplan());
        result.init(analyzer);
    } else {
        throw new NotImplementedException("Planning not implemented for table ref class: " + tblRef.getClass());
    }
    return result;
}
#method_after
private PlanNode createTableRefNode(TableRef tblRef, boolean fastPartitionKeyScans, Analyzer analyzer) throws ImpalaException {
    PlanNode result = null;
    if (tblRef instanceof BaseTableRef) {
        result = createScanNode(tblRef, fastPartitionKeyScans, analyzer);
    } else if (tblRef instanceof CollectionTableRef) {
        if (tblRef.isRelative()) {
            Preconditions.checkState(ctx_.hasSubplan());
            result = new UnnestNode(ctx_.getNextNodeId(), ctx_.getSubplan(), (CollectionTableRef) tblRef);
            result.init(analyzer);
        } else {
            result = createScanNode(tblRef, false, analyzer);
        }
    } else if (tblRef instanceof InlineViewRef) {
        result = createInlineViewPlan(analyzer, (InlineViewRef) tblRef);
    } else if (tblRef instanceof SingularRowSrcTableRef) {
        Preconditions.checkState(ctx_.hasSubplan());
        result = new SingularRowSrcNode(ctx_.getNextNodeId(), ctx_.getSubplan());
        result.init(analyzer);
    } else {
        throw new NotImplementedException("Planning not implemented for table ref class: " + tblRef.getClass());
    }
    return result;
}
#end_block

#method_before
@Test
public void testParquetFileFormat() throws DatabaseNotFoundException, InvalidStorageDescriptorException {
    String[] parquetSerDe = new String[] { "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe", "parquet.hive.serde.ParquetHiveSerDe" };
    String[] inputFormats = new String[] { "org.apache.impala.hive.serde.ParquetInputFormat", "parquet.hive.DeprecatedParquetInputFormat", "parquet.hive.MapredParquetInputFormat", "org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat" };
    String[] outputFormats = new String[] { "org.apache.impala.hive.serde.ParquetOutputFormat", "parquet.hive.DeprecatedParquetOutputFormat", "parquet.hive.MapredParquetOutputFormat", "org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat" };
    for (String serDe : parquetSerDe) {
        SerDeInfo serDeInfo = new SerDeInfo();
        serDeInfo.setSerializationLib(serDe);
        serDeInfo.setParameters(new HashMap<String, String>());
        for (String inputFormat : inputFormats) {
            for (String outputFormat : outputFormats) {
                StorageDescriptor sd = new StorageDescriptor();
                sd.setSerdeInfo(serDeInfo);
                sd.setInputFormat(inputFormat);
                sd.setOutputFormat(outputFormat);
                assertNotNull(HdfsStorageDescriptor.fromStorageDescriptor("fakeTblName", sd));
            }
        }
    }
}
#method_after
@Test
public void testParquetFileFormat() throws DatabaseNotFoundException, InvalidStorageDescriptorException {
    String[] parquetSerDe = new String[] { "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe", "parquet.hive.serde.ParquetHiveSerDe" };
    String[] inputFormats = new String[] { "com.cloudera.impala.hive.serde.ParquetInputFormat", "parquet.hive.DeprecatedParquetInputFormat", "parquet.hive.MapredParquetInputFormat", "org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat" };
    String[] outputFormats = new String[] { "org.apache.impala.hive.serde.ParquetOutputFormat", "parquet.hive.DeprecatedParquetOutputFormat", "parquet.hive.MapredParquetOutputFormat", "org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat" };
    for (String serDe : parquetSerDe) {
        SerDeInfo serDeInfo = new SerDeInfo();
        serDeInfo.setSerializationLib(serDe);
        serDeInfo.setParameters(new HashMap<String, String>());
        for (String inputFormat : inputFormats) {
            for (String outputFormat : outputFormats) {
                StorageDescriptor sd = new StorageDescriptor();
                sd.setSerdeInfo(serDeInfo);
                sd.setInputFormat(inputFormat);
                sd.setOutputFormat(outputFormat);
                assertNotNull(HdfsStorageDescriptor.fromStorageDescriptor("fakeTblName", sd));
            }
        }
    }
}
#end_block

#method_before
public static BinaryPredicate normalizeSlotRefComparison(BinaryPredicate predicate, Analyzer analyzer) throws AnalysisException {
    SlotRef ref = predicate.getBoundSlot();
    if (ref == null)
        return null;
    if (ref != predicate.getChild(0).unwrapSlotRef(true)) {
        Preconditions.checkState(ref == predicate.getChild(1).unwrapSlotRef(true));
        predicate = new BinaryPredicate(predicate.getOp().converse(), ref, predicate.getChild(0));
        predicate.analyzeNoThrow(analyzer);
    }
    predicate.foldConstantChildren(analyzer);
    predicate.analyzeNoThrow(analyzer);
    return predicate;
}
#method_after
public static BinaryPredicate normalizeSlotRefComparison(BinaryPredicate predicate, Analyzer analyzer) {
    SlotRef ref = null;
    if (predicate.getChild(0) instanceof SlotRef) {
        ref = (SlotRef) predicate.getChild(0);
    } else if (predicate.getChild(1) instanceof SlotRef) {
        ref = (SlotRef) predicate.getChild(1);
    }
    if (ref == null)
        return null;
    if (ref != predicate.getChild(0)) {
        Preconditions.checkState(ref == predicate.getChild(1));
        predicate = new BinaryPredicate(predicate.getOp().converse(), ref, predicate.getChild(0));
        predicate.analyzeNoThrow(analyzer);
    }
    try {
        predicate.foldConstantChildren(analyzer);
    } catch (AnalysisException ex) {
        // Throws if the expression cannot be evaluated by the BE.
        return null;
    }
    predicate.analyzeNoThrow(analyzer);
    if (!(predicate.getChild(1) instanceof LiteralExpr))
        return null;
    return predicate;
}
#end_block

#method_before
@Override
public void load(boolean reuseMetadata, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    try {
        clearColumns();
        msTable_ = msTbl;
        // Load columns.
        List<FieldSchema> fieldSchemas = client.getFields(db_.getName(), name_);
        for (int i = 0; i < fieldSchemas.size(); ++i) {
            FieldSchema s = fieldSchemas.get(i);
            Type type = parseColumnType(s);
            Column col = new Column(s.getName(), type, s.getComment(), i);
            addColumn(col);
        }
        // These fields are irrelevant for views.
        numClusteringCols_ = 0;
        numRows_ = -1;
        init();
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for view: " + name_, e);
    }
}
#method_after
@Override
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    try {
        clearColumns();
        msTable_ = msTbl;
        // Load columns.
        List<FieldSchema> fieldSchemas = client.getFields(db_.getName(), name_);
        for (int i = 0; i < fieldSchemas.size(); ++i) {
            FieldSchema s = fieldSchemas.get(i);
            Type type = parseColumnType(s);
            Column col = new Column(s.getName(), type, s.getComment(), i);
            addColumn(col);
        }
        // These fields are irrelevant for views.
        numClusteringCols_ = 0;
        numRows_ = -1;
        init();
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for view: " + name_, e);
    }
}
#end_block

#method_before
public static boolean copyToLocal(Path source, Path dest) {
    try {
        FileSystem fs = source.getFileSystem(CONF);
        fs.copyToLocalFile(source, dest);
    } catch (IOException e) {
        return false;
    }
    return true;
}
#method_after
public static void copyToLocal(Path source, Path dest) throws IOException {
    FileSystem fs = source.getFileSystem(CONF);
    fs.copyToLocalFile(source, dest);
}
#end_block

#method_before
@Override
protected void toThrift(TPlanNode msg) {
    Preconditions.checkState(!children_.isEmpty(), "ExchangeNode must have at least one child");
    msg.node_type = TPlanNodeType.EXCHANGE_NODE;
    msg.exchange_node = new TExchangeNode();
    for (TupleId tid : tupleIds_) {
        msg.exchange_node.addToInput_row_tuples(tid.asInt());
    }
    if (mergeInfo_ != null) {
        TSortInfo sortInfo = new TSortInfo(Expr.treesToThrift(mergeInfo_.getOrderingExprs()), mergeInfo_.getIsAscOrder(), mergeInfo_.getNullsFirst());
        msg.exchange_node.setSort_info(sortInfo);
        msg.exchange_node.setOffset(offset_);
    }
}
#method_after
@Override
protected void toThrift(TPlanNode msg) {
    msg.node_type = TPlanNodeType.EXCHANGE_NODE;
    msg.exchange_node = new TExchangeNode();
    for (TupleId tid : tupleIds_) {
        msg.exchange_node.addToInput_row_tuples(tid.asInt());
    }
    if (mergeInfo_ != null) {
        TSortInfo sortInfo = new TSortInfo(Expr.treesToThrift(mergeInfo_.getOrderingExprs()), mergeInfo_.getIsAscOrder(), mergeInfo_.getNullsFirst());
        msg.exchange_node.setSort_info(sortInfo);
        msg.exchange_node.setOffset(offset_);
    }
}
#end_block

#method_before
public static List<Function> extractFunctions(String db, org.apache.hadoop.hive.metastore.api.Function function) throws ImpalaRuntimeException {
    List<Function> result = Lists.newArrayList();
    List<String> addedSignatures = Lists.newArrayList();
    boolean compatible = true;
    StringBuilder warnMessage = new StringBuilder();
    if (function.getFunctionType() != FunctionType.JAVA) {
        compatible = false;
        warnMessage.append("Function type: " + function.getFunctionType().name() + " is not supported. Only " + FunctionType.JAVA.name() + " functions " + "are supported.");
    }
    if (function.getResourceUrisSize() != 1) {
        compatible = false;
        List<String> resourceUris = Lists.newArrayList();
        for (ResourceUri resource : function.getResourceUris()) {
            resourceUris.add(resource.getUri());
        }
        warnMessage.append("Impala does not support multiple Jars for dependencies." + "(" + Joiner.on(",").join(resourceUris) + ") ");
    }
    if (function.getResourceUris().get(0).getResourceType() != ResourceType.JAR) {
        compatible = false;
        warnMessage.append("Function binary type: " + function.getResourceUris().get(0).getResourceType().name() + " is not supported. Only " + ResourceType.JAR.name() + " type is supported.");
    }
    if (!compatible) {
        LOG.warn("Skipping load of incompatible Java function: " + function.getFunctionName() + ". " + warnMessage.toString());
        return result;
    }
    String jarUri = function.getResourceUris().get(0).getUri();
    Class<?> udfClass = null;
    try {
        Path localJarPath = new Path(LOCAL_LIBRARY_PATH, UUID.randomUUID().toString() + ".jar");
        if (!FileSystemUtil.copyToLocal(new Path(jarUri), localJarPath)) {
            String errorMsg = "Error loading Java function: " + db + "." + function.getFunctionName() + ". Couldn't copy " + jarUri + " to local path: " + localJarPath.toString();
            LOG.error(errorMsg);
            throw new ImpalaRuntimeException(errorMsg);
        }
        URL[] classLoaderUrls = new URL[] { new URL(localJarPath.toString()) };
        URLClassLoader urlClassLoader = new URLClassLoader(classLoaderUrls);
        udfClass = urlClassLoader.loadClass(function.getClassName());
        // TODO: Remove this once we support Java UDAF/UDTF
        if (FunctionUtils.getUDFClassType(udfClass) != FunctionUtils.UDFClassType.UDF) {
            LOG.warn("Ignoring load of incompatible Java function: " + function.getFunctionName() + " as " + FunctionUtils.getUDFClassType(udfClass) + " is not a supported type. Only UDFs are supported");
            return result;
        }
        // object.
        for (Method m : udfClass.getMethods()) {
            if (!m.getName().equals("evaluate"))
                continue;
            Function fn = ScalarFunction.fromHiveFunction(db, function.getFunctionName(), function.getClassName(), m.getParameterTypes(), m.getReturnType(), jarUri);
            if (fn == null) {
                LOG.warn("Ignoring incompatible method: " + m.toString() + " during load of " + "Hive UDF:" + function.getFunctionName() + " from " + udfClass);
                continue;
            }
            if (!addedSignatures.contains(fn.signatureString())) {
                result.add(fn);
                addedSignatures.add(fn.signatureString());
            }
        }
    } catch (ClassNotFoundException c) {
        String errorMsg = "Error loading Java function: " + db + "." + function.getFunctionName() + ". Symbol class " + udfClass + "not found in Jar: " + jarUri;
        LOG.error(errorMsg);
        throw new ImpalaRuntimeException(errorMsg, c);
    } catch (Exception e) {
        LOG.error("Skipping function load: " + function.getFunctionName(), e);
        throw new ImpalaRuntimeException("Error extracting functions", e);
    }
    return result;
}
#method_after
public static List<Function> extractFunctions(String db, org.apache.hadoop.hive.metastore.api.Function function) throws ImpalaRuntimeException {
    List<Function> result = Lists.newArrayList();
    List<String> addedSignatures = Lists.newArrayList();
    StringBuilder warnMessage = new StringBuilder();
    if (!isFunctionCompatible(function, warnMessage)) {
        LOG.warn("Skipping load of incompatible function: " + function.getFunctionName() + ". " + warnMessage.toString());
        return result;
    }
    String jarUri = function.getResourceUris().get(0).getUri();
    Class<?> udfClass = null;
    try {
        Path localJarPath = new Path(LOCAL_LIBRARY_PATH, UUID.randomUUID().toString() + ".jar");
        try {
            FileSystemUtil.copyToLocal(new Path(jarUri), localJarPath);
        } catch (IOException e) {
            String errorMsg = "Error loading Java function: " + db + "." + function.getFunctionName() + ". Couldn't copy " + jarUri + " to local path: " + localJarPath.toString();
            LOG.error(errorMsg, e);
            throw new ImpalaRuntimeException(errorMsg);
        }
        URL[] classLoaderUrls = new URL[] { new URL(localJarPath.toString()) };
        URLClassLoader urlClassLoader = new URLClassLoader(classLoaderUrls);
        udfClass = urlClassLoader.loadClass(function.getClassName());
        // TODO: Remove this once we support Java UDAF/UDTF
        if (FunctionUtils.getUDFClassType(udfClass) != FunctionUtils.UDFClassType.UDF) {
            LOG.warn("Ignoring load of incompatible Java function: " + function.getFunctionName() + " as " + FunctionUtils.getUDFClassType(udfClass) + " is not a supported type. Only UDFs are supported");
            return result;
        }
        // object.
        for (Method m : udfClass.getMethods()) {
            if (!m.getName().equals(UdfExecutor.UDF_FUNCTION_NAME))
                continue;
            Function fn = ScalarFunction.fromHiveFunction(db, function.getFunctionName(), function.getClassName(), m.getParameterTypes(), m.getReturnType(), jarUri);
            if (fn == null) {
                LOG.warn("Ignoring incompatible method: " + m.toString() + " during load of " + "Hive UDF:" + function.getFunctionName() + " from " + udfClass);
                continue;
            }
            if (!addedSignatures.contains(fn.signatureString())) {
                result.add(fn);
                addedSignatures.add(fn.signatureString());
            }
        }
    } catch (ClassNotFoundException c) {
        String errorMsg = "Error loading Java function: " + db + "." + function.getFunctionName() + ". Symbol class " + udfClass + "not found in Jar: " + jarUri;
        LOG.error(errorMsg);
        throw new ImpalaRuntimeException(errorMsg, c);
    } catch (Exception e) {
        LOG.error("Skipping function load: " + function.getFunctionName(), e);
        throw new ImpalaRuntimeException("Error extracting functions", e);
    } catch (LinkageError e) {
        String errorMsg = "Error resolving dependencies for Java function: " + db + "." + function.getFunctionName();
        LOG.error(errorMsg);
        throw new ImpalaRuntimeException(errorMsg, e);
    }
    return result;
}
#end_block

#method_before
public void reset() throws CatalogException {
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        nextTableId_.set(0);
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        MetaStoreClient msClient = metaStoreClientPool_.getClient();
        try {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                List<org.apache.hadoop.hive.metastore.api.Function> javaFns = Lists.newArrayList();
                for (String javaFn : msClient.getHiveClient().getFunctions(dbName, "*")) {
                    javaFns.add(msClient.getHiveClient().getFunction(dbName, javaFn));
                }
                org.apache.hadoop.hive.metastore.api.Database msDb = msClient.getHiveClient().getDatabase(dbName);
                Db db = new Db(dbName, this, msDb);
                // Restore UDFs that aren't persisted.
                Db oldDb = oldDbCache.get(db.getName().toLowerCase());
                if (oldDb != null) {
                    for (Function fn : oldDb.getTransientFunctions()) {
                        db.addFunction(fn);
                        fn.setCatalogVersion(incrementAndGetCatalogVersion());
                    }
                }
                loadFunctionsFromDbParams(db, msDb);
                loadJavaFunctions(db, javaFns);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                newDbCache.put(db.getName().toLowerCase(), db);
                for (String tableName : msClient.getHiveClient().getAllTables(dbName)) {
                    Table incompleteTbl = IncompleteTable.createUninitializedTable(getNextTableId(), db, tableName);
                    incompleteTbl.setCatalogVersion(incrementAndGetCatalogVersion());
                    db.addTable(incompleteTbl);
                    if (loadInBackground_) {
                        tblsToBackgroundLoad.add(new TTableName(dbName.toLowerCase(), tableName.toLowerCase()));
                    }
                }
            }
        } finally {
            msClient.release();
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#method_after
public void reset() throws CatalogException {
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        nextTableId_.set(0);
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                dbName = dbName.toLowerCase();
                Db oldDb = oldDbCache.get(dbName);
                Pair<Db, List<TTableName>> invalidatedDb = invalidateDb(msClient, dbName, oldDb);
                if (invalidatedDb == null)
                    continue;
                newDbCache.put(dbName, invalidatedDb.first);
                tblsToBackgroundLoad.addAll(invalidatedDb.second);
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#end_block

#method_before
public Table reloadTable(Table tbl) throws CatalogException {
    LOG.debug(String.format("Refreshing table metadata: %s", tbl.getFullName()));
    TTableName tblName = new TTableName(tbl.getDb().getName().toLowerCase(), tbl.getName().toLowerCase());
    Db db = tbl.getDb();
    if (tbl instanceof IncompleteTable) {
        TableLoadingMgr.LoadRequest loadReq;
        long previousCatalogVersion;
        // Return the table if it is already loaded or submit a new load request.
        catalogLock_.readLock().lock();
        try {
            previousCatalogVersion = tbl.getCatalogVersion();
            loadReq = tableLoadingMgr_.loadAsync(tblName);
        } finally {
            catalogLock_.readLock().unlock();
        }
        Preconditions.checkNotNull(loadReq);
        try {
            // only apply the update if the existing table hasn't changed.
            return replaceTableIfUnchanged(loadReq.get(), previousCatalogVersion);
        } finally {
            loadReq.close();
        }
    }
    catalogLock_.writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        MetaStoreClient msClient = getMetaStoreClient();
        try {
            org.apache.hadoop.hive.metastore.api.Table msTbl = null;
            try {
                msTbl = msClient.getHiveClient().getTable(db.getName(), tblName.getTable_name());
            } catch (Exception e) {
                throw new TableLoadingException("Error loading metadata for table: " + db.getName() + "." + tblName.getTable_name(), e);
            }
            tbl.load(true, msClient.getHiveClient(), msTbl);
        } finally {
            msClient.release();
        }
        tbl.setCatalogVersion(newCatalogVersion);
        return tbl;
    }
}
#method_after
public Table reloadTable(Table tbl) throws CatalogException {
    LOG.debug(String.format("Refreshing table metadata: %s", tbl.getFullName()));
    TTableName tblName = new TTableName(tbl.getDb().getName().toLowerCase(), tbl.getName().toLowerCase());
    Db db = tbl.getDb();
    if (tbl instanceof IncompleteTable) {
        TableLoadingMgr.LoadRequest loadReq;
        long previousCatalogVersion;
        // Return the table if it is already loaded or submit a new load request.
        catalogLock_.readLock().lock();
        try {
            previousCatalogVersion = tbl.getCatalogVersion();
            loadReq = tableLoadingMgr_.loadAsync(tblName);
        } finally {
            catalogLock_.readLock().unlock();
        }
        Preconditions.checkNotNull(loadReq);
        try {
            // only apply the update if the existing table hasn't changed.
            return replaceTableIfUnchanged(loadReq.get(), previousCatalogVersion);
        } finally {
            loadReq.close();
        }
    }
    catalogLock_.writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Table msTbl = null;
            try {
                msTbl = msClient.getHiveClient().getTable(db.getName(), tblName.getTable_name());
            } catch (Exception e) {
                throw new TableLoadingException("Error loading metadata for table: " + db.getName() + "." + tblName.getTable_name(), e);
            }
            tbl.load(true, msClient.getHiveClient(), msTbl);
        }
        tbl.setCatalogVersion(newCatalogVersion);
        return tbl;
    }
}
#end_block

#method_before
public boolean invalidateTable(TTableName tableName, Pair<Db, Table> updatedObjects) {
    Preconditions.checkNotNull(updatedObjects);
    updatedObjects.first = null;
    updatedObjects.second = null;
    LOG.debug(String.format("Invalidating table metadata: %s.%s", tableName.getDb_name(), tableName.getTable_name()));
    String dbName = tableName.getDb_name();
    String tblName = tableName.getTable_name();
    // Stores whether the table exists in the metastore. Can have three states:
    // 1) true - Table exists in metastore.
    // 2) false - Table does not exist in metastore.
    // 3) unknown (null) - There was exception thrown by the metastore client.
    Boolean tableExistsInMetaStore;
    Db db = null;
    MetaStoreClient msClient = getMetaStoreClient();
    try {
        org.apache.hadoop.hive.metastore.api.Database msDb = null;
        try {
            tableExistsInMetaStore = msClient.getHiveClient().tableExists(dbName, tblName);
        } catch (UnknownDBException e) {
            // The parent database does not exist in the metastore. Treat this the same
            // as if the table does not exist.
            tableExistsInMetaStore = false;
        } catch (TException e) {
            LOG.error("Error executing tableExists() metastore call: " + tblName, e);
            tableExistsInMetaStore = null;
        }
        if (tableExistsInMetaStore != null && !tableExistsInMetaStore) {
            updatedObjects.second = removeTable(dbName, tblName);
            return true;
        }
        db = getDb(dbName);
        if ((db == null || !db.containsTable(tblName)) && tableExistsInMetaStore == null) {
            // table exists in the metastore. Do nothing.
            return false;
        } else if (db == null && tableExistsInMetaStore) {
            // must be valid since tableExistsInMetaStore is true.
            try {
                msDb = msClient.getHiveClient().getDatabase(dbName);
                Preconditions.checkNotNull(msDb);
                db = new Db(dbName, this, msDb);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                addDb(db);
                updatedObjects.first = db;
            } catch (TException e) {
                // The metastore database cannot be get. Log the error and return.
                LOG.error("Error executing getDatabase() metastore call: " + dbName, e);
                return false;
            }
        }
    } finally {
        msClient.release();
    }
    // Add a new uninitialized table to the table cache, effectively invalidating
    // any existing entry. The metadata for the table will be loaded lazily, on the
    // on the next access to the table.
    Table newTable = IncompleteTable.createUninitializedTable(getNextTableId(), db, tblName);
    newTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(newTable);
    if (loadInBackground_) {
        tableLoadingMgr_.backgroundLoad(new TTableName(dbName.toLowerCase(), tblName.toLowerCase()));
    }
    updatedObjects.second = newTable;
    return false;
}
#method_after
public boolean invalidateTable(TTableName tableName, Pair<Db, Table> updatedObjects) {
    Preconditions.checkNotNull(updatedObjects);
    updatedObjects.first = null;
    updatedObjects.second = null;
    LOG.debug(String.format("Invalidating table metadata: %s.%s", tableName.getDb_name(), tableName.getTable_name()));
    String dbName = tableName.getDb_name();
    String tblName = tableName.getTable_name();
    // Stores whether the table exists in the metastore. Can have three states:
    // 1) true - Table exists in metastore.
    // 2) false - Table does not exist in metastore.
    // 3) unknown (null) - There was exception thrown by the metastore client.
    Boolean tableExistsInMetaStore;
    Db db = null;
    try (MetaStoreClient msClient = getMetaStoreClient()) {
        org.apache.hadoop.hive.metastore.api.Database msDb = null;
        try {
            tableExistsInMetaStore = msClient.getHiveClient().tableExists(dbName, tblName);
        } catch (UnknownDBException e) {
            // The parent database does not exist in the metastore. Treat this the same
            // as if the table does not exist.
            tableExistsInMetaStore = false;
        } catch (TException e) {
            LOG.error("Error executing tableExists() metastore call: " + tblName, e);
            tableExistsInMetaStore = null;
        }
        if (tableExistsInMetaStore != null && !tableExistsInMetaStore) {
            updatedObjects.second = removeTable(dbName, tblName);
            return true;
        }
        db = getDb(dbName);
        if ((db == null || !db.containsTable(tblName)) && tableExistsInMetaStore == null) {
            // table exists in the metastore. Do nothing.
            return false;
        } else if (db == null && tableExistsInMetaStore) {
            // must be valid since tableExistsInMetaStore is true.
            try {
                msDb = msClient.getHiveClient().getDatabase(dbName);
                Preconditions.checkNotNull(msDb);
                db = new Db(dbName, this, msDb);
                db.setCatalogVersion(incrementAndGetCatalogVersion());
                addDb(db);
                updatedObjects.first = db;
            } catch (TException e) {
                // The metastore database cannot be get. Log the error and return.
                LOG.error("Error executing getDatabase() metastore call: " + dbName, e);
                return false;
            }
        }
    }
    // Add a new uninitialized table to the table cache, effectively invalidating
    // any existing entry. The metadata for the table will be loaded lazily, on the
    // on the next access to the table.
    Table newTable = IncompleteTable.createUninitializedTable(getNextTableId(), db, tblName);
    newTable.setCatalogVersion(incrementAndGetCatalogVersion());
    db.addTable(newTable);
    if (loadInBackground_) {
        tableLoadingMgr_.backgroundLoad(new TTableName(dbName.toLowerCase(), tblName.toLowerCase()));
    }
    updatedObjects.second = newTable;
    return false;
}
#end_block

#method_before
public Table reloadPartition(Table tbl, List<TPartitionKeyValue> partitionSpec) throws CatalogException {
    catalogLock_.writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        HdfsTable hdfsTable = (HdfsTable) tbl;
        HdfsPartition hdfsPartition = hdfsTable.getPartitionFromThriftPartitionSpec(partitionSpec);
        // Retrieve partition name from existing partition or construct it from
        // the partition spec
        String partitionName = hdfsPartition == null ? HdfsTable.constructPartitionName(partitionSpec) : hdfsPartition.getPartitionName();
        LOG.debug(String.format("Refreshing Partition metadata: %s %s", hdfsTable.getFullName(), partitionName));
        MetaStoreClient msClient = getMetaStoreClient();
        try {
            org.apache.hadoop.hive.metastore.api.Partition hmsPartition = null;
            try {
                hmsPartition = msClient.getHiveClient().getPartition(hdfsTable.getDb().getName(), hdfsTable.getName(), partitionName);
            } catch (NoSuchObjectException e) {
                // catalog
                if (hdfsPartition != null) {
                    hdfsTable.dropPartition(partitionSpec);
                    hdfsTable.setCatalogVersion(newCatalogVersion);
                }
                return hdfsTable;
            } catch (Exception e) {
                throw new CatalogException("Error loading metadata for partition: " + hdfsTable.getFullName() + " " + partitionName, e);
            }
            hdfsTable.reloadPartition(hdfsPartition, hmsPartition);
        } finally {
            msClient.release();
        }
        hdfsTable.setCatalogVersion(newCatalogVersion);
        return hdfsTable;
    }
}
#method_after
public Table reloadPartition(Table tbl, List<TPartitionKeyValue> partitionSpec) throws CatalogException {
    catalogLock_.writeLock().lock();
    synchronized (tbl) {
        long newCatalogVersion = incrementAndGetCatalogVersion();
        catalogLock_.writeLock().unlock();
        HdfsTable hdfsTable = (HdfsTable) tbl;
        HdfsPartition hdfsPartition = hdfsTable.getPartitionFromThriftPartitionSpec(partitionSpec);
        // Retrieve partition name from existing partition or construct it from
        // the partition spec
        String partitionName = hdfsPartition == null ? HdfsTable.constructPartitionName(partitionSpec) : hdfsPartition.getPartitionName();
        LOG.debug(String.format("Refreshing Partition metadata: %s %s", hdfsTable.getFullName(), partitionName));
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            org.apache.hadoop.hive.metastore.api.Partition hmsPartition = null;
            try {
                hmsPartition = msClient.getHiveClient().getPartition(hdfsTable.getDb().getName(), hdfsTable.getName(), partitionName);
            } catch (NoSuchObjectException e) {
                // catalog
                if (hdfsPartition != null) {
                    hdfsTable.dropPartition(partitionSpec);
                    hdfsTable.setCatalogVersion(newCatalogVersion);
                }
                return hdfsTable;
            } catch (Exception e) {
                throw new CatalogException("Error loading metadata for partition: " + hdfsTable.getFullName() + " " + partitionName, e);
            }
            hdfsTable.reloadPartition(hdfsPartition, hmsPartition);
        }
        hdfsTable.setCatalogVersion(newCatalogVersion);
        return hdfsTable;
    }
}
#end_block

#method_before
private void loadColumns(List<FieldSchema> fieldSchemas, HiveMetaStoreClient client) throws TableLoadingException {
    int pos = 0;
    for (FieldSchema s : fieldSchemas) {
        Column col = new Column(s.getName(), parseColumnType(s), s.getComment(), pos);
        Preconditions.checkArgument(isSupportedColumnType(col.getType()));
        addColumn(col);
        ++pos;
    }
}
#method_after
private void loadColumns(List<FieldSchema> fieldSchemas, IMetaStoreClient client) throws TableLoadingException {
    int pos = 0;
    for (FieldSchema s : fieldSchemas) {
        Column col = new Column(s.getName(), parseColumnType(s), s.getComment(), pos);
        Preconditions.checkArgument(isSupportedColumnType(col.getType()));
        addColumn(col);
        ++pos;
    }
}
#end_block

#method_before
@Override
public void load(boolean reuseMetadata, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    Preconditions.checkNotNull(msTbl);
    msTable_ = msTbl;
    clearColumns();
    LOG.debug("load table: " + db_.getName() + "." + name_);
    String dataSourceName = getRequiredTableProperty(msTbl, TBL_PROP_DATA_SRC_NAME, null);
    String location = getRequiredTableProperty(msTbl, TBL_PROP_LOCATION, dataSourceName);
    String className = getRequiredTableProperty(msTbl, TBL_PROP_CLASS, dataSourceName);
    String apiVersionString = getRequiredTableProperty(msTbl, TBL_PROP_API_VER, dataSourceName);
    dataSource_ = new TDataSource(dataSourceName, location, className, apiVersionString);
    initString_ = getRequiredTableProperty(msTbl, TBL_PROP_INIT_STRING, dataSourceName);
    if (msTbl.getPartitionKeysSize() > 0) {
        throw new TableLoadingException("Data source table cannot contain clustering " + "columns: " + name_);
    }
    numClusteringCols_ = 0;
    try {
        // Create column objects.
        List<FieldSchema> fieldSchemas = getMetaStoreTable().getSd().getCols();
        loadColumns(fieldSchemas, client);
        // Set table stats.
        numRows_ = getRowCount(super.getMetaStoreTable().getParameters());
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for data source table: " + name_, e);
    }
}
#method_after
@Override
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    Preconditions.checkNotNull(msTbl);
    msTable_ = msTbl;
    clearColumns();
    LOG.debug("load table: " + db_.getName() + "." + name_);
    String dataSourceName = getRequiredTableProperty(msTbl, TBL_PROP_DATA_SRC_NAME, null);
    String location = getRequiredTableProperty(msTbl, TBL_PROP_LOCATION, dataSourceName);
    String className = getRequiredTableProperty(msTbl, TBL_PROP_CLASS, dataSourceName);
    String apiVersionString = getRequiredTableProperty(msTbl, TBL_PROP_API_VER, dataSourceName);
    dataSource_ = new TDataSource(dataSourceName, location, className, apiVersionString);
    initString_ = getRequiredTableProperty(msTbl, TBL_PROP_INIT_STRING, dataSourceName);
    if (msTbl.getPartitionKeysSize() > 0) {
        throw new TableLoadingException("Data source table cannot contain clustering " + "columns: " + name_);
    }
    numClusteringCols_ = 0;
    try {
        // Create column objects.
        List<FieldSchema> fieldSchemas = getMetaStoreTable().getSd().getCols();
        loadColumns(fieldSchemas, client);
        // Set table stats.
        numRows_ = getRowCount(super.getMetaStoreTable().getParameters());
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for data source table: " + name_, e);
    }
}
#end_block

#method_before
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        TLineageGraph thriftLineageGraph = analysisResult.getThriftLineageGraph();
        if (thriftLineageGraph != null && thriftLineageGraph.isSetQuery_text()) {
            result.catalog_op_request.setLineage_graph(thriftLineageGraph);
        }
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt() || analysisResult.isUpdateStmt() || analysisResult.isDeleteStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    if (RuntimeEnv.INSTANCE.isTestEnv() && queryCtx.request.query_options.mt_num_cores != 1) {
        // TODO: this is just to be able to run tests; implement this
        List<PlanFragment> planRoots = planner.createParallelPlans();
        for (PlanFragment planRoot : planRoots) {
            TPlanFragmentTree thriftPlan = planRoot.treeToThrift();
            queryExecRequest.addToMt_plans(thriftPlan);
        }
        queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
        queryExecRequest.setQuery_ctx(queryCtx);
        explainString.append(planner.getExplainString(Lists.newArrayList(planRoots.get(0)), queryExecRequest, TExplainLevel.STANDARD));
        queryExecRequest.setQuery_plan(explainString.toString());
        result.setQuery_exec_request(queryExecRequest);
        return result;
    }
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int idx = 0; idx < fragments.size(); ++idx) {
        PlanFragment fragment = fragments.get(idx);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, idx);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    // Assemble a similar list for corrupt stats
    Set<TTableName> tablesWithCorruptStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
        if (scanNode.hasCorruptTableStats()) {
            tablesWithCorruptStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    for (TTableName tableName : tablesWithCorruptStats) {
        queryCtx.addToTables_with_corrupt_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use EXTENDED by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.EXTENDED;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    TLineageGraph thriftLineageGraph = analysisResult.getThriftLineageGraph();
    if (thriftLineageGraph != null && thriftLineageGraph.isSetQuery_text()) {
        queryExecRequest.setLineage_graph(thriftLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else if (analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt()) {
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    } else {
        Preconditions.checkState(analysisResult.isUpdateStmt() || analysisResult.isDeleteStmt());
        result.stmt_type = TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
    }
    validateTableIds(analysisResult.getAnalyzer(), result);
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#method_after
public TExecRequest createExecRequest(TQueryCtx queryCtx, StringBuilder explainString) throws ImpalaException {
    // Analyze the statement
    AnalysisContext.AnalysisResult analysisResult = analyzeStmt(queryCtx);
    EventSequence timeline = analysisResult.getAnalyzer().getTimeline();
    timeline.markEvent("Analysis finished");
    Preconditions.checkNotNull(analysisResult.getStmt());
    TExecRequest result = new TExecRequest();
    result.setQuery_options(queryCtx.request.getQuery_options());
    result.setAccess_events(analysisResult.getAccessEvents());
    result.analysis_warnings = analysisResult.getAnalyzer().getWarnings();
    if (analysisResult.isCatalogOp()) {
        result.stmt_type = TStmtType.DDL;
        createCatalogOpRequest(analysisResult, result);
        TLineageGraph thriftLineageGraph = analysisResult.getThriftLineageGraph();
        if (thriftLineageGraph != null && thriftLineageGraph.isSetQuery_text()) {
            result.catalog_op_request.setLineage_graph(thriftLineageGraph);
        }
        // All DDL operations except for CTAS are done with analysis at this point.
        if (!analysisResult.isCreateTableAsSelectStmt())
            return result;
    } else if (analysisResult.isLoadDataStmt()) {
        result.stmt_type = TStmtType.LOAD;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("summary", Type.STRING.toThrift()))));
        result.setLoad_data_request(analysisResult.getLoadDataStmt().toThrift());
        return result;
    } else if (analysisResult.isSetStmt()) {
        result.stmt_type = TStmtType.SET;
        result.setResult_set_metadata(new TResultSetMetadata(Arrays.asList(new TColumn("option", Type.STRING.toThrift()), new TColumn("value", Type.STRING.toThrift()))));
        result.setSet_query_option_request(analysisResult.getSetStmt().toThrift());
        return result;
    }
    // create TQueryExecRequest
    Preconditions.checkState(analysisResult.isQueryStmt() || analysisResult.isDmlStmt() || analysisResult.isCreateTableAsSelectStmt() || analysisResult.isUpdateStmt() || analysisResult.isDeleteStmt());
    TQueryExecRequest queryExecRequest = new TQueryExecRequest();
    // create plan
    LOG.debug("create plan");
    Planner planner = new Planner(analysisResult, queryCtx);
    if (RuntimeEnv.INSTANCE.isTestEnv() && queryCtx.request.query_options.mt_num_cores > 0) {
        // TODO: this is just to be able to run tests; implement this
        List<PlanFragment> planRoots = planner.createParallelPlans();
        for (PlanFragment planRoot : planRoots) {
            TPlanFragmentTree thriftPlan = planRoot.treeToThrift();
            queryExecRequest.addToMt_plans(thriftPlan);
        }
        queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
        queryExecRequest.setQuery_ctx(queryCtx);
        explainString.append(planner.getExplainString(Lists.newArrayList(planRoots.get(0)), queryExecRequest, TExplainLevel.STANDARD));
        queryExecRequest.setQuery_plan(explainString.toString());
        result.setQuery_exec_request(queryExecRequest);
        return result;
    }
    ArrayList<PlanFragment> fragments = planner.createPlan();
    List<ScanNode> scanNodes = Lists.newArrayList();
    // map from fragment to its index in queryExecRequest.fragments; needed for
    // queryExecRequest.dest_fragment_idx
    Map<PlanFragment, Integer> fragmentIdx = Maps.newHashMap();
    for (int idx = 0; idx < fragments.size(); ++idx) {
        PlanFragment fragment = fragments.get(idx);
        Preconditions.checkNotNull(fragment.getPlanRoot());
        fragment.getPlanRoot().collect(Predicates.instanceOf(ScanNode.class), scanNodes);
        fragmentIdx.put(fragment, idx);
    }
    // set fragment destinations
    for (int i = 1; i < fragments.size(); ++i) {
        PlanFragment dest = fragments.get(i).getDestFragment();
        Integer idx = fragmentIdx.get(dest);
        Preconditions.checkState(idx != null);
        queryExecRequest.addToDest_fragment_idx(idx.intValue());
    }
    // Set scan ranges/locations for scan nodes.
    // Also assemble list of tables names missing stats for assembling a warning message.
    LOG.debug("get scan range locations");
    Set<TTableName> tablesMissingStats = Sets.newTreeSet();
    // Assemble a similar list for corrupt stats
    Set<TTableName> tablesWithCorruptStats = Sets.newTreeSet();
    for (ScanNode scanNode : scanNodes) {
        queryExecRequest.putToPer_node_scan_ranges(scanNode.getId().asInt(), scanNode.getScanRangeLocations());
        if (scanNode.isTableMissingStats()) {
            tablesMissingStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
        if (scanNode.hasCorruptTableStats()) {
            tablesWithCorruptStats.add(scanNode.getTupleDesc().getTableName().toThrift());
        }
    }
    queryExecRequest.setHost_list(analysisResult.getAnalyzer().getHostIndex().getList());
    for (TTableName tableName : tablesMissingStats) {
        queryCtx.addToTables_missing_stats(tableName);
    }
    for (TTableName tableName : tablesWithCorruptStats) {
        queryCtx.addToTables_with_corrupt_stats(tableName);
    }
    // or if all tables have stats.
    if (queryCtx.request.query_options.isDisable_unsafe_spills() && !tablesMissingStats.isEmpty() && !analysisResult.getAnalyzer().hasPlanHints()) {
        queryCtx.setDisable_spilling(true);
    }
    // estimates of scan nodes rely on them.
    try {
        planner.computeResourceReqs(fragments, true, queryExecRequest);
    } catch (Exception e) {
        // Turn exceptions into a warning to allow the query to execute.
        LOG.error("Failed to compute resource requirements for query\n" + queryCtx.request.getStmt(), e);
    }
    // The fragment at this point has all state set, serialize it to thrift.
    for (PlanFragment fragment : fragments) {
        TPlanFragment thriftFragment = fragment.toThrift();
        queryExecRequest.addToFragments(thriftFragment);
    }
    // Use EXTENDED by default for all non-explain statements.
    TExplainLevel explainLevel = TExplainLevel.EXTENDED;
    // Use the query option for explain stmts and tests (e.g., planner tests).
    if (analysisResult.isExplainStmt() || RuntimeEnv.INSTANCE.isTestEnv()) {
        explainLevel = queryCtx.request.query_options.getExplain_level();
    }
    // Global query parameters to be set in each TPlanExecRequest.
    queryExecRequest.setQuery_ctx(queryCtx);
    explainString.append(planner.getExplainString(fragments, queryExecRequest, explainLevel));
    queryExecRequest.setQuery_plan(explainString.toString());
    queryExecRequest.setDesc_tbl(analysisResult.getAnalyzer().getDescTbl().toThrift());
    TLineageGraph thriftLineageGraph = analysisResult.getThriftLineageGraph();
    if (thriftLineageGraph != null && thriftLineageGraph.isSetQuery_text()) {
        queryExecRequest.setLineage_graph(thriftLineageGraph);
    }
    if (analysisResult.isExplainStmt()) {
        // Return the EXPLAIN request
        createExplainRequest(explainString.toString(), result);
        return result;
    }
    result.setQuery_exec_request(queryExecRequest);
    if (analysisResult.isQueryStmt()) {
        // fill in the metadata
        LOG.debug("create result set metadata");
        result.stmt_type = TStmtType.QUERY;
        result.query_exec_request.stmt_type = result.stmt_type;
        TResultSetMetadata metadata = new TResultSetMetadata();
        QueryStmt queryStmt = analysisResult.getQueryStmt();
        int colCnt = queryStmt.getColLabels().size();
        for (int i = 0; i < colCnt; ++i) {
            TColumn colDesc = new TColumn();
            colDesc.columnName = queryStmt.getColLabels().get(i);
            colDesc.columnType = queryStmt.getResultExprs().get(i).getType().toThrift();
            metadata.addToColumns(colDesc);
        }
        result.setResult_set_metadata(metadata);
    } else if (analysisResult.isInsertStmt() || analysisResult.isCreateTableAsSelectStmt()) {
        // For CTAS the overall TExecRequest statement type is DDL, but the
        // query_exec_request should be DML
        result.stmt_type = analysisResult.isCreateTableAsSelectStmt() ? TStmtType.DDL : TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
        // create finalization params of insert stmt
        InsertStmt insertStmt = analysisResult.getInsertStmt();
        if (insertStmt.getTargetTable() instanceof HdfsTable) {
            TFinalizeParams finalizeParams = new TFinalizeParams();
            finalizeParams.setIs_overwrite(insertStmt.isOverwrite());
            finalizeParams.setTable_name(insertStmt.getTargetTableName().getTbl());
            finalizeParams.setTable_id(insertStmt.getTargetTable().getId().asInt());
            String db = insertStmt.getTargetTableName().getDb();
            finalizeParams.setTable_db(db == null ? queryCtx.session.database : db);
            HdfsTable hdfsTable = (HdfsTable) insertStmt.getTargetTable();
            finalizeParams.setHdfs_base_dir(hdfsTable.getHdfsBaseDir());
            finalizeParams.setStaging_dir(hdfsTable.getHdfsBaseDir() + "/_impala_insert_staging");
            queryExecRequest.setFinalize_params(finalizeParams);
        }
    } else {
        Preconditions.checkState(analysisResult.isUpdateStmt() || analysisResult.isDeleteStmt());
        result.stmt_type = TStmtType.DML;
        result.query_exec_request.stmt_type = TStmtType.DML;
    }
    validateTableIds(analysisResult.getAnalyzer(), result);
    timeline.markEvent("Planning finished");
    result.setTimeline(analysisResult.getAnalyzer().getTimeline().toThrift());
    return result;
}
#end_block

#method_before
public static TDescribeResult buildDescribeFormattedResult(Table table) {
    TDescribeResult descResult = new TDescribeResult();
    descResult.results = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTable = table.getMetaStoreTable().deepCopy();
    // (value is null).
    for (FieldSchema fs : msTable.getSd().getCols()) fs.setComment(table.getColumn(fs.getName()).getComment());
    for (FieldSchema fs : msTable.getPartitionKeys()) {
        fs.setComment(table.getColumn(fs.getName()).getComment());
    }
    // To avoid initializing any of the SerDe classes in the metastore table Thrift
    // struct, create the ql.metadata.Table object by calling the empty c'tor and
    // then calling setTTable().
    org.apache.hadoop.hive.ql.metadata.Table hiveTable = new org.apache.hadoop.hive.ql.metadata.Table();
    hiveTable.setTTable(msTable);
    StringBuilder sb = new StringBuilder();
    // First add all the columns (includes partition columns).
    sb.append(MetaDataFormatUtils.getAllColumnsInformation(msTable.getSd().getCols(), msTable.getPartitionKeys(), true, false, true));
    // Add the extended table metadata information.
    sb.append(MetaDataFormatUtils.getTableInformation(hiveTable));
    for (String line : sb.toString().split("\n")) {
        // To match Hive's HiveServer2 output, split each line into multiple column
        // values based on the field delimiter.
        String[] columns = line.split(MetaDataFormatUtils.FIELD_DELIM);
        TResultRow resultRow = new TResultRow();
        for (int i = 0; i < NUM_DESC_FORMATTED_RESULT_COLS; ++i) {
            TColumnValue colVal = new TColumnValue();
            colVal.setString_val(null);
            if (columns.length > i) {
                // Add the column value.
                colVal.setString_val(columns[i]);
            }
            resultRow.addToColVals(colVal);
        }
        descResult.results.add(resultRow);
    }
    return descResult;
}
#method_after
public static TDescribeResult buildDescribeFormattedResult(Table table) {
    TDescribeResult descResult = new TDescribeResult();
    descResult.results = Lists.newArrayList();
    org.apache.hadoop.hive.metastore.api.Table msTable = table.getMetaStoreTable().deepCopy();
    // For some table formats (e.g. Avro) the column list in the table can differ from the
    // one returned by the Hive metastore. To handle this we use the column list from the
    // table which has already reconciled those differences.
    msTable.getSd().setCols(Column.toFieldSchemas(table.getNonClusteringColumns()));
    msTable.setPartitionKeys(Column.toFieldSchemas(table.getClusteringColumns()));
    // To avoid initializing any of the SerDe classes in the metastore table Thrift
    // struct, create the ql.metadata.Table object by calling the empty c'tor and
    // then calling setTTable().
    org.apache.hadoop.hive.ql.metadata.Table hiveTable = new org.apache.hadoop.hive.ql.metadata.Table();
    hiveTable.setTTable(msTable);
    StringBuilder sb = new StringBuilder();
    // First add all the columns (includes partition columns).
    sb.append(MetaDataFormatUtils.getAllColumnsInformation(msTable.getSd().getCols(), msTable.getPartitionKeys(), true, false, true));
    // Add the extended table metadata information.
    sb.append(MetaDataFormatUtils.getTableInformation(hiveTable));
    for (String line : sb.toString().split("\n")) {
        // To match Hive's HiveServer2 output, split each line into multiple column
        // values based on the field delimiter.
        String[] columns = line.split(MetaDataFormatUtils.FIELD_DELIM);
        TResultRow resultRow = new TResultRow();
        for (int i = 0; i < NUM_DESC_FORMATTED_RESULT_COLS; ++i) {
            TColumnValue colVal = new TColumnValue();
            colVal.setString_val(null);
            if (columns.length > i) {
                // Add the column value.
                colVal.setString_val(columns[i]);
            }
            resultRow.addToColVals(colVal);
        }
        descResult.results.add(resultRow);
    }
    return descResult;
}
#end_block

#method_before
public void loadConfig() {
    if (Strings.isNullOrEmpty(configFile_)) {
        throw new IllegalArgumentException("A valid path to a sentry-site.xml config " + "file must be set using --sentry_config to enable authorization.");
    }
    File configFile = new File(configFile_);
    if (!configFile.exists()) {
        throw new RuntimeException("Sentry configuration file does not exist: " + configFile_);
    }
    if (!configFile.canRead()) {
        throw new RuntimeException("Cannot read Sentry configuration file: " + configFile_);
    }
    // Load the config.
    try {
        config_.addResource(configFile.toURI().toURL());
    } catch (MalformedURLException e) {
        throw new RuntimeException("Invalid Sentry config file path: " + configFile_, e);
    }
}
#method_after
public void loadConfig() {
    if (Strings.isNullOrEmpty(configFile_)) {
        throw new IllegalArgumentException("A valid path to a sentry-site.xml config " + "file must be set using --sentry_config to enable authorization.");
    }
    File configFile = new File(configFile_);
    if (!configFile.exists()) {
        String configFilePath = "\"" + configFile_ + "\"";
        throw new RuntimeException("Sentry configuration file does not exist: " + configFilePath);
    }
    if (!configFile.canRead()) {
        throw new RuntimeException("Cannot read Sentry configuration file: " + configFile_);
    }
    // Load the config.
    try {
        config_.addResource(configFile.toURI().toURL());
    } catch (MalformedURLException e) {
        throw new RuntimeException("Invalid Sentry config file path: " + configFile_, e);
    }
}
#end_block

#method_before
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    isRepartition_ = null;
    resultExprs_.clear();
}
#method_after
@Override
public void reset() {
    super.reset();
    if (withClause_ != null)
        withClause_.reset();
    targetTableName_ = originalTableName_;
    queryStmt_.reset();
    table_ = null;
    partitionKeyExprs_.clear();
    hasShuffleHint_ = false;
    hasNoShuffleHint_ = false;
    resultExprs_.clear();
}
#end_block

#method_before
private void analyzePlanHints(Analyzer analyzer) throws AnalysisException {
    if (planHints_ == null)
        return;
    if (!planHints_.isEmpty() && (partitionKeyValues_ == null || table_ instanceof HBaseTable)) {
        throw new AnalysisException("INSERT hints are only supported for inserting into " + "partitioned Hdfs tables.");
    }
    for (String hint : planHints_) {
        if (hint.equalsIgnoreCase("SHUFFLE")) {
            if (isRepartition_ != null && !isRepartition_) {
                throw new AnalysisException("Conflicting INSERT hint: " + hint);
            }
            isRepartition_ = Boolean.TRUE;
            analyzer.setHasPlanHints();
        } else if (hint.equalsIgnoreCase("NOSHUFFLE")) {
            if (isRepartition_ != null && isRepartition_) {
                throw new AnalysisException("Conflicting INSERT hint: " + hint);
            }
            isRepartition_ = Boolean.FALSE;
            analyzer.setHasPlanHints();
        } else {
            analyzer.addWarning("INSERT hint not recognized: " + hint);
        }
    }
}
#method_after
private void analyzePlanHints(Analyzer analyzer) throws AnalysisException {
    if (planHints_ == null)
        return;
    if (!planHints_.isEmpty() && table_ instanceof HBaseTable) {
        throw new AnalysisException("INSERT hints are only supported for inserting into " + "Hdfs tables.");
    }
    for (String hint : planHints_) {
        if (hint.equalsIgnoreCase("SHUFFLE")) {
            if (hasNoShuffleHint_) {
                throw new AnalysisException("Conflicting INSERT hint: " + hint);
            }
            hasShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else if (hint.equalsIgnoreCase("NOSHUFFLE")) {
            if (hasShuffleHint_) {
                throw new AnalysisException("Conflicting INSERT hint: " + hint);
            }
            hasNoShuffleHint_ = true;
            analyzer.setHasPlanHints();
        } else {
            analyzer.addWarning("INSERT hint not recognized: " + hint);
        }
    }
    // Both flags may be false or one of them may be true, but not both.
    Preconditions.checkState((!hasShuffleHint_ && !hasNoShuffleHint_) || (hasShuffleHint_ ^ hasNoShuffleHint_));
}
#end_block

#method_before
public Text evaluate() {
    return new Text("New UDF");
}
#method_after
public Text evaluate() {
    return new Text("Old UDF");
}
#end_block

#method_before
public Path getTablePath(org.apache.hadoop.hive.metastore.api.Table msTbl) throws NoSuchObjectException, MetaException, TException {
    MetaStoreClient client = getMetaStoreClient();
    try {
        // location property of the parent database.
        if (msTbl.getSd().getLocation() == null || msTbl.getSd().getLocation().isEmpty()) {
            String dbLocation = client.getHiveClient().getDatabase(msTbl.getDbName()).getLocationUri();
            return new Path(dbLocation, msTbl.getTableName().toLowerCase());
        } else {
            return new Path(msTbl.getSd().getLocation());
        }
    } finally {
        client.release();
    }
}
#method_after
public Path getTablePath(org.apache.hadoop.hive.metastore.api.Table msTbl) throws NoSuchObjectException, MetaException, TException {
    try (MetaStoreClient msClient = getMetaStoreClient()) {
        // location property of the parent database.
        if (msTbl.getSd().getLocation() == null || msTbl.getSd().getLocation().isEmpty()) {
            String dbLocation = msClient.getHiveClient().getDatabase(msTbl.getDbName()).getLocationUri();
            return new Path(dbLocation, msTbl.getTableName().toLowerCase());
        } else {
            return new Path(msTbl.getSd().getLocation());
        }
    }
}
#end_block

#method_before
public void setSubplan(PlanNode subplan) {
    Preconditions.checkState(children_.size() == 1);
    children_.add(subplan);
    tblRefIds_.addAll(subplan.getTblRefIds());
    tupleIds_.addAll(subplan.getTupleIds());
    nullableTupleIds_.addAll(subplan.getNullableTupleIds());
}
#method_after
public void setSubplan(PlanNode subplan) {
    Preconditions.checkState(children_.size() == 1);
    subplan_ = subplan;
    children_.add(subplan);
    computeTupleIds();
}
#end_block

#method_before
public HiveMetaStoreClient getHiveClient() {
    return hiveClient_;
}
#method_after
public IMetaStoreClient getHiveClient() {
    return hiveClient_;
}
#end_block

#method_before
private void markInUse() {
    isInUse_ = true;
}
#method_after
private void markInUse() {
    Preconditions.checkState(!isInUse_);
    isInUse_ = true;
}
#end_block

#method_before
public MetaStoreClient getClient() {
    // classloader, otherwise VersionInfo will fail in it's c'tor.
    if (Thread.currentThread().getContextClassLoader() == null) {
        Thread.currentThread().setContextClassLoader(ClassLoader.getSystemClassLoader());
    }
    MetaStoreClient client = clientPool_.poll();
    // local Kerberos state (see IMPALA-825).
    synchronized (this) {
        try {
            Thread.sleep(clientCreationDelayMs_);
        } catch (InterruptedException e) {
        /* ignore */
        }
        if (client == null) {
            client = new MetaStoreClient(hiveConf_);
        } else {
            // TODO: Due to Hive Metastore bugs, there is leftover state from previous client
            // connections so we are unable to reuse the same connection. For now simply
            // reconnect each time. One possible culprit is HIVE-5181.
            client = new MetaStoreClient(hiveConf_);
        }
    }
    client.markInUse();
    return client;
}
#method_after
public MetaStoreClient getClient() {
    // classloader, otherwise VersionInfo will fail in it's c'tor.
    if (Thread.currentThread().getContextClassLoader() == null) {
        Thread.currentThread().setContextClassLoader(ClassLoader.getSystemClassLoader());
    }
    MetaStoreClient client = clientPool_.poll();
    // local Kerberos state (see IMPALA-825).
    if (client == null) {
        synchronized (this) {
            try {
                Thread.sleep(clientCreationDelayMs_);
            } catch (InterruptedException e) {
            /* ignore */
            }
            client = new MetaStoreClient(hiveConf_);
        }
    }
    client.markInUse();
    return client;
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    super.init(analyzer);
    Preconditions.checkState(eqJoinConjuncts_.isEmpty());
    // Set the proper join operator based on whether predicates are assigned or not.
    if (conjuncts_.isEmpty() && otherJoinConjuncts_.isEmpty() && !joinOp_.isSemiJoin() && !joinOp_.isOuterJoin()) {
        joinOp_ = JoinOperator.CROSS_JOIN;
    } else if (joinOp_.isCrossJoin()) {
        // A cross join with predicates is an inner join.
        joinOp_ = JoinOperator.INNER_JOIN;
    }
    orderJoinConjunctsByCost();
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaException {
    super.init(analyzer);
    Preconditions.checkState(eqJoinConjuncts_.isEmpty());
    // Set the proper join operator based on whether predicates are assigned or not.
    if (conjuncts_.isEmpty() && otherJoinConjuncts_.isEmpty() && !joinOp_.isSemiJoin() && !joinOp_.isOuterJoin()) {
        joinOp_ = JoinOperator.CROSS_JOIN;
    } else if (joinOp_.isCrossJoin()) {
        // A cross join with predicates is an inner join.
        joinOp_ = JoinOperator.INNER_JOIN;
    }
    orderJoinConjunctsByCost();
    computeStats(analyzer);
}
#end_block

#method_before
public PlanFragment createInsertFragment(PlanFragment inputFragment, InsertStmt insertStmt, Analyzer analyzer, ArrayList<PlanFragment> fragments) throws ImpalaException {
    List<Expr> partitionExprs = insertStmt.getPartitionKeyExprs();
    Boolean partitionHint = insertStmt.isRepartition();
    if (partitionExprs.isEmpty())
        return inputFragment;
    if (partitionHint != null && !partitionHint)
        return inputFragment;
    // we ignore constants for the sake of partitioning
    List<Expr> nonConstPartitionExprs = Lists.newArrayList(partitionExprs);
    Expr.removeConstants(nonConstPartitionExprs);
    DataPartition inputPartition = inputFragment.getDataPartition();
    // do nothing if the input fragment is already appropriately partitioned
    if (analyzer.equivSets(inputPartition.getPartitionExprs(), nonConstPartitionExprs)) {
        return inputFragment;
    }
    // if it is distributed across all nodes; if so, don't repartition
    if (Expr.isSubset(inputPartition.getPartitionExprs(), nonConstPartitionExprs)) {
        long numPartitions = getNumDistinctValues(inputPartition.getPartitionExprs());
        if (numPartitions >= inputFragment.getNumNodes())
            return inputFragment;
    }
    // don't repartition if the resulting number of partitions is too low to get good
    // parallelism
    long numPartitions = getNumDistinctValues(nonConstPartitionExprs);
    // don't repartition if we know we have fewer partitions than nodes
    // (ie, default to repartitioning if col stats are missing)
    // TODO: we want to repartition if the resulting files would otherwise
    // be very small (less than some reasonable multiple of the recommended block size);
    // in order to do that, we need to come up with an estimate of the avg row size
    // in the particular file format of the output table/partition.
    // We should always know on how many nodes our input is running.
    Preconditions.checkState(inputFragment.getNumNodes() != -1);
    if (partitionHint == null && numPartitions > 0 && numPartitions <= inputFragment.getNumNodes()) {
        return inputFragment;
    }
    Preconditions.checkState(partitionHint == null || partitionHint);
    ExchangeNode exchNode = new ExchangeNode(ctx_.getNextNodeId());
    exchNode.addChild(inputFragment.getPlanRoot());
    exchNode.init(analyzer);
    Preconditions.checkState(exchNode.hasValidStats());
    DataPartition partition = DataPartition.hashPartitioned(nonConstPartitionExprs);
    PlanFragment fragment = new PlanFragment(ctx_.getNextFragmentId(), exchNode, partition);
    inputFragment.setDestination(exchNode);
    inputFragment.setOutputPartition(partition);
    fragments.add(fragment);
    return fragment;
}
#method_after
public PlanFragment createInsertFragment(PlanFragment inputFragment, InsertStmt insertStmt, Analyzer analyzer, ArrayList<PlanFragment> fragments) throws ImpalaException {
    if (insertStmt.hasNoShuffleHint())
        return inputFragment;
    List<Expr> partitionExprs = Lists.newArrayList(insertStmt.getPartitionKeyExprs());
    // Ignore constants for the sake of partitioning.
    Expr.removeConstants(partitionExprs);
    // Do nothing if the input fragment is already appropriately partitioned.
    DataPartition inputPartition = inputFragment.getDataPartition();
    if (!partitionExprs.isEmpty() && analyzer.equivSets(inputPartition.getPartitionExprs(), partitionExprs)) {
        return inputFragment;
    }
    // Make a cost-based decision only if no user hint was supplied.
    if (!insertStmt.hasShuffleHint()) {
        // if it is distributed across all nodes. If so, don't repartition.
        if (Expr.isSubset(inputPartition.getPartitionExprs(), partitionExprs)) {
            long numPartitions = getNumDistinctValues(inputPartition.getPartitionExprs());
            if (numPartitions >= inputFragment.getNumNodes())
                return inputFragment;
        }
        // Don't repartition if we know we have fewer partitions than nodes
        // (ie, default to repartitioning if col stats are missing).
        // TODO: We want to repartition if the resulting files would otherwise
        // be very small (less than some reasonable multiple of the recommended block size).
        // In order to do that, we need to come up with an estimate of the avg row size
        // in the particular file format of the output table/partition.
        // We should always know on how many nodes our input is running.
        long numPartitions = getNumDistinctValues(partitionExprs);
        Preconditions.checkState(inputFragment.getNumNodes() != -1);
        if (numPartitions > 0 && numPartitions <= inputFragment.getNumNodes()) {
            return inputFragment;
        }
    }
    ExchangeNode exchNode = new ExchangeNode(ctx_.getNextNodeId(), inputFragment.getPlanRoot());
    exchNode.init(analyzer);
    Preconditions.checkState(exchNode.hasValidStats());
    DataPartition partition;
    if (partitionExprs.isEmpty()) {
        partition = DataPartition.UNPARTITIONED;
    } else {
        partition = DataPartition.hashPartitioned(partitionExprs);
    }
    PlanFragment fragment = new PlanFragment(ctx_.getNextFragmentId(), exchNode, partition);
    inputFragment.setDestination(exchNode);
    inputFragment.setOutputPartition(partition);
    fragments.add(fragment);
    return fragment;
}
#end_block

#method_before
private PlanFragment createMergeFragment(PlanFragment inputFragment) throws ImpalaException {
    Preconditions.checkState(inputFragment.isPartitioned());
    ExchangeNode mergePlan = new ExchangeNode(ctx_.getNextNodeId());
    mergePlan.addChild(inputFragment.getPlanRoot());
    mergePlan.init(ctx_.getRootAnalyzer());
    Preconditions.checkState(mergePlan.hasValidStats());
    PlanFragment fragment = new PlanFragment(ctx_.getNextFragmentId(), mergePlan, DataPartition.UNPARTITIONED);
    inputFragment.setDestination(mergePlan);
    return fragment;
}
#method_after
private PlanFragment createMergeFragment(PlanFragment inputFragment) throws ImpalaException {
    Preconditions.checkState(inputFragment.isPartitioned());
    ExchangeNode mergePlan = new ExchangeNode(ctx_.getNextNodeId(), inputFragment.getPlanRoot());
    mergePlan.init(ctx_.getRootAnalyzer());
    Preconditions.checkState(mergePlan.hasValidStats());
    PlanFragment fragment = new PlanFragment(ctx_.getNextFragmentId(), mergePlan, DataPartition.UNPARTITIONED);
    inputFragment.setDestination(mergePlan);
    return fragment;
}
#end_block

#method_before
private PlanFragment createPartitionedHashJoinFragment(HashJoinNode node, Analyzer analyzer, boolean lhsHasCompatPartition, boolean rhsHasCompatPartition, PlanFragment leftChildFragment, PlanFragment rightChildFragment, List<Expr> lhsJoinExprs, List<Expr> rhsJoinExprs, ArrayList<PlanFragment> fragments) throws ImpalaException {
    node.setDistributionMode(HashJoinNode.DistributionMode.PARTITIONED);
    // on partition exprs (both using the canonical equivalence class representatives).
    if (lhsHasCompatPartition && rhsHasCompatPartition && isCompatPartition(leftChildFragment.getDataPartition(), rightChildFragment.getDataPartition(), lhsJoinExprs, rhsJoinExprs, analyzer)) {
        node.setChild(0, leftChildFragment.getPlanRoot());
        node.setChild(1, rightChildFragment.getPlanRoot());
        // fix up PlanNode.fragment_ for the migrated PlanNode tree of the rhs child
        leftChildFragment.setFragmentInPlanTree(node.getChild(1));
        // Relocate input fragments of rightChildFragment to leftChildFragment.
        for (PlanFragment rhsInput : rightChildFragment.getChildren()) {
            leftChildFragment.getChildren().add(rhsInput);
        }
        // Remove right fragment because its plan tree has been merged into leftFragment.
        fragments.remove(rightChildFragment);
        leftChildFragment.setPlanRoot(node);
        return leftChildFragment;
    }
    // The lhs input fragment is already partitioned on the join exprs.
    // Make the HashJoin the new root of leftChildFragment and set the join's
    // first child to the lhs plan root. The second child of the join is an
    // ExchangeNode that is fed by the rhsInputFragment whose sink repartitions
    // its data by the rhs join exprs.
    DataPartition rhsJoinPartition = null;
    if (lhsHasCompatPartition) {
        rhsJoinPartition = getCompatPartition(lhsJoinExprs, leftChildFragment.getDataPartition(), rhsJoinExprs, analyzer);
        if (rhsJoinPartition != null) {
            node.setChild(0, leftChildFragment.getPlanRoot());
            connectChildFragment(node, 1, leftChildFragment, rightChildFragment);
            rightChildFragment.setOutputPartition(rhsJoinPartition);
            leftChildFragment.setPlanRoot(node);
            return leftChildFragment;
        }
    }
    // Same as above but with rhs and lhs reversed.
    DataPartition lhsJoinPartition = null;
    if (rhsHasCompatPartition) {
        lhsJoinPartition = getCompatPartition(rhsJoinExprs, rightChildFragment.getDataPartition(), lhsJoinExprs, analyzer);
        if (lhsJoinPartition != null) {
            node.setChild(1, rightChildFragment.getPlanRoot());
            connectChildFragment(node, 0, rightChildFragment, leftChildFragment);
            leftChildFragment.setOutputPartition(lhsJoinPartition);
            rightChildFragment.setPlanRoot(node);
            return rightChildFragment;
        }
    }
    Preconditions.checkState(lhsJoinPartition == null);
    Preconditions.checkState(rhsJoinPartition == null);
    lhsJoinPartition = DataPartition.hashPartitioned(Expr.cloneList(lhsJoinExprs));
    rhsJoinPartition = DataPartition.hashPartitioned(Expr.cloneList(rhsJoinExprs));
    // Neither lhs nor rhs are already partitioned on the join exprs.
    // Create a new parent fragment containing a HashJoin node with two
    // ExchangeNodes as inputs; the latter are the destinations of the
    // left- and rightChildFragments, which now partition their output
    // on their respective join exprs.
    // The new fragment is hash-partitioned on the lhs input join exprs.
    ExchangeNode lhsExchange = new ExchangeNode(ctx_.getNextNodeId());
    lhsExchange.addChild(leftChildFragment.getPlanRoot());
    lhsExchange.computeStats(null);
    node.setChild(0, lhsExchange);
    ExchangeNode rhsExchange = new ExchangeNode(ctx_.getNextNodeId());
    rhsExchange.addChild(rightChildFragment.getPlanRoot());
    rhsExchange.computeStats(null);
    node.setChild(1, rhsExchange);
    // Connect the child fragments in a new fragment, and set the data partition
    // of the new fragment and its child fragments.
    PlanFragment joinFragment = new PlanFragment(ctx_.getNextFragmentId(), node, lhsJoinPartition);
    leftChildFragment.setDestination(lhsExchange);
    leftChildFragment.setOutputPartition(lhsJoinPartition);
    rightChildFragment.setDestination(rhsExchange);
    rightChildFragment.setOutputPartition(rhsJoinPartition);
    return joinFragment;
}
#method_after
private PlanFragment createPartitionedHashJoinFragment(HashJoinNode node, Analyzer analyzer, boolean lhsHasCompatPartition, boolean rhsHasCompatPartition, PlanFragment leftChildFragment, PlanFragment rightChildFragment, List<Expr> lhsJoinExprs, List<Expr> rhsJoinExprs, ArrayList<PlanFragment> fragments) throws ImpalaException {
    node.setDistributionMode(HashJoinNode.DistributionMode.PARTITIONED);
    // on partition exprs (both using the canonical equivalence class representatives).
    if (lhsHasCompatPartition && rhsHasCompatPartition && isCompatPartition(leftChildFragment.getDataPartition(), rightChildFragment.getDataPartition(), lhsJoinExprs, rhsJoinExprs, analyzer)) {
        node.setChild(0, leftChildFragment.getPlanRoot());
        node.setChild(1, rightChildFragment.getPlanRoot());
        // fix up PlanNode.fragment_ for the migrated PlanNode tree of the rhs child
        leftChildFragment.setFragmentInPlanTree(node.getChild(1));
        // Relocate input fragments of rightChildFragment to leftChildFragment.
        for (PlanFragment rhsInput : rightChildFragment.getChildren()) {
            leftChildFragment.getChildren().add(rhsInput);
        }
        // Remove right fragment because its plan tree has been merged into leftFragment.
        fragments.remove(rightChildFragment);
        leftChildFragment.setPlanRoot(node);
        return leftChildFragment;
    }
    // The lhs input fragment is already partitioned on the join exprs.
    // Make the HashJoin the new root of leftChildFragment and set the join's
    // first child to the lhs plan root. The second child of the join is an
    // ExchangeNode that is fed by the rhsInputFragment whose sink repartitions
    // its data by the rhs join exprs.
    DataPartition rhsJoinPartition = null;
    if (lhsHasCompatPartition) {
        rhsJoinPartition = getCompatPartition(lhsJoinExprs, leftChildFragment.getDataPartition(), rhsJoinExprs, analyzer);
        if (rhsJoinPartition != null) {
            node.setChild(0, leftChildFragment.getPlanRoot());
            connectChildFragment(node, 1, leftChildFragment, rightChildFragment);
            rightChildFragment.setOutputPartition(rhsJoinPartition);
            leftChildFragment.setPlanRoot(node);
            return leftChildFragment;
        }
    }
    // Same as above but with rhs and lhs reversed.
    DataPartition lhsJoinPartition = null;
    if (rhsHasCompatPartition) {
        lhsJoinPartition = getCompatPartition(rhsJoinExprs, rightChildFragment.getDataPartition(), lhsJoinExprs, analyzer);
        if (lhsJoinPartition != null) {
            node.setChild(1, rightChildFragment.getPlanRoot());
            connectChildFragment(node, 0, rightChildFragment, leftChildFragment);
            leftChildFragment.setOutputPartition(lhsJoinPartition);
            rightChildFragment.setPlanRoot(node);
            return rightChildFragment;
        }
    }
    Preconditions.checkState(lhsJoinPartition == null);
    Preconditions.checkState(rhsJoinPartition == null);
    lhsJoinPartition = DataPartition.hashPartitioned(Expr.cloneList(lhsJoinExprs));
    rhsJoinPartition = DataPartition.hashPartitioned(Expr.cloneList(rhsJoinExprs));
    // Neither lhs nor rhs are already partitioned on the join exprs.
    // Create a new parent fragment containing a HashJoin node with two
    // ExchangeNodes as inputs; the latter are the destinations of the
    // left- and rightChildFragments, which now partition their output
    // on their respective join exprs.
    // The new fragment is hash-partitioned on the lhs input join exprs.
    ExchangeNode lhsExchange = new ExchangeNode(ctx_.getNextNodeId(), leftChildFragment.getPlanRoot());
    lhsExchange.computeStats(null);
    node.setChild(0, lhsExchange);
    ExchangeNode rhsExchange = new ExchangeNode(ctx_.getNextNodeId(), rightChildFragment.getPlanRoot());
    rhsExchange.computeStats(null);
    node.setChild(1, rhsExchange);
    // Connect the child fragments in a new fragment, and set the data partition
    // of the new fragment and its child fragments.
    PlanFragment joinFragment = new PlanFragment(ctx_.getNextFragmentId(), node, lhsJoinPartition);
    leftChildFragment.setDestination(lhsExchange);
    leftChildFragment.setOutputPartition(lhsJoinPartition);
    rightChildFragment.setDestination(rhsExchange);
    rightChildFragment.setOutputPartition(rhsJoinPartition);
    return joinFragment;
}
#end_block

#method_before
private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment rightChildFragment, PlanFragment leftChildFragment, long perNodeMemLimit, ArrayList<PlanFragment> fragments) throws ImpalaException {
    // broadcast: send the rightChildFragment's output to each node executing
    // the leftChildFragment; the cost across all nodes is proportional to the
    // total amount of data sent
    Analyzer analyzer = ctx_.getRootAnalyzer();
    PlanNode rhsTree = rightChildFragment.getPlanRoot();
    long rhsDataSize = 0;
    long broadcastCost = Long.MAX_VALUE;
    if (rhsTree.getCardinality() != -1 && leftChildFragment.getNumNodes() != -1) {
        rhsDataSize = Math.round(rhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(rhsTree));
        broadcastCost = rhsDataSize * leftChildFragment.getNumNodes();
    }
    LOG.debug("broadcast: cost=" + Long.toString(broadcastCost));
    LOG.debug("card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()) + " #nodes=" + Integer.toString(leftChildFragment.getNumNodes()));
    // repartition: both left- and rightChildFragment are partitioned on the
    // join exprs
    PlanNode lhsTree = leftChildFragment.getPlanRoot();
    long partitionCost = Long.MAX_VALUE;
    List<Expr> lhsJoinExprs = Lists.newArrayList();
    List<Expr> rhsJoinExprs = Lists.newArrayList();
    for (Expr joinConjunct : node.getEqJoinConjuncts()) {
        // no remapping necessary
        lhsJoinExprs.add(joinConjunct.getChild(0).clone());
        rhsJoinExprs.add(joinConjunct.getChild(1).clone());
    }
    boolean lhsHasCompatPartition = false;
    boolean rhsHasCompatPartition = false;
    if (lhsTree.getCardinality() != -1 && rhsTree.getCardinality() != -1) {
        lhsHasCompatPartition = analyzer.equivSets(lhsJoinExprs, leftChildFragment.getDataPartition().getPartitionExprs());
        rhsHasCompatPartition = analyzer.equivSets(rhsJoinExprs, rightChildFragment.getDataPartition().getPartitionExprs());
        double lhsCost = (lhsHasCompatPartition) ? 0.0 : Math.round(lhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(lhsTree));
        double rhsCost = (rhsHasCompatPartition) ? 0.0 : Math.round(rhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(rhsTree));
        partitionCost = Math.round(lhsCost + rhsCost);
    }
    LOG.debug("partition: cost=" + Long.toString(partitionCost));
    LOG.debug("lhs card=" + Long.toString(lhsTree.getCardinality()) + " row_size=" + Float.toString(lhsTree.getAvgRowSize()));
    LOG.debug("rhs card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()));
    LOG.debug(rhsTree.getExplainString());
    boolean doBroadcast = false;
    // we're unable to estimate the cost
    if ((node.getJoinOp() != JoinOperator.RIGHT_OUTER_JOIN && node.getJoinOp() != JoinOperator.FULL_OUTER_JOIN && node.getJoinOp() != JoinOperator.RIGHT_SEMI_JOIN && node.getJoinOp() != JoinOperator.RIGHT_ANTI_JOIN && // size is less than the pernode memlimit
    (node.getDistributionModeHint() == DistributionMode.BROADCAST || perNodeMemLimit == 0 || Math.round(rhsDataSize * PlannerContext.HASH_TBL_SPACE_OVERHEAD) <= perNodeMemLimit) && // join is more costly than a partitioned join
    (node.getDistributionModeHint() == DistributionMode.BROADCAST || (node.getDistributionModeHint() != DistributionMode.PARTITIONED && broadcastCost <= partitionCost))) || node.getJoinOp().isNullAwareLeftAntiJoin()) {
        doBroadcast = true;
    }
    PlanFragment hjFragment = null;
    if (doBroadcast) {
        node.setDistributionMode(HashJoinNode.DistributionMode.BROADCAST);
        // Doesn't create a new fragment, but modifies leftChildFragment to execute
        // the join; the build input is provided by an ExchangeNode, which is the
        // destination of the rightChildFragment's output
        node.setChild(0, leftChildFragment.getPlanRoot());
        connectChildFragment(node, 1, leftChildFragment, rightChildFragment);
        leftChildFragment.setPlanRoot(node);
        hjFragment = leftChildFragment;
    } else {
        hjFragment = createPartitionedHashJoinFragment(node, analyzer, lhsHasCompatPartition, rhsHasCompatPartition, leftChildFragment, rightChildFragment, lhsJoinExprs, rhsJoinExprs, fragments);
    }
    for (RuntimeFilter filter : node.getRuntimeFilters()) {
        filter.setIsBroadcast(doBroadcast);
        filter.computeHasLocalTargets();
        // Work around IMPALA-3450, where cardinalities might be wrong in single-node plans
        // with UNION and LIMITs.
        // TODO: Remove.
        filter.computeNdvEstimate();
    }
    return hjFragment;
}
#method_after
private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment rightChildFragment, PlanFragment leftChildFragment, long perNodeMemLimit, ArrayList<PlanFragment> fragments) throws ImpalaException {
    // For both join types, the total cost is calculated as the amount of data
    // sent over the network, plus the amount of data inserted into the hash table.
    // broadcast: send the rightChildFragment's output to each node executing
    // the leftChildFragment, and build a hash table with it on each node.
    Analyzer analyzer = ctx_.getRootAnalyzer();
    PlanNode rhsTree = rightChildFragment.getPlanRoot();
    long rhsDataSize = 0;
    long broadcastCost = Long.MAX_VALUE;
    if (rhsTree.getCardinality() != -1) {
        rhsDataSize = Math.round(rhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(rhsTree));
        if (leftChildFragment.getNumNodes() != -1) {
            broadcastCost = 2 * rhsDataSize * leftChildFragment.getNumNodes();
        }
    }
    LOG.debug("broadcast: cost=" + Long.toString(broadcastCost));
    LOG.debug("card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()) + " #nodes=" + Integer.toString(leftChildFragment.getNumNodes()));
    // repartition: both left- and rightChildFragment are partitioned on the
    // join exprs, and a hash table is built with the rightChildFragment's output.
    PlanNode lhsTree = leftChildFragment.getPlanRoot();
    long partitionCost = Long.MAX_VALUE;
    List<Expr> lhsJoinExprs = Lists.newArrayList();
    List<Expr> rhsJoinExprs = Lists.newArrayList();
    for (Expr joinConjunct : node.getEqJoinConjuncts()) {
        // no remapping necessary
        lhsJoinExprs.add(joinConjunct.getChild(0).clone());
        rhsJoinExprs.add(joinConjunct.getChild(1).clone());
    }
    boolean lhsHasCompatPartition = false;
    boolean rhsHasCompatPartition = false;
    if (lhsTree.getCardinality() != -1 && rhsTree.getCardinality() != -1) {
        lhsHasCompatPartition = analyzer.equivSets(lhsJoinExprs, leftChildFragment.getDataPartition().getPartitionExprs());
        rhsHasCompatPartition = analyzer.equivSets(rhsJoinExprs, rightChildFragment.getDataPartition().getPartitionExprs());
        double lhsNetworkCost = (lhsHasCompatPartition) ? 0.0 : Math.round(lhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(lhsTree));
        double rhsNetworkCost = (rhsHasCompatPartition) ? 0.0 : rhsDataSize;
        partitionCost = Math.round(lhsNetworkCost + rhsNetworkCost + rhsDataSize);
    }
    LOG.debug("partition: cost=" + Long.toString(partitionCost));
    LOG.debug("lhs card=" + Long.toString(lhsTree.getCardinality()) + " row_size=" + Float.toString(lhsTree.getAvgRowSize()));
    LOG.debug("rhs card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()));
    LOG.debug(rhsTree.getExplainString());
    boolean doBroadcast = false;
    // we're unable to estimate the cost
    if ((node.getJoinOp() != JoinOperator.RIGHT_OUTER_JOIN && node.getJoinOp() != JoinOperator.FULL_OUTER_JOIN && node.getJoinOp() != JoinOperator.RIGHT_SEMI_JOIN && node.getJoinOp() != JoinOperator.RIGHT_ANTI_JOIN && // size is less than the pernode memlimit
    (node.getDistributionModeHint() == DistributionMode.BROADCAST || perNodeMemLimit == 0 || Math.round(rhsDataSize * PlannerContext.HASH_TBL_SPACE_OVERHEAD) <= perNodeMemLimit) && // join is more costly than a partitioned join
    (node.getDistributionModeHint() == DistributionMode.BROADCAST || (node.getDistributionModeHint() != DistributionMode.PARTITIONED && broadcastCost <= partitionCost))) || node.getJoinOp().isNullAwareLeftAntiJoin()) {
        doBroadcast = true;
    }
    PlanFragment hjFragment = null;
    if (doBroadcast) {
        node.setDistributionMode(HashJoinNode.DistributionMode.BROADCAST);
        // Doesn't create a new fragment, but modifies leftChildFragment to execute
        // the join; the build input is provided by an ExchangeNode, which is the
        // destination of the rightChildFragment's output
        node.setChild(0, leftChildFragment.getPlanRoot());
        connectChildFragment(node, 1, leftChildFragment, rightChildFragment);
        leftChildFragment.setPlanRoot(node);
        hjFragment = leftChildFragment;
    } else {
        hjFragment = createPartitionedHashJoinFragment(node, analyzer, lhsHasCompatPartition, rhsHasCompatPartition, leftChildFragment, rightChildFragment, lhsJoinExprs, rhsJoinExprs, fragments);
    }
    for (RuntimeFilter filter : node.getRuntimeFilters()) {
        filter.setIsBroadcast(doBroadcast);
        filter.computeHasLocalTargets();
        // Work around IMPALA-3450, where cardinalities might be wrong in single-node plans
        // with UNION and LIMITs.
        // TODO: Remove.
        filter.computeNdvEstimate();
    }
    return hjFragment;
}
#end_block

#method_before
private void connectChildFragment(PlanNode node, int childIdx, PlanFragment parentFragment, PlanFragment childFragment) throws ImpalaException {
    ExchangeNode exchangeNode = new ExchangeNode(ctx_.getNextNodeId());
    exchangeNode.addChild(childFragment.getPlanRoot());
    exchangeNode.init(ctx_.getRootAnalyzer());
    exchangeNode.setFragment(parentFragment);
    node.setChild(childIdx, exchangeNode);
    childFragment.setDestination(exchangeNode);
}
#method_after
private void connectChildFragment(PlanNode node, int childIdx, PlanFragment parentFragment, PlanFragment childFragment) throws ImpalaException {
    ExchangeNode exchangeNode = new ExchangeNode(ctx_.getNextNodeId(), childFragment.getPlanRoot());
    exchangeNode.init(ctx_.getRootAnalyzer());
    exchangeNode.setFragment(parentFragment);
    node.setChild(childIdx, exchangeNode);
    childFragment.setDestination(exchangeNode);
}
#end_block

#method_before
private PlanFragment createParentFragment(PlanFragment childFragment, DataPartition parentPartition) throws ImpalaException {
    ExchangeNode exchangeNode = new ExchangeNode(ctx_.getNextNodeId());
    exchangeNode.addChild(childFragment.getPlanRoot());
    exchangeNode.init(ctx_.getRootAnalyzer());
    PlanFragment parentFragment = new PlanFragment(ctx_.getNextFragmentId(), exchangeNode, parentPartition);
    childFragment.setDestination(exchangeNode);
    childFragment.setOutputPartition(parentPartition);
    return parentFragment;
}
#method_after
private PlanFragment createParentFragment(PlanFragment childFragment, DataPartition parentPartition) throws ImpalaException {
    ExchangeNode exchangeNode = new ExchangeNode(ctx_.getNextNodeId(), childFragment.getPlanRoot());
    exchangeNode.init(ctx_.getRootAnalyzer());
    PlanFragment parentFragment = new PlanFragment(ctx_.getNextFragmentId(), exchangeNode, parentPartition);
    childFragment.setDestination(exchangeNode);
    childFragment.setOutputPartition(parentPartition);
    return parentFragment;
}
#end_block

#method_before
public String getConjunctSql() {
    List<String> partitionCols = Lists.newArrayList();
    for (int i = 0; i < getTable().getNumClusteringCols(); ++i) {
        partitionCols.add(ToSqlUtils.getIdentSql(getTable().getColumns().get(i).getName()));
    }
    List<String> conjuncts = Lists.newArrayList();
    for (int i = 0; i < partitionCols.size(); ++i) {
        LiteralExpr expr = getPartitionValues().get(i);
        String sql = expr.toSql();
        if (expr instanceof NullLiteral || sql.isEmpty()) {
            conjuncts.add(ToSqlUtils.getIdentSql(partitionCols.get(i)) + " IS NULL");
        } else {
            conjuncts.add(ToSqlUtils.getIdentSql(partitionCols.get(i)) + "=" + sql);
        }
    }
    return "(" + Joiner.on(" AND ").join(conjuncts) + ")";
}
#method_after
public String getConjunctSql() {
    List<String> partColSql = Lists.newArrayList();
    for (Column partCol : getTable().getClusteringColumns()) {
        partColSql.add(ToSqlUtils.getIdentSql(partCol.getName()));
    }
    List<String> conjuncts = Lists.newArrayList();
    for (int i = 0; i < partColSql.size(); ++i) {
        LiteralExpr partVal = getPartitionValues().get(i);
        String partValSql = partVal.toSql();
        if (partVal instanceof NullLiteral || partValSql.isEmpty()) {
            conjuncts.add(partColSql.get(i) + " IS NULL");
        } else {
            conjuncts.add(partColSql.get(i) + "=" + partValSql);
        }
    }
    return "(" + Joiner.on(" AND ").join(conjuncts) + ")";
}
#end_block

#method_before
public void analyzePlanHints(Analyzer analyzer) {
    if (planHints_ == null)
        return;
    for (String hint : planHints_) {
        if (!hint.equalsIgnoreCase("straight_join")) {
            analyzer.addWarning("PLAN hint not recognized: " + hint);
        }
        isStraightJoin_ = true;
        analyzer.setHasPlanHints();
    }
}
#method_after
public void analyzePlanHints(Analyzer analyzer) {
    if (planHints_ == null)
        return;
    for (String hint : planHints_) {
        if (!hint.equalsIgnoreCase("straight_join")) {
            analyzer.addWarning("PLAN hint not recognized: " + hint);
        }
        analyzer.setIsStraightJoin();
    }
}
#end_block

#method_before
public void reset() {
    for (SelectListItem item : items_) {
        if (!item.isStar())
            item.getExpr().reset();
    }
    isStraightJoin_ = false;
}
#method_after
public void reset() {
    for (SelectListItem item : items_) {
        if (!item.isStar())
            item.getExpr().reset();
    }
}
#end_block

#method_before
public void load(boolean reuseMetadata, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl, boolean loadFileMetadata, boolean loadTableSchema, Set<String> partitionsToUpdate) throws TableLoadingException {
    // turn all exceptions into TableLoadingException
    msTable_ = msTbl;
    try {
        if (loadTableSchema)
            loadSchema(client, msTbl);
        if (reuseMetadata && getCatalogVersion() == Catalog.INITIAL_CATALOG_VERSION) {
            // This is the special case of CTAS that creates a 'temp' table that does not
            // actually exist in the Hive Metastore.
            initializePartitionMetadata(msTbl);
            updateStatsFromHmsTable(msTbl);
            return;
        }
        // Load partition and file metadata
        if (!reuseMetadata) {
            // Load all partitions from Hive Metastore, including file metadata.
            LOG.debug("load table from Hive Metastore: " + db_.getName() + "." + name_);
            List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
            msPartitions.addAll(MetaStoreUtil.fetchAllPartitions(client, db_.getName(), name_, NUM_PARTITION_FETCH_RETRIES));
            loadAllPartitions(msPartitions, msTbl);
        } else {
            // Incrementally update this table's partitions and file metadata
            LOG.debug("incremental update for table: " + db_.getName() + "." + name_);
            Preconditions.checkState(partitionsToUpdate == null || loadFileMetadata);
            updateMdFromHmsTable(msTbl);
            if (msTbl.getPartitionKeysSize() == 0) {
                if (loadFileMetadata)
                    updateUnpartitionedTableFileMd();
            } else {
                updatePartitionsFromHms(client, partitionsToUpdate, loadFileMetadata);
            }
        }
        if (loadTableSchema)
            setAvroSchema(client, msTbl);
        updateStatsFromHmsTable(msTbl);
    } catch (TableLoadingException e) {
        throw e;
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for table: " + name_, e);
    }
}
#method_after
@Override
public void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    load(reuseMetadata, client, msTbl, true, true, null);
}
#end_block

#method_before
private void updatePartitionsFromHms(HiveMetaStoreClient client, Set<String> partitionsToUpdate, boolean loadFileMetadata) throws Exception {
    LOG.debug("sync table partitions: " + name_);
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    Preconditions.checkState(msTbl.getPartitionKeysSize() != 0);
    Preconditions.checkState(loadFileMetadata || partitionsToUpdate == null);
    // Retrieve all the partition names from the Hive Metastore. We need this to
    // identify the delta between partitions of the local HdfsTable and the table entry
    // in the Hive Metastore. Note: This is a relatively "cheap" operation
    // (~.3 secs for 30K partitions).
    Set<String> msPartitionNames = Sets.newHashSet();
    msPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
    // Names of loaded partitions in this table
    Set<String> partitionNames = Sets.newHashSet();
    // Partitions for which file metadata must be loaded
    List<HdfsPartition> partitionsToUpdateFileMd = Lists.newArrayList();
    // Partitions that need to be dropped and recreated from scratch
    List<HdfsPartition> dirtyPartitions = Lists.newArrayList();
    // Partitions that need to be removed from this table. That includes dirty
    // partitions as well as partitions that were removed from the Hive Metastore.
    List<HdfsPartition> partitionsToRemove = Lists.newArrayList();
    // partitions that no longer exist in the Hive Metastore.
    for (HdfsPartition partition : partitionMap_.values()) {
        // Ignore the default partition
        if (partition.isDefaultPartition())
            continue;
        // that were removed from HMS using some external process, e.g. Hive.
        if (!msPartitionNames.contains(partition.getPartitionName())) {
            partitionsToRemove.add(partition);
        }
        if (partition.isDirty()) {
            // Dirty partitions are updated by removing them from table's partition
            // list and loading them from the Hive Metastore.
            dirtyPartitions.add(partition);
        } else {
            if (partitionsToUpdate == null && loadFileMetadata) {
                partitionsToUpdateFileMd.add(partition);
            }
        }
        Preconditions.checkNotNull(partition.getCachedMsPartitionDescriptor());
        partitionNames.add(partition.getPartitionName());
    }
    partitionsToRemove.addAll(dirtyPartitions);
    for (HdfsPartition partition : partitionsToRemove) dropPartition(partition);
    // Load dirty partitions from Hive Metastore
    loadPartitionsFromMetastore(dirtyPartitions, client);
    // Identify and load partitions that were added in the Hive Metastore but don't
    // exist in this table.
    Set<String> newPartitionsInHms = Sets.difference(msPartitionNames, partitionNames);
    loadPartitionsFromMetastore(newPartitionsInHms, client);
    // reloaded by loadPartitionsFromMetastore().
    if (partitionsToUpdate != null) {
        partitionsToUpdate.removeAll(newPartitionsInHms);
    }
    // descriptors and block metadata of a table (e.g. REFRESH statement).
    if (loadFileMetadata) {
        if (partitionsToUpdate != null) {
            // Only reload file metadata of partitions specified in 'partitionsToUpdate'
            Preconditions.checkState(partitionsToUpdateFileMd.isEmpty());
            partitionsToUpdateFileMd = getPartitionsByName(partitionsToUpdate);
        }
        loadPartitionFileMetadata(partitionsToUpdateFileMd);
    }
}
#method_after
private void updatePartitionsFromHms(IMetaStoreClient client, Set<String> partitionsToUpdate, boolean loadFileMetadata) throws Exception {
    LOG.debug("sync table partitions: " + name_);
    org.apache.hadoop.hive.metastore.api.Table msTbl = getMetaStoreTable();
    Preconditions.checkNotNull(msTbl);
    Preconditions.checkState(msTbl.getPartitionKeysSize() != 0);
    Preconditions.checkState(loadFileMetadata || partitionsToUpdate == null);
    // Retrieve all the partition names from the Hive Metastore. We need this to
    // identify the delta between partitions of the local HdfsTable and the table entry
    // in the Hive Metastore. Note: This is a relatively "cheap" operation
    // (~.3 secs for 30K partitions).
    Set<String> msPartitionNames = Sets.newHashSet();
    msPartitionNames.addAll(client.listPartitionNames(db_.getName(), name_, (short) -1));
    // Names of loaded partitions in this table
    Set<String> partitionNames = Sets.newHashSet();
    // Partitions for which file metadata must be loaded
    List<HdfsPartition> partitionsToUpdateFileMd = Lists.newArrayList();
    // Partitions that need to be dropped and recreated from scratch
    List<HdfsPartition> dirtyPartitions = Lists.newArrayList();
    // Partitions that need to be removed from this table. That includes dirty
    // partitions as well as partitions that were removed from the Hive Metastore.
    List<HdfsPartition> partitionsToRemove = Lists.newArrayList();
    // partitions that no longer exist in the Hive Metastore.
    for (HdfsPartition partition : partitionMap_.values()) {
        // Ignore the default partition
        if (partition.isDefaultPartition())
            continue;
        // that were removed from HMS using some external process, e.g. Hive.
        if (!msPartitionNames.contains(partition.getPartitionName())) {
            partitionsToRemove.add(partition);
        }
        if (partition.isDirty()) {
            // Dirty partitions are updated by removing them from table's partition
            // list and loading them from the Hive Metastore.
            dirtyPartitions.add(partition);
        } else {
            if (partitionsToUpdate == null && loadFileMetadata) {
                partitionsToUpdateFileMd.add(partition);
            }
        }
        Preconditions.checkNotNull(partition.getCachedMsPartitionDescriptor());
        partitionNames.add(partition.getPartitionName());
    }
    partitionsToRemove.addAll(dirtyPartitions);
    for (HdfsPartition partition : partitionsToRemove) dropPartition(partition);
    // Load dirty partitions from Hive Metastore
    loadPartitionsFromMetastore(dirtyPartitions, client);
    // Identify and load partitions that were added in the Hive Metastore but don't
    // exist in this table.
    Set<String> newPartitionsInHms = Sets.difference(msPartitionNames, partitionNames);
    loadPartitionsFromMetastore(newPartitionsInHms, client);
    // reloaded by loadPartitionsFromMetastore().
    if (partitionsToUpdate != null) {
        partitionsToUpdate.removeAll(newPartitionsInHms);
    }
    // descriptors and block metadata of a table (e.g. REFRESH statement).
    if (loadFileMetadata) {
        if (partitionsToUpdate != null) {
            // Only reload file metadata of partitions specified in 'partitionsToUpdate'
            Preconditions.checkState(partitionsToUpdateFileMd.isEmpty());
            partitionsToUpdateFileMd = getPartitionsByName(partitionsToUpdate);
        }
        loadPartitionFileMetadata(partitionsToUpdateFileMd);
    }
}
#end_block

#method_before
private void setAvroSchema(HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws Exception {
    Preconditions.checkState(isSchemaLoaded_);
    String inputFormat = msTbl.getSd().getInputFormat();
    if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO || hasAvroData_) {
        // Look for Avro schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
        // taking precedence.
        List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
        schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
        schemaSearchLocations.add(getMetaStoreTable().getParameters());
        avroSchema_ = AvroSchemaUtils.getAvroSchema(schemaSearchLocations);
        if (avroSchema_ == null) {
            // No Avro schema was explicitly set in the table metadata, so infer the Avro
            // schema from the column definitions.
            Schema inferredSchema = AvroSchemaConverter.convertFieldSchemas(msTbl.getSd().getCols(), getFullName());
            avroSchema_ = inferredSchema.toString();
        }
        String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
        if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
            // using the fields from the storage descriptor (same as Hive).
            return;
        } else {
            // Generate new FieldSchemas from the Avro schema. This step reconciles
            // differences in the column definitions and the Avro schema. For
            // Impala-created tables this step is not necessary because the same
            // resolution is done during table creation. But Hive-created tables
            // store the original column definitions, and not the reconciled ones.
            List<ColumnDef> colDefs = ColumnDef.createFromFieldSchemas(msTbl.getSd().getCols());
            List<ColumnDef> avroCols = AvroSchemaParser.parse(avroSchema_);
            StringBuilder warning = new StringBuilder();
            List<ColumnDef> reconciledColDefs = AvroSchemaUtils.reconcileSchemas(colDefs, avroCols, warning);
            if (warning.length() != 0) {
                LOG.warn(String.format("Warning while loading table %s:\n%s", getFullName(), warning.toString()));
            }
            AvroSchemaUtils.setFromSerdeComment(reconciledColDefs);
            // Reset and update nonPartFieldSchemas_ to the reconcicled colDefs.
            nonPartFieldSchemas_.clear();
            nonPartFieldSchemas_.addAll(ColumnDef.toFieldSchemas(reconciledColDefs));
            // Update the columns as per the reconciled colDefs and re-load stats.
            clearColumns();
            addColumnsFromFieldSchemas(msTbl.getPartitionKeys());
            addColumnsFromFieldSchemas(nonPartFieldSchemas_);
            loadAllColumnStats(client);
        }
    }
}
#method_after
private void setAvroSchema(IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws Exception {
    Preconditions.checkState(isSchemaLoaded_);
    String inputFormat = msTbl.getSd().getInputFormat();
    if (HdfsFileFormat.fromJavaClassName(inputFormat) == HdfsFileFormat.AVRO || hasAvroData_) {
        // Look for Avro schema in TBLPROPERTIES and in SERDEPROPERTIES, with the latter
        // taking precedence.
        List<Map<String, String>> schemaSearchLocations = Lists.newArrayList();
        schemaSearchLocations.add(getMetaStoreTable().getSd().getSerdeInfo().getParameters());
        schemaSearchLocations.add(getMetaStoreTable().getParameters());
        avroSchema_ = AvroSchemaUtils.getAvroSchema(schemaSearchLocations);
        if (avroSchema_ == null) {
            // No Avro schema was explicitly set in the table metadata, so infer the Avro
            // schema from the column definitions.
            Schema inferredSchema = AvroSchemaConverter.convertFieldSchemas(msTbl.getSd().getCols(), getFullName());
            avroSchema_ = inferredSchema.toString();
        }
        String serdeLib = msTbl.getSd().getSerdeInfo().getSerializationLib();
        if (serdeLib == null || serdeLib.equals("org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe")) {
            // using the fields from the storage descriptor (same as Hive).
            return;
        } else {
            // Generate new FieldSchemas from the Avro schema. This step reconciles
            // differences in the column definitions and the Avro schema. For
            // Impala-created tables this step is not necessary because the same
            // resolution is done during table creation. But Hive-created tables
            // store the original column definitions, and not the reconciled ones.
            List<ColumnDef> colDefs = ColumnDef.createFromFieldSchemas(msTbl.getSd().getCols());
            List<ColumnDef> avroCols = AvroSchemaParser.parse(avroSchema_);
            StringBuilder warning = new StringBuilder();
            List<ColumnDef> reconciledColDefs = AvroSchemaUtils.reconcileSchemas(colDefs, avroCols, warning);
            if (warning.length() != 0) {
                LOG.warn(String.format("Warning while loading table %s:\n%s", getFullName(), warning.toString()));
            }
            AvroSchemaUtils.setFromSerdeComment(reconciledColDefs);
            // Reset and update nonPartFieldSchemas_ to the reconcicled colDefs.
            nonPartFieldSchemas_.clear();
            nonPartFieldSchemas_.addAll(ColumnDef.toFieldSchemas(reconciledColDefs));
            // Update the columns as per the reconciled colDefs and re-load stats.
            clearColumns();
            addColumnsFromFieldSchemas(msTbl.getPartitionKeys());
            addColumnsFromFieldSchemas(nonPartFieldSchemas_);
            loadAllColumnStats(client);
        }
    }
}
#end_block

#method_before
private void loadSchema(HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws Exception {
    nonPartFieldSchemas_.clear();
    // set nullPartitionKeyValue from the hive conf.
    nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
    // set NULL indicator string from table properties
    nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
    if (nullColumnValue_ == null)
        nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
    // Excludes partition columns.
    nonPartFieldSchemas_.addAll(msTbl.getSd().getCols());
    // The number of clustering columns is the number of partition keys.
    numClusteringCols_ = msTbl.getPartitionKeys().size();
    partitionLocationCompressor_.setClusteringColumns(numClusteringCols_);
    clearColumns();
    // Add all columns to the table. Ordering is important: partition columns first,
    // then all other columns.
    addColumnsFromFieldSchemas(msTbl.getPartitionKeys());
    addColumnsFromFieldSchemas(nonPartFieldSchemas_);
    loadAllColumnStats(client);
    isSchemaLoaded_ = true;
}
#method_after
private void loadSchema(IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws Exception {
    nonPartFieldSchemas_.clear();
    // set nullPartitionKeyValue from the hive conf.
    nullPartitionKeyValue_ = client.getConfigValue("hive.exec.default.partition.name", "__HIVE_DEFAULT_PARTITION__");
    // set NULL indicator string from table properties
    nullColumnValue_ = msTbl.getParameters().get(serdeConstants.SERIALIZATION_NULL_FORMAT);
    if (nullColumnValue_ == null)
        nullColumnValue_ = DEFAULT_NULL_COLUMN_VALUE;
    // Excludes partition columns.
    nonPartFieldSchemas_.addAll(msTbl.getSd().getCols());
    // The number of clustering columns is the number of partition keys.
    numClusteringCols_ = msTbl.getPartitionKeys().size();
    partitionLocationCompressor_.setClusteringColumns(numClusteringCols_);
    clearColumns();
    // Add all columns to the table. Ordering is important: partition columns first,
    // then all other columns.
    addColumnsFromFieldSchemas(msTbl.getPartitionKeys());
    addColumnsFromFieldSchemas(nonPartFieldSchemas_);
    loadAllColumnStats(client);
    isSchemaLoaded_ = true;
}
#end_block

#method_before
private void loadPartitionsFromMetastore(Set<String> partitionNames, HiveMetaStoreClient client) throws Exception {
    Preconditions.checkNotNull(partitionNames);
    if (partitionNames.isEmpty())
        return;
    // Load partition metadata from Hive Metastore.
    List<org.apache.hadoop.hive.metastore.api.Partition> msPartitions = Lists.newArrayList();
    msPartitions.addAll(MetaStoreUtil.fetchPartitionsByName(client, Lists.newArrayList(partitionNames), db_.getName(), name_));
    Map<FsKey, FileBlocksInfo> fileBlocksToLoad = Maps.newHashMap();
    for (org.apache.hadoop.hive.metastore.api.Partition msPartition : msPartitions) {
        HdfsPartition partition = createPartition(msPartition.getSd(), msPartition, fileBlocksToLoad);
        addPartition(partition);
        // this table's partition list. Skip the partition.
        if (partition == null)
            continue;
        if (partition.getFileFormat() == HdfsFileFormat.AVRO)
            hasAvroData_ = true;
        if (msPartition.getParameters() != null) {
            partition.setNumRows(getRowCount(msPartition.getParameters()));
        }
        if (!TAccessLevelUtil.impliesWriteAccess(partition.getAccessLevel())) {
            // TODO: READ_ONLY isn't exactly correct because the it's possible the
            // partition does not have READ permissions either. When we start checking
            // whether we can READ from a table, this should be updated to set the
            // table's access level to the "lowest" effective level across all
            // partitions. That is, if one partition has READ_ONLY and another has
            // WRITE_ONLY the table's access level should be NONE.
            accessLevel_ = TAccessLevel.READ_ONLY;
        }
    }
    loadDiskIds(fileBlocksToLoad);
}
#method_after
private void loadPartitionsFromMetastore(List<HdfsPartition> partitions, IMetaStoreClient client) throws Exception {
    Preconditions.checkNotNull(partitions);
    if (partitions.isEmpty())
        return;
    LOG.info(String.format("Incrementally updating %d/%d partitions.", partitions.size(), partitionMap_.size()));
    Set<String> partitionNames = Sets.newHashSet();
    for (HdfsPartition part : partitions) {
        partitionNames.add(part.getPartitionName());
    }
    loadPartitionsFromMetastore(partitionNames, client);
}
#end_block

#method_before
@Test
public void TestConfigValidation() throws InternalException {
    String sentryConfig = ctx_.authzConfig.getSentryConfig().getConfigFile();
    // Valid configs pass validation.
    AuthorizationConfig config = AuthorizationConfig.createHadoopGroupAuthConfig("server1", AUTHZ_POLICY_FILE, sentryConfig);
    config.validateConfig();
    Assert.assertTrue(config.isEnabled());
    Assert.assertTrue(config.isFileBasedPolicy());
    config = AuthorizationConfig.createHadoopGroupAuthConfig("server1", null, sentryConfig);
    config.validateConfig();
    Assert.assertTrue(config.isEnabled());
    Assert.assertTrue(!config.isFileBasedPolicy());
    // Invalid configs
    // No sentry configuration file.
    config = AuthorizationConfig.createHadoopGroupAuthConfig("server1", AUTHZ_POLICY_FILE, null);
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
    } catch (Exception e) {
        Assert.assertEquals(e.getMessage(), "A valid path to a sentry-site.xml config " + "file must be set using --sentry_config to enable authorization.");
    }
    // Empty / null server name.
    config = AuthorizationConfig.createHadoopGroupAuthConfig("", AUTHZ_POLICY_FILE, sentryConfig);
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (IllegalArgumentException e) {
        Assert.assertEquals(e.getMessage(), "Authorization is enabled but the server name is null or empty. Set the " + "server name using the impalad --server_name flag.");
    }
    config = AuthorizationConfig.createHadoopGroupAuthConfig(null, AUTHZ_POLICY_FILE, sentryConfig);
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (IllegalArgumentException e) {
        Assert.assertEquals(e.getMessage(), "Authorization is enabled but the server name is null or empty. Set the " + "server name using the impalad --server_name flag.");
    }
    // Sentry config file does not exist.
    config = AuthorizationConfig.createHadoopGroupAuthConfig("server1", "", "/path/does/not/exist.xml");
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (Exception e) {
        Assert.assertEquals(e.getMessage(), "Sentry configuration file does not exist: /path/does/not/exist.xml");
    }
    // Invalid ResourcePolicyProvider class name.
    config = new AuthorizationConfig("server1", AUTHZ_POLICY_FILE, "", "ClassDoesNotExist");
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (IllegalArgumentException e) {
        Assert.assertEquals(e.getMessage(), "The authorization policy provider class 'ClassDoesNotExist' was not found.");
    }
    // Valid class name, but class is not derived from ResourcePolicyProvider
    config = new AuthorizationConfig("server1", AUTHZ_POLICY_FILE, "", this.getClass().getName());
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (IllegalArgumentException e) {
        Assert.assertEquals(e.getMessage(), String.format("The authorization policy " + "provider class '%s' must be a subclass of '%s'.", this.getClass().getName(), ResourceAuthorizationProvider.class.getName()));
    }
    // Config validations skipped if authorization disabled
    config = new AuthorizationConfig("", "", "", "");
    Assert.assertFalse(config.isEnabled());
    config = new AuthorizationConfig(null, "", "", null);
    Assert.assertFalse(config.isEnabled());
    config = new AuthorizationConfig("", null, "", "");
    Assert.assertFalse(config.isEnabled());
    config = new AuthorizationConfig(null, null, null, null);
    Assert.assertFalse(config.isEnabled());
}
#method_after
@Test
public void TestConfigValidation() throws InternalException {
    String sentryConfig = ctx_.authzConfig.getSentryConfig().getConfigFile();
    // Valid configs pass validation.
    AuthorizationConfig config = AuthorizationConfig.createHadoopGroupAuthConfig("server1", AUTHZ_POLICY_FILE, sentryConfig);
    config.validateConfig();
    Assert.assertTrue(config.isEnabled());
    Assert.assertTrue(config.isFileBasedPolicy());
    config = AuthorizationConfig.createHadoopGroupAuthConfig("server1", null, sentryConfig);
    config.validateConfig();
    Assert.assertTrue(config.isEnabled());
    Assert.assertTrue(!config.isFileBasedPolicy());
    // Invalid configs
    // No sentry configuration file.
    config = AuthorizationConfig.createHadoopGroupAuthConfig("server1", AUTHZ_POLICY_FILE, null);
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
    } catch (Exception e) {
        Assert.assertEquals(e.getMessage(), "A valid path to a sentry-site.xml config " + "file must be set using --sentry_config to enable authorization.");
    }
    // Empty / null server name.
    config = AuthorizationConfig.createHadoopGroupAuthConfig("", AUTHZ_POLICY_FILE, sentryConfig);
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (IllegalArgumentException e) {
        Assert.assertEquals(e.getMessage(), "Authorization is enabled but the server name is null or empty. Set the " + "server name using the impalad --server_name flag.");
    }
    config = AuthorizationConfig.createHadoopGroupAuthConfig(null, AUTHZ_POLICY_FILE, sentryConfig);
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (IllegalArgumentException e) {
        Assert.assertEquals(e.getMessage(), "Authorization is enabled but the server name is null or empty. Set the " + "server name using the impalad --server_name flag.");
    }
    // Sentry config file does not exist.
    config = AuthorizationConfig.createHadoopGroupAuthConfig("server1", "", "/path/does/not/exist.xml");
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (Exception e) {
        Assert.assertEquals(e.getMessage(), "Sentry configuration file does not exist: \"/path/does/not/exist.xml\"");
    }
    // Invalid ResourcePolicyProvider class name.
    config = new AuthorizationConfig("server1", AUTHZ_POLICY_FILE, "", "ClassDoesNotExist");
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (IllegalArgumentException e) {
        Assert.assertEquals(e.getMessage(), "The authorization policy provider class 'ClassDoesNotExist' was not found.");
    }
    // Valid class name, but class is not derived from ResourcePolicyProvider
    config = new AuthorizationConfig("server1", AUTHZ_POLICY_FILE, "", this.getClass().getName());
    Assert.assertTrue(config.isEnabled());
    try {
        config.validateConfig();
        fail("Expected configuration to fail.");
    } catch (IllegalArgumentException e) {
        Assert.assertEquals(e.getMessage(), String.format("The authorization policy " + "provider class '%s' must be a subclass of '%s'.", this.getClass().getName(), ResourceAuthorizationProvider.class.getName()));
    }
    // Config validations skipped if authorization disabled
    config = new AuthorizationConfig("", "", "", "");
    Assert.assertFalse(config.isEnabled());
    config = new AuthorizationConfig(null, "", "", null);
    Assert.assertFalse(config.isEnabled());
    config = new AuthorizationConfig("", null, "", "");
    Assert.assertFalse(config.isEnabled());
    config = new AuthorizationConfig(null, null, null, null);
    Assert.assertFalse(config.isEnabled());
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        // Subqueries need to be rewritten by the StmtRewriter first.
        if (analyzer.containsSubquery())
            return;
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the partition clause to the create statement.
    if (partitionKeys_ != null) {
        int colCnt = tmpQueryStmt.getColLabels().size();
        int partColCnt = partitionKeys_.size();
        if (partColCnt >= colCnt) {
            throw new AnalysisException(String.format("Number of partition columns (%s) " + "must be smaller than the number of columns in the select statement (%s).", partColCnt, colCnt));
        }
        int firstCol = colCnt - partColCnt;
        for (int i = firstCol, j = 0; i < colCnt; ++i, ++j) {
            String partitionLabel = partitionKeys_.get(j);
            String colLabel = tmpQueryStmt.getColLabels().get(i);
            // input column list.
            if (!partitionLabel.equals(colLabel)) {
                throw new AnalysisException(String.format("Partition column name " + "mismatch: %s != %s", partitionLabel, colLabel));
            }
            ColumnDef colDef = new ColumnDef(colLabel, null, null);
            colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
            createStmt_.getPartitionColumnDefs().add(colDef);
        }
        // Remove partition columns from table column list.
        tmpQueryStmt.getColLabels().subList(firstCol, colCnt).clear();
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    createStmt_.getColumnDefs().clear();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, null);
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE"));
    }
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient();
    try {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        // Create a "temp" table based off the given metastore.api.Table object. Normally,
        // the CatalogService assigns all table IDs, but in this case we need to assign the
        // "temp" table an ID locally. This table ID cannot conflict with any table in the
        // SelectStmt (or the BE will be very confused). To ensure the ID is unique within
        // this query, just assign it the invalid table ID. The CatalogServer will assign
        // this table a proper ID once it is created there as part of the CTAS execution.
        Table table = Table.fromMetastoreTable(TableId.createInvalidId(), db, msTbl);
        Preconditions.checkState(table != null && (table instanceof HdfsTable || table instanceof KuduTable));
        table.load(true, client.getHiveClient(), msTbl);
        insertStmt_.setTargetTable(table);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    } finally {
        client.release();
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed())
        return;
    super.analyze(analyzer);
    // The analysis for CTAS happens in two phases - the first phase happens before
    // the target table exists and we want to validate the CREATE statement and the
    // query portion of the insert statement. If this passes, analysis will be run
    // over the full INSERT statement. To avoid duplicate registrations of table/colRefs,
    // create a new root analyzer and clone the query statement for this initial pass.
    Analyzer dummyRootAnalyzer = new Analyzer(analyzer.getCatalog(), analyzer.getQueryCtx(), analyzer.getAuthzConfig());
    QueryStmt tmpQueryStmt = insertStmt_.getQueryStmt().clone();
    try {
        Analyzer tmpAnalyzer = new Analyzer(dummyRootAnalyzer);
        tmpAnalyzer.setUseHiveColLabels(true);
        tmpQueryStmt.analyze(tmpAnalyzer);
        // Subqueries need to be rewritten by the StmtRewriter first.
        if (analyzer.containsSubquery())
            return;
    } finally {
        // Record missing tables in the original analyzer.
        analyzer.getMissingTbls().addAll(dummyRootAnalyzer.getMissingTbls());
    }
    // Add the columns from the partition clause to the create statement.
    if (partitionKeys_ != null) {
        int colCnt = tmpQueryStmt.getColLabels().size();
        int partColCnt = partitionKeys_.size();
        if (partColCnt >= colCnt) {
            throw new AnalysisException(String.format("Number of partition columns (%s) " + "must be smaller than the number of columns in the select statement (%s).", partColCnt, colCnt));
        }
        int firstCol = colCnt - partColCnt;
        for (int i = firstCol, j = 0; i < colCnt; ++i, ++j) {
            String partitionLabel = partitionKeys_.get(j);
            String colLabel = tmpQueryStmt.getColLabels().get(i);
            // input column list.
            if (!partitionLabel.equals(colLabel)) {
                throw new AnalysisException(String.format("Partition column name " + "mismatch: %s != %s", partitionLabel, colLabel));
            }
            ColumnDef colDef = new ColumnDef(colLabel, null, null);
            colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
            createStmt_.getPartitionColumnDefs().add(colDef);
        }
        // Remove partition columns from table column list.
        tmpQueryStmt.getColLabels().subList(firstCol, colCnt).clear();
    }
    // Add the columns from the select statement to the create statement.
    int colCnt = tmpQueryStmt.getColLabels().size();
    createStmt_.getColumnDefs().clear();
    for (int i = 0; i < colCnt; ++i) {
        ColumnDef colDef = new ColumnDef(tmpQueryStmt.getColLabels().get(i), null, null);
        colDef.setType(tmpQueryStmt.getBaseTblResultExprs().get(i).getType());
        createStmt_.getColumnDefs().add(colDef);
    }
    createStmt_.analyze(analyzer);
    if (!SUPPORTED_INSERT_FORMATS.contains(createStmt_.getFileFormat())) {
        throw new AnalysisException(String.format("CREATE TABLE AS SELECT " + "does not support (%s) file format. Supported formats are: (%s)", createStmt_.getFileFormat().toString().replace("_", ""), "PARQUET, TEXTFILE"));
    }
    // The full privilege check for the database will be done as part of the INSERT
    // analysis.
    Db db = analyzer.getDb(createStmt_.getDb(), Privilege.ANY);
    if (db == null) {
        throw new AnalysisException(Analyzer.DB_DOES_NOT_EXIST_ERROR_MSG + createStmt_.getDb());
    }
    // Running analysis on the INSERT portion of the CTAS requires the target INSERT
    // table to "exist". For CTAS the table does not exist yet, so create a "temp"
    // table to run analysis against. The schema of this temp table should exactly
    // match the schema of the table that will be created by running the CREATE
    // statement.
    org.apache.hadoop.hive.metastore.api.Table msTbl = CatalogOpExecutor.createMetaStoreTable(createStmt_.toThrift());
    try (MetaStoreClient client = analyzer.getCatalog().getMetaStoreClient()) {
        // Set a valid location of this table using the same rules as the metastore. If the
        // user specified a location for the table this will be a no-op.
        msTbl.getSd().setLocation(analyzer.getCatalog().getTablePath(msTbl).toString());
        // Create a "temp" table based off the given metastore.api.Table object. Normally,
        // the CatalogService assigns all table IDs, but in this case we need to assign the
        // "temp" table an ID locally. This table ID cannot conflict with any table in the
        // SelectStmt (or the BE will be very confused). To ensure the ID is unique within
        // this query, just assign it the invalid table ID. The CatalogServer will assign
        // this table a proper ID once it is created there as part of the CTAS execution.
        Table table = Table.fromMetastoreTable(TableId.createInvalidId(), db, msTbl);
        Preconditions.checkState(table != null && (table instanceof HdfsTable || table instanceof KuduTable));
        table.load(true, client.getHiveClient(), msTbl);
        insertStmt_.setTargetTable(table);
    } catch (TableLoadingException e) {
        throw new AnalysisException(e.getMessage(), e);
    } catch (Exception e) {
        throw new AnalysisException(e.getMessage(), e);
    }
    // Finally, run analysis on the insert statement.
    insertStmt_.analyze(analyzer);
}
#end_block

#method_before
@Override
public /**
 * For hbase tables, we can support tables with columns we don't understand at
 * all (e.g. map) as long as the user does not select those. This is in contrast
 * to hdfs tables since we typically need to understand all columns to make sense
 * of the file at all.
 */
void load(boolean reuseMetadata, HiveMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    Preconditions.checkNotNull(getMetaStoreTable());
    try {
        msTable_ = msTbl;
        hbaseTableName_ = getHBaseTableName(getMetaStoreTable());
        // Warm up the connection and verify the table exists.
        getHBaseTable().close();
        columnFamilies_ = null;
        Map<String, String> serdeParams = getMetaStoreTable().getSd().getSerdeInfo().getParameters();
        String hbaseColumnsMapping = serdeParams.get(HBaseSerDe.HBASE_COLUMNS_MAPPING);
        if (hbaseColumnsMapping == null) {
            throw new MetaException("No hbase.columns.mapping defined in Serde.");
        }
        String hbaseTableDefaultStorageType = getMetaStoreTable().getParameters().get(HBaseSerDe.HBASE_TABLE_DEFAULT_STORAGE_TYPE);
        boolean tableDefaultStorageIsBinary = false;
        if (hbaseTableDefaultStorageType != null && !hbaseTableDefaultStorageType.isEmpty()) {
            if (hbaseTableDefaultStorageType.equalsIgnoreCase("binary")) {
                tableDefaultStorageIsBinary = true;
            } else if (!hbaseTableDefaultStorageType.equalsIgnoreCase("string")) {
                throw new SerDeException("Error: " + HBaseSerDe.HBASE_TABLE_DEFAULT_STORAGE_TYPE + " parameter must be specified as" + " 'string' or 'binary'; '" + hbaseTableDefaultStorageType + "' is not a valid specification for this table/serde property.");
            }
        }
        // Parse HBase column-mapping string.
        List<FieldSchema> fieldSchemas = getMetaStoreTable().getSd().getCols();
        List<String> hbaseColumnFamilies = new ArrayList<String>();
        List<String> hbaseColumnQualifiers = new ArrayList<String>();
        List<Boolean> hbaseColumnBinaryEncodings = new ArrayList<Boolean>();
        parseColumnMapping(tableDefaultStorageIsBinary, hbaseColumnsMapping, fieldSchemas, hbaseColumnFamilies, hbaseColumnQualifiers, hbaseColumnBinaryEncodings);
        Preconditions.checkState(hbaseColumnFamilies.size() == hbaseColumnQualifiers.size());
        Preconditions.checkState(fieldSchemas.size() == hbaseColumnFamilies.size());
        // Populate tmp cols in the order they appear in the Hive metastore.
        // We will reorder the cols below.
        List<HBaseColumn> tmpCols = Lists.newArrayList();
        // Store the key column separately.
        // TODO: Change this to an ArrayList once we support composite row keys.
        HBaseColumn keyCol = null;
        for (int i = 0; i < fieldSchemas.size(); ++i) {
            FieldSchema s = fieldSchemas.get(i);
            Type t = Type.INVALID;
            try {
                t = parseColumnType(s);
            } catch (TableLoadingException e) {
            // Ignore hbase types we don't support yet. We can load the metadata
            // but won't be able to select from it.
            }
            HBaseColumn col = new HBaseColumn(s.getName(), hbaseColumnFamilies.get(i), hbaseColumnQualifiers.get(i), hbaseColumnBinaryEncodings.get(i), t, s.getComment(), -1);
            if (col.getColumnFamily().equals(ROW_KEY_COLUMN_FAMILY)) {
                // Store the row key column separately from the rest
                keyCol = col;
            } else {
                tmpCols.add(col);
            }
        }
        Preconditions.checkState(keyCol != null);
        // The backend assumes that the row key column is always first and
        // that the remaining HBase columns are ordered by columnFamily,columnQualifier,
        // so the final position depends on the other mapped HBase columns.
        // Sort columns and update positions.
        Collections.sort(tmpCols);
        clearColumns();
        keyCol.setPosition(0);
        addColumn(keyCol);
        // Update the positions of the remaining columns
        for (int i = 0; i < tmpCols.size(); ++i) {
            HBaseColumn col = tmpCols.get(i);
            col.setPosition(i + 1);
            addColumn(col);
        }
        // Set table stats.
        numRows_ = getRowCount(super.getMetaStoreTable().getParameters());
        // since we don't support composite hbase rowkeys yet, all hbase tables have a
        // single clustering col
        numClusteringCols_ = 1;
        loadAllColumnStats(client);
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for HBase table: " + name_, e);
    }
}
#method_after
@Override
public /**
 * For hbase tables, we can support tables with columns we don't understand at
 * all (e.g. map) as long as the user does not select those. This is in contrast
 * to hdfs tables since we typically need to understand all columns to make sense
 * of the file at all.
 */
void load(boolean reuseMetadata, IMetaStoreClient client, org.apache.hadoop.hive.metastore.api.Table msTbl) throws TableLoadingException {
    Preconditions.checkNotNull(getMetaStoreTable());
    try {
        msTable_ = msTbl;
        hbaseTableName_ = getHBaseTableName(getMetaStoreTable());
        // Warm up the connection and verify the table exists.
        getHBaseTable().close();
        columnFamilies_ = null;
        Map<String, String> serdeParams = getMetaStoreTable().getSd().getSerdeInfo().getParameters();
        String hbaseColumnsMapping = serdeParams.get(HBaseSerDe.HBASE_COLUMNS_MAPPING);
        if (hbaseColumnsMapping == null) {
            throw new MetaException("No hbase.columns.mapping defined in Serde.");
        }
        String hbaseTableDefaultStorageType = getMetaStoreTable().getParameters().get(HBaseSerDe.HBASE_TABLE_DEFAULT_STORAGE_TYPE);
        boolean tableDefaultStorageIsBinary = false;
        if (hbaseTableDefaultStorageType != null && !hbaseTableDefaultStorageType.isEmpty()) {
            if (hbaseTableDefaultStorageType.equalsIgnoreCase("binary")) {
                tableDefaultStorageIsBinary = true;
            } else if (!hbaseTableDefaultStorageType.equalsIgnoreCase("string")) {
                throw new SerDeException("Error: " + HBaseSerDe.HBASE_TABLE_DEFAULT_STORAGE_TYPE + " parameter must be specified as" + " 'string' or 'binary'; '" + hbaseTableDefaultStorageType + "' is not a valid specification for this table/serde property.");
            }
        }
        // Parse HBase column-mapping string.
        List<FieldSchema> fieldSchemas = getMetaStoreTable().getSd().getCols();
        List<String> hbaseColumnFamilies = new ArrayList<String>();
        List<String> hbaseColumnQualifiers = new ArrayList<String>();
        List<Boolean> hbaseColumnBinaryEncodings = new ArrayList<Boolean>();
        parseColumnMapping(tableDefaultStorageIsBinary, hbaseColumnsMapping, fieldSchemas, hbaseColumnFamilies, hbaseColumnQualifiers, hbaseColumnBinaryEncodings);
        Preconditions.checkState(hbaseColumnFamilies.size() == hbaseColumnQualifiers.size());
        Preconditions.checkState(fieldSchemas.size() == hbaseColumnFamilies.size());
        // Populate tmp cols in the order they appear in the Hive metastore.
        // We will reorder the cols below.
        List<HBaseColumn> tmpCols = Lists.newArrayList();
        // Store the key column separately.
        // TODO: Change this to an ArrayList once we support composite row keys.
        HBaseColumn keyCol = null;
        for (int i = 0; i < fieldSchemas.size(); ++i) {
            FieldSchema s = fieldSchemas.get(i);
            Type t = Type.INVALID;
            try {
                t = parseColumnType(s);
            } catch (TableLoadingException e) {
            // Ignore hbase types we don't support yet. We can load the metadata
            // but won't be able to select from it.
            }
            HBaseColumn col = new HBaseColumn(s.getName(), hbaseColumnFamilies.get(i), hbaseColumnQualifiers.get(i), hbaseColumnBinaryEncodings.get(i), t, s.getComment(), -1);
            if (col.getColumnFamily().equals(ROW_KEY_COLUMN_FAMILY)) {
                // Store the row key column separately from the rest
                keyCol = col;
            } else {
                tmpCols.add(col);
            }
        }
        Preconditions.checkState(keyCol != null);
        // The backend assumes that the row key column is always first and
        // that the remaining HBase columns are ordered by columnFamily,columnQualifier,
        // so the final position depends on the other mapped HBase columns.
        // Sort columns and update positions.
        Collections.sort(tmpCols);
        clearColumns();
        keyCol.setPosition(0);
        addColumn(keyCol);
        // Update the positions of the remaining columns
        for (int i = 0; i < tmpCols.size(); ++i) {
            HBaseColumn col = tmpCols.get(i);
            col.setPosition(i + 1);
            addColumn(col);
        }
        // Set table stats.
        numRows_ = getRowCount(super.getMetaStoreTable().getParameters());
        // since we don't support composite hbase rowkeys yet, all hbase tables have a
        // single clustering col
        numClusteringCols_ = 1;
        loadAllColumnStats(client);
    } catch (Exception e) {
        throw new TableLoadingException("Failed to load metadata for HBase table: " + name_, e);
    }
}
#end_block

#method_before
private static KuduPredicate intInList(int... values) {
    KuduPredicate.InListBuilder builder = KuduPredicate.newInListBuilder(intCol);
    for (int value : values) {
        builder.addValue(value);
    }
    return builder.build();
}
#method_after
private static KuduPredicate intInList(Integer... values) {
    return KuduPredicate.newInListPredicate(intCol, Arrays.asList(values));
}
#end_block

#method_before
private static KuduPredicate boolInList(boolean... values) {
    KuduPredicate.InListBuilder builder = KuduPredicate.newInListBuilder(boolCol);
    for (boolean value : values) {
        builder.addValue(value);
    }
    return builder.build();
}
#method_after
private static KuduPredicate boolInList(Boolean... values) {
    return KuduPredicate.newInListPredicate(boolCol, Arrays.asList(values));
}
#end_block

#method_before
private static KuduPredicate stringInList(String... values) {
    KuduPredicate.InListBuilder builder = KuduPredicate.newInListBuilder(stringCol);
    for (String value : values) {
        builder.addValue(value);
    }
    return builder.build();
}
#method_after
private static KuduPredicate stringInList(String... values) {
    return KuduPredicate.newInListPredicate(stringCol, Arrays.asList(values));
}
#end_block

#method_before
@Test
public void testAllTypesMerge() {
    testMerge(KuduPredicate.newComparisonPredicate(boolCol, GREATER_EQUAL, false), KuduPredicate.newComparisonPredicate(boolCol, LESS, true), new KuduPredicate(KuduPredicate.PredicateType.EQUALITY, boolCol, Bytes.fromBoolean(false), null));
    testMerge(KuduPredicate.newComparisonPredicate(boolCol, GREATER_EQUAL, false), KuduPredicate.newComparisonPredicate(boolCol, LESS_EQUAL, true), KuduPredicate.newIsNotNullPredicate(boolCol));
    testMerge(KuduPredicate.newComparisonPredicate(byteCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(byteCol, LESS, 10), new KuduPredicate(RANGE, byteCol, new byte[] { (byte) 0 }, new byte[] { (byte) 10 }));
    testMerge(KuduPredicate.newComparisonPredicate(shortCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(shortCol, LESS, 10), new KuduPredicate(RANGE, shortCol, Bytes.fromShort((short) 0), Bytes.fromShort((short) 10)));
    testMerge(KuduPredicate.newComparisonPredicate(longCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(longCol, LESS, 10), new KuduPredicate(RANGE, longCol, Bytes.fromLong(0), Bytes.fromLong(10)));
    testMerge(KuduPredicate.newComparisonPredicate(floatCol, GREATER_EQUAL, 123.45f), KuduPredicate.newComparisonPredicate(floatCol, LESS, 678.90f), new KuduPredicate(RANGE, floatCol, Bytes.fromFloat(123.45f), Bytes.fromFloat(678.90f)));
    testMerge(KuduPredicate.newComparisonPredicate(doubleCol, GREATER_EQUAL, 123.45), KuduPredicate.newComparisonPredicate(doubleCol, LESS, 678.90), new KuduPredicate(RANGE, doubleCol, Bytes.fromDouble(123.45), Bytes.fromDouble(678.90)));
    testMerge(KuduPredicate.newComparisonPredicate(binaryCol, GREATER_EQUAL, new byte[] { 0, 1, 2, 3, 4, 5, 6 }), KuduPredicate.newComparisonPredicate(binaryCol, LESS, new byte[] { 10 }), new KuduPredicate(RANGE, binaryCol, new byte[] { 0, 1, 2, 3, 4, 5, 6 }, new byte[] { 10 }));
}
#method_after
@Test
public void testAllTypesMerge() {
    testMerge(KuduPredicate.newComparisonPredicate(boolCol, GREATER_EQUAL, false), KuduPredicate.newComparisonPredicate(boolCol, LESS, true), new KuduPredicate(KuduPredicate.PredicateType.EQUALITY, boolCol, Bytes.fromBoolean(false), null));
    testMerge(KuduPredicate.newComparisonPredicate(boolCol, GREATER_EQUAL, false), KuduPredicate.newComparisonPredicate(boolCol, LESS_EQUAL, true), KuduPredicate.newIsNotNullPredicate(boolCol));
    testMerge(KuduPredicate.newComparisonPredicate(byteCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(byteCol, LESS, 10), new KuduPredicate(RANGE, byteCol, new byte[] { (byte) 0 }, new byte[] { (byte) 10 }));
    testMerge(KuduPredicate.newInListPredicate(byteCol, ImmutableList.of((byte) 12, (byte) 14, (byte) 16, (byte) 18)), KuduPredicate.newInListPredicate(byteCol, ImmutableList.of((byte) 14, (byte) 18, (byte) 20)), KuduPredicate.newInListPredicate(byteCol, ImmutableList.of((byte) 14, (byte) 18)));
    testMerge(KuduPredicate.newComparisonPredicate(shortCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(shortCol, LESS, 10), new KuduPredicate(RANGE, shortCol, Bytes.fromShort((short) 0), Bytes.fromShort((short) 10)));
    testMerge(KuduPredicate.newInListPredicate(shortCol, ImmutableList.of((short) 12, (short) 14, (short) 16, (short) 18)), KuduPredicate.newInListPredicate(shortCol, ImmutableList.of((short) 14, (short) 18, (short) 20)), KuduPredicate.newInListPredicate(shortCol, ImmutableList.of((short) 14, (short) 18)));
    testMerge(KuduPredicate.newComparisonPredicate(longCol, GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(longCol, LESS, 10), new KuduPredicate(RANGE, longCol, Bytes.fromLong(0), Bytes.fromLong(10)));
    testMerge(KuduPredicate.newInListPredicate(longCol, ImmutableList.of(12L, 14L, 16L, 18L)), KuduPredicate.newInListPredicate(longCol, ImmutableList.of(14L, 18L, 20L)), KuduPredicate.newInListPredicate(longCol, ImmutableList.of(14L, 18L)));
    testMerge(KuduPredicate.newComparisonPredicate(floatCol, GREATER_EQUAL, 123.45f), KuduPredicate.newComparisonPredicate(floatCol, LESS, 678.90f), new KuduPredicate(RANGE, floatCol, Bytes.fromFloat(123.45f), Bytes.fromFloat(678.90f)));
    testMerge(KuduPredicate.newInListPredicate(floatCol, ImmutableList.of(12f, 14f, 16f, 18f)), KuduPredicate.newInListPredicate(floatCol, ImmutableList.of(14f, 18f, 20f)), KuduPredicate.newInListPredicate(floatCol, ImmutableList.of(14f, 18f)));
    testMerge(KuduPredicate.newComparisonPredicate(doubleCol, GREATER_EQUAL, 123.45), KuduPredicate.newComparisonPredicate(doubleCol, LESS, 678.90), new KuduPredicate(RANGE, doubleCol, Bytes.fromDouble(123.45), Bytes.fromDouble(678.90)));
    testMerge(KuduPredicate.newInListPredicate(doubleCol, ImmutableList.of(12d, 14d, 16d, 18d)), KuduPredicate.newInListPredicate(doubleCol, ImmutableList.of(14d, 18d, 20d)), KuduPredicate.newInListPredicate(doubleCol, ImmutableList.of(14d, 18d)));
    testMerge(KuduPredicate.newComparisonPredicate(binaryCol, GREATER_EQUAL, new byte[] { 0, 1, 2, 3, 4, 5, 6 }), KuduPredicate.newComparisonPredicate(binaryCol, LESS, new byte[] { 10 }), new KuduPredicate(RANGE, binaryCol, new byte[] { 0, 1, 2, 3, 4, 5, 6 }, new byte[] { 10 }));
    testMerge(KuduPredicate.newInListPredicate(binaryCol, ImmutableList.of("a".getBytes(), "b".getBytes(), "c".getBytes(), "d".getBytes())), KuduPredicate.newInListPredicate(binaryCol, ImmutableList.of("b".getBytes(), "d".getBytes(), "e".getBytes())), KuduPredicate.newInListPredicate(binaryCol, ImmutableList.of("b".getBytes(), "d".getBytes())));
}
#end_block

#method_before
@Test
public void testToString() {
    Assert.assertEquals("`bool` = true", KuduPredicate.newComparisonPredicate(boolCol, EQUAL, true).toString());
    Assert.assertEquals("`byte` = 11", KuduPredicate.newComparisonPredicate(byteCol, EQUAL, 11).toString());
    Assert.assertEquals("`short` = 11", KuduPredicate.newComparisonPredicate(shortCol, EQUAL, 11).toString());
    Assert.assertEquals("`int` = -123", KuduPredicate.newComparisonPredicate(intCol, EQUAL, -123).toString());
    Assert.assertEquals("`long` = 5454", KuduPredicate.newComparisonPredicate(longCol, EQUAL, 5454).toString());
    Assert.assertEquals("`float` = 123.456", KuduPredicate.newComparisonPredicate(floatCol, EQUAL, 123.456f).toString());
    Assert.assertEquals("`double` = 123.456", KuduPredicate.newComparisonPredicate(doubleCol, EQUAL, 123.456).toString());
    Assert.assertEquals("`string` = \"my string\"", KuduPredicate.newComparisonPredicate(stringCol, EQUAL, "my string").toString());
    Assert.assertEquals("`binary` = 0xAB01CD", KuduPredicate.newComparisonPredicate(binaryCol, EQUAL, new byte[] { (byte) 0xAB, (byte) 0x01, (byte) 0xCD }).toString());
    Assert.assertEquals("`int` IN (-10, 0, 10)", intInList(10, 0, -10).toString());
    Assert.assertEquals("`string` IS NOT NULL", KuduPredicate.newIsNotNullPredicate(stringCol).toString());
}
#method_after
@Test
public void testToString() {
    Assert.assertEquals("`bool` = true", KuduPredicate.newComparisonPredicate(boolCol, EQUAL, true).toString());
    Assert.assertEquals("`byte` = 11", KuduPredicate.newComparisonPredicate(byteCol, EQUAL, 11).toString());
    Assert.assertEquals("`short` = 11", KuduPredicate.newComparisonPredicate(shortCol, EQUAL, 11).toString());
    Assert.assertEquals("`int` = -123", KuduPredicate.newComparisonPredicate(intCol, EQUAL, -123).toString());
    Assert.assertEquals("`long` = 5454", KuduPredicate.newComparisonPredicate(longCol, EQUAL, 5454).toString());
    Assert.assertEquals("`float` = 123.456", KuduPredicate.newComparisonPredicate(floatCol, EQUAL, 123.456f).toString());
    Assert.assertEquals("`double` = 123.456", KuduPredicate.newComparisonPredicate(doubleCol, EQUAL, 123.456).toString());
    Assert.assertEquals("`string` = \"my string\"", KuduPredicate.newComparisonPredicate(stringCol, EQUAL, "my string").toString());
    Assert.assertEquals("`binary` = 0xAB01CD", KuduPredicate.newComparisonPredicate(binaryCol, EQUAL, new byte[] { (byte) 0xAB, (byte) 0x01, (byte) 0xCD }).toString());
    Assert.assertEquals("`int` IN (-10, 0, 10)", intInList(10, 0, -10).toString());
    Assert.assertEquals("`string` IS NOT NULL", KuduPredicate.newIsNotNullPredicate(stringCol).toString());
    Assert.assertEquals("`bool` = true", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(true)).toString());
    Assert.assertEquals("`bool` = false", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(false)).toString());
    Assert.assertEquals("`bool` IS NOT NULL", KuduPredicate.newInListPredicate(boolCol, ImmutableList.of(false, true, true)).toString());
    Assert.assertEquals("`byte` IN (1, 10, 100)", KuduPredicate.newInListPredicate(byteCol, ImmutableList.of((byte) 1, (byte) 10, (byte) 100)).toString());
    Assert.assertEquals("`short` IN (1, 10, 100)", KuduPredicate.newInListPredicate(shortCol, ImmutableList.of((short) 1, (short) 100, (short) 10)).toString());
    Assert.assertEquals("`int` IN (1, 10, 100)", KuduPredicate.newInListPredicate(intCol, ImmutableList.of(1, 100, 10)).toString());
    Assert.assertEquals("`long` IN (1, 10, 100)", KuduPredicate.newInListPredicate(longCol, ImmutableList.of(1L, 100L, 10L)).toString());
    Assert.assertEquals("`float` IN (78.9, 123.456)", KuduPredicate.newInListPredicate(floatCol, ImmutableList.of(123.456f, 78.9f)).toString());
    Assert.assertEquals("`double` IN (78.9, 123.456)", KuduPredicate.newInListPredicate(doubleCol, ImmutableList.of(123.456d, 78.9d)).toString());
    Assert.assertEquals("`string` IN (\"a\", \"my string\")", KuduPredicate.newInListPredicate(stringCol, ImmutableList.of("my string", "a")).toString());
    Assert.assertEquals("`binary` IN (0x00, 0xAB01CD)", KuduPredicate.newInListPredicate(binaryCol, ImmutableList.of(new byte[] { (byte) 0xAB, (byte) 0x01, (byte) 0xCD }, new byte[] { (byte) 0x00 })).toString());
}
#end_block

#method_before
@Test
public void testScanWithPredicates() throws Exception {
    Schema schema = createManyStringsSchema();
    syncClient.createTable(tableName, schema, createTableOptions());
    KuduSession session = syncClient.newSession();
    session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_BACKGROUND);
    KuduTable table = syncClient.openTable(tableName);
    for (int i = 0; i < 100; i++) {
        Insert insert = table.newInsert();
        PartialRow row = insert.getRow();
        row.addString("key", String.format("key_%02d", i));
        row.addString("c1", "c1_" + i);
        row.addString("c2", "c2_" + i);
        session.apply(insert);
    }
    session.flush();
    assertEquals(100, scanTableToStrings(table).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER_EQUAL, "key_50")).size());
    assertEquals(25, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_74")).size());
    assertEquals(25, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_24"), KuduPredicate.newComparisonPredicate(schema.getColumn("c1"), LESS_EQUAL, "c1_49")).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_24"), KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER_EQUAL, "key_50")).size());
    assertEquals(0, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("c1"), GREATER, "c1_30"), KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), LESS, "c2_20")).size());
    assertEquals(0, scanTableToStrings(table, // Short circuit scan
    KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), GREATER, "c2_30"), KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), LESS, "c2_20")).size());
    // IN list
    assertEquals(3, scanTableToStrings(table, KuduPredicate.newInListBuilder(schema.getColumn("key")).addValue("key_30").addValue("key_01").addValue("invalid").addValue("key_99").build()).size());
    assertEquals(3, scanTableToStrings(table, KuduPredicate.newInListBuilder(schema.getColumn("c2")).addValue("c2_30").addValue("c2_1").addValue("invalid").addValue("c2_99").build()).size());
    assertEquals(2, scanTableToStrings(table, KuduPredicate.newInListBuilder(schema.getColumn("c2")).addValue("c2_30").addValue("c2_1").addValue("invalid").addValue("c2_99").build(), KuduPredicate.newIsNotNullPredicate(schema.getColumn("c2")), KuduPredicate.newInListBuilder(schema.getColumn("key")).addValue("key_30").addValue("key_45").addValue("invalid").addValue("key_99").build()).size());
}
#method_after
@Test
public void testScanWithPredicates() throws Exception {
    Schema schema = createManyStringsSchema();
    syncClient.createTable(tableName, schema, createTableOptions());
    KuduSession session = syncClient.newSession();
    session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_BACKGROUND);
    KuduTable table = syncClient.openTable(tableName);
    for (int i = 0; i < 100; i++) {
        Insert insert = table.newInsert();
        PartialRow row = insert.getRow();
        row.addString("key", String.format("key_%02d", i));
        row.addString("c1", "c1_" + i);
        row.addString("c2", "c2_" + i);
        session.apply(insert);
    }
    session.flush();
    assertEquals(100, scanTableToStrings(table).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER_EQUAL, "key_50")).size());
    assertEquals(25, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_74")).size());
    assertEquals(25, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_24"), KuduPredicate.newComparisonPredicate(schema.getColumn("c1"), LESS_EQUAL, "c1_49")).size());
    assertEquals(50, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER, "key_24"), KuduPredicate.newComparisonPredicate(schema.getColumn("key"), GREATER_EQUAL, "key_50")).size());
    assertEquals(0, scanTableToStrings(table, KuduPredicate.newComparisonPredicate(schema.getColumn("c1"), GREATER, "c1_30"), KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), LESS, "c2_20")).size());
    assertEquals(0, scanTableToStrings(table, // Short circuit scan
    KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), GREATER, "c2_30"), KuduPredicate.newComparisonPredicate(schema.getColumn("c2"), LESS, "c2_20")).size());
    // IS NOT NULL
    assertEquals(100, scanTableToStrings(table, KuduPredicate.newIsNotNullPredicate(schema.getColumn("c2")), KuduPredicate.newIsNotNullPredicate(schema.getColumn("key"))).size());
    // IN list
    assertEquals(3, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("key"), ImmutableList.of("key_30", "key_01", "invalid", "key_99"))).size());
    assertEquals(3, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("c2"), ImmutableList.of("c2_30", "c2_1", "invalid", "c2_99"))).size());
    assertEquals(2, scanTableToStrings(table, KuduPredicate.newInListPredicate(schema.getColumn("c2"), ImmutableList.of("c2_30", "c2_1", "invalid", "c2_99")), KuduPredicate.newIsNotNullPredicate(schema.getColumn("c2")), KuduPredicate.newInListPredicate(schema.getColumn("key"), ImmutableList.of("key_30", "key_45", "invalid", "key_99"))).size());
}
#end_block

#method_before
@Test
public void testRangePartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b STRING, c INT8)
    // PRIMARY KEY (a, b, c))
    // DISTRIBUTE BY RANGE(c, b);
    // PARTITION BY RANGE (a, b, c)
    // (PARTITION              VALUES < (0, "m"),
    // PARTITION  (0, "m") <= VALUES < (10, "r")
    // PARTITION (10, "r") <= VALUES);
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.STRING).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("c", "b"));
    PartialRow split = schema.newPartialRow();
    split.addByte("c", (byte) 0);
    split.addString("b", "m");
    tableBuilder.addSplitRow(split);
    split.addByte("c", (byte) 10);
    split.addString("b", "r");
    tableBuilder.addSplitRow(split);
    String tableName = "testRangePartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(3, countPartitions(table, partitions));
    // c < -10
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, -10)));
    // c = -10
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, -10)));
    // c < 10
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 10)));
    // c < 100
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 100)));
    // c >= -10
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10)));
    // c >= 0
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10)));
    // c >= 5
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 5)));
    // c >= 10
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10)));
    // c >= 100
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 100)));
    // c >= -10
    // c < 0
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10), KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 0)));
    // c >= 5
    // c < 100
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 100)));
    // b = ""
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, "")));
    // b >= "z"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "z")));
    // b < "a"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "a")));
    // b >= "m"
    // b < "z"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "m"), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "z")));
    // c >= 10
    // b >= "r"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "r")));
    // c >= 10
    // b < "r"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "r")));
    // c = 10
    // b < "r"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "r")));
    // c < 0
    // b < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // c < 0
    // b < "z"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "z")));
    // c = 0
    // b = "m\0"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, "m\0")));
    // c = 0
    // b < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // c = 0
    // b < "m\0"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m\0")));
    // c = 0
    // c = 2
    assertEquals(0, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // a IN (1, 2)
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newInListBuilder(c).addValue(1).addValue(2).build()));
    // a IN (0, 1, 2)
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListBuilder(c).addValue(0).addValue(1).addValue(2).build()));
    // a IN (-10, 0)
    // B < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newInListBuilder(c).addValue(0).addValue(-10).build(), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // a IN (-10, 0)
    // B < "m\0"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListBuilder(c).addValue(0).addValue(-10).build(), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m\0")));
}
#method_after
@Test
public void testRangePartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b STRING, c INT8)
    // PRIMARY KEY (a, b, c))
    // DISTRIBUTE BY RANGE(c, b);
    // PARTITION BY RANGE (a, b, c)
    // (PARTITION              VALUES < (0, "m"),
    // PARTITION  (0, "m") <= VALUES < (10, "r")
    // PARTITION (10, "r") <= VALUES);
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.STRING).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("c", "b"));
    PartialRow split = schema.newPartialRow();
    split.addByte("c", (byte) 0);
    split.addString("b", "m");
    tableBuilder.addSplitRow(split);
    split.addByte("c", (byte) 10);
    split.addString("b", "r");
    tableBuilder.addSplitRow(split);
    String tableName = "testRangePartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(3, countPartitions(table, partitions));
    // c < -10
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, -10)));
    // c = -10
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, -10)));
    // c < 10
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 10)));
    // c < 100
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 100)));
    // c >= -10
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10)));
    // c >= 0
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10)));
    // c >= 5
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 5)));
    // c >= 10
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10)));
    // c >= 100
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 100)));
    // c >= -10
    // c < 0
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, -10), KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 0)));
    // c >= 5
    // c < 100
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 5), KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 100)));
    // b = ""
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, "")));
    // b >= "z"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "z")));
    // b < "a"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "a")));
    // b >= "m"
    // b < "z"
    assertEquals(3, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "m"), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "z")));
    // c >= 10
    // b >= "r"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.GREATER_EQUAL, "r")));
    // c >= 10
    // b < "r"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "r")));
    // c = 10
    // b < "r"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 10), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "r")));
    // c < 0
    // b < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // c < 0
    // b < "z"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.LESS, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "z")));
    // c = 0
    // b = "m\0"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, "m\0")));
    // c = 0
    // b < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // c = 0
    // b < "m\0"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m\0")));
    // c = 0
    // c = 2
    assertEquals(0, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // a IN (1, 2)
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 1, (byte) 2))));
    // a IN (0, 1, 2)
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 1, (byte) 2))));
    // a IN (-10, 0)
    // B < "m"
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) -10, (byte) 0)), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m")));
    // a IN (-10, 0)
    // B < "m\0"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) -10, (byte) 0)), KuduPredicate.newComparisonPredicate(b, ComparisonOp.LESS, "m\0")));
}
#end_block

#method_before
@Test
public void testHashPartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c)
    // PARTITION BY HASH (a) PARTITIONS 2,
    // HASH (b, c) PARTITIONS 2;
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(new ArrayList<String>());
    tableBuilder.addHashPartitions(ImmutableList.of("a"), 2);
    tableBuilder.addHashPartitions(ImmutableList.of("b", "c"), 2);
    String tableName = "testHashPartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(4, countPartitions(table, partitions));
    // a = 0;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.EQUAL, 0)));
    // a >= 0;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0)));
    // a >= 0;
    // a < 1;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(a, ComparisonOp.LESS, 1)));
    // a >= 0;
    // a < 2;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(a, ComparisonOp.LESS, 2)));
    // b = 1;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1)));
    // b = 1;
    // c = 2;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // a = 0;
    // b = 1;
    // c = 2;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // a IN (0, 10)
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newInListBuilder(a).addValue(0).addValue(10).build()));
}
#method_after
@Test
public void testHashPartitionPruning() throws Exception {
    // CREATE TABLE t
    // (a INT8, b INT8, c INT8)
    // PRIMARY KEY (a, b, c)
    // PARTITION BY HASH (a) PARTITIONS 2,
    // HASH (b, c) PARTITIONS 2;
    ColumnSchema a = new ColumnSchema.ColumnSchemaBuilder("a", Type.INT8).key(true).build();
    ColumnSchema b = new ColumnSchema.ColumnSchemaBuilder("b", Type.INT8).key(true).build();
    ColumnSchema c = new ColumnSchema.ColumnSchemaBuilder("c", Type.INT8).key(true).build();
    Schema schema = new Schema(ImmutableList.of(a, b, c));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(new ArrayList<String>());
    tableBuilder.addHashPartitions(ImmutableList.of("a"), 2);
    tableBuilder.addHashPartitions(ImmutableList.of("b", "c"), 2);
    String tableName = "testHashPartitionPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(4, countPartitions(table, partitions));
    // a = 0;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.EQUAL, 0)));
    // a >= 0;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0)));
    // a >= 0;
    // a < 1;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(a, ComparisonOp.LESS, 1)));
    // a >= 0;
    // a < 2;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.GREATER_EQUAL, 0), KuduPredicate.newComparisonPredicate(a, ComparisonOp.LESS, 2)));
    // b = 1;
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1)));
    // b = 1;
    // c = 2;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // a = 0;
    // b = 1;
    // c = 2;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(a, ComparisonOp.EQUAL, 0), KuduPredicate.newComparisonPredicate(b, ComparisonOp.EQUAL, 1), KuduPredicate.newComparisonPredicate(c, ComparisonOp.EQUAL, 2)));
    // a IN (0, 10)
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newInListPredicate(c, ImmutableList.of((byte) 0, (byte) 10))));
}
#end_block

#method_before
@Test
public void testPruning() throws Exception {
    // CREATE TABLE timeseries
    // (host STRING, metric STRING, timestamp UNIXTIME_MICROS, value DOUBLE)
    // PRIMARY KEY (host, metric, time)
    // DISTRIBUTE BY
    // RANGE(time)
    // (PARTITION VALUES < 10,
    // PARTITION VALUES >= 10);
    // HASH (host, metric) 2 PARTITIONS;
    ColumnSchema host = new ColumnSchema.ColumnSchemaBuilder("host", Type.STRING).key(true).build();
    ColumnSchema metric = new ColumnSchema.ColumnSchemaBuilder("metric", Type.STRING).key(true).build();
    ColumnSchema timestamp = new ColumnSchema.ColumnSchemaBuilder("timestamp", Type.UNIXTIME_MICROS).key(true).build();
    ColumnSchema value = new ColumnSchema.ColumnSchemaBuilder("value", Type.DOUBLE).build();
    Schema schema = new Schema(ImmutableList.of(host, metric, timestamp, value));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("timestamp"));
    PartialRow split = schema.newPartialRow();
    split.addLong("timestamp", 10);
    tableBuilder.addSplitRow(split);
    tableBuilder.addHashPartitions(ImmutableList.of("host", "metric"), 2);
    String tableName = "testPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(4, countPartitions(table, partitions));
    // host = "a"
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    // timestamp >= 9;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 9)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    // timestamp < 20;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 20)));
    // host = "a"
    // metric = "a"
    // timestamp < 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 10)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10)));
    // host = "a"
    // metric = "a"
    // timestamp = 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // partition key < (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }));
    // partition key >= (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}));
    // timestamp = 10
    // partition key < (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp = 10
    // partition key >= (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp IN (0, 9)
    // host = "a"
    // metric IN ("foo", "bar")
    // 
    // We do not prune hash partitions based on IN list predicates (yet),
    // so the IN list on the hash columns is really just testing that it doesn't fail.
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListBuilder(timestamp).addValue(0).addValue(9).build(), KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newInListBuilder(metric).addValue("foo").addValue("bar").build()));
    // timestamp IN (10, 100)
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListBuilder(timestamp).addValue(10).addValue(100).build()));
    // timestamp IN (9, 10)
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newInListBuilder(timestamp).addValue(9).addValue(10).build()));
    // timestamp IS NOT NULL
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newIsNotNullPredicate(timestamp)));
}
#method_after
@Test
public void testPruning() throws Exception {
    // CREATE TABLE timeseries
    // (host STRING, metric STRING, timestamp UNIXTIME_MICROS, value DOUBLE)
    // PRIMARY KEY (host, metric, time)
    // DISTRIBUTE BY
    // RANGE(time)
    // (PARTITION VALUES < 10,
    // PARTITION VALUES >= 10);
    // HASH (host, metric) 2 PARTITIONS;
    ColumnSchema host = new ColumnSchema.ColumnSchemaBuilder("host", Type.STRING).key(true).build();
    ColumnSchema metric = new ColumnSchema.ColumnSchemaBuilder("metric", Type.STRING).key(true).build();
    ColumnSchema timestamp = new ColumnSchema.ColumnSchemaBuilder("timestamp", Type.UNIXTIME_MICROS).key(true).build();
    ColumnSchema value = new ColumnSchema.ColumnSchemaBuilder("value", Type.DOUBLE).build();
    Schema schema = new Schema(ImmutableList.of(host, metric, timestamp, value));
    CreateTableOptions tableBuilder = new CreateTableOptions();
    tableBuilder.setRangePartitionColumns(ImmutableList.of("timestamp"));
    PartialRow split = schema.newPartialRow();
    split.addLong("timestamp", 10);
    tableBuilder.addSplitRow(split);
    tableBuilder.addHashPartitions(ImmutableList.of("host", "metric"), 2);
    String tableName = "testPruning-" + System.currentTimeMillis();
    syncClient.createTable(tableName, schema, tableBuilder);
    KuduTable table = syncClient.openTable(tableName);
    List<Partition> partitions = getTablePartitions(table);
    // No Predicates
    assertEquals(4, countPartitions(table, partitions));
    // host = "a"
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a")));
    // host = "a"
    // metric = "a"
    // timestamp >= 9;
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 9)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    // timestamp < 20;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 20)));
    // host = "a"
    // metric = "a"
    // timestamp < 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.LESS, 10)));
    // host = "a"
    // metric = "a"
    // timestamp >= 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.GREATER_EQUAL, 10)));
    // host = "a"
    // metric = "a"
    // timestamp = 10;
    assertEquals(1, countPartitions(table, partitions, KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(metric, ComparisonOp.EQUAL, "a"), KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // partition key < (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }));
    // partition key >= (hash=1)
    assertEquals(2, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}));
    // timestamp = 10
    // partition key < (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] {}, new byte[] { 0, 0, 0, 1 }, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp = 10
    // partition key >= (hash=1)
    assertEquals(1, countPartitions(table, partitions, new byte[] { 0, 0, 0, 1 }, new byte[] {}, KuduPredicate.newComparisonPredicate(timestamp, ComparisonOp.EQUAL, 10)));
    // timestamp IN (0, 9)
    // host = "a"
    // metric IN ("foo", "bar")
    // 
    // We do not prune hash partitions based on IN list predicates (yet),
    // so the IN list on the hash columns is really just testing that it doesn't fail.
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(0L, 9L)), KuduPredicate.newComparisonPredicate(host, ComparisonOp.EQUAL, "a"), KuduPredicate.newInListPredicate(metric, ImmutableList.of("foo", "bar"))));
    // timestamp IN (10, 100)
    assertEquals(2, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(10L, 100L))));
    // timestamp IN (9, 10)
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newInListPredicate(timestamp, ImmutableList.of(9L, 10L))));
    // timestamp IS NOT NULL
    assertEquals(4, countPartitions(table, partitions, KuduPredicate.newIsNotNullPredicate(timestamp)));
}
#end_block

#method_before
private static byte[] pushPredicatesIntoLowerBoundRangeKey(Schema schema, PartitionSchema.RangeSchema rangeSchema, Map<String, KuduPredicate> predicates) {
    PartialRow row = schema.newPartialRow();
    int pushedPredicates = 0;
    List<Integer> rangePartitionColumnIdxs = idsToIndexes(schema, rangeSchema.getColumns());
    // stopping after the first missing predicate.
    for (int idx : rangePartitionColumnIdxs) {
        ColumnSchema column = schema.getColumnByIndex(idx);
        KuduPredicate predicate = predicates.get(column.getName());
        if (predicate == null)
            break;
        switch(predicate.getType()) {
            case RANGE:
                if (predicate.getLower() == null)
                    break;
            // fall through
            case EQUALITY:
                row.setRaw(idx, predicate.getLower());
                pushedPredicates++;
                break;
            case IN_LIST:
                row.setRaw(idx, predicate.getInListValues()[0]);
                pushedPredicates++;
                break;
            case IS_NOT_NULL:
                break;
            default:
                throw new IllegalArgumentException(String.format("unexpected predicate type can not be pushed into key: %s", predicate));
        }
    }
    // If no predicates were pushed, no need to do any more work.
    if (pushedPredicates == 0)
        return AsyncKuduClient.EMPTY_ARRAY;
    // For each remaining column in the partition key, fill it with the minimum value.
    Iterator<Integer> remainingIdxs = rangePartitionColumnIdxs.listIterator(pushedPredicates);
    while (remainingIdxs.hasNext()) {
        row.setMin(remainingIdxs.next());
    }
    return KeyEncoder.encodeRangePartitionKey(row, rangeSchema);
}
#method_after
private static byte[] pushPredicatesIntoLowerBoundRangeKey(Schema schema, PartitionSchema.RangeSchema rangeSchema, Map<String, KuduPredicate> predicates) {
    PartialRow row = schema.newPartialRow();
    int pushedPredicates = 0;
    List<Integer> rangePartitionColumnIdxs = idsToIndexes(schema, rangeSchema.getColumns());
    // stopping after the first missing predicate.
    loop: for (int idx : rangePartitionColumnIdxs) {
        ColumnSchema column = schema.getColumnByIndex(idx);
        KuduPredicate predicate = predicates.get(column.getName());
        if (predicate == null)
            break;
        switch(predicate.getType()) {
            case RANGE:
                if (predicate.getLower() == null)
                    break loop;
            // fall through
            case EQUALITY:
                row.setRaw(idx, predicate.getLower());
                pushedPredicates++;
                break;
            case IS_NOT_NULL:
                break loop;
            case IN_LIST:
                row.setRaw(idx, predicate.getInListValues()[0]);
                pushedPredicates++;
                break;
            default:
                throw new IllegalArgumentException(String.format("unexpected predicate type can not be pushed into key: %s", predicate));
        }
    }
    // If no predicates were pushed, no need to do any more work.
    if (pushedPredicates == 0)
        return AsyncKuduClient.EMPTY_ARRAY;
    // For each remaining column in the partition key, fill it with the minimum value.
    Iterator<Integer> remainingIdxs = rangePartitionColumnIdxs.listIterator(pushedPredicates);
    while (remainingIdxs.hasNext()) {
        row.setMin(remainingIdxs.next());
    }
    return KeyEncoder.encodeRangePartitionKey(row, rangeSchema);
}
#end_block

#method_before
private static byte[] pushPredicatesIntoUpperBoundRangeKey(Schema schema, PartitionSchema.RangeSchema rangeSchema, Map<String, KuduPredicate> predicates) {
    PartialRow row = schema.newPartialRow();
    int pushedPredicates = 0;
    KuduPredicate finalPredicate = null;
    List<Integer> rangePartitionColumnIdxs = idsToIndexes(schema, rangeSchema.getColumns());
    // the first missing predicate.
    loop: for (int idx : rangePartitionColumnIdxs) {
        ColumnSchema column = schema.getColumnByIndex(idx);
        KuduPredicate predicate = predicates.get(column.getName());
        if (predicate == null)
            break;
        switch(predicate.getType()) {
            case EQUALITY:
                row.setRaw(idx, predicate.getLower());
                pushedPredicates++;
                finalPredicate = predicate;
                break;
            case RANGE:
                {
                    if (predicate.getUpper() != null) {
                        row.setRaw(idx, predicate.getUpper());
                        pushedPredicates++;
                        finalPredicate = predicate;
                    }
                    // constraint.
                    break loop;
                }
            case IN_LIST:
                {
                    byte[][] values = predicate.getInListValues();
                    row.setRaw(idx, values[values.length - 1]);
                    pushedPredicates++;
                    finalPredicate = predicate;
                    break;
                }
            case IS_NOT_NULL:
                break;
            default:
                throw new IllegalArgumentException(String.format("unexpected predicate type can not be pushed into key: %s", predicate));
        }
    }
    // If no predicates were pushed, no need to do any more work.
    if (pushedPredicates == 0)
        return AsyncKuduClient.EMPTY_ARRAY;
    // key to convert it to an exclusive upper bound.
    if (finalPredicate.getType() == KuduPredicate.PredicateType.EQUALITY || finalPredicate.getType() == KuduPredicate.PredicateType.IN_LIST) {
        incrementKey(row, rangePartitionColumnIdxs.subList(0, pushedPredicates));
    }
    // Step 3: Fill the remaining columns without predicates with the min value.
    Iterator<Integer> remainingIdxs = rangePartitionColumnIdxs.listIterator(pushedPredicates);
    while (remainingIdxs.hasNext()) {
        row.setMin(remainingIdxs.next());
    }
    return KeyEncoder.encodeRangePartitionKey(row, rangeSchema);
}
#method_after
private static byte[] pushPredicatesIntoUpperBoundRangeKey(Schema schema, PartitionSchema.RangeSchema rangeSchema, Map<String, KuduPredicate> predicates) {
    PartialRow row = schema.newPartialRow();
    int pushedPredicates = 0;
    KuduPredicate finalPredicate = null;
    List<Integer> rangePartitionColumnIdxs = idsToIndexes(schema, rangeSchema.getColumns());
    // the first missing predicate.
    loop: for (int idx : rangePartitionColumnIdxs) {
        ColumnSchema column = schema.getColumnByIndex(idx);
        KuduPredicate predicate = predicates.get(column.getName());
        if (predicate == null)
            break;
        switch(predicate.getType()) {
            case EQUALITY:
                row.setRaw(idx, predicate.getLower());
                pushedPredicates++;
                finalPredicate = predicate;
                break;
            case RANGE:
                if (predicate.getUpper() != null) {
                    row.setRaw(idx, predicate.getUpper());
                    pushedPredicates++;
                    finalPredicate = predicate;
                }
                // constraint.
                break loop;
            case IS_NOT_NULL:
                break loop;
            case IN_LIST:
                {
                    byte[][] values = predicate.getInListValues();
                    row.setRaw(idx, values[values.length - 1]);
                    pushedPredicates++;
                    finalPredicate = predicate;
                    break;
                }
            default:
                throw new IllegalArgumentException(String.format("unexpected predicate type can not be pushed into key: %s", predicate));
        }
    }
    // If no predicates were pushed, no need to do any more work.
    if (pushedPredicates == 0)
        return AsyncKuduClient.EMPTY_ARRAY;
    // key to convert it to an exclusive upper bound.
    if (finalPredicate.getType() == KuduPredicate.PredicateType.EQUALITY || finalPredicate.getType() == KuduPredicate.PredicateType.IN_LIST) {
        incrementKey(row, rangePartitionColumnIdxs.subList(0, pushedPredicates));
    }
    // Step 3: Fill the remaining columns without predicates with the min value.
    Iterator<Integer> remainingIdxs = rangePartitionColumnIdxs.listIterator(pushedPredicates);
    while (remainingIdxs.hasNext()) {
        row.setMin(remainingIdxs.next());
    }
    return KeyEncoder.encodeRangePartitionKey(row, rangeSchema);
}
#end_block

#method_before
KuduPredicate merge(KuduPredicate other) {
    Preconditions.checkArgument(column.equals(other.column), "predicates from different columns may not be merged");
    // NONE predicates dominate.
    if (other.type == PredicateType.NONE)
        return other;
    // added.
    if (other.type == PredicateType.IS_NOT_NULL)
        return this;
    switch(type) {
        case NONE:
            return this;
        case IS_NOT_NULL:
            return other;
        case EQUALITY:
            {
                if (other.type == PredicateType.EQUALITY) {
                    if (compare(column, lower, other.lower) != 0) {
                        return none(this.column);
                    } else {
                        return this;
                    }
                } else if (other.type == PredicateType.RANGE) {
                    if (other.rangeContains(lower)) {
                        return this;
                    } else {
                        return none(this.column);
                    }
                } else {
                    Preconditions.checkState(other.type == PredicateType.IN_LIST);
                    return other.merge(this);
                }
            }
        case RANGE:
            {
                if (other.type == PredicateType.EQUALITY || other.type == PredicateType.IN_LIST) {
                    return other.merge(this);
                } else {
                    Preconditions.checkState(other.type == PredicateType.RANGE);
                    byte[] newLower = other.lower == null || (lower != null && compare(column, lower, other.lower) >= 0) ? lower : other.lower;
                    byte[] newUpper = other.upper == null || (upper != null && compare(column, upper, other.upper) <= 0) ? upper : other.upper;
                    if (newLower != null && newUpper != null && compare(column, newLower, newUpper) >= 0) {
                        return none(column);
                    } else {
                        if (newLower != null && newUpper != null && areConsecutive(newLower, newUpper)) {
                            return new KuduPredicate(PredicateType.EQUALITY, column, newLower, null);
                        } else {
                            return new KuduPredicate(PredicateType.RANGE, column, newLower, newUpper);
                        }
                    }
                }
            }
        case IN_LIST:
            {
                if (other.type == PredicateType.EQUALITY) {
                    if (this.inListContains(other.lower)) {
                        return other;
                    } else {
                        return none(column);
                    }
                } else if (other.type == PredicateType.RANGE) {
                    InListBuilder builder = new InListBuilder(column);
                    for (byte[] value : inListValues) {
                        if (other.rangeContains(value)) {
                            builder.addValueRaw(value);
                        }
                    }
                    return builder.build();
                } else {
                    Preconditions.checkState(other.type == PredicateType.IN_LIST);
                    InListBuilder builder = new InListBuilder(column);
                    for (byte[] value : inListValues) {
                        if (other.inListContains(value)) {
                            builder.addValueRaw(value);
                        }
                    }
                    return builder.build();
                }
            }
        default:
            throw new IllegalStateException(String.format("unknown predicate type %s", this));
    }
}
#method_after
KuduPredicate merge(KuduPredicate other) {
    Preconditions.checkArgument(column.equals(other.column), "predicates from different columns may not be merged");
    // NONE predicates dominate.
    if (other.type == PredicateType.NONE)
        return other;
    // added.
    if (other.type == PredicateType.IS_NOT_NULL)
        return this;
    switch(type) {
        case NONE:
            return this;
        case IS_NOT_NULL:
            return other;
        case EQUALITY:
            {
                if (other.type == PredicateType.EQUALITY) {
                    if (compare(column, lower, other.lower) != 0) {
                        return none(this.column);
                    } else {
                        return this;
                    }
                } else if (other.type == PredicateType.RANGE) {
                    if (other.rangeContains(lower)) {
                        return this;
                    } else {
                        return none(this.column);
                    }
                } else {
                    Preconditions.checkState(other.type == PredicateType.IN_LIST);
                    return other.merge(this);
                }
            }
        case RANGE:
            {
                if (other.type == PredicateType.EQUALITY || other.type == PredicateType.IN_LIST) {
                    return other.merge(this);
                } else {
                    Preconditions.checkState(other.type == PredicateType.RANGE);
                    byte[] newLower = other.lower == null || (lower != null && compare(column, lower, other.lower) >= 0) ? lower : other.lower;
                    byte[] newUpper = other.upper == null || (upper != null && compare(column, upper, other.upper) <= 0) ? upper : other.upper;
                    if (newLower != null && newUpper != null && compare(column, newLower, newUpper) >= 0) {
                        return none(column);
                    } else {
                        if (newLower != null && newUpper != null && areConsecutive(newLower, newUpper)) {
                            return new KuduPredicate(PredicateType.EQUALITY, column, newLower, null);
                        } else {
                            return new KuduPredicate(PredicateType.RANGE, column, newLower, newUpper);
                        }
                    }
                }
            }
        case IN_LIST:
            {
                if (other.type == PredicateType.EQUALITY) {
                    if (this.inListContains(other.lower)) {
                        return other;
                    } else {
                        return none(column);
                    }
                } else if (other.type == PredicateType.RANGE) {
                    List<byte[]> values = new ArrayList<>();
                    for (byte[] value : inListValues) {
                        if (other.rangeContains(value)) {
                            values.add(value);
                        }
                    }
                    return buildInList(column, values);
                } else {
                    Preconditions.checkState(other.type == PredicateType.IN_LIST);
                    List<byte[]> values = new ArrayList<>();
                    for (byte[] value : inListValues) {
                        if (other.inListContains(value)) {
                            values.add(value);
                        }
                    }
                    return buildInList(column, values);
                }
            }
        default:
            throw new IllegalStateException(String.format("unknown predicate type %s", this));
    }
}
#end_block

#method_before
@InterfaceAudience.Private
public static KuduPredicate fromPB(Schema schema, Common.ColumnPredicatePB pb) {
    ColumnSchema column = schema.getColumn(pb.getColumn());
    switch(pb.getPredicateCase()) {
        case EQUALITY:
            {
                return new KuduPredicate(PredicateType.EQUALITY, column, pb.getEquality().getValue().toByteArray(), null);
            }
        case RANGE:
            {
                Common.ColumnPredicatePB.Range range = pb.getRange();
                return new KuduPredicate(PredicateType.RANGE, column, range.hasLower() ? range.getLower().toByteArray() : null, range.hasUpper() ? range.getUpper().toByteArray() : null);
            }
        case IS_NOT_NULL:
            {
                return newIsNotNullPredicate(column);
            }
        case IN_LIST:
            {
                Common.ColumnPredicatePB.InList inList = pb.getInList();
                InListBuilder builder = newInListBuilder(column);
                for (ByteString value : inList.getValuesList()) {
                    builder.addValueRaw(value.toByteArray());
                }
                return builder.build();
            }
        default:
            throw new IllegalArgumentException("unknown predicate type");
    }
}
#method_after
@InterfaceAudience.Private
public static KuduPredicate fromPB(Schema schema, Common.ColumnPredicatePB pb) {
    final ColumnSchema column = schema.getColumn(pb.getColumn());
    switch(pb.getPredicateCase()) {
        case EQUALITY:
            return new KuduPredicate(PredicateType.EQUALITY, column, pb.getEquality().getValue().toByteArray(), null);
        case RANGE:
            {
                Common.ColumnPredicatePB.Range range = pb.getRange();
                return new KuduPredicate(PredicateType.RANGE, column, range.hasLower() ? range.getLower().toByteArray() : null, range.hasUpper() ? range.getUpper().toByteArray() : null);
            }
        case IS_NOT_NULL:
            return newIsNotNullPredicate(column);
        case IN_LIST:
            {
                Common.ColumnPredicatePB.InList inList = pb.getInList();
                SortedSet<byte[]> values = new TreeSet<>(new Comparator<byte[]>() {

                    @Override
                    public int compare(byte[] a, byte[] b) {
                        return KuduPredicate.compare(column, a, b);
                    }
                });
                for (ByteString value : inList.getValuesList()) {
                    values.add(value.toByteArray());
                }
                return buildInList(column, values);
            }
        default:
            throw new IllegalArgumentException("unknown predicate type");
    }
}
#end_block

#method_before
private boolean checkAndReportError(String message, KuduException e) {
    // READ_AT_SNAPSHOT scanners.
    if (e.getStatus().isTimedOut()) {
        LOG.warn("Received a scan timeout", e);
        return true;
    }
    // the "Invalid call sequence ID" message.
    if (!e.getStatus().isNotFound() && !e.getStatus().getMessage().contains("Invalid call sequence ID")) {
        reportError(message, e);
        return false;
    }
    return true;
}
#method_after
private boolean checkAndReportError(String message, KuduException e) {
    // TODO revisit once KUDU-1656 is taken care of.
    if (e.getStatus().isTimedOut()) {
        LOG.warn("Received a scan timeout", e);
        return true;
    }
    // the "Invalid call sequence ID" message.
    if (!e.getStatus().isNotFound() && !e.getStatus().getMessage().contains("Invalid call sequence ID")) {
        reportError(message, e);
        return false;
    }
    return true;
}
#end_block

#method_before
private boolean hasRowErrorAndReport(OperationResponse resp) {
    if (resp != null && resp.hasRowError()) {
        reportError("The following RPC " + resp.getOperation().getRow() + " returned this error: " + resp.getRowError(), null);
        return true;
    }
    if (resp == null) {
        return false;
    }
    long writeTimestamp = resp.getWriteTimestampRaw();
    if (writeTimestamp != 0) {
        sharedWriteTimestamp = writeTimestamp;
    }
    return false;
}
#method_after
private boolean hasRowErrorAndReport(OperationResponse resp) {
    if (resp != null && resp.hasRowError()) {
        reportError("The following RPC " + resp.getOperation().getRow() + " returned this error: " + resp.getRowError(), null);
        return true;
    }
    if (resp == null) {
        return false;
    }
    sharedWriteTimestamp = resp.getWriteTimestampRaw();
    return false;
}
#end_block

#method_before
private boolean checkAndReportError(String message, KuduException e) {
    // "Invalid call sequence ID" message.
    if (!e.getStatus().isNotFound() && !e.getStatus().getMessage().contains("Invalid call sequence ID")) {
        reportError(message, e);
        return false;
    }
    return true;
}
#method_after
private boolean checkAndReportError(String message, KuduException e) {
    // the "Invalid call sequence ID" message.
    if (!e.getStatus().isNotFound() && !e.getStatus().getMessage().contains("Invalid call sequence ID")) {
        reportError(message, e);
        return false;
    }
    return true;
}
#end_block

#method_before
protected static int countRowsInScan(AsyncKuduScanner scanner) throws Exception {
    final AtomicInteger counter = new AtomicInteger();
    Callback<Object, RowResultIterator> cb = new Callback<Object, RowResultIterator>() {

        @Override
        public Object call(RowResultIterator arg) throws Exception {
            if (arg == null)
                return null;
            counter.addAndGet(arg.getNumRows());
            return null;
        }
    };
    while (scanner.hasMoreRows()) {
        Deferred<RowResultIterator> data = scanner.nextRows();
        data.addCallbacks(cb, defaultErrorCB);
        data.join(DEFAULT_SLEEP);
    }
    Deferred<RowResultIterator> closer = scanner.close();
    closer.addCallbacks(cb, defaultErrorCB);
    closer.join(DEFAULT_SLEEP);
    return counter.get();
}
#method_after
protected static int countRowsInScan(AsyncKuduScanner scanner) throws Exception {
    final AtomicInteger counter = new AtomicInteger();
    Callback<Object, RowResultIterator> cb = new Callback<Object, RowResultIterator>() {

        @Override
        public Object call(RowResultIterator arg) throws Exception {
            if (arg == null)
                return null;
            counter.addAndGet(arg.getNumRows());
            return null;
        }
    };
    while (scanner.hasMoreRows()) {
        Deferred<RowResultIterator> data = scanner.nextRows();
        data.addCallbacks(cb, defaultErrorCB);
        data.join(DEFAULT_SLEEP);
    }
    return counter.get();
}
#end_block

#method_before
protected static int countRowsInScan(KuduScanner scanner) throws KuduException {
    int counter = 0;
    while (scanner.hasMoreRows()) {
        counter += scanner.nextRows().getNumRows();
    }
    scanner.close();
    return counter;
}
#method_after
protected static int countRowsInScan(KuduScanner scanner) throws KuduException {
    int counter = 0;
    while (scanner.hasMoreRows()) {
        counter += scanner.nextRows().getNumRows();
    }
    return counter;
}
#end_block

#method_before
protected long countRowsInTable(KuduTable table, KuduPredicate... predicates) throws KuduException {
    long count = 0;
    KuduScanner.KuduScannerBuilder scanBuilder = syncClient.newScannerBuilder(table);
    for (KuduPredicate predicate : predicates) {
        scanBuilder.addPredicate(predicate);
    }
    scanBuilder.setProjectedColumnIndexes(ImmutableList.<Integer>of());
    KuduScanner scanner = scanBuilder.build();
    while (scanner.hasMoreRows()) {
        count += scanner.nextRows().getNumRows();
    }
    return count;
}
#method_after
protected long countRowsInTable(KuduTable table, KuduPredicate... predicates) throws KuduException {
    KuduScanner.KuduScannerBuilder scanBuilder = syncClient.newScannerBuilder(table);
    for (KuduPredicate predicate : predicates) {
        scanBuilder.addPredicate(predicate);
    }
    scanBuilder.setProjectedColumnIndexes(ImmutableList.<Integer>of());
    return countRowsInScan(scanBuilder.build());
}
#end_block

#method_before
static KuduException transformException(Exception e) {
    // Message is allowed to be null, so if we let it go through we'll NPE building the Status
    // down the line.
    String message = e.getMessage() == null ? "" : e.getMessage();
    if (e instanceof KuduException) {
        return (KuduException) e;
    } else if (e instanceof DeferredGroupException) {
    // TODO anything we can do to improve on that kind of exception?
    } else if (e instanceof TimeoutException) {
        Status statusTimeout = Status.TimedOut(message);
        return new NonRecoverableException(statusTimeout, e);
    } else if (e instanceof InterruptedException) {
        // Need to reset the interrupt flag since we caught it but aren't handling it.
        Thread.currentThread().interrupt();
        Status statusAborted = Status.Aborted(message);
        return new NonRecoverableException(statusAborted, e);
    }
    Status status = Status.IOError(message);
    return new NonRecoverableException(status, e);
}
#method_after
static KuduException transformException(Exception e) {
    // The message may be null.
    String message = e.getMessage() == null ? "" : e.getMessage();
    if (e instanceof KuduException) {
        return (KuduException) e;
    } else if (e instanceof DeferredGroupException) {
    // TODO anything we can do to improve on that kind of exception?
    } else if (e instanceof TimeoutException) {
        Status statusTimeout = Status.TimedOut(message);
        return new NonRecoverableException(statusTimeout, e);
    } else if (e instanceof InterruptedException) {
        // Need to reset the interrupt flag since we caught it but aren't handling it.
        Thread.currentThread().interrupt();
        Status statusAborted = Status.Aborted(message);
        return new NonRecoverableException(statusAborted, e);
    }
    Status status = Status.IOError(message);
    return new NonRecoverableException(status, e);
}
#end_block

#method_before
public void loadConfig() {
    if (Strings.isNullOrEmpty(configFile_)) {
        throw new IllegalArgumentException("A valid path to a sentry-site.xml config " + "file must be set using --sentry_config to enable authorization.");
    }
    File configFile = new File(configFile_);
    if (!configFile.exists()) {
        throw new RuntimeException("Sentry configuration file does not exist: " + configFile_);
    }
    if (!configFile.canRead()) {
        throw new RuntimeException("Cannot read Sentry configuration file: " + configFile_);
    }
    // Load the config.
    try {
        config_.addResource(configFile.toURI().toURL());
    } catch (MalformedURLException e) {
        throw new RuntimeException("Invalid Sentry config file path: " + configFile_, e);
    }
}
#method_after
public void loadConfig() {
    if (Strings.isNullOrEmpty(configFile_)) {
        throw new IllegalArgumentException("A valid path to a sentry-site.xml config " + "file must be set using --sentry_config to enable authorization.");
    }
    File configFile = new File(configFile_);
    if (!configFile.exists()) {
        String configFilePath = "\"" + configFile_ + "\"";
        throw new RuntimeException("Sentry configuration file does not exist: " + configFilePath);
    }
    if (!configFile.canRead()) {
        throw new RuntimeException("Cannot read Sentry configuration file: " + configFile_);
    }
    // Load the config.
    try {
        config_.addResource(configFile.toURI().toURL());
    } catch (MalformedURLException e) {
        throw new RuntimeException("Invalid Sentry config file path: " + configFile_, e);
    }
}
#end_block

#method_before
public static boolean copyToLocal(Path source, Path dest) {
    try {
        FileSystem fs = source.getFileSystem(CONF);
        fs.copyToLocalFile(source, dest);
    } catch (IOException e) {
        LOG.error("Error copying file at path " + source.toString() + " to " + dest.toString(), e);
        return false;
    }
    return true;
}
#method_after
public static void copyToLocal(Path source, Path dest) throws IOException {
    FileSystem fs = source.getFileSystem(CONF);
    fs.copyToLocalFile(source, dest);
}
#end_block

#method_before
public static boolean isFunctionCompatible(org.apache.hadoop.hive.metastore.api.Function fn, StringBuilder incompatMsg) {
    boolean isCompatible = true;
    if (fn.getFunctionType() != FunctionType.JAVA) {
        isCompatible = false;
        incompatMsg.append("Function type: " + fn.getFunctionType().name() + " is not supported. Only " + FunctionType.JAVA.name() + " functions " + "are supported.");
    } else if (fn.getResourceUrisSize() == 0) {
        isCompatible = false;
        incompatMsg.append("No executable binary resource (like a JAR file) is " + "associated with this function");
    } else if (fn.getResourceUrisSize() != 1) {
        isCompatible = false;
        List<String> resourceUris = Lists.newArrayList();
        for (ResourceUri resource : fn.getResourceUris()) {
            resourceUris.add(resource.getUri());
        }
        incompatMsg.append("Impala does not support multiple Jars for dependencies." + "(" + Joiner.on(",").join(resourceUris) + ") ");
    } else if (fn.getResourceUris().get(0).getResourceType() != ResourceType.JAR) {
        isCompatible = false;
        incompatMsg.append("Function binary type: " + fn.getResourceUris().get(0).getResourceType().name() + " is not supported. Only " + ResourceType.JAR.name() + " type is supported.");
    }
    return isCompatible;
}
#method_after
public static boolean isFunctionCompatible(org.apache.hadoop.hive.metastore.api.Function fn, StringBuilder incompatMsg) {
    boolean isCompatible = true;
    if (fn.getFunctionType() != FunctionType.JAVA) {
        isCompatible = false;
        incompatMsg.append("Function type: " + fn.getFunctionType().name() + " is not supported. Only " + FunctionType.JAVA.name() + " functions " + "are supported.");
    } else if (fn.getResourceUrisSize() == 0) {
        isCompatible = false;
        incompatMsg.append("No executable binary resource (like a JAR file) is " + "associated with this function. To fix this, recreate the function by " + "specifying a 'location' in the function create statement.");
    } else if (fn.getResourceUrisSize() != 1) {
        isCompatible = false;
        List<String> resourceUris = Lists.newArrayList();
        for (ResourceUri resource : fn.getResourceUris()) {
            resourceUris.add(resource.getUri());
        }
        incompatMsg.append("Impala does not support multiple Jars for dependencies." + "(" + Joiner.on(",").join(resourceUris) + ") ");
    } else if (fn.getResourceUris().get(0).getResourceType() != ResourceType.JAR) {
        isCompatible = false;
        incompatMsg.append("Function binary type: " + fn.getResourceUris().get(0).getResourceType().name() + " is not supported. Only " + ResourceType.JAR.name() + " type is supported.");
    }
    return isCompatible;
}
#end_block

#method_before
private KuduTable createNewTable(String tableName) throws Exception {
    ArrayList<ColumnSchema> columns = new ArrayList<>(10);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key", Type.INT32).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("byte", Type.INT8).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("short", Type.INT16).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("int", Type.INT32).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("long", Type.INT64).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("binary", Type.BINARY).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("string", Type.STRING).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("bool", Type.BOOL).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("float", Type.FLOAT).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("double", Type.DOUBLE).build());
    CreateTableOptions createOptions = new CreateTableOptions().addHashPartitions(ImmutableList.of("key"), 3).setNumReplicas(1);
    KuduTable table = createTable(tableName, new Schema(columns), createOptions);
    return table;
}
#method_after
private KuduTable createNewTable(String tableName) throws Exception {
    ArrayList<ColumnSchema> columns = new ArrayList<>(10);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key", Type.INT32).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("byteFld", Type.INT8).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("shortFld", Type.INT16).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("intFld", Type.INT32).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("longFld", Type.INT64).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("binaryFld", Type.BINARY).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("stringFld", Type.STRING).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("boolFld", Type.BOOL).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("floatFld", Type.FLOAT).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("doubleFld", Type.DOUBLE).build());
    CreateTableOptions createOptions = new CreateTableOptions().addHashPartitions(ImmutableList.of("key"), 3).setNumReplicas(1);
    KuduTable table = createTable(tableName, new Schema(columns), createOptions);
    return table;
}
#end_block

#method_before
private void testEvents(int eventCount, int perEventRowCount, String operation) throws Exception {
    String tableName = String.format("test%sevents%srowseach%s", eventCount, perEventRowCount, operation);
    KuduTable table = createNewTable(tableName);
    KuduSink sink = createSink(tableName, operation);
    Channel channel = new MemoryChannel();
    Configurables.configure(channel, new Context());
    sink.setChannel(channel);
    sink.start();
    Transaction tx = channel.getTransaction();
    tx.begin();
    for (int i = 0; i < eventCount; i++) {
        StringBuilder payload = new StringBuilder();
        for (int j = 0; j < perEventRowCount; j++) {
            String baseRow = "|1%1$d%2$d1,%1$d,%1$d,%1$d,%1$d,binary," + "string,false,%1$d.%1$d,%1$d.%1$d,%1$d|";
            String row = String.format(baseRow, i, j);
            payload.append(row);
        }
        Event e = EventBuilder.withBody(payload.toString().getBytes());
        channel.put(e);
    }
    // 
    if (eventCount > 0) {
        // All such rows go in one event.
        if (operation.equals("upsert")) {
            StringBuilder upserts = new StringBuilder();
            for (int j = 0; j < perEventRowCount; j++) {
                String row = String.format("|1%2$d%3$d1,%1$d,%1$d,%1$d,%1$d,binary," + "string,false,%1$d.%1$d,%1$d.%1$d,%1$d|", 1, 0, j);
                upserts.append(row);
            }
            Event e = EventBuilder.withBody(upserts.toString().getBytes());
            channel.put(e);
        }
        // Also check some bad/corner cases.
        String mismatchInInt = "|1,2,taco,4,5,x,y,true,1.0.2.0,999|";
        String emptyString = "";
        String[] testCases = { mismatchInInt, emptyString };
        for (String testCase : testCases) {
            Event e = EventBuilder.withBody(testCase.getBytes());
            channel.put(e);
        }
    }
    tx.commit();
    tx.close();
    Sink.Status status = sink.process();
    if (eventCount == 0) {
        assertTrue("incorrect status for empty channel", status == Sink.Status.BACKOFF);
    } else {
        assertTrue("incorrect status for non-empty channel", status != Sink.Status.BACKOFF);
    }
    List<String> rows = scanTableToStrings(table);
    assertEquals(eventCount * perEventRowCount + " row(s) expected", eventCount * perEventRowCount, rows.size());
    ArrayList<String> rightAnswers = new ArrayList<>(eventCount * perEventRowCount);
    for (int i = 0; i < eventCount; i++) {
        for (int j = 0; j < perEventRowCount; j++) {
            int value = operation.equals("upsert") && i == 0 ? 1 : i;
            String baseAnswer = "INT32 key=1%2$d%3$d1, INT8 byte=%1$d, INT16 short=%1$d, " + "INT32 int=%1$d, INT64 long=%1$d, BINARY binary=\"binary\", STRING string=string, " + "BOOL bool=false, FLOAT float=%1$d.%1$d, DOUBLE double=%1$d.%1$d";
            String rightAnswer = String.format(baseAnswer, value, i, j);
            rightAnswers.add(rightAnswer);
        }
    }
    Collections.sort(rightAnswers);
    for (int k = 0; k < eventCount * perEventRowCount; k++) {
        assertEquals("incorrect row", rightAnswers.get(k), rows.get(k));
    }
}
#method_after
private void testEvents(int eventCount, int perEventRowCount, String operation) throws Exception {
    String tableName = String.format("test%sevents%srowseach%s", eventCount, perEventRowCount, operation);
    KuduTable table = createNewTable(tableName);
    KuduSink sink = createSink(tableName, operation);
    Channel channel = new MemoryChannel();
    Configurables.configure(channel, new Context());
    sink.setChannel(channel);
    sink.start();
    Transaction tx = channel.getTransaction();
    tx.begin();
    for (int i = 0; i < eventCount; i++) {
        StringBuilder payload = new StringBuilder();
        for (int j = 0; j < perEventRowCount; j++) {
            String baseRow = "|1%1$d%2$d1,%1$d,%1$d,%1$d,%1$d,binary," + "string,false,%1$d.%1$d,%1$d.%1$d,%1$d|";
            String row = String.format(baseRow, i, j);
            payload.append(row);
        }
        Event e = EventBuilder.withBody(payload.toString().getBytes());
        channel.put(e);
    }
    if (eventCount > 0) {
        // All such rows go in one event.
        if (operation.equals("upsert")) {
            StringBuilder upserts = new StringBuilder();
            for (int j = 0; j < perEventRowCount; j++) {
                String row = String.format("|1%2$d%3$d1,%1$d,%1$d,%1$d,%1$d,binary," + "string,false,%1$d.%1$d,%1$d.%1$d,%1$d|", 1, 0, j);
                upserts.append(row);
            }
            Event e = EventBuilder.withBody(upserts.toString().getBytes());
            channel.put(e);
        }
        // Also check some bad/corner cases.
        String mismatchInInt = "|1,2,taco,4,5,x,y,true,1.0.2.0,999|";
        String emptyString = "";
        String[] testCases = { mismatchInInt, emptyString };
        for (String testCase : testCases) {
            Event e = EventBuilder.withBody(testCase.getBytes());
            channel.put(e);
        }
    }
    tx.commit();
    tx.close();
    Sink.Status status = sink.process();
    if (eventCount == 0) {
        assertTrue("incorrect status for empty channel", status == Sink.Status.BACKOFF);
    } else {
        assertTrue("incorrect status for non-empty channel", status != Sink.Status.BACKOFF);
    }
    List<String> rows = scanTableToStrings(table);
    assertEquals(eventCount * perEventRowCount + " row(s) expected", eventCount * perEventRowCount, rows.size());
    ArrayList<String> rightAnswers = new ArrayList<>(eventCount * perEventRowCount);
    for (int i = 0; i < eventCount; i++) {
        for (int j = 0; j < perEventRowCount; j++) {
            int value = operation.equals("upsert") && i == 0 ? 1 : i;
            String baseAnswer = "INT32 key=1%2$d%3$d1, INT8 byteFld=%1$d, INT16 shortFld=%1$d, " + "INT32 intFld=%1$d, INT64 longFld=%1$d, BINARY binaryFld=\"binary\", " + "STRING stringFld=string, BOOL boolFld=false, FLOAT floatFld=%1$d.%1$d, " + "DOUBLE doubleFld=%1$d.%1$d";
            String rightAnswer = String.format(baseAnswer, value, i, j);
            rightAnswers.add(rightAnswer);
        }
    }
    Collections.sort(rightAnswers);
    for (int k = 0; k < eventCount * perEventRowCount; k++) {
        assertEquals("incorrect row", rightAnswers.get(k), rows.get(k));
    }
}
#end_block

#method_before
@Override
public List<Operation> getOperations(Event event) throws FlumeException {
    String raw = new String(event.getBody(), charset);
    Matcher m = pattern.matcher(raw);
    boolean match = false;
    Schema schema = table.getSchema();
    List<Operation> ops = Lists.newArrayList();
    while (m.find()) {
        match = true;
        Operation op;
        switch(operation.toLowerCase()) {
            case "upsert":
                op = table.newUpsert();
                break;
            case "insert":
                op = table.newInsert();
                break;
            default:
                throw new FlumeException(String.format("Unrecognized operation type '%s' in getOperations: " + "this should never happen!", operation));
        }
        PartialRow row = op.getRow();
        for (ColumnSchema col : schema.getColumns()) {
            try {
                CoerceAndSet(row, m.group(col.getName()), col.getName(), col.getType());
            } catch (NumberFormatException e) {
                String msg = String.format("Raw value '%s' couldn't be parsed to type %s for column '%s'", raw, col.getType(), col.getName());
                LogOrThrow(skipBadColumnValue, msg, e);
            } catch (IllegalArgumentException e) {
                String msg = String.format("Column '%s' has no matching group in '%s'", col.getName(), raw);
                LogOrThrow(skipMissingColumn, msg, e);
            } catch (Exception e) {
                throw new FlumeException("Failed to create Kudu operation", e);
            }
        }
        ops.add(op);
    }
    if (!match && warnUnmatchedRows) {
        logger.warn("Failed to match the pattern '{}' in '{}'", pattern, raw);
    }
    return ops;
}
#method_after
@Override
public List<Operation> getOperations(Event event) throws FlumeException {
    String raw = new String(event.getBody(), charset);
    Matcher m = pattern.matcher(raw);
    boolean match = false;
    Schema schema = table.getSchema();
    List<Operation> ops = Lists.newArrayList();
    while (m.find()) {
        match = true;
        Operation op;
        switch(operation.toLowerCase()) {
            case "upsert":
                op = table.newUpsert();
                break;
            case "insert":
                op = table.newInsert();
                break;
            default:
                throw new FlumeException(String.format("Unrecognized operation type '%s' in getOperations: " + "this should never happen!", operation));
        }
        PartialRow row = op.getRow();
        for (ColumnSchema col : schema.getColumns()) {
            try {
                CoerceAndSet(m.group(col.getName()), col.getName(), col.getType(), row);
            } catch (NumberFormatException e) {
                String msg = String.format("Raw value '%s' couldn't be parsed to type %s for column '%s'", raw, col.getType(), col.getName());
                LogOrThrow(skipBadColumnValue, msg, e);
            } catch (IllegalArgumentException e) {
                String msg = String.format("Column '%s' has no matching group in '%s'", col.getName(), raw);
                LogOrThrow(skipMissingColumn, msg, e);
            } catch (Exception e) {
                throw new FlumeException("Failed to create Kudu operation", e);
            }
        }
        ops.add(op);
    }
    if (!match && warnUnmatchedRows) {
        logger.warn("Failed to match the pattern '{}' in '{}'", pattern, raw);
    }
    return ops;
}
#end_block

#method_before
private void CoerceAndSet(PartialRow row, String rawVal, String colName, Type type) throws NumberFormatException {
    switch(type) {
        case INT8:
            row.addByte(colName, Byte.parseByte(rawVal));
            break;
        case INT16:
            row.addShort(colName, Short.parseShort(rawVal));
            break;
        case INT32:
            row.addInt(colName, Integer.parseInt(rawVal));
            break;
        case INT64:
            row.addLong(colName, Long.parseLong(rawVal));
            break;
        case BINARY:
            row.addBinary(colName, rawVal.getBytes(charset));
            break;
        case STRING:
            row.addString(colName, rawVal);
            break;
        case BOOL:
            row.addBoolean(colName, Boolean.parseBoolean(rawVal));
            break;
        case FLOAT:
            row.addFloat(colName, Float.parseFloat(rawVal));
            break;
        case DOUBLE:
            row.addDouble(colName, Double.parseDouble(rawVal));
            break;
        case TIMESTAMP:
            row.addLong(colName, Long.parseLong(rawVal));
            break;
        default:
            logger.warn("got unknown type {} for column '{}'-- ignoring this column", type, colName);
    }
}
#method_after
private void CoerceAndSet(String rawVal, String colName, Type type, PartialRow row) throws NumberFormatException {
    switch(type) {
        case INT8:
            row.addByte(colName, Byte.parseByte(rawVal));
            break;
        case INT16:
            row.addShort(colName, Short.parseShort(rawVal));
            break;
        case INT32:
            row.addInt(colName, Integer.parseInt(rawVal));
            break;
        case INT64:
            row.addLong(colName, Long.parseLong(rawVal));
            break;
        case BINARY:
            row.addBinary(colName, rawVal.getBytes(charset));
            break;
        case STRING:
            row.addString(colName, rawVal);
            break;
        case BOOL:
            row.addBoolean(colName, Boolean.parseBoolean(rawVal));
            break;
        case FLOAT:
            row.addFloat(colName, Float.parseFloat(rawVal));
            break;
        case DOUBLE:
            row.addDouble(colName, Double.parseDouble(rawVal));
            break;
        case TIMESTAMP:
            row.addLong(colName, Long.parseLong(rawVal));
            break;
        default:
            logger.warn("got unknown type {} for column '{}'-- ignoring this column", type, colName);
    }
}
#end_block

#method_before
public static Schema getSchemaWithAllTypes() {
    List<ColumnSchema> columns = ImmutableList.of(new ColumnSchema.ColumnSchemaBuilder("int8", Type.INT8).key(true).build(), new ColumnSchema.ColumnSchemaBuilder("int16", Type.INT16).build(), new ColumnSchema.ColumnSchemaBuilder("int32", Type.INT32).build(), new ColumnSchema.ColumnSchemaBuilder("int64", Type.INT64).build(), new ColumnSchema.ColumnSchemaBuilder("bool", Type.BOOL).build(), new ColumnSchema.ColumnSchemaBuilder("float", Type.FLOAT).build(), new ColumnSchema.ColumnSchemaBuilder("double", Type.DOUBLE).build(), new ColumnSchema.ColumnSchemaBuilder("string", Type.STRING).build(), new ColumnSchema.ColumnSchemaBuilder("binary-array", Type.BINARY).build(), new ColumnSchema.ColumnSchemaBuilder("binary-bytebuffer", Type.BINARY).build(), new ColumnSchema.ColumnSchemaBuilder("null", Type.STRING).nullable(true).build(), new ColumnSchema.ColumnSchemaBuilder("timestamp", Type.TIMESTAMP).build());
    return new Schema(columns);
}
#method_after
public static Schema getSchemaWithAllTypes() {
    List<ColumnSchema> columns = ImmutableList.of(new ColumnSchema.ColumnSchemaBuilder("int8", Type.INT8).key(true).build(), new ColumnSchema.ColumnSchemaBuilder("int16", Type.INT16).build(), new ColumnSchema.ColumnSchemaBuilder("int32", Type.INT32).build(), new ColumnSchema.ColumnSchemaBuilder("int64", Type.INT64).build(), new ColumnSchema.ColumnSchemaBuilder("bool", Type.BOOL).build(), new ColumnSchema.ColumnSchemaBuilder("float", Type.FLOAT).build(), new ColumnSchema.ColumnSchemaBuilder("double", Type.DOUBLE).build(), new ColumnSchema.ColumnSchemaBuilder("string", Type.STRING).build(), new ColumnSchema.ColumnSchemaBuilder("binary-array", Type.BINARY).build(), new ColumnSchema.ColumnSchemaBuilder("binary-bytebuffer", Type.BINARY).build(), new ColumnSchema.ColumnSchemaBuilder("null", Type.STRING).nullable(true).build(), new ColumnSchema.ColumnSchemaBuilder("timestamp", Type.UNIXTIME_MICROS).build());
    return new Schema(columns);
}
#end_block

#method_before
private void removeClientFromIpCache(final TabletClient client, final SocketAddress remote) {
    if (remote == null) {
        // Can't continue without knowing the remote address.
        return;
    }
    String hostport;
    if (remote instanceof InetSocketAddress) {
        final InetSocketAddress sock = (InetSocketAddress) remote;
        final InetAddress addr = sock.getAddress();
        if (addr == null) {
            LOG.error("WTF?  Unresolved IP for " + remote + ".  This shouldn't happen.");
            return;
        } else {
            hostport = addr.getHostAddress() + ':' + sock.getPort();
        }
    } else {
        LOG.error("WTF?  Found a non-InetSocketAddress remote: " + remote + ".  This shouldn't happen.");
        return;
    }
    TabletClient old;
    synchronized (ip2client) {
        old = ip2client.remove(hostport);
    }
    LOG.debug("Removed from IP cache: {" + hostport + "} -> {" + client + "}");
    if (old == null) {
        // Currently we're seeing this message when masters are disconnected and the hostport we got
        // above is different than the one the user passes (that we use to populate ip2client). At
        // worst this doubles the entries for masters, which has an insignificant impact.
        // TODO When fixed, make this a WARN again.
        LOG.trace("When expiring " + client + " from the client cache (host:port=" + hostport + "), it was found that there was no entry" + " corresponding to " + remote + ".  This shouldn't happen.");
    } else {
        client2tablets.remove(old);
    }
}
#method_after
private void removeClientFromIpCache(final TabletClient client, final SocketAddress remote) {
    if (remote == null) {
        // Can't continue without knowing the remote address.
        return;
    }
    String hostport;
    if (remote instanceof InetSocketAddress) {
        final InetSocketAddress sock = (InetSocketAddress) remote;
        final InetAddress addr = sock.getAddress();
        if (addr == null) {
            LOG.error("WTF?  Unresolved IP for " + remote + ".  This shouldn't happen.");
            return;
        } else {
            hostport = addr.getHostAddress() + ':' + sock.getPort();
        }
    } else {
        LOG.error("WTF?  Found a non-InetSocketAddress remote: " + remote + ".  This shouldn't happen.");
        return;
    }
    TabletClient old;
    synchronized (ip2client) {
        old = ip2client.remove(hostport);
    }
    LOG.debug("Removed from IP cache: {" + hostport + "} -> {" + client + "}");
    if (old == null) {
        // Currently we're seeing this message when masters are disconnected and the hostport we got
        // above is different than the one the user passes (that we use to populate ip2client). At
        // worst this doubles the entries for masters, which has an insignificant impact.
        // TODO When fixed, make this a WARN again.
        LOG.trace("When expiring " + client + " from the client cache (host:port=" + hostport + "), it was found that there was no entry" + " corresponding to " + remote + ".  This shouldn't happen.");
    } else {
        removeClient(old);
    }
}
#end_block

#method_before
@BeforeClass
public static void setupAvroSchemaBeforeClass() {
    org.apache.avro.Schema.Parser parser = new org.apache.avro.Schema.Parser();
    try {
        schemaLiteral = Files.toString(new File(schemaPath), Charsets.UTF_8);
        schema = parser.parse(new File(schemaPath));
    } catch (IOException e) {
        throw new FlumeException("Unable to open and parse schema file!", e);
    }
}
#method_after
@BeforeClass
public static void setupAvroSchemaBeforeClass() {
    try {
        schemaLiteral = Files.toString(new File(schemaPath), Charsets.UTF_8);
    } catch (IOException e) {
        throw new FlumeException("Unable to read schema file!", e);
    }
}
#end_block

#method_before
private KuduSink createSink(String tableName, Context ctx) {
    KuduSink sink = new KuduSink(syncClient);
    HashMap<String, String> parameters = new HashMap<>();
    parameters.put(TABLE_NAME, tableName);
    parameters.put(MASTER_ADDRESSES, getMasterAddresses());
    parameters.put(PRODUCER, "org.apache.kudu.flume.sink.AvroKuduOperationsProducer");
    Context context = new Context(parameters);
    context.putAll(ctx.getParameters());
    Configurables.configure(sink, context);
    return sink;
}
#method_after
private KuduSink createSink(String tableName, Context ctx) {
    KuduSink sink = new KuduSink(syncClient);
    HashMap<String, String> parameters = new HashMap<>();
    parameters.put(TABLE_NAME, tableName);
    parameters.put(MASTER_ADDRESSES, getMasterAddresses());
    parameters.put(PRODUCER, AvroKuduOperationsProducer.class.getName());
    Context context = new Context(parameters);
    context.putAll(ctx.getParameters());
    Configurables.configure(sink, context);
    return sink;
}
#end_block

#method_before
@Override
public void start() {
    Preconditions.checkState(table == null && session == null, "Please call stop before calling start on an old instance.");
    // client is not null only inside tests
    if (client == null) {
        client = new KuduClient.KuduClientBuilder(masterAddresses).build();
    }
    session = client.newSession();
    session.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH);
    session.setTimeoutMillis(timeoutMillis);
    session.setIgnoreAllDuplicateRows(ignoreDuplicateRows);
    try {
        table = client.openTable(tableName);
    } catch (Exception e) {
        sinkCounter.incrementConnectionFailedCount();
        String msg = String.format("Could not open Kudu table '%s'", tableName);
        logger.error(msg, e);
        throw new FlumeException(msg, e);
    }
    eventProducer.initialize(table);
    super.start();
    sinkCounter.incrementConnectionCreatedCount();
    sinkCounter.start();
}
#method_after
@Override
public void start() {
    Preconditions.checkState(table == null && session == null, "Please call stop before calling start on an old instance.");
    // client is not null only inside tests
    if (client == null) {
        client = new KuduClient.KuduClientBuilder(masterAddresses).build();
    }
    session = client.newSession();
    session.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH);
    session.setTimeoutMillis(timeoutMillis);
    session.setIgnoreAllDuplicateRows(ignoreDuplicateRows);
    try {
        table = client.openTable(tableName);
    } catch (Exception e) {
        sinkCounter.incrementConnectionFailedCount();
        String msg = String.format("Could not open Kudu table '%s'", tableName);
        logger.error(msg, e);
        throw new FlumeException(msg, e);
    }
    operationsProducer.initialize(table);
    super.start();
    sinkCounter.incrementConnectionCreatedCount();
    sinkCounter.start();
}
#end_block

#method_before
@Override
public void stop() {
    eventProducer.close();
    try {
        if (client != null) {
            client.shutdown();
        }
        client = null;
        table = null;
        session = null;
    } catch (Exception e) {
        throw new FlumeException("Error closing client", e);
    }
    sinkCounter.incrementConnectionClosedCount();
    sinkCounter.stop();
}
#method_after
@Override
public void stop() {
    Exception ex = null;
    try {
        operationsProducer.close();
    } catch (Exception e) {
        ex = e;
        logger.error("Error closing operations producer", e);
    }
    try {
        if (client != null) {
            client.shutdown();
        }
        client = null;
        table = null;
        session = null;
    } catch (Exception e) {
        ex = e;
        logger.error("Error closing client", e);
    }
    sinkCounter.incrementConnectionClosedCount();
    sinkCounter.stop();
    if (ex != null) {
        throw new FlumeException("Error stopping sink", ex);
    }
}
#end_block

#method_before
@SuppressWarnings("unchecked")
@Override
public void configure(Context context) {
    masterAddresses = context.getString(MASTER_ADDRESSES);
    Preconditions.checkNotNull(masterAddresses, String.format("Missing master addresses. Please specify property '$s'.", MASTER_ADDRESSES));
    tableName = context.getString(TABLE_NAME);
    Preconditions.checkNotNull(tableName, String.format("Missing table name. Please specify property '%s'", TABLE_NAME));
    batchSize = context.getLong(BATCH_SIZE, DEFAULT_BATCH_SIZE);
    timeoutMillis = context.getLong(TIMEOUT_MILLIS, DEFAULT_TIMEOUT_MILLIS);
    ignoreDuplicateRows = context.getBoolean(IGNORE_DUPLICATE_ROWS, DEFAULT_IGNORE_DUPLICATE_ROWS);
    String operationProducerType = context.getString(PRODUCER);
    // Check for operations producer, if null set default operations producer type.
    if (operationProducerType == null || operationProducerType.isEmpty()) {
        operationProducerType = DEFAULT_KUDU_OPERATION_PRODUCER;
        logger.warn("No Kudu operations producer provided, using default");
    }
    Context producerContext = new Context();
    producerContext.putAll(context.getSubProperties(KuduSinkConfigurationConstants.PRODUCER_PREFIX));
    try {
        Class<? extends KuduOperationsProducer> clazz = (Class<? extends KuduOperationsProducer>) Class.forName(operationProducerType);
        eventProducer = clazz.newInstance();
        eventProducer.configure(producerContext);
    } catch (Exception e) {
        logger.error("Could not instantiate Kudu operations producer", e);
        Throwables.propagate(e);
    }
    sinkCounter = new SinkCounter(this.getName());
}
#method_after
@SuppressWarnings("unchecked")
@Override
public void configure(Context context) {
    masterAddresses = context.getString(MASTER_ADDRESSES);
    Preconditions.checkNotNull(masterAddresses, "Missing master addresses. Please specify property '$s'.", MASTER_ADDRESSES);
    tableName = context.getString(TABLE_NAME);
    Preconditions.checkNotNull(tableName, "Missing table name. Please specify property '%s'", TABLE_NAME);
    batchSize = context.getLong(BATCH_SIZE, DEFAULT_BATCH_SIZE);
    timeoutMillis = context.getLong(TIMEOUT_MILLIS, DEFAULT_TIMEOUT_MILLIS);
    ignoreDuplicateRows = context.getBoolean(IGNORE_DUPLICATE_ROWS, DEFAULT_IGNORE_DUPLICATE_ROWS);
    String operationProducerType = context.getString(PRODUCER);
    // Check for operations producer, if null set default operations producer type.
    if (operationProducerType == null || operationProducerType.isEmpty()) {
        operationProducerType = DEFAULT_KUDU_OPERATION_PRODUCER;
        logger.warn("No Kudu operations producer provided, using default");
    }
    Context producerContext = new Context();
    producerContext.putAll(context.getSubProperties(KuduSinkConfigurationConstants.PRODUCER_PREFIX));
    try {
        Class<? extends KuduOperationsProducer> clazz = (Class<? extends KuduOperationsProducer>) Class.forName(operationProducerType);
        operationsProducer = clazz.newInstance();
        operationsProducer.configure(producerContext);
    } catch (Exception e) {
        logger.error("Could not instantiate Kudu operations producer", e);
        Throwables.propagate(e);
    }
    sinkCounter = new SinkCounter(this.getName());
}
#end_block

#method_before
@Override
public Status process() throws EventDeliveryException {
    if (session.hasPendingOperations()) {
        // pile on the KuduSession.
        return Status.BACKOFF;
    }
    Channel channel = getChannel();
    Transaction txn = channel.getTransaction();
    txn.begin();
    try {
        long txnEventCount = 0;
        for (; txnEventCount < batchSize; txnEventCount++) {
            Event event = channel.take();
            if (event == null) {
                break;
            }
            List<Operation> operations = eventProducer.getOperations(event);
            for (Operation o : operations) {
                session.apply(o);
            }
        }
        logger.debug("Flushing {} events", txnEventCount);
        List<OperationResponse> responses = session.flush();
        if (responses != null) {
            for (OperationResponse response : responses) {
                // is enabled in the config.
                if (response.hasRowError()) {
                    throw new EventDeliveryException("Failed to flush one or more changes. " + "Transaction rolled back: " + response.getRowError().toString());
                }
            }
        }
        if (txnEventCount == 0) {
            sinkCounter.incrementBatchEmptyCount();
        } else if (txnEventCount == batchSize) {
            sinkCounter.incrementBatchCompleteCount();
        } else {
            sinkCounter.incrementBatchUnderflowCount();
        }
        txn.commit();
        if (txnEventCount == 0) {
            return Status.BACKOFF;
        }
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        return Status.READY;
    } catch (Throwable e) {
        txn.rollback();
        String msg = "Failed to commit transaction. Transaction rolled back.";
        logger.error(msg, e);
        if (e instanceof Error || e instanceof RuntimeException) {
            Throwables.propagate(e);
        } else {
            logger.error(msg, e);
            throw new EventDeliveryException(msg, e);
        }
    } finally {
        txn.close();
    }
    return Status.BACKOFF;
}
#method_after
@Override
public Status process() throws EventDeliveryException {
    if (session.hasPendingOperations()) {
        // pile on the KuduSession.
        return Status.BACKOFF;
    }
    Channel channel = getChannel();
    Transaction txn = channel.getTransaction();
    txn.begin();
    try {
        long txnEventCount = 0;
        for (; txnEventCount < batchSize; txnEventCount++) {
            Event event = channel.take();
            if (event == null) {
                break;
            }
            List<Operation> operations = operationsProducer.getOperations(event);
            for (Operation o : operations) {
                session.apply(o);
            }
        }
        logger.debug("Flushing {} events", txnEventCount);
        List<OperationResponse> responses = session.flush();
        if (responses != null) {
            for (OperationResponse response : responses) {
                // is enabled in the config.
                if (response.hasRowError()) {
                    throw new EventDeliveryException("Failed to flush one or more changes. " + "Transaction rolled back: " + response.getRowError().toString());
                }
            }
        }
        if (txnEventCount == 0) {
            sinkCounter.incrementBatchEmptyCount();
        } else if (txnEventCount == batchSize) {
            sinkCounter.incrementBatchCompleteCount();
        } else {
            sinkCounter.incrementBatchUnderflowCount();
        }
        txn.commit();
        if (txnEventCount == 0) {
            return Status.BACKOFF;
        }
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        return Status.READY;
    } catch (Throwable e) {
        txn.rollback();
        String msg = "Failed to commit transaction. Transaction rolled back.";
        logger.error(msg, e);
        if (e instanceof Error || e instanceof RuntimeException) {
            Throwables.propagate(e);
        } else {
            logger.error(msg, e);
            throw new EventDeliveryException(msg, e);
        }
    } finally {
        txn.close();
    }
    return Status.BACKOFF;
}
#end_block

#method_before
private void setupOp(Operation op, GenericRecord record) {
    PartialRow row = op.getRow();
    for (ColumnSchema col : table.getSchema().getColumns()) {
        String name = col.getName();
        Object value = record.get(name);
        if (value == null) {
            if (col.isNullable()) {
                row.setNull(name);
            } else {
            // leave unset for possible Kudu default
            }
        } else {
            // a larger type.
            try {
                switch(col.getType()) {
                    case BOOL:
                        row.addBoolean(name, (boolean) value);
                        break;
                    case INT8:
                        row.addByte(name, (byte) value);
                        break;
                    case INT16:
                        row.addShort(name, (short) value);
                        break;
                    case INT32:
                        row.addInt(name, (int) value);
                        break;
                    // Fall through
                    case INT64:
                    case TIMESTAMP:
                        row.addLong(name, (long) value);
                        break;
                    case FLOAT:
                        row.addFloat(name, (float) value);
                        break;
                    case DOUBLE:
                        row.addDouble(name, (double) value);
                        break;
                    case STRING:
                        row.addString(name, value.toString());
                        break;
                    case BINARY:
                        row.addBinary(name, (byte[]) value);
                        break;
                    default:
                        throw new FlumeException(String.format("Unrecognized type %s for column %s", col.getType().toString(), name));
                }
            } catch (ClassCastException e) {
                throw new FlumeException(String.format("Failed to coerce value for column `%s` to type %s", col.getName(), col.getType()));
            }
        }
    }
}
#method_after
private void setupOp(Operation op, GenericRecord record) {
    PartialRow row = op.getRow();
    for (ColumnSchema col : table.getSchema().getColumns()) {
        String name = col.getName();
        Object value = record.get(name);
        if (value == null) {
            if (col.isNullable()) {
                row.setNull(name);
            } else {
            // leave unset for possible Kudu default
            }
        } else {
            // a larger type.
            try {
                switch(col.getType()) {
                    case BOOL:
                        row.addBoolean(name, (boolean) value);
                        break;
                    case INT8:
                        row.addByte(name, (byte) value);
                        break;
                    case INT16:
                        row.addShort(name, (short) value);
                        break;
                    case INT32:
                        row.addInt(name, (int) value);
                        break;
                    // Fall through
                    case INT64:
                    case TIMESTAMP:
                        row.addLong(name, (long) value);
                        break;
                    case FLOAT:
                        row.addFloat(name, (float) value);
                        break;
                    case DOUBLE:
                        row.addDouble(name, (double) value);
                        break;
                    case STRING:
                        row.addString(name, value.toString());
                        break;
                    case BINARY:
                        row.addBinary(name, (byte[]) value);
                        break;
                    default:
                        throw new FlumeException(String.format("Unrecognized type %s for column %s", col.getType().toString(), name));
                }
            } catch (ClassCastException e) {
                throw new FlumeException(String.format("Failed to coerce value for column '%s' to type %s", col.getName(), col.getType()));
            }
        }
    }
}
#end_block

#method_before
private KuduTable createNewTable(String tableName) throws Exception {
    LOG.info("Creating new table...");
    ArrayList<ColumnSchema> columns = new ArrayList<>(2);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key", Type.STRING).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("payload", Type.BINARY).key(false).build());
    CreateTableOptions createOptions = new CreateTableOptions().setRangePartitionColumns(ImmutableList.of("key")).setNumReplicas(1);
    KuduTable table = createTable(tableName, new Schema(columns), createOptions);
    LOG.info("Created new table.");
    return table;
}
#method_after
private KuduTable createNewTable(String tableName) throws Exception {
    LOG.info("Creating new table...");
    ArrayList<ColumnSchema> columns = new ArrayList<>(2);
    columns.add(new ColumnSchema.ColumnSchemaBuilder(KEY_COLUMN_DEFAULT, Type.STRING).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder(PAYLOAD_COLUMN_DEFAULT, Type.BINARY).key(false).build());
    CreateTableOptions createOptions = new CreateTableOptions().setRangePartitionColumns(ImmutableList.of(KEY_COLUMN_DEFAULT)).setNumReplicas(1);
    KuduTable table = createTable(tableName, new Schema(columns), createOptions);
    LOG.info("Created new table.");
    return table;
}
#end_block

#method_before
@Test
public void testEmptyChannelWithInsert() throws Exception {
    testEvents(0, "false");
}
#method_after
@Test
public void testEmptyChannelWithInsert() throws Exception {
    testEvents(0, "insert");
}
#end_block

#method_before
@Test
public void testOneEventWithInsert() throws Exception {
    testEvents(1, "false");
}
#method_after
@Test
public void testOneEventWithInsert() throws Exception {
    testEvents(1, "insert");
}
#end_block

#method_before
@Test
public void testThreeEventsWithInsert() throws Exception {
    testEvents(3, "false");
}
#method_after
@Test
public void testThreeEventsWithInsert() throws Exception {
    testEvents(3, "insert");
}
#end_block

#method_before
@Test
public void testEmptyChannelWithUpsert() throws Exception {
    testEvents(0, "true");
}
#method_after
@Test
public void testEmptyChannelWithUpsert() throws Exception {
    testEvents(0, "upsert");
}
#end_block

#method_before
@Test
public void testOneEventWithUpsert() throws Exception {
    testEvents(1, "true");
}
#method_after
@Test
public void testOneEventWithUpsert() throws Exception {
    testEvents(1, "upsert");
}
#end_block

#method_before
@Test
public void testThreeEventsWithUpsert() throws Exception {
    testEvents(3, "true");
}
#method_after
@Test
public void testThreeEventsWithUpsert() throws Exception {
    testEvents(3, "upsert");
}
#end_block

#method_before
@Test
public void testDuplicateRowsWithUpsert() throws Exception {
    LOG.info("Testing events with upsert...");
    KuduTable table = createNewTable("testDupUpsertEvents");
    String tableName = table.getName();
    Context ctx = new Context(ImmutableMap.of("producer.upsert", "true"));
    KuduSink sink = createSink(tableName, ctx);
    Channel channel = new MemoryChannel();
    Configurables.configure(channel, new Context());
    sink.setChannel(channel);
    sink.start();
    Transaction tx = channel.getTransaction();
    tx.begin();
    int numRows = 3;
    for (int i = 0; i < numRows; i++) {
        Event e = EventBuilder.withBody(String.format("payload body %s", i), Charsets.UTF_8);
        e.setHeaders(ImmutableMap.of("key", String.format("key %s", i)));
        channel.put(e);
    }
    tx.commit();
    tx.close();
    Sink.Status status = sink.process();
    assertTrue("incorrect status for non-empty channel", status != Sink.Status.BACKOFF);
    List<String> rows = scanTableToStrings(table);
    assertEquals(numRows + " row(s) expected", numRows, rows.size());
    for (int i = 0; i < numRows; i++) {
        assertTrue("incorrect payload", rows.get(i).contains("payload body " + i));
    }
    Transaction utx = channel.getTransaction();
    utx.begin();
    Event dup = EventBuilder.withBody("payload body upserted".getBytes());
    dup.setHeaders(ImmutableMap.of("key", String.format("key %s", 0)));
    channel.put(dup);
    utx.commit();
    utx.close();
    Sink.Status upStatus = sink.process();
    assertTrue("incorrect status for non-empty channel", upStatus != Sink.Status.BACKOFF);
    List<String> upRows = scanTableToStrings(table);
    assertEquals(numRows + " row(s) expected", numRows, upRows.size());
    assertTrue("incorrect payload", upRows.get(0).contains("payload body upserted"));
    for (int i = 1; i < numRows; i++) {
        assertTrue("incorrect payload", upRows.get(i).contains("payload body " + i));
    }
    LOG.info("Testing events with upsert finished successfully.");
}
#method_after
@Test
public void testDuplicateRowsWithUpsert() throws Exception {
    LOG.info("Testing events with upsert...");
    KuduTable table = createNewTable("testDupUpsertEvents");
    String tableName = table.getName();
    Context ctx = new Context(ImmutableMap.of(PRODUCER_PREFIX + OPERATION_PROP, "upsert"));
    KuduSink sink = createSink(tableName, ctx);
    Channel channel = new MemoryChannel();
    Configurables.configure(channel, new Context());
    sink.setChannel(channel);
    sink.start();
    Transaction tx = channel.getTransaction();
    tx.begin();
    int numRows = 3;
    for (int i = 0; i < numRows; i++) {
        Event e = EventBuilder.withBody(String.format("payload body %s", i), Charsets.UTF_8);
        e.setHeaders(ImmutableMap.of(KEY_COLUMN_DEFAULT, String.format("key %s", i)));
        channel.put(e);
    }
    tx.commit();
    tx.close();
    Sink.Status status = sink.process();
    assertTrue("incorrect status for non-empty channel", status != Sink.Status.BACKOFF);
    List<String> rows = scanTableToStrings(table);
    assertEquals(numRows + " row(s) expected", numRows, rows.size());
    for (int i = 0; i < numRows; i++) {
        assertTrue("incorrect payload", rows.get(i).contains("payload body " + i));
    }
    Transaction utx = channel.getTransaction();
    utx.begin();
    Event dup = EventBuilder.withBody("payload body upserted".getBytes());
    dup.setHeaders(ImmutableMap.of("key", String.format("key %s", 0)));
    channel.put(dup);
    utx.commit();
    utx.close();
    Sink.Status upStatus = sink.process();
    assertTrue("incorrect status for non-empty channel", upStatus != Sink.Status.BACKOFF);
    List<String> upRows = scanTableToStrings(table);
    assertEquals(numRows + " row(s) expected", numRows, upRows.size());
    assertTrue("incorrect payload", upRows.get(0).contains("payload body upserted"));
    for (int i = 1; i < numRows; i++) {
        assertTrue("incorrect payload", upRows.get(i).contains("payload body " + i));
    }
    LOG.info("Testing events with upsert finished successfully.");
}
#end_block

#method_before
private void testEvents(int eventCount, String upsert) throws Exception {
    LOG.info("Testing {} events...", eventCount);
    KuduTable table = createNewTable("test" + eventCount + "eventsUp" + upsert);
    String tableName = table.getName();
    Context ctx = new Context(ImmutableMap.of("producer.upsert", upsert));
    KuduSink sink = createSink(tableName, ctx);
    Channel channel = new MemoryChannel();
    Configurables.configure(channel, new Context());
    sink.setChannel(channel);
    sink.start();
    Transaction tx = channel.getTransaction();
    tx.begin();
    for (int i = 0; i < eventCount; i++) {
        Event e = EventBuilder.withBody(String.format("payload body %s", i).getBytes());
        e.setHeaders(ImmutableMap.of("key", String.format("key %s", i)));
        channel.put(e);
    }
    tx.commit();
    tx.close();
    Sink.Status status = sink.process();
    if (eventCount == 0) {
        assertTrue("incorrect status for empty channel", status == Sink.Status.BACKOFF);
    } else {
        assertTrue("incorrect status for non-empty channel", status != Sink.Status.BACKOFF);
    }
    List<String> rows = scanTableToStrings(table);
    assertEquals(eventCount + " row(s) expected", eventCount, rows.size());
    for (int i = 0; i < eventCount; i++) {
        assertTrue("incorrect payload", rows.get(i).contains("payload body " + i));
    }
    LOG.info("Testing {} events finished successfully.", eventCount);
}
#method_after
private void testEvents(int eventCount, String operation) throws Exception {
    LOG.info("Testing {} events...", eventCount);
    KuduTable table = createNewTable("test" + eventCount + "events" + operation);
    String tableName = table.getName();
    Context ctx = new Context(ImmutableMap.of(PRODUCER_PREFIX + OPERATION_PROP, operation));
    KuduSink sink = createSink(tableName, ctx);
    Channel channel = new MemoryChannel();
    Configurables.configure(channel, new Context());
    sink.setChannel(channel);
    sink.start();
    Transaction tx = channel.getTransaction();
    tx.begin();
    for (int i = 0; i < eventCount; i++) {
        Event e = EventBuilder.withBody(String.format("payload body %s", i).getBytes());
        e.setHeaders(ImmutableMap.of("key", String.format("key %s", i)));
        channel.put(e);
    }
    tx.commit();
    tx.close();
    Sink.Status status = sink.process();
    if (eventCount == 0) {
        assertTrue("incorrect status for empty channel", status == Sink.Status.BACKOFF);
    } else {
        assertTrue("incorrect status for non-empty channel", status != Sink.Status.BACKOFF);
    }
    List<String> rows = scanTableToStrings(table);
    assertEquals(eventCount + " row(s) expected", eventCount, rows.size());
    for (int i = 0; i < eventCount; i++) {
        assertTrue("incorrect payload", rows.get(i).contains("payload body " + i));
    }
    LOG.info("Testing {} events finished successfully.", eventCount);
}
#end_block

#method_before
private KuduSink createSink(String tableName, Context ctx) {
    LOG.info("Creating Kudu sink for '{}' table...", tableName);
    KuduSink sink = new KuduSink(syncClient);
    HashMap<String, String> parameters = new HashMap<>();
    parameters.put(KuduSinkConfigurationConstants.TABLE_NAME, tableName);
    parameters.put(KuduSinkConfigurationConstants.MASTER_ADDRESSES, getMasterAddresses());
    parameters.put(KuduSinkConfigurationConstants.PRODUCER, "org.apache.kudu.flume.sink.SimpleKeyedKuduOperationsProducer");
    Context context = new Context(parameters);
    context.putAll(ctx.getParameters());
    Configurables.configure(sink, context);
    LOG.info("Created Kudu sink for '{}' table.", tableName);
    return sink;
}
#method_after
private KuduSink createSink(String tableName, Context ctx) {
    LOG.info("Creating Kudu sink for '{}' table...", tableName);
    KuduSink sink = new KuduSink(syncClient);
    HashMap<String, String> parameters = new HashMap<>();
    parameters.put(TABLE_NAME, tableName);
    parameters.put(MASTER_ADDRESSES, getMasterAddresses());
    parameters.put(PRODUCER, SimpleKeyedKuduOperationsProducer.class.getName());
    Context context = new Context(parameters);
    context.putAll(ctx.getParameters());
    Configurables.configure(sink, context);
    LOG.info("Created Kudu sink for '{}' table.", tableName);
    return sink;
}
#end_block

#method_before
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = new KuduClientBuilder(kuduTable_.getKuduMasterAddresses()).build()) {
        org.kududb.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeMemLayout(analyzer);
    computeStats(analyzer);
}
#method_after
@Override
public void init(Analyzer analyzer) throws ImpalaRuntimeException {
    assignConjuncts(analyzer);
    analyzer.createEquivConjuncts(tupleIds_.get(0), conjuncts_);
    conjuncts_ = orderConjunctsByCost(conjuncts_);
    try (KuduClient client = new KuduClientBuilder(kuduTable_.getKuduMasterAddresses()).build()) {
        org.apache.kudu.client.KuduTable rpcTable = client.openTable(kuduTable_.getKuduTableName());
        validateSchema(rpcTable);
        // Extract predicates that can be evaluated by Kudu.
        extractKuduConjuncts(analyzer, client, rpcTable);
        // Materialize the slots of the remaining conjuncts (i.e. those not pushed to Kudu)
        analyzer.materializeSlots(conjuncts_);
        // Creates Kudu scan tokens and sets the scan range locations.
        computeScanRangeLocations(analyzer, client, rpcTable);
    } catch (Exception e) {
        throw new ImpalaRuntimeException("Unable to initialize the Kudu scan node", e);
    }
    computeMemLayout(analyzer);
    computeStats(analyzer);
}
#end_block

#method_before
private void computeScanRangeLocations(Analyzer analyzer, KuduClient client, org.kududb.client.KuduTable rpcTable) throws ImpalaRuntimeException {
    scanRanges_ = Lists.newArrayList();
    List<KuduScanToken> scanTokens = createScanTokens(client, rpcTable);
    for (KuduScanToken token : scanTokens) {
        LocatedTablet tablet = token.getTablet();
        List<TScanRangeLocation> locations = Lists.newArrayList();
        if (tablet.getReplicas().isEmpty()) {
            throw new ImpalaRuntimeException(String.format("At least one tablet does not have any replicas. Tablet ID: %s", new String(tablet.getTabletId(), Charsets.UTF_8)));
        }
        // TODO: Consider only using the master replica.
        for (LocatedTablet.Replica replica : tablet.getReplicas()) {
            TNetworkAddress address = new TNetworkAddress(replica.getRpcHost(), replica.getRpcPort());
            // Use the network address to look up the host in the global list
            Integer hostIndex = analyzer.getHostIndex().getIndex(address);
            locations.add(new TScanRangeLocation(hostIndex));
            hostIndexSet_.add(hostIndex);
        }
        TScanRange scanRange = new TScanRange();
        try {
            scanRange.setKudu_scan_token(token.serialize());
        } catch (IOException e) {
            throw new ImpalaRuntimeException("Unable to serialize Kudu scan token=" + token.toString(), e);
        }
        TScanRangeLocations locs = new TScanRangeLocations();
        locs.setScan_range(scanRange);
        locs.locations = locations;
        scanRanges_.add(locs);
    }
}
#method_after
private void computeScanRangeLocations(Analyzer analyzer, KuduClient client, org.apache.kudu.client.KuduTable rpcTable) throws ImpalaRuntimeException {
    scanRanges_ = Lists.newArrayList();
    List<KuduScanToken> scanTokens = createScanTokens(client, rpcTable);
    for (KuduScanToken token : scanTokens) {
        LocatedTablet tablet = token.getTablet();
        List<TScanRangeLocation> locations = Lists.newArrayList();
        if (tablet.getReplicas().isEmpty()) {
            throw new ImpalaRuntimeException(String.format("At least one tablet does not have any replicas. Tablet ID: %s", new String(tablet.getTabletId(), Charsets.UTF_8)));
        }
        for (LocatedTablet.Replica replica : tablet.getReplicas()) {
            TNetworkAddress address = new TNetworkAddress(replica.getRpcHost(), replica.getRpcPort());
            // Use the network address to look up the host in the global list
            Integer hostIndex = analyzer.getHostIndex().getIndex(address);
            locations.add(new TScanRangeLocation(hostIndex));
            hostIndexSet_.add(hostIndex);
        }
        TScanRange scanRange = new TScanRange();
        try {
            scanRange.setKudu_scan_token(token.serialize());
        } catch (IOException e) {
            throw new ImpalaRuntimeException("Unable to serialize Kudu scan token=" + token.toString(), e);
        }
        TScanRangeLocations locs = new TScanRangeLocations();
        locs.setScan_range(scanRange);
        locs.locations = locations;
        scanRanges_.add(locs);
    }
}
#end_block

#method_before
private List<KuduScanToken> createScanTokens(KuduClient client, org.kududb.client.KuduTable rpcTable) {
    List<String> projectedCols = Lists.newArrayList();
    for (SlotDescriptor desc : getTupleDesc().getSlots()) {
        if (desc.isMaterialized())
            projectedCols.add(desc.getColumn().getName());
    }
    KuduScanTokenBuilder tokenBuilder = client.newScanTokenBuilder(rpcTable);
    tokenBuilder.setProjectedColumnNames(projectedCols);
    for (KuduPredicate predicate : kuduPredicates_) {
        tokenBuilder.addPredicate(predicate);
    }
    return tokenBuilder.build();
}
#method_after
private List<KuduScanToken> createScanTokens(KuduClient client, org.apache.kudu.client.KuduTable rpcTable) {
    List<String> projectedCols = Lists.newArrayList();
    for (SlotDescriptor desc : getTupleDesc().getSlots()) {
        if (desc.isMaterialized())
            projectedCols.add(desc.getColumn().getName());
    }
    KuduScanTokenBuilder tokenBuilder = client.newScanTokenBuilder(rpcTable);
    tokenBuilder.setProjectedColumnNames(projectedCols);
    for (KuduPredicate predicate : kuduPredicates_) tokenBuilder.addPredicate(predicate);
    return tokenBuilder.build();
}
#end_block

#method_before
private void extractKuduConjuncts(Analyzer analyzer, KuduClient client, org.kududb.client.KuduTable rpcTable) {
    ListIterator<Expr> i = conjuncts_.listIterator();
    while (i.hasNext()) {
        Expr e = i.next();
        if (!(e instanceof BinaryPredicate))
            continue;
        BinaryPredicate comparisonPred = (BinaryPredicate) e;
        // TODO KUDU-931 look into handling implicit/explicit casts on the SlotRef.
        comparisonPred = BinaryPredicate.normalizeSlotRefComparison(comparisonPred, analyzer);
        if (comparisonPred == null)
            continue;
        KuduPredicate kuduPredicate = getKuduPredicate(rpcTable, comparisonPred);
        if (kuduPredicate != null) {
            i.remove();
            kuduConjuncts_.add(comparisonPred);
            kuduPredicates_.add(kuduPredicate);
        }
    }
}
#method_after
private void extractKuduConjuncts(Analyzer analyzer, KuduClient client, org.apache.kudu.client.KuduTable rpcTable) {
    ListIterator<Expr> it = conjuncts_.listIterator();
    while (it.hasNext()) {
        if (tryConvertKuduPredicate(analyzer, rpcTable, it.next()))
            it.remove();
    }
}
#end_block

#method_before
private static ComparisonOp getKuduOperator(BinaryPredicate.Operator op) {
    switch(op) {
        case GT:
            return ComparisonOp.GREATER;
        case LT:
            return ComparisonOp.LESS;
        case GE:
            return ComparisonOp.GREATER_EQUAL;
        case LE:
            return ComparisonOp.LESS_EQUAL;
        case EQ:
            return ComparisonOp.EQUAL;
        default:
            return null;
    }
}
#method_after
private static KuduPredicate.ComparisonOp getKuduOperator(BinaryPredicate.Operator op) {
    switch(op) {
        case GT:
            return ComparisonOp.GREATER;
        case LT:
            return ComparisonOp.LESS;
        case GE:
            return ComparisonOp.GREATER_EQUAL;
        case LE:
            return ComparisonOp.LESS_EQUAL;
        case EQ:
            return ComparisonOp.EQUAL;
        default:
            return null;
    }
}
#end_block

#method_before
@BeforeClass
public static void setUp() throws Exception {
    // Use 8 cores for resource estimation.
    RuntimeEnv.INSTANCE.setNumCores(8);
    // Set test env to control the explain level.
    RuntimeEnv.INSTANCE.setTestEnv(true);
    // Mimic the 3 node test mini-cluster.
    TUpdateMembershipRequest updateReq = new TUpdateMembershipRequest();
    updateReq.setIp_addresses(Sets.newHashSet("127.0.0.1"));
    updateReq.setHostnames(Sets.newHashSet("localhost"));
    updateReq.setNum_nodes(3);
    MembershipSnapshot.update(updateReq);
}
#method_after
@BeforeClass
public static void setUp() throws Exception {
    // Use 8 cores for resource estimation.
    RuntimeEnv.INSTANCE.setNumCores(8);
    // Set test env to control the explain level.
    RuntimeEnv.INSTANCE.setTestEnv(true);
    // Mimic the 3 node test mini-cluster.
    TUpdateMembershipRequest updateReq = new TUpdateMembershipRequest();
    updateReq.setIp_addresses(Sets.newHashSet("127.0.0.1"));
    updateReq.setHostnames(Sets.newHashSet("localhost"));
    updateReq.setNum_nodes(3);
    MembershipSnapshot.update(updateReq);
    if (RuntimeEnv.INSTANCE.isKuduSupported()) {
        kuduClient_ = new KuduClient.KuduClientBuilder("127.0.0.1:7051").build();
    }
}
#end_block

#method_before
@AfterClass
public static void cleanUp() {
    RuntimeEnv.INSTANCE.reset();
}
#method_after
@AfterClass
public static void cleanUp() throws Exception {
    RuntimeEnv.INSTANCE.reset();
    if (kuduClient_ != null) {
        kuduClient_.close();
        kuduClient_ = null;
    }
}
#end_block

#method_before
private StringBuilder printScanRangeLocations(TQueryExecRequest execRequest) {
    StringBuilder result = new StringBuilder();
    if (execRequest.per_node_scan_ranges == null) {
        return result;
    }
    for (Map.Entry<Integer, List<TScanRangeLocations>> entry : execRequest.per_node_scan_ranges.entrySet()) {
        result.append("NODE " + entry.getKey().toString() + ":\n");
        if (entry.getValue() == null) {
            continue;
        }
        for (TScanRangeLocations locations : entry.getValue()) {
            // print scan range
            result.append("  ");
            if (locations.scan_range.isSetHdfs_file_split()) {
                THdfsFileSplit split = locations.scan_range.getHdfs_file_split();
                THdfsTable table = findTable(entry.getKey());
                THdfsPartition partition = table.getPartitions().get(split.partition_id);
                THdfsPartitionLocation location = partition.getLocation();
                String file_location = location.getSuffix();
                if (location.prefix_index != -1) {
                    file_location = table.getPartition_prefixes().get(location.prefix_index) + file_location;
                }
                Path filePath = new Path(file_location, split.file_name);
                filePath = cleanseFilePath(filePath);
                result.append("HDFS SPLIT " + filePath.toString() + " " + Long.toString(split.offset) + ":" + Long.toString(split.length));
            }
            if (locations.scan_range.isSetHbase_key_range()) {
                THBaseKeyRange keyRange = locations.scan_range.getHbase_key_range();
                Integer hostIdx = locations.locations.get(0).host_idx;
                TNetworkAddress networkAddress = execRequest.getHost_list().get(hostIdx);
                result.append("HBASE KEYRANGE ");
                result.append("port=" + networkAddress.port + " ");
                if (keyRange.isSetStartKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStartKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
                result.append(":");
                if (keyRange.isSetStopKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStopKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
            }
            if (locations.scan_range.isSetKudu_scan_token()) {
                result.append("KUDU KEYRANGE ");
                ByteBuffer bb = locations.scan_range.kudu_scan_token;
                byte[] buf = new byte[bb.remaining()];
                bb.get(buf, 0, bb.remaining());
                ScanTokenPB token = null;
                try {
                    token = ScanTokenPB.parseFrom(buf);
                } catch (InvalidProtocolBufferException e) {
                    throw new IllegalStateException("Unable to parse Kudu scan token", e);
                }
                ByteString lowerBound = token.getLowerBoundPartitionKey();
                ByteString upperBound = token.getUpperBoundPartitionKey();
                result.append(Arrays.toString(lowerBound.toByteArray()));
                result.append(":");
                result.append(Arrays.toString(upperBound.toByteArray()));
            }
            result.append("\n");
        }
    }
    return result;
}
#method_after
private StringBuilder printScanRangeLocations(TQueryExecRequest execRequest) {
    StringBuilder result = new StringBuilder();
    if (execRequest.per_node_scan_ranges == null) {
        return result;
    }
    for (Map.Entry<Integer, List<TScanRangeLocations>> entry : execRequest.per_node_scan_ranges.entrySet()) {
        result.append("NODE " + entry.getKey().toString() + ":\n");
        if (entry.getValue() == null) {
            continue;
        }
        for (TScanRangeLocations locations : entry.getValue()) {
            // print scan range
            result.append("  ");
            if (locations.scan_range.isSetHdfs_file_split()) {
                THdfsFileSplit split = locations.scan_range.getHdfs_file_split();
                THdfsTable table = findTable(entry.getKey());
                THdfsPartition partition = table.getPartitions().get(split.partition_id);
                THdfsPartitionLocation location = partition.getLocation();
                String file_location = location.getSuffix();
                if (location.prefix_index != -1) {
                    file_location = table.getPartition_prefixes().get(location.prefix_index) + file_location;
                }
                Path filePath = new Path(file_location, split.file_name);
                filePath = cleanseFilePath(filePath);
                result.append("HDFS SPLIT " + filePath.toString() + " " + Long.toString(split.offset) + ":" + Long.toString(split.length));
            }
            if (locations.scan_range.isSetHbase_key_range()) {
                THBaseKeyRange keyRange = locations.scan_range.getHbase_key_range();
                Integer hostIdx = locations.locations.get(0).host_idx;
                TNetworkAddress networkAddress = execRequest.getHost_list().get(hostIdx);
                result.append("HBASE KEYRANGE ");
                result.append("port=" + networkAddress.port + " ");
                if (keyRange.isSetStartKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStartKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
                result.append(":");
                if (keyRange.isSetStopKey()) {
                    result.append(HBaseScanNode.printKey(keyRange.getStopKey().getBytes()));
                } else {
                    result.append("<unbounded>");
                }
            }
            if (locations.scan_range.isSetKudu_scan_token()) {
                Preconditions.checkNotNull(kuduClient_, "Test should not be invoked on platforms that do not support Kudu.");
                try {
                    result.append(KuduScanToken.stringifySerializedToken(locations.scan_range.kudu_scan_token.array(), kuduClient_));
                } catch (IOException e) {
                    throw new IllegalStateException("Unable to parse Kudu scan token", e);
                }
            }
            result.append("\n");
        }
    }
    return result;
}
#end_block

#method_before
@Override
public void configure(Context context) {
    this.upsert = context.getBoolean(UPSERT_PROP, DEFAULT_UPSERT);
    String schemaPath = context.getString(SCHEMA_PROP);
    if (schemaPath == null) {
        throw new FlumeException(String.format("Missing required parameter %s", SCHEMA_PROP));
    }
    Path path = new Path(schemaPath);
    Schema.Parser parser = new Schema.Parser();
    try {
        FileSystem fs = FileSystem.get(path.toUri(), new Configuration());
        reader = new GenericDatumReader<>(parser.parse(fs.open(path)));
    } catch (IOException e) {
        throw new FlumeException(String.format("Unable to open and parse schema file %s: %s", schemaPath, e.getMessage()), e);
    }
}
#method_after
@Override
public void configure(Context context) {
    this.operation = context.getString(OPERATION_PROP, DEFAULT_OPERATION);
    String schemaPath = context.getString(SCHEMA_PROP);
    if (schemaPath != null) {
        defaultSchemaURL = schemaPath;
    }
}
#end_block

#method_before
@Override
public void initialize(Event event, KuduTable table) {
    this.payload = event.getBody();
    this.table = table;
}
#method_after
@Override
public void initialize(Event event, KuduTable table) {
    this.payload = event.getBody();
    this.table = table;
    this.reader = readers.getUnchecked(getSchema(event));
}
#end_block

#method_before
@Override
public List<Operation> getOperations() throws FlumeException {
    List<Operation> ops = new ArrayList<>();
    DataFileReader<GenericRecord> payloadReader;
    try {
        payloadReader = new DataFileReader<>(new SeekableByteArrayInput(payload), reader);
    } catch (IOException e) {
        throw new FlumeException(String.format("Failed to open record reader for event: %s", e.getMessage()), e);
    }
    for (GenericRecord record : payloadReader) {
        Operation op = upsert ? table.newUpsert() : table.newInsert();
        setupOp(op, record);
        ops.add(op);
    }
    return ops;
}
#method_after
@Override
public List<Operation> getOperations() throws FlumeException {
    decoder = DecoderFactory.get().binaryDecoder(payload, decoder);
    try {
        reuse = reader.read(reuse, decoder);
    } catch (IOException e) {
        throw new FlumeException("Cannot deserialize event", e);
    }
    Operation op;
    switch(operation.toLowerCase()) {
        case "upsert":
            op = table.newUpsert();
            break;
        case "insert":
            op = table.newInsert();
            break;
        default:
            throw new FlumeException(String.format("Unexpected operation %s", operation));
    }
    setupOp(op, reuse);
    return Collections.singletonList(op);
}
#end_block

#method_before
private void setupOp(Operation op, GenericRecord record) {
    PartialRow row = op.getRow();
    for (ColumnSchema col : table.getSchema().getColumns()) {
        String name = col.getName();
        Object value = record.get(name);
        if (value == null) {
            if (col.isNullable()) {
                row.setNull(name);
            } else {
                throw new FlumeException(String.format("Column %s is not nullable but was decoded as null", name));
            }
        } else {
            // a larger type.
            switch(col.getType()) {
                case BOOL:
                    row.addBoolean(name, (boolean) value);
                    break;
                case INT8:
                    row.addByte(name, (byte) value);
                    break;
                case INT16:
                    row.addShort(name, (short) value);
                    break;
                case INT32:
                    row.addInt(name, (int) value);
                    break;
                // Fall through
                case INT64:
                case TIMESTAMP:
                    row.addLong(name, (long) value);
                    break;
                case FLOAT:
                    row.addFloat(name, (float) value);
                    break;
                case DOUBLE:
                    row.addDouble(name, (double) value);
                    break;
                case STRING:
                    row.addString(name, value.toString());
                    break;
                case BINARY:
                    row.addBinary(name, (byte[]) value);
                    break;
                default:
                    throw new FlumeException(String.format("Unrecognized type %s for column %s", col.getType().toString(), name));
            }
        }
    }
}
#method_after
private void setupOp(Operation op, GenericRecord record) {
    PartialRow row = op.getRow();
    for (ColumnSchema col : table.getSchema().getColumns()) {
        String name = col.getName();
        Object value = record.get(name);
        if (value == null) {
            if (col.isNullable()) {
                row.setNull(name);
            } else {
            // leave unset for possible Kudu default
            }
        } else {
            // a larger type.
            try {
                switch(col.getType()) {
                    case BOOL:
                        row.addBoolean(name, (boolean) value);
                        break;
                    case INT8:
                        row.addByte(name, (byte) value);
                        break;
                    case INT16:
                        row.addShort(name, (short) value);
                        break;
                    case INT32:
                        row.addInt(name, (int) value);
                        break;
                    // Fall through
                    case INT64:
                    case TIMESTAMP:
                        row.addLong(name, (long) value);
                        break;
                    case FLOAT:
                        row.addFloat(name, (float) value);
                        break;
                    case DOUBLE:
                        row.addDouble(name, (double) value);
                        break;
                    case STRING:
                        row.addString(name, value.toString());
                        break;
                    case BINARY:
                        row.addBinary(name, (byte[]) value);
                        break;
                    default:
                        throw new FlumeException(String.format("Unrecognized type %s for column %s", col.getType().toString(), name));
                }
            } catch (ClassCastException e) {
                throw new FlumeException(String.format("Failed to coerce value for column %s to type %s", col.getName(), col.getType()));
            }
        }
    }
}
#end_block

#method_before
@BeforeClass
public static void setupAvroSchemaBeforeClass() {
    org.apache.avro.Schema.Parser parser = new org.apache.avro.Schema.Parser();
    try {
        schema = parser.parse(new File(schemaPath));
    } catch (IOException e) {
        throw new FlumeException("Unable to open and parse schema file!", e);
    }
    writer = new DataFileWriter<>(new GenericDatumWriter<GenericRecord>(schema));
}
#method_after
@BeforeClass
public static void setupAvroSchemaBeforeClass() {
    org.apache.avro.Schema.Parser parser = new org.apache.avro.Schema.Parser();
    try {
        schemaLiteral = Files.toString(new File(schemaPath), Charsets.UTF_8);
        schema = parser.parse(new File(schemaPath));
    } catch (IOException e) {
        throw new FlumeException("Unable to open and parse schema file!", e);
    }
}
#end_block

#method_before
@Test
public void testEmptyChannel() throws Exception {
    testEvents(0, 1);
}
#method_after
@Test
public void testEmptyChannel() throws Exception {
    testEvents(0, SchemaLocation.GLOBAL);
}
#end_block

#method_before
@Test
public void testOneEvent() throws Exception {
    testEvents(1, 1);
}
#method_after
@Test
public void testOneEvent() throws Exception {
    testEvents(1, SchemaLocation.GLOBAL);
}
#end_block

#method_before
@Test
public void testThreeEvents() throws Exception {
    testEvents(3, 1);
}
#method_after
@Test
public void testThreeEvents() throws Exception {
    testEvents(3, SchemaLocation.GLOBAL);
}
#end_block

#method_before
private void testEvents(int eventCount, int recordsPerEvent) throws Exception {
    KuduTable table = createNewTable(String.format("test%sevents%srecordsper", eventCount, recordsPerEvent));
    String tableName = table.getName();
    Context ctx = new Context(ImmutableMap.of("producer.schema", schemaPath));
    KuduSink sink = createSink(tableName, ctx);
    Channel channel = new MemoryChannel();
    Configurables.configure(channel, new Context());
    sink.setChannel(channel);
    sink.start();
    Transaction tx = channel.getTransaction();
    tx.begin();
    writeEventsToChannel(channel, eventCount, recordsPerEvent);
    tx.commit();
    tx.close();
    Sink.Status status = sink.process();
    if (eventCount == 0) {
        assertTrue("incorrect status for empty channel", status == Sink.Status.BACKOFF);
    } else {
        assertTrue("incorrect status for non-empty channel", status != Sink.Status.BACKOFF);
    }
    List<String> answers = makeAnswers(eventCount, recordsPerEvent);
    List<String> rows = scanTableToStrings(table);
    assertEquals("wrong number of rows inserted", answers.size(), rows.size());
    assertArrayEquals("wrong rows inserted", answers.toArray(), rows.toArray());
}
#method_after
private void testEvents(int eventCount, SchemaLocation schemaLocation) throws Exception {
    KuduTable table = createNewTable(String.format("test%sevents%s", eventCount, schemaLocation));
    String tableName = table.getName();
    String schemaURI = new File(schemaPath).getAbsoluteFile().toURI().toString();
    Context ctx = schemaLocation != SchemaLocation.GLOBAL ? new Context() : new Context(ImmutableMap.of(PRODUCER_PREFIX + SCHEMA_PROP, schemaURI));
    KuduSink sink = createSink(tableName, ctx);
    Channel channel = new MemoryChannel();
    Configurables.configure(channel, new Context());
    sink.setChannel(channel);
    sink.start();
    Transaction tx = channel.getTransaction();
    tx.begin();
    writeEventsToChannel(channel, eventCount, schemaLocation);
    tx.commit();
    tx.close();
    Sink.Status status = sink.process();
    if (eventCount == 0) {
        assertEquals("incorrect status for empty channel", status, Sink.Status.BACKOFF);
    } else {
        assertEquals("incorrect status for non-empty channel", status, Sink.Status.READY);
    }
    List<String> answers = makeAnswers(eventCount);
    List<String> rows = scanTableToStrings(table);
    assertEquals("wrong number of rows inserted", answers.size(), rows.size());
    assertArrayEquals("wrong rows inserted", answers.toArray(), rows.toArray());
}
#end_block

#method_before
private KuduTable createNewTable(String tableName) throws Exception {
    ArrayList<ColumnSchema> columns = new ArrayList<>(3);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key", Type.INT32).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("long", Type.INT64).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("double", Type.DOUBLE).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("null", Type.STRING).nullable(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("string", Type.STRING).build());
    CreateTableOptions createOptions = new CreateTableOptions().setRangePartitionColumns(ImmutableList.of("key")).setNumReplicas(1);
    return createTable(tableName, new Schema(columns), createOptions);
}
#method_after
private KuduTable createNewTable(String tableName) throws Exception {
    ArrayList<ColumnSchema> columns = new ArrayList<>(5);
    columns.add(new ColumnSchema.ColumnSchemaBuilder("key", Type.INT32).key(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("longField", Type.INT64).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("doubleField", Type.DOUBLE).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("nullableField", Type.STRING).nullable(true).build());
    columns.add(new ColumnSchema.ColumnSchemaBuilder("stringField", Type.STRING).build());
    CreateTableOptions createOptions = new CreateTableOptions().setRangePartitionColumns(ImmutableList.of("key")).setNumReplicas(1);
    return createTable(tableName, new Schema(columns), createOptions);
}
#end_block

#method_before
private KuduSink createSink(String tableName, Context ctx) {
    KuduSink sink = new KuduSink(syncClient);
    HashMap<String, String> parameters = new HashMap<>();
    parameters.put(KuduSinkConfigurationConstants.TABLE_NAME, tableName);
    parameters.put(KuduSinkConfigurationConstants.MASTER_ADDRESSES, getMasterAddresses());
    parameters.put(KuduSinkConfigurationConstants.PRODUCER, "org.apache.kudu.flume.sink.AvroKuduEventProducer");
    Context context = new Context(parameters);
    context.putAll(ctx.getParameters());
    Configurables.configure(sink, context);
    return sink;
}
#method_after
private KuduSink createSink(String tableName, Context ctx) {
    KuduSink sink = new KuduSink(syncClient);
    HashMap<String, String> parameters = new HashMap<>();
    parameters.put(TABLE_NAME, tableName);
    parameters.put(MASTER_ADDRESSES, getMasterAddresses());
    parameters.put(PRODUCER, "org.apache.kudu.flume.sink.AvroKuduEventProducer");
    Context context = new Context(parameters);
    context.putAll(ctx.getParameters());
    Configurables.configure(sink, context);
    return sink;
}
#end_block

#method_before
private void writeEventsToChannel(Channel channel, int eventCount, int recordsPerEvent) throws Exception {
    for (int i = 0; i < eventCount; i++) {
        ByteArrayOutputStream payloadOS = new ByteArrayOutputStream();
        writer.create(schema, payloadOS);
        for (int j = 0; j < recordsPerEvent; j++) {
            GenericRecord record = new GenericData.Record(schema);
            record.put("key", 10 * i + j);
            record.put("long", 2L * i);
            record.put("double", 2.71828 * i);
            record.put("null", i % 2 == 0 ? null : "taco");
            record.put("string", String.format("hello %d", i));
            writer.append(record);
        }
        writer.flush();
        Event e = EventBuilder.withBody(payloadOS.toByteArray());
        channel.put(e);
        writer.close();
    }
}
#method_after
private void writeEventsToChannel(Channel channel, int eventCount, SchemaLocation schemaLocation) throws Exception {
    for (int i = 0; i < eventCount; i++) {
        AvroKuduEventProducerTestRecord record = new AvroKuduEventProducerTestRecord();
        record.setKey(10 * i);
        record.setLongField(2L * i);
        record.setDoubleField(2.71828 * i);
        record.setNullableField(i % 2 == 0 ? null : "taco");
        record.setStringField(String.format("hello %d", i));
        ByteArrayOutputStream out = new ByteArrayOutputStream();
        Encoder encoder = EncoderFactory.get().binaryEncoder(out, null);
        DatumWriter<AvroKuduEventProducerTestRecord> writer = new SpecificDatumWriter<>(AvroKuduEventProducerTestRecord.class);
        writer.write(record, encoder);
        encoder.flush();
        Event e = EventBuilder.withBody(out.toByteArray());
        if (schemaLocation == SchemaLocation.URL) {
            String schemaURI = new File(schemaPath).getAbsoluteFile().toURI().toString();
            e.setHeaders(ImmutableMap.of(SCHEMA_URL_HEADER, schemaURI));
        } else if (schemaLocation == SchemaLocation.LITERAL) {
            e.setHeaders(ImmutableMap.of(SCHEMA_LITERAL_HEADER, schemaLiteral));
        }
        channel.put(e);
    }
}
#end_block

#method_before
private List<String> makeAnswers(int eventCount, int recordsPerEvent) {
    List<String> answers = Lists.newArrayList();
    for (int i = 0; i < eventCount; i++) {
        for (int j = 0; j < recordsPerEvent; j++) {
            answers.add(String.format("INT32 key=%s, INT64 long=%s, DOUBLE double=%s, STRING null=%s, STRING string=hello %s", 10 * i + j, 2 * i, 2.71828 * i, i % 2 == 0 ? "NULL" : "taco", i));
        }
    }
    Collections.sort(answers);
    return answers;
}
#method_after
private List<String> makeAnswers(int eventCount) {
    List<String> answers = Lists.newArrayList();
    for (int i = 0; i < eventCount; i++) {
        answers.add(String.format("INT32 key=%s, INT64 longField=%s, DOUBLE doubleField=%s, " + "STRING nullableField=%s, STRING stringField=hello %s", 10 * i, 2 * i, 2.71828 * i, i % 2 == 0 ? "NULL" : "taco", i));
    }
    Collections.sort(answers);
    return answers;
}
#end_block

#method_before
private String splitRowToString(ArrayList<LiteralExpr> splitRow) {
    StringBuilder builder = new StringBuilder();
    builder.append("(");
    List<String> rangeElementStrings = Lists.newArrayList();
    for (LiteralExpr rangeElement : splitRow) {
        if (rangeElement instanceof StringLiteral) {
            rangeElementStrings.add("'" + rangeElement.getStringValue() + "'");
        } else {
            rangeElementStrings.add((rangeElement.getStringValue()));
        }
    }
    builder.append(Joiner.on(", ").join(rangeElementStrings));
    builder.append(")");
    return builder.toString();
}
#method_after
private String splitRowToString(ArrayList<LiteralExpr> splitRow) {
    StringBuilder builder = new StringBuilder();
    builder.append("(");
    List<String> rangeElementStrings = Lists.newArrayList();
    for (LiteralExpr rangeElement : splitRow) {
        rangeElementStrings.add(rangeElement.toSql());
    }
    builder.append(Joiner.on(", ").join(rangeElementStrings));
    builder.append(")");
    return builder.toString();
}
#end_block

#method_before
public void reset() throws CatalogException {
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        nextTableId_.set(0);
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                try {
                    List<org.apache.hadoop.hive.metastore.api.Function> javaFns = Lists.newArrayList();
                    for (String javaFn : msClient.getHiveClient().getFunctions(dbName, "*")) {
                        javaFns.add(msClient.getHiveClient().getFunction(dbName, javaFn));
                    }
                    org.apache.hadoop.hive.metastore.api.Database msDb = msClient.getHiveClient().getDatabase(dbName);
                    Db db = new Db(dbName, this, msDb);
                    // Restore UDFs that aren't persisted.
                    Db oldDb = oldDbCache.get(db.getName().toLowerCase());
                    if (oldDb != null) {
                        for (Function fn : oldDb.getTransientFunctions()) {
                            db.addFunction(fn);
                            fn.setCatalogVersion(incrementAndGetCatalogVersion());
                        }
                    }
                    loadFunctionsFromDbParams(db, msDb);
                    loadJavaFunctions(db, javaFns);
                    db.setCatalogVersion(incrementAndGetCatalogVersion());
                    newDbCache.put(db.getName().toLowerCase(), db);
                    for (String tableName : msClient.getHiveClient().getAllTables(dbName)) {
                        Table incompleteTbl = IncompleteTable.createUninitializedTable(getNextTableId(), db, tableName);
                        incompleteTbl.setCatalogVersion(incrementAndGetCatalogVersion());
                        db.addTable(incompleteTbl);
                        if (loadInBackground_) {
                            tblsToBackgroundLoad.add(new TTableName(dbName.toLowerCase(), tableName.toLowerCase()));
                        }
                    }
                } catch (NoSuchObjectException e) {
                    LOG.error("Error loading metadata for" + dbName);
                }
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#method_after
public void reset() throws CatalogException {
    // First update the policy metadata.
    if (sentryProxy_ != null) {
        // Sentry Service is enabled.
        try {
            // Update the authorization policy, waiting for the result to complete.
            sentryProxy_.refresh();
        } catch (Exception e) {
            throw new CatalogException("Error updating authorization policy: ", e);
        }
    }
    catalogLock_.writeLock().lock();
    try {
        nextTableId_.set(0);
        // Not all Java UDFs are persisted to the metastore. The ones which aren't
        // should be restored once the catalog has been invalidated.
        Map<String, Db> oldDbCache = dbCache_.get();
        // Build a new DB cache, populate it, and replace the existing cache in one
        // step.
        ConcurrentHashMap<String, Db> newDbCache = new ConcurrentHashMap<String, Db>();
        List<TTableName> tblsToBackgroundLoad = Lists.newArrayList();
        try (MetaStoreClient msClient = getMetaStoreClient()) {
            for (String dbName : msClient.getHiveClient().getAllDatabases()) {
                dbName = dbName.toLowerCase();
                Db oldDb = oldDbCache.get(dbName);
                Pair<Db, List<TTableName>> invalidatedDb = invalidateDb(msClient, dbName, oldDb);
                if (invalidatedDb == null)
                    continue;
                newDbCache.put(dbName, invalidatedDb.first);
                tblsToBackgroundLoad.addAll(invalidatedDb.second);
            }
        }
        dbCache_.set(newDbCache);
        // Submit tables for background loading.
        for (TTableName tblName : tblsToBackgroundLoad) {
            tableLoadingMgr_.backgroundLoad(tblName);
        }
    } catch (Exception e) {
        LOG.error(e);
        throw new CatalogException("Error initializing Catalog. Catalog may be empty.", e);
    } finally {
        catalogLock_.writeLock().unlock();
    }
}
#end_block

#method_before
public PlanFragment createInsertFragment(PlanFragment inputFragment, InsertStmt insertStmt, Analyzer analyzer, ArrayList<PlanFragment> fragments) throws ImpalaException {
    List<Expr> partitionExprs = Lists.newArrayList(insertStmt.getPartitionKeyExprs());
    if (partitionExprs.isEmpty())
        return inputFragment;
    Boolean partitionHint = insertStmt.isRepartition();
    if (partitionHint != null && !partitionHint)
        return inputFragment;
    // do nothing if the input fragment is already appropriately partitioned
    DataPartition inputPartition = inputFragment.getDataPartition();
    if (analyzer.equivSets(inputPartition.getPartitionExprs(), partitionExprs)) {
        return inputFragment;
    }
    // ignore constants for the sake of partitioning
    Expr.removeConstants(partitionExprs);
    // make a cost-based decision only if no user hint was supplied
    if (partitionHint == null) {
        // if it is distributed across all nodes; if so, don't repartition
        if (Expr.isSubset(inputPartition.getPartitionExprs(), partitionExprs)) {
            long numPartitions = getNumDistinctValues(inputPartition.getPartitionExprs());
            if (numPartitions >= inputFragment.getNumNodes())
                return inputFragment;
        }
        // don't repartition if we know we have fewer partitions than nodes
        // (ie, default to repartitioning if col stats are missing)
        // TODO: we want to repartition if the resulting files would otherwise
        // be very small (less than some reasonable multiple of the recommended block size);
        // in order to do that, we need to come up with an estimate of the avg row size
        // in the particular file format of the output table/partition.
        // We should always know on how many nodes our input is running.
        long numPartitions = getNumDistinctValues(partitionExprs);
        Preconditions.checkState(inputFragment.getNumNodes() != -1);
        if (numPartitions > 0 && numPartitions <= inputFragment.getNumNodes()) {
            return inputFragment;
        }
    }
    Preconditions.checkState(partitionHint == null || partitionHint);
    if (partitionExprs.isEmpty()) {
        // Shuffle hint was supplied with constant partition exprs. Merge results
        // at the coordinator and execute the table sink there.
        Preconditions.checkState(partitionHint != null);
        PlanFragment result = createMergeFragment(inputFragment);
        fragments.add(result);
        return result;
    }
    ExchangeNode exchNode = new ExchangeNode(ctx_.getNextNodeId(), inputFragment.getPlanRoot());
    exchNode.init(analyzer);
    Preconditions.checkState(exchNode.hasValidStats());
    DataPartition partition = DataPartition.hashPartitioned(partitionExprs);
    PlanFragment fragment = new PlanFragment(ctx_.getNextFragmentId(), exchNode, partition);
    inputFragment.setDestination(exchNode);
    inputFragment.setOutputPartition(partition);
    fragments.add(fragment);
    return fragment;
}
#method_after
public PlanFragment createInsertFragment(PlanFragment inputFragment, InsertStmt insertStmt, Analyzer analyzer, ArrayList<PlanFragment> fragments) throws ImpalaException {
    if (insertStmt.hasNoShuffleHint())
        return inputFragment;
    List<Expr> partitionExprs = Lists.newArrayList(insertStmt.getPartitionKeyExprs());
    // Ignore constants for the sake of partitioning.
    Expr.removeConstants(partitionExprs);
    // Do nothing if the input fragment is already appropriately partitioned.
    DataPartition inputPartition = inputFragment.getDataPartition();
    if (!partitionExprs.isEmpty() && analyzer.equivSets(inputPartition.getPartitionExprs(), partitionExprs)) {
        return inputFragment;
    }
    // Make a cost-based decision only if no user hint was supplied.
    if (!insertStmt.hasShuffleHint()) {
        // if it is distributed across all nodes. If so, don't repartition.
        if (Expr.isSubset(inputPartition.getPartitionExprs(), partitionExprs)) {
            long numPartitions = getNumDistinctValues(inputPartition.getPartitionExprs());
            if (numPartitions >= inputFragment.getNumNodes())
                return inputFragment;
        }
        // Don't repartition if we know we have fewer partitions than nodes
        // (ie, default to repartitioning if col stats are missing).
        // TODO: We want to repartition if the resulting files would otherwise
        // be very small (less than some reasonable multiple of the recommended block size).
        // In order to do that, we need to come up with an estimate of the avg row size
        // in the particular file format of the output table/partition.
        // We should always know on how many nodes our input is running.
        long numPartitions = getNumDistinctValues(partitionExprs);
        Preconditions.checkState(inputFragment.getNumNodes() != -1);
        if (numPartitions > 0 && numPartitions <= inputFragment.getNumNodes()) {
            return inputFragment;
        }
    }
    ExchangeNode exchNode = new ExchangeNode(ctx_.getNextNodeId(), inputFragment.getPlanRoot());
    exchNode.init(analyzer);
    Preconditions.checkState(exchNode.hasValidStats());
    DataPartition partition;
    if (partitionExprs.isEmpty()) {
        partition = DataPartition.UNPARTITIONED;
    } else {
        partition = DataPartition.hashPartitioned(partitionExprs);
    }
    PlanFragment fragment = new PlanFragment(ctx_.getNextFragmentId(), exchNode, partition);
    inputFragment.setDestination(exchNode);
    inputFragment.setOutputPartition(partition);
    fragments.add(fragment);
    return fragment;
}
#end_block

#method_before
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    LOG.debug("collecting partitions for table " + tbl_.getName());
    numPartitionsMissingStats_ = 0;
    totalFiles_ = 0;
    totalBytes_ = 0;
    if (tbl_.getNumClusteringCols() == 0) {
        cardinality_ = tbl_.getNumRows();
        if ((cardinality_ < -1) || (cardinality_ == 0 && tbl_.getTotalHdfsBytes() > 0)) {
            hasCorruptTableStats_ = true;
        }
        if (!partitions_.isEmpty()) {
            totalFiles_ += partitions_.get(0).getFileDescriptors().size();
            totalBytes_ += partitions_.get(0).getSize();
        } else {
            // Nothing to scan. Definitely a cardinality of 0 even if we have no stats.
            cardinality_ = 0;
        }
    } else {
        cardinality_ = 0;
        boolean hasValidPartitionCardinality = false;
        for (HdfsPartition p : partitions_) {
            // Check for corrupt table stats
            if ((p.getNumRows() < -1) || (p.getNumRows() == 0 && p.getSize() > 0)) {
                hasCorruptTableStats_ = true;
            }
            // enough to change the planning outcome
            if (p.getNumRows() > -1) {
                cardinality_ = addCardinalities(cardinality_, p.getNumRows());
                hasValidPartitionCardinality = true;
            } else {
                ++numPartitionsMissingStats_;
            }
            totalFiles_ += p.getFileDescriptors().size();
            totalBytes_ += p.getSize();
        }
        if (!partitions_.isEmpty() && !hasValidPartitionCardinality) {
            // if none of the partitions knew its number of rows, we fall back on
            // the table stats
            cardinality_ = tbl_.getNumRows();
        }
    }
    // Adjust cardinality for all collections referenced along the tuple's path.
    if (cardinality_ != -1) {
        for (Type t : desc_.getPath().getMatchedTypes()) {
            if (t.isCollectionType())
                cardinality_ *= PlannerContext.AVG_COLLECTION_SIZE;
        }
    }
    inputCardinality_ = cardinality_;
    // Sanity check scan node cardinality.
    if (!(cardinality_ >= 0 || cardinality_ == -1)) {
        hasCorruptTableStats_ = true;
        cardinality_ = -1;
    }
    if (cardinality_ > 0) {
        LOG.debug("cardinality_=" + Long.toString(cardinality_) + " sel=" + Double.toString(computeSelectivity()));
        cardinality_ = Math.round(cardinality_ * computeSelectivity());
        // IMPALA-2165: Avoid setting the cardinality to 0 after rounding.
        cardinality_ = Math.max(cardinality_, 1);
    }
    cardinality_ = capAtLimit(cardinality_);
    LOG.debug("computeStats HdfsScan: cardinality_=" + Long.toString(cardinality_));
    computeNumNodes(analyzer, cardinality_);
    LOG.debug("computeStats HdfsScan: #nodes=" + Integer.toString(numNodes_));
}
#method_after
@Override
public void computeStats(Analyzer analyzer) {
    super.computeStats(analyzer);
    LOG.debug("collecting partitions for table " + tbl_.getName());
    numPartitionsMissingStats_ = 0;
    totalFiles_ = 0;
    totalBytes_ = 0;
    if (tbl_.getNumClusteringCols() == 0) {
        cardinality_ = tbl_.getNumRows();
        if (cardinality_ < -1 || (cardinality_ == 0 && tbl_.getTotalHdfsBytes() > 0)) {
            hasCorruptTableStats_ = true;
        }
        if (partitions_.isEmpty()) {
            // Nothing to scan. Definitely a cardinality of 0 even if we have no stats.
            cardinality_ = 0;
        } else {
            Preconditions.checkState(partitions_.size() == 1);
            totalFiles_ += partitions_.get(0).getFileDescriptors().size();
            totalBytes_ += partitions_.get(0).getSize();
        }
    } else {
        cardinality_ = 0;
        boolean hasValidPartitionCardinality = false;
        for (HdfsPartition p : partitions_) {
            // Check for corrupt table stats
            if (p.getNumRows() < -1 || (p.getNumRows() == 0 && p.getSize() > 0)) {
                hasCorruptTableStats_ = true;
            }
            // enough to change the planning outcome
            if (p.getNumRows() > -1) {
                cardinality_ = addCardinalities(cardinality_, p.getNumRows());
                hasValidPartitionCardinality = true;
            } else {
                ++numPartitionsMissingStats_;
            }
            totalFiles_ += p.getFileDescriptors().size();
            totalBytes_ += p.getSize();
        }
        if (!partitions_.isEmpty() && !hasValidPartitionCardinality) {
            // if none of the partitions knew its number of rows, we fall back on
            // the table stats
            cardinality_ = tbl_.getNumRows();
        }
    }
    // Adjust cardinality for all collections referenced along the tuple's path.
    if (cardinality_ != -1) {
        for (Type t : desc_.getPath().getMatchedTypes()) {
            if (t.isCollectionType())
                cardinality_ *= PlannerContext.AVG_COLLECTION_SIZE;
        }
    }
    inputCardinality_ = cardinality_;
    // Sanity check scan node cardinality.
    if (cardinality_ < -1) {
        hasCorruptTableStats_ = true;
        cardinality_ = -1;
    }
    if (cardinality_ > 0) {
        LOG.debug("cardinality_=" + Long.toString(cardinality_) + " sel=" + Double.toString(computeSelectivity()));
        cardinality_ = Math.round(cardinality_ * computeSelectivity());
        // IMPALA-2165: Avoid setting the cardinality to 0 after rounding.
        cardinality_ = Math.max(cardinality_, 1);
    }
    cardinality_ = capAtLimit(cardinality_);
    LOG.debug("computeStats HdfsScan: cardinality_=" + Long.toString(cardinality_));
    computeNumNodes(analyzer, cardinality_);
    LOG.debug("computeStats HdfsScan: #nodes=" + Integer.toString(numNodes_));
}
#end_block

#method_before
private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment rightChildFragment, PlanFragment leftChildFragment, long perNodeMemLimit, ArrayList<PlanFragment> fragments) throws ImpalaException {
    // For both join types, the total cost is calculated as the amount of data
    // sent over the network, plus the amount of data inserted into the hash table.
    // broadcast: send the rightChildFragment's output to each node executing
    // the leftChildFragment, and build a hash table with it on each node.
    Analyzer analyzer = ctx_.getRootAnalyzer();
    PlanNode rhsTree = rightChildFragment.getPlanRoot();
    long rhsDataSize = 0;
    long broadcastCost = Long.MAX_VALUE;
    if (rhsTree.getCardinality() != -1 && leftChildFragment.getNumNodes() != -1) {
        rhsDataSize = Math.round(rhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(rhsTree));
        broadcastCost = 2 * rhsDataSize * leftChildFragment.getNumNodes();
    }
    LOG.debug("broadcast: cost=" + Long.toString(broadcastCost));
    LOG.debug("card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()) + " #nodes=" + Integer.toString(leftChildFragment.getNumNodes()));
    // repartition: both left- and rightChildFragment are partitioned on the
    // join exprs, and a hash table is built with the rightChildFragment's output.
    PlanNode lhsTree = leftChildFragment.getPlanRoot();
    long partitionCost = Long.MAX_VALUE;
    List<Expr> lhsJoinExprs = Lists.newArrayList();
    List<Expr> rhsJoinExprs = Lists.newArrayList();
    for (Expr joinConjunct : node.getEqJoinConjuncts()) {
        // no remapping necessary
        lhsJoinExprs.add(joinConjunct.getChild(0).clone());
        rhsJoinExprs.add(joinConjunct.getChild(1).clone());
    }
    boolean lhsHasCompatPartition = false;
    boolean rhsHasCompatPartition = false;
    if (lhsTree.getCardinality() != -1 && rhsTree.getCardinality() != -1) {
        lhsHasCompatPartition = analyzer.equivSets(lhsJoinExprs, leftChildFragment.getDataPartition().getPartitionExprs());
        rhsHasCompatPartition = analyzer.equivSets(rhsJoinExprs, rightChildFragment.getDataPartition().getPartitionExprs());
        double lhsCost = (lhsHasCompatPartition) ? 0.0 : Math.round(lhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(lhsTree));
        double rhsCost = (rhsHasCompatPartition) ? 0.0 : Math.round(rhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(rhsTree));
        partitionCost = Math.round(lhsCost + 2 * rhsCost);
    }
    LOG.debug("partition: cost=" + Long.toString(partitionCost));
    LOG.debug("lhs card=" + Long.toString(lhsTree.getCardinality()) + " row_size=" + Float.toString(lhsTree.getAvgRowSize()));
    LOG.debug("rhs card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()));
    LOG.debug(rhsTree.getExplainString());
    boolean doBroadcast = false;
    // we're unable to estimate the cost
    if ((node.getJoinOp() != JoinOperator.RIGHT_OUTER_JOIN && node.getJoinOp() != JoinOperator.FULL_OUTER_JOIN && node.getJoinOp() != JoinOperator.RIGHT_SEMI_JOIN && node.getJoinOp() != JoinOperator.RIGHT_ANTI_JOIN && // size is less than the pernode memlimit
    (node.getDistributionModeHint() == DistributionMode.BROADCAST || perNodeMemLimit == 0 || Math.round(rhsDataSize * PlannerContext.HASH_TBL_SPACE_OVERHEAD) <= perNodeMemLimit) && // join is more costly than a partitioned join
    (node.getDistributionModeHint() == DistributionMode.BROADCAST || (node.getDistributionModeHint() != DistributionMode.PARTITIONED && broadcastCost <= partitionCost))) || node.getJoinOp().isNullAwareLeftAntiJoin()) {
        doBroadcast = true;
    }
    PlanFragment hjFragment = null;
    if (doBroadcast) {
        node.setDistributionMode(HashJoinNode.DistributionMode.BROADCAST);
        // Doesn't create a new fragment, but modifies leftChildFragment to execute
        // the join; the build input is provided by an ExchangeNode, which is the
        // destination of the rightChildFragment's output
        node.setChild(0, leftChildFragment.getPlanRoot());
        connectChildFragment(node, 1, leftChildFragment, rightChildFragment);
        leftChildFragment.setPlanRoot(node);
        hjFragment = leftChildFragment;
    } else {
        hjFragment = createPartitionedHashJoinFragment(node, analyzer, lhsHasCompatPartition, rhsHasCompatPartition, leftChildFragment, rightChildFragment, lhsJoinExprs, rhsJoinExprs, fragments);
    }
    for (RuntimeFilter filter : node.getRuntimeFilters()) {
        filter.setIsBroadcast(doBroadcast);
        filter.computeHasLocalTargets();
        // Work around IMPALA-3450, where cardinalities might be wrong in single-node plans
        // with UNION and LIMITs.
        // TODO: Remove.
        filter.computeNdvEstimate();
    }
    return hjFragment;
}
#method_after
private PlanFragment createHashJoinFragment(HashJoinNode node, PlanFragment rightChildFragment, PlanFragment leftChildFragment, long perNodeMemLimit, ArrayList<PlanFragment> fragments) throws ImpalaException {
    // For both join types, the total cost is calculated as the amount of data
    // sent over the network, plus the amount of data inserted into the hash table.
    // broadcast: send the rightChildFragment's output to each node executing
    // the leftChildFragment, and build a hash table with it on each node.
    Analyzer analyzer = ctx_.getRootAnalyzer();
    PlanNode rhsTree = rightChildFragment.getPlanRoot();
    long rhsDataSize = 0;
    long broadcastCost = Long.MAX_VALUE;
    if (rhsTree.getCardinality() != -1) {
        rhsDataSize = Math.round(rhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(rhsTree));
        if (leftChildFragment.getNumNodes() != -1) {
            broadcastCost = 2 * rhsDataSize * leftChildFragment.getNumNodes();
        }
    }
    LOG.debug("broadcast: cost=" + Long.toString(broadcastCost));
    LOG.debug("card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()) + " #nodes=" + Integer.toString(leftChildFragment.getNumNodes()));
    // repartition: both left- and rightChildFragment are partitioned on the
    // join exprs, and a hash table is built with the rightChildFragment's output.
    PlanNode lhsTree = leftChildFragment.getPlanRoot();
    long partitionCost = Long.MAX_VALUE;
    List<Expr> lhsJoinExprs = Lists.newArrayList();
    List<Expr> rhsJoinExprs = Lists.newArrayList();
    for (Expr joinConjunct : node.getEqJoinConjuncts()) {
        // no remapping necessary
        lhsJoinExprs.add(joinConjunct.getChild(0).clone());
        rhsJoinExprs.add(joinConjunct.getChild(1).clone());
    }
    boolean lhsHasCompatPartition = false;
    boolean rhsHasCompatPartition = false;
    if (lhsTree.getCardinality() != -1 && rhsTree.getCardinality() != -1) {
        lhsHasCompatPartition = analyzer.equivSets(lhsJoinExprs, leftChildFragment.getDataPartition().getPartitionExprs());
        rhsHasCompatPartition = analyzer.equivSets(rhsJoinExprs, rightChildFragment.getDataPartition().getPartitionExprs());
        double lhsNetworkCost = (lhsHasCompatPartition) ? 0.0 : Math.round(lhsTree.getCardinality() * ExchangeNode.getAvgSerializedRowSize(lhsTree));
        double rhsNetworkCost = (rhsHasCompatPartition) ? 0.0 : rhsDataSize;
        partitionCost = Math.round(lhsNetworkCost + rhsNetworkCost + rhsDataSize);
    }
    LOG.debug("partition: cost=" + Long.toString(partitionCost));
    LOG.debug("lhs card=" + Long.toString(lhsTree.getCardinality()) + " row_size=" + Float.toString(lhsTree.getAvgRowSize()));
    LOG.debug("rhs card=" + Long.toString(rhsTree.getCardinality()) + " row_size=" + Float.toString(rhsTree.getAvgRowSize()));
    LOG.debug(rhsTree.getExplainString());
    boolean doBroadcast = false;
    // we're unable to estimate the cost
    if ((node.getJoinOp() != JoinOperator.RIGHT_OUTER_JOIN && node.getJoinOp() != JoinOperator.FULL_OUTER_JOIN && node.getJoinOp() != JoinOperator.RIGHT_SEMI_JOIN && node.getJoinOp() != JoinOperator.RIGHT_ANTI_JOIN && // size is less than the pernode memlimit
    (node.getDistributionModeHint() == DistributionMode.BROADCAST || perNodeMemLimit == 0 || Math.round(rhsDataSize * PlannerContext.HASH_TBL_SPACE_OVERHEAD) <= perNodeMemLimit) && // join is more costly than a partitioned join
    (node.getDistributionModeHint() == DistributionMode.BROADCAST || (node.getDistributionModeHint() != DistributionMode.PARTITIONED && broadcastCost <= partitionCost))) || node.getJoinOp().isNullAwareLeftAntiJoin()) {
        doBroadcast = true;
    }
    PlanFragment hjFragment = null;
    if (doBroadcast) {
        node.setDistributionMode(HashJoinNode.DistributionMode.BROADCAST);
        // Doesn't create a new fragment, but modifies leftChildFragment to execute
        // the join; the build input is provided by an ExchangeNode, which is the
        // destination of the rightChildFragment's output
        node.setChild(0, leftChildFragment.getPlanRoot());
        connectChildFragment(node, 1, leftChildFragment, rightChildFragment);
        leftChildFragment.setPlanRoot(node);
        hjFragment = leftChildFragment;
    } else {
        hjFragment = createPartitionedHashJoinFragment(node, analyzer, lhsHasCompatPartition, rhsHasCompatPartition, leftChildFragment, rightChildFragment, lhsJoinExprs, rhsJoinExprs, fragments);
    }
    for (RuntimeFilter filter : node.getRuntimeFilters()) {
        filter.setIsBroadcast(doBroadcast);
        filter.computeHasLocalTargets();
        // Work around IMPALA-3450, where cardinalities might be wrong in single-node plans
        // with UNION and LIMITs.
        // TODO: Remove.
        filter.computeNdvEstimate();
    }
    return hjFragment;
}
#end_block

#method_before
public boolean resolve() {
    if (isResolved_)
        return true;
    Preconditions.checkState(rootDesc_ != null || rootTable_ != null);
    Type currentType = null;
    int rawPathIdx = 0;
    if (rootPath_ != null) {
        // Continue resolving this path relative to the rootPath_.
        currentType = rootPath_.destType();
        rawPathIdx = rootPath_.getRawPath().size();
    } else if (rootDesc_ != null) {
        currentType = rootDesc_.getType();
    } else {
        // Directly start from the item type because only implicit paths are allowed.
        currentType = rootTable_.getType().getItemType();
    }
    // True if the next raw-path element must match explicitly.
    boolean expectExplicitMatch = false;
    // Map all remaining raw-path elements to field types and positions.
    while (rawPathIdx < rawPath_.size()) {
        if (!currentType.isComplexType())
            return false;
        StructType structType = getTypeAsStruct(currentType);
        // Resolve explicit path.
        StructField field = structType.getField(rawPath_.get(rawPathIdx));
        if (field == null) {
            // Resolve implicit path.
            if (!expectExplicitMatch && structType instanceof CollectionStructType) {
                field = ((CollectionStructType) structType).getOptionalField();
                // directly from an array reference.
                if (!structType.isMapType() && field.getType() instanceof MapType) {
                    return false;
                }
            } else {
                // Failed to resolve implicit or explicit path.
                return false;
            }
            // Update the physical types/positions.
            matchedTypes_.add(field.getType());
            matchedPositions_.add(field.getPosition());
            currentType = field.getType();
            // After an implicit match there must be an explicit match.
            expectExplicitMatch = true;
            // Do not consume a raw-path element.
            continue;
        }
        // After an explicit match we could have an implicit match again.
        expectExplicitMatch = false;
        matchedTypes_.add(field.getType());
        matchedPositions_.add(field.getPosition());
        if (field.getType().isCollectionType() && firstCollectionPathIdx_ == -1) {
            Preconditions.checkState(firstCollectionTypeIdx_ == -1);
            firstCollectionPathIdx_ = rawPathIdx;
            firstCollectionTypeIdx_ = matchedTypes_.size() - 1;
        }
        currentType = field.getType();
        ++rawPathIdx;
    }
    Preconditions.checkState(matchedTypes_.size() == matchedPositions_.size());
    Preconditions.checkState(matchedTypes_.size() >= rawPath_.size());
    isResolved_ = true;
    return true;
}
#method_after
public boolean resolve() {
    if (isResolved_)
        return true;
    Preconditions.checkState(rootDesc_ != null || rootTable_ != null);
    Type currentType = null;
    int rawPathIdx = 0;
    if (rootPath_ != null) {
        // Continue resolving this path relative to the rootPath_.
        currentType = rootPath_.destType();
        rawPathIdx = rootPath_.getRawPath().size();
    } else if (rootDesc_ != null) {
        currentType = rootDesc_.getType();
    } else {
        // Directly start from the item type because only implicit paths are allowed.
        currentType = rootTable_.getType().getItemType();
    }
    // Map all remaining raw-path elements to field types and positions.
    while (rawPathIdx < rawPath_.size()) {
        if (!currentType.isComplexType())
            return false;
        StructType structType = getTypeAsStruct(currentType);
        // Resolve explicit path.
        StructField field = structType.getField(rawPath_.get(rawPathIdx));
        if (field == null) {
            // Resolve implicit path.
            if (structType instanceof CollectionStructType) {
                field = ((CollectionStructType) structType).getOptionalField();
                // Collections must be matched explicitly.
                if (field.getType().isCollectionType())
                    return false;
            } else {
                // Failed to resolve implicit or explicit path.
                return false;
            }
            // Update the physical types/positions.
            matchedTypes_.add(field.getType());
            matchedPositions_.add(field.getPosition());
            currentType = field.getType();
            // Do not consume a raw-path element.
            continue;
        }
        matchedTypes_.add(field.getType());
        matchedPositions_.add(field.getPosition());
        if (field.getType().isCollectionType() && firstCollectionPathIdx_ == -1) {
            Preconditions.checkState(firstCollectionTypeIdx_ == -1);
            firstCollectionPathIdx_ = rawPathIdx;
            firstCollectionTypeIdx_ = matchedTypes_.size() - 1;
        }
        currentType = field.getType();
        ++rawPathIdx;
    }
    Preconditions.checkState(matchedTypes_.size() == matchedPositions_.size());
    Preconditions.checkState(matchedTypes_.size() >= rawPath_.size());
    isResolved_ = true;
    return true;
}
#end_block

#method_before
@SuppressWarnings("unchecked")
@Test
public void TestImplicitAndExplicitPaths() {
    // Check that there are no implicit field names for base tables.
    String[] implicitFieldNames = new String[] { Path.ARRAY_POS_FIELD_NAME, Path.ARRAY_ITEM_FIELD_NAME, Path.MAP_KEY_FIELD_NAME, Path.MAP_VALUE_FIELD_NAME };
    for (String field : implicitFieldNames) {
        AnalysisError(String.format("select %s from functional.alltypes", field), String.format("Could not resolve column/field reference: '%s'", field));
    }
    addTestDb("d", null);
    // Test array of scalars. Only explicit paths make sense.
    addTestTable("create table d.t1 (c array<int>)");
    testSlotRefPath("select item from d.t1.c", path(0, 0));
    testSlotRefPath("select pos from d.t1.c", path(0, 1));
    AnalysisError("select item.item from d.t1.c", "Could not resolve column/field reference: 'item.item'");
    AnalysisError("select item.pos from d.t1.c", "Could not resolve column/field reference: 'item.pos'");
    // Test star expansion.
    testStarPath("select * from d.t1.c", path(0, 0));
    testStarPath("select c.* from d.t1.c", path(0, 0));
    // Array of structs. No name conflicts with implicit fields. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t2 (c array<struct<f:int>>)");
    testSlotRefPath("select f from d.t2.c", path(0, 0, 0));
    testSlotRefPath("select item.f from d.t2.c", path(0, 0, 0));
    testSlotRefPath("select pos from d.t2.c", path(0, 1));
    AnalysisError("select item from d.t2.c", "Expr 'item' in select list returns a complex type 'STRUCT<f:INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("select item.pos from d.t2.c", "Could not resolve column/field reference: 'item.pos'");
    // Test star expansion.
    testStarPath("select * from d.t2.c", path(0, 0, 0));
    testStarPath("select c.* from d.t2.c", path(0, 0, 0));
    // Array of structs and maps with name conflicts. Both implicit and explicit
    // paths are allowed.
    addTestTable("create table d.t3 (c array<struct<f:int,item:int,pos:int>>, " + "d array<map<int, int>>)");
    testSlotRefPath("select f from d.t3.c", path(0, 0, 0));
    testSlotRefPath("select item.f from d.t3.c", path(0, 0, 0));
    testSlotRefPath("select item.item from d.t3.c", path(0, 0, 1));
    testSlotRefPath("select item.pos from d.t3.c", path(0, 0, 2));
    testSlotRefPath("select pos from d.t3.c", path(0, 1));
    AnalysisError("select item from d.t3.c", "Expr 'item' in select list returns a complex type " + "'STRUCT<f:INT,item:INT,pos:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test implicit key/value reference is not abused when using a map within an array.
    AnalysisError("select key from d.t3.d", "Could not resolve column/field reference: 'key'");
    AnalysisError("select value from d.t3.d", "Could not resolve column/field reference: 'value'");
    AnalysisError("select m.key from d.t3.d m", "Could not resolve column/field reference: 'm.key'");
    AnalysisError("select m.value from d.t3.d m", "Could not resolve column/field reference: 'm.value'");
    AnalysisError("select key from d.t3 t join t.d", "Could not resolve column/field reference: 'key'");
    AnalysisError("select value from d.t3 t join t.d", "Could not resolve column/field reference: 'value'");
    AnalysisError("select m.key from d.t3 t join t.d m", "Could not resolve column/field reference: 'm.key'");
    AnalysisError("select m.value from d.t3 t join t.d m", "Could not resolve column/field reference: 'm.value'");
    // Test star expansion.
    testStarPath("select * from d.t3.c", path(0, 0, 0), path(0, 0, 1), path(0, 0, 2));
    testStarPath("select c.* from d.t3.c", path(0, 0, 0), path(0, 0, 1), path(0, 0, 2));
    // Map with a scalar key and value. Only implicit paths make sense.
    addTestTable("create table d.t4 (c map<int,string>)");
    testSlotRefPath("select key from d.t4.c", path(0, 0));
    testSlotRefPath("select value from d.t4.c", path(0, 1));
    AnalysisError("select value.value from d.t4.c", "Could not resolve column/field reference: 'value.value'");
    // Test star expansion.
    testStarPath("select * from d.t4.c", path(0, 0), path(0, 1));
    testStarPath("select c.* from d.t4.c", path(0, 0), path(0, 1));
    // Map with a scalar key and struct value. No name conflicts. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t5 (c map<int,struct<f:int>>)");
    testSlotRefPath("select key from d.t5.c", path(0, 0));
    testSlotRefPath("select f from d.t5.c", path(0, 1, 0));
    testSlotRefPath("select value.f from d.t5.c", path(0, 1, 0));
    AnalysisError("select value.value from d.t5.c", "Could not resolve column/field reference: 'value.value'");
    AnalysisError("select value from d.t5.c", "Expr 'value' in select list returns a complex type " + "'STRUCT<f:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t5.c", path(0, 0), path(0, 1, 0));
    testStarPath("select c.* from d.t5.c", path(0, 0), path(0, 1, 0));
    // Map with a scalar key and struct value with name conflicts. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t6 (c map<int,struct<f:int,key:int,value:int>>)");
    testSlotRefPath("select key from d.t6.c", path(0, 0));
    testSlotRefPath("select f from d.t6.c", path(0, 1, 0));
    testSlotRefPath("select value.f from d.t6.c", path(0, 1, 0));
    testSlotRefPath("select value.key from d.t6.c", path(0, 1, 1));
    testSlotRefPath("select value.value from d.t6.c", path(0, 1, 2));
    AnalysisError("select value from d.t6.c", "Expr 'value' in select list returns a complex type " + "'STRUCT<f:INT,key:INT,value:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t6.c", path(0, 0), path(0, 1, 0), path(0, 1, 1), path(0, 1, 2));
    testStarPath("select c.* from d.t6.c", path(0, 0), path(0, 1, 0), path(0, 1, 1), path(0, 1, 2));
    // Test implicit/explicit paths on a complicated schema.
    addTestTable("create table d.t7 (" + "c1 int, " + "c2 decimal(10, 4), " + "c3 array<struct<a1:array<int>,a2:array<struct<x:int,y:int,a3:array<int>>>>>, " + "c4 bigint, " + "c5 map<int,struct<m1:map<int,string>," + "                  m2:map<int,struct<x:int,y:int,m3:map<int,int>>>>>)");
    // Test paths with c3.
    testTableRefPath("select 1 from d.t7.c3.a1", path(2, 0, 0), null);
    testTableRefPath("select 1 from d.t7.c3.item.a1", path(2, 0, 0), null);
    testSlotRefPath("select item from d.t7.c3.a1", path(2, 0, 0, 0));
    testSlotRefPath("select item from d.t7.c3.item.a1", path(2, 0, 0, 0));
    testTableRefPath("select 1 from d.t7.c3.a2", path(2, 0, 1), null);
    testTableRefPath("select 1 from d.t7.c3.item.a2", path(2, 0, 1), null);
    testSlotRefPath("select x from d.t7.c3.a2", path(2, 0, 1, 0, 0));
    testSlotRefPath("select x from d.t7.c3.item.a2", path(2, 0, 1, 0, 0));
    testTableRefPath("select 1 from d.t7.c3.a2.a3", path(2, 0, 1, 0, 2), null);
    testTableRefPath("select 1 from d.t7.c3.item.a2.item.a3", path(2, 0, 1, 0, 2), null);
    testSlotRefPath("select item from d.t7.c3.a2.a3", path(2, 0, 1, 0, 2, 0));
    testSlotRefPath("select item from d.t7.c3.item.a2.item.a3", path(2, 0, 1, 0, 2, 0));
    // Test path assembly with multiple tuple descriptors.
    testTableRefPath("select 1 from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 2), path(2, 0, 1, 0, 2));
    testTableRefPath("select 1 from d.t7, t7.c3, c3.item.a2, a2.item.a3", path(2, 0, 1, 0, 2), path(2, 0, 1, 0, 2));
    testSlotRefPath("select y from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 1));
    testSlotRefPath("select y, x from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 0));
    testSlotRefPath("select x, y from d.t7, t7.c3.item.a2, a2.a3", path(2, 0, 1, 0, 1));
    testSlotRefPath("select a1.item from d.t7, t7.c3, c3.a1, c3.a2, a2.a3", path(2, 0, 0, 0));
    // Test materialized path.
    testTableRefPath("select 1 from d.t7, t7.c3.a1", path(2, 0, 0), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3.a2", path(2, 0, 1), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3.a2.a3", path(2, 0, 1, 0, 2), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3, c3.a2.a3", path(2, 0, 1, 0, 2), path(2, 0, 1));
    // Test paths with c5.
    testTableRefPath("select 1 from d.t7.c5.m1", path(4, 1, 0), null);
    testTableRefPath("select 1 from d.t7.c5.value.m1", path(4, 1, 0), null);
    testSlotRefPath("select key from d.t7.c5.m1", path(4, 1, 0, 0));
    testSlotRefPath("select key from d.t7.c5.value.m1", path(4, 1, 0, 0));
    testSlotRefPath("select value from d.t7.c5.m1", path(4, 1, 0, 1));
    testSlotRefPath("select value from d.t7.c5.value.m1", path(4, 1, 0, 1));
    testTableRefPath("select 1 from d.t7.c5.m2", path(4, 1, 1), null);
    testTableRefPath("select 1 from d.t7.c5.value.m2", path(4, 1, 1), null);
    testSlotRefPath("select key from d.t7.c5.m2", path(4, 1, 1, 0));
    testSlotRefPath("select key from d.t7.c5.value.m2", path(4, 1, 1, 0));
    testSlotRefPath("select x from d.t7.c5.m2", path(4, 1, 1, 1, 0));
    testSlotRefPath("select x from d.t7.c5.value.m2", path(4, 1, 1, 1, 0));
    testTableRefPath("select 1 from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2), null);
    testTableRefPath("select 1 from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2), null);
    testSlotRefPath("select key from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2, 0));
    testSlotRefPath("select key from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2, 0));
    testSlotRefPath("select value from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2, 1));
    testSlotRefPath("select value from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2, 1));
    // Test path assembly with multiple tuple descriptors.
    testTableRefPath("select 1 from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 2), path(4, 1, 1, 1, 2));
    testTableRefPath("select 1 from d.t7, t7.c5, c5.value.m2, m2.value.m3", path(4, 1, 1, 1, 2), path(4, 1, 1, 1, 2));
    testSlotRefPath("select y from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 1));
    testSlotRefPath("select y, x from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 0));
    testSlotRefPath("select x, y from d.t7, t7.c5.value.m2, m2.m3", path(4, 1, 1, 1, 1));
    testSlotRefPath("select m1.key from d.t7, t7.c5, c5.m1, c5.m2, m2.m3", path(4, 1, 0, 0));
    // Test materialized path.
    testTableRefPath("select 1 from d.t7, t7.c5.m1", path(4, 1, 0), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5.m2", path(4, 1, 1), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5.m2.m3", path(4, 1, 1, 1, 2), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5, c5.m2.m3", path(4, 1, 1, 1, 2), path(4, 1, 1));
    // Tests that an attempted implicit match must be succeeded by an explicit match.
    addTestTable("create table d.t8 (" + "s1 struct<" + "  s2:struct<" + "    a:array<" + "      array<struct<" + "        e:int,f:string>>>>>)");
    // Explanation of test:
    // - d.t8.s1.s2.a resolves to a CollectionStructType with fields 'item' and 'pos'
    // - we are allowed to implicitly skip the 'item' field
    // - d.t8.s1.s2.a.item again resolves to a CollectionStructType with 'item' and 'pos'
    // - however, we are not allowed to implicitly skip 'item' again, since we have
    // already skipped 'item' previously
    // - the rule is: an implicit match must be followed by an explicit one
    AnalysisError("select f from d.t8.s1.s2.a", "Could not resolve column/field reference: 'f'");
    AnalysisError("select 1 from d.t8.s1.s2.a, a.f", "Could not resolve table reference: 'a.f'");
}
#method_after
@SuppressWarnings("unchecked")
@Test
public void TestImplicitAndExplicitPaths() {
    // Check that there are no implicit field names for base tables.
    String[] implicitFieldNames = new String[] { Path.ARRAY_POS_FIELD_NAME, Path.ARRAY_ITEM_FIELD_NAME, Path.MAP_KEY_FIELD_NAME, Path.MAP_VALUE_FIELD_NAME };
    for (String field : implicitFieldNames) {
        AnalysisError(String.format("select %s from functional.alltypes", field), String.format("Could not resolve column/field reference: '%s'", field));
    }
    addTestDb("d", null);
    // Test array of scalars. Only explicit paths make sense.
    addTestTable("create table d.t1 (c array<int>)");
    testSlotRefPath("select item from d.t1.c", path(0, 0));
    testSlotRefPath("select pos from d.t1.c", path(0, 1));
    AnalysisError("select item.item from d.t1.c", "Could not resolve column/field reference: 'item.item'");
    AnalysisError("select item.pos from d.t1.c", "Could not resolve column/field reference: 'item.pos'");
    // Test star expansion.
    testStarPath("select * from d.t1.c", path(0, 0));
    testStarPath("select c.* from d.t1.c", path(0, 0));
    // Array of structs. No name conflicts with implicit fields. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t2 (c array<struct<f:int>>)");
    testSlotRefPath("select f from d.t2.c", path(0, 0, 0));
    testSlotRefPath("select item.f from d.t2.c", path(0, 0, 0));
    testSlotRefPath("select pos from d.t2.c", path(0, 1));
    AnalysisError("select item from d.t2.c", "Expr 'item' in select list returns a complex type 'STRUCT<f:INT>'.\n" + "Only scalar types are allowed in the select list.");
    AnalysisError("select item.pos from d.t2.c", "Could not resolve column/field reference: 'item.pos'");
    // Test star expansion.
    testStarPath("select * from d.t2.c", path(0, 0, 0));
    testStarPath("select c.* from d.t2.c", path(0, 0, 0));
    // Array of structs with name conflicts. Both implicit and explicit
    // paths are allowed.
    addTestTable("create table d.t3 (c array<struct<f:int,item:int,pos:int>>)");
    testSlotRefPath("select f from d.t3.c", path(0, 0, 0));
    testSlotRefPath("select item.f from d.t3.c", path(0, 0, 0));
    testSlotRefPath("select item.item from d.t3.c", path(0, 0, 1));
    testSlotRefPath("select item.pos from d.t3.c", path(0, 0, 2));
    testSlotRefPath("select pos from d.t3.c", path(0, 1));
    AnalysisError("select item from d.t3.c", "Expr 'item' in select list returns a complex type " + "'STRUCT<f:INT,item:INT,pos:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t3.c", path(0, 0, 0), path(0, 0, 1), path(0, 0, 2));
    testStarPath("select c.* from d.t3.c", path(0, 0, 0), path(0, 0, 1), path(0, 0, 2));
    // Map with a scalar key and value. Only implicit paths make sense.
    addTestTable("create table d.t4 (c map<int,string>)");
    testSlotRefPath("select key from d.t4.c", path(0, 0));
    testSlotRefPath("select value from d.t4.c", path(0, 1));
    AnalysisError("select value.value from d.t4.c", "Could not resolve column/field reference: 'value.value'");
    // Test star expansion.
    testStarPath("select * from d.t4.c", path(0, 0), path(0, 1));
    testStarPath("select c.* from d.t4.c", path(0, 0), path(0, 1));
    // Map with a scalar key and struct value. No name conflicts. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t5 (c map<int,struct<f:int>>)");
    testSlotRefPath("select key from d.t5.c", path(0, 0));
    testSlotRefPath("select f from d.t5.c", path(0, 1, 0));
    testSlotRefPath("select value.f from d.t5.c", path(0, 1, 0));
    AnalysisError("select value.value from d.t5.c", "Could not resolve column/field reference: 'value.value'");
    AnalysisError("select value from d.t5.c", "Expr 'value' in select list returns a complex type " + "'STRUCT<f:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t5.c", path(0, 0), path(0, 1, 0));
    testStarPath("select c.* from d.t5.c", path(0, 0), path(0, 1, 0));
    // Map with a scalar key and struct value with name conflicts. Both implicit and
    // explicit paths are allowed.
    addTestTable("create table d.t6 (c map<int,struct<f:int,key:int,value:int>>)");
    testSlotRefPath("select key from d.t6.c", path(0, 0));
    testSlotRefPath("select f from d.t6.c", path(0, 1, 0));
    testSlotRefPath("select value.f from d.t6.c", path(0, 1, 0));
    testSlotRefPath("select value.key from d.t6.c", path(0, 1, 1));
    testSlotRefPath("select value.value from d.t6.c", path(0, 1, 2));
    AnalysisError("select value from d.t6.c", "Expr 'value' in select list returns a complex type " + "'STRUCT<f:INT,key:INT,value:INT>'.\n" + "Only scalar types are allowed in the select list.");
    // Test star expansion.
    testStarPath("select * from d.t6.c", path(0, 0), path(0, 1, 0), path(0, 1, 1), path(0, 1, 2));
    testStarPath("select c.* from d.t6.c", path(0, 0), path(0, 1, 0), path(0, 1, 1), path(0, 1, 2));
    // Test implicit/explicit paths on a complicated schema.
    addTestTable("create table d.t7 (" + "c1 int, " + "c2 decimal(10, 4), " + "c3 array<struct<a1:array<int>,a2:array<struct<x:int,y:int,a3:array<int>>>>>, " + "c4 bigint, " + "c5 map<int,struct<m1:map<int,string>," + "                  m2:map<int,struct<x:int,y:int,m3:map<int,int>>>>>)");
    // Test paths with c3.
    testTableRefPath("select 1 from d.t7.c3.a1", path(2, 0, 0), null);
    testTableRefPath("select 1 from d.t7.c3.item.a1", path(2, 0, 0), null);
    testSlotRefPath("select item from d.t7.c3.a1", path(2, 0, 0, 0));
    testSlotRefPath("select item from d.t7.c3.item.a1", path(2, 0, 0, 0));
    testTableRefPath("select 1 from d.t7.c3.a2", path(2, 0, 1), null);
    testTableRefPath("select 1 from d.t7.c3.item.a2", path(2, 0, 1), null);
    testSlotRefPath("select x from d.t7.c3.a2", path(2, 0, 1, 0, 0));
    testSlotRefPath("select x from d.t7.c3.item.a2", path(2, 0, 1, 0, 0));
    testTableRefPath("select 1 from d.t7.c3.a2.a3", path(2, 0, 1, 0, 2), null);
    testTableRefPath("select 1 from d.t7.c3.item.a2.item.a3", path(2, 0, 1, 0, 2), null);
    testSlotRefPath("select item from d.t7.c3.a2.a3", path(2, 0, 1, 0, 2, 0));
    testSlotRefPath("select item from d.t7.c3.item.a2.item.a3", path(2, 0, 1, 0, 2, 0));
    // Test path assembly with multiple tuple descriptors.
    testTableRefPath("select 1 from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 2), path(2, 0, 1, 0, 2));
    testTableRefPath("select 1 from d.t7, t7.c3, c3.item.a2, a2.item.a3", path(2, 0, 1, 0, 2), path(2, 0, 1, 0, 2));
    testSlotRefPath("select y from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 1));
    testSlotRefPath("select y, x from d.t7, t7.c3, c3.a2, a2.a3", path(2, 0, 1, 0, 0));
    testSlotRefPath("select x, y from d.t7, t7.c3.item.a2, a2.a3", path(2, 0, 1, 0, 1));
    testSlotRefPath("select a1.item from d.t7, t7.c3, c3.a1, c3.a2, a2.a3", path(2, 0, 0, 0));
    // Test materialized path.
    testTableRefPath("select 1 from d.t7, t7.c3.a1", path(2, 0, 0), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3.a2", path(2, 0, 1), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3.a2.a3", path(2, 0, 1, 0, 2), path(2));
    testTableRefPath("select 1 from d.t7, t7.c3, c3.a2.a3", path(2, 0, 1, 0, 2), path(2, 0, 1));
    // Test paths with c5.
    testTableRefPath("select 1 from d.t7.c5.m1", path(4, 1, 0), null);
    testTableRefPath("select 1 from d.t7.c5.value.m1", path(4, 1, 0), null);
    testSlotRefPath("select key from d.t7.c5.m1", path(4, 1, 0, 0));
    testSlotRefPath("select key from d.t7.c5.value.m1", path(4, 1, 0, 0));
    testSlotRefPath("select value from d.t7.c5.m1", path(4, 1, 0, 1));
    testSlotRefPath("select value from d.t7.c5.value.m1", path(4, 1, 0, 1));
    testTableRefPath("select 1 from d.t7.c5.m2", path(4, 1, 1), null);
    testTableRefPath("select 1 from d.t7.c5.value.m2", path(4, 1, 1), null);
    testSlotRefPath("select key from d.t7.c5.m2", path(4, 1, 1, 0));
    testSlotRefPath("select key from d.t7.c5.value.m2", path(4, 1, 1, 0));
    testSlotRefPath("select x from d.t7.c5.m2", path(4, 1, 1, 1, 0));
    testSlotRefPath("select x from d.t7.c5.value.m2", path(4, 1, 1, 1, 0));
    testTableRefPath("select 1 from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2), null);
    testTableRefPath("select 1 from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2), null);
    testSlotRefPath("select key from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2, 0));
    testSlotRefPath("select key from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2, 0));
    testSlotRefPath("select value from d.t7.c5.m2.m3", path(4, 1, 1, 1, 2, 1));
    testSlotRefPath("select value from d.t7.c5.value.m2.value.m3", path(4, 1, 1, 1, 2, 1));
    // Test path assembly with multiple tuple descriptors.
    testTableRefPath("select 1 from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 2), path(4, 1, 1, 1, 2));
    testTableRefPath("select 1 from d.t7, t7.c5, c5.value.m2, m2.value.m3", path(4, 1, 1, 1, 2), path(4, 1, 1, 1, 2));
    testSlotRefPath("select y from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 1));
    testSlotRefPath("select y, x from d.t7, t7.c5, c5.m2, m2.m3", path(4, 1, 1, 1, 0));
    testSlotRefPath("select x, y from d.t7, t7.c5.value.m2, m2.m3", path(4, 1, 1, 1, 1));
    testSlotRefPath("select m1.key from d.t7, t7.c5, c5.m1, c5.m2, m2.m3", path(4, 1, 0, 0));
    // Test materialized path.
    testTableRefPath("select 1 from d.t7, t7.c5.m1", path(4, 1, 0), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5.m2", path(4, 1, 1), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5.m2.m3", path(4, 1, 1, 1, 2), path(4));
    testTableRefPath("select 1 from d.t7, t7.c5, c5.m2.m3", path(4, 1, 1, 1, 2), path(4, 1, 1));
    // Tests that implicit references are not allowed through collection types.
    addTestTable("create table d.t8 (" + "c1 array<map<string, string>>," + "c2 map<string, array<struct<a:int>>>," + "c3 struct<s1:struct<a:array<array<struct<e:int, f:string>>>>>)");
    testImplicitPathFailure("d.t8", true, "c1", "key", "value");
    testImplicitPathFailure("d.t8", true, "c2", "pos");
    testImplicitPathFailure("d.t8.c3.s1", false, "a", "f");
}
#end_block

#method_before
@Test
public void TestCreateKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Create Kudu Table with all required properties
    AnalyzesOk("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // Check that all properties are present
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    // Check that properties are not empty
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'=''," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='asd'," + "'kudu.master_addresses' = '', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    // Don't allow caching
    AnalysisError("create table tab (x int) cached in 'testPool' " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "A Kudu table cannot be cached in HDFS.");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b) into 8 buckets, hash(c) into 2 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash into 8 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a'" + ")", "A data distribution must be specified using a DISTRIBUTE BY clause.");
    // DISTRIBUTE BY is not required for external tables.
    AnalyzesOk("create external table tab (a int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a'" + ")");
    // Number of buckets must be larger 1
    AnalysisError("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b) into 8 buckets, hash(c) into 1 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Number of buckets in DISTRIBUTE BY clause 'HASH(c) INTO 1 BUCKETS' must " + "be larger than 1");
    // Key ranges must match the column types.
    // TODO(kudu-merge) uncomment this when IMPALA-3156 is addressed.
    // AnalysisError("create table tab (a int, b int, c int, d int) " +
    // "distribute by hash(a,b,c) into 8 buckets, " +
    // "range(a) split rows ((1),('abc'),(3)) " +
    // "tblproperties (" +
    // "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " +
    // "'kudu.table_name'='tab'," +
    // "'kudu.master_addresses' = '127.0.0.1:8080', " +
    // "'kudu.key_columns' = 'a,b,c')");
    // Distribute range data types are picked up during analysis and forwarded to Kudu
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b,c) into 8 buckets, " + "range(a) split rows ((1),(2),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c')");
    // No float split keys
    AnalysisError("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b,c) into 8 buckets, " + "range(a) split rows ((1.2),('abc'),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Only integral and string values allowed for split rows.");
}
#method_after
@Test
public void TestCreateKuduTable() {
    TestUtils.assumeKuduIsSupported();
    // Create Kudu Table with all required properties
    AnalyzesOk("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080, 127.0.0.1:8081', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // Check that all properties are present
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    // Check that properties are not empty
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'=''," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    AnalysisError("create table tab (x int) " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='asd'," + "'kudu.master_addresses' = '', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Kudu table is missing parameters in table properties. Please verify " + "if kudu.table_name, kudu.master_addresses, and kudu.key_columns are " + "present and have valid values.");
    // Don't allow caching
    AnalysisError("create table tab (x int) cached in 'testPool' " + "distribute by hash into 2 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "A Kudu table cannot be cached in HDFS.");
    // Flexible Partitioning
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b) into 8 buckets, hash(c) into 2 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash into 8 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")");
    // DISTRIBUTE BY is required for managed tables.
    AnalysisError("create table tab (a int) tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a'" + ")", "A data distribution must be specified using the DISTRIBUTE BY clause.");
    // DISTRIBUTE BY is not allowed for external tables.
    AnalysisError("create external table tab (a int) " + "distribute by hash into 3 buckets tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a'" + ")", "The DISTRIBUTE BY clause may not be specified for external tables.");
    // Number of buckets must be larger 1
    AnalysisError("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b) into 8 buckets, hash(c) into 1 buckets " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Number of buckets in DISTRIBUTE BY clause 'HASH(c) INTO 1 BUCKETS' must " + "be larger than 1");
    // Key ranges must match the column types.
    // TODO(kudu-merge) uncomment this when IMPALA-3156 is addressed.
    // AnalysisError("create table tab (a int, b int, c int, d int) " +
    // "distribute by hash(a,b,c) into 8 buckets, " +
    // "range(a) split rows ((1),('abc'),(3)) " +
    // "tblproperties (" +
    // "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " +
    // "'kudu.table_name'='tab'," +
    // "'kudu.master_addresses' = '127.0.0.1:8080', " +
    // "'kudu.key_columns' = 'a,b,c')");
    // Distribute range data types are picked up during analysis and forwarded to Kudu
    AnalyzesOk("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b,c) into 8 buckets, " + "range(a) split rows ((1),(2),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c')");
    // No float split keys
    AnalysisError("create table tab (a int, b int, c int, d int) " + "distribute by hash(a,b,c) into 8 buckets, " + "range(a) split rows ((1.2),('abc'),(3)) " + "tblproperties (" + "'storage_handler'='com.cloudera.kudu.hive.KuduStorageHandler', " + "'kudu.table_name'='tab'," + "'kudu.master_addresses' = '127.0.0.1:8080', " + "'kudu.key_columns' = 'a,b,c'" + ")", "Only integral and string values allowed for split rows.");
}
#end_block

#method_before
private void analyzeKuduTable(Analyzer analyzer) throws AnalysisException {
    // Validate that Kudu table is correctly specified.
    if (!KuduTable.tableParamsAreValid(getTblProperties())) {
        throw new AnalysisException("Kudu table is missing parameters " + String.format("in table properties. Please verify if %s, %s, and %s are " + "present and have valid values.", KuduTable.KEY_TABLE_NAME, KuduTable.KEY_MASTER_ADDRESSES, KuduTable.KEY_KEY_COLUMNS));
    }
    // Kudu table cannot be a cached table
    if (cachingOp_ != null) {
        throw new AnalysisException("A Kudu table cannot be cached in HDFS.");
    }
    if (distributeParams_ != null) {
        List<String> keyColumns = KuduUtil.parseKeyColumnsAsList(getTblProperties().get(KuduTable.KEY_KEY_COLUMNS));
        for (DistributeParam d : distributeParams_) {
            // If the columns are not set, default to all key columns
            if (d.getColumns() == null)
                d.setColumns(keyColumns);
            d.analyze(analyzer);
        }
    } else if (!isExternal_) {
        throw new AnalysisException("A data distribution must be specified using a DISTRIBUTE BY clause.");
    }
}
#method_after
private void analyzeKuduTable(Analyzer analyzer) throws AnalysisException {
    // Validate that Kudu table is correctly specified.
    if (!KuduTable.tableParamsAreValid(getTblProperties())) {
        throw new AnalysisException("Kudu table is missing parameters " + String.format("in table properties. Please verify if %s, %s, and %s are " + "present and have valid values.", KuduTable.KEY_TABLE_NAME, KuduTable.KEY_MASTER_ADDRESSES, KuduTable.KEY_KEY_COLUMNS));
    }
    // Kudu table cannot be a cached table
    if (cachingOp_ != null) {
        throw new AnalysisException("A Kudu table cannot be cached in HDFS.");
    }
    if (distributeParams_ != null) {
        if (isExternal_) {
            throw new AnalysisException("The DISTRIBUTE BY clause may not be specified for external tables.");
        }
        List<String> keyColumns = KuduUtil.parseKeyColumnsAsList(getTblProperties().get(KuduTable.KEY_KEY_COLUMNS));
        for (DistributeParam d : distributeParams_) {
            // If the columns are not set, default to all key columns
            if (d.getColumns() == null)
                d.setColumns(keyColumns);
            d.analyze(analyzer);
        }
    } else if (!isExternal_) {
        throw new AnalysisException("A data distribution must be specified using the DISTRIBUTE BY clause.");
    }
}
#end_block

#method_before
protected Table addTestView(String createViewSql) {
    CreateViewStmt createVieweStmt = (CreateViewStmt) AnalyzesOk(createViewSql);
    Db db = catalog_.getDb(createVieweStmt.getDb());
    Preconditions.checkNotNull(db, "Test views must be created in an existing db.");
    QueryStmt viewStmt = (QueryStmt) AnalyzesOk(createVieweStmt.getInlineViewDef());
    View dummyView = View.createTestView(db, createVieweStmt.getTbl(), viewStmt);
    db.addTable(dummyView);
    testTables_.add(dummyView);
    return dummyView;
}
#method_after
protected Table addTestView(String createViewSql) {
    CreateViewStmt createViewStmt = (CreateViewStmt) AnalyzesOk(createViewSql);
    Db db = catalog_.getDb(createViewStmt.getDb());
    Preconditions.checkNotNull(db, "Test views must be created in an existing db.");
    QueryStmt viewStmt = (QueryStmt) AnalyzesOk(createViewStmt.getInlineViewDef());
    View dummyView = View.createTestView(db, createViewStmt.getTbl(), viewStmt);
    db.addTable(dummyView);
    testTables_.add(dummyView);
    return dummyView;
}
#end_block

#method_before
public Deferred<AlterTableResponse> alterTable(String name, AlterTableOptions ato) {
    checkIsClosed();
    AlterTableRequest alter = new AlterTableRequest(this.masterTable, name, ato);
    alter.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    Deferred<AlterTableResponse> response = sendRpcToTablet(alter);
    if (ato.hasAddDropRangePartitions()) {
        // Clear the table locations cache so the new partition is immediately visible.
        Callback clearCacheCB = new Callback() {

            @Override
            public Object call(Object resp) throws Exception {
                // case the alter table operation actually succeeded.
                if (resp instanceof AlterTableResponse) {
                    AlterTableResponse response = (AlterTableResponse) resp;
                    String tableId = response.getTableId();
                    if (tableId != null) {
                        tableLocations.remove(tableId);
                    } else {
                        tableLocations.clear();
                    }
                } else {
                    tableLocations.clear();
                }
                return resp;
            }

            @Override
            public String toString() {
                return "ClearTableLocationsCacheCB";
            }
        };
        return response.addCallback(clearCacheCB).addErrback(clearCacheCB);
    }
    return response;
}
#method_after
public Deferred<AlterTableResponse> alterTable(String name, AlterTableOptions ato) {
    checkIsClosed();
    AlterTableRequest alter = new AlterTableRequest(this.masterTable, name, ato);
    alter.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    Deferred<AlterTableResponse> response = sendRpcToTablet(alter);
    if (ato.hasAddDropRangePartitions()) {
        // Clear the table locations cache so the new partition is immediately visible.
        return response.addCallback(new Callback<AlterTableResponse, AlterTableResponse>() {

            @Override
            public AlterTableResponse call(AlterTableResponse resp) {
                // Otherwise, we clear the caches for all tables.
                if (resp.getTableId() != null) {
                    tableLocations.remove(resp.getTableId());
                } else {
                    tableLocations.clear();
                }
                return resp;
            }

            @Override
            public String toString() {
                return "ClearTableLocationsCacheCB";
            }
        }).addErrback(new Callback<Exception, Exception>() {

            @Override
            public Exception call(Exception e) {
                // We clear the cache even on failure, just in
                // case the alter table operation actually succeeded.
                tableLocations.clear();
                return e;
            }

            @Override
            public String toString() {
                return "ClearTableLocationsCacheEB";
            }
        });
    }
    return response;
}
#end_block

#method_before
@Test(timeout = 100000)
public void testAlterTable() throws Exception {
    String tableName = name.getMethodName();
    createTable(tableName, basicSchema, getBasicCreateTableOptions());
    try {
        // Add a col.
        AlterTableOptions ato = new AlterTableOptions().addColumn("testaddint", Type.INT32, 4);
        submitAlterAndCheck(ato, tableName);
        // Rename that col.
        ato = new AlterTableOptions().renameColumn("testaddint", "newtestaddint");
        submitAlterAndCheck(ato, tableName);
        // Delete it.
        ato = new AlterTableOptions().dropColumn("newtestaddint");
        submitAlterAndCheck(ato, tableName);
        String newTableName = tableName + "new";
        // Rename our table.
        ato = new AlterTableOptions().renameTable(newTableName);
        submitAlterAndCheck(ato, tableName, newTableName);
        // Rename it back.
        ato = new AlterTableOptions().renameTable(tableName);
        submitAlterAndCheck(ato, newTableName, tableName);
        // Try adding two columns, where one is nullable.
        ato = new AlterTableOptions().addColumn("testaddmulticolnotnull", Type.INT32, 4).addNullableColumn("testaddmulticolnull", Type.STRING);
        submitAlterAndCheck(ato, tableName);
        // Try altering a table that doesn't exist.
        String nonExistingTableName = "table_does_not_exist";
        try {
            syncClient.alterTable(nonExistingTableName, ato);
            fail("Shouldn't be able to alter a table that doesn't exist");
        } catch (KuduException ex) {
            assertTrue(ex.getStatus().isNotFound());
        }
        try {
            syncClient.isAlterTableDone(nonExistingTableName);
            fail("Shouldn't be able to query if an alter table is done here");
        } catch (KuduException ex) {
            assertTrue(ex.getStatus().isNotFound());
        }
    } finally {
        // Normally Java tests accumulate tables without issue, deleting them all
        // when shutting down the mini cluster at the end of every test class.
        // However, testGetLocations below expects a certain table count, so
        // we'll delete our table to ensure there's no interaction between them.
        syncClient.deleteTable(tableName);
    }
}
#method_after
@Test(timeout = 100000)
public void testAlterTable() throws Exception {
    String tableName = name.getMethodName() + System.currentTimeMillis();
    createTable(tableName, basicSchema, getBasicCreateTableOptions());
    try {
        // Add a col.
        AlterTableOptions ato = new AlterTableOptions().addColumn("testaddint", Type.INT32, 4);
        submitAlterAndCheck(ato, tableName);
        // Rename that col.
        ato = new AlterTableOptions().renameColumn("testaddint", "newtestaddint");
        submitAlterAndCheck(ato, tableName);
        // Delete it.
        ato = new AlterTableOptions().dropColumn("newtestaddint");
        submitAlterAndCheck(ato, tableName);
        String newTableName = tableName + "new";
        // Rename our table.
        ato = new AlterTableOptions().renameTable(newTableName);
        submitAlterAndCheck(ato, tableName, newTableName);
        // Rename it back.
        ato = new AlterTableOptions().renameTable(tableName);
        submitAlterAndCheck(ato, newTableName, tableName);
        // Try adding two columns, where one is nullable.
        ato = new AlterTableOptions().addColumn("testaddmulticolnotnull", Type.INT32, 4).addNullableColumn("testaddmulticolnull", Type.STRING);
        submitAlterAndCheck(ato, tableName);
        // Try altering a table that doesn't exist.
        String nonExistingTableName = "table_does_not_exist";
        try {
            syncClient.alterTable(nonExistingTableName, ato);
            fail("Shouldn't be able to alter a table that doesn't exist");
        } catch (KuduException ex) {
            assertTrue(ex.getStatus().isNotFound());
        }
        try {
            syncClient.isAlterTableDone(nonExistingTableName);
            fail("Shouldn't be able to query if an alter table is done here");
        } catch (KuduException ex) {
            assertTrue(ex.getStatus().isNotFound());
        }
    } finally {
        // Normally Java tests accumulate tables without issue, deleting them all
        // when shutting down the mini cluster at the end of every test class.
        // However, testGetLocations below expects a certain table count, so
        // we'll delete our table to ensure there's no interaction between them.
        syncClient.deleteTable(tableName);
    }
}
#end_block

#method_before
@Test(timeout = 100000)
public void testLocateTableNonCoveringRange() throws Exception {
    String tableName = name.getMethodName();
    syncClient.createTable(tableName, basicSchema, getBasicTableOptionsWithNonCoveredRange());
    KuduTable table = syncClient.openTable(tableName);
    List<LocatedTablet> tablets;
    // all tablets
    tablets = table.getTabletsLocations(null, null, 100000);
    assertEquals(3, tablets.size());
    assertArrayEquals(getKeyInBytes(0), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(50), tablets.get(0).getPartition().getPartitionKeyEnd());
    assertArrayEquals(getKeyInBytes(50), tablets.get(1).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(100), tablets.get(1).getPartition().getPartitionKeyEnd());
    assertArrayEquals(getKeyInBytes(200), tablets.get(2).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(300), tablets.get(2).getPartition().getPartitionKeyEnd());
    // key < 50
    tablets = table.getTabletsLocations(null, getKeyInBytes(50), 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(0), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(50), tablets.get(0).getPartition().getPartitionKeyEnd());
    // key >= 300
    tablets = table.getTabletsLocations(getKeyInBytes(300), null, 100000);
    assertEquals(0, tablets.size());
    // key >= 299
    tablets = table.getTabletsLocations(getKeyInBytes(299), null, 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(200), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(300), tablets.get(0).getPartition().getPartitionKeyEnd());
    // key >= 150 && key < 250
    tablets = table.getTabletsLocations(getKeyInBytes(150), getKeyInBytes(250), 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(200), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(300), tablets.get(0).getPartition().getPartitionKeyEnd());
}
#method_after
@Test(timeout = 100000)
public void testLocateTableNonCoveringRange() throws Exception {
    String tableName = name.getMethodName() + System.currentTimeMillis();
    syncClient.createTable(tableName, basicSchema, getBasicTableOptionsWithNonCoveredRange());
    KuduTable table = syncClient.openTable(tableName);
    List<LocatedTablet> tablets;
    // all tablets
    tablets = table.getTabletsLocations(null, null, 100000);
    assertEquals(3, tablets.size());
    assertArrayEquals(getKeyInBytes(0), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(50), tablets.get(0).getPartition().getPartitionKeyEnd());
    assertArrayEquals(getKeyInBytes(50), tablets.get(1).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(100), tablets.get(1).getPartition().getPartitionKeyEnd());
    assertArrayEquals(getKeyInBytes(200), tablets.get(2).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(300), tablets.get(2).getPartition().getPartitionKeyEnd());
    // key < 50
    tablets = table.getTabletsLocations(null, getKeyInBytes(50), 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(0), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(50), tablets.get(0).getPartition().getPartitionKeyEnd());
    // key >= 300
    tablets = table.getTabletsLocations(getKeyInBytes(300), null, 100000);
    assertEquals(0, tablets.size());
    // key >= 299
    tablets = table.getTabletsLocations(getKeyInBytes(299), null, 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(200), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(300), tablets.get(0).getPartition().getPartitionKeyEnd());
    // key >= 150 && key < 250
    tablets = table.getTabletsLocations(getKeyInBytes(150), getKeyInBytes(250), 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(200), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(300), tablets.get(0).getPartition().getPartitionKeyEnd());
}
#end_block

#method_before
@Test(timeout = 100000)
public void testAlterTableNonCoveringRange() throws Exception {
    String tableName = name.getMethodName();
    syncClient.createTable(tableName, basicSchema, getBasicTableOptionsWithNonCoveredRange());
    KuduTable table = syncClient.openTable(tableName);
    KuduSession session = syncClient.newSession();
    AlterTableOptions ato = new AlterTableOptions();
    PartialRow bLowerBound = schema.newPartialRow();
    bLowerBound.addInt("key", 300);
    PartialRow bUpperBound = schema.newPartialRow();
    bUpperBound.addInt("key", 400);
    ato.addRangePartition(bLowerBound, bUpperBound);
    syncClient.alterTable(tableName, ato);
    Insert insert = createBasicSchemaInsert(table, 301);
    session.apply(insert);
    List<LocatedTablet> tablets;
    // all tablets
    tablets = table.getTabletsLocations(getKeyInBytes(300), null, 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(300), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(400), tablets.get(0).getPartition().getPartitionKeyEnd());
    insert = createBasicSchemaInsert(table, 201);
    session.apply(insert);
    ato = new AlterTableOptions();
    bLowerBound = schema.newPartialRow();
    bLowerBound.addInt("key", 200);
    bUpperBound = schema.newPartialRow();
    bUpperBound.addInt("key", 300);
    ato.dropRangePartition(bLowerBound, bUpperBound);
    syncClient.alterTable(tableName, ato);
    insert = createBasicSchemaInsert(table, 202);
    try {
        session.apply(insert);
        fail("Should get a non-recoverable");
    } catch (NonCoveredRangeException e) {
    // Expected.
    }
}
#method_after
@Test(timeout = 100000)
public void testAlterTableNonCoveringRange() throws Exception {
    String tableName = name.getMethodName() + System.currentTimeMillis();
    syncClient.createTable(tableName, basicSchema, getBasicTableOptionsWithNonCoveredRange());
    KuduTable table = syncClient.openTable(tableName);
    KuduSession session = syncClient.newSession();
    AlterTableOptions ato = new AlterTableOptions();
    PartialRow bLowerBound = schema.newPartialRow();
    bLowerBound.addInt("key", 300);
    PartialRow bUpperBound = schema.newPartialRow();
    bUpperBound.addInt("key", 400);
    ato.addRangePartition(bLowerBound, bUpperBound);
    syncClient.alterTable(tableName, ato);
    Insert insert = createBasicSchemaInsert(table, 301);
    session.apply(insert);
    List<LocatedTablet> tablets;
    // all tablets
    tablets = table.getTabletsLocations(getKeyInBytes(300), null, 100000);
    assertEquals(1, tablets.size());
    assertArrayEquals(getKeyInBytes(300), tablets.get(0).getPartition().getPartitionKeyStart());
    assertArrayEquals(getKeyInBytes(400), tablets.get(0).getPartition().getPartitionKeyEnd());
    insert = createBasicSchemaInsert(table, 201);
    session.apply(insert);
    ato = new AlterTableOptions();
    bLowerBound = schema.newPartialRow();
    bLowerBound.addInt("key", 200);
    bUpperBound = schema.newPartialRow();
    bUpperBound.addInt("key", 300);
    ato.dropRangePartition(bLowerBound, bUpperBound);
    syncClient.alterTable(tableName, ato);
    insert = createBasicSchemaInsert(table, 202);
    try {
        session.apply(insert);
        fail("Should get a non-recoverable");
    } catch (NonCoveredRangeException e) {
    // Expected.
    }
}
#end_block

#method_before
public Deferred<AlterTableResponse> alterTable(String name, AlterTableOptions ato) {
    checkIsClosed();
    AlterTableRequest alter = new AlterTableRequest(this.masterTable, name, ato);
    alter.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    Deferred<AlterTableResponse> response = sendRpcToTablet(alter);
    if (ato.hasAddDropRangePartitions()) {
        // Clear the caches so the new partition is immediately visible.
        Callback clearCacheCB = new Callback() {

            @Override
            public Object call(Object resp) throws Exception {
                tableLocations.clear();
                tablet2client.clear();
                return resp;
            }

            @Override
            public String toString() {
                return "ClearCacheCB";
            }
        };
        return response.addCallback(clearCacheCB).addErrback(clearCacheCB);
    }
    return response;
}
#method_after
public Deferred<AlterTableResponse> alterTable(String name, AlterTableOptions ato) {
    checkIsClosed();
    AlterTableRequest alter = new AlterTableRequest(this.masterTable, name, ato);
    alter.setTimeoutMillis(defaultAdminOperationTimeoutMs);
    Deferred<AlterTableResponse> response = sendRpcToTablet(alter);
    if (ato.hasAddDropRangePartitions()) {
        // Clear the table locations cache so the new partition is immediately visible.
        Callback clearCacheCB = new Callback() {

            @Override
            public Object call(Object resp) throws Exception {
                tableLocations.clear();
                return resp;
            }

            @Override
            public String toString() {
                return "ClearCacheCB";
            }
        };
        return response.addCallback(clearCacheCB).addErrback(clearCacheCB);
    }
    return response;
}
#end_block

#method_before
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws NonRecoverableException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary CSLMs because in
    // the most common case the table should already be present
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        // Early creating the tablet so that it parses out the pb
        RemoteTablet rt = createTabletFromPb(tableId, tabletPb);
        Slice tabletId = rt.tabletId;
        // If we already know about this one, just refresh the locations
        RemoteTablet currentTablet = tablet2client.get(tabletId);
        if (currentTablet != null) {
            currentTablet.refreshTabletClients(tabletPb);
            tablets.add(currentTablet);
            continue;
        }
        // Putting it here first doesn't make it visible because tabletsCache is always looked up
        // first.
        RemoteTablet oldRt = tablet2client.putIfAbsent(tabletId, rt);
        if (oldRt != null) {
            // someone beat us to it
            continue;
        }
        LOG.info("Discovered tablet {} for table '{}' with partition {}", tabletId.toString(Charset.defaultCharset()), tableName, rt.getPartition());
        rt.refreshTabletClients(tabletPb);
        // This is making this tablet available
        // Even if two clients were racing in this method they are putting the same RemoteTablet
        // with the same start key in the CSLM in the end
        tablets.add(rt);
    }
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
}
#method_after
@VisibleForTesting
void discoverTablets(KuduTable table, byte[] requestPartitionKey, List<Master.TabletLocationsPB> locations, long ttl) throws NonRecoverableException {
    String tableId = table.getTableId();
    String tableName = table.getName();
    // Doing a get first instead of putIfAbsent to avoid creating unnecessary
    // table locations caches because in the most common case the table should
    // already be present.
    TableLocationsCache locationsCache = tableLocations.get(tableId);
    if (locationsCache == null) {
        locationsCache = new TableLocationsCache();
        TableLocationsCache existingLocationsCache = tableLocations.putIfAbsent(tableId, locationsCache);
        if (existingLocationsCache != null) {
            locationsCache = existingLocationsCache;
        }
    }
    // Build the list of discovered remote tablet instances. If we have
    // already discovered the tablet, its locations are refreshed.
    List<RemoteTablet> tablets = new ArrayList<>(locations.size());
    for (Master.TabletLocationsPB tabletPb : locations) {
        // Early creating the tablet so that it parses out the pb.
        RemoteTablet rt = createTabletFromPb(tableId, tabletPb);
        Slice tabletId = rt.tabletId;
        // If we already know about this tablet, refresh the locations.
        RemoteTablet currentTablet = tablet2client.get(tabletId);
        if (currentTablet != null) {
            currentTablet.refreshTabletClients(tabletPb);
            tablets.add(currentTablet);
            continue;
        }
        // Putting it here first doesn't make it visible because tabletsCache is always looked up
        // first.
        RemoteTablet oldRt = tablet2client.putIfAbsent(tabletId, rt);
        if (oldRt != null) {
            // someone beat us to it
            continue;
        }
        LOG.info("Discovered tablet {} for table '{}' with partition {}", tabletId.toString(Charset.defaultCharset()), tableName, rt.getPartition());
        rt.refreshTabletClients(tabletPb);
        tablets.add(rt);
    }
    // Give the locations to the tablet location cache for the table, so that it
    // can cache them and discover non-covered ranges.
    locationsCache.cacheTabletLocations(tablets, requestPartitionKey, ttl);
}
#end_block

#method_before
public AlterTableOptions addRangePartition(PartialRow lowerBound, PartialRow upperBound) {
    Preconditions.checkNotNull(lowerBound);
    Preconditions.checkNotNull(upperBound);
    Preconditions.checkArgument(lowerBound.getSchema().equals(upperBound.getSchema()));
    AlterTableRequestPB.Step.Builder step = pb.addAlterSchemaStepsBuilder();
    step.setType(AlterTableRequestPB.StepType.ADD_RANGE_PARTITION);
    AlterTableRequestPB.AddRangePartition.Builder builder = AlterTableRequestPB.AddRangePartition.newBuilder();
    builder.setRangeBounds(new Operation.OperationsEncoder().encodeLowerAndUpperBounds(lowerBound, upperBound));
    step.setAddRangePartition(builder);
    if (!pb.hasSchema()) {
        pb.setSchema(ProtobufHelper.schemaToPb(lowerBound.getSchema()));
    }
    hasAddDropRangePartitions = true;
    return this;
}
#method_after
public AlterTableOptions addRangePartition(PartialRow lowerBound, PartialRow upperBound) {
    Preconditions.checkNotNull(lowerBound);
    Preconditions.checkNotNull(upperBound);
    Preconditions.checkArgument(lowerBound.getSchema().equals(upperBound.getSchema()));
    AlterTableRequestPB.Step.Builder step = pb.addAlterSchemaStepsBuilder();
    step.setType(AlterTableRequestPB.StepType.ADD_RANGE_PARTITION);
    AlterTableRequestPB.AddRangePartition.Builder builder = AlterTableRequestPB.AddRangePartition.newBuilder();
    builder.setRangeBounds(new Operation.OperationsEncoder().encodeLowerAndUpperBounds(lowerBound, upperBound));
    step.setAddRangePartition(builder);
    if (!pb.hasSchema()) {
        pb.setSchema(ProtobufHelper.schemaToPb(lowerBound.getSchema()));
    }
    return this;
}
#end_block

#method_before
public AlterTableOptions dropRangePartition(PartialRow lowerBound, PartialRow upperBound) {
    Preconditions.checkNotNull(lowerBound);
    Preconditions.checkNotNull(upperBound);
    Preconditions.checkArgument(lowerBound.getSchema().equals(upperBound.getSchema()));
    AlterTableRequestPB.Step.Builder step = pb.addAlterSchemaStepsBuilder();
    step.setType(AlterTableRequestPB.StepType.DROP_RANGE_PARTITION);
    AlterTableRequestPB.DropRangePartition.Builder builder = AlterTableRequestPB.DropRangePartition.newBuilder();
    builder.setRangeBounds(new Operation.OperationsEncoder().encodeLowerAndUpperBounds(lowerBound, upperBound));
    step.setDropRangePartition(builder);
    if (!pb.hasSchema()) {
        pb.setSchema(ProtobufHelper.schemaToPb(lowerBound.getSchema()));
    }
    hasAddDropRangePartitions = true;
    return this;
}
#method_after
public AlterTableOptions dropRangePartition(PartialRow lowerBound, PartialRow upperBound) {
    Preconditions.checkNotNull(lowerBound);
    Preconditions.checkNotNull(upperBound);
    Preconditions.checkArgument(lowerBound.getSchema().equals(upperBound.getSchema()));
    AlterTableRequestPB.Step.Builder step = pb.addAlterSchemaStepsBuilder();
    step.setType(AlterTableRequestPB.StepType.DROP_RANGE_PARTITION);
    AlterTableRequestPB.DropRangePartition.Builder builder = AlterTableRequestPB.DropRangePartition.newBuilder();
    builder.setRangeBounds(new Operation.OperationsEncoder().encodeLowerAndUpperBounds(lowerBound, upperBound));
    step.setDropRangePartition(builder);
    if (!pb.hasSchema()) {
        pb.setSchema(ProtobufHelper.schemaToPb(lowerBound.getSchema()));
    }
    return this;
}
#end_block

#method_before
@InterfaceAudience.Private
boolean hasAddDropRangePartitions() {
    return hasAddDropRangePartitions;
}
#method_after
@InterfaceAudience.Private
boolean hasAddDropRangePartitions() {
    return pb.hasSchema();
}
#end_block

#method_before
public void cacheTabletLocations(List<RemoteTablet> tablets, byte[] requestPartitionKey, long ttl) {
    if (requestPartitionKey == null) {
        // Master lookup.
        Preconditions.checkArgument(tablets.size() == 1);
        Entry entry = Entry.tablet(tablets.get(0), TimeUnit.DAYS.toMillis(1));
        synchronized (monitor) {
            entries.clear();
            entries.put(AsyncKuduClient.EMPTY_ARRAY, entry);
        }
        return;
    }
    List<Entry> newEntries = new ArrayList<>();
    if (tablets.isEmpty()) {
        // If there are no tablets in the response, then the table is empty. If
        // there were any tablets in the table they would have been returned, since
        // the master guarantees that if the partition key falls in a non-covered
        // range, the previous tablet will be returned, and we did not set an upper
        // bound partition key on the request.
        newEntries.add(Entry.nonCoveredRange(AsyncKuduClient.EMPTY_ARRAY, AsyncKuduClient.EMPTY_ARRAY, ttl));
    } else {
        // The comments below will reference the following diagram:
        // 
        // +---+   +---+---+
        // |   |   |   |   |
        // A | B | C | D | E | F
        // |   |   |   |   |
        // +---+   +---+---+
        // 
        // It depicts a tablet locations response from the master containing three
        // tablets: B, D and E. Three non-covered ranges are present: A, C, and F.
        // An RPC response containing B, D and E could occur if the lookup partition
        // key falls in A, B, or C, although the existence of A as an initial
        // non-covered range can only be inferred if the lookup partition key falls
        // in A.
        final byte[] firstLowerBound = tablets.get(0).getPartition().getPartitionKeyStart();
        if (Bytes.memcmp(requestPartitionKey, firstLowerBound) < 0) {
            // If the first tablet is past the requested partition key, then the
            // partition key falls in an initial non-covered range, such as A.
            newEntries.add(Entry.nonCoveredRange(AsyncKuduClient.EMPTY_ARRAY, firstLowerBound, ttl));
        }
        // lastUpperBound tracks the upper bound of the previously processed
        // entry, so that we can determine when we have found a non-covered range.
        byte[] lastUpperBound = firstLowerBound;
        for (RemoteTablet tablet : tablets) {
            final byte[] tabletLowerBound = tablet.getPartition().getPartitionKeyStart();
            final byte[] tabletUpperBound = tablet.getPartition().getPartitionKeyEnd();
            if (Bytes.memcmp(lastUpperBound, tabletLowerBound) < 0) {
                // There is a non-covered range between the previous tablet and this tablet.
                // This will discover C while processing the tablet location for D.
                newEntries.add(Entry.nonCoveredRange(lastUpperBound, tabletLowerBound, ttl));
            }
            lastUpperBound = tabletUpperBound;
            // Now add the tablet itself (such as B, D, or E).
            newEntries.add(Entry.tablet(tablet, ttl));
        }
        if (lastUpperBound.length > 0 && tablets.size() < AsyncKuduClient.MAX_RETURNED_TABLE_LOCATIONS) {
            // There is a non-covered range between the last tablet and the end of the
            // partition key space, such as F.
            newEntries.add(Entry.nonCoveredRange(lastUpperBound, AsyncKuduClient.EMPTY_ARRAY, ttl));
        }
    }
    byte[] discoveredlowerBound = newEntries.get(0).getLowerBoundPartitionKey();
    byte[] discoveredUpperBound = newEntries.get(newEntries.size() - 1).getUpperBoundPartitionKey();
    LOG.debug("Discovered table locations:\t{}", newEntries);
    synchronized (monitor) {
        // Remove all existing overlapping entries, and add the new entries.
        Map.Entry<byte[], Entry> floorEntry = entries.floorEntry(discoveredlowerBound);
        if (floorEntry != null && Bytes.memcmp(requestPartitionKey, floorEntry.getValue().getUpperBoundPartitionKey()) < 0) {
            discoveredlowerBound = floorEntry.getKey();
        }
        NavigableMap<byte[], Entry> overlappingEntries = entries.tailMap(discoveredlowerBound, true);
        if (discoveredUpperBound.length > 0) {
            overlappingEntries = overlappingEntries.headMap(discoveredUpperBound, false);
        }
        if (LOG.isTraceEnabled()) {
            LOG.trace("Existing table locations:\t\t{}", entries.values());
            LOG.trace("Removing table locations:\t\t{}", overlappingEntries.values());
        }
        overlappingEntries.clear();
        for (Entry entry : newEntries) {
            entries.put(entry.getLowerBoundPartitionKey(), entry);
        }
    }
}
#method_after
public void cacheTabletLocations(List<RemoteTablet> tablets, byte[] requestPartitionKey, long ttl) {
    long deadline = System.nanoTime() + ttl * TimeUnit.MILLISECONDS.toNanos(1);
    if (requestPartitionKey == null) {
        // Master lookup.
        Preconditions.checkArgument(tablets.size() == 1);
        Entry entry = Entry.tablet(tablets.get(0), TimeUnit.DAYS.toMillis(1));
        synchronized (monitor) {
            entries.clear();
            entries.put(AsyncKuduClient.EMPTY_ARRAY, entry);
        }
        return;
    }
    List<Entry> newEntries = new ArrayList<>();
    if (tablets.isEmpty()) {
        // If there are no tablets in the response, then the table is empty. If
        // there were any tablets in the table they would have been returned, since
        // the master guarantees that if the partition key falls in a non-covered
        // range, the previous tablet will be returned, and we did not set an upper
        // bound partition key on the request.
        newEntries.add(Entry.nonCoveredRange(AsyncKuduClient.EMPTY_ARRAY, AsyncKuduClient.EMPTY_ARRAY, deadline));
    } else {
        // The comments below will reference the following diagram:
        // 
        // +---+   +---+---+
        // |   |   |   |   |
        // A | B | C | D | E | F
        // |   |   |   |   |
        // +---+   +---+---+
        // 
        // It depicts a tablet locations response from the master containing three
        // tablets: B, D and E. Three non-covered ranges are present: A, C, and F.
        // An RPC response containing B, D and E could occur if the lookup partition
        // key falls in A, B, or C, although the existence of A as an initial
        // non-covered range can only be inferred if the lookup partition key falls
        // in A.
        final byte[] firstLowerBound = tablets.get(0).getPartition().getPartitionKeyStart();
        if (Bytes.memcmp(requestPartitionKey, firstLowerBound) < 0) {
            // If the first tablet is past the requested partition key, then the
            // partition key falls in an initial non-covered range, such as A.
            newEntries.add(Entry.nonCoveredRange(AsyncKuduClient.EMPTY_ARRAY, firstLowerBound, deadline));
        }
        // lastUpperBound tracks the upper bound of the previously processed
        // entry, so that we can determine when we have found a non-covered range.
        byte[] lastUpperBound = firstLowerBound;
        for (RemoteTablet tablet : tablets) {
            final byte[] tabletLowerBound = tablet.getPartition().getPartitionKeyStart();
            final byte[] tabletUpperBound = tablet.getPartition().getPartitionKeyEnd();
            if (Bytes.memcmp(lastUpperBound, tabletLowerBound) < 0) {
                // There is a non-covered range between the previous tablet and this tablet.
                // This will discover C while processing the tablet location for D.
                newEntries.add(Entry.nonCoveredRange(lastUpperBound, tabletLowerBound, deadline));
            }
            lastUpperBound = tabletUpperBound;
            // Now add the tablet itself (such as B, D, or E).
            newEntries.add(Entry.tablet(tablet, deadline));
        }
        if (lastUpperBound.length > 0 && tablets.size() < AsyncKuduClient.MAX_RETURNED_TABLE_LOCATIONS) {
            // There is a non-covered range between the last tablet and the end of the
            // partition key space, such as F.
            newEntries.add(Entry.nonCoveredRange(lastUpperBound, AsyncKuduClient.EMPTY_ARRAY, deadline));
        }
    }
    byte[] discoveredlowerBound = newEntries.get(0).getLowerBoundPartitionKey();
    byte[] discoveredUpperBound = newEntries.get(newEntries.size() - 1).getUpperBoundPartitionKey();
    LOG.debug("Discovered table locations:\t{}", newEntries);
    synchronized (monitor) {
        // Remove all existing overlapping entries, and add the new entries.
        Map.Entry<byte[], Entry> floorEntry = entries.floorEntry(discoveredlowerBound);
        if (floorEntry != null && Bytes.memcmp(requestPartitionKey, floorEntry.getValue().getUpperBoundPartitionKey()) < 0) {
            discoveredlowerBound = floorEntry.getKey();
        }
        NavigableMap<byte[], Entry> overlappingEntries = entries.tailMap(discoveredlowerBound, true);
        if (discoveredUpperBound.length > 0) {
            overlappingEntries = overlappingEntries.headMap(discoveredUpperBound, false);
        }
        if (LOG.isTraceEnabled()) {
            LOG.trace("Existing table locations:\t\t{}", entries.values());
            LOG.trace("Removing table locations:\t\t{}", overlappingEntries.values());
        }
        overlappingEntries.clear();
        for (Entry entry : newEntries) {
            entries.put(entry.getLowerBoundPartitionKey(), entry);
        }
    }
}
#end_block

#method_before
public static Entry nonCoveredRange(byte[] lowerBoundPartitionKey, byte[] upperBoundPartitionKey, long ttl) {
    return new Entry(null, lowerBoundPartitionKey, upperBoundPartitionKey, ttl);
}
#method_after
public static Entry nonCoveredRange(byte[] lowerBoundPartitionKey, byte[] upperBoundPartitionKey, long deadline) {
    return new Entry(null, lowerBoundPartitionKey, upperBoundPartitionKey, deadline);
}
#end_block

#method_before
public static Entry tablet(RemoteTablet tablet, long ttl) {
    return new Entry(tablet, null, null, ttl);
}
#method_after
public static Entry tablet(RemoteTablet tablet, long deadline) {
    return new Entry(tablet, null, null, deadline);
}
#end_block

#method_before
public long ttl() {
    return deadline - System.nanoTime();
}
#method_after
private long ttl() {
    return TimeUnit.NANOSECONDS.toMillis(deadline - System.nanoTime());
}
#end_block

#method_before
@Test
public void TestResetMetadata() {
    AnalyzesOk("invalidate metadata");
    AnalyzesOk("invalidate metadata functional.alltypessmall");
    AnalyzesOk("invalidate metadata functional.alltypes_view");
    AnalyzesOk("invalidate metadata functional.bad_serde");
    AnalyzesOk("refresh functional.alltypessmall");
    AnalyzesOk("refresh functional.alltypes_view");
    AnalyzesOk("refresh functional.bad_serde");
    // invalidate metadata <table name> checks the Hive Metastore for table existence
    // and should not throw an AnalysisError if the table or db does not exist.
    AnalyzesOk("invalidate metadata functional.unknown_table");
    AnalyzesOk("invalidate metadata unknown_db.unknown_table");
    AnalysisError("refresh functional.unknown_table", "Table does not exist: functional.unknown_table");
    AnalysisError("refresh unknown_db.unknown_table", "Database does not exist: unknown_db");
}
#method_after
@Test
public void TestResetMetadata() {
    AnalyzesOk("invalidate metadata");
    AnalyzesOk("invalidate metadata functional.alltypessmall");
    AnalyzesOk("invalidate metadata functional.alltypes_view");
    AnalyzesOk("invalidate metadata functional.bad_serde");
    AnalyzesOk("refresh functional.alltypessmall");
    AnalyzesOk("refresh functional.alltypes_view");
    AnalyzesOk("refresh functional.bad_serde");
    AnalyzesOk("refresh functional.alltypessmall partition (year=2009, month=1)");
    AnalyzesOk("refresh functional.alltypessmall partition (year=2009, month=NULL)");
    // invalidate metadata <table name> checks the Hive Metastore for table existence
    // and should not throw an AnalysisError if the table or db does not exist.
    AnalyzesOk("invalidate metadata functional.unknown_table");
    AnalyzesOk("invalidate metadata unknown_db.unknown_table");
    AnalysisError("refresh functional.unknown_table", "Table does not exist: functional.unknown_table");
    AnalysisError("refresh unknown_db.unknown_table", "Database does not exist: unknown_db");
    AnalysisError("refresh functional.alltypessmall partition (year=2009, int_col=10)", "Column 'int_col' is not a partition column in table: functional.alltypessmall");
    AnalysisError("refresh functional.alltypessmall partition (year=2009)", "Items in partition spec must exactly match the partition columns in " + "the table definition: functional.alltypessmall (1 vs 2)");
    AnalysisError("refresh functional.alltypessmall partition (year=2009, year=2009)", "Duplicate partition key name: year");
    AnalysisError("refresh functional.alltypessmall partition (year=2009, month='foo')", "Value of partition spec (column=month) has incompatible type: 'STRING'. " + "Expected type: 'INT'");
    AnalysisError("refresh functional.zipcode_incomes partition (year=2009, month=1)", "Table is not partitioned: functional.zipcode_incomes");
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    Preconditions.checkNotNull(getPrivilegeRequirement());
    desc_ = analyzer.registerTableRef(this);
    isAnalyzed_ = true;
    analyzeHints(analyzer);
    analyzeJoin(analyzer);
    analyzeSkipHeaderLineCount();
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    analyzer.registerAuthAndAuditEvent(resolvedPath_.getRootTable(), analyzer);
    desc_ = analyzer.registerTableRef(this);
    isAnalyzed_ = true;
    analyzeHints(analyzer);
    analyzeJoin(analyzer);
    analyzeSkipHeaderLineCount();
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    Preconditions.checkNotNull(getPrivilegeRequirement());
    desc_ = analyzer.registerTableRef(this);
    if (isRelative() && !analyzer.isWithClause()) {
        SlotDescriptor parentSlotDesc = analyzer.registerSlotRef(resolvedPath_);
        parentSlotDesc.setItemTupleDesc(desc_);
        collectionExpr_ = new SlotRef(parentSlotDesc);
        // Must always be materialized to ensure the correct cardinality after unnesting.
        analyzer.materializeSlots(collectionExpr_);
        Analyzer parentAnalyzer = analyzer.findAnalyzer(resolvedPath_.getRootDesc().getId());
        Preconditions.checkNotNull(parentAnalyzer);
        if (parentAnalyzer != analyzer) {
            TableRef parentRef = parentAnalyzer.getTableRef(resolvedPath_.getRootDesc().getId());
            Preconditions.checkNotNull(parentRef);
            // InlineViews are currently not supported as a parent ref.
            Preconditions.checkState(!(parentRef instanceof InlineViewRef));
            correlatedTupleIds_.add(parentRef.getId());
        }
    }
    if (!isRelative()) {
        // Register a column-level privilege request for the collection-typed column.
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().allOf(Privilege.SELECT).onColumn(desc_.getTableName().getDb(), desc_.getTableName().getTbl(), desc_.getPath().getRawPath().get(0)).toRequest());
    }
    isAnalyzed_ = true;
    analyzeHints(analyzer);
    // TODO: For joins on nested collections some join ops can be simplified
    // due to the containment relationship of the parent and child. For example,
    // a FULL OUTER JOIN would become a LEFT OUTER JOIN, or a RIGHT SEMI JOIN
    // would become an INNER or CROSS JOIN.
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    desc_ = analyzer.registerTableRef(this);
    if (isRelative() && !analyzer.isWithClause()) {
        SlotDescriptor parentSlotDesc = analyzer.registerSlotRef(resolvedPath_);
        parentSlotDesc.setItemTupleDesc(desc_);
        collectionExpr_ = new SlotRef(parentSlotDesc);
        // Must always be materialized to ensure the correct cardinality after unnesting.
        analyzer.materializeSlots(collectionExpr_);
        Analyzer parentAnalyzer = analyzer.findAnalyzer(resolvedPath_.getRootDesc().getId());
        Preconditions.checkNotNull(parentAnalyzer);
        if (parentAnalyzer != analyzer) {
            TableRef parentRef = parentAnalyzer.getTableRef(resolvedPath_.getRootDesc().getId());
            Preconditions.checkNotNull(parentRef);
            // InlineViews are currently not supported as a parent ref.
            Preconditions.checkState(!(parentRef instanceof InlineViewRef));
            correlatedTupleIds_.add(parentRef.getId());
        }
    }
    if (!isRelative()) {
        // Register a table-level privilege request as well as a column-level privilege request
        // for the collection-typed column.
        Preconditions.checkNotNull(resolvedPath_.getRootTable());
        analyzer.registerAuthAndAuditEvent(resolvedPath_.getRootTable(), analyzer);
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().allOf(Privilege.SELECT).onColumn(desc_.getTableName().getDb(), desc_.getTableName().getTbl(), desc_.getPath().getRawPath().get(0)).toRequest());
    }
    isAnalyzed_ = true;
    analyzeHints(analyzer);
    // TODO: For joins on nested collections some join ops can be simplified
    // due to the containment relationship of the parent and child. For example,
    // a FULL OUTER JOIN would become a LEFT OUTER JOIN, or a RIGHT SEMI JOIN
    // would become an INNER or CROSS JOIN.
    analyzeJoin(analyzer);
}
#end_block

#method_before
public void reset() {
    for (int i = 0; i < size(); ++i) {
        TableRef origTblRef = get(i);
        if (origTblRef.isResolved() && !(origTblRef instanceof InlineViewRef)) {
            // Replace resolved table refs with unresolved ones.
            TableRef newTblRef = new TableRef(origTblRef);
            // Use the fully qualified raw path to preserve the original resolution.
            // Otherwise, non-fully qualified paths might incorrectly match a local view.
            // TODO for 2.3: This full qualification preserves analysis state which is
            // contraty to the intended semantics of reset(). We could address this issue by
            // changing the WITH-clause analysis to register local views that have
            // fully-qualified table refs, and then remove the full qualification here.
            newTblRef.rawPath_ = origTblRef.getResolvedPath().getFullyQualifiedRawPath();
            set(i, newTblRef);
        }
        get(i).reset();
    }
    this.analyzed_ = false;
}
#method_after
public void reset() {
    for (int i = 0; i < size(); ++i) {
        TableRef origTblRef = get(i);
        if (origTblRef.isResolved() && !(origTblRef instanceof InlineViewRef)) {
            // Replace resolved table refs with unresolved ones.
            TableRef newTblRef = new TableRef(origTblRef);
            // Use the fully qualified raw path to preserve the original resolution.
            // Otherwise, non-fully qualified paths might incorrectly match a local view.
            // TODO for 2.3: This full qualification preserves analysis state which is
            // contrary to the intended semantics of reset(). We could address this issue by
            // changing the WITH-clause analysis to register local views that have
            // fully-qualified table refs, and then remove the full qualification here.
            newTblRef.rawPath_ = origTblRef.getResolvedPath().getFullyQualifiedRawPath();
            set(i, newTblRef);
        }
        get(i).reset();
    }
    this.analyzed_ = false;
}
#end_block

#method_before
@Test
public void TestPlanHints() {
    // All plan-hint styles embed a comma-separated list of hints.
    String[][] hintStyles = new String[][] { // traditional commented hint
    new String[] { "/* +", "*/" }, // eol commented hint
    new String[] { "-- +", "\n" }, // eol commented hint
    new String[] { "\n-- +", "\n" }, // legacy style
    new String[] { "[", "]" } };
    String[][] commentStyles = new String[][] { // traditional comment
    new String[] { "/*", "*/" }, // eol comment
    new String[] { "--", "\n" } };
    for (String[] hintStyle : hintStyles) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test join hints.
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b on(a.id = b.id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a cross join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        // Multiple comma-separated hints.
        TestJoinHints(String.format("select * from functional.alltypes a join " + "%sbroadcast,shuffle,foo,bar%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast", "shuffle", "foo", "bar");
        // Test hints in a multi-way join.
        TestJoinHints(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, prefix, suffix, prefix, suffix, prefix, suffix), "broadcast", "shuffle", "broadcast", "shuffle");
        // Test hints in a multi-way join (flipped prefix/suffix -> bad hint start/ends).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, suffix, prefix, prefix, suffix, suffix, prefix));
        // Test hints in a multi-way join (missing prefixes/suffixes).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", suffix, suffix, suffix, suffix, prefix, "", "", ""));
        // Test insert hints.
        TestInsertHints(String.format("insert into t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert overwrite t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t partition(x, y) %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t(a, b) partition(x, y) %sshuffle%s select * from t", prefix, suffix), "shuffle");
        TestInsertHints(String.format("insert overwrite t(a, b) partition(x, y) %sfoo,bar,baz%s select * from t", prefix, suffix), "foo", "bar", "baz");
        // Test TableRef hints.
        TestTableHints(String.format("select * from functional.alltypes %sschedule_random_replica%s", prefix, suffix), "schedule_random_replica");
        // Test both TableRef and join hints.
        TestTableAndJoinHints(String.format("select * from functional.alltypes a %sschedule_random_replica%s join " + "%sbroadcast%s functional.alltypes b using(id)", prefix, suffix, prefix, suffix, prefix, suffix), "schedule_random_replica", "broadcast");
        // prefix and suffix.
        if (prefix.contains("[")) {
            prefix = "";
            suffix = "";
        }
        TestSelectListHints(String.format("select %sstraight_join%s * from functional.alltypes a", prefix, suffix), "straight_join");
        // Only the new hint-style is recognized
        if (!prefix.equals("")) {
            TestSelectListHints(String.format("select %sfoo,bar,baz%s * from functional.alltypes a", prefix, suffix), "foo", "bar", "baz");
        }
        if (prefix.isEmpty())
            continue;
        // Test mixing commented hints and comments.
        for (String[] commentStyle : commentStyles) {
            String commentPrefix = commentStyle[0];
            String commentSuffix = commentStyle[1];
            String queryTemplate = "$1comment$2 select $1comment$2 $3straight_join$4 $1comment$2 * " + "from $1comment$2 functional.alltypes a join $1comment$2 $3shuffle$4 " + "$1comment$2 functional.alltypes b $1comment$2 on $1comment$2 " + "(a.id = b.id)";
            String query = queryTemplate.replaceAll("\\$1", commentPrefix).replaceAll("\\$2", commentSuffix).replaceAll("\\$3", prefix).replaceAll("\\$4", suffix);
            TestSelectListHints(query, "straight_join");
            TestJoinHints(query, "shuffle");
        }
    }
    // No "+" at the beginning so the comment is not recognized as a hint.
    TestJoinHints("select * from functional.alltypes a join /* comment */" + "functional.alltypes b using (int_col)", (String) null);
    TestSelectListHints("select /* comment */ * from functional.alltypes", (String) null);
    TestInsertHints("insert into t(a, b) partition(x, y) /* comment */ select 1", (String) null);
    TestSelectListHints("select /* -- +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* abcdef +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- abcdef +straight_join\n * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- /*+straight_join\n * from functional.alltypes", (String) null);
    // Commented hints cannot span lines (recognized as comments instead).
    TestSelectListHints("select /*\n +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_join \n*/ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_\njoin */ * from functional.alltypes", (String) null);
    ParserError("select -- +straight_join * from functional.alltypes");
    ParserError("select \n-- +straight_join * from functional.alltypes");
    // Missing "/*" or "/*"
    ParserError("select * from functional.alltypes a join + */" + "functional.alltypes b using (int_col)");
    ParserError("select * from functional.alltypes a join /* + " + "functional.alltypes b using (int_col)");
    // Test empty hint tokens.
    TestSelectListHints("select /* +straight_join, ,, */ * from functional.alltypes", "straight_join");
    // Traditional commented hints are not parsed inside a comment.
    ParserError("select /* /* +straight_join */ */ * from functional.alltypes");
}
#method_after
@Test
public void TestPlanHints() {
    // All plan-hint styles embed a comma-separated list of hints.
    String[][] hintStyles = new String[][] { // traditional commented hint
    new String[] { "/* +", "*/" }, // eol commented hint
    new String[] { "-- +", "\n" }, // eol commented hint
    new String[] { "\n-- +", "\n" }, // legacy style
    new String[] { "[", "]" } };
    String[][] commentStyles = new String[][] { // traditional comment
    new String[] { "/*", "*/" }, // eol comment
    new String[] { "--", "\n" } };
    for (String[] hintStyle : hintStyles) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Test join hints.
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b on(a.id = b.id)", prefix, suffix), "broadcast");
        TestJoinHints(String.format("select * from functional.alltypes a cross join %sbroadcast%s " + "functional.alltypes b", prefix, suffix), "broadcast");
        // Multiple comma-separated hints.
        TestJoinHints(String.format("select * from functional.alltypes a join " + "%sbroadcast,shuffle,foo,bar%s " + "functional.alltypes b using(id)", prefix, suffix), "broadcast", "shuffle", "foo", "bar");
        // Test hints in a multi-way join.
        TestJoinHints(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, prefix, suffix, prefix, suffix, prefix, suffix), "broadcast", "shuffle", "broadcast", "shuffle");
        // Test hints in a multi-way join (flipped prefix/suffix -> bad hint start/ends).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", prefix, suffix, suffix, prefix, prefix, suffix, suffix, prefix));
        // Test hints in a multi-way join (missing prefixes/suffixes).
        ParserError(String.format("select * from functional.alltypes a " + "join %sbroadcast%s functional.alltypes b using(id) " + "join %sshuffle%s functional.alltypes c using(int_col) " + "join %sbroadcast%s functional.alltypes d using(int_col) " + "join %sshuffle%s functional.alltypes e using(string_col)", suffix, suffix, suffix, suffix, prefix, "", "", ""));
        // Test insert hints.
        TestInsertHints(String.format("insert into t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert overwrite t %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t partition(x, y) %snoshuffle%s select * from t", prefix, suffix), "noshuffle");
        TestInsertHints(String.format("insert into t(a, b) partition(x, y) %sshuffle%s select * from t", prefix, suffix), "shuffle");
        TestInsertHints(String.format("insert overwrite t(a, b) partition(x, y) %sfoo,bar,baz%s select * from t", prefix, suffix), "foo", "bar", "baz");
        // Test TableRef hints.
        TestTableHints(String.format("select * from functional.alltypes %sschedule_disk_local%s", prefix, suffix), "schedule_disk_local");
        TestTableHints(String.format("select * from functional.alltypes %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s", prefix, suffix), "schedule_cache_local", "schedule_random_replica");
        TestTableHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s" + ", functional.alltypes b %sschedule_remote%s", prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "schedule_remote");
        // Test both TableRef and join hints.
        TestTableAndJoinHints(String.format("select * from functional.alltypes a %sschedule_cache_local," + "schedule_random_replica%s join %sbroadcast%s functional.alltypes b " + "%sschedule_remote%s using(id)", prefix, suffix, prefix, suffix, prefix, suffix), "schedule_cache_local", "schedule_random_replica", "broadcast", "schedule_remote");
        // prefix and suffix.
        if (prefix.contains("[")) {
            prefix = "";
            suffix = "";
        }
        TestSelectListHints(String.format("select %sstraight_join%s * from functional.alltypes a", prefix, suffix), "straight_join");
        // Only the new hint-style is recognized
        if (!prefix.equals("")) {
            TestSelectListHints(String.format("select %sfoo,bar,baz%s * from functional.alltypes a", prefix, suffix), "foo", "bar", "baz");
        }
        if (prefix.isEmpty())
            continue;
        // Test mixing commented hints and comments.
        for (String[] commentStyle : commentStyles) {
            String commentPrefix = commentStyle[0];
            String commentSuffix = commentStyle[1];
            String queryTemplate = "$1comment$2 select $1comment$2 $3straight_join$4 $1comment$2 * " + "from $1comment$2 functional.alltypes a join $1comment$2 $3shuffle$4 " + "$1comment$2 functional.alltypes b $1comment$2 on $1comment$2 " + "(a.id = b.id)";
            String query = queryTemplate.replaceAll("\\$1", commentPrefix).replaceAll("\\$2", commentSuffix).replaceAll("\\$3", prefix).replaceAll("\\$4", suffix);
            TestSelectListHints(query, "straight_join");
            TestJoinHints(query, "shuffle");
        }
    }
    // No "+" at the beginning so the comment is not recognized as a hint.
    TestJoinHints("select * from functional.alltypes a join /* comment */" + "functional.alltypes b using (int_col)", (String) null);
    TestSelectListHints("select /* comment */ * from functional.alltypes", (String) null);
    TestInsertHints("insert into t(a, b) partition(x, y) /* comment */ select 1", (String) null);
    TestSelectListHints("select /* -- +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* abcdef +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- abcdef +straight_join\n * from functional.alltypes", (String) null);
    TestSelectListHints("select \n-- /*+straight_join\n * from functional.alltypes", (String) null);
    // Commented hints cannot span lines (recognized as comments instead).
    TestSelectListHints("select /*\n +straight_join */ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_join \n*/ * from functional.alltypes", (String) null);
    TestSelectListHints("select /* +straight_\njoin */ * from functional.alltypes", (String) null);
    ParserError("select -- +straight_join * from functional.alltypes");
    ParserError("select \n-- +straight_join * from functional.alltypes");
    // Missing "/*" or "/*"
    ParserError("select * from functional.alltypes a join + */" + "functional.alltypes b using (int_col)");
    ParserError("select * from functional.alltypes a join /* + " + "functional.alltypes b using (int_col)");
    // Test empty hint tokens.
    TestSelectListHints("select /* +straight_join, ,, */ * from functional.alltypes", "straight_join");
    // Traditional commented hints are not parsed inside a comment.
    ParserError("select /* /* +straight_join */ */ * from functional.alltypes");
}
#end_block

#method_before
@Test
public void TestResetMetadata() {
    ParsesOk("invalidate metadata");
    ParsesOk("invalidate metadata Foo");
    ParsesOk("invalidate metadata Foo.S");
    ParsesOk("refresh Foo");
    ParsesOk("refresh Foo.S");
    ParserError("invalidate");
    ParserError("invalidate metadata Foo.S.S");
    ParserError("REFRESH Foo.S.S");
    ParserError("refresh");
}
#method_after
@Test
public void TestResetMetadata() {
    ParsesOk("invalidate metadata");
    ParsesOk("invalidate metadata Foo");
    ParsesOk("invalidate metadata Foo.S");
    ParsesOk("refresh Foo");
    ParsesOk("refresh Foo.S");
    ParsesOk("refresh Foo partition (col=2)");
    ParsesOk("refresh Foo.S partition (col=2)");
    ParsesOk("refresh Foo.S partition (col1 = 2, col2 = 3)");
    ParserError("invalidate");
    ParserError("invalidate metadata Foo.S.S");
    ParserError("invalidate metadata partition (col=2)");
    ParserError("invalidate metadata Foo.S partition (col=2)");
    ParserError("REFRESH Foo.S.S");
    ParserError("refresh");
    ParserError("refresh Foo.S partition (col1 = 2, col2)");
    ParserError("refresh Foo.S partition ()");
}
#end_block

#method_before
@Test
public void TestInSubqueries() throws AnalysisException {
    String[] colNames = { "bool_col", "tinyint_col", "smallint_col", "int_col", "bigint_col", "float_col", "double_col", "string_col", "date_string_col", "timestamp_col" };
    String[] joinOperators = { "inner join", "left outer join", "right outer join", "left semi join", "left anti join" };
    // [NOT] IN subquery predicates
    String[] operators = { "in", "not in" };
    for (String op : operators) {
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select id from functional.alltypestiny)", op));
        // Using column and table aliases similar to the ones produced by the
        // column/table alias generators during a rewrite.
        AnalyzesOk(String.format("select id `$c$1` from functional.alltypestiny `$a$1` " + "where id %s (select id from functional.alltypessmall)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "t.id %s (select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select t.id, max(t.int_col) from " + "functional.alltypes t where t.int_col %s (select int_col from " + "functional.alltypesagg) group by t.id having count(*) < 10", op));
        AnalyzesOk(String.format("select t.bigint_col, t.string_col from " + "functional.alltypes t where t.id %s (select id from " + "functional.alltypesagg where int_col < 10) order by bigint_col", op));
        AnalyzesOk(String.format("select * from functional.alltypes a where a.id %s " + "(select id from functional.alltypes b where a.id = b.id)", op));
        // Complex expressions
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id + int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "t.int_col + 1 %s (select int_col - 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "abs(t.double_col) %s (select int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select NULL from functional.alltypes t where " + "cast(t.double_col as int) %s (select int_col from " + "functional.alltypestiny)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes where id %s " + "(select 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select 1 + 1 from functional.alltypestiny group by int_col)", op));
        AnalyzesOk(String.format("select max(id) from functional.alltypes where id %s " + "(select max(id) from functional.alltypesagg a where a.int_col < 10) " + "and bool_col = false", op));
        // Subquery returns multiple columns
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select id, int_col from functional.alltypessmall)", op), "Subquery must return a single column: (SELECT id, int_col " + "FROM functional.alltypessmall)");
        // Subquery returns an incompatible column type
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select timestamp_col from functional.alltypessmall)", op), "Incompatible return types 'INT' and 'TIMESTAMP' of exprs 'id' and " + "'timestamp_col'.");
        // Different column types in the subquery predicate
        for (String col : colNames) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.%s %s " + "(select a.%s from functional.alltypestiny a)", col, op, col));
        }
        // Decimal in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.double_col %s (select d3 from functional.decimal_tbl a)", op));
        // Varchar in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.string_col %s (select cast(a.string_col as varchar(1)) from " + "functional.alltypestiny a)", op));
        // Subqueries with multiple predicates in the WHERE clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col > 10 and " + "a.tinyint_col < 5)", op));
        // Subqueries with a GROUP BY clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.double_col < 10.1 " + "group by a.id)", op));
        // Subqueries with GROUP BY and HAVING clauses
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.bool_col = true and " + "int_col < 10 group by id having count(*) < 10)", op));
        // Subqueries with a LIMIT clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where id < 100 limit 10)", op));
        // Subqueries with multiple tables in the FROM clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, functional.alltypessmall s " + "where a.int_col = s.int_col and s.bigint_col < 100 and a.tinyint_col < 10)", op));
        // Different join operators between the tables in subquery's FROM clause
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a %s functional.alltypessmall " + "s on a.int_col = s.int_col where a.bool_col = false)", op, joinOp));
        }
        // Subquery with relative table references
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where id %s " + "(select f1 from t.struct_array_col a, t.int_array_col b " + "where f2 = 'xyz' and b.item < 3 group by f1 having count(*) > 2 limit 5)", op));
        // Correlated predicates in the subquery's ON clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.bigint_col = a.bigint_col and " + "s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on a.bool_col = s.bool_col and t.int_col = 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on ifnull(s.int_col, s.int_col + 20) = " + "t.int_col + t.bigint_col)", op));
        // Subqueries with inline views
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, " + "(select * from functional.alltypessmall) s where s.int_col = a.int_col " + "and s.bool_col = false)", op));
        // Subqueries with inline views that contain subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select id from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select g.* from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a where " + "a.bigint_col = 100)", op));
        // Multiple tables in the FROM clause of the outer query block
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t %s " + "functional.alltypessmall s on t.int_col = s.int_col where " + "t.tinyint_col %s (select tinyint_col from functional.alltypesagg) " + "and t.bool_col = false and t.bigint_col = 10", joinOp, op));
        }
        // Subqueries in WITH clause
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a where " + "id %s (select id from functional.alltypestiny)) select * from t where " + "t.bool_col = false and t.int_col = 10", op));
        // Subqueries in WITH and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny s)) select * from t " + "where t.int_col in (select int_col from functional.alltypessmall) and " + "t.bool_col = false", op));
        // Subqueries in WITH, FROM and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny)) select t.* from t, " + "(select * from functional.alltypesagg g where g.id in " + "(select id from functional.alltypes)) s where s.string_col = t.string_col " + "and t.int_col in (select int_col from functional.alltypessmall) and " + "s.bool_col = false", op));
        // Correlated subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col) " + "and t.bool_col = false", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col + 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + a.int_col  = " + "a.bigint_col and a.bool_col = true)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col = false and " + "a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col)", op));
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where id %s " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 = a.f1)", op));
        // Multiple nesting levels (uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg where int_col %s " + "(select int_col from functional.alltypestiny) and bool_col = false) " + "and bigint_col < 1000", op, op));
        // Multiple nesting levels (correlated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where a.int_col = t.int_col " + "and a.tinyint_col %s (select tinyint_col from functional.alltypestiny s " + "where s.bigint_col = a.bigint_col))", op, op));
        // Multiple nesting levels (correlated and uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col " + "and a.int_col %s (select int_col from functional.alltypestiny s))", op, op));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t1 where id %s " + "(select f11 from t1.complex_nested_struct_col.f2 t2 " + "where t1.year = f11 and f11 %s " + "(select value.f21 from t2.f12 where key = 'test'))", op, op));
        // NOT ([NOT] IN predicate)
        AnalyzesOk(String.format("select * from functional.alltypes t where not (id %s " + "(select id from functional.alltypesagg))", op));
        // Different cmp operators in the correlation predicate
        for (String cmpOp : cmpOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t " + "where t.id %s (select a.id from functional.alltypesagg a where " + "t.int_col %s a.int_col)", op, cmpOp));
        }
        // Uncorrelated IN subquery with analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col %s (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", op));
    }
    // Constant on the left hand side
    AnalyzesOk("select * from functional.alltypes a where 1 in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)");
    AnalysisError("select * from functional.alltypes a where 1 not in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)", "Unsupported NOT IN predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypesagg s WHERE s.int_col = a.int_col)");
    // IN subquery that is equivalent to an uncorrelated EXISTS subquery
    AnalysisError("select * from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg)", "Unsupported " + "predicate with subquery: 1 IN (SELECT int_col FROM functional.alltypesagg)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        // Allowed because the subquery only has relative table refs.
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 %s a.f1)", cmpOp));
        // Not allowed because the subquery has absolute table refs.
        AnalysisError(String.format("select 1 from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg g where g.id %s t.id)", cmpOp), String.format("Unsupported predicate with subquery: 1 " + "IN (SELECT int_col FROM functional.alltypesagg g WHERE g.id %s t.id)", cmpOp));
    }
    // NOT IN subquery with a correlated non-equi predicate is ok if the subquery only
    // has relative table refs
    AnalyzesOk("select 1 from functional.allcomplextypes t where id not in " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1)");
    // NOT IN subquery with a correlated non-equi predicate is not ok if the subquery
    // has absolute table refs
    AnalysisError("select 1 from functional.alltypes t where 1 not in " + "(select id from functional.alltypestiny g where g.id < t.id)", "Unsupported predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypestiny g WHERE g.id < t.id)");
    // Statement with a GROUP BY and a correlated IN subquery that has a non-equi
    // correlated predicate and only relative table refs
    AnalyzesOk("select id, count(*) from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1) " + "group by id");
    // Statement with a GROUP BY and a correlated IN subquery that has a non-equi
    // correlated predicate and absolute table refs
    AnalysisError("select id, count(*) from functional.alltypes t " + "where 1 IN (select id from functional.alltypesagg g where t.int_col < " + "g.int_col) group by id", "Unsupported predicate with subquery: 1 IN " + "(SELECT id FROM functional.alltypesagg g WHERE t.int_col < g.int_col)");
    // Reference a non-existing table in the subquery
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s left outer join p on " + "(s.int_col = p.int_col))", "Could not resolve table reference: 'p'");
    // Reference a non-existing column from a table in the outer scope
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s where s.int_col = t.bad_col)", "Could not resolve column/field reference: 't.bad_col'");
    // Referencing the same table in the inner and the outer query block
    // No explicit alias
    AnalyzesOk("select id from functional.alltypestiny where int_col in " + "(select int_col from functional.alltypestiny)");
    // Different alias between inner and outer block referencing the same table
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny p)");
    // Alias only in the outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny)");
    // Same alias in both inner and outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny t)");
    // Binary predicate with non-comparable operands
    AnalysisError("select * from functional.alltypes t where " + "(id in (select id from functional.alltypestiny)) = 'string_val'", "operands of type BOOLEAN and STRING are not comparable: " + "(id IN (SELECT id FROM functional.alltypestiny)) = 'string_val'");
    // OR with subquery predicates
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select id from functional.alltypesagg) or t.bool_col = false", "Subqueries in OR predicates are not supported: t.id IN " + "(SELECT id FROM functional.alltypesagg) OR t.bool_col = FALSE");
    AnalysisError("select * from functional.alltypes t where not (t.id in " + "(select id from functional.alltypesagg) and t.int_col = 10)", "Subqueries in OR predicates are not supported: t.id NOT IN " + "(SELECT id FROM functional.alltypesagg) OR t.int_col != 10");
    AnalysisError("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg g where g.bool_col = false) " + "or t.bool_col = true", "Subqueries in OR predicates are not " + "supported: EXISTS (SELECT * FROM functional.alltypesagg g WHERE " + "g.bool_col = FALSE) OR t.bool_col = TRUE");
    AnalysisError("select * from functional.alltypes t where t.id = " + "(select min(id) from functional.alltypesagg g) or t.id = 10", "Subqueries in OR predicates are not supported: t.id = " + "(SELECT min(id) FROM functional.alltypesagg g) OR t.id = 10");
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1) " + "or id < 10", "Subqueries in OR predicates are not supported: " + "id IN (SELECT f1 FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1) " + "OR id < 10");
    // TODO for 2.3: Modify the StmtRewriter to allow this case with relative refs.
    // Correlated subquery with relative table refs and OR predicate is not allowed
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1 " + "or id < 10)", "Disjunctions with correlated predicates are not supported: " + "t.int_struct_col.f1 < a.f1 OR id < 10");
    // Correlated subquery with absolute table refs and OR predicate is not allowed
    AnalysisError("select * from functional.alltypes t where id in " + "(select id from functional.alltypesagg a where " + "a.int_col = t.int_col or a.bool_col = false)", "Disjunctions " + "with correlated predicates are not supported: a.int_col = " + "t.int_col OR a.bool_col = FALSE");
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny) and (bool_col = false or " + "int_col = 10)");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT are not allowed
    // with relative table refs in the subquery
    // TODO for 2.3: Modify the StmtRewriter to allow this case with relative refs
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select count(f1) from t.struct_array_col a where t.int_struct_col.f1 < a.f1)", "Unsupported correlated subquery with grouping and/or aggregation: " + "SELECT count(f1) FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT are not allowed
    // with absolute table refs in the subquery
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select max(a.id) from functional.alltypesagg a where " + "t.int_col = a.int_col)", "Unsupported correlated subquery with grouping " + "and/or aggregation: SELECT max(a.id) FROM functional.alltypesagg a " + "WHERE t.int_col = a.int_col");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select a.id from functional.alltypesagg a where " + "t.int_col = a.int_col group by a.id)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT a.id FROM " + "functional.alltypesagg a WHERE t.int_col = a.int_col GROUP BY a.id");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select distinct a.id from functional.alltypesagg a where " + "a.bigint_col = t.bigint_col)", "Unsupported correlated subquery with " + "grouping and/or aggregation: SELECT DISTINCT a.id FROM " + "functional.alltypesagg a WHERE a.bigint_col = t.bigint_col");
    // NOT compound predicates with OR
    AnalyzesOk("select * from functional.alltypes t where not (" + "id in (select id from functional.alltypesagg) or int_col < 10)");
    AnalyzesOk("select * from functional.alltypes t where not (" + "t.id < 10 or not (t.int_col in (select int_col from " + "functional.alltypesagg) and t.bool_col = false))");
    // Multiple subquery predicates
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny where int_col = 10) and int_col in " + "(select int_col from functional.alltypessmall where bigint_col = 1000) and " + "string_col not in (select string_col from functional.alltypesagg where " + "tinyint_col > 10) and bool_col = false");
    AnalyzesOk("select id, year, month from functional.allcomplextypes t where id in " + "(select item from t.int_array_col where item < 10) and id not in " + "(select f1 from t.struct_array_col where f2 = 'test')");
    // Correlated subquery with a LIMIT clause
    AnalysisError("select * from functional.alltypes t where id in " + "(select s.id from functional.alltypesagg s where s.int_col = t.int_col " + "limit 1)", "Unsupported correlated subquery with a LIMIT clause: " + "SELECT s.id FROM functional.alltypesagg s WHERE s.int_col = t.int_col " + "LIMIT 1");
    // Correlated IN with an analytic function
    AnalysisError("select id, int_col, bool_col from functional.alltypestiny t1 " + "where int_col in (select min(bigint_col) over (partition by bool_col) " + "from functional.alltypessmall t2 where t1.id < t2.id)", "Unsupported " + "correlated subquery with grouping and/or aggregation: SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE t1.id < t2.id");
    // IN subquery in binary predicate
    AnalysisError("select * from functional.alltypestiny where " + "(tinyint_col in (1,2)) = (bool_col in (select bool_col from " + "functional.alltypes))", "IN subquery predicates are not supported " + "in binary predicates: (tinyint_col IN (1, 2)) = (bool_col IN (SELECT " + "bool_col FROM functional.alltypes))");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "int_col in (select 1 as int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "int_col not in (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
    // NOT IN uncorrelated aggregate subquery with a constant
    AnalysisError("select * from functional.alltypestiny t1 where " + "10 not in (select max(int_col) from functional.alltypestiny)", "Unsupported NOT IN predicate with subquery: 10 NOT IN (SELECT " + "max(int_col) FROM functional.alltypestiny)");
    AnalysisError("select * from functional.alltypestiny t1 where " + "(10 - 2) not in (select count(*) from functional.alltypestiny)", "Unsupported NOT IN predicate with subquery: (10 - 2) NOT IN " + "(SELECT count(*) FROM functional.alltypestiny)");
}
#method_after
@Test
public void TestInSubqueries() throws AnalysisException {
    String[] colNames = { "bool_col", "tinyint_col", "smallint_col", "int_col", "bigint_col", "float_col", "double_col", "string_col", "date_string_col", "timestamp_col" };
    String[] joinOperators = { "inner join", "left outer join", "right outer join", "left semi join", "left anti join" };
    // [NOT] IN subquery predicates
    String[] operators = { "in", "not in" };
    for (String op : operators) {
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select id from functional.alltypestiny)", op));
        // Using column and table aliases similar to the ones produced by the
        // column/table alias generators during a rewrite.
        AnalyzesOk(String.format("select id `$c$1` from functional.alltypestiny `$a$1` " + "where id %s (select id from functional.alltypessmall)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "t.id %s (select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select t.id, max(t.int_col) from " + "functional.alltypes t where t.int_col %s (select int_col from " + "functional.alltypesagg) group by t.id having count(*) < 10", op));
        AnalyzesOk(String.format("select t.bigint_col, t.string_col from " + "functional.alltypes t where t.id %s (select id from " + "functional.alltypesagg where int_col < 10) order by bigint_col", op));
        AnalyzesOk(String.format("select * from functional.alltypes a where a.id %s " + "(select id from functional.alltypes b where a.id = b.id)", op));
        // Complex expressions
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id + int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "t.int_col + 1 %s (select int_col - 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "abs(t.double_col) %s (select int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select NULL from functional.alltypes t where " + "cast(t.double_col as int) %s (select int_col from " + "functional.alltypestiny)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes where id %s " + "(select 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select 1 + 1 from functional.alltypestiny group by int_col)", op));
        AnalyzesOk(String.format("select max(id) from functional.alltypes where id %s " + "(select max(id) from functional.alltypesagg a where a.int_col < 10) " + "and bool_col = false", op));
        // Subquery returns multiple columns
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select id, int_col from functional.alltypessmall)", op), "Subquery must return a single column: (SELECT id, int_col " + "FROM functional.alltypessmall)");
        // Subquery returns an incompatible column type
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select timestamp_col from functional.alltypessmall)", op), "Incompatible return types 'INT' and 'TIMESTAMP' of exprs 'id' and " + "'timestamp_col'.");
        // Different column types in the subquery predicate
        for (String col : colNames) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.%s %s " + "(select a.%s from functional.alltypestiny a)", col, op, col));
        }
        // Decimal in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.double_col %s (select d3 from functional.decimal_tbl a)", op));
        // Varchar in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.string_col %s (select cast(a.string_col as varchar(1)) from " + "functional.alltypestiny a)", op));
        // Subqueries with multiple predicates in the WHERE clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col > 10 and " + "a.tinyint_col < 5)", op));
        // Subqueries with a GROUP BY clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.double_col < 10.1 " + "group by a.id)", op));
        // Subqueries with GROUP BY and HAVING clauses
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.bool_col = true and " + "int_col < 10 group by id having count(*) < 10)", op));
        // Subqueries with a LIMIT clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where id < 100 limit 10)", op));
        // Subqueries with multiple tables in the FROM clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, functional.alltypessmall s " + "where a.int_col = s.int_col and s.bigint_col < 100 and a.tinyint_col < 10)", op));
        // Different join operators between the tables in subquery's FROM clause
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a %s functional.alltypessmall " + "s on a.int_col = s.int_col where a.bool_col = false)", op, joinOp));
        }
        // Subquery with relative table references
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where id %s " + "(select f1 from t.struct_array_col a, t.int_array_col b " + "where f2 = 'xyz' and b.item < 3 group by f1 having count(*) > 2 limit 5)", op));
        // Correlated predicates in the subquery's ON clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.bigint_col = a.bigint_col and " + "s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on a.bool_col = s.bool_col and t.int_col = 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on ifnull(s.int_col, s.int_col + 20) = " + "t.int_col + t.bigint_col)", op));
        // Subqueries with inline views
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, " + "(select * from functional.alltypessmall) s where s.int_col = a.int_col " + "and s.bool_col = false)", op));
        // Subqueries with inline views that contain subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select id from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select g.* from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a where " + "a.bigint_col = 100)", op));
        // Multiple tables in the FROM clause of the outer query block
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t %s " + "functional.alltypessmall s on t.int_col = s.int_col where " + "t.tinyint_col %s (select tinyint_col from functional.alltypesagg) " + "and t.bool_col = false and t.bigint_col = 10", joinOp, op));
        }
        // Subqueries in WITH clause
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a where " + "id %s (select id from functional.alltypestiny)) select * from t where " + "t.bool_col = false and t.int_col = 10", op));
        // Subqueries in WITH and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny s)) select * from t " + "where t.int_col in (select int_col from functional.alltypessmall) and " + "t.bool_col = false", op));
        // Subqueries in WITH, FROM and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny)) select t.* from t, " + "(select * from functional.alltypesagg g where g.id in " + "(select id from functional.alltypes)) s where s.string_col = t.string_col " + "and t.int_col in (select int_col from functional.alltypessmall) and " + "s.bool_col = false", op));
        // Correlated subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col) " + "and t.bool_col = false", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col + 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + a.int_col  = " + "a.bigint_col and a.bool_col = true)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col = false and " + "a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col)", op));
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where id %s " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 = a.f1)", op));
        // Test correlated BETWEEN predicates.
        AnalyzesOk(String.format("select 1 from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where " + " a.tinyint_col between t.tinyint_col and t.smallint_col and " + " a.smallint_col between 10 and t.int_col and " + " 20 between t.bigint_col and a.int_col and " + " t.float_col between a.float_col and a.double_col and " + " t.string_col between a.string_col and t.date_string_col and " + " a.double_col between round(acos(t.float_col), 2) " + " and cast(t.string_col as int))", op));
        // Multiple nesting levels (uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg where int_col %s " + "(select int_col from functional.alltypestiny) and bool_col = false) " + "and bigint_col < 1000", op, op));
        // Multiple nesting levels (correlated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where a.int_col = t.int_col " + "and a.tinyint_col %s (select tinyint_col from functional.alltypestiny s " + "where s.bigint_col = a.bigint_col))", op, op));
        // Multiple nesting levels (correlated and uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col " + "and a.int_col %s (select int_col from functional.alltypestiny s))", op, op));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t1 where id %s " + "(select f11 from t1.complex_nested_struct_col.f2 t2 " + "where t1.year = f11 and f11 %s " + "(select value.f21 from t2.f12 where key = 'test'))", op, op));
        // NOT ([NOT] IN predicate)
        AnalyzesOk(String.format("select * from functional.alltypes t where not (id %s " + "(select id from functional.alltypesagg))", op));
        // Different cmp operators in the correlation predicate
        for (String cmpOp : cmpOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t " + "where t.id %s (select a.id from functional.alltypesagg a where " + "t.int_col %s a.int_col)", op, cmpOp));
        }
        // Uncorrelated IN subquery with analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col %s (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", op));
    }
    // Constant on the left hand side
    AnalyzesOk("select * from functional.alltypes a where 1 in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)");
    AnalysisError("select * from functional.alltypes a where 1 not in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)", "Unsupported NOT IN predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypesagg s WHERE s.int_col = a.int_col)");
    // IN subquery that is equivalent to an uncorrelated EXISTS subquery
    AnalysisError("select * from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg)", "Unsupported " + "predicate with subquery: 1 IN (SELECT int_col FROM functional.alltypesagg)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        // Allowed because the subquery only has relative table refs.
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 %s a.f1)", cmpOp));
        // Not allowed because the subquery has absolute table refs.
        AnalysisError(String.format("select 1 from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg g where g.id %s t.id)", cmpOp), String.format("Unsupported predicate with subquery: 1 " + "IN (SELECT int_col FROM functional.alltypesagg g WHERE g.id %s t.id)", cmpOp));
    }
    // NOT IN subquery with a correlated non-equi predicate is ok if the subquery only
    // has relative table refs
    AnalyzesOk("select 1 from functional.allcomplextypes t where id not in " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1)");
    // NOT IN subquery with a correlated non-equi predicate is not ok if the subquery
    // has absolute table refs
    AnalysisError("select 1 from functional.alltypes t where 1 not in " + "(select id from functional.alltypestiny g where g.id < t.id)", "Unsupported predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypestiny g WHERE g.id < t.id)");
    // Statement with a GROUP BY and a correlated IN subquery that has a non-equi
    // correlated predicate and only relative table refs
    AnalyzesOk("select id, count(*) from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1) " + "group by id");
    // Statement with a GROUP BY and a correlated IN subquery that has a non-equi
    // correlated predicate and absolute table refs
    AnalysisError("select id, count(*) from functional.alltypes t " + "where 1 IN (select id from functional.alltypesagg g where t.int_col < " + "g.int_col) group by id", "Unsupported predicate with subquery: 1 IN " + "(SELECT id FROM functional.alltypesagg g WHERE t.int_col < g.int_col)");
    // Reference a non-existing table in the subquery
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s left outer join p on " + "(s.int_col = p.int_col))", "Could not resolve table reference: 'p'");
    // Reference a non-existing column from a table in the outer scope
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s where s.int_col = t.bad_col)", "Could not resolve column/field reference: 't.bad_col'");
    // Referencing the same table in the inner and the outer query block
    // No explicit alias
    AnalyzesOk("select id from functional.alltypestiny where int_col in " + "(select int_col from functional.alltypestiny)");
    // Different alias between inner and outer block referencing the same table
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny p)");
    // Alias only in the outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny)");
    // Same alias in both inner and outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny t)");
    // Binary predicate with non-comparable operands
    AnalysisError("select * from functional.alltypes t where " + "(id in (select id from functional.alltypestiny)) = 'string_val'", "operands of type BOOLEAN and STRING are not comparable: " + "(id IN (SELECT id FROM functional.alltypestiny)) = 'string_val'");
    // OR with subquery predicates
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select id from functional.alltypesagg) or t.bool_col = false", "Subqueries in OR predicates are not supported: t.id IN " + "(SELECT id FROM functional.alltypesagg) OR t.bool_col = FALSE");
    AnalysisError("select * from functional.alltypes t where not (t.id in " + "(select id from functional.alltypesagg) and t.int_col = 10)", "Subqueries in OR predicates are not supported: t.id NOT IN " + "(SELECT id FROM functional.alltypesagg) OR t.int_col != 10");
    AnalysisError("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg g where g.bool_col = false) " + "or t.bool_col = true", "Subqueries in OR predicates are not " + "supported: EXISTS (SELECT * FROM functional.alltypesagg g WHERE " + "g.bool_col = FALSE) OR t.bool_col = TRUE");
    AnalysisError("select * from functional.alltypes t where t.id = " + "(select min(id) from functional.alltypesagg g) or t.id = 10", "Subqueries in OR predicates are not supported: t.id = " + "(SELECT min(id) FROM functional.alltypesagg g) OR t.id = 10");
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1) " + "or id < 10", "Subqueries in OR predicates are not supported: " + "id IN (SELECT f1 FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1) " + "OR id < 10");
    // TODO for 2.3: Modify the StmtRewriter to allow this case with relative refs.
    // Correlated subquery with relative table refs and OR predicate is not allowed
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1 " + "or id < 10)", "Disjunctions with correlated predicates are not supported: " + "t.int_struct_col.f1 < a.f1 OR id < 10");
    // Correlated subquery with absolute table refs and OR predicate is not allowed
    AnalysisError("select * from functional.alltypes t where id in " + "(select id from functional.alltypesagg a where " + "a.int_col = t.int_col or a.bool_col = false)", "Disjunctions " + "with correlated predicates are not supported: a.int_col = " + "t.int_col OR a.bool_col = FALSE");
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny) and (bool_col = false or " + "int_col = 10)");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT are not allowed
    // with relative table refs in the subquery
    // TODO for 2.3: Modify the StmtRewriter to allow this case with relative refs
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select count(f1) from t.struct_array_col a where t.int_struct_col.f1 < a.f1)", "Unsupported correlated subquery with grouping and/or aggregation: " + "SELECT count(f1) FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT are not allowed
    // with absolute table refs in the subquery
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select max(a.id) from functional.alltypesagg a where " + "t.int_col = a.int_col)", "Unsupported correlated subquery with grouping " + "and/or aggregation: SELECT max(a.id) FROM functional.alltypesagg a " + "WHERE t.int_col = a.int_col");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select a.id from functional.alltypesagg a where " + "t.int_col = a.int_col group by a.id)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT a.id FROM " + "functional.alltypesagg a WHERE t.int_col = a.int_col GROUP BY a.id");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select distinct a.id from functional.alltypesagg a where " + "a.bigint_col = t.bigint_col)", "Unsupported correlated subquery with " + "grouping and/or aggregation: SELECT DISTINCT a.id FROM " + "functional.alltypesagg a WHERE a.bigint_col = t.bigint_col");
    // NOT compound predicates with OR
    AnalyzesOk("select * from functional.alltypes t where not (" + "id in (select id from functional.alltypesagg) or int_col < 10)");
    AnalyzesOk("select * from functional.alltypes t where not (" + "t.id < 10 or not (t.int_col in (select int_col from " + "functional.alltypesagg) and t.bool_col = false))");
    // Multiple subquery predicates
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny where int_col = 10) and int_col in " + "(select int_col from functional.alltypessmall where bigint_col = 1000) and " + "string_col not in (select string_col from functional.alltypesagg where " + "tinyint_col > 10) and bool_col = false");
    AnalyzesOk("select id, year, month from functional.allcomplextypes t where id in " + "(select item from t.int_array_col where item < 10) and id not in " + "(select f1 from t.struct_array_col where f2 = 'test')");
    // Correlated subquery with a LIMIT clause
    AnalysisError("select * from functional.alltypes t where id in " + "(select s.id from functional.alltypesagg s where s.int_col = t.int_col " + "limit 1)", "Unsupported correlated subquery with a LIMIT clause: " + "SELECT s.id FROM functional.alltypesagg s WHERE s.int_col = t.int_col " + "LIMIT 1");
    // Correlated IN with an analytic function
    AnalysisError("select id, int_col, bool_col from functional.alltypestiny t1 " + "where int_col in (select min(bigint_col) over (partition by bool_col) " + "from functional.alltypessmall t2 where t1.id < t2.id)", "Unsupported " + "correlated subquery with grouping and/or aggregation: SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE t1.id < t2.id");
    // IN subquery in binary predicate
    AnalysisError("select * from functional.alltypestiny where " + "(tinyint_col in (1,2)) = (bool_col in (select bool_col from " + "functional.alltypes))", "IN subquery predicates are not supported " + "in binary predicates: (tinyint_col IN (1, 2)) = (bool_col IN (SELECT " + "bool_col FROM functional.alltypes))");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "int_col in (select 1 as int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "int_col not in (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
    // NOT IN uncorrelated aggregate subquery with a constant
    AnalysisError("select * from functional.alltypestiny t1 where " + "10 not in (select max(int_col) from functional.alltypestiny)", "Unsupported NOT IN predicate with subquery: 10 NOT IN (SELECT " + "max(int_col) FROM functional.alltypestiny)");
    AnalysisError("select * from functional.alltypestiny t1 where " + "(10 - 2) not in (select count(*) from functional.alltypestiny)", "Unsupported NOT IN predicate with subquery: (10 - 2) NOT IN " + "(SELECT count(*) FROM functional.alltypestiny)");
}
#end_block

#method_before
@Test
public void TestExistsSubqueries() throws AnalysisException {
    String[] existsOperators = { "exists", "not exists" };
    for (String op : existsOperators) {
        // [NOT] EXISTS predicate (correlated)
        AnalyzesOk(String.format("select * from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.id = t.id)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.int_col = t.int_col and p.bool_col = false)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col and a.bool_col = " + "t.bool_col)", op));
        // Multiple [NOT] EXISTS predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypessmall s where s.id = t.id) and " + "%s (select NULL from functional.alltypesagg g where t.int_col = g.int_col)", op, op));
        // OR between two subqueries
        AnalysisError(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id) or %s " + "(select * from functional.alltypessmall s where s.int_col = t.int_col)", op, op), String.format("Subqueries in OR predicates are not supported: %s " + "(SELECT * FROM functional.alltypesagg a WHERE a.id = t.id) OR %s (SELECT " + "* FROM functional.alltypessmall s WHERE s.int_col = t.int_col)", op.toUpperCase(), op.toUpperCase()));
        // Complex correlation predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id + 1) and " + "%s (select 1 from functional.alltypes s where s.int_col + s.bigint_col = " + "t.bigint_col + 1)", op, op));
        // Correlated predicates
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg g where t.int_col = g.int_col " + "and t.bool_col = false)", op));
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select id from functional.alltypessmall s where t.tinyint_col = " + "s.tinyint_col and t.bool_col)", op));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.int_col = s.int_col))", op, op));
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.bool_col = " + "s.bool_col))", op, op));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t1 where %s " + "(select f11 from t1.complex_nested_struct_col.f2 t2 " + "where t1.id = f11 and %s " + "(select value.f21 from t2.f12 where key = 'test'))", op, op));
        // Correlated EXISTS subquery with aggregation only in the HAVING clause
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col = t2.int_col having count(*) > 1)", op));
        // Correlated EXISTS subquery with a group by and aggregation
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t " + "where %s (select id, count(*) from functional.alltypesagg g where " + "t.id = g.id group by id having count(*) > 2)", op));
        // Correlated EXISTS subquery with a HAVING clause but no grouping or
        // aggregate exprs
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col = t2.int_col having t2.int_col > 1)", op), "Unsupported correlated EXISTS subquery with a HAVING clause: " + "SELECT 1 FROM functional.alltypestiny t2 WHERE t1.int_col = " + "t2.int_col HAVING t2.int_col > 1");
        // Correlated EXISTS subquery with a HAVING clause and non-equality
        // correlated predicates
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col < t2.int_col and t1.id = t2.id group by t2.id " + "having count(1) = 1)", op), "Unsupported correlated " + "EXISTS subquery with a HAVING clause: SELECT 1 FROM " + "functional.alltypestiny t2 WHERE t1.int_col < t2.int_col AND " + "t1.id = t2.id GROUP BY t2.id HAVING count(1) = 1");
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where t1.id = t2.id " + "and (t1.string_col like t2.string_col) = true group by t2.id " + "having count(1) = 1)", op), "Unsupported correlated EXISTS subquery " + "with a HAVING clause: SELECT 1 FROM functional.alltypestiny t2 WHERE " + "t1.id = t2.id AND (t1.string_col LIKE t2.string_col) = TRUE GROUP BY " + "t2.id HAVING count(1) = 1");
        AnalysisError(String.format("select id from functional.allcomplextypes t where %s " + "(select avg(f1) from t.struct_array_col a where t.int_struct_col.f1 < a.f1 " + "and a.f2 != 'xyz' group by a.f2 having count(*) > 2)", op), "Unsupported correlated EXISTS subquery with a HAVING clause: " + "SELECT avg(f1) FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1 " + "AND a.f2 != 'xyz' GROUP BY a.f2 HAVING count(*) > 2");
        // Correlated EXISTS subquery with an analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where %s (select min(bigint_col) over " + "(partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id)", op));
        // Correlated EXISTS subquery with an analytic function and a group by
        // clause
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where exists (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 " + "where t1.id = t2.id group by bigint_col, bool_col)", op));
        // Correlated [NOT] EXISTS subquery with relative table refs.
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where %s " + "(select item from t.int_array_col a where t.id = a.item)", op));
        String[] nullOps = { "is null", "is not null" };
        for (String nullOp : nullOps) {
            // Uncorrelated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes where %s " + "(select * from functional.alltypestiny) %s and id < 5", op, nullOp));
            // Correlated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes t where " + "%s (select 1 from functional.alltypestiny s where t.id = s.id) " + "%s and t.bool_col = false", op, nullOp));
        }
    }
    // Uncorrelated EXISTS subquery with an analytic function
    AnalyzesOk("select * from functional.alltypestiny t " + "where EXISTS (select id, min(int_col) over (partition by bool_col) " + "from functional.alltypesagg a where bigint_col < 10)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        // Allowed because the subquery only has relative table refs.
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t where exists " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 %s a.f1)", cmpOp));
        // Not allowed because the subquery has absolute table refs.
        AnalysisError(String.format("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where t.id %s a.id)", cmpOp), String.format("Unsupported predicate with subquery: EXISTS (SELECT * FROM " + "functional.alltypesagg a WHERE t.id %s a.id)", cmpOp));
    }
    // Uncorrelated EXISTS in a query with GROUP BY
    AnalyzesOk("select id, count(*) from functional.alltypes t " + "where exists (select 1 from functional.alltypestiny where id < 5) group by id");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join is legal with only relative refs in the subquery
    AnalyzesOk("select id from functional.allcomplextypes t where " + "exists (select 1 from t.int_array_col a where t.id = 10)");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join is illegal with absolute table refs in the subquery
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select int_col + 1 from functional.alltypessmall s where " + "t.int_col = 10)", "Unsupported predicate with subquery: EXISTS " + "(SELECT int_col + 1 FROM functional.alltypessmall s WHERE t.int_col = 10)");
    // Uncorrelated EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where exists " + "(select * from functional.alltypesagg where id < 10)");
    AnalyzesOk("select id from functional.alltypestiny where exists " + "(select id from functional.alltypessmall where bool_col = false)");
    AnalyzesOk("select 1 from functional.alltypestiny t where exists " + "(select 1 from functional.alltypessmall where id < 5)");
    AnalyzesOk("select 1 + 1 from functional.alltypestiny where exists " + "(select null from functional.alltypessmall where id != 5)");
    AnalyzesOk(String.format("select id from functional.allcomplextypes t where exists " + "(select item from t.int_array_col a where item < 10)"));
    // Multiple nesting levels with uncorrelated EXISTS
    AnalyzesOk("select id from functional.alltypes where exists " + "(select id from functional.alltypestiny where int_col < 10 and exists (" + "select id from functional.alltypessmall where bool_col = true))");
    // Uncorrelated NOT EXISTS with relative table ref
    AnalyzesOk(String.format("select id from functional.allcomplextypes t where not exists " + "(select item from t.int_array_col a where item < 10)"));
    // Uncorrelated NOT EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where not exists " + "(select 1 from functional.alltypessmall where bool_col = false)");
    // Subquery references an explicit alias from the outer block in the FROM
    // clause
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select * from t)", "Illegal table reference to non-collection type: 't'");
    // Uncorrelated subquery with no FROM clause
    AnalyzesOk("select * from functional.alltypes where exists (select 1,2)");
    // EXISTS subquery in a binary predicate
    AnalysisError("select * from functional.alltypes where " + "if(exists(select * from functional.alltypesagg), 1, 0) = 1", "EXISTS subquery predicates are not supported in binary predicates: " + "if(EXISTS (SELECT * FROM functional.alltypesagg), 1, 0) = 1");
    // Correlated subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select 1 from functional.alltypesagg g where t.id = g.id limit 1)");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "exists (select int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "not exists (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#method_after
@Test
public void TestExistsSubqueries() throws AnalysisException {
    String[] existsOperators = { "exists", "not exists" };
    for (String op : existsOperators) {
        // [NOT] EXISTS predicate (correlated)
        AnalyzesOk(String.format("select * from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.id = t.id)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.int_col = t.int_col and p.bool_col = false)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col and a.bool_col = " + "t.bool_col)", op));
        // Multiple [NOT] EXISTS predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypessmall s where s.id = t.id) and " + "%s (select NULL from functional.alltypesagg g where t.int_col = g.int_col)", op, op));
        // OR between two subqueries
        AnalysisError(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id) or %s " + "(select * from functional.alltypessmall s where s.int_col = t.int_col)", op, op), String.format("Subqueries in OR predicates are not supported: %s " + "(SELECT * FROM functional.alltypesagg a WHERE a.id = t.id) OR %s (SELECT " + "* FROM functional.alltypessmall s WHERE s.int_col = t.int_col)", op.toUpperCase(), op.toUpperCase()));
        // Complex correlation predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id + 1) and " + "%s (select 1 from functional.alltypes s where s.int_col + s.bigint_col = " + "t.bigint_col + 1)", op, op));
        // Correlated predicates
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg g where t.int_col = g.int_col " + "and t.bool_col = false)", op));
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select id from functional.alltypessmall s where t.tinyint_col = " + "s.tinyint_col and t.bool_col)", op));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.int_col = s.int_col))", op, op));
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.bool_col = " + "s.bool_col))", op, op));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t1 where %s " + "(select f11 from t1.complex_nested_struct_col.f2 t2 " + "where t1.id = f11 and %s " + "(select value.f21 from t2.f12 where key = 'test'))", op, op));
        // Correlated EXISTS subquery with aggregation only in the HAVING clause
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col = t2.int_col having count(*) > 1)", op));
        // Correlated EXISTS subquery with a group by and aggregation
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t " + "where %s (select id, count(*) from functional.alltypesagg g where " + "t.id = g.id group by id having count(*) > 2)", op));
        // Correlated EXISTS subquery with a HAVING clause but no grouping or
        // aggregate exprs
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col = t2.int_col having t2.int_col > 1)", op), "Unsupported correlated EXISTS subquery with a HAVING clause: " + "SELECT 1 FROM functional.alltypestiny t2 WHERE t1.int_col = " + "t2.int_col HAVING t2.int_col > 1");
        // Correlated EXISTS subquery with a HAVING clause and non-equality
        // correlated predicates
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col < t2.int_col and t1.id = t2.id group by t2.id " + "having count(1) = 1)", op), "Unsupported correlated " + "EXISTS subquery with a HAVING clause: SELECT 1 FROM " + "functional.alltypestiny t2 WHERE t1.int_col < t2.int_col AND " + "t1.id = t2.id GROUP BY t2.id HAVING count(1) = 1");
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where t1.id = t2.id " + "and (t1.string_col like t2.string_col) = true group by t2.id " + "having count(1) = 1)", op), "Unsupported correlated EXISTS subquery " + "with a HAVING clause: SELECT 1 FROM functional.alltypestiny t2 WHERE " + "t1.id = t2.id AND (t1.string_col LIKE t2.string_col) = TRUE GROUP BY " + "t2.id HAVING count(1) = 1");
        AnalysisError(String.format("select id from functional.allcomplextypes t where %s " + "(select avg(f1) from t.struct_array_col a where t.int_struct_col.f1 < a.f1 " + "and a.f2 != 'xyz' group by a.f2 having count(*) > 2)", op), "Unsupported correlated EXISTS subquery with a HAVING clause: " + "SELECT avg(f1) FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1 " + "AND a.f2 != 'xyz' GROUP BY a.f2 HAVING count(*) > 2");
        // Correlated EXISTS subquery with an analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where %s (select min(bigint_col) over " + "(partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id)", op));
        // Correlated EXISTS subquery with an analytic function and a group by
        // clause
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where exists (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 " + "where t1.id = t2.id group by bigint_col, bool_col)", op));
        // Correlated [NOT] EXISTS subquery with relative table refs.
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where %s " + "(select item from t.int_array_col a where t.id = a.item)", op));
        String[] nullOps = { "is null", "is not null" };
        for (String nullOp : nullOps) {
            // Uncorrelated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes where %s " + "(select * from functional.alltypestiny) %s and id < 5", op, nullOp));
            // Correlated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes t where " + "%s (select 1 from functional.alltypestiny s where t.id = s.id) " + "%s and t.bool_col = false", op, nullOp));
        }
    }
    // Uncorrelated EXISTS subquery with an analytic function
    AnalyzesOk("select * from functional.alltypestiny t " + "where EXISTS (select id, min(int_col) over (partition by bool_col) " + "from functional.alltypesagg a where bigint_col < 10)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        // Allowed because the subquery only has relative table refs.
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t where exists " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 %s a.f1)", cmpOp));
        // Not allowed because the subquery has absolute table refs.
        AnalysisError(String.format("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where t.id %s a.id)", cmpOp), String.format("Unsupported predicate with subquery: EXISTS (SELECT * FROM " + "functional.alltypesagg a WHERE t.id %s a.id)", cmpOp));
    }
    // Correlated BETWEEN predicate with relative table refs.
    AnalyzesOk("select 1 from functional.allcomplextypes t where exists " + "(select a.f1 from t.struct_array_col a " + " where a.f1 between t.int_struct_col.f1 and t.int_struct_col.f2)");
    // Correlated BETWEEN predicate with absolute table refs.
    AnalysisError("select 1 from functional.alltypes t where EXISTS " + "(select id from functional.alltypessmall a " + " where a.int_col between t.tinyint_col and t.bigint_col)", "Unsupported predicate with subquery: " + "EXISTS (SELECT id FROM functional.alltypessmall a " + "WHERE a.int_col >= t.tinyint_col AND a.int_col <= t.bigint_col)");
    // Uncorrelated EXISTS in a query with GROUP BY
    AnalyzesOk("select id, count(*) from functional.alltypes t " + "where exists (select 1 from functional.alltypestiny where id < 5) group by id");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join is legal with only relative refs in the subquery
    AnalyzesOk("select id from functional.allcomplextypes t where " + "exists (select 1 from t.int_array_col a where t.id = 10)");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join is illegal with absolute table refs in the subquery
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select int_col + 1 from functional.alltypessmall s where " + "t.int_col = 10)", "Unsupported predicate with subquery: EXISTS " + "(SELECT int_col + 1 FROM functional.alltypessmall s WHERE t.int_col = 10)");
    // Uncorrelated EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where exists " + "(select * from functional.alltypesagg where id < 10)");
    AnalyzesOk("select id from functional.alltypestiny where exists " + "(select id from functional.alltypessmall where bool_col = false)");
    AnalyzesOk("select 1 from functional.alltypestiny t where exists " + "(select 1 from functional.alltypessmall where id < 5)");
    AnalyzesOk("select 1 + 1 from functional.alltypestiny where exists " + "(select null from functional.alltypessmall where id != 5)");
    AnalyzesOk(String.format("select id from functional.allcomplextypes t where exists " + "(select item from t.int_array_col a where item < 10)"));
    // Multiple nesting levels with uncorrelated EXISTS
    AnalyzesOk("select id from functional.alltypes where exists " + "(select id from functional.alltypestiny where int_col < 10 and exists (" + "select id from functional.alltypessmall where bool_col = true))");
    // Uncorrelated NOT EXISTS with relative table ref
    AnalyzesOk(String.format("select id from functional.allcomplextypes t where not exists " + "(select item from t.int_array_col a where item < 10)"));
    // Uncorrelated NOT EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where not exists " + "(select 1 from functional.alltypessmall where bool_col = false)");
    // Subquery references an explicit alias from the outer block in the FROM
    // clause
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select * from t)", "Illegal table reference to non-collection type: 't'");
    // Uncorrelated subquery with no FROM clause
    AnalyzesOk("select * from functional.alltypes where exists (select 1,2)");
    // EXISTS subquery in a binary predicate
    AnalysisError("select * from functional.alltypes where " + "if(exists(select * from functional.alltypesagg), 1, 0) = 1", "EXISTS subquery predicates are not supported in binary predicates: " + "if(EXISTS (SELECT * FROM functional.alltypesagg), 1, 0) = 1");
    // Correlated subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select 1 from functional.alltypesagg g where t.id = g.id limit 1)");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "exists (select int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "not exists (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#end_block

#method_before
public TResetMetadataResponse execResetMetadata(TResetMetadataRequest req) throws CatalogException {
    TResetMetadataResponse resp = new TResetMetadataResponse();
    resp.setResult(new TCatalogUpdateResult());
    resp.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (req.isSetTable_name()) {
        // Tracks any CatalogObjects updated/added/removed as a result of
        // the invalidate metadata or refresh call. For refresh() it is only expected
        // that a table be modified, but for invalidateTable() the table's parent database
        // may have also been added if it did not previously exist in the catalog.
        Pair<Db, Table> modifiedObjects = new Pair<Db, Table>(null, null);
        boolean wasRemoved = false;
        if (req.isIs_refresh()) {
            TableName tblName = TableName.fromThrift(req.getTable_name());
            Table tbl = getExistingTable(tblName.getDb(), tblName.getTbl());
            if (tbl == null) {
                modifiedObjects.second = null;
            } else {
                modifiedObjects.second = catalog_.reloadTable(tbl);
            }
        } else {
            wasRemoved = catalog_.invalidateTable(req.getTable_name(), modifiedObjects);
        }
        if (modifiedObjects.first == null) {
            TCatalogObject thriftTable = TableToTCatalogObject(modifiedObjects.second);
            if (modifiedObjects.second != null) {
                // processed as a direct DDL operation.
                if (wasRemoved) {
                    resp.getResult().setRemoved_catalog_object_DEPRECATED(thriftTable);
                } else {
                    resp.getResult().setUpdated_catalog_object_DEPRECATED(thriftTable);
                }
            } else {
                // Table does not exist in the meta store and Impala catalog, throw error.
                throw new TableNotFoundException("Table not found: " + req.getTable_name().getDb_name() + "." + req.getTable_name().getTable_name());
            }
            resp.getResult().setVersion(thriftTable.getCatalog_version());
        } else {
            // If there were two catalog objects modified it indicates there was an
            // "invalidateTable()" call that added a new table AND database to the catalog.
            Preconditions.checkState(!req.isIs_refresh());
            Preconditions.checkNotNull(modifiedObjects.first);
            Preconditions.checkNotNull(modifiedObjects.second);
            // The database should always have a lower catalog version than the table because
            // it needs to be created before the table can be added.
            Preconditions.checkState(modifiedObjects.first.getCatalogVersion() < modifiedObjects.second.getCatalogVersion());
            // Since multiple catalog objects were modified, don't treat this as a direct DDL
            // operation. Just set the overall catalog version and the impalad will wait for
            // a statestore heartbeat that contains the update.
            resp.getResult().setVersion(modifiedObjects.second.getCatalogVersion());
        }
    } else {
        // Invalidate the entire catalog if no table name is provided.
        Preconditions.checkArgument(!req.isIs_refresh());
        catalog_.reset();
        resp.result.setVersion(catalog_.getCatalogVersion());
    }
    resp.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    return resp;
}
#method_after
public TResetMetadataResponse execResetMetadata(TResetMetadataRequest req) throws CatalogException {
    TResetMetadataResponse resp = new TResetMetadataResponse();
    resp.setResult(new TCatalogUpdateResult());
    resp.getResult().setCatalog_service_id(JniCatalog.getServiceId());
    if (req.isSetTable_name()) {
        // Tracks any CatalogObjects updated/added/removed as a result of
        // the invalidate metadata or refresh call. For refresh() it is only expected
        // that a table be modified, but for invalidateTable() the table's parent database
        // may have also been added if it did not previously exist in the catalog.
        Pair<Db, Table> modifiedObjects = new Pair<Db, Table>(null, null);
        boolean wasRemoved = false;
        if (req.isIs_refresh()) {
            TableName tblName = TableName.fromThrift(req.getTable_name());
            Table tbl = getExistingTable(tblName.getDb(), tblName.getTbl());
            if (tbl == null) {
                modifiedObjects.second = null;
            } else {
                if (req.isSetPartition_spec()) {
                    modifiedObjects.second = catalog_.reloadPartition(tbl, req.getPartition_spec());
                } else {
                    modifiedObjects.second = catalog_.reloadTable(tbl);
                }
            }
        } else {
            wasRemoved = catalog_.invalidateTable(req.getTable_name(), modifiedObjects);
        }
        if (modifiedObjects.first == null) {
            TCatalogObject thriftTable = TableToTCatalogObject(modifiedObjects.second);
            if (modifiedObjects.second != null) {
                // processed as a direct DDL operation.
                if (wasRemoved) {
                    resp.getResult().setRemoved_catalog_object_DEPRECATED(thriftTable);
                } else {
                    resp.getResult().setUpdated_catalog_object_DEPRECATED(thriftTable);
                }
            } else {
                // Table does not exist in the meta store and Impala catalog, throw error.
                throw new TableNotFoundException("Table not found: " + req.getTable_name().getDb_name() + "." + req.getTable_name().getTable_name());
            }
            resp.getResult().setVersion(thriftTable.getCatalog_version());
        } else {
            // If there were two catalog objects modified it indicates there was an
            // "invalidateTable()" call that added a new table AND database to the catalog.
            Preconditions.checkState(!req.isIs_refresh());
            Preconditions.checkNotNull(modifiedObjects.first);
            Preconditions.checkNotNull(modifiedObjects.second);
            // The database should always have a lower catalog version than the table because
            // it needs to be created before the table can be added.
            Preconditions.checkState(modifiedObjects.first.getCatalogVersion() < modifiedObjects.second.getCatalogVersion());
            // Since multiple catalog objects were modified, don't treat this as a direct DDL
            // operation. Just set the overall catalog version and the impalad will wait for
            // a statestore heartbeat that contains the update.
            resp.getResult().setVersion(modifiedObjects.second.getCatalogVersion());
        }
    } else {
        // Invalidate the entire catalog if no table name is provided.
        Preconditions.checkArgument(!req.isIs_refresh());
        catalog_.reset();
        resp.result.setVersion(catalog_.getCatalogVersion());
    }
    resp.getResult().setStatus(new TStatus(TErrorCode.OK, new ArrayList<String>()));
    return resp;
}
#end_block

#method_before
@Test
public void planHintsTest() {
    String[][] hintStyles = new String[][] { // traditional commented hint
    new String[] { "/* +", "*/" }, // eol commented hint
    new String[] { "\n-- +", "\n" }, // legacy style
    new String[] { "[", "]" } };
    for (String[] hintStyle : hintStyles) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Join hint.
        testToSql(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b on a.id = b.id", prefix, suffix), "SELECT * FROM functional.alltypes a INNER JOIN \n-- +broadcast\n " + "functional.alltypes b ON a.id = b.id");
        // Insert hint.
        testToSql(String.format("insert into functional.alltypes(int_col, bool_col) " + "partition(year, month) %snoshuffle%s " + "select int_col, bool_col, year, month from functional.alltypes", prefix, suffix), "INSERT INTO TABLE functional.alltypes(int_col, bool_col) " + "PARTITION (year, month) \n-- +noshuffle\n " + "SELECT int_col, bool_col, year, month FROM functional.alltypes");
        // Table hint
        testToSql(String.format("select * from functional.alltypes at %sschedule_random_replica%s", prefix, suffix), "SELECT * FROM functional.alltypes at \n-- +schedule_random_replica\n");
        testToSql(String.format("select * from functional.alltypes %sschedule_random_replica%s", prefix, suffix), "SELECT * FROM functional.alltypes \n-- +schedule_random_replica\n");
        testToSql(String.format("select c1 from (select at.tinyint_col as c1 from functional.alltypes at " + "%sschedule_random_replica%s) s1", prefix, suffix), "SELECT c1 FROM (SELECT at.tinyint_col c1 FROM functional.alltypes at \n-- +" + "schedule_random_replica\n) s1");
        // Select-list hint. The legacy-style hint has no prefix and suffix.
        if (prefix.contains("[")) {
            prefix = "";
            suffix = "";
        }
        // Comment-style select-list plan hint.
        testToSql(String.format("select %sstraight_join%s * from functional.alltypes", prefix, suffix), "SELECT \n-- +straight_join\n * FROM functional.alltypes");
        testToSql(String.format("select distinct %sstraight_join%s * from functional.alltypes", prefix, suffix), "SELECT DISTINCT \n-- +straight_join\n * FROM functional.alltypes");
    }
}
#method_after
@Test
public void planHintsTest() {
    String[][] hintStyles = new String[][] { // traditional commented hint
    new String[] { "/* +", "*/" }, // eol commented hint
    new String[] { "\n-- +", "\n" }, // legacy style
    new String[] { "[", "]" } };
    for (String[] hintStyle : hintStyles) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        // Join hint.
        testToSql(String.format("select * from functional.alltypes a join %sbroadcast%s " + "functional.alltypes b on a.id = b.id", prefix, suffix), "SELECT * FROM functional.alltypes a INNER JOIN \n-- +broadcast\n " + "functional.alltypes b ON a.id = b.id");
        // Insert hint.
        testToSql(String.format("insert into functional.alltypes(int_col, bool_col) " + "partition(year, month) %snoshuffle%s " + "select int_col, bool_col, year, month from functional.alltypes", prefix, suffix), "INSERT INTO TABLE functional.alltypes(int_col, bool_col) " + "PARTITION (year, month) \n-- +noshuffle\n " + "SELECT int_col, bool_col, year, month FROM functional.alltypes");
        // Table hint
        testToSql(String.format("select * from functional.alltypes at %sschedule_random_replica%s", prefix, suffix), "SELECT * FROM functional.alltypes at \n-- +schedule_random_replica\n");
        testToSql(String.format("select * from functional.alltypes %sschedule_random_replica%s", prefix, suffix), "SELECT * FROM functional.alltypes \n-- +schedule_random_replica\n");
        testToSql(String.format("select * from functional.alltypes %sschedule_random_replica," + "schedule_disk_local%s", prefix, suffix), "SELECT * FROM functional.alltypes \n-- +schedule_random_replica," + "schedule_disk_local\n");
        testToSql(String.format("select c1 from (select at.tinyint_col as c1 from functional.alltypes at " + "%sschedule_random_replica%s) s1", prefix, suffix), "SELECT c1 FROM (SELECT at.tinyint_col c1 FROM functional.alltypes at \n-- +" + "schedule_random_replica\n) s1");
        // Select-list hint. The legacy-style hint has no prefix and suffix.
        if (prefix.contains("[")) {
            prefix = "";
            suffix = "";
        }
        // Comment-style select-list plan hint.
        testToSql(String.format("select %sstraight_join%s * from functional.alltypes", prefix, suffix), "SELECT \n-- +straight_join\n * FROM functional.alltypes");
        testToSql(String.format("select distinct %sstraight_join%s * from functional.alltypes", prefix, suffix), "SELECT DISTINCT \n-- +straight_join\n * FROM functional.alltypes");
    }
}
#end_block

#method_before
@Test
public void TestTableHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        for (String alias : new String[] { "", "a" }) {
            AnalyzesOk(String.format("select * from functional.alltypes %s " + "%sschedule_random_replica%s", alias, prefix, suffix));
            String name = alias.isEmpty() ? "functional.alltypes" : alias;
            AnalyzesOk(String.format("select * from functional.alltypes %s %sFOO%s", alias, prefix, suffix), String.format("Table hint not recognized for table %s: " + "FOO", name));
            // Table hints not supported for HBase tables
            AnalyzesOk(String.format("select * from functional_hbase.alltypes %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints only supported for Hdfs tables");
            // Table hints not supported for catalog views
            AnalyzesOk(String.format("select * from functional.alltypes_view %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints not supported for inline view and collections");
            // Table hints not supported for with clauses
            AnalyzesOk(String.format("with t as (select 1) select * from t %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints not supported for inline view and collections");
        }
        // Table hints not supported for inline views
        AnalyzesOk(String.format("select * from (select tinyint_col * 2 as c1 from " + "functional.alltypes) as v1 %sschedule_random_replica%s", prefix, suffix), "Table hints not supported for inline view and collections");
        // Table hints not supported for collection tables
        AnalyzesOk(String.format("select item from functional.allcomplextypes, " + "allcomplextypes.int_array_col %sschedule_random_replica%s", prefix, suffix), "Table hints not supported for inline view and collections");
    }
}
#method_after
@Test
public void TestTableHints() throws AnalysisException {
    for (String[] hintStyle : getHintStyles()) {
        String prefix = hintStyle[0];
        String suffix = hintStyle[1];
        for (String alias : new String[] { "", "a" }) {
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_cache_local%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_disk_local%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_remote%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %sschedule_remote," + "schedule_random_replica%s", alias, prefix, suffix));
            AnalyzesOk(String.format("select * from functional.alltypes %s %s" + "schedule_random_replica,schedule_remote%s", alias, prefix, suffix));
            String name = alias.isEmpty() ? "functional.alltypes" : alias;
            AnalyzesOk(String.format("select * from functional.alltypes %s %sFOO%s", alias, prefix, suffix), String.format("Table hint not recognized for table %s: " + "FOO", name));
            // Table hints not supported for HBase tables
            AnalyzesOk(String.format("select * from functional_hbase.alltypes %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints only supported for Hdfs tables");
            // Table hints not supported for catalog views
            AnalyzesOk(String.format("select * from functional.alltypes_view %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints not supported for inline view and collections");
            // Table hints not supported for with clauses
            AnalyzesOk(String.format("with t as (select 1) select * from t %s " + "%sschedule_random_replica%s", alias, prefix, suffix), "Table hints not supported for inline view and collections");
        }
        // Table hints not supported for inline views
        AnalyzesOk(String.format("select * from (select tinyint_col * 2 as c1 from " + "functional.alltypes) as v1 %sschedule_random_replica%s", prefix, suffix), "Table hints not supported for inline view and collections");
        // Table hints not supported for collection tables
        AnalyzesOk(String.format("select item from functional.allcomplextypes, " + "allcomplextypes.int_array_col %sschedule_random_replica%s", prefix, suffix), "Table hints not supported for inline view and collections");
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    correlatedTupleIds_.addAll(queryStmt_.getCorrelatedTupleIds(inlineViewAnalyzer_));
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // not into it)
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i).toLowerCase();
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        Path p = new Path(desc_, Lists.newArrayList(colName));
        Preconditions.checkState(p.resolve());
        SlotDescriptor slotDesc = analyzer.registerSlotRef(p);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicate(colExpr)) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getUniqueAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getUniqueAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    analyzeHints(analyzer);
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (isAnalyzed_)
        return;
    // Analyze the inline view query statement with its own analyzer
    inlineViewAnalyzer_ = new Analyzer(analyzer);
    // Catalog views refs require special analysis settings for authorization.
    boolean isCatalogView = (view_ != null && !view_.isLocalView());
    if (isCatalogView) {
        analyzer.registerAuthAndAuditEvent(view_, analyzer);
        if (inlineViewAnalyzer_.isExplain()) {
            // If the user does not have privileges on the view's definition
            // then we report a masked authorization error so as not to reveal
            // privileged information (e.g., the existence of a table).
            inlineViewAnalyzer_.setAuthErrMsg(String.format("User '%s' does not have privileges to " + "EXPLAIN this statement.", analyzer.getUser().getName()));
        } else {
            // If this is not an EXPLAIN statement, auth checks for the view
            // definition should be disabled.
            inlineViewAnalyzer_.setEnablePrivChecks(false);
        }
    }
    inlineViewAnalyzer_.setUseHiveColLabels(isCatalogView ? true : analyzer.useHiveColLabels());
    queryStmt_.analyze(inlineViewAnalyzer_);
    correlatedTupleIds_.addAll(queryStmt_.getCorrelatedTupleIds(inlineViewAnalyzer_));
    if (explicitColLabels_ != null) {
        Preconditions.checkState(explicitColLabels_.size() == queryStmt_.getColLabels().size());
    }
    inlineViewAnalyzer_.setHasLimitOffsetClause(queryStmt_.hasLimit() || queryStmt_.hasOffset());
    queryStmt_.getMaterializedTupleIds(materializedTupleIds_);
    desc_ = analyzer.registerTableRef(this);
    // true now that we have assigned desc
    isAnalyzed_ = true;
    // For constant selects we materialize its exprs into a tuple.
    if (materializedTupleIds_.isEmpty()) {
        Preconditions.checkState(queryStmt_ instanceof SelectStmt);
        Preconditions.checkState(((SelectStmt) queryStmt_).getTableRefs().isEmpty());
        desc_.setIsMaterialized(true);
        materializedTupleIds_.add(desc_.getId());
    }
    // not into it)
    for (int i = 0; i < getColLabels().size(); ++i) {
        String colName = getColLabels().get(i).toLowerCase();
        Expr colExpr = queryStmt_.getResultExprs().get(i);
        Path p = new Path(desc_, Lists.newArrayList(colName));
        Preconditions.checkState(p.resolve());
        SlotDescriptor slotDesc = analyzer.registerSlotRef(p);
        slotDesc.setSourceExpr(colExpr);
        slotDesc.setStats(ColumnStats.fromExpr(colExpr));
        SlotRef slotRef = new SlotRef(slotDesc);
        smap_.put(slotRef, colExpr);
        baseTblSmap_.put(slotRef, queryStmt_.getBaseTblResultExprs().get(i));
        if (createAuxPredicate(colExpr)) {
            analyzer.createAuxEquivPredicate(new SlotRef(slotDesc), colExpr.clone());
        }
    }
    LOG.trace("inline view " + getUniqueAlias() + " smap: " + smap_.debugString());
    LOG.trace("inline view " + getUniqueAlias() + " baseTblSmap: " + baseTblSmap_.debugString());
    analyzeHints(analyzer);
    // Now do the remaining join analysis
    analyzeJoin(analyzer);
}
#end_block

#method_before
@Test
public void TestSelect() throws ImpalaException {
    // Can select from table that user has privileges on.
    AuthzOk("select * from functional.alltypesagg");
    AuthzOk("select * from functional_seq_snap.alltypes");
    // Can select from view that user has privileges on even though he/she doesn't
    // have privileges on underlying tables.
    AuthzOk("select * from functional.complex_view");
    // User has permission to select the view but not on the view (alltypes_view)
    // referenced in its view definition.
    AuthzOk("select * from functional.view_view");
    // User does not have SELECT privileges on this view.
    AuthzError("select * from functional.complex_view_sub", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.complex_view_sub");
    // User has SELECT privileges on the view and the join table.
    AuthzOk("select a.id from functional.view_view a " + "join functional.alltypesagg b ON (a.id = b.id)");
    // User has SELECT privileges on the view, but does not have privileges
    // to select join table.
    AuthzError("select a.id from functional.view_view a " + "join functional.alltypes b ON (a.id = b.id)", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypes");
    // Constant select.
    AuthzOk("select 1");
    // Unqualified table name.
    AuthzError("select * from alltypes", "User '%s' does not have privileges to execute 'SELECT' on: default.alltypes");
    // Select with no privileges on table.
    AuthzError("select * from functional.alltypes", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypes");
    // Select with no privileges on view.
    AuthzError("select * from functional.complex_view_sub", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.complex_view_sub");
    // Select without referencing a column.
    AuthzError("select 1 from functional.alltypes", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypes");
    // Select from non-existent table.
    AuthzError("select 1 from functional.notbl", "User '%s' does not have privileges to execute 'SELECT' on: functional.notbl");
    // Select from non-existent db.
    AuthzError("select 1 from nodb.alltypes", "User '%s' does not have privileges to execute 'SELECT' on: nodb.alltypes");
    // Table within inline view is authorized properly.
    AuthzError("select a.* from (select * from functional.alltypes) a", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypes");
    // Table within inline view is authorized properly (user has permission).
    AuthzOk("select a.* from (select * from functional.alltypesagg) a");
    // User has SELECT privileges on all the columns of 'alltypestiny'
    AuthzOk("select * from functional.alltypestiny");
    // No SELECT privileges on all the columns of 'alltypessmall'
    AuthzError("select * from functional.alltypessmall", "User '%s' does " + "not have privileges to execute 'SELECT' on: functional.alltypessmall");
    // No SELECT privileges on table 'alltypessmall'
    AuthzError("select count(*) from functional.alltypessmall", "User '%s' does " + "not have privileges to execute 'SELECT' on: functional.alltypessmall");
    AuthzError("select 1 from functional.alltypessmall", "User '%s' does not " + "have privileges to execute 'SELECT' on: functional.alltypessmall");
    AuthzOk("select 1, id from functional.alltypessmall");
    // No SELECT privileges on column 'month'
    AuthzError("select id, int_col, year, month from functional.alltypessmall", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypessmall");
    // User has column-level privileges on all referenced columns
    AuthzOk("select id, count(int_col) from functional.alltypessmall where " + "year = 2010 group by id");
    // No SELECT privileges on 'int_array_col'
    AuthzError("select a.id, b.item from functional.allcomplextypes a, " + "a.int_array_col b", "User '%s' does not have privileges to execute " + "'SELECT' on: functional.allcomplextypes");
    // Sufficient column-level privileges on both scalar and nested columns
    AuthzOk("select a.int_struct_col.f1 from functional.allcomplextypes a " + "where a.id = 1");
    AuthzOk("select pos, item.f1, f2 from functional.allcomplextypes t, " + "t.struct_array_col");
    AuthzOk("select * from functional.allcomplextypes.struct_array_col");
    AuthzOk("select key from functional.allcomplextypes.int_map_col");
    AuthzOk("select id, b.key from functional.allcomplextypes a, a.int_map_col b");
    // No SELECT privileges on 'alltypessmall'
    AuthzError("select a.* from functional.alltypesagg cross join " + "functional.alltypessmall b", "User '%s' does not have privileges to execute " + "'SELECT' on: functional.alltypessmall");
    // User has SELECT privileges on all columns referenced in the inline view
    AuthzOk("select * from (select id, int_col from functional.alltypessmall) v");
}
#method_after
@Test
public void TestSelect() throws ImpalaException {
    // Can select from table that user has privileges on.
    AuthzOk("select * from functional.alltypesagg");
    AuthzOk("select * from functional_seq_snap.alltypes");
    // Can select from view that user has privileges on even though he/she doesn't
    // have privileges on underlying tables.
    AuthzOk("select * from functional.complex_view");
    // User has permission to select the view but not on the view (alltypes_view)
    // referenced in its view definition.
    AuthzOk("select * from functional.view_view");
    // User does not have SELECT privileges on this view.
    AuthzError("select * from functional.complex_view_sub", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.complex_view_sub");
    // User has SELECT privileges on the view and the join table.
    AuthzOk("select a.id from functional.view_view a " + "join functional.alltypesagg b ON (a.id = b.id)");
    // User has SELECT privileges on the view, but does not have privileges
    // to select join table.
    AuthzError("select a.id from functional.view_view a " + "join functional.alltypes b ON (a.id = b.id)", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypes");
    // Tests authorization after a statement has been rewritten (IMPALA-3915).
    // User has SELECT privileges on the view which contains a subquery.
    AuthzOk("select * from functional_seq_snap.subquery_view");
    // User does not have SELECT privileges on the view which contains a subquery.
    AuthzError("select * from functional_rc.subquery_view", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional_rc.subquery_view");
    // Constant select.
    AuthzOk("select 1");
    // Unqualified table name.
    AuthzError("select * from alltypes", "User '%s' does not have privileges to execute 'SELECT' on: default.alltypes");
    // Select with no privileges on table.
    AuthzError("select * from functional.alltypes", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypes");
    // Select with no privileges on view.
    AuthzError("select * from functional.complex_view_sub", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.complex_view_sub");
    // Select without referencing a column.
    AuthzError("select 1 from functional.alltypes", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypes");
    // Select from non-existent table.
    AuthzError("select 1 from functional.notbl", "User '%s' does not have privileges to execute 'SELECT' on: functional.notbl");
    // Select from non-existent db.
    AuthzError("select 1 from nodb.alltypes", "User '%s' does not have privileges to execute 'SELECT' on: nodb.alltypes");
    // Table within inline view is authorized properly.
    AuthzError("select a.* from (select * from functional.alltypes) a", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypes");
    // Table within inline view is authorized properly (user has permission).
    AuthzOk("select a.* from (select * from functional.alltypesagg) a");
    // User has SELECT privileges on all the columns of 'alltypestiny'
    AuthzOk("select * from functional.alltypestiny");
    // No SELECT privileges on all the columns of 'alltypessmall'
    AuthzError("select * from functional.alltypessmall", "User '%s' does " + "not have privileges to execute 'SELECT' on: functional.alltypessmall");
    // No SELECT privileges on table 'alltypessmall'
    AuthzError("select count(*) from functional.alltypessmall", "User '%s' does " + "not have privileges to execute 'SELECT' on: functional.alltypessmall");
    AuthzError("select 1 from functional.alltypessmall", "User '%s' does not " + "have privileges to execute 'SELECT' on: functional.alltypessmall");
    AuthzOk("select 1, id from functional.alltypessmall");
    // No SELECT privileges on column 'month'
    AuthzError("select id, int_col, year, month from functional.alltypessmall", "User '%s' does not have privileges to execute 'SELECT' on: " + "functional.alltypessmall");
    // User has column-level privileges on all referenced columns
    AuthzOk("select id, count(int_col) from functional.alltypessmall where " + "year = 2010 group by id");
    // No SELECT privileges on 'int_array_col'
    AuthzError("select a.id, b.item from functional.allcomplextypes a, " + "a.int_array_col b", "User '%s' does not have privileges to execute " + "'SELECT' on: functional.allcomplextypes");
    // Sufficient column-level privileges on both scalar and nested columns
    AuthzOk("select a.int_struct_col.f1 from functional.allcomplextypes a " + "where a.id = 1");
    AuthzOk("select pos, item.f1, f2 from functional.allcomplextypes t, " + "t.struct_array_col");
    AuthzOk("select * from functional.allcomplextypes.struct_array_col");
    AuthzOk("select key from functional.allcomplextypes.int_map_col");
    AuthzOk("select id, b.key from functional.allcomplextypes a, a.int_map_col b");
    // No SELECT privileges on 'alltypessmall'
    AuthzError("select a.* from functional.alltypesagg cross join " + "functional.alltypessmall b", "User '%s' does not have privileges to execute " + "'SELECT' on: functional.alltypessmall");
    // User has SELECT privileges on all columns referenced in the inline view
    AuthzOk("select * from (select id, int_col from functional.alltypessmall) v");
}
#end_block

#method_before
@Test
public void TestResetMetadata() throws ImpalaException {
    // Positive cases (user has privileges on these tables/views).
    AuthzOk("invalidate metadata functional.alltypesagg");
    AuthzOk("refresh functional.alltypesagg");
    AuthzOk("invalidate metadata functional.view_view");
    AuthzOk("refresh functional.view_view");
    AuthzError("invalidate metadata unknown_db.alltypessmall", "User '%s' does not have privileges to access: unknown_db.alltypessmall");
    AuthzError("invalidate metadata functional_seq.alltypessmall", "User '%s' does not have privileges to access: functional_seq.alltypessmall");
    AuthzError("invalidate metadata functional.alltypes_view", "User '%s' does not have privileges to access: functional.alltypes_view");
    AuthzError("invalidate metadata functional.unknown_table", "User '%s' does not have privileges to access: functional.unknown_table");
    AuthzError("invalidate metadata functional.alltypessmall", "User '%s' does not have privileges to access: functional.alltypessmall");
    AuthzError("refresh functional.alltypessmall", "User '%s' does not have privileges to access: functional.alltypessmall");
    AuthzError("refresh functional.alltypes_view", "User '%s' does not have privileges to access: functional.alltypes_view");
    // Only column-level privileges on the table
    AuthzError("invalidate metadata functional.alltypestiny", "User '%s' does not " + "have privileges to access: functional.alltypestiny");
    // Only column-level privileges on the table
    AuthzError("refresh functional.alltypestiny", "User '%s' does not have " + "privileges to access: functional.alltypestiny");
    AuthzError("invalidate metadata", "User '%s' does not have privileges to access: server");
    // file-based policy.
    if (ctx_.authzConfig.isFileBasedPolicy())
        return;
    SentryPolicyService sentryService = createSentryService();
    try {
        sentryService.grantRoleToGroup(USER, "admin", USER.getName());
        ((ImpaladTestCatalog) ctx_.catalog).reset();
        AuthzOk("invalidate metadata");
    } finally {
        sentryService.revokeRoleFromGroup(USER, "admin", USER.getName());
        ((ImpaladTestCatalog) ctx_.catalog).reset();
    }
}
#method_after
@Test
public void TestResetMetadata() throws ImpalaException {
    // Positive cases (user has privileges on these tables/views).
    AuthzOk("invalidate metadata functional.alltypesagg");
    AuthzOk("refresh functional.alltypesagg");
    AuthzOk("invalidate metadata functional.view_view");
    AuthzOk("refresh functional.view_view");
    // Positive cases for checking refresh partition
    AuthzOk("refresh functional.alltypesagg partition (year=2010, month=1, day=1)");
    AuthzOk("refresh functional.alltypes partition (year=2009, month=1)");
    AuthzOk("refresh functional_seq_snap.alltypes partition (year=2009, month=1)");
    AuthzError("invalidate metadata unknown_db.alltypessmall", "User '%s' does not have privileges to access: unknown_db.alltypessmall");
    AuthzError("invalidate metadata functional_seq.alltypessmall", "User '%s' does not have privileges to access: functional_seq.alltypessmall");
    AuthzError("invalidate metadata functional.alltypes_view", "User '%s' does not have privileges to access: functional.alltypes_view");
    AuthzError("invalidate metadata functional.unknown_table", "User '%s' does not have privileges to access: functional.unknown_table");
    AuthzError("invalidate metadata functional.alltypessmall", "User '%s' does not have privileges to access: functional.alltypessmall");
    AuthzError("refresh functional.alltypessmall", "User '%s' does not have privileges to access: functional.alltypessmall");
    AuthzError("refresh functional.alltypes_view", "User '%s' does not have privileges to access: functional.alltypes_view");
    // Only column-level privileges on the table
    AuthzError("invalidate metadata functional.alltypestiny", "User '%s' does not " + "have privileges to access: functional.alltypestiny");
    // Only column-level privileges on the table
    AuthzError("refresh functional.alltypestiny", "User '%s' does not have " + "privileges to access: functional.alltypestiny");
    AuthzError("invalidate metadata", "User '%s' does not have privileges to access: server");
    AuthzError("refresh functional_rc.alltypesagg partition (year=2010, month=1, day=1)", "User '%s' does not have privileges to access: functional_rc.alltypesagg");
    AuthzError("refresh functional_rc.alltypesagg partition (year=2010, month=1, day=9999)", "User '%s' does not have privileges to access: functional_rc.alltypesagg");
    // file-based policy.
    if (ctx_.authzConfig.isFileBasedPolicy())
        return;
    SentryPolicyService sentryService = createSentryService();
    try {
        sentryService.grantRoleToGroup(USER, "admin", USER.getName());
        ((ImpaladTestCatalog) ctx_.catalog).reset();
        AuthzOk("invalidate metadata");
    } finally {
        sentryService.revokeRoleFromGroup(USER, "admin", USER.getName());
        ((ImpaladTestCatalog) ctx_.catalog).reset();
    }
}
#end_block

#method_before
private static void rewriteWhereClauseSubqueries(SelectStmt stmt, Analyzer analyzer) throws AnalysisException {
    int numTableRefs = stmt.fromClause_.size();
    ArrayList<Expr> exprsWithSubqueries = Lists.newArrayList();
    ExprSubstitutionMap smap = new ExprSubstitutionMap();
    // Replace all BetweenPredicates that contain subqueries with their
    // equivalent compound predicates.
    stmt.whereClause_ = replaceBetweenPredicates(stmt.whereClause_);
    // can currently be rewritten as a join.
    for (Expr conjunct : stmt.whereClause_.getConjuncts()) {
        List<Subquery> subqueries = Lists.newArrayList();
        conjunct.collectAll(Predicates.instanceOf(Subquery.class), subqueries);
        if (subqueries.size() == 0)
            continue;
        if (subqueries.size() > 1) {
            throw new AnalysisException("Multiple subqueries are not supported in " + "expression: " + conjunct.toSql());
        }
        if (!(conjunct instanceof InPredicate) && !(conjunct instanceof ExistsPredicate) && !(conjunct instanceof BinaryPredicate) && !conjunct.contains(Expr.IS_SCALAR_SUBQUERY)) {
            throw new AnalysisException("Non-scalar subquery is not supported in " + "expression: " + conjunct.toSql());
        }
        if (conjunct instanceof ExistsPredicate) {
            // Check if we can determine the result of an ExistsPredicate during analysis.
            // If so, replace the predicate with a BoolLiteral predicate and remove it from
            // the list of predicates to be rewritten.
            BoolLiteral boolLiteral = replaceExistsPredicate((ExistsPredicate) conjunct);
            if (boolLiteral != null) {
                boolLiteral.analyze(analyzer);
                smap.put(conjunct, boolLiteral);
                continue;
            }
        }
        // Replace all the supported exprs with subqueries with true BoolLiterals
        // using an smap.
        BoolLiteral boolLiteral = new BoolLiteral(true);
        boolLiteral.analyze(analyzer);
        smap.put(conjunct, boolLiteral);
        exprsWithSubqueries.add(conjunct);
    }
    stmt.whereClause_ = stmt.whereClause_.substitute(smap, analyzer, false);
    boolean hasNewVisibleTuple = false;
    // with 'stmt'.
    for (Expr expr : exprsWithSubqueries) {
        if (mergeExpr(stmt, rewriteExpr(expr, analyzer), analyzer)) {
            hasNewVisibleTuple = true;
        }
    }
    if (canEliminate(stmt.whereClause_))
        stmt.whereClause_ = null;
    if (hasNewVisibleTuple)
        replaceUnqualifiedStarItems(stmt, numTableRefs);
}
#method_after
private static void rewriteWhereClauseSubqueries(SelectStmt stmt, Analyzer analyzer) throws AnalysisException {
    int numTableRefs = stmt.fromClause_.size();
    ArrayList<Expr> exprsWithSubqueries = Lists.newArrayList();
    ExprSubstitutionMap smap = new ExprSubstitutionMap();
    // Replace all BetweenPredicates with their equivalent compound predicates.
    stmt.whereClause_ = rewriteBetweenPredicates(stmt.whereClause_);
    // can currently be rewritten as a join.
    for (Expr conjunct : stmt.whereClause_.getConjuncts()) {
        List<Subquery> subqueries = Lists.newArrayList();
        conjunct.collectAll(Predicates.instanceOf(Subquery.class), subqueries);
        if (subqueries.size() == 0)
            continue;
        if (subqueries.size() > 1) {
            throw new AnalysisException("Multiple subqueries are not supported in " + "expression: " + conjunct.toSql());
        }
        if (!(conjunct instanceof InPredicate) && !(conjunct instanceof ExistsPredicate) && !(conjunct instanceof BinaryPredicate) && !conjunct.contains(Expr.IS_SCALAR_SUBQUERY)) {
            throw new AnalysisException("Non-scalar subquery is not supported in " + "expression: " + conjunct.toSql());
        }
        if (conjunct instanceof ExistsPredicate) {
            // Check if we can determine the result of an ExistsPredicate during analysis.
            // If so, replace the predicate with a BoolLiteral predicate and remove it from
            // the list of predicates to be rewritten.
            BoolLiteral boolLiteral = replaceExistsPredicate((ExistsPredicate) conjunct);
            if (boolLiteral != null) {
                boolLiteral.analyze(analyzer);
                smap.put(conjunct, boolLiteral);
                continue;
            }
        }
        // Replace all the supported exprs with subqueries with true BoolLiterals
        // using an smap.
        BoolLiteral boolLiteral = new BoolLiteral(true);
        boolLiteral.analyze(analyzer);
        smap.put(conjunct, boolLiteral);
        exprsWithSubqueries.add(conjunct);
    }
    stmt.whereClause_ = stmt.whereClause_.substitute(smap, analyzer, false);
    boolean hasNewVisibleTuple = false;
    // with 'stmt'.
    for (Expr expr : exprsWithSubqueries) {
        if (mergeExpr(stmt, rewriteExpr(expr, analyzer), analyzer)) {
            hasNewVisibleTuple = true;
        }
    }
    if (canEliminate(stmt.whereClause_))
        stmt.whereClause_ = null;
    if (hasNewVisibleTuple)
        replaceUnqualifiedStarItems(stmt, numTableRefs);
}
#end_block

#method_before
@Test
public void TestSelect() throws AuthorizationException, AnalysisException {
    // Simple select from a table.
    Set<TAccessEvent> accessEvents = AnalyzeAccessEvents("select * from functional.alltypesagg");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypesagg", TCatalogObjectType.TABLE, "SELECT")));
    // Select from a view. Expect to get 3 events back - one for the view and two
    // for the underlying objects that the view accesses.
    accessEvents = AnalyzeAccessEvents("select * from functional.view_view");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.view_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("functional.alltypes_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("functional.alltypes", TCatalogObjectType.TABLE, "SELECT")));
    // Select from an inline-view.
    accessEvents = AnalyzeAccessEvents("select a.* from (select * from functional.alltypesagg) a");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypesagg", TCatalogObjectType.TABLE, "SELECT")));
    // Select from collection table references.
    accessEvents = AnalyzeAccessEvents("select item from functional.allcomplextypes.int_array_col");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.allcomplextypes", TCatalogObjectType.TABLE, "SELECT")));
    accessEvents = AnalyzeAccessEvents("select item from functional.allcomplextypes a, a.int_array_col");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.allcomplextypes", TCatalogObjectType.TABLE, "SELECT")));
}
#method_after
@Test
public void TestSelect() throws AuthorizationException, AnalysisException {
    // Simple select from a table.
    Set<TAccessEvent> accessEvents = AnalyzeAccessEvents("select * from functional.alltypesagg");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypesagg", TCatalogObjectType.TABLE, "SELECT")));
    // Select from a view. Expect to get 3 events back - one for the view and two
    // for the underlying objects that the view accesses.
    accessEvents = AnalyzeAccessEvents("select * from functional.view_view");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.view_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("functional.alltypes_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("functional.alltypes", TCatalogObjectType.TABLE, "SELECT")));
    // Tests audit events after a statement has been rewritten (IMPALA-3915).
    // Select from a view that contains a subquery.
    accessEvents = AnalyzeAccessEvents("select * from functional_rc.subquery_view");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional_rc.alltypessmall", TCatalogObjectType.TABLE, "SELECT"), new TAccessEvent("functional_rc.alltypes", TCatalogObjectType.TABLE, "SELECT"), new TAccessEvent("functional_rc.subquery_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("_impala_builtins", TCatalogObjectType.DATABASE, "VIEW_METADATA")));
    // Select from an inline view.
    accessEvents = AnalyzeAccessEvents("select a.* from (select * from functional.alltypesagg) a");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypesagg", TCatalogObjectType.TABLE, "SELECT")));
    // Select from collection table references.
    accessEvents = AnalyzeAccessEvents("select item from functional.allcomplextypes.int_array_col");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.allcomplextypes", TCatalogObjectType.TABLE, "SELECT")));
    accessEvents = AnalyzeAccessEvents("select item from functional.allcomplextypes a, a.int_array_col");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.allcomplextypes", TCatalogObjectType.TABLE, "SELECT")));
}
#end_block

#method_before
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    tableRef.analyze(this);
    Path resolvedPath = tableRef.getResolvedPath();
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof KuduTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef);
    } else {
        return new CollectionTableRef(tableRef);
    }
}
#method_after
public TableRef resolveTableRef(TableRef tableRef) throws AnalysisException {
    // Return the table if it is already resolved.
    if (tableRef.isResolved())
        return tableRef;
    // Try to find a matching local view.
    if (tableRef.getPath().size() == 1) {
        // Searches the hierarchy of analyzers bottom-up for a registered local view with
        // a matching alias.
        String viewAlias = tableRef.getPath().get(0).toLowerCase();
        Analyzer analyzer = this;
        do {
            View localView = analyzer.localViews_.get(viewAlias);
            if (localView != null)
                return new InlineViewRef(localView, tableRef);
            analyzer = (analyzer.ancestors_.isEmpty() ? null : analyzer.ancestors_.get(0));
        } while (analyzer != null);
    }
    // Resolve the table ref's path and determine what resolved table ref
    // to replace it with.
    List<String> rawPath = tableRef.getPath();
    Path resolvedPath = null;
    try {
        resolvedPath = resolvePath(tableRef.getPath(), PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath.size() > 1) {
                registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath.get(0), rawPath.get(1)).allOf(Privilege.SELECT).toRequest());
            }
            registerPrivReq(new PrivilegeRequestBuilder().onTable(getDefaultDb(), rawPath.get(0)).allOf(Privilege.SELECT).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath)), e);
    }
    Preconditions.checkNotNull(resolvedPath);
    if (resolvedPath.destTable() != null) {
        Table table = resolvedPath.destTable();
        Preconditions.checkNotNull(table);
        if (table instanceof View)
            return new InlineViewRef((View) table, tableRef);
        // The table must be a base table.
        Preconditions.checkState(table instanceof HdfsTable || table instanceof KuduTable || table instanceof HBaseTable || table instanceof DataSourceTable);
        return new BaseTableRef(tableRef, resolvedPath);
    } else {
        return new CollectionTableRef(tableRef, resolvedPath);
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    try {
        resolvedPath_ = analyzer.resolvePath(rawPath_, PathType.TABLE_REF);
    } catch (AnalysisException e) {
        if (!analyzer.hasMissingTbls()) {
            // table/database if the user is not authorized.
            if (rawPath_.size() > 1) {
                analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(rawPath_.get(0), rawPath_.get(1)).allOf(getPrivilegeRequirement()).toRequest());
            }
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(analyzer.getDefaultDb(), rawPath_.get(0)).allOf(getPrivilegeRequirement()).toRequest());
        }
        throw e;
    } catch (TableLoadingException e) {
        throw new AnalysisException(String.format("Failed to load metadata for table: '%s'", Joiner.on(".").join(rawPath_)), e);
    }
    Preconditions.checkState(resolvedPath_ != null);
    if (resolvedPath_.isRootedAtTable()) {
        // Add access event for auditing.
        Table table = resolvedPath_.getRootTable();
        if (table instanceof View) {
            View view = (View) table;
            if (!view.isLocalView()) {
                analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.VIEW, getPrivilegeRequirement().toString()));
            }
        } else {
            analyzer.addAccessEvent(new TAccessEvent(table.getFullName(), TCatalogObjectType.TABLE, getPrivilegeRequirement().toString()));
        }
        // Add privilege requests for authorization.
        TableName tableName = table.getTableName();
        analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(tableName.getDb(), tableName.getTbl()).allOf(getPrivilegeRequirement()).toRequest());
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    throw new IllegalStateException("Should not call analyze() on an unresolved TableRef.");
}
#end_block

#method_before
private void analyzeTableHints(Analyzer analyzer) {
    if (tableHints_ == null)
        return;
    if (!(this instanceof BaseTableRef)) {
        analyzer.addWarning("Table hints not supported for inline view and collections");
        return;
    }
    // BaseTableRef will always have their path resolved at this point.
    Preconditions.checkState(getResolvedPath() != null);
    if (getResolvedPath().destTable() != null && !(getResolvedPath().destTable() instanceof HdfsTable)) {
        analyzer.addWarning("Table hints only supported for Hdfs tables");
    }
    for (String hint : tableHints_) {
        if (hint.equalsIgnoreCase("SCHEDULE_RANDOM_REPLICA")) {
            analyzer.setHasPlanHints();
            randomReplica_ = true;
        } else {
            Preconditions.checkState(getAliases() != null && getAliases().length > 0);
            analyzer.addWarning("Table hint not recognized for table " + getUniqueAlias() + ": " + hint);
        }
    }
}
#method_after
private void analyzeTableHints(Analyzer analyzer) {
    if (tableHints_ == null)
        return;
    if (!(this instanceof BaseTableRef)) {
        analyzer.addWarning("Table hints not supported for inline view and collections");
        return;
    }
    // BaseTableRef will always have their path resolved at this point.
    Preconditions.checkState(getResolvedPath() != null);
    if (getResolvedPath().destTable() != null && !(getResolvedPath().destTable() instanceof HdfsTable)) {
        analyzer.addWarning("Table hints only supported for Hdfs tables");
    }
    for (String hint : tableHints_) {
        if (hint.equalsIgnoreCase("SCHEDULE_CACHE_LOCAL")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.CACHE_LOCAL;
        } else if (hint.equalsIgnoreCase("SCHEDULE_DISK_LOCAL")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.DISK_LOCAL;
        } else if (hint.equalsIgnoreCase("SCHEDULE_REMOTE")) {
            analyzer.setHasPlanHints();
            replicaPreference_ = TReplicaPreference.REMOTE;
        } else if (hint.equalsIgnoreCase("SCHEDULE_RANDOM_REPLICA")) {
            analyzer.setHasPlanHints();
            randomReplica_ = true;
        } else {
            Preconditions.checkState(getAliases() != null && getAliases().length > 0);
            analyzer.addWarning("Table hint not recognized for table " + getUniqueAlias() + ": " + hint);
        }
    }
}
#end_block

#method_before
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (tableName_ != null) {
        String dbName = analyzer.getTargetDbName(tableName_);
        tableName_ = new TableName(dbName, tableName_.getTbl());
        if (isRefresh_) {
            // metadata if it is not yet in this impalad's catalog cache.
            if (!analyzer.dbContainsTable(dbName, tableName_.getTbl(), Privilege.ANY)) {
                // to Impala.
                throw new AnalysisException(Analyzer.TBL_DOES_NOT_EXIST_ERROR_MSG + tableName_);
            }
        } else {
            // Verify the user has privileges to access this table.
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(dbName, tableName_.getTbl()).any().toRequest());
        }
    } else {
        analyzer.registerPrivReq(new PrivilegeRequest(Privilege.ALL));
    }
}
#method_after
@Override
public void analyze(Analyzer analyzer) throws AnalysisException {
    if (tableName_ != null) {
        String dbName = analyzer.getTargetDbName(tableName_);
        tableName_ = new TableName(dbName, tableName_.getTbl());
        if (isRefresh_) {
            // metadata if it is not yet in this impalad's catalog cache.
            if (!analyzer.dbContainsTable(dbName, tableName_.getTbl(), Privilege.ANY)) {
                // to Impala.
                throw new AnalysisException(Analyzer.TBL_DOES_NOT_EXIST_ERROR_MSG + tableName_);
            }
            if (partitionSpec_ != null) {
                partitionSpec_.setPrivilegeRequirement(Privilege.ANY);
                partitionSpec_.analyze(analyzer);
            }
        } else {
            // Verify the user has privileges to access this table.
            analyzer.registerPrivReq(new PrivilegeRequestBuilder().onTable(dbName, tableName_.getTbl()).any().toRequest());
        }
    } else {
        analyzer.registerPrivReq(new PrivilegeRequest(Privilege.ALL));
    }
}
#end_block

#method_before
@Override
public String toSql() {
    StringBuilder result = new StringBuilder();
    if (isRefresh_) {
        result.append("INVALIDATE METADATA");
    } else {
        result.append("REFRESH");
    }
    if (tableName_ != null)
        result.append(" ").append(tableName_);
    return result.toString();
}
#method_after
@Override
public String toSql() {
    StringBuilder result = new StringBuilder();
    if (isRefresh_) {
        result.append("INVALIDATE METADATA");
    } else {
        result.append("REFRESH");
    }
    if (tableName_ != null)
        result.append(" ").append(tableName_);
    if (partitionSpec_ != null)
        result.append(" " + partitionSpec_.toSql());
    return result.toString();
}
#end_block

#method_before
public TResetMetadataRequest toThrift() {
    TResetMetadataRequest params = new TResetMetadataRequest();
    params.setIs_refresh(isRefresh_);
    if (tableName_ != null) {
        params.setTable_name(new TTableName(tableName_.getDb(), tableName_.getTbl()));
    }
    return params;
}
#method_after
public TResetMetadataRequest toThrift() {
    TResetMetadataRequest params = new TResetMetadataRequest();
    params.setIs_refresh(isRefresh_);
    if (tableName_ != null) {
        params.setTable_name(new TTableName(tableName_.getDb(), tableName_.getTbl()));
    }
    if (partitionSpec_ != null) {
        params.setPartition_spec(partitionSpec_.toThrift());
    }
    return params;
}
#end_block

#method_before
public void authorize(AuthorizationChecker authzChecker) throws AuthorizationException, InternalException {
    Preconditions.checkNotNull(analysisResult_);
    Analyzer analyzer = getAnalyzer();
    // Process statements for which column-level privilege requests may be registered.
    if (analysisResult_.isQueryStmt() || analysisResult_.isInsertStmt() || analysisResult_.isUpdateStmt() || analysisResult_.isDeleteStmt() || analysisResult_.isCreateTableAsSelectStmt() || analysisResult_.isCreateViewStmt() || analysisResult_.isAlterViewStmt()) {
        // Map of table name to a list of privilege requests associated with that table.
        // These include both table-level and column-level privilege requests.
        Map<String, List<PrivilegeRequest>> tablePrivReqs = Maps.newHashMap();
        // Privilege requests that are not column or table-level.
        List<PrivilegeRequest> otherPrivReqs = Lists.newArrayList();
        // Group the registered privilege requests based on the table they reference.
        for (PrivilegeRequest privReq : analyzer.getPrivilegeReqs()) {
            String tableName = privReq.getAuthorizeable().getFullTableName();
            if (tableName == null) {
                otherPrivReqs.add(privReq);
            } else {
                List<PrivilegeRequest> requests = tablePrivReqs.get(tableName);
                if (requests == null) {
                    requests = Lists.newArrayList();
                    tablePrivReqs.put(tableName, requests);
                }
                // The table-level SELECT must be the first table-level request, and it
                // must precede all column-level privilege requests.
                Preconditions.checkState((requests.isEmpty() || !(privReq.getAuthorizeable() instanceof AuthorizeableColumn)) || (requests.get(0).getAuthorizeable() instanceof AuthorizeableTable && requests.get(0).getPrivilege() == Privilege.SELECT));
                requests.add(privReq);
            }
        }
        // Check any non-table, non-column privilege requests first.
        for (PrivilegeRequest request : otherPrivReqs) {
            authorizePrivilegeRequest(authzChecker, request);
        }
        // column-level privilege requests.
        for (Map.Entry<String, List<PrivilegeRequest>> entry : tablePrivReqs.entrySet()) {
            authorizeTableAccess(authzChecker, entry.getValue());
        }
    } else {
        for (PrivilegeRequest privReq : analyzer.getPrivilegeReqs()) {
            Preconditions.checkState(!(privReq.getAuthorizeable() instanceof AuthorizeableColumn) || analysisResult_.isDescribeTableStmt());
            authorizePrivilegeRequest(authzChecker, privReq);
        }
    }
    // Check any masked requests.
    for (Pair<PrivilegeRequest, String> maskedReq : analyzer.getMaskedPrivilegeReqs()) {
        if (!authzChecker.hasAccess(analyzer.getUser(), maskedReq.first)) {
            throw new AuthorizationException(maskedReq.second);
        }
    }
}
#method_after
public void authorize(AuthorizationChecker authzChecker) throws AuthorizationException, InternalException {
    Preconditions.checkNotNull(analysisResult_);
    Analyzer analyzer = getAnalyzer();
    // except for DESCRIBE TABLE or REFRESH/INVALIDATE statements
    if (analysisResult_.isQueryStmt() || analysisResult_.isInsertStmt() || analysisResult_.isUpdateStmt() || analysisResult_.isDeleteStmt() || analysisResult_.isCreateTableAsSelectStmt() || analysisResult_.isCreateViewStmt() || analysisResult_.isAlterViewStmt()) {
        // Map of table name to a list of privilege requests associated with that table.
        // These include both table-level and column-level privilege requests.
        Map<String, List<PrivilegeRequest>> tablePrivReqs = Maps.newHashMap();
        // Privilege requests that are not column or table-level.
        List<PrivilegeRequest> otherPrivReqs = Lists.newArrayList();
        // Group the registered privilege requests based on the table they reference.
        for (PrivilegeRequest privReq : analyzer.getPrivilegeReqs()) {
            String tableName = privReq.getAuthorizeable().getFullTableName();
            if (tableName == null) {
                otherPrivReqs.add(privReq);
            } else {
                List<PrivilegeRequest> requests = tablePrivReqs.get(tableName);
                if (requests == null) {
                    requests = Lists.newArrayList();
                    tablePrivReqs.put(tableName, requests);
                }
                // The table-level SELECT must be the first table-level request, and it
                // must precede all column-level privilege requests.
                Preconditions.checkState((requests.isEmpty() || !(privReq.getAuthorizeable() instanceof AuthorizeableColumn)) || (requests.get(0).getAuthorizeable() instanceof AuthorizeableTable && requests.get(0).getPrivilege() == Privilege.SELECT));
                requests.add(privReq);
            }
        }
        // Check any non-table, non-column privilege requests first.
        for (PrivilegeRequest request : otherPrivReqs) {
            authorizePrivilegeRequest(authzChecker, request);
        }
        // column-level privilege requests.
        for (Map.Entry<String, List<PrivilegeRequest>> entry : tablePrivReqs.entrySet()) {
            authorizeTableAccess(authzChecker, entry.getValue());
        }
    } else {
        for (PrivilegeRequest privReq : analyzer.getPrivilegeReqs()) {
            Preconditions.checkState(!(privReq.getAuthorizeable() instanceof AuthorizeableColumn) || analysisResult_.isDescribeTableStmt() || analysisResult_.isResetMetadataStmt());
            authorizePrivilegeRequest(authzChecker, privReq);
        }
    }
    // Check any masked requests.
    for (Pair<PrivilegeRequest, String> maskedReq : analyzer.getMaskedPrivilegeReqs()) {
        if (!authzChecker.hasAccess(analyzer.getUser(), maskedReq.first)) {
            throw new AuthorizationException(maskedReq.second);
        }
    }
}
#end_block

#method_before
@Test
public void TestInSubqueries() throws AnalysisException {
    String[] colNames = { "bool_col", "tinyint_col", "smallint_col", "int_col", "bigint_col", "float_col", "double_col", "string_col", "date_string_col", "timestamp_col" };
    String[] joinOperators = { "inner join", "left outer join", "right outer join", "left semi join", "left anti join" };
    // [NOT] IN subquery predicates
    String[] operators = { "in", "not in" };
    for (String op : operators) {
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select id from functional.alltypestiny)", op));
        // Using column and table aliases similar to the ones produced by the
        // column/table alias generators during a rewrite.
        AnalyzesOk(String.format("select id `$c$1` from functional.alltypestiny `$a$1` " + "where id %s (select id from functional.alltypessmall)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "t.id %s (select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select t.id, max(t.int_col) from " + "functional.alltypes t where t.int_col %s (select int_col from " + "functional.alltypesagg) group by t.id having count(*) < 10", op));
        AnalyzesOk(String.format("select t.bigint_col, t.string_col from " + "functional.alltypes t where t.id %s (select id from " + "functional.alltypesagg where int_col < 10) order by bigint_col", op));
        AnalyzesOk(String.format("select * from functional.alltypes a where a.id %s " + "(select id from functional.alltypes b where a.id = b.id)", op));
        // Complex expressions
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id + int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "t.int_col + 1 %s (select int_col - 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "abs(t.double_col) %s (select int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select NULL from functional.alltypes t where " + "cast(t.double_col as int) %s (select int_col from " + "functional.alltypestiny)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes where id %s " + "(select 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select 1 + 1 from functional.alltypestiny group by int_col)", op));
        AnalyzesOk(String.format("select max(id) from functional.alltypes where id %s " + "(select max(id) from functional.alltypesagg a where a.int_col < 10) " + "and bool_col = false", op));
        // Subquery returns multiple columns
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select id, int_col from functional.alltypessmall)", op), "Subquery must return a single column: (SELECT id, int_col " + "FROM functional.alltypessmall)");
        // Subquery returns an incompatible column type
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select timestamp_col from functional.alltypessmall)", op), "Incompatible return types 'INT' and 'TIMESTAMP' of exprs 'id' and " + "'timestamp_col'.");
        // Different column types in the subquery predicate
        for (String col : colNames) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.%s %s " + "(select a.%s from functional.alltypestiny a)", col, op, col));
        }
        // Decimal in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.double_col %s (select d3 from functional.decimal_tbl a)", op));
        // Varchar in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.string_col %s (select cast(a.string_col as varchar(1)) from " + "functional.alltypestiny a)", op));
        // Subqueries with multiple predicates in the WHERE clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col > 10 and " + "a.tinyint_col < 5)", op));
        // Subqueries with a GROUP BY clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.double_col < 10.1 " + "group by a.id)", op));
        // Subqueries with GROUP BY and HAVING clauses
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.bool_col = true and " + "int_col < 10 group by id having count(*) < 10)", op));
        // Subqueries with a LIMIT clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where id < 100 limit 10)", op));
        // Subqueries with multiple tables in the FROM clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, functional.alltypessmall s " + "where a.int_col = s.int_col and s.bigint_col < 100 and a.tinyint_col < 10)", op));
        // Different join operators between the tables in subquery's FROM clause
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a %s functional.alltypessmall " + "s on a.int_col = s.int_col where a.bool_col = false)", op, joinOp));
        }
        // Subquery with relative table references
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where id %s " + "(select f1 from t.struct_array_col a, t.int_array_col b " + "where f2 = 'xyz' and b.item < 3 group by f1 having count(*) > 2 limit 5)", op));
        // Correlated predicates in the subquery's ON clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.bigint_col = a.bigint_col and " + "s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on a.bool_col = s.bool_col and t.int_col = 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on ifnull(s.int_col, s.int_col + 20) = " + "t.int_col + t.bigint_col)", op));
        // Subqueries with inline views
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, " + "(select * from functional.alltypessmall) s where s.int_col = a.int_col " + "and s.bool_col = false)", op));
        // Subqueries with inline views that contain subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select id from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select g.* from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a where " + "a.bigint_col = 100)", op));
        // Multiple tables in the FROM clause of the outer query block
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t %s " + "functional.alltypessmall s on t.int_col = s.int_col where " + "t.tinyint_col %s (select tinyint_col from functional.alltypesagg) " + "and t.bool_col = false and t.bigint_col = 10", joinOp, op));
        }
        // Subqueries in WITH clause
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a where " + "id %s (select id from functional.alltypestiny)) select * from t where " + "t.bool_col = false and t.int_col = 10", op));
        // Subqueries in WITH and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny s)) select * from t " + "where t.int_col in (select int_col from functional.alltypessmall) and " + "t.bool_col = false", op));
        // Subqueries in WITH, FROM and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny)) select t.* from t, " + "(select * from functional.alltypesagg g where g.id in " + "(select id from functional.alltypes)) s where s.string_col = t.string_col " + "and t.int_col in (select int_col from functional.alltypessmall) and " + "s.bool_col = false", op));
        // Correlated subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col) " + "and t.bool_col = false", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col + 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + a.int_col  = " + "a.bigint_col and a.bool_col = true)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col = false and " + "a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a " + " where a.int_col between t.tinyint_col and t.bigint_col)", op));
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where id %s " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 = a.f1)", op));
        // Multiple nesting levels (uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg where int_col %s " + "(select int_col from functional.alltypestiny) and bool_col = false) " + "and bigint_col < 1000", op, op));
        // Multiple nesting levels (correlated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where a.int_col = t.int_col " + "and a.tinyint_col %s (select tinyint_col from functional.alltypestiny s " + "where s.bigint_col = a.bigint_col))", op, op));
        // Multiple nesting levels (correlated and uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col " + "and a.int_col %s (select int_col from functional.alltypestiny s))", op, op));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t1 where id %s " + "(select f11 from t1.complex_nested_struct_col.f2 t2 " + "where t1.year = f11 and f11 %s " + "(select value.f21 from t2.f12 where key = 'test'))", op, op));
        // NOT ([NOT] IN predicate)
        AnalyzesOk(String.format("select * from functional.alltypes t where not (id %s " + "(select id from functional.alltypesagg))", op));
        // Different cmp operators in the correlation predicate
        for (String cmpOp : cmpOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t " + "where t.id %s (select a.id from functional.alltypesagg a where " + "t.int_col %s a.int_col)", op, cmpOp));
        }
        // Uncorrelated IN subquery with analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col %s (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", op));
    }
    // Constant on the left hand side
    AnalyzesOk("select * from functional.alltypes a where 1 in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)");
    AnalysisError("select * from functional.alltypes a where 1 not in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)", "Unsupported NOT IN predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypesagg s WHERE s.int_col = a.int_col)");
    // IN subquery that is equivalent to an uncorrelated EXISTS subquery
    AnalysisError("select * from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg)", "Unsupported " + "predicate with subquery: 1 IN (SELECT int_col FROM functional.alltypesagg)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        // Allowed because the subquery only has relative table refs.
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 %s a.f1)", cmpOp));
        // Not allowed because the subquery has absolute table refs.
        AnalysisError(String.format("select 1 from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg g where g.id %s t.id)", cmpOp), String.format("Unsupported predicate with subquery: 1 " + "IN (SELECT int_col FROM functional.alltypesagg g WHERE g.id %s t.id)", cmpOp));
    }
    // NOT IN subquery with a correlated non-equi predicate is ok if the subquery only
    // has relative table refs
    AnalyzesOk("select 1 from functional.allcomplextypes t where id not in " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1)");
    // NOT IN subquery with a correlated non-equi predicate is not ok if the subquery
    // has absolute table refs
    AnalysisError("select 1 from functional.alltypes t where 1 not in " + "(select id from functional.alltypestiny g where g.id < t.id)", "Unsupported predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypestiny g WHERE g.id < t.id)");
    // Statement with a GROUP BY and a correlated IN subquery that has a non-equi
    // correlated predicate and only relative table refs
    AnalyzesOk("select id, count(*) from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1) " + "group by id");
    // Statement with a GROUP BY and a correlated IN subquery that has a non-equi
    // correlated predicate and absolute table refs
    AnalysisError("select id, count(*) from functional.alltypes t " + "where 1 IN (select id from functional.alltypesagg g where t.int_col < " + "g.int_col) group by id", "Unsupported predicate with subquery: 1 IN " + "(SELECT id FROM functional.alltypesagg g WHERE t.int_col < g.int_col)");
    // Reference a non-existing table in the subquery
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s left outer join p on " + "(s.int_col = p.int_col))", "Could not resolve table reference: 'p'");
    // Reference a non-existing column from a table in the outer scope
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s where s.int_col = t.bad_col)", "Could not resolve column/field reference: 't.bad_col'");
    // Referencing the same table in the inner and the outer query block
    // No explicit alias
    AnalyzesOk("select id from functional.alltypestiny where int_col in " + "(select int_col from functional.alltypestiny)");
    // Different alias between inner and outer block referencing the same table
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny p)");
    // Alias only in the outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny)");
    // Same alias in both inner and outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny t)");
    // Binary predicate with non-comparable operands
    AnalysisError("select * from functional.alltypes t where " + "(id in (select id from functional.alltypestiny)) = 'string_val'", "operands of type BOOLEAN and STRING are not comparable: " + "(id IN (SELECT id FROM functional.alltypestiny)) = 'string_val'");
    // OR with subquery predicates
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select id from functional.alltypesagg) or t.bool_col = false", "Subqueries in OR predicates are not supported: t.id IN " + "(SELECT id FROM functional.alltypesagg) OR t.bool_col = FALSE");
    AnalysisError("select * from functional.alltypes t where not (t.id in " + "(select id from functional.alltypesagg) and t.int_col = 10)", "Subqueries in OR predicates are not supported: t.id NOT IN " + "(SELECT id FROM functional.alltypesagg) OR t.int_col != 10");
    AnalysisError("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg g where g.bool_col = false) " + "or t.bool_col = true", "Subqueries in OR predicates are not " + "supported: EXISTS (SELECT * FROM functional.alltypesagg g WHERE " + "g.bool_col = FALSE) OR t.bool_col = TRUE");
    AnalysisError("select * from functional.alltypes t where t.id = " + "(select min(id) from functional.alltypesagg g) or t.id = 10", "Subqueries in OR predicates are not supported: t.id = " + "(SELECT min(id) FROM functional.alltypesagg g) OR t.id = 10");
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1) " + "or id < 10", "Subqueries in OR predicates are not supported: " + "id IN (SELECT f1 FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1) " + "OR id < 10");
    // TODO for 2.3: Modify the StmtRewriter to allow this case with relative refs.
    // Correlated subquery with relative table refs and OR predicate is not allowed
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1 " + "or id < 10)", "Disjunctions with correlated predicates are not supported: " + "t.int_struct_col.f1 < a.f1 OR id < 10");
    // Correlated subquery with absolute table refs and OR predicate is not allowed
    AnalysisError("select * from functional.alltypes t where id in " + "(select id from functional.alltypesagg a where " + "a.int_col = t.int_col or a.bool_col = false)", "Disjunctions " + "with correlated predicates are not supported: a.int_col = " + "t.int_col OR a.bool_col = FALSE");
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny) and (bool_col = false or " + "int_col = 10)");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT are not allowed
    // with relative table refs in the subquery
    // TODO for 2.3: Modify the StmtRewriter to allow this case with relative refs
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select count(f1) from t.struct_array_col a where t.int_struct_col.f1 < a.f1)", "Unsupported correlated subquery with grouping and/or aggregation: " + "SELECT count(f1) FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT are not allowed
    // with absolute table refs in the subquery
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select max(a.id) from functional.alltypesagg a where " + "t.int_col = a.int_col)", "Unsupported correlated subquery with grouping " + "and/or aggregation: SELECT max(a.id) FROM functional.alltypesagg a " + "WHERE t.int_col = a.int_col");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select a.id from functional.alltypesagg a where " + "t.int_col = a.int_col group by a.id)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT a.id FROM " + "functional.alltypesagg a WHERE t.int_col = a.int_col GROUP BY a.id");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select distinct a.id from functional.alltypesagg a where " + "a.bigint_col = t.bigint_col)", "Unsupported correlated subquery with " + "grouping and/or aggregation: SELECT DISTINCT a.id FROM " + "functional.alltypesagg a WHERE a.bigint_col = t.bigint_col");
    // NOT compound predicates with OR
    AnalyzesOk("select * from functional.alltypes t where not (" + "id in (select id from functional.alltypesagg) or int_col < 10)");
    AnalyzesOk("select * from functional.alltypes t where not (" + "t.id < 10 or not (t.int_col in (select int_col from " + "functional.alltypesagg) and t.bool_col = false))");
    // Multiple subquery predicates
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny where int_col = 10) and int_col in " + "(select int_col from functional.alltypessmall where bigint_col = 1000) and " + "string_col not in (select string_col from functional.alltypesagg where " + "tinyint_col > 10) and bool_col = false");
    AnalyzesOk("select id, year, month from functional.allcomplextypes t where id in " + "(select item from t.int_array_col where item < 10) and id not in " + "(select f1 from t.struct_array_col where f2 = 'test')");
    // Correlated subquery with a LIMIT clause
    AnalysisError("select * from functional.alltypes t where id in " + "(select s.id from functional.alltypesagg s where s.int_col = t.int_col " + "limit 1)", "Unsupported correlated subquery with a LIMIT clause: " + "SELECT s.id FROM functional.alltypesagg s WHERE s.int_col = t.int_col " + "LIMIT 1");
    // Correlated IN with an analytic function
    AnalysisError("select id, int_col, bool_col from functional.alltypestiny t1 " + "where int_col in (select min(bigint_col) over (partition by bool_col) " + "from functional.alltypessmall t2 where t1.id < t2.id)", "Unsupported " + "correlated subquery with grouping and/or aggregation: SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE t1.id < t2.id");
    // IN subquery in binary predicate
    AnalysisError("select * from functional.alltypestiny where " + "(tinyint_col in (1,2)) = (bool_col in (select bool_col from " + "functional.alltypes))", "IN subquery predicates are not supported " + "in binary predicates: (tinyint_col IN (1, 2)) = (bool_col IN (SELECT " + "bool_col FROM functional.alltypes))");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "int_col in (select 1 as int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "int_col not in (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
    // NOT IN uncorrelated aggregate subquery with a constant
    AnalysisError("select * from functional.alltypestiny t1 where " + "10 not in (select max(int_col) from functional.alltypestiny)", "Unsupported NOT IN predicate with subquery: 10 NOT IN (SELECT " + "max(int_col) FROM functional.alltypestiny)");
    AnalysisError("select * from functional.alltypestiny t1 where " + "(10 - 2) not in (select count(*) from functional.alltypestiny)", "Unsupported NOT IN predicate with subquery: (10 - 2) NOT IN " + "(SELECT count(*) FROM functional.alltypestiny)");
}
#method_after
@Test
public void TestInSubqueries() throws AnalysisException {
    String[] colNames = { "bool_col", "tinyint_col", "smallint_col", "int_col", "bigint_col", "float_col", "double_col", "string_col", "date_string_col", "timestamp_col" };
    String[] joinOperators = { "inner join", "left outer join", "right outer join", "left semi join", "left anti join" };
    // [NOT] IN subquery predicates
    String[] operators = { "in", "not in" };
    for (String op : operators) {
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select id from functional.alltypestiny)", op));
        // Using column and table aliases similar to the ones produced by the
        // column/table alias generators during a rewrite.
        AnalyzesOk(String.format("select id `$c$1` from functional.alltypestiny `$a$1` " + "where id %s (select id from functional.alltypessmall)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t where " + "t.id %s (select id from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select t.id, max(t.int_col) from " + "functional.alltypes t where t.int_col %s (select int_col from " + "functional.alltypesagg) group by t.id having count(*) < 10", op));
        AnalyzesOk(String.format("select t.bigint_col, t.string_col from " + "functional.alltypes t where t.id %s (select id from " + "functional.alltypesagg where int_col < 10) order by bigint_col", op));
        AnalyzesOk(String.format("select * from functional.alltypes a where a.id %s " + "(select id from functional.alltypes b where a.id = b.id)", op));
        // Complex expressions
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select id + int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select 1 from functional.alltypes t where " + "t.int_col + 1 %s (select int_col - 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "abs(t.double_col) %s (select int_col from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select NULL from functional.alltypes t where " + "cast(t.double_col as int) %s (select int_col from " + "functional.alltypestiny)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes where id %s " + "(select 1 from functional.alltypestiny)", op));
        AnalyzesOk(String.format("select * from functional.alltypes where id %s " + "(select 1 + 1 from functional.alltypestiny group by int_col)", op));
        AnalyzesOk(String.format("select max(id) from functional.alltypes where id %s " + "(select max(id) from functional.alltypesagg a where a.int_col < 10) " + "and bool_col = false", op));
        // Subquery returns multiple columns
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select id, int_col from functional.alltypessmall)", op), "Subquery must return a single column: (SELECT id, int_col " + "FROM functional.alltypessmall)");
        // Subquery returns an incompatible column type
        AnalysisError(String.format("select * from functional.alltypestiny t where id %s " + "(select timestamp_col from functional.alltypessmall)", op), "Incompatible return types 'INT' and 'TIMESTAMP' of exprs 'id' and " + "'timestamp_col'.");
        // Different column types in the subquery predicate
        for (String col : colNames) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.%s %s " + "(select a.%s from functional.alltypestiny a)", col, op, col));
        }
        // Decimal in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.double_col %s (select d3 from functional.decimal_tbl a)", op));
        // Varchar in the subquery predicate
        AnalyzesOk(String.format("select * from functional.alltypes t where " + "t.string_col %s (select cast(a.string_col as varchar(1)) from " + "functional.alltypestiny a)", op));
        // Subqueries with multiple predicates in the WHERE clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.int_col > 10 and " + "a.tinyint_col < 5)", op));
        // Subqueries with a GROUP BY clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.double_col < 10.1 " + "group by a.id)", op));
        // Subqueries with GROUP BY and HAVING clauses
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where a.bool_col = true and " + "int_col < 10 group by id having count(*) < 10)", op));
        // Subqueries with a LIMIT clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a where id < 100 limit 10)", op));
        // Subqueries with multiple tables in the FROM clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, functional.alltypessmall s " + "where a.int_col = s.int_col and s.bigint_col < 100 and a.tinyint_col < 10)", op));
        // Different join operators between the tables in subquery's FROM clause
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a %s functional.alltypessmall " + "s on a.int_col = s.int_col where a.bool_col = false)", op, joinOp));
        }
        // Subquery with relative table references
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where id %s " + "(select f1 from t.struct_array_col a, t.int_array_col b " + "where f2 = 'xyz' and b.item < 3 group by f1 having count(*) > 2 limit 5)", op));
        // Correlated predicates in the subquery's ON clause
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on s.bigint_col = a.bigint_col and " + "s.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on a.bool_col = s.bool_col and t.int_col = 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypesagg a left outer join " + "functional.alltypessmall s on ifnull(s.int_col, s.int_col + 20) = " + "t.int_col + t.bigint_col)", op));
        // Subqueries with inline views
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from functional.alltypestiny a, " + "(select * from functional.alltypessmall) s where s.int_col = a.int_col " + "and s.bool_col = false)", op));
        // Subqueries with inline views that contain subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select id from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where t.id %s " + "(select a.id from (select g.* from functional.alltypesagg g where " + "g.int_col in (select int_col from functional.alltypestiny)) a where " + "a.bigint_col = 100)", op));
        // Multiple tables in the FROM clause of the outer query block
        for (String joinOp : joinOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t %s " + "functional.alltypessmall s on t.int_col = s.int_col where " + "t.tinyint_col %s (select tinyint_col from functional.alltypesagg) " + "and t.bool_col = false and t.bigint_col = 10", joinOp, op));
        }
        // Subqueries in WITH clause
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a where " + "id %s (select id from functional.alltypestiny)) select * from t where " + "t.bool_col = false and t.int_col = 10", op));
        // Subqueries in WITH and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny s)) select * from t " + "where t.int_col in (select int_col from functional.alltypessmall) and " + "t.bool_col = false", op));
        // Subqueries in WITH, FROM and WHERE clauses
        AnalyzesOk(String.format("with t as (select a.* from functional.alltypes a " + "where id %s (select id from functional.alltypestiny)) select t.* from t, " + "(select * from functional.alltypesagg g where g.id in " + "(select id from functional.alltypes)) s where s.string_col = t.string_col " + "and t.int_col in (select int_col from functional.alltypessmall) and " + "s.bool_col = false", op));
        // Correlated subqueries
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col) " + "and t.bool_col = false", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + 1 = a.int_col + 1)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col + a.int_col  = " + "a.bigint_col and a.bool_col = true)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col = false and " + "a.int_col < 10)", op));
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.bool_col)", op));
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where id %s " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 = a.f1)", op));
        // Test correlated BETWEEN predicates.
        AnalyzesOk(String.format("select 1 from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where " + " a.tinyint_col between t.tinyint_col and t.smallint_col and " + " a.smallint_col between 10 and t.int_col and " + " 20 between t.bigint_col and a.int_col and " + " t.float_col between a.float_col and a.double_col and " + " t.string_col between a.string_col and t.date_string_col and " + " a.double_col between round(acos(t.float_col), 2) " + " and cast(t.string_col as int))", op));
        // Multiple nesting levels (uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg where int_col %s " + "(select int_col from functional.alltypestiny) and bool_col = false) " + "and bigint_col < 1000", op, op));
        // Multiple nesting levels (correlated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where a.int_col = t.int_col " + "and a.tinyint_col %s (select tinyint_col from functional.alltypestiny s " + "where s.bigint_col = a.bigint_col))", op, op));
        // Multiple nesting levels (correlated and uncorrelated queries)
        AnalyzesOk(String.format("select * from functional.alltypes t where id %s " + "(select id from functional.alltypesagg a where t.int_col = a.int_col " + "and a.int_col %s (select int_col from functional.alltypestiny s))", op, op));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t1 where id %s " + "(select f11 from t1.complex_nested_struct_col.f2 t2 " + "where t1.year = f11 and f11 %s " + "(select value.f21 from t2.f12 where key = 'test'))", op, op));
        // NOT ([NOT] IN predicate)
        AnalyzesOk(String.format("select * from functional.alltypes t where not (id %s " + "(select id from functional.alltypesagg))", op));
        // Different cmp operators in the correlation predicate
        for (String cmpOp : cmpOperators) {
            AnalyzesOk(String.format("select * from functional.alltypes t " + "where t.id %s (select a.id from functional.alltypesagg a where " + "t.int_col %s a.int_col)", op, cmpOp));
        }
        // Uncorrelated IN subquery with analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where int_col %s (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 where " + "int_col < 10)", op));
    }
    // Constant on the left hand side
    AnalyzesOk("select * from functional.alltypes a where 1 in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)");
    AnalysisError("select * from functional.alltypes a where 1 not in " + "(select id from functional.alltypesagg s where s.int_col = a.int_col)", "Unsupported NOT IN predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypesagg s WHERE s.int_col = a.int_col)");
    // IN subquery that is equivalent to an uncorrelated EXISTS subquery
    AnalysisError("select * from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg)", "Unsupported " + "predicate with subquery: 1 IN (SELECT int_col FROM functional.alltypesagg)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        // Allowed because the subquery only has relative table refs.
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 %s a.f1)", cmpOp));
        // Not allowed because the subquery has absolute table refs.
        AnalysisError(String.format("select 1 from functional.alltypes t where 1 in " + "(select int_col from functional.alltypesagg g where g.id %s t.id)", cmpOp), String.format("Unsupported predicate with subquery: 1 " + "IN (SELECT int_col FROM functional.alltypesagg g WHERE g.id %s t.id)", cmpOp));
    }
    // NOT IN subquery with a correlated non-equi predicate is ok if the subquery only
    // has relative table refs
    AnalyzesOk("select 1 from functional.allcomplextypes t where id not in " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1)");
    // NOT IN subquery with a correlated non-equi predicate is not ok if the subquery
    // has absolute table refs
    AnalysisError("select 1 from functional.alltypes t where 1 not in " + "(select id from functional.alltypestiny g where g.id < t.id)", "Unsupported predicate with subquery: 1 NOT IN (SELECT id FROM " + "functional.alltypestiny g WHERE g.id < t.id)");
    // Statement with a GROUP BY and a correlated IN subquery that has a non-equi
    // correlated predicate and only relative table refs
    AnalyzesOk("select id, count(*) from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1) " + "group by id");
    // Statement with a GROUP BY and a correlated IN subquery that has a non-equi
    // correlated predicate and absolute table refs
    AnalysisError("select id, count(*) from functional.alltypes t " + "where 1 IN (select id from functional.alltypesagg g where t.int_col < " + "g.int_col) group by id", "Unsupported predicate with subquery: 1 IN " + "(SELECT id FROM functional.alltypesagg g WHERE t.int_col < g.int_col)");
    // Reference a non-existing table in the subquery
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s left outer join p on " + "(s.int_col = p.int_col))", "Could not resolve table reference: 'p'");
    // Reference a non-existing column from a table in the outer scope
    AnalysisError("select * from functional.alltypestiny t where id in " + "(select id from functional.alltypessmall s where s.int_col = t.bad_col)", "Could not resolve column/field reference: 't.bad_col'");
    // Referencing the same table in the inner and the outer query block
    // No explicit alias
    AnalyzesOk("select id from functional.alltypestiny where int_col in " + "(select int_col from functional.alltypestiny)");
    // Different alias between inner and outer block referencing the same table
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny p)");
    // Alias only in the outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny)");
    // Same alias in both inner and outer block
    AnalyzesOk("select id from functional.alltypestiny t where int_col in " + "(select int_col from functional.alltypestiny t)");
    // Binary predicate with non-comparable operands
    AnalysisError("select * from functional.alltypes t where " + "(id in (select id from functional.alltypestiny)) = 'string_val'", "operands of type BOOLEAN and STRING are not comparable: " + "(id IN (SELECT id FROM functional.alltypestiny)) = 'string_val'");
    // OR with subquery predicates
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select id from functional.alltypesagg) or t.bool_col = false", "Subqueries in OR predicates are not supported: t.id IN " + "(SELECT id FROM functional.alltypesagg) OR t.bool_col = FALSE");
    AnalysisError("select * from functional.alltypes t where not (t.id in " + "(select id from functional.alltypesagg) and t.int_col = 10)", "Subqueries in OR predicates are not supported: t.id NOT IN " + "(SELECT id FROM functional.alltypesagg) OR t.int_col != 10");
    AnalysisError("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg g where g.bool_col = false) " + "or t.bool_col = true", "Subqueries in OR predicates are not " + "supported: EXISTS (SELECT * FROM functional.alltypesagg g WHERE " + "g.bool_col = FALSE) OR t.bool_col = TRUE");
    AnalysisError("select * from functional.alltypes t where t.id = " + "(select min(id) from functional.alltypesagg g) or t.id = 10", "Subqueries in OR predicates are not supported: t.id = " + "(SELECT min(id) FROM functional.alltypesagg g) OR t.id = 10");
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1) " + "or id < 10", "Subqueries in OR predicates are not supported: " + "id IN (SELECT f1 FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1) " + "OR id < 10");
    // TODO for 2.3: Modify the StmtRewriter to allow this case with relative refs.
    // Correlated subquery with relative table refs and OR predicate is not allowed
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 < a.f1 " + "or id < 10)", "Disjunctions with correlated predicates are not supported: " + "t.int_struct_col.f1 < a.f1 OR id < 10");
    // Correlated subquery with absolute table refs and OR predicate is not allowed
    AnalysisError("select * from functional.alltypes t where id in " + "(select id from functional.alltypesagg a where " + "a.int_col = t.int_col or a.bool_col = false)", "Disjunctions " + "with correlated predicates are not supported: a.int_col = " + "t.int_col OR a.bool_col = FALSE");
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny) and (bool_col = false or " + "int_col = 10)");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT are not allowed
    // with relative table refs in the subquery
    // TODO for 2.3: Modify the StmtRewriter to allow this case with relative refs
    AnalysisError("select id from functional.allcomplextypes t where id in" + "(select count(f1) from t.struct_array_col a where t.int_struct_col.f1 < a.f1)", "Unsupported correlated subquery with grouping and/or aggregation: " + "SELECT count(f1) FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1");
    // Correlated subqueries with GROUP BY, AGG functions or DISTINCT are not allowed
    // with absolute table refs in the subquery
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select max(a.id) from functional.alltypesagg a where " + "t.int_col = a.int_col)", "Unsupported correlated subquery with grouping " + "and/or aggregation: SELECT max(a.id) FROM functional.alltypesagg a " + "WHERE t.int_col = a.int_col");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select a.id from functional.alltypesagg a where " + "t.int_col = a.int_col group by a.id)", "Unsupported correlated " + "subquery with grouping and/or aggregation: SELECT a.id FROM " + "functional.alltypesagg a WHERE t.int_col = a.int_col GROUP BY a.id");
    AnalysisError("select * from functional.alltypes t where t.id in " + "(select distinct a.id from functional.alltypesagg a where " + "a.bigint_col = t.bigint_col)", "Unsupported correlated subquery with " + "grouping and/or aggregation: SELECT DISTINCT a.id FROM " + "functional.alltypesagg a WHERE a.bigint_col = t.bigint_col");
    // NOT compound predicates with OR
    AnalyzesOk("select * from functional.alltypes t where not (" + "id in (select id from functional.alltypesagg) or int_col < 10)");
    AnalyzesOk("select * from functional.alltypes t where not (" + "t.id < 10 or not (t.int_col in (select int_col from " + "functional.alltypesagg) and t.bool_col = false))");
    // Multiple subquery predicates
    AnalyzesOk("select * from functional.alltypes t where id in " + "(select id from functional.alltypestiny where int_col = 10) and int_col in " + "(select int_col from functional.alltypessmall where bigint_col = 1000) and " + "string_col not in (select string_col from functional.alltypesagg where " + "tinyint_col > 10) and bool_col = false");
    AnalyzesOk("select id, year, month from functional.allcomplextypes t where id in " + "(select item from t.int_array_col where item < 10) and id not in " + "(select f1 from t.struct_array_col where f2 = 'test')");
    // Correlated subquery with a LIMIT clause
    AnalysisError("select * from functional.alltypes t where id in " + "(select s.id from functional.alltypesagg s where s.int_col = t.int_col " + "limit 1)", "Unsupported correlated subquery with a LIMIT clause: " + "SELECT s.id FROM functional.alltypesagg s WHERE s.int_col = t.int_col " + "LIMIT 1");
    // Correlated IN with an analytic function
    AnalysisError("select id, int_col, bool_col from functional.alltypestiny t1 " + "where int_col in (select min(bigint_col) over (partition by bool_col) " + "from functional.alltypessmall t2 where t1.id < t2.id)", "Unsupported " + "correlated subquery with grouping and/or aggregation: SELECT " + "min(bigint_col) OVER (PARTITION BY bool_col) FROM " + "functional.alltypessmall t2 WHERE t1.id < t2.id");
    // IN subquery in binary predicate
    AnalysisError("select * from functional.alltypestiny where " + "(tinyint_col in (1,2)) = (bool_col in (select bool_col from " + "functional.alltypes))", "IN subquery predicates are not supported " + "in binary predicates: (tinyint_col IN (1, 2)) = (bool_col IN (SELECT " + "bool_col FROM functional.alltypes))");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "int_col in (select 1 as int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "int_col not in (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
    // NOT IN uncorrelated aggregate subquery with a constant
    AnalysisError("select * from functional.alltypestiny t1 where " + "10 not in (select max(int_col) from functional.alltypestiny)", "Unsupported NOT IN predicate with subquery: 10 NOT IN (SELECT " + "max(int_col) FROM functional.alltypestiny)");
    AnalysisError("select * from functional.alltypestiny t1 where " + "(10 - 2) not in (select count(*) from functional.alltypestiny)", "Unsupported NOT IN predicate with subquery: (10 - 2) NOT IN " + "(SELECT count(*) FROM functional.alltypestiny)");
}
#end_block

#method_before
@Test
public void TestExistsSubqueries() throws AnalysisException {
    String[] existsOperators = { "exists", "not exists" };
    for (String op : existsOperators) {
        // [NOT] EXISTS predicate (correlated)
        AnalyzesOk(String.format("select * from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.id = t.id)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.int_col = t.int_col and p.bool_col = false)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col and a.bool_col = " + "t.bool_col)", op));
        // TODO: Enable this test when we support rewriting EXISTS subqueries as
        // non-equi joins.
        // AnalyzesOk(String.format("select 1 from functional.alltypes t where %s " +
        // "(select id from functional.alltypessmall a " +
        // " where a.int_col between t.tinyint_col and t.bigint_col)", op));
        // Multiple [NOT] EXISTS predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypessmall s where s.id = t.id) and " + "%s (select NULL from functional.alltypesagg g where t.int_col = g.int_col)", op, op));
        // OR between two subqueries
        AnalysisError(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id) or %s " + "(select * from functional.alltypessmall s where s.int_col = t.int_col)", op, op), String.format("Subqueries in OR predicates are not supported: %s " + "(SELECT * FROM functional.alltypesagg a WHERE a.id = t.id) OR %s (SELECT " + "* FROM functional.alltypessmall s WHERE s.int_col = t.int_col)", op.toUpperCase(), op.toUpperCase()));
        // Complex correlation predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id + 1) and " + "%s (select 1 from functional.alltypes s where s.int_col + s.bigint_col = " + "t.bigint_col + 1)", op, op));
        // Correlated predicates
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg g where t.int_col = g.int_col " + "and t.bool_col = false)", op));
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select id from functional.alltypessmall s where t.tinyint_col = " + "s.tinyint_col and t.bool_col)", op));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.int_col = s.int_col))", op, op));
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.bool_col = " + "s.bool_col))", op, op));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t1 where %s " + "(select f11 from t1.complex_nested_struct_col.f2 t2 " + "where t1.id = f11 and %s " + "(select value.f21 from t2.f12 where key = 'test'))", op, op));
        // Correlated EXISTS subquery with aggregation only in the HAVING clause
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col = t2.int_col having count(*) > 1)", op));
        // Correlated EXISTS subquery with a group by and aggregation
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t " + "where %s (select id, count(*) from functional.alltypesagg g where " + "t.id = g.id group by id having count(*) > 2)", op));
        // Correlated EXISTS subquery with a HAVING clause but no grouping or
        // aggregate exprs
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col = t2.int_col having t2.int_col > 1)", op), "Unsupported correlated EXISTS subquery with a HAVING clause: " + "SELECT 1 FROM functional.alltypestiny t2 WHERE t1.int_col = " + "t2.int_col HAVING t2.int_col > 1");
        // Correlated EXISTS subquery with a HAVING clause and non-equality
        // correlated predicates
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col < t2.int_col and t1.id = t2.id group by t2.id " + "having count(1) = 1)", op), "Unsupported correlated " + "EXISTS subquery with a HAVING clause: SELECT 1 FROM " + "functional.alltypestiny t2 WHERE t1.int_col < t2.int_col AND " + "t1.id = t2.id GROUP BY t2.id HAVING count(1) = 1");
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where t1.id = t2.id " + "and (t1.string_col like t2.string_col) = true group by t2.id " + "having count(1) = 1)", op), "Unsupported correlated EXISTS subquery " + "with a HAVING clause: SELECT 1 FROM functional.alltypestiny t2 WHERE " + "t1.id = t2.id AND (t1.string_col LIKE t2.string_col) = TRUE GROUP BY " + "t2.id HAVING count(1) = 1");
        AnalysisError(String.format("select id from functional.allcomplextypes t where %s " + "(select avg(f1) from t.struct_array_col a where t.int_struct_col.f1 < a.f1 " + "and a.f2 != 'xyz' group by a.f2 having count(*) > 2)", op), "Unsupported correlated EXISTS subquery with a HAVING clause: " + "SELECT avg(f1) FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1 " + "AND a.f2 != 'xyz' GROUP BY a.f2 HAVING count(*) > 2");
        // Correlated EXISTS subquery with an analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where %s (select min(bigint_col) over " + "(partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id)", op));
        // Correlated EXISTS subquery with an analytic function and a group by
        // clause
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where exists (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 " + "where t1.id = t2.id group by bigint_col, bool_col)", op));
        // Correlated [NOT] EXISTS subquery with relative table refs.
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where %s " + "(select item from t.int_array_col a where t.id = a.item)", op));
        String[] nullOps = { "is null", "is not null" };
        for (String nullOp : nullOps) {
            // Uncorrelated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes where %s " + "(select * from functional.alltypestiny) %s and id < 5", op, nullOp));
            // Correlated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes t where " + "%s (select 1 from functional.alltypestiny s where t.id = s.id) " + "%s and t.bool_col = false", op, nullOp));
        }
    }
    // Uncorrelated EXISTS subquery with an analytic function
    AnalyzesOk("select * from functional.alltypestiny t " + "where EXISTS (select id, min(int_col) over (partition by bool_col) " + "from functional.alltypesagg a where bigint_col < 10)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        // Allowed because the subquery only has relative table refs.
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t where exists " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 %s a.f1)", cmpOp));
        // Not allowed because the subquery has absolute table refs.
        AnalysisError(String.format("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where t.id %s a.id)", cmpOp), String.format("Unsupported predicate with subquery: EXISTS (SELECT * FROM " + "functional.alltypesagg a WHERE t.id %s a.id)", cmpOp));
    }
    // Uncorrelated EXISTS in a query with GROUP BY
    AnalyzesOk("select id, count(*) from functional.alltypes t " + "where exists (select 1 from functional.alltypestiny where id < 5) group by id");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join is legal with only relative refs in the subquery
    AnalyzesOk("select id from functional.allcomplextypes t where " + "exists (select 1 from t.int_array_col a where t.id = 10)");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join is illegal with absolute table refs in the subquery
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select int_col + 1 from functional.alltypessmall s where " + "t.int_col = 10)", "Unsupported predicate with subquery: EXISTS " + "(SELECT int_col + 1 FROM functional.alltypessmall s WHERE t.int_col = 10)");
    // Uncorrelated EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where exists " + "(select * from functional.alltypesagg where id < 10)");
    AnalyzesOk("select id from functional.alltypestiny where exists " + "(select id from functional.alltypessmall where bool_col = false)");
    AnalyzesOk("select 1 from functional.alltypestiny t where exists " + "(select 1 from functional.alltypessmall where id < 5)");
    AnalyzesOk("select 1 + 1 from functional.alltypestiny where exists " + "(select null from functional.alltypessmall where id != 5)");
    AnalyzesOk(String.format("select id from functional.allcomplextypes t where exists " + "(select item from t.int_array_col a where item < 10)"));
    // Multiple nesting levels with uncorrelated EXISTS
    AnalyzesOk("select id from functional.alltypes where exists " + "(select id from functional.alltypestiny where int_col < 10 and exists (" + "select id from functional.alltypessmall where bool_col = true))");
    // Uncorrelated NOT EXISTS with relative table ref
    AnalyzesOk(String.format("select id from functional.allcomplextypes t where not exists " + "(select item from t.int_array_col a where item < 10)"));
    // Uncorrelated NOT EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where not exists " + "(select 1 from functional.alltypessmall where bool_col = false)");
    // Subquery references an explicit alias from the outer block in the FROM
    // clause
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select * from t)", "Illegal table reference to non-collection type: 't'");
    // Uncorrelated subquery with no FROM clause
    AnalyzesOk("select * from functional.alltypes where exists (select 1,2)");
    // EXISTS subquery in a binary predicate
    AnalysisError("select * from functional.alltypes where " + "if(exists(select * from functional.alltypesagg), 1, 0) = 1", "EXISTS subquery predicates are not supported in binary predicates: " + "if(EXISTS (SELECT * FROM functional.alltypesagg), 1, 0) = 1");
    // Correlated subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select 1 from functional.alltypesagg g where t.id = g.id limit 1)");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "exists (select int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "not exists (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#method_after
@Test
public void TestExistsSubqueries() throws AnalysisException {
    String[] existsOperators = { "exists", "not exists" };
    for (String op : existsOperators) {
        // [NOT] EXISTS predicate (correlated)
        AnalyzesOk(String.format("select * from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.id = t.id)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t " + "where %s (select * from functional.alltypestiny p where " + "p.int_col = t.int_col and p.bool_col = false)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col)", op));
        AnalyzesOk(String.format("select count(*) from functional.alltypes t, " + "functional.alltypessmall s where s.id = t.id and %s (select * from " + "functional.alltypestiny a where a.int_col = t.int_col and a.bool_col = " + "t.bool_col)", op));
        // Multiple [NOT] EXISTS predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypessmall s where s.id = t.id) and " + "%s (select NULL from functional.alltypesagg g where t.int_col = g.int_col)", op, op));
        // OR between two subqueries
        AnalysisError(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id) or %s " + "(select * from functional.alltypessmall s where s.int_col = t.int_col)", op, op), String.format("Subqueries in OR predicates are not supported: %s " + "(SELECT * FROM functional.alltypesagg a WHERE a.id = t.id) OR %s (SELECT " + "* FROM functional.alltypessmall s WHERE s.int_col = t.int_col)", op.toUpperCase(), op.toUpperCase()));
        // Complex correlation predicates
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg a where a.id = t.id + 1) and " + "%s (select 1 from functional.alltypes s where s.int_col + s.bigint_col = " + "t.bigint_col + 1)", op, op));
        // Correlated predicates
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select * from functional.alltypesagg g where t.int_col = g.int_col " + "and t.bool_col = false)", op));
        AnalyzesOk(String.format("select * from functional.alltypestiny t where " + "%s (select id from functional.alltypessmall s where t.tinyint_col = " + "s.tinyint_col and t.bool_col)", op));
        // Multiple nesting levels
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.int_col = s.int_col))", op, op));
        AnalyzesOk(String.format("select * from functional.alltypes t where %s " + "(select * from functional.alltypessmall s where t.id = s.id and %s " + "(select * from functional.alltypestiny g where g.bool_col = " + "s.bool_col))", op, op));
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t1 where %s " + "(select f11 from t1.complex_nested_struct_col.f2 t2 " + "where t1.id = f11 and %s " + "(select value.f21 from t2.f12 where key = 'test'))", op, op));
        // Correlated EXISTS subquery with aggregation only in the HAVING clause
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col = t2.int_col having count(*) > 1)", op));
        // Correlated EXISTS subquery with a group by and aggregation
        AnalyzesOk(String.format("select 1 from functional.alltypestiny t " + "where %s (select id, count(*) from functional.alltypesagg g where " + "t.id = g.id group by id having count(*) > 2)", op));
        // Correlated EXISTS subquery with a HAVING clause but no grouping or
        // aggregate exprs
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col = t2.int_col having t2.int_col > 1)", op), "Unsupported correlated EXISTS subquery with a HAVING clause: " + "SELECT 1 FROM functional.alltypestiny t2 WHERE t1.int_col = " + "t2.int_col HAVING t2.int_col > 1");
        // Correlated EXISTS subquery with a HAVING clause and non-equality
        // correlated predicates
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where " + "t1.int_col < t2.int_col and t1.id = t2.id group by t2.id " + "having count(1) = 1)", op), "Unsupported correlated " + "EXISTS subquery with a HAVING clause: SELECT 1 FROM " + "functional.alltypestiny t2 WHERE t1.int_col < t2.int_col AND " + "t1.id = t2.id GROUP BY t2.id HAVING count(1) = 1");
        AnalysisError(String.format("select 1 from functional.alltypestiny t1 " + "where %s (select 1 from functional.alltypestiny t2 where t1.id = t2.id " + "and (t1.string_col like t2.string_col) = true group by t2.id " + "having count(1) = 1)", op), "Unsupported correlated EXISTS subquery " + "with a HAVING clause: SELECT 1 FROM functional.alltypestiny t2 WHERE " + "t1.id = t2.id AND (t1.string_col LIKE t2.string_col) = TRUE GROUP BY " + "t2.id HAVING count(1) = 1");
        AnalysisError(String.format("select id from functional.allcomplextypes t where %s " + "(select avg(f1) from t.struct_array_col a where t.int_struct_col.f1 < a.f1 " + "and a.f2 != 'xyz' group by a.f2 having count(*) > 2)", op), "Unsupported correlated EXISTS subquery with a HAVING clause: " + "SELECT avg(f1) FROM t.struct_array_col a WHERE t.int_struct_col.f1 < a.f1 " + "AND a.f2 != 'xyz' GROUP BY a.f2 HAVING count(*) > 2");
        // Correlated EXISTS subquery with an analytic function
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where %s (select min(bigint_col) over " + "(partition by bool_col) from functional.alltypessmall t2 where " + "t1.id = t2.id)", op));
        // Correlated EXISTS subquery with an analytic function and a group by
        // clause
        AnalyzesOk(String.format("select id, int_col, bool_col from " + "functional.alltypestiny t1 where exists (select min(bigint_col) " + "over (partition by bool_col) from functional.alltypessmall t2 " + "where t1.id = t2.id group by bigint_col, bool_col)", op));
        // Correlated [NOT] EXISTS subquery with relative table refs.
        AnalyzesOk(String.format("select id from functional.allcomplextypes t where %s " + "(select item from t.int_array_col a where t.id = a.item)", op));
        String[] nullOps = { "is null", "is not null" };
        for (String nullOp : nullOps) {
            // Uncorrelated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes where %s " + "(select * from functional.alltypestiny) %s and id < 5", op, nullOp));
            // Correlated EXISTS subquery in an IS [NOT] NULL predicate
            AnalyzesOk(String.format("select * from functional.alltypes t where " + "%s (select 1 from functional.alltypestiny s where t.id = s.id) " + "%s and t.bool_col = false", op, nullOp));
        }
    }
    // Uncorrelated EXISTS subquery with an analytic function
    AnalyzesOk("select * from functional.alltypestiny t " + "where EXISTS (select id, min(int_col) over (partition by bool_col) " + "from functional.alltypesagg a where bigint_col < 10)");
    // Different non-equi comparison operators in the correlated predicate
    String[] nonEquiCmpOperators = { "!=", "<=", ">=", ">", "<" };
    for (String cmpOp : nonEquiCmpOperators) {
        // Allowed because the subquery only has relative table refs.
        AnalyzesOk(String.format("select 1 from functional.allcomplextypes t where exists " + "(select f1 from t.struct_array_col a where t.int_struct_col.f1 %s a.f1)", cmpOp));
        // Not allowed because the subquery has absolute table refs.
        AnalysisError(String.format("select * from functional.alltypes t where exists " + "(select * from functional.alltypesagg a where t.id %s a.id)", cmpOp), String.format("Unsupported predicate with subquery: EXISTS (SELECT * FROM " + "functional.alltypesagg a WHERE t.id %s a.id)", cmpOp));
    }
    // Correlated BETWEEN predicate with relative table refs.
    AnalyzesOk("select 1 from functional.allcomplextypes t where exists " + "(select a.f1 from t.struct_array_col a " + " where a.f1 between t.int_struct_col.f1 and t.int_struct_col.f2)");
    // Correlated BETWEEN predicate with absolute table refs.
    AnalysisError("select 1 from functional.alltypes t where EXISTS " + "(select id from functional.alltypessmall a " + " where a.int_col between t.tinyint_col and t.bigint_col)", "Unsupported predicate with subquery: " + "EXISTS (SELECT id FROM functional.alltypessmall a " + "WHERE a.int_col >= t.tinyint_col AND a.int_col <= t.bigint_col)");
    // Uncorrelated EXISTS in a query with GROUP BY
    AnalyzesOk("select id, count(*) from functional.alltypes t " + "where exists (select 1 from functional.alltypestiny where id < 5) group by id");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join is legal with only relative refs in the subquery
    AnalyzesOk("select id from functional.allcomplextypes t where " + "exists (select 1 from t.int_array_col a where t.id = 10)");
    // Subquery with a correlated predicate that cannot be transformed into an
    // equi-join is illegal with absolute table refs in the subquery
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select int_col + 1 from functional.alltypessmall s where " + "t.int_col = 10)", "Unsupported predicate with subquery: EXISTS " + "(SELECT int_col + 1 FROM functional.alltypessmall s WHERE t.int_col = 10)");
    // Uncorrelated EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where exists " + "(select * from functional.alltypesagg where id < 10)");
    AnalyzesOk("select id from functional.alltypestiny where exists " + "(select id from functional.alltypessmall where bool_col = false)");
    AnalyzesOk("select 1 from functional.alltypestiny t where exists " + "(select 1 from functional.alltypessmall where id < 5)");
    AnalyzesOk("select 1 + 1 from functional.alltypestiny where exists " + "(select null from functional.alltypessmall where id != 5)");
    AnalyzesOk(String.format("select id from functional.allcomplextypes t where exists " + "(select item from t.int_array_col a where item < 10)"));
    // Multiple nesting levels with uncorrelated EXISTS
    AnalyzesOk("select id from functional.alltypes where exists " + "(select id from functional.alltypestiny where int_col < 10 and exists (" + "select id from functional.alltypessmall where bool_col = true))");
    // Uncorrelated NOT EXISTS with relative table ref
    AnalyzesOk(String.format("select id from functional.allcomplextypes t where not exists " + "(select item from t.int_array_col a where item < 10)"));
    // Uncorrelated NOT EXISTS subquery
    AnalyzesOk("select * from functional.alltypestiny where not exists " + "(select 1 from functional.alltypessmall where bool_col = false)");
    // Subquery references an explicit alias from the outer block in the FROM
    // clause
    AnalysisError("select * from functional.alltypestiny t where " + "exists (select * from t)", "Illegal table reference to non-collection type: 't'");
    // Uncorrelated subquery with no FROM clause
    AnalyzesOk("select * from functional.alltypes where exists (select 1,2)");
    // EXISTS subquery in a binary predicate
    AnalysisError("select * from functional.alltypes where " + "if(exists(select * from functional.alltypesagg), 1, 0) = 1", "EXISTS subquery predicates are not supported in binary predicates: " + "if(EXISTS (SELECT * FROM functional.alltypesagg), 1, 0) = 1");
    // Correlated subquery with a LIMIT clause
    AnalyzesOk("select count(*) from functional.alltypes t where exists " + "(select 1 from functional.alltypesagg g where t.id = g.id limit 1)");
    // Column labels may conflict after the rewrite as an inline view
    AnalyzesOk("select int_col from functional.alltypestiny where " + "exists (select int_col from functional.alltypesagg)");
    AnalyzesOk("select int_col from functional.alltypestiny a where " + "not exists (select 1 as int_col from functional.alltypesagg b " + "where a.int_col = b.int_col)");
}
#end_block

#method_before
@Test
public void TestSelect() throws AuthorizationException, AnalysisException {
    // Simple select from a table.
    Set<TAccessEvent> accessEvents = AnalyzeAccessEvents("select * from functional.alltypesagg");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypesagg", TCatalogObjectType.TABLE, "SELECT")));
    // Select from a view. Expect to get 3 events back - one for the view and two
    // for the underlying objects that the view accesses.
    accessEvents = AnalyzeAccessEvents("select * from functional.view_view");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.view_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("functional.alltypes_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("functional.alltypes", TCatalogObjectType.TABLE, "SELECT")));
    // Tests audit events after a statement has been rewritten (IMPALA-3915).
    // Select from a view that contains a subquery.
    accessEvents = AnalyzeAccessEvents("select * from functional_rc.subquery_view");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypessmall", TCatalogObjectType.TABLE, "SELECT"), new TAccessEvent("functional.alltypes", TCatalogObjectType.TABLE, "SELECT"), new TAccessEvent("functional_rc.subquery_view", TCatalogObjectType.VIEW, "SELECT")));
    // Select from an inline view.
    accessEvents = AnalyzeAccessEvents("select a.* from (select * from functional.alltypesagg) a");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypesagg", TCatalogObjectType.TABLE, "SELECT")));
    // Select from collection table references.
    accessEvents = AnalyzeAccessEvents("select item from functional.allcomplextypes.int_array_col");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.allcomplextypes", TCatalogObjectType.TABLE, "SELECT")));
    accessEvents = AnalyzeAccessEvents("select item from functional.allcomplextypes a, a.int_array_col");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.allcomplextypes", TCatalogObjectType.TABLE, "SELECT")));
}
#method_after
@Test
public void TestSelect() throws AuthorizationException, AnalysisException {
    // Simple select from a table.
    Set<TAccessEvent> accessEvents = AnalyzeAccessEvents("select * from functional.alltypesagg");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypesagg", TCatalogObjectType.TABLE, "SELECT")));
    // Select from a view. Expect to get 3 events back - one for the view and two
    // for the underlying objects that the view accesses.
    accessEvents = AnalyzeAccessEvents("select * from functional.view_view");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.view_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("functional.alltypes_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("functional.alltypes", TCatalogObjectType.TABLE, "SELECT")));
    // Tests audit events after a statement has been rewritten (IMPALA-3915).
    // Select from a view that contains a subquery.
    accessEvents = AnalyzeAccessEvents("select * from functional_rc.subquery_view");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional_rc.alltypessmall", TCatalogObjectType.TABLE, "SELECT"), new TAccessEvent("functional_rc.alltypes", TCatalogObjectType.TABLE, "SELECT"), new TAccessEvent("functional_rc.subquery_view", TCatalogObjectType.VIEW, "SELECT"), new TAccessEvent("_impala_builtins", TCatalogObjectType.DATABASE, "VIEW_METADATA")));
    // Select from an inline view.
    accessEvents = AnalyzeAccessEvents("select a.* from (select * from functional.alltypesagg) a");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.alltypesagg", TCatalogObjectType.TABLE, "SELECT")));
    // Select from collection table references.
    accessEvents = AnalyzeAccessEvents("select item from functional.allcomplextypes.int_array_col");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.allcomplextypes", TCatalogObjectType.TABLE, "SELECT")));
    accessEvents = AnalyzeAccessEvents("select item from functional.allcomplextypes a, a.int_array_col");
    Assert.assertEquals(accessEvents, Sets.newHashSet(new TAccessEvent("functional.allcomplextypes", TCatalogObjectType.TABLE, "SELECT")));
}
#end_block

