131
#method_before
private List<Measurements> measurementsList(Series series) {
    List<Measurements> measurementsList = new LinkedList<>();
    String timestamp;
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            Measurements measurements = new Measurements(serie.getName(), influxV9Utils.filterPrivateTags(serie.getTags()));
            for (String[] values : serie.getValues()) {
                final int length = values[0].length();
                if (length == 20) {
                    timestamp = values[0].substring(0, 19) + ".000Z";
                } else {
                    final String millisecond = values[0].substring(20, length - 1);
                    final String millisecond_3d = StringUtils.rightPad(millisecond, 3, '0');
                    timestamp = values[0].substring(0, 19) + '.' + millisecond_3d + 'Z';
                }
                measurements.addMeasurement(new Object[] { timestamp, Double.parseDouble(values[1]), getValueMeta(values) });
            }
            measurementsList.add(measurements);
        }
    }
    return measurementsList;
}
#method_after
private List<Measurements> measurementsList(Series series) {
    List<Measurements> measurementsList = new LinkedList<>();
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            Measurements measurements = new Measurements(serie.getName(), influxV9Utils.filterPrivateTags(serie.getTags()));
            for (String[] values : serie.getValues()) {
                final String timestamp = influxV9Utils.threeDigitMillisTimestamp(values[0]);
                measurements.addMeasurement(new Object[] { timestamp, Double.parseDouble(values[1]), getValueMeta(values) });
            }
            measurementsList.add(measurements);
        }
    }
    return measurementsList;
}
#end_block

#method_before
private List<Measurements> measurementsList(Series series) {
    int length;
    String millisecond;
    String millisecond_3d;
    String timestamp;
    List<Measurements> measurementsList = new LinkedList<>();
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            Measurements measurements = new Measurements(serie.getName(), influxV9Utils.filterPrivateTags(serie.getTags()));
            for (String[] values : serie.getValues()) {
                length = values[0].length();
                millisecond = values[0].substring(20, length - 1);
                millisecond_3d = StringUtils.rightPad(millisecond, 3, '0');
                timestamp = values[0].substring(0, 19) + '.' + millisecond_3d + 'Z';
                measurements.addMeasurement(new Object[] { timestamp, Double.parseDouble(values[1]), getValueMeta(values) });
            }
            measurementsList.add(measurements);
        }
    }
    return measurementsList;
}
#method_after
private List<Measurements> measurementsList(Series series) {
    List<Measurements> measurementsList = new LinkedList<>();
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            Measurements measurements = new Measurements(serie.getName(), influxV9Utils.filterPrivateTags(serie.getTags()));
            for (String[] values : serie.getValues()) {
                final int length = values[0].length();
                final String millisecond = values[0].substring(20, length - 1);
                final String millisecond_3d = StringUtils.rightPad(millisecond, 3, '0');
                final String timestamp = values[0].substring(0, 19) + '.' + millisecond_3d + 'Z';
                measurements.addMeasurement(new Object[] { timestamp, Double.parseDouble(values[1]), getValueMeta(values) });
            }
            measurementsList.add(measurements);
        }
    }
    return measurementsList;
}
#end_block

#method_before
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((dimensions == null) ? 0 : dimensions.hashCode());
    result = prime * result + ((name == null) ? 0 : name.hashCode());
    result = prime * result + (int) (timestamp ^ (timestamp >>> 32));
    long temp;
    temp = (value == null) ? 0 : Double.doubleToLongBits(value);
    result = prime * result + (int) (temp ^ (temp >>> 32));
    return result;
}
#method_after
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + ((dimensions == null) ? 0 : dimensions.hashCode());
    result = prime * result + ((name == null) ? 0 : name.hashCode());
    result = prime * result + ((timestamp == null) ? 0 : (int) (timestamp ^ (timestamp >>> 32)));
    long temp;
    temp = (value == null) ? 0 : Double.doubleToLongBits(value);
    result = prime * result + (int) (temp ^ (temp >>> 32));
    return result;
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldErrorOnCreateWithoutTimestamp() throws Exception {
    String json = jsonFixture("fixtures/metricWithoutTimestamp.json");
    CreateMetricCommand metric = fromJson(json, CreateMetricCommand.class);
    ClientResponse response = createResponseFor(metric);
    assertEquals(response.getStatus(), 422, "Timestamp may not be empty");
}
#method_after
@SuppressWarnings("unchecked")
public void shouldErrorOnCreateWithoutTimestamp() throws Exception {
    String json = jsonFixture("fixtures/metricWithoutTimestamp.json");
    CreateMetricCommand metric = fromJson(json, CreateMetricCommand.class);
    ClientResponse response = createResponseFor(metric);
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "[timestamp may not be null");
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public void shouldErrorOnCreateWithZeroTimestamp() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, 0L, 0.0, valueMeta));
    assertEquals(response.getStatus(), 422, "Timestamp may not be zero");
}
#method_after
@SuppressWarnings("unchecked")
public void shouldErrorOnCreateWithZeroTimestamp() {
    ClientResponse response = createResponseFor(new CreateMetricCommand("test_metrictype", dimensions, 0L, 0.0, valueMeta));
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, String.format("Timestamp %s is out of legal range", 0L));
}
#end_block

#method_before
public void shouldErrorOnCreateWithoutName() throws Exception {
    String json = jsonFixture("fixtures/metricWithoutName.json");
    ClientResponse response = createResponseFor(json);
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "[name may not be empty");
}
#method_after
public void shouldErrorOnCreateWithoutName() throws Exception {
    String json = jsonFixture("fixtures/metricWithoutName.json");
    CreateMetricCommand metric = fromJson(json, CreateMetricCommand.class);
    metric.timestamp = timestamp;
    ClientResponse response = createResponseFor(metric);
    ErrorMessages.assertThat(response.getEntity(String.class)).matches("unprocessable_entity", 422, "[name may not be empty");
}
#end_block

#method_before
public void validateContentLength(final Integer contentLength) {
    LOGGER.debug("validateContentLength(length=%d)", contentLength);
    if (contentLength == null) {
        throw Exceptions.lengthRequired("Content length header is missing", "Content length is required to estimate if payload can be processed");
    }
    if (contentLength >= this.config.logSize) {
        throw Exceptions.payloadToLarge("Log payload size exceeded", String.format("Maximum allowed size is %d bytes", this.config.logSize));
    }
}
#method_after
public void validateContentLength(final Integer contentLength) {
    LOGGER.debug("validateContentLength(length=%d)", contentLength);
    if (contentLength == null) {
        throw Exceptions.lengthRequired("Content length header is missing", "Content length is required to estimate if payload can be processed");
    }
    if (contentLength >= this.config.logSize) {
        throw Exceptions.payloadTooLarge("Log payload size exceeded", String.format("Maximum allowed size is %d bytes", this.config.logSize));
    }
}
#end_block

#method_before
boolean evaluate(final long now, long alarmDelay) {
    final AlarmState newState;
    if (immediateAlarmEvaluate()) {
        newState = AlarmState.ALARM;
    } else {
        if (!stats.shouldEvaluate(now, alarmDelay)) {
            return false;
        }
        newState = determineAlarmState();
    }
    if (shouldSendStateChange(newState) && (stats.shouldEvaluate(now, alarmDelay) || (newState == AlarmState.ALARM && this.subAlarm.canEvaluateImmediately()))) {
        setSubAlarmState(newState);
        return true;
    }
    return false;
}
#method_after
boolean evaluate(final long now, long alarmDelay) {
    final AlarmState newState;
    if (immediateAlarmEvaluate()) {
        newState = AlarmState.ALARM;
    } else {
        if (!stats.shouldEvaluate(now, alarmDelay)) {
            return false;
        }
        newState = determineAlarmStateUsingView();
    }
    if (shouldSendStateChange(newState) && (stats.shouldEvaluate(now, alarmDelay) || (newState == AlarmState.ALARM && this.subAlarm.canEvaluateImmediately()))) {
        setSubAlarmState(newState);
        return true;
    }
    return false;
}
#end_block

#method_before
public void addToBatch(AlarmStateTransitionedEvent message, String id) {
    String metricsString = getSerializedString(message.metrics, id);
    // Validate metricsString does not exceed a suitable maximum upper bound
    if (metricsString.length() * 4 >= MAX_LENGTH_VARCHAR) {
        metricsString = "[]";
        logger.warn("length of metricsString for alarm ID {} exceeds max length of {}", message.alarmId, MAX_LENGTH_VARCHAR);
    }
    String subAlarmsString = getSerializedString(message.subAlarms, id);
    // Validate subAlarmsString does not exceed a suitable maximum upper bound
    if (subAlarmsString.length() * 4 >= MAX_LENGTH_VARCHAR) {
        subAlarmsString = "[]";
        logger.warn("length of subAlarmsString for alarm ID {} exceeds max length of {}", message.alarmId, MAX_LENGTH_VARCHAR);
    }
    String timeStamp = simpleDateFormat.format(new Date(message.timestamp));
    batch.add().bind("tenant_id", message.tenantId).bind("alarm_id", message.alarmId).bind("metrics", metricsString).bind("old_state", message.oldState.name()).bind("new_state", message.newState.name()).bind("sub_alarms", subAlarmsString).bind("reason", message.stateChangeReason).bind("reason_data", "{}").bind("time_stamp", timeStamp);
    this.msgCnt++;
}
#method_after
public void addToBatch(AlarmStateTransitionedEvent message, String id) {
    String metricsString = getSerializedString(message.metrics, id);
    // Validate metricsString does not exceed a sufficient maximum upper bound
    if (metricsString.length() * MAX_BYTES_PER_CHAR >= MAX_LENGTH_VARCHAR) {
        metricsString = "[]";
        logger.warn("length of metricsString for alarm ID {} exceeds max length of {}", message.alarmId, MAX_LENGTH_VARCHAR);
    }
    String subAlarmsString = getSerializedString(message.subAlarms, id);
    // Validate subAlarmsString does not exceed a sufficient maximum upper bound
    if (subAlarmsString.length() * MAX_BYTES_PER_CHAR >= MAX_LENGTH_VARCHAR) {
        subAlarmsString = "[]";
        logger.warn("length of subAlarmsString for alarm ID {} exceeds max length of {}", message.alarmId, MAX_LENGTH_VARCHAR);
    }
    String timeStamp = simpleDateFormat.format(new Date(message.timestamp));
    batch.add().bind("tenant_id", message.tenantId).bind("alarm_id", message.alarmId).bind("metrics", metricsString).bind("old_state", message.oldState.name()).bind("new_state", message.newState.name()).bind("sub_alarms", subAlarmsString).bind("reason", message.stateChangeReason).bind("reason_data", "{}").bind("time_stamp", timeStamp);
    this.msgCnt++;
}
#end_block

#method_before
public Log newLog(final LogRequestBean logRequestBean, final boolean validate) {
    LOGGER.debug(LOG_MARKER, "Creating new log from bean = {}, validation is {}", logRequestBean, validate ? "enabled" : "disabled");
    Preconditions.checkNotNull(logRequestBean, "LogBean must not be null");
    Preconditions.checkNotNull(logRequestBean.getPayload(), "Payload should not be null");
    final String payload = logRequestBean.getPayload();
    final Log log;
    try {
        log = this.payloadTransformers.get(logRequestBean.getContentType()).transform(payload);
    } catch (Exception exp) {
        LOGGER.error(LOG_MARKER_ERROR, "Failed to unpack payload \n\"{}\"", payload);
        throw Exceptions.unprocessableEntity("{} couldn't be processed", payload);
    }
    log.setApplicationType(LogApplicationTypeValidator.normalize(logRequestBean.getApplicationType()));
    log.setDimensions(DimensionValidation.normalize(logRequestBean.getDimensions()));
    if (validate) {
        this.validate(log);
    }
    return log;
}
#method_after
public Log newLog(final LogRequestBean logRequestBean, final boolean validate) {
    LOGGER.debug(LOG_MARKER, "Creating new log from bean = {}, validation is {}", logRequestBean, validate ? "enabled" : "disabled");
    Preconditions.checkNotNull(logRequestBean, "LogBean must not be null");
    Preconditions.checkNotNull(logRequestBean.getPayload(), "Payload should not be null");
    final String payload = logRequestBean.getPayload();
    final Log log;
    try {
        log = this.payloadTransformers.get(logRequestBean.getContentType()).transform(payload);
    } catch (Exception exp) {
        LOGGER.warn(LOG_MARKER_WARN, "Failed to unpack payload \n\"{}\"", payload);
        throw Exceptions.unprocessableEntity("{} couldn't be processed", payload);
    }
    log.setApplicationType(LogApplicationTypeValidator.normalize(logRequestBean.getApplicationType()));
    log.setDimensions(DimensionValidation.normalize(logRequestBean.getDimensions()));
    if (validate) {
        this.validate(log);
    }
    return log;
}
#end_block

#method_before
public void validate(final Log log) {
    LOGGER.trace(LOG_MARKER, "Validating log {}", log);
    try {
        if (log.getApplicationType() != null && !log.getApplicationType().isEmpty()) {
            LogApplicationTypeValidator.validate(log.getApplicationType());
        }
        if (log.getDimensions() != null) {
            DimensionValidation.validate(log.getDimensions(), null);
        }
        if (log.getMessage().length() > MAX_LOG_LENGTH) {
            throw Exceptions.unprocessableEntity("Log must be %d characters or less", MAX_LOG_LENGTH);
        }
    } catch (Exception exp) {
        LOGGER.error(LOG_MARKER_ERROR, "Log {} not valid, error is {}", log, exp);
        throw exp;
    }
    LOGGER.debug(LOG_MARKER, "Log {} considered valid", log);
}
#method_after
public void validate(final Log log) {
    LOGGER.trace(LOG_MARKER, "Validating log {}", log);
    try {
        if (log.getApplicationType() != null && !log.getApplicationType().isEmpty()) {
            LogApplicationTypeValidator.validate(log.getApplicationType());
        }
        if (log.getDimensions() != null) {
            DimensionValidation.validate(log.getDimensions(), null);
        }
        if (log.getMessage().length() > MAX_LOG_LENGTH) {
            throw Exceptions.unprocessableEntity("Log must be %d characters or less", MAX_LOG_LENGTH);
        }
    } catch (Exception exp) {
        LOGGER.warn(LOG_MARKER_WARN, "Log {} not valid, error is {}", log, exp);
        throw exp;
    }
    LOGGER.debug(LOG_MARKER, "Log {} considered valid", log);
}
#end_block

#method_before
@BeforeTest
@SuppressWarnings("unchecked")
protected void beforeMethod() {
    dimensions.clear();
    dimensions.put("a", "b");
    config = new ApiConfig();
    config.region = REGION;
    config.logTopic = TOPIC;
    this.producer = Mockito.mock(Producer.class);
    this.serializer = Mockito.spy(new LogSerializer());
    this.logService = Mockito.spy(new LogService(this.config, this.producer, this.serializer));
}
#method_after
@BeforeTest
@SuppressWarnings("unchecked")
protected void beforeMethod() {
    dimensions.clear();
    dimensions.put("a", "b");
    config = new ApiConfig();
    config.region = REGION;
    config.logTopic = TOPIC;
    this.producer = Mockito.mock(Producer.class);
    this.serializer = Mockito.spy(new LogSerializer(new ApplicationModule().objectMapper()));
    this.logService = Mockito.spy(new LogService(this.config, this.producer, this.serializer));
}
#end_block

#method_before
@Override
protected void configure() {
    this.bind(LogSerializer.class).in(Singleton.class);
    this.bind(LogService.class).in(Singleton.class);
    // bind payload transformers
    this.bind(JsonPayloadTransformer.class).in(Singleton.class);
}
#method_after
// object mapper configuration
@Override
protected void configure() {
    this.bind(LogSerializer.class).in(Singleton.class);
    this.bind(LogService.class).in(Singleton.class);
    // bind payload transformers
    this.bind(JsonPayloadTransformer.class).in(Singleton.class);
}
#end_block

#method_before
@POST
@Timed
@Consumes(MediaType.APPLICATION_JSON)
@Path("/single")
public void single(@Context Request request, @HeaderParam("X-Tenant-Id") String tenantId, @HeaderParam("X-Roles") String roles, @HeaderParam("X-Application-Type") String applicationType, @HeaderParam("X-Dimensions") String dimensionsStr, @QueryParam("tenant_id") String crossTenantId, String payload) {
    LOGGER.debug("/single/{}", tenantId);
    if (!this.isDelegate(roles)) {
        LOGGER.trace(String.format("/single/%s is not delegated request, checking for crossTenantIt", tenantId));
        if (!Strings.isNullOrEmpty(crossTenantId)) {
            throw Exceptions.forbidden("Project %s cannot POST cross tenant metrics", tenantId);
        }
    }
    final Log log = service.newLog(new LogRequestBean().setApplicationType(applicationType).setDimensions(this.getDimensions(dimensionsStr)).setContentType(this.getContentType(request)).setPayload(payload), VALIDATE_LOG);
    tenantId = this.getTenantId(tenantId, crossTenantId);
    LOGGER.debug("Shipping log={},tenantId={} pair to kafka", log, tenantId);
    this.service.sendToKafka(log, tenantId);
}
#method_after
@POST
@Timed
@Consumes(MediaType.APPLICATION_JSON)
@Path("/single")
public void single(@Context Request request, @HeaderParam("X-Tenant-Id") String tenantId, @HeaderParam("X-Roles") String roles, @HeaderParam("X-Application-Type") String applicationType, @HeaderParam("X-Dimensions") String dimensionsStr, @QueryParam("tenant_id") String crossTenantId, String payload) {
    LOGGER.debug("/single/{}", tenantId);
    if (!this.isDelegate(roles)) {
        LOGGER.trace(String.format("/single/%s is not delegated request, checking for crossTenantId", tenantId));
        if (!Strings.isNullOrEmpty(crossTenantId)) {
            throw Exceptions.forbidden("Project %s cannot POST cross tenant metrics", tenantId);
        }
    }
    final Log log = service.newLog(new LogRequestBean().setApplicationType(applicationType).setDimensions(this.getDimensions(dimensionsStr)).setContentType(this.getContentType(request)).setPayload(payload), VALIDATE_LOG);
    tenantId = this.getTenantId(tenantId, crossTenantId);
    LOGGER.debug("Shipping log={},tenantId={} pair to kafka", log, tenantId);
    this.service.sendToKafka(log, tenantId);
}
#end_block

#method_before
public Log setDimensions(final Map<String, String> dimensions) {
    if (dimensions != null) {
        this.put("dimensions", dimensions);
    }
    return this;
}
#method_after
public Log setDimensions(final Map<String, String> dimensions) {
    if (dimensions != null) {
        this.put(KEY_DIMENSIONS, dimensions);
    }
    return this;
}
#end_block

#method_before
protected <T> T fromJson(final byte[] logJson, final Class<T> target) {
    try {
        return OBJECT_MAPPER.readValue(new String(logJson, "UTF-8"), target);
    } catch (Exception e) {
        throw Exceptions.uncheck(e, "Failed to parse log json: %s", new String(logJson));
    }
}
#method_after
protected <T> T fromJson(final byte[] logJson, final Class<T> target) {
    try {
        return this.objectMapper.readValue(new String(logJson, "UTF-8"), target);
    } catch (Exception e) {
        throw Exceptions.uncheck(e, "Failed to parse log json: %s", new String(logJson));
    }
}
#end_block

#method_before
protected <T> String toJson(final T log) {
    try {
        return OBJECT_MAPPER.writeValueAsString(log);
    } catch (JsonProcessingException e) {
        return null;
    }
}
#method_after
protected <T> String toJson(final T log) {
    try {
        return this.objectMapper.writeValueAsString(log);
    } catch (JsonProcessingException e) {
        throw Exceptions.uncheck(e, "Failed to create log json: %s", log);
    }
}
#end_block

#method_before
@BeforeMethod
public void setUp() throws Exception {
    this.serializer = new LogSerializer();
}
#method_after
@BeforeMethod
public void setUp() throws Exception {
    this.serializer = new LogSerializer(new ApplicationModule().objectMapper());
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
protected void setupResources() throws Exception {
    super.setupResources();
    applicationType = "apache";
    dimensionsStr = "app_name:WebService01,environment:production";
    dimensionsMap = Maps.newHashMap();
    dimensionsMap.put("app_name", "WebService01");
    dimensionsMap.put("environment", "production");
    jsonPayload = "{\n  \"message\":\"Hello, world!\",\n  \"from\":\"hoover\"\n}";
    jsonMessage = "Hello, world!";
    tenantId = "abc";
    longString = "12345678901234567890123456789012345678901234567890" + "12345678901234567890123456789012345678901234567890" + "12345678901234567890123456789012345678901234567890" + "12345678901234567890123456789012345678901234567890" + "12345678901234567890123456789012345678901234567890123456";
    final LogSerializer serializer = new LogSerializer();
    service = Mockito.spy(new LogService(null, null, serializer));
    service.setJsonPayloadTransformer(new JsonPayloadTransformer(serializer));
    doNothing().when(service).sendToKafka(any(Log.class), anyString());
    doCallRealMethod().when(this.service).newLog(any(LogRequestBean.class), anyBoolean());
    doCallRealMethod().when(this.service).validate(any(Log.class));
    addResources(new LogResource(service));
}
#method_after
@Override
@SuppressWarnings("unchecked")
protected void setupResources() throws Exception {
    super.setupResources();
    applicationType = "apache";
    dimensionsStr = "app_name:WebService01,environment:production";
    dimensionsMap = Maps.newHashMap();
    dimensionsMap.put("app_name", "WebService01");
    dimensionsMap.put("environment", "production");
    jsonPayload = "{\n  \"message\":\"Hello, world!\",\n  \"from\":\"hoover\"\n}";
    jsonMessage = "Hello, world!";
    tenantId = "abc";
    longString = "12345678901234567890123456789012345678901234567890" + "12345678901234567890123456789012345678901234567890" + "12345678901234567890123456789012345678901234567890" + "12345678901234567890123456789012345678901234567890" + "12345678901234567890123456789012345678901234567890123456";
    final LogSerializer serializer = new LogSerializer(new ApplicationModule().objectMapper());
    service = Mockito.spy(new LogService(null, null, serializer));
    service.setJsonPayloadTransformer(new JsonPayloadTransformer(serializer));
    doNothing().when(service).sendToKafka(any(Log.class), anyString());
    doCallRealMethod().when(this.service).newLog(any(LogRequestBean.class), anyBoolean());
    doCallRealMethod().when(this.service).validate(any(Log.class));
    addResources(new LogResource(service));
}
#end_block

#method_before
public void shouldBeLocaleIndependent() {
    AlarmSubExpression alarmSubExpression = new AlarmSubExpression(AggregateFunction.MIN, new MetricDefinition("hpcs.compute", ImmutableMap.<String, String>builder().put("instance_id", "5").put("metric_name", "cpu").put("device", "1").build()), AlarmOperator.LT, 1.2, 60, 1);
    assertEquals(alarmSubExpression.getExpression(), "min(hpcs.compute{instance_id=5, metric_name=cpu, device=1}) < 1.2");
}
#method_after
public void shouldBeLocaleIndependent() {
    List<Locale> localeList = Lists.newArrayList(Locale.GERMAN, Locale.CHINA, Locale.FRANCE, Locale.JAPAN, Locale.CANADA, Locale.KOREA);
    for (Locale locale : localeList) {
        Locale.setDefault(locale);
        AlarmSubExpression alarmSubExpression = new AlarmSubExpression(AggregateFunction.MIN, new MetricDefinition("hpcs.compute", ImmutableMap.<String, String>builder().put("instance_id", "5").put("metric_name", "cpu").put("device", "1").build()), AlarmOperator.LT, 1.2, 60, 1);
        assertEquals(alarmSubExpression.getExpression(), EXPECTED_EXPRESSION, "Not correct expression for locale " + locale.getDisplayName());
    }
}
#end_block

#method_before
private String buildStatusData(AbstractBuild<?, ?> build) {
    Hudson hudson = Hudson.getInstance();
    AbstractProject<?, ?> project = build.getProject();
    Map data = new HashMap<String, String>();
    data.put("name", project.getName());
    data.put("number", build.getNumber());
    data.put("manager", masterName);
    data.put("worker", this.worker.getWorkerID());
    data.put("label", project.getAssignedLabel());
    String rootUrl = Hudson.getInstance().getRootUrl();
    if (rootUrl != null) {
        data.put("url", rootUrl + build.getUrl());
    }
    Result result = build.getResult();
    if (result != null) {
        data.put("result", result.toString());
    }
    Gson gson = new Gson();
    return gson.toJson(data);
}
#method_after
private String buildStatusData(AbstractBuild<?, ?> build) {
    Hudson hudson = Hudson.getInstance();
    AbstractProject<?, ?> project = build.getProject();
    Map data = new HashMap<String, String>();
    data.put("name", project.getName());
    data.put("number", build.getNumber());
    data.put("manager", masterName);
    data.put("worker", this.worker.getWorkerID());
    String rootUrl = Hudson.getInstance().getRootUrl();
    if (rootUrl != null) {
        data.put("url", rootUrl + build.getUrl());
    }
    Result result = build.getResult();
    if (result != null) {
        data.put("result", result.toString());
    }
    ArrayList<String> nodeLabels = new ArrayList<String>();
    Node node = build.getBuiltOn();
    if (node != null) {
        Set<LabelAtom> nodeLabelAtoms = node.getAssignedLabels();
        for (LabelAtom labelAtom : nodeLabelAtoms) {
            nodeLabels.add(labelAtom.getDisplayName());
        }
    }
    data.put("node_labels", nodeLabels);
    data.put("node_name", node.getNodeName());
    Gson gson = new Gson();
    return gson.toJson(data);
}
#end_block

#method_before
@Test(groups = "orm")
public void shouldFind() {
    checkList(repo.find("Not a tenant id", null, null, null, null, null, null, null, null, 1, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, null, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, null, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", null, null, null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("flavor_id", "222").build(), null, null, null, null, null, 1, false), alarm1, alarm3);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, AlarmState.UNDETERMINED, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, AlarmState.UNDETERMINED, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, null, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, null, null, 1, false), alarm2);
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, DateTime.now(UTC_TIMEZONE), null, 0, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, ISO_8601_FORMATTER.parseDateTime("2015-03-15T00:00:00Z"), null, 0, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, ISO_8601_FORMATTER.parseDateTime("2015-03-14T00:00:00Z"), null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
}
#method_after
@Test(groups = "orm")
public void shouldFind() {
    checkList(repo.find("Not a tenant id", null, null, null, null, null, null, null, null, 1, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, null, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, null, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", null, null, null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("flavor_id", "222").build(), null, null, null, null, null, 1, false), alarm1, alarm3);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").put("hostname", "roland").build(), null, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, null, null, AlarmState.UNDETERMINED, null, null, null, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), null, null, null, null, null, 1, false), alarm1, alarm2);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", null, null, null, null, null, null, 1, false), alarm1, alarm2, alarm3);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, AlarmState.UNDETERMINED, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, AlarmState.UNDETERMINED, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, null, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, null, null, 1, false), alarm2);
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, DateTime.now(UTC_TIMEZONE), null, 0, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, ISO_8601_FORMATTER.parseDateTime("2015-03-15T00:00:00Z"), null, 0, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, ISO_8601_FORMATTER.parseDateTime("2015-03-14T00:00:00Z"), null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
}
#end_block

#method_before
private DateTime getDateField(final String alarmId, final String fieldName) {
    Session session = null;
    DateTime time = null;
    try {
        session = sessionFactory.openSession();
        final List<?> rows = session.createQuery(String.format("select %s from AlarmDb where id = :alarmId", fieldName)).setString("alarmId", alarmId).list();
        time = new DateTime(((Timestamp) rows.get(0)).getTime(), UTC_TIMEZONE);
    } finally {
        if (session != null) {
            session.close();
        }
    }
    return time;
}
#method_after
private DateTime getDateField(final String alarmId, final String fieldName) {
    Session session = null;
    DateTime time = null;
    try {
        session = sessionFactory.openSession();
        final String queryString = String.format("select %s from AlarmDb where id = :alarmId", fieldName);
        final List<?> rows = session.createQuery(queryString).setString("alarmId", alarmId).list();
        time = new DateTime(((Timestamp) rows.get(0)).getTime(), UTC_TIMEZONE);
    } finally {
        if (session != null) {
            session.close();
        }
    }
    return time;
}
#end_block

#method_before
@Override
public NotificationMethod update(String tenantId, String notificationMethodId, String name, NotificationMethodType type, String address) {
    Session session = null;
    Transaction tx = null;
    try {
        session = sessionFactory.openSession();
        final NotificationMethodDb result = this.byTenantIdAndName(session, tenantId, name);
        if (result != null && !result.getId().equalsIgnoreCase(notificationMethodId)) {
            throw new EntityExistsException("Notification method %s \"%s\" already exists.", tenantId, name);
        }
        tx = session.beginTransaction();
        NotificationMethodDb db;
        if ((db = (NotificationMethodDb) session.get(NotificationMethodDb.class, notificationMethodId)) == null) {
            throw new EntityNotFoundException("No notification method exists for %s", notificationMethodId);
        }
        db.setName(name);
        db.setType(AlarmNotificationMethodType.valueOf(type.name()));
        db.setAddress(address);
        session.save(db);
        tx.commit();
        tx = null;
        return this.convertToNotificationMethod(db);
    } catch (RuntimeException e) {
        this.rollbackIfNotNull(tx);
        throw e;
    } finally {
        if (session != null) {
            session.close();
        }
    }
}
#method_after
@Override
public NotificationMethod update(String tenantId, String notificationMethodId, String name, NotificationMethodType type, String address) {
    Session session = null;
    Transaction tx = null;
    try {
        session = sessionFactory.openSession();
        final NotificationMethodDb result = this.byTenantIdAndName(session, tenantId, name);
        if (result != null && !result.getId().equalsIgnoreCase(notificationMethodId)) {
            throw new EntityExistsException("Notification method %s \"%s\" already exists.", tenantId, name);
        }
        tx = session.beginTransaction();
        NotificationMethodDb db;
        if ((db = session.get(NotificationMethodDb.class, notificationMethodId)) == null) {
            throw new EntityNotFoundException("No notification method exists for %s", notificationMethodId);
        }
        db.setName(name);
        db.setType(AlarmNotificationMethodType.valueOf(type.name()));
        db.setAddress(address);
        session.save(db);
        tx.commit();
        tx = null;
        return this.convertToNotificationMethod(db);
    } catch (RuntimeException e) {
        this.rollbackIfNotNull(tx);
        throw e;
    } finally {
        if (session != null) {
            session.close();
        }
    }
}
#end_block

#method_before
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, String lifecycleState, String link, DateTime stateUpdatedStart, String offset, int limit, boolean enforceLimit) {
    logger.trace(ORM_LOG_MARKER, "find(...) entering");
    List<Alarm> alarms;
    alarms = findInternal(tenantId, alarmDefId, metricName, metricDimensions, state, lifecycleState, link, stateUpdatedStart, offset, (3 * limit / 2), enforceLimit);
    if (limit == 0 || !enforceLimit)
        return alarms;
    if (alarms.size() > limit) {
        for (int i = alarms.size() - 1; i > limit; i--) {
            alarms.remove(i);
        }
    } else if (alarms.size() > 0) {
        while (alarms.size() < limit) {
            List<Alarm> alarms2;
            int diff = limit - alarms.size();
            String offset2 = alarms.get(alarms.size() - 1).getId();
            alarms2 = findInternal(tenantId, alarmDefId, metricName, metricDimensions, state, lifecycleState, link, stateUpdatedStart, offset2, (2 * diff), enforceLimit);
            if (alarms2.size() == 0)
                break;
            for (int i = 0; i < alarms2.size() && i < diff; i++) alarms.add(alarms2.get(i));
        }
    }
    return alarms;
}
#method_after
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, String lifecycleState, String link, DateTime stateUpdatedStart, String offset, int limit, boolean enforceLimit) {
    logger.trace(ORM_LOG_MARKER, "find(...) entering");
    List<Alarm> alarms = this.findInternal(tenantId, alarmDefId, metricName, metricDimensions, state, lifecycleState, link, stateUpdatedStart, offset, (3 * limit / 2), enforceLimit);
    if (limit == 0 || !enforceLimit)
        return alarms;
    if (alarms.size() > limit) {
        for (int i = alarms.size() - 1; i > limit; i--) {
            alarms.remove(i);
        }
    } else if (alarms.size() > 0) {
        while (alarms.size() < limit) {
            List<Alarm> alarms2;
            int diff = limit - alarms.size();
            String offset2 = alarms.get(alarms.size() - 1).getId();
            alarms2 = this.findInternal(tenantId, alarmDefId, metricName, metricDimensions, state, lifecycleState, link, stateUpdatedStart, offset2, (2 * diff), enforceLimit);
            if (alarms2.size() == 0)
                break;
            for (int i = 0; i < alarms2.size() && i < diff; i++) alarms.add(alarms2.get(i));
        }
    }
    return alarms;
}
#end_block

#method_before
private List<Alarm> createAlarms(List<Object[]> alarmList) {
    List<Alarm> alarms = Lists.newLinkedList();
    Alarm alarm = null;
    String previousAlarmId = null;
    BinaryId previousDimensionSetId = null;
    List<MetricDefinition> alarmedMetrics = null;
    Map<String, String> dimensionMap = new HashMap<>();
    for (Object[] alarmRow : alarmList) {
        String alarmDefinitionId = (String) alarmRow[0];
        AlarmSeverity severity = Conversions.variantToEnum(alarmRow[1], AlarmSeverity.class);
        AlarmState alarmState = Conversions.variantToEnum(alarmRow[4], AlarmState.class);
        DateTime updatedTimestamp = Conversions.variantToDateTime(alarmRow[5]);
        DateTime createdTimestamp = Conversions.variantToDateTime(alarmRow[6]);
        BinaryId dimensionSetId = this.convertBinaryId(alarmRow[13]);
        DateTime stateUpdatedTimestamp = Conversions.variantToDateTime(alarmRow[12]);
        String alarm_definition_name = (String) alarmRow[2];
        String id = (String) alarmRow[3];
        String lifecycle_state = (String) alarmRow[10];
        String link = (String) alarmRow[11];
        String metric_name = (String) alarmRow[7];
        String dimension_name = (String) alarmRow[8];
        String dimension_value = (String) alarmRow[9];
        if (!id.equals(previousAlarmId)) {
            alarmedMetrics = new ArrayList<>();
            dimensionMap = Maps.newHashMap();
            alarmedMetrics.add(new MetricDefinition(metric_name, dimensionMap));
            alarm = new Alarm(id, alarmDefinitionId, alarm_definition_name, severity.name(), alarmedMetrics, alarmState, lifecycle_state, link, stateUpdatedTimestamp, updatedTimestamp, createdTimestamp);
            alarms.add(alarm);
            previousDimensionSetId = dimensionSetId;
        }
        if (!dimensionSetId.equals(previousDimensionSetId)) {
            dimensionMap = Maps.newHashMap();
            alarmedMetrics.add(new MetricDefinition(metric_name, dimensionMap));
        }
        dimensionMap.put(dimension_name, dimension_value);
        previousDimensionSetId = dimensionSetId;
        previousAlarmId = id;
    }
    return alarms;
}
#method_after
private List<Alarm> createAlarms(List<Object[]> alarmList) {
    List<Alarm> alarms = Lists.newLinkedList();
    String previousAlarmId = null;
    BinaryId previousDimensionSetId = null;
    List<MetricDefinition> alarmedMetrics = null;
    Map<String, String> dimensionMap = new HashMap<>();
    for (Object[] alarmRow : alarmList) {
        String alarmDefinitionId = (String) alarmRow[0];
        AlarmSeverity severity = Conversions.variantToEnum(alarmRow[1], AlarmSeverity.class);
        AlarmState alarmState = Conversions.variantToEnum(alarmRow[4], AlarmState.class);
        DateTime updatedTimestamp = Conversions.variantToDateTime(alarmRow[5]);
        DateTime createdTimestamp = Conversions.variantToDateTime(alarmRow[6]);
        BinaryId dimensionSetId = this.convertBinaryId(alarmRow[13]);
        DateTime stateUpdatedTimestamp = Conversions.variantToDateTime(alarmRow[12]);
        String alarm_definition_name = (String) alarmRow[2];
        String id = (String) alarmRow[3];
        String lifecycle_state = (String) alarmRow[10];
        String link = (String) alarmRow[11];
        String metric_name = (String) alarmRow[7];
        String dimension_name = (String) alarmRow[8];
        String dimension_value = (String) alarmRow[9];
        if (!id.equals(previousAlarmId)) {
            alarmedMetrics = new ArrayList<>();
            dimensionMap = Maps.newHashMap();
            alarmedMetrics.add(new MetricDefinition(metric_name, dimensionMap));
            alarms.add(new Alarm(id, alarmDefinitionId, alarm_definition_name, severity.name(), alarmedMetrics, alarmState, lifecycle_state, link, stateUpdatedTimestamp, updatedTimestamp, createdTimestamp));
            previousDimensionSetId = dimensionSetId;
        }
        if (!dimensionSetId.equals(previousDimensionSetId)) {
            dimensionMap = Maps.newHashMap();
            alarmedMetrics.add(new MetricDefinition(metric_name, dimensionMap));
        }
        dimensionMap.put(dimension_name, dimension_value);
        previousDimensionSetId = dimensionSetId;
        previousAlarmId = id;
    }
    return alarms;
}
#end_block

#method_before
private void setDates(final DateTime createdAt, final DateTime updatedAt) {
    final Date date = DateTime.now(DateTimeZone.UTC).toDate();
    if (createdAt == null && updatedAt == null) {
        this.updatedAt = date;
        this.createdAt = date;
    } else if (createdAt == null) {
        this.createdAt = date;
    } else if (updatedAt == null) {
        this.updatedAt = date;
    } else {
        this.createdAt = createdAt.toDateTime(DateTimeZone.UTC).toDate();
        this.updatedAt = updatedAt.toDateTime(DateTimeZone.UTC).toDate();
    }
}
#method_after
private void setDates(final DateTime createdAt, final DateTime updatedAt) {
    final Date date = DateTime.now(DateTimeZone.UTC).toDate();
    if (createdAt == null) {
        this.createdAt = date;
    } else {
        this.createdAt = createdAt.toDateTime(DateTimeZone.UTC).toDate();
    }
    if (updatedAt == null) {
        this.updatedAt = date;
    } else {
        this.updatedAt = updatedAt.toDateTime(DateTimeZone.UTC).toDate();
    }
}
#end_block

#method_before
public static DateTime variantToDateTime(final Object variant, final DateTimeZone timeZone) {
    return new DateTime(variant, timeZone);
}
#method_after
public static DateTime variantToDateTime(final Object variant, final DateTimeZone timeZone) {
    if (variant instanceof DateTime) {
        return ((DateTime) variant).toDateTime(timeZone);
    }
    return new DateTime(variant, timeZone);
}
#end_block

#method_before
@SuppressWarnings("unchecked")
public static <T extends Enum<T>> T variantToEnum(final Object variant, final Class<T> enumClazz) {
    if (variant instanceof String) {
        return Enum.valueOf(enumClazz, ((String) variant).trim().toUpperCase());
    } else if (variant instanceof Number) {
        final Integer index = variantToInteger(variant);
        final T[] enumConstants = enumClazz.getEnumConstants();
        if (index < 0 || index >= enumConstants.length) {
            throw new IllegalArgumentException(String.format("Variant of type \"%s\", and value \"%s\" exceeds maximum number of constants %d", variant.getClass(), variant, enumConstants.length));
        }
        return enumConstants[index];
    } else if (variant instanceof Enum) {
        return (T) variant;
    }
    throw new IllegalArgumentException(String.format("Variant of type \"%s\", and value \"%s\" is not a Number.", variant.getClass(), variant));
}
#method_after
@SuppressWarnings("unchecked")
public static <T extends Enum<T>> T variantToEnum(final Object variant, final Class<T> enumClazz) {
    if (variant == null) {
        return null;
    }
    if (variant instanceof String) {
        return Enum.valueOf(enumClazz, ((String) variant).trim().toUpperCase());
    } else if (variant instanceof Number) {
        final Integer index = variantToInteger(variant);
        final T[] enumConstants = enumClazz.getEnumConstants();
        if (index < 0 || index >= enumConstants.length) {
            throw new IllegalArgumentException(String.format("Variant of type \"%s\", and value \"%s\" is out of range [, %d]", variant.getClass(), variant, enumConstants.length));
        }
        return enumConstants[index];
    } else if (variant instanceof Enum) {
        return (T) variant;
    }
    throw new IllegalArgumentException(String.format("\"%s\", and value \"%s\" is not one of %s", variant.getClass(), variant, Arrays.toString(SUPPORTED_VARIANT_TO_ENUM_TYPES)));
}
#end_block

#method_before
@Override
public void prepare(Map stormConf, Object registrationArgument, TopologyContext context, IErrorReporter errorReporter) {
    logger = LoggerFactory.getLogger(Logging.categoryFor(getClass(), context));
    parseConfig(stormConf);
    if (registrationArgument instanceof Map) {
        parseConfig((Map<?, ?>) registrationArgument);
    }
    initClient();
    logger.info("topologyName ({}), clean(topologyName) ({})", new Object[] { topologyName, clean(topologyName) });
}
#method_after
@Override
public void prepare(Map stormConf, Object registrationArgument, TopologyContext context, IErrorReporter errorReporter) {
    logger = LoggerFactory.getLogger(Logging.categoryFor(getClass(), context));
    /* Sets up locals from the config STATSD_WHITELIST, STATSD_HOST ... */
    parseConfig(stormConf);
    /* Sets up local vars from config vars if present */
    if (registrationArgument instanceof Map) {
        parseConfig((Map<?, ?>) registrationArgument);
    }
    initClient();
    logger.info("topologyName ({}), " + "clean(topologyName) ({})", new Object[] { topologyName, clean(topologyName) });
}
#end_block

#method_before
@SuppressWarnings("unchecked")
void parseConfig(Map<?, ?> conf) {
    if (conf.containsKey(Config.TOPOLOGY_NAME)) {
        topologyName = (String) conf.get(Config.TOPOLOGY_NAME);
    }
    if (conf.containsKey(STATSD_HOST)) {
        statsdHost = (String) conf.get(STATSD_HOST);
    }
    if (conf.containsKey(STATSD_PORT)) {
        statsdPort = ((Number) conf.get(STATSD_PORT)).intValue();
    }
    if (conf.containsKey(STATSD_DIMENSIONS)) {
        statsdDimensions = mapToJsonStr((Map<String, String>) conf.get(STATSD_DIMENSIONS));
        if (!isValidJSON(statsdDimensions)) {
            logger.error("Ignoring dimensions element invalid JSON ({})", new Object[] { statsdDimensions });
            // You get default dimensions
            statsdDimensions = monascaStatsdDimPrefix + defaultDimensions;
        } else {
            statsdDimensions = monascaStatsdDimPrefix + statsdDimensions;
        }
    }
    if (conf.containsKey(STATSD_WHITELIST)) {
        whiteList = (List) conf.get(STATSD_WHITELIST);
    }
    if (conf.containsKey(STATSD_METRICMAP)) {
        metricMap = (Map) conf.get(STATSD_METRICMAP);
    }
    if (conf.containsKey(STATSD_DEBUGMETRICS)) {
        debugMetrics = (Boolean) conf.get(STATSD_DEBUGMETRICS);
    }
}
#method_after
@SuppressWarnings("unchecked")
void parseConfig(Map<?, ?> conf) {
    if (conf.containsKey(Config.TOPOLOGY_NAME)) {
        topologyName = (String) conf.get(Config.TOPOLOGY_NAME);
    }
    if (conf.containsKey(STATSD_HOST)) {
        statsdHost = (String) conf.get(STATSD_HOST);
    }
    if (conf.containsKey(STATSD_PORT)) {
        statsdPort = ((Number) conf.get(STATSD_PORT)).intValue();
    }
    if (conf.containsKey(STATSD_DIMENSIONS)) {
        statsdDimensions = mapToJsonStr((Map<String, String>) conf.get(STATSD_DIMENSIONS));
        if (!isValidJSON(statsdDimensions)) {
            logger.error("Ignoring dimensions element invalid JSON ({})", new Object[] { statsdDimensions });
            // You get default dimensions
            statsdDimensions = monascaStatsdDimPrefix + defaultDimensions;
        } else {
            statsdDimensions = monascaStatsdDimPrefix + statsdDimensions;
        }
    }
    if (conf.containsKey(STATSD_WHITELIST)) {
        whiteList = (List<String>) conf.get(STATSD_WHITELIST);
    }
    if (conf.containsKey(STATSD_METRICMAP)) {
        metricMap = (Map<String, String>) conf.get(STATSD_METRICMAP);
    }
    if (conf.containsKey(STATSD_DEBUGMETRICS)) {
        debugMetrics = (Boolean) conf.get(STATSD_DEBUGMETRICS);
    }
}
#end_block

#method_before
private void reportUOM(String s, Double number) {
    String metricName = null;
    StringBuilder results = new StringBuilder();
    if (whiteList.contains(s)) {
        if (!metricMap.isEmpty() && metricMap.containsKey(s)) {
            metricName = metricMap.get(s);
        } else /* Send the unmapped uom as the same name storm calls it */
        {
            metricName = s;
        }
    }
    if (debugMetrics) {
        String mappedName = new String();
        if (!metricMap.isEmpty() && metricMap.containsKey(s)) {
            mappedName = metricMap.get(s);
        } else {
            mappedName = s;
        }
        logger.info(", RawMetricName, {}, MappedMetricName, {}, val, {}", new Object[] { s, mappedName, number });
    }
    if (metricName != null && !metricName.isEmpty()) {
        results = results.append(metricName).append(":").append(String.valueOf(number)).append("|c").append(statsdDimensions);
        report(results.toString());
    }
}
#method_after
private void reportUOM(String s, Double number) {
    String metricName = null;
    StringBuilder results = new StringBuilder();
    Boolean published = false;
    if (whiteList.contains(s)) {
        if (!metricMap.isEmpty() && metricMap.containsKey(s)) {
            metricName = metricMap.get(s);
        } else /* Send the unmapped uom as the same name storm calls it */
        {
            metricName = s;
        }
        /* Make sure we don't send metric names that may be null or empty */
        if (metricName != null && !metricName.isEmpty()) {
            published = true;
        }
    }
    if (debugMetrics) {
        String mappedName = new String();
        if (!metricMap.isEmpty() && metricMap.containsKey(s)) {
            mappedName = metricMap.get(s);
        } else {
            mappedName = s;
        }
        logger.info(", RawMetricName, {}, MappedMetricName, {}, " + "val, {}, {}", new Object[] { s, mappedName, number, published == true ? "PUBLISHED" : "UNPUBLISHED" });
    }
    if (published) {
        results = results.append(metricName).append(":").append(String.valueOf(number)).append("|c").append(statsdDimensions);
        report(results.toString());
    }
}
#end_block

#method_before
private String buildAuth(final String userName, final String password, final String projectId, final String projectName, final String userDomain, final String projectDomain) {
    final JsonObject jsonUserDomain = new JsonObject();
    if (!userDomain.isEmpty()) {
        jsonUserDomain.addProperty("name", userDomain);
    } else {
        jsonUserDomain.addProperty("id", "default");
    }
    final JsonObject user = new JsonObject();
    user.addProperty("name", userName);
    user.addProperty("password", password);
    user.add("domain", jsonUserDomain);
    final JsonObject passwordHolder = new JsonObject();
    passwordHolder.add("user", user);
    final JsonArray methods = new JsonArray();
    methods.add(new JsonPrimitive("password"));
    final JsonObject identity = new JsonObject();
    identity.add("methods", methods);
    identity.add("password", passwordHolder);
    boolean scopeDefined = false;
    final JsonObject project = new JsonObject();
    // If project id is available, it is preferred
    if (!projectId.isEmpty()) {
        project.addProperty("id", projectId);
        scopeDefined = true;
    } else if (!projectName.isEmpty()) {
        final JsonObject jsonProjectDomain = new JsonObject();
        if (!projectDomain.isEmpty()) {
            jsonProjectDomain.addProperty("name", projectDomain);
        } else {
            jsonProjectDomain.addProperty("id", "default");
        }
        project.add("domain", jsonProjectDomain);
        project.addProperty("name", projectName);
        scopeDefined = true;
    }
    final JsonObject auth = new JsonObject();
    auth.add("identity", identity);
    if (scopeDefined) {
        final JsonObject scope = new JsonObject();
        scope.add("project", project);
        auth.add("scope", scope);
    }
    final JsonObject outer = new JsonObject();
    outer.add("auth", auth);
    return outer.toString();
}
#method_after
private String buildAuth(final String userName, final String password, final String projectId, final String projectName, final String userDomainName, final String projectDomainName) {
    final JsonObject UserDomain = new JsonObject();
    if (!userDomainName.isEmpty()) {
        UserDomain.addProperty("name", userDomainName);
    } else {
        UserDomain.addProperty("id", "default");
    }
    final JsonObject user = new JsonObject();
    user.addProperty("name", userName);
    user.addProperty("password", password);
    user.add("domain", UserDomain);
    final JsonObject passwordHolder = new JsonObject();
    passwordHolder.add("user", user);
    final JsonArray methods = new JsonArray();
    methods.add(new JsonPrimitive("password"));
    final JsonObject identity = new JsonObject();
    identity.add("methods", methods);
    identity.add("password", passwordHolder);
    boolean scopeDefined = false;
    final JsonObject project = new JsonObject();
    // If project id is available, it is preferred
    if (!projectId.isEmpty()) {
        project.addProperty("id", projectId);
        scopeDefined = true;
    } else if (!projectName.isEmpty()) {
        final JsonObject ProjectDomain = new JsonObject();
        if (!projectDomainName.isEmpty()) {
            ProjectDomain.addProperty("name", projectDomainName);
        } else {
            ProjectDomain.addProperty("id", "default");
        }
        project.add("domain", ProjectDomain);
        project.addProperty("name", projectName);
        scopeDefined = true;
    }
    final JsonObject auth = new JsonObject();
    auth.add("identity", identity);
    if (scopeDefined) {
        final JsonObject scope = new JsonObject();
        scope.add("project", project);
        auth.add("scope", scope);
    }
    final JsonObject outer = new JsonObject();
    outer.add("auth", auth);
    return outer.toString();
}
#end_block

#method_before
public static Integer variantToInteger(Object variant) {
    if (variant instanceof Integer) {
        return ((Integer) variant);
    } else if (variant instanceof Long) {
        return ((Long) variant).intValue();
    } else {
        throw new IllegalArgumentException(String.format("Type is not Long or Integer: 1%s", variant));
    }
}
#method_after
public static Integer variantToInteger(Object variant) {
    if (variant instanceof Number) {
        return ((Number) variant).intValue();
    } else {
        throw new IllegalArgumentException(String.format("Variant of type \"%s\", and value \"%s\" is not a Number.", variant.getClass(), variant));
    }
}
#end_block

#method_before
@Override
protected void configure() {
    if (config.hibernate.getSupportEnabled()) {
        bind(AlarmRepo.class).to(AlarmSqlRepoImpl.class).in(Singleton.class);
        bind(AlarmDefinitionRepo.class).to(AlarmDefinitionSqlRepoImpl.class).in(Singleton.class);
        bind(NotificationMethodRepo.class).to(NotificationMethodSqlRepoImpl.class).in(Singleton.class);
    } else {
        bind(AlarmRepo.class).to(AlarmMySqlRepoImpl.class).in(Singleton.class);
        bind(AlarmDefinitionRepo.class).to(AlarmDefinitionMySqlRepoImpl.class).in(Singleton.class);
        bind(NotificationMethodRepo.class).to(NotificationMethodMySqlRepoImpl.class).in(Singleton.class);
        bind(MySQLUtils.class);
        bind(PersistUtils.class).in(Singleton.class);
    }
    if (config.databaseConfiguration.getDatabaseType().trim().equalsIgnoreCase(VERTICA)) {
        if (config.hibernate.getSupportEnabled()) {
            bind(AlarmStateHistoryRepo.class).to(AlarmStateHistoryVerticaRepoHibernateImpl.class).in(Singleton.class);
        } else {
            bind(AlarmStateHistoryRepo.class).to(AlarmStateHistoryVerticaRepoImpl.class).in(Singleton.class);
        }
        bind(MetricDefinitionRepo.class).to(MetricDefinitionVerticaRepoImpl.class).in(Singleton.class);
        bind(MeasurementRepo.class).to(MeasurementVerticaRepoImpl.class).in(Singleton.class);
        bind(StatisticRepo.class).to(StatisticVerticaRepoImpl.class).in(Singleton.class);
    } else if (config.databaseConfiguration.getDatabaseType().trim().equalsIgnoreCase(INFLUXDB)) {
        if (config.influxDB.getVersion() != null && !config.influxDB.getVersion().equalsIgnoreCase(INFLUXDB_V9)) {
            System.err.println("Found unsupported Influxdb version: " + config.influxDB.getVersion());
            System.err.println("Supported Influxdb versions are 'v9'");
            System.err.println("Check your config file");
            System.exit(1);
        }
        bind(InfluxV9Utils.class).in(Singleton.class);
        bind(InfluxV9RepoReader.class).in(Singleton.class);
        if (config.hibernate.getSupportEnabled()) {
            bind(AlarmStateHistoryRepo.class).to(InfluxV9AlarmStateHistoryHibernateRepo.class).in(Singleton.class);
        } else {
            bind(AlarmStateHistoryRepo.class).to(InfluxV9AlarmStateHistoryRepo.class).in(Singleton.class);
        }
        bind(MetricDefinitionRepo.class).to(InfluxV9MetricDefinitionRepo.class).in(Singleton.class);
        bind(MeasurementRepo.class).to(InfluxV9MeasurementRepo.class).in(Singleton.class);
        bind(StatisticRepo.class).to(InfluxV9StatisticRepo.class).in(Singleton.class);
    } else {
        throw new ProvisionException("Failed to detect supported database. Supported databases are " + "'vertica' and 'influxdb'. Check your config file.");
    }
}
#method_after
@Override
protected void configure() {
    final boolean hibernateEnabled = this.isHibernateEnabled();
    this.bindUtils(hibernateEnabled);
    if (hibernateEnabled) {
        this.bind(AlarmRepo.class).to(AlarmSqlRepoImpl.class).in(Singleton.class);
        this.bind(AlarmDefinitionRepo.class).to(AlarmDefinitionSqlRepoImpl.class).in(Singleton.class);
        this.bind(NotificationMethodRepo.class).to(NotificationMethodSqlRepoImpl.class).in(Singleton.class);
    } else {
        bind(AlarmRepo.class).to(AlarmMySqlRepoImpl.class).in(Singleton.class);
        bind(AlarmDefinitionRepo.class).to(AlarmDefinitionMySqlRepoImpl.class).in(Singleton.class);
        bind(NotificationMethodRepo.class).to(NotificationMethodMySqlRepoImpl.class).in(Singleton.class);
        bind(PersistUtils.class).in(Singleton.class);
    }
    if (config.databaseConfiguration.getDatabaseType().trim().equalsIgnoreCase(VERTICA)) {
        bind(AlarmStateHistoryRepo.class).to(AlarmStateHistoryVerticaRepoImpl.class).in(Singleton.class);
        bind(MetricDefinitionRepo.class).to(MetricDefinitionVerticaRepoImpl.class).in(Singleton.class);
        bind(MeasurementRepo.class).to(MeasurementVerticaRepoImpl.class).in(Singleton.class);
        bind(StatisticRepo.class).to(StatisticVerticaRepoImpl.class).in(Singleton.class);
    } else if (config.databaseConfiguration.getDatabaseType().trim().equalsIgnoreCase(INFLUXDB)) {
        if (config.influxDB.getVersion() != null && !config.influxDB.getVersion().equalsIgnoreCase(INFLUXDB_V9)) {
            System.err.println("Found unsupported Influxdb version: " + config.influxDB.getVersion());
            System.err.println("Supported Influxdb versions are 'v9'");
            System.err.println("Check your config file");
            System.exit(1);
        }
        bind(InfluxV9Utils.class).in(Singleton.class);
        bind(InfluxV9RepoReader.class).in(Singleton.class);
        bind(AlarmStateHistoryRepo.class).to(InfluxV9AlarmStateHistoryRepo.class).in(Singleton.class);
        bind(MetricDefinitionRepo.class).to(InfluxV9MetricDefinitionRepo.class).in(Singleton.class);
        bind(MeasurementRepo.class).to(InfluxV9MeasurementRepo.class).in(Singleton.class);
        bind(StatisticRepo.class).to(InfluxV9StatisticRepo.class).in(Singleton.class);
    } else {
        throw new ProvisionException("Failed to detect supported database. Supported databases are " + "'vertica' and 'influxdb'. Check your config file.");
    }
}
#end_block

#method_before
@Override
protected void configure() {
    bind(ApiConfig.class).toInstance(config);
    bind(MetricRegistry.class).toInstance(environment.metrics());
    if (!config.hibernate.getSupportEnabled()) {
        bind(DataSourceFactory.class).annotatedWith(Names.named("mysql")).toInstance(config.mysql);
    }
    bind(DataSourceFactory.class).annotatedWith(Names.named("vertica")).toInstance(config.vertica);
    install(new ApplicationModule());
    install(new DomainModule());
    install(new InfrastructureModule(this.config));
}
#method_after
@Override
protected void configure() {
    bind(ApiConfig.class).toInstance(config);
    bind(MetricRegistry.class).toInstance(environment.metrics());
    if (!this.isHibernateEnabled()) {
        bind(DataSourceFactory.class).annotatedWith(Names.named("mysql")).toInstance(config.mysql);
    }
    bind(DataSourceFactory.class).annotatedWith(Names.named("vertica")).toInstance(config.vertica);
    install(new ApplicationModule());
    install(new DomainModule());
    install(new InfrastructureModule(this.config));
}
#end_block

#method_before
@Provides
@Singleton
@Named("orm")
public SessionFactory getSessionFactory() {
    try {
        Configuration configuration = new Configuration();
        configuration.addAnnotatedClass(AlarmDb.class);
        configuration.addAnnotatedClass(AlarmDefinitionDb.class);
        configuration.addAnnotatedClass(AlarmMetricDb.class);
        configuration.addAnnotatedClass(MetricDefinitionDb.class);
        configuration.addAnnotatedClass(MetricDefinitionDimensionsDb.class);
        configuration.addAnnotatedClass(MetricDimensionDb.class);
        configuration.addAnnotatedClass(SubAlarmDefinitionDb.class);
        configuration.addAnnotatedClass(SubAlarmDefinitionDimensionDb.class);
        configuration.addAnnotatedClass(SubAlarmDb.class);
        configuration.addAnnotatedClass(AlarmActionDb.class);
        configuration.addAnnotatedClass(NotificationMethodDb.class);
        configuration.setProperties(this.getHikariProperties());
        ServiceRegistry serviceRegistry = new StandardServiceRegistryBuilder().applySettings(configuration.getProperties()).build();
        // builds a session factory from the service registry
        return configuration.buildSessionFactory(serviceRegistry);
    } catch (Throwable ex) {
        throw new ProvisionException("Failed to provision ORM DBI", ex);
    }
}
#method_after
@Provides
@Singleton
@Named("orm")
public SessionFactory getSessionFactory() {
    if (config.hibernate == null) {
        throw new ProvisionException("Unable to provision ORM DBI, couldn't locate hibernate configuration");
    }
    try {
        Configuration configuration = new Configuration();
        configuration.addAnnotatedClass(AlarmDb.class);
        configuration.addAnnotatedClass(AlarmActionDb.class);
        configuration.addAnnotatedClass(AlarmActionId.class);
        configuration.addAnnotatedClass(AlarmDefinitionDb.class);
        configuration.addAnnotatedClass(AlarmMetricDb.class);
        configuration.addAnnotatedClass(AlarmMetricId.class);
        configuration.addAnnotatedClass(MetricDefinitionDb.class);
        configuration.addAnnotatedClass(MetricDefinitionDimensionsDb.class);
        configuration.addAnnotatedClass(MetricDimensionDb.class);
        configuration.addAnnotatedClass(SubAlarmDefinitionDb.class);
        configuration.addAnnotatedClass(SubAlarmDefinitionDimensionDb.class);
        configuration.addAnnotatedClass(SubAlarmDb.class);
        configuration.addAnnotatedClass(NotificationMethodDb.class);
        configuration.setProperties(this.getORMProperties(this.config.hibernate.getDataSourceClassName()));
        ServiceRegistry serviceRegistry = new StandardServiceRegistryBuilder().applySettings(configuration.getProperties()).build();
        // builds a session factory from the service registry
        return configuration.buildSessionFactory(serviceRegistry);
    } catch (Throwable ex) {
        throw new ProvisionException("Failed to provision ORM DBI", ex);
    }
}
#end_block

#method_before
@Provides
@Singleton
public SessionFactory sessionFactory() {
    try {
        Configuration configuration = new Configuration();
        configuration.addAnnotatedClass(AlarmDb.class);
        configuration.addAnnotatedClass(AlarmDefinitionDb.class);
        configuration.addAnnotatedClass(AlarmMetricDb.class);
        configuration.addAnnotatedClass(MetricDefinitionDb.class);
        configuration.addAnnotatedClass(MetricDefinitionDimensionsDb.class);
        configuration.addAnnotatedClass(MetricDimensionDb.class);
        configuration.addAnnotatedClass(SubAlarmDefinitionDb.class);
        configuration.addAnnotatedClass(SubAlarmDefinitionDimensionDb.class);
        configuration.addAnnotatedClass(SubAlarmDb.class);
        configuration.addAnnotatedClass(AlarmActionDb.class);
        configuration.addAnnotatedClass(NotificationMethodDb.class);
        configuration.setProperties(this.getHikariProperties());
        ServiceRegistry serviceRegistry = new StandardServiceRegistryBuilder().applySettings(configuration.getProperties()).build();
        // builds a session factory from the service registry
        return configuration.buildSessionFactory(serviceRegistry);
    } catch (Throwable ex) {
        throw new ProvisionException("Failed to provision Hibernate DB", ex);
    }
}
#method_after
@Provides
@Singleton
public SessionFactory sessionFactory() {
    try {
        Configuration configuration = new Configuration();
        configuration.addAnnotatedClass(AlarmDb.class);
        configuration.addAnnotatedClass(AlarmDefinitionDb.class);
        configuration.addAnnotatedClass(AlarmMetricDb.class);
        configuration.addAnnotatedClass(MetricDefinitionDb.class);
        configuration.addAnnotatedClass(MetricDefinitionDimensionsDb.class);
        configuration.addAnnotatedClass(MetricDimensionDb.class);
        configuration.addAnnotatedClass(SubAlarmDefinitionDb.class);
        configuration.addAnnotatedClass(SubAlarmDefinitionDimensionDb.class);
        configuration.addAnnotatedClass(SubAlarmDb.class);
        configuration.addAnnotatedClass(AlarmActionDb.class);
        configuration.addAnnotatedClass(NotificationMethodDb.class);
        // retrieve hikari properties for right driver
        configuration.setProperties(this.getHikariProperties(this.dbConfig.getDriverClass()));
        final ServiceRegistry serviceRegistry = new StandardServiceRegistryBuilder().applySettings(configuration.getProperties()).build();
        // builds a session factory from the service registry
        return configuration.buildSessionFactory(serviceRegistry);
    } catch (Throwable ex) {
        throw new ProvisionException("Failed to provision Hibernate DB", ex);
    }
}
#end_block

#method_before
private Properties getHikariProperties() {
    Properties properties = new Properties();
    properties.put("hibernate.connection.provider_class", dbConfig.getProviderClass());
    properties.put("hibernate.hbm2ddl.auto", dbConfig.getAutoConfig());
    properties.put("show_sql", false);
    properties.put("hibernate.hikari.dataSourceClassName", dbConfig.getDriverClass());
    properties.put("hibernate.hikari.dataSource.serverName", dbConfig.getServerName());
    properties.put("hibernate.hikari.dataSource.portNumber", dbConfig.getPortNumber());
    properties.put("hibernate.hikari.dataSource.databaseName", dbConfig.getDatabaseName());
    properties.put("hibernate.hikari.dataSource.user", dbConfig.getUser());
    properties.put("hibernate.hikari.dataSource.password", dbConfig.getPassword());
    properties.put("hibernate.hikari.dataSource.initialConnections", dbConfig.getMinSize());
    properties.put("hibernate.hikari.dataSource.maxConnections", dbConfig.getMaxSize());
    properties.put("hibernate.hikari.connectionTestQuery", dbConfig.getValidationQuery());
    properties.put("hibernate.hikari.connectionTimeout", "5000");
    properties.put("hibernate.hikari.initializationFailFast", "false");
    return properties;
}
#method_after
private Properties getHikariProperties(final String dataSourceClassName) {
    final Properties properties = new Properties();
    // different drivers requires different sets of properties
    switch(dataSourceClassName) {
        case POSTGRES_DS_CLASS:
            this.handlePostgresORMProperties(properties);
            break;
        case MYSQL_DS_CLASS:
            this.handleMySQLORMProperties(properties);
            break;
        default:
            throw new ProvisionException(String.format("%s is not supported, valid data sources are %s", dataSourceClassName, Arrays.asList(POSTGRES_DS_CLASS, MYSQL_DS_CLASS)));
    }
    // different drivers requires different sets of properties
    // driver agnostic properties
    this.handleCommonORMProperties(properties);
    return properties;
}
#end_block

#method_before
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, DateTime endTime, List<String> statisticsCols, int period, String offset, int limit, Boolean mergeMetricsFlag) throws MultipleMetricsException {
    List<Statistics> statisticsList = new ArrayList<>();
    // Sort the column names so that they match the order of the statistics in the results.
    List<String> statisticsColumns = createColumnsList(statisticsCols);
    try (Handle h = db.open()) {
        Map<byte[], Statistics> byteMap = findDefIds(h, tenantId, name, dimensions);
        if (byteMap.isEmpty()) {
            return statisticsList;
        }
        if (!Boolean.TRUE.equals(mergeMetricsFlag) && byteMap.keySet().size() > 1) {
            throw new MultipleMetricsException(name, dimensions);
        }
        List<List<Object>> statisticsListList = new ArrayList<>();
        String sql = createQuery(byteMap.keySet(), period, startTime, endTime, offset, statisticsCols);
        logger.debug("vertics sql: {}", sql);
        Query<Map<String, Object>> query = h.createQuery(sql).bind("start_time", startTime).bind("end_time", endTime).bind("limit", limit + 1);
        if (offset != null && !offset.isEmpty()) {
            logger.debug("binding offset: {}", offset);
            long offsetMs = DateTime.parse(offset).getMillis();
            // 
            // The 'next' link is the last timeslice returned, so
            // we increment offset by the interval to begin at the
            // next time bucket.
            // 
            offsetMs += period * 1000;
            query.bind("offset", new Timestamp(offsetMs));
        }
        List<Map<String, Object>> rows = query.list();
        for (Map<String, Object> row : rows) {
            List<Object> statisticsRow = parseRow(row);
            statisticsListList.add(statisticsRow);
        }
        // Just use the first entry in the byteMap to get the def name and dimensions.
        Statistics statistics = byteMap.entrySet().iterator().next().getValue();
        statistics.setColumns(statisticsColumns);
        if (Boolean.TRUE.equals(mergeMetricsFlag) && byteMap.keySet().size() > 1) {
            // Wipe out the dimensions.
            statistics.setDimensions(new HashMap<String, String>());
        }
        statistics.setStatistics(statisticsListList);
        statisticsList.add(statistics);
    }
    return statisticsList;
}
#method_after
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, DateTime endTime, List<String> statisticsCols, int period, String offset, int limit, Boolean mergeMetricsFlag) throws MultipleMetricsException {
    List<Statistics> statisticsList = new ArrayList<>();
    // Sort the column names so that they match the order of the statistics in the results.
    List<String> statisticsColumns = createColumnsList(statisticsCols);
    try (Handle h = db.open()) {
        Map<byte[], Statistics> byteMap = findDefIds(h, tenantId, name, dimensions);
        if (byteMap.isEmpty()) {
            return statisticsList;
        }
        if (!Boolean.TRUE.equals(mergeMetricsFlag) && byteMap.keySet().size() > 1) {
            throw new MultipleMetricsException(name, dimensions);
        }
        List<List<Object>> statisticsListList = new ArrayList<>();
        String sql = createQuery(byteMap.keySet(), period, startTime, endTime, offset, statisticsCols);
        logger.debug("vertics sql: {}", sql);
        Query<Map<String, Object>> query = h.createQuery(sql).bind("start_time", startTime).bind("end_time", endTime).bind("limit", limit + 1);
        if (offset != null && !offset.isEmpty()) {
            logger.debug("binding offset: {}", offset);
            query.bind("offset", new Timestamp(DateTime.parse(offset).getMillis()));
        }
        List<Map<String, Object>> rows = query.list();
        for (Map<String, Object> row : rows) {
            List<Object> statisticsRow = parseRow(row);
            statisticsListList.add(statisticsRow);
        }
        // Just use the first entry in the byteMap to get the def name and dimensions.
        Statistics statistics = byteMap.entrySet().iterator().next().getValue();
        statistics.setColumns(statisticsColumns);
        if (Boolean.TRUE.equals(mergeMetricsFlag) && byteMap.keySet().size() > 1) {
            // Wipe out the dimensions.
            statistics.setDimensions(new HashMap<String, String>());
        }
        statistics.setStatistics(statisticsListList);
        statisticsList.add(statistics);
    }
    return statisticsList;
}
#end_block

#method_before
private String createQuery(Set<byte[]> defDimIdSet, int period, DateTime startTime, DateTime endTime, String offset, List<String> statistics) {
    StringBuilder sb = new StringBuilder();
    sb.append("SELECT " + createColumnsStr(statistics));
    if (period >= 1) {
        sb.append("Time_slice(time_stamp, " + period);
        sb.append(") AS time_interval");
    }
    sb.append(" FROM MonMetrics.Measurements ");
    String inClause = createInClause(defDimIdSet);
    sb.append("WHERE to_hex(definition_dimensions_id) " + inClause);
    sb.append(createWhereClause(startTime, endTime, offset));
    if (period >= 1) {
        sb.append("group by Time_slice(time_stamp, " + period);
        sb.append(") order by time_interval");
    }
    sb.append(" limit :limit");
    return sb.toString();
}
#method_after
private String createQuery(Set<byte[]> defDimIdSet, int period, DateTime startTime, DateTime endTime, String offset, List<String> statistics) {
    StringBuilder sb = new StringBuilder();
    sb.append("SELECT " + createColumnsStr(statistics));
    if (period >= 1) {
        sb.append("Time_slice(time_stamp, " + period);
        sb.append(", 'SECOND', 'END') AS time_interval");
    }
    sb.append(" FROM MonMetrics.Measurements ");
    String inClause = createInClause(defDimIdSet);
    sb.append("WHERE to_hex(definition_dimensions_id) " + inClause);
    sb.append(createWhereClause(startTime, endTime, offset));
    if (period >= 1) {
        sb.append("group by Time_slice(time_stamp, " + period);
        sb.append(", 'SECOND', 'END') order by time_interval");
    }
    sb.append(" limit :limit");
    return sb.toString();
}
#end_block

#method_before
public void run() {
    logger.info("[{}]: run", this.threadId);
    final ConsumerIterator<byte[], byte[]> it = kafkaChannel.getKafkaStream().iterator();
    logger.debug("[{}]: KafkaChannel has stream iterator", this.threadId);
    while (!this.stop) {
        try {
            try {
                if (isInterrupted()) {
                    this.fatalErrorDetected = true;
                    break;
                }
                if (it.hasNext()) {
                    if (isInterrupted()) {
                        this.fatalErrorDetected = true;
                        break;
                    }
                    final String msg = new String(it.next().message());
                    logger.debug("[{}]: {}", this.threadId, msg);
                    publishEvent(msg);
                }
            } catch (kafka.consumer.ConsumerTimeoutException cte) {
                if (isInterrupted()) {
                    this.fatalErrorDetected = true;
                    break;
                }
                publishHeartbeat();
            }
        } catch (Exception e) {
            logger.error("[{}]: caught fatal exception while publishing msg. Shutting entire persister down now!", this.threadId);
            this.stop = true;
            this.fatalErrorDetected = true;
            this.executorService.shutdownNow();
            try {
                this.executorService.awaitTermination(5, TimeUnit.SECONDS);
            } catch (InterruptedException e1) {
                logger.error("[{}]: caught exception while awaiting termination", this.threadId, e1);
            }
            LogManager.shutdown();
            System.exit(-1);
        }
    }
    logger.info("[{}]: shutting down", this.threadId);
    this.kafkaChannel.stop();
}
#method_after
public void run() {
    logger.info("[{}]: run", this.threadId);
    final ConsumerIterator<byte[], byte[]> it = kafkaChannel.getKafkaStream().iterator();
    logger.debug("[{}]: KafkaChannel has stream iterator", this.threadId);
    while (!this.stop) {
        try {
            try {
                if (isInterrupted()) {
                    this.fatalErrorDetected = true;
                    break;
                }
                if (it.hasNext()) {
                    if (isInterrupted()) {
                        this.fatalErrorDetected = true;
                        break;
                    }
                    final String msg = new String(it.next().message());
                    logger.debug("[{}]: {}", this.threadId, msg);
                    publishEvent(msg);
                }
            } catch (kafka.consumer.ConsumerTimeoutException cte) {
                if (isInterrupted()) {
                    this.fatalErrorDetected = true;
                    break;
                }
                publishHeartbeat();
            }
        } catch (Throwable e) {
            logger.error("[{}]: caught fatal exception while publishing msg. Shutting entire persister down now!", this.threadId, e);
            this.stop = true;
            this.fatalErrorDetected = true;
            this.executorService.shutdownNow();
            try {
                this.executorService.awaitTermination(5, TimeUnit.SECONDS);
            } catch (InterruptedException e1) {
                logger.info("[{}]:  interrupted while awaiting termination", this.threadId, e1);
            }
            LogManager.shutdown();
            System.exit(1);
        }
    }
    logger.info("[{}]: shutting down", this.threadId);
    this.kafkaChannel.stop();
}
#end_block

#method_before
private InfluxPoint[] getInfluxPointArry(String id) {
    List<InfluxPoint> influxPointList = new LinkedList<>();
    for (AlarmStateTransitionedEvent event : this.alarmStateTransitionedEventList) {
        Map<String, Object> valueMap = new HashMap<>();
        if (event.tenantId == null) {
            logger.error("[{}]: tenant id cannot be null. Dropping alarm state history event.", id);
            continue;
        } else {
            valueMap.put("tenant_id", event.tenantId);
        }
        if (event.alarmId == null) {
            logger.error("[{}]: alarm id cannot be null. Dropping alarm state history event.", id);
            continue;
        } else {
            valueMap.put("alarm_id", event.alarmId);
        }
        if (event.metrics == null) {
            logger.error("[{}]: metrics cannot be null. Settings metrics to empty JSON", id);
            valueMap.put("metrics", "{}");
        } else {
            try {
                valueMap.put("metrics", this.objectMapper.writeValueAsString(event.metrics));
            } catch (JsonProcessingException e) {
                logger.error("[{}]: failed to serialize metrics {}", id, event.metrics, e);
                logger.error("[{}]: setting metrics to empty JSON", id);
                valueMap.put("metrics", "{}");
            }
        }
        if (event.oldState == null) {
            logger.error("[{}]: old state cannot be null. Setting old state to empty string.", id);
            valueMap.put("old_state", "");
        } else {
            valueMap.put("old_state", event.oldState);
        }
        if (event.newState == null) {
            logger.error("[{}]: new state cannot be null. Setting new state to empty string.", id);
            valueMap.put("new_state", "");
        } else {
            valueMap.put("new_state", event.newState);
        }
        if (event.subAlarms == null) {
            logger.error("[{}]: sub alarms cannot be null. Setting sub alarms to empty JSON", id);
            valueMap.put("sub_alarms", "{}");
        } else {
            try {
                valueMap.put("sub_alarms", this.objectMapper.writeValueAsString(event.subAlarms));
            } catch (JsonProcessingException e) {
                logger.error("[{}]: failed to serialize sub alarms {}", id, event.subAlarms, e);
                logger.error("[{}]: Setting sub_alarms to empty JSON", id);
                valueMap.put("sub_alarms", "{}");
            }
        }
        if (event.stateChangeReason == null) {
            logger.error("[{}]: reason cannot be null. Setting reason to empty string.", id);
            valueMap.put("reason", "");
        } else {
            valueMap.put("reason", event.stateChangeReason);
        }
        valueMap.put("reason_data", "{}");
        DateTime dateTime = new DateTime(event.timestamp, DateTimeZone.UTC);
        String dateString = this.dateFormatter.print(dateTime);
        Map<String, String> tags = new HashMap<>();
        tags.put("tenant_id", event.tenantId);
        tags.put("alarm_id", event.alarmId);
        InfluxPoint influxPoint = new InfluxPoint(ALARM_STATE_HISTORY_NAME, tags, dateString, valueMap);
        influxPointList.add(influxPoint);
    }
    return influxPointList.toArray(new InfluxPoint[influxPointList.size()]);
}
#method_after
private InfluxPoint[] getInfluxPointArry(String id) {
    List<InfluxPoint> influxPointList = new LinkedList<>();
    for (AlarmStateTransitionedEvent event : this.alarmStateTransitionedEventList) {
        Map<String, Object> valueMap = new HashMap<>();
        if (event.tenantId == null) {
            logger.error("[{}]: tenant id cannot be null. Dropping alarm state history event.", id);
            continue;
        } else {
            valueMap.put("tenant_id", event.tenantId);
        }
        if (event.alarmId == null) {
            logger.error("[{}]: alarm id cannot be null. Dropping alarm state history event.", id);
            continue;
        } else {
            valueMap.put("alarm_id", event.alarmId);
        }
        if (event.metrics == null) {
            logger.error("[{}]: metrics cannot be null. Settings metrics to empty JSON", id);
            valueMap.put("metrics", "{}");
        } else {
            try {
                valueMap.put("metrics", this.objectMapper.writeValueAsString(event.metrics));
            } catch (JsonProcessingException e) {
                logger.error("[{}]: failed to serialize metrics {}", id, event.metrics, e);
                logger.error("[{}]: setting metrics to empty JSON", id);
                valueMap.put("metrics", "{}");
            }
        }
        if (event.oldState == null) {
            logger.error("[{}]: old state cannot be null. Setting old state to empty string.", id);
            valueMap.put("old_state", "");
        } else {
            valueMap.put("old_state", event.oldState);
        }
        if (event.newState == null) {
            logger.error("[{}]: new state cannot be null. Setting new state to empty string.", id);
            valueMap.put("new_state", "");
        } else {
            valueMap.put("new_state", event.newState);
        }
        if (event.subAlarms == null) {
            logger.debug("[{}]: sub alarms is null. Setting sub alarms to empty JSON", id);
            valueMap.put("sub_alarms", "[]");
        } else {
            try {
                valueMap.put("sub_alarms", this.objectMapper.writeValueAsString(event.subAlarms));
            } catch (JsonProcessingException e) {
                logger.error("[{}]: failed to serialize sub alarms {}", id, event.subAlarms, e);
                logger.error("[{}]: Setting sub_alarms to empty JSON", id);
                valueMap.put("sub_alarms", "[]");
            }
        }
        if (event.stateChangeReason == null) {
            logger.error("[{}]: reason cannot be null. Setting reason to empty string.", id);
            valueMap.put("reason", "");
        } else {
            valueMap.put("reason", event.stateChangeReason);
        }
        valueMap.put("reason_data", "{}");
        DateTime dateTime = new DateTime(event.timestamp, DateTimeZone.UTC);
        String dateString = this.dateFormatter.print(dateTime);
        Map<String, String> tags = new HashMap<>();
        tags.put("tenant_id", event.tenantId);
        tags.put("alarm_id", event.alarmId);
        InfluxPoint influxPoint = new InfluxPoint(ALARM_STATE_HISTORY_NAME, tags, dateString, valueMap);
        influxPointList.add(influxPoint);
    }
    return influxPointList.toArray(new InfluxPoint[influxPointList.size()]);
}
#end_block

#method_before
private Map<String, String> dims(String[] vals, String[] cols) {
    Map<String, String> dims = new HashMap<>();
    for (int i = 0; i < cols.length; ++i) {
        // Dimension names that start with underscore are reserved. (_key, _id, _region, _tenant_id)
        if (!cols[i].startsWith("_")) {
            if (!vals[i].equalsIgnoreCase("null")) {
                dims.put(cols[i], vals[i]);
            }
        }
    }
    return dims;
}
#method_after
private Map<String, String> dims(String[] vals, String[] cols) {
    Map<String, String> dims = new HashMap<>();
    for (int i = 0; i < cols.length; ++i) {
        if (!cols[i].startsWith("_")) {
            if (!vals[i].equalsIgnoreCase("null")) {
                dims.put(cols[i], vals[i]);
            }
        }
    }
    return dims;
}
#end_block

#method_before
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, String lifecycleState, String link, DateTime stateUpdatedStart, String offset, int limit, boolean enforceLimit) {
    StringBuilder sbWhere = new StringBuilder("(select a.id " + "from alarm as a, alarm_definition as ad " + "where ad.id = a.alarm_definition_id " + "and ad.deleted_at is null " + "and ad.tenant_id = :tenantId ");
    if (alarmDefId != null) {
        sbWhere.append(" and ad.id = :alarmDefId ");
    }
    if (metricName != null) {
        sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id " + "inner join (select distinct id from metric_definition " + "            where name = :metricName) as md " + "on md.id = mdd.metric_definition_id ");
        buildJoinClauseFor(metricDimensions, sbWhere);
        sbWhere.append(")");
    } else if (metricDimensions != null) {
        sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id ");
        buildJoinClauseFor(metricDimensions, sbWhere);
        sbWhere.append(")");
    }
    if (state != null) {
        sbWhere.append(" and a.state = :state");
    }
    if (lifecycleState != null) {
        sbWhere.append(" and a.lifecycle_state = :lifecycleState");
    }
    if (link != null) {
        sbWhere.append(" and a.link = :link");
    }
    if (stateUpdatedStart != null) {
        sbWhere.append(" and a.state_updated_at >= :stateUpdatedStart");
    }
    if (offset != null) {
        sbWhere.append(" and a.id > :offset");
    }
    sbWhere.append(" order by a.id ASC ");
    if (enforceLimit && limit > 0) {
        sbWhere.append(" limit :limit");
    }
    sbWhere.append(")");
    String sql = String.format(FIND_ALARMS_SQL, sbWhere);
    try (Handle h = db.open()) {
        final Query<Map<String, Object>> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        if (lifecycleState != null) {
            q.bind("lifecycleState", lifecycleState);
        }
        if (link != null) {
            q.bind("link", link);
        }
        if (stateUpdatedStart != null) {
            q.bind("stateUpdatedStart", stateUpdatedStart.toString().replace('Z', ' '));
        }
        if (offset != null) {
            q.bind("offset", offset);
        }
        if (enforceLimit && limit > 0) {
            q.bind("limit", limit + 1);
        }
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        final List<Map<String, Object>> rows = q.list();
        final List<Alarm> alarms = createAlarms(tenantId, rows);
        return alarms;
    }
}
#method_after
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, String lifecycleState, String link, DateTime stateUpdatedStart, String offset, int limit, boolean enforceLimit) {
    StringBuilder sbWhere = new StringBuilder("(select a.id " + "from alarm as a, alarm_definition as ad " + "where ad.id = a.alarm_definition_id " + "  and ad.deleted_at is null " + "  and ad.tenant_id = :tenantId ");
    if (alarmDefId != null) {
        sbWhere.append(" and ad.id = :alarmDefId ");
    }
    if (metricName != null) {
        sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id " + "inner join (select distinct id from metric_definition " + "            where name = :metricName) as md " + "  on md.id = mdd.metric_definition_id ");
        buildJoinClauseFor(metricDimensions, sbWhere);
        sbWhere.append(")");
    } else if (metricDimensions != null) {
        sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id ");
        buildJoinClauseFor(metricDimensions, sbWhere);
        sbWhere.append(")");
    }
    if (state != null) {
        sbWhere.append(" and a.state = :state");
    }
    if (lifecycleState != null) {
        sbWhere.append(" and a.lifecycle_state = :lifecycleState");
    }
    if (link != null) {
        sbWhere.append(" and a.link = :link");
    }
    if (stateUpdatedStart != null) {
        sbWhere.append(" and a.state_updated_at >= :stateUpdatedStart");
    }
    if (offset != null) {
        sbWhere.append(" and a.id > :offset");
    }
    sbWhere.append(" order by a.id ASC ");
    if (enforceLimit && limit > 0) {
        sbWhere.append(" limit :limit");
    }
    sbWhere.append(")");
    String sql = String.format(FIND_ALARMS_SQL, sbWhere);
    try (Handle h = db.open()) {
        final Query<Map<String, Object>> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        if (lifecycleState != null) {
            q.bind("lifecycleState", lifecycleState);
        }
        if (link != null) {
            q.bind("link", link);
        }
        if (stateUpdatedStart != null) {
            q.bind("stateUpdatedStart", stateUpdatedStart.toString().replace('Z', ' '));
        }
        if (offset != null) {
            q.bind("offset", offset);
        }
        if (enforceLimit && limit > 0) {
            q.bind("limit", limit + 1);
        }
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        final List<Map<String, Object>> rows = q.list();
        final List<Alarm> alarms = createAlarms(tenantId, rows);
        return alarms;
    }
}
#end_block

#method_before
@Override
public int hashCode() {
    int hashCode = 31;
    hashCode += this.alarmDefinitionId.hashCode();
    hashCode += dimensions.hashCode();
    return hashCode;
}
#method_after
@Override
public int hashCode() {
    final int prime = 31;
    int result = 1;
    result = prime * result + alarmDefinitionId.hashCode();
    result = prime * result + dimensions.hashCode();
    return result;
}
#end_block

#method_before
@PATCH
@Timed
@Path("/{alarm_id}")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
public Alarm patch(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_id") String alarmId, @NotEmpty Map<String, String> fields) throws JsonMappingException {
    String stateStr = fields.get("state");
    String lifecycleState = fields.get("lifecycle_state");
    String link = fields.get("link");
    AlarmState state = stateStr == null ? null : Validation.parseAndValidate(AlarmState.class, stateStr);
    Validation.validateLifecycleStateAndLink(lifecycleState, link);
    return fixAlarmLinks(uriInfo, service.patch(tenantId, alarmId, state, lifecycleState, link));
}
#method_after
@PATCH
@Timed
@Path("/{alarm_id}")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
public Alarm patch(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_id") String alarmId, @NotEmpty Map<String, String> fields) throws JsonMappingException {
    String stateStr = fields.get("state");
    String lifecycleState = fields.get("lifecycle_state");
    String link = fields.get("link");
    AlarmState state = stateStr == null ? null : Validation.parseAndValidate(AlarmState.class, stateStr);
    Validation.validateLifecycleState(lifecycleState);
    Validation.validateLink(link);
    return fixAlarmLinks(uriInfo, service.patch(tenantId, alarmId, state, lifecycleState, link));
}
#end_block

#method_before
@PUT
@Timed
@Path("/{alarm_id}")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
public Alarm update(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_id") String alarmId, @Valid UpdateAlarmCommand command) {
    return fixAlarmLinks(uriInfo, service.update(tenantId, alarmId, command));
}
#method_after
@PUT
@Timed
@Path("/{alarm_id}")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
public Alarm update(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_id") String alarmId, @Valid UpdateAlarmCommand command) {
    Validation.validateLifecycleState(command.lifecycleState);
    Validation.validateLink(command.link);
    return fixAlarmLinks(uriInfo, service.update(tenantId, alarmId, command));
}
#end_block

#method_before
public Alarm patch(String tenantId, String alarmId, AlarmState state, String lifecycleState, String link) {
    Alarm oldAlarm = repo.findById(tenantId, alarmId);
    state = (state == null) ? oldAlarm.getState() : state;
    lifecycleState = (lifecycleState == null) ? oldAlarm.getLifecycleState() : lifecycleState;
    link = (link == null) ? oldAlarm.getLink() : link;
    if (oldAlarm.getState().equals(state) && oldAlarm.getLifecycleState().equals(lifecycleState) && oldAlarm.getLink().equals(link)) {
        return oldAlarm;
    }
    Alarm alarm = updateInternal(tenantId, alarmId, state, lifecycleState, link);
    return alarm;
}
#method_after
public Alarm patch(String tenantId, String alarmId, AlarmState state, String lifecycleState, String link) {
    Alarm oldAlarm = repo.findById(tenantId, alarmId);
    if (state == null && lifecycleState == null && link == null) {
        return oldAlarm;
    }
    state = (state == null) ? oldAlarm.getState() : state;
    lifecycleState = (lifecycleState == null) ? oldAlarm.getLifecycleState() : lifecycleState;
    link = (link == null) ? oldAlarm.getLink() : link;
    Alarm alarm = updateInternal(tenantId, alarmId, state, lifecycleState, link);
    return alarm;
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (!super.equals(obj))
        return false;
    if (getClass() != obj.getClass())
        return false;
    Alarm other = (Alarm) obj;
    if (alarmDefinition == null) {
        if (other.alarmDefinition != null)
            return false;
    } else if (!alarmDefinition.equals(other.alarmDefinition))
        return false;
    if (links == null) {
        if (other.links != null)
            return false;
    } else if (!links.equals(other.links))
        return false;
    if (metrics == null) {
        if (other.metrics != null)
            return false;
    } else if (!metrics.equals(other.metrics))
        return false;
    if (state != other.state)
        return false;
    if (lifecycleState == null) {
        if (other.lifecycleState != null)
            return false;
    } else if (!lifecycleState.equals(other.lifecycleState))
        return false;
    if (link == null) {
        if (other.link != null)
            return false;
    } else if (!link.equals(other.link))
        return false;
    // Ignore timezones, only check milliseconds since epoch
    if (stateUpdatedTimestamp.getMillis() != other.stateUpdatedTimestamp.getMillis()) {
        return false;
    }
    if (updatedTimestamp.getMillis() != other.updatedTimestamp.getMillis()) {
        return false;
    }
    if (createdTimestamp.getMillis() != other.createdTimestamp.getMillis()) {
        return false;
    }
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (!super.equals(obj))
        return false;
    if (getClass() != obj.getClass())
        return false;
    Alarm other = (Alarm) obj;
    if (alarmDefinition == null) {
        if (other.alarmDefinition != null)
            return false;
    } else if (!alarmDefinition.equals(other.alarmDefinition))
        return false;
    if (links == null) {
        if (other.links != null)
            return false;
    } else if (!links.equals(other.links))
        return false;
    if (metrics == null) {
        if (other.metrics != null)
            return false;
    } else if (!metrics.equals(other.metrics))
        return false;
    if (state != other.state)
        return false;
    if (lifecycleState == null) {
        if (other.lifecycleState != null)
            return false;
    } else if (!lifecycleState.equals(other.lifecycleState))
        return false;
    if (link == null) {
        if (other.link != null)
            return false;
    } else if (!link.equals(other.link))
        return false;
    // Ignore timezones, only check milliseconds since epoch
    if (stateUpdatedTimestamp != other.stateUpdatedTimestamp) {
        if (stateUpdatedTimestamp == null || other.stateUpdatedTimestamp == null) {
            return false;
        } else if (stateUpdatedTimestamp.getMillis() != other.stateUpdatedTimestamp.getMillis()) {
            return false;
        }
    }
    if (updatedTimestamp != other.updatedTimestamp) {
        if (updatedTimestamp == null || other.updatedTimestamp == null) {
            return false;
        } else if (updatedTimestamp.getMillis() != other.updatedTimestamp.getMillis()) {
            return false;
        }
    }
    if (createdTimestamp != other.createdTimestamp) {
        if (createdTimestamp == null || other.createdTimestamp == null) {
            return false;
        } else if (createdTimestamp.getMillis() != other.createdTimestamp.getMillis()) {
            return false;
        }
    }
    return true;
}
#end_block

#method_before
public static Map<String, String> parseAndValidateNameAndDimensions(String name, String dimensionsStr, boolean nameRequiredFlag) {
    Map<String, String> dimensions = parseAndValidateDimensions(dimensionsStr);
    String service = dimensions.get(Services.SERVICE_DIMENSION);
    MetricNameValidation.validate(name, service, nameRequiredFlag);
    return dimensions;
}
#method_after
public static Map<String, String> parseAndValidateNameAndDimensions(String name, String dimensionsStr, boolean nameRequiredFlag) {
    Map<String, String> dimensions = parseAndValidateDimensions(dimensionsStr);
    MetricNameValidation.validate(name, nameRequiredFlag);
    return dimensions;
}
#end_block

#method_before
public static Map<String, String> parseAndValidateDimensions(String dimensionsStr) {
    Validation.validateNotNullOrEmpty(dimensionsStr, "dimensions");
    Map<String, String> dimensions = new HashMap<String, String>();
    for (String dimensionStr : COMMA_SPLITTER.split(dimensionsStr)) {
        String[] dimensionArr = Iterables.toArray(COLON_SPLITTER.split(dimensionStr), String.class);
        if (dimensionArr.length == 2)
            dimensions.put(dimensionArr[0], dimensionArr[1]);
    }
    String service = dimensions.get(Services.SERVICE_DIMENSION);
    DimensionValidation.validate(dimensions, service);
    return dimensions;
}
#method_after
public static Map<String, String> parseAndValidateDimensions(String dimensionsStr) {
    Validation.validateNotNullOrEmpty(dimensionsStr, "dimensions");
    Map<String, String> dimensions = new HashMap<String, String>();
    for (String dimensionStr : COMMA_SPLITTER.split(dimensionsStr)) {
        String[] dimensionArr = Iterables.toArray(COLON_SPLITTER.split(dimensionStr), String.class);
        if (dimensionArr.length == 2)
            dimensions.put(dimensionArr[0], dimensionArr[1]);
    }
    DimensionValidation.validate(dimensions);
    return dimensions;
}
#end_block

#method_before
@PATCH
@Timed
@Path("/{alarm_definition_id}")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
@SuppressWarnings("unchecked")
public AlarmDefinition patch(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_definition_id") String alarmDefinitionId, @Valid PatchAlarmCommand command) throws JsonMappingException {
    command.validate();
    AlarmExpression alarmExpression = command.expression == null ? null : AlarmValidation.validateNormalizeAndGet(command.expression);
    return Links.hydrate(service.patch(tenantId, alarmDefinitionId, command.name, command.description, command.severity, command.expression, alarmExpression, command.matchBy, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions), uriInfo, true);
}
#method_after
@PATCH
@Timed
@Path("/{alarm_definition_id}")
@Consumes(MediaType.APPLICATION_JSON)
@Produces(MediaType.APPLICATION_JSON)
public AlarmDefinition patch(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @PathParam("alarm_definition_id") String alarmDefinitionId, @Valid PatchAlarmDefinitionCommand command) throws JsonMappingException {
    command.validate();
    AlarmExpression alarmExpression = command.expression == null ? null : AlarmValidation.validateNormalizeAndGet(command.expression);
    return Links.hydrate(service.patch(tenantId, alarmDefinitionId, command.name, command.description, command.severity, command.expression, alarmExpression, command.matchBy, command.actionsEnabled, command.alarmActions, command.okActions, command.undeterminedActions), uriInfo, true);
}
#end_block

#method_before
@Override
public int flush(String id) throws RepoException {
    try {
        long startTime = System.currentTimeMillis();
        Timer.Context context = commitTimer.time();
        executeBatches(id);
        writeRowsFromTempStagingTablesToPermTables(id);
        Stopwatch sw = Stopwatch.createStarted();
        handle.commit();
        handle.begin();
        sw.stop();
        logger.debug("[{}]: committing and beginning new transaction took: {}", id, sw.toString());
        long endTime = System.currentTimeMillis();
        context.stop();
        logger.debug("[{}]: total time for writing measurements, definitions, and dimensions to vertica took {} ms", id, endTime - startTime);
        updateIdCaches(id);
        int commitCnt = this.measurementCnt;
        this.measurementCnt = 0;
        return commitCnt;
    } catch (Exception e) {
        logger.error("[{}]: failed to write measurements, definitions, and dimensions to vertica", id, e);
        throw new RepoException("failed to commit batch to vertica", e);
    }
}
#method_after
@Override
public int flush(String id) throws RepoException {
    try {
        Stopwatch swOuter = Stopwatch.createStarted();
        Timer.Context context = commitTimer.time();
        executeBatches(id);
        writeRowsFromTempStagingTablesToPermTables(id);
        Stopwatch swInner = Stopwatch.createStarted();
        handle.commit();
        swInner.stop();
        logger.debug("[{}]: committing transaction took: {}", id, swInner);
        swInner.reset().start();
        handle.begin();
        swInner.stop();
        logger.debug("[{}]: beginning new transaction took: {}", id, swInner);
        context.stop();
        swOuter.stop();
        logger.debug("[{}]: total time for writing measurements, definitions, and dimensions to vertica took {}", id, swOuter);
        updateIdCaches(id);
        int commitCnt = this.measurementCnt;
        this.measurementCnt = 0;
        return commitCnt;
    } catch (Exception e) {
        logger.error("[{}]: failed to write measurements, definitions, and dimensions to vertica", id, e);
        throw new RepoException("failed to commit batch to vertica", e);
    }
}
#end_block

#method_before
private void executeBatches(String id) {
    Stopwatch sw = Stopwatch.createStarted();
    metricsBatch.execute();
    stagedDefinitionsBatch.execute();
    stagedDimensionsBatch.execute();
    stagedDefinitionDimensionsBatch.execute();
    sw.stop();
    logger.debug("[{}]: executing batches took {}: ", id, sw.toString());
}
#method_after
private void executeBatches(String id) {
    Stopwatch sw = Stopwatch.createStarted();
    metricsBatch.execute();
    stagedDefinitionsBatch.execute();
    stagedDimensionsBatch.execute();
    stagedDefinitionDimensionsBatch.execute();
    sw.stop();
    logger.debug("[{}]: executing batches took {}: ", id, sw);
}
#end_block

#method_before
private void updateIdCaches(String id) {
    Stopwatch sw = Stopwatch.createStarted();
    for (Sha1HashId defId : definitionIdSet) {
        definitionsIdCache.put(defId, defId);
    }
    for (Sha1HashId dimId : dimensionIdSet) {
        dimensionsIdCache.put(dimId, dimId);
    }
    for (Sha1HashId defDimsId : definitionDimensionsIdSet) {
        definitionDimensionsIdCache.put(defDimsId, defDimsId);
    }
    clearTempCaches();
    sw.stop();
    logger.debug("[{}]: clearing temp caches took: {}", id, sw.toString());
}
#method_after
private void updateIdCaches(String id) {
    Stopwatch sw = Stopwatch.createStarted();
    for (Sha1HashId defId : definitionIdSet) {
        definitionsIdCache.put(defId, defId);
    }
    for (Sha1HashId dimId : dimensionIdSet) {
        dimensionsIdCache.put(dimId, dimId);
    }
    for (Sha1HashId defDimsId : definitionDimensionsIdSet) {
        definitionDimensionsIdCache.put(defDimsId, defDimsId);
    }
    clearTempCaches();
    sw.stop();
    logger.debug("[{}]: clearing temp caches took: {}", id, sw);
}
#end_block

#method_before
private void writeRowsFromTempStagingTablesToPermTables(String id) {
    Stopwatch sw = Stopwatch.createStarted();
    handle.execute(definitionsTempStagingTableInsertStmt);
    handle.execute("truncate table " + definitionsTempStagingTableName);
    sw.stop();
    logger.debug("[{}]: flushing definitions temp staging table took: {}", id, sw.toString());
    sw.reset().start();
    handle.execute(dimensionsTempStagingTableInsertStmt);
    handle.execute("truncate table " + dimensionsTempStagingTableName);
    sw.stop();
    logger.debug("[{}]: flushing dimensions temp staging table took: {}", id, sw.toString());
    sw.reset().start();
    handle.execute(definitionDimensionsTempStagingTableInsertStmt);
    handle.execute("truncate table " + definitionDimensionsTempStagingTableName);
    sw.stop();
    logger.debug("[{}]: flushing definition dimensions temp staging table took: {}", id, sw.toString());
}
#method_after
private void writeRowsFromTempStagingTablesToPermTables(String id) {
    Stopwatch sw = Stopwatch.createStarted();
    handle.execute(definitionsTempStagingTableInsertStmt);
    handle.execute("truncate table " + definitionsTempStagingTableName);
    sw.stop();
    logger.debug("[{}]: flushing definitions temp staging table took: {}", id, sw);
    sw.reset().start();
    handle.execute(dimensionsTempStagingTableInsertStmt);
    handle.execute("truncate table " + dimensionsTempStagingTableName);
    sw.stop();
    logger.debug("[{}]: flushing dimensions temp staging table took: {}", id, sw);
    sw.reset().start();
    handle.execute(definitionDimensionsTempStagingTableInsertStmt);
    handle.execute("truncate table " + definitionDimensionsTempStagingTableName);
    sw.stop();
    logger.debug("[{}]: flushing definition dimensions temp staging table took: {}", id, sw);
}
#end_block

#method_before
public void reconnect() {
    LOG.info("---- Worker " + this + " starting reconnect for " + session.toString());
    // In case we held the availability lock earlier, release it.
    availability.unlock(this);
    try {
        session.initSession(ioAvailable, this);
        if (id != null) {
            sendToAll(new GearmanPacketImpl(GearmanPacketMagic.REQ, GearmanPacketType.SET_CLIENT_ID, ByteUtils.toUTF8Bytes(id)));
        }
        // this will cause a grab-job event
        functionRegistry.setUpdated(true);
    } catch (IOException e) {
        try {
            Thread.sleep(2000);
        } catch (InterruptedException e1) {
            LOG.warn("---- Worker " + this + " interrupted while reconnecting", e);
            return;
        }
    }
    LOG.info("---- Worker " + this + " ending reconnect for " + session.toString());
}
#method_after
public void reconnect() {
    LOG.info("---- Worker " + this + " starting reconnect for " + session.toString());
    // In case we held the availability lock earlier, release it.
    availability.unlock(this);
    try {
        session.initSession(ioAvailable, this);
        if (id != null) {
            sendToAll(new GearmanPacketImpl(GearmanPacketMagic.REQ, GearmanPacketType.SET_CLIENT_ID, ByteUtils.toUTF8Bytes(id)));
        }
        // Reset events so that we don't process events from the old
        // connection.
        eventList = new ConcurrentLinkedQueue<GearmanSessionEvent>();
        // this will cause a grab-job event
        functionRegistry.setUpdated(true);
    } catch (IOException e) {
        try {
            Thread.sleep(2000);
        } catch (InterruptedException e1) {
            LOG.warn("---- Worker " + this + " interrupted while reconnecting", e);
            return;
        }
    }
    LOG.info("---- Worker " + this + " ending reconnect for " + session.toString());
}
#end_block

#method_before
private void registerFunctions() throws IOException {
    Set<GearmanFunctionFactory> functions = functionRegistry.getFunctions();
    if (functions == null) {
        return;
    }
    functionMap.clear();
    sendToAll(new GearmanPacketImpl(GearmanPacketMagic.REQ, GearmanPacketType.RESET_ABILITIES, new byte[0]));
    session.driveSessionIO();
    if (!isRunning())
        return;
    for (GearmanFunctionFactory factory : functions) {
        FunctionDefinition def = new FunctionDefinition(0, factory);
        functionMap.put(factory.getFunctionName(), def);
        sendToAll(generateCanDoPacket(def));
        session.driveSessionIO();
        if (!isRunning())
            return;
        LOG.debug("---- Worker " + this + " registered function " + factory.getFunctionName());
    }
    enqueueNoopEvent();
}
#method_after
private void registerFunctions() throws IOException {
    Set<GearmanFunctionFactory> functions = functionRegistry.getFunctions();
    if (functions == null) {
        return;
    }
    functionMap.clear();
    sendToAll(new GearmanPacketImpl(GearmanPacketMagic.REQ, GearmanPacketType.RESET_ABILITIES, new byte[0]));
    session.driveSessionIO();
    if (!isRunning())
        return;
    for (GearmanFunctionFactory factory : functions) {
        FunctionDefinition def = new FunctionDefinition(0, factory);
        functionMap.put(factory.getFunctionName(), def);
        sendToAll(generateCanDoPacket(def));
        session.driveSessionIO();
        if (!isRunning())
            return;
        LOG.debug("---- Worker " + this + " registered function " + factory.getFunctionName());
    }
    GearmanSessionEvent nextEvent = eventList.peek();
    if (nextEvent == null || nextEvent.getPacket().getPacketType() != GearmanPacketType.NOOP) {
        // Simulate a NOOP packet which will kick off a GRAB_JOB cycle
        // if we're sleeping.  If we get a real NOOP in the mean time,
        // it should be fine because GearmanJobServerSession ignores a
        // NOOP if PRE_SLEEP is not on the stack.
        GearmanPacket p = new GearmanPacketImpl(GearmanPacketMagic.RES, GearmanPacketType.NOOP, new byte[0]);
        GearmanSessionEvent event = new GearmanSessionEvent(p, session);
        session.handleSessionEvent(event);
    }
}
#end_block

#method_before
public void enqueueNoopEvent() {
    // Simulate a NOOP packet which will kick off a GRAB_JOB cycle
    // If we get a real NOOP in the mean time, it should be fine
    // because GearmanJobServerSession ignores a NOOP if PRE_SLEEP
    // is not on the stack.
    GearmanPacket p = new GearmanPacketImpl(GearmanPacketMagic.RES, GearmanPacketType.NOOP, new byte[0]);
    GearmanSessionEvent event = new GearmanSessionEvent(p, session);
    enqueueEvent(event);
}
#method_after
public void enqueueNoopEvent() {
    // Simulate a NOOP packet which will kick off a GRAB_JOB cycle.
    // This unconditionally enqueues the NOOP which will send a GRAB_JOB
    // and should only be used when you know you need to send a GRAB_JOB.
    // Cases like worker start, post function run, post failure.
    GearmanPacket p = new GearmanPacketImpl(GearmanPacketMagic.RES, GearmanPacketType.NOOP, new byte[0]);
    GearmanSessionEvent event = new GearmanSessionEvent(p, session);
    enqueueEvent(event);
}
#end_block

#method_before
public void work() {
    GearmanSessionEvent event = null;
    GearmanFunction function = null;
    LOG.info("---- Worker " + this + " starting work");
    if (!state.equals(State.IDLE)) {
        throw new IllegalStateException("Can not call work while worker " + "is running or shutting down");
    }
    state = State.RUNNING;
    while (isRunning()) {
        LOG.debug("---- Worker " + this + " top of run loop");
        if (!session.isInitialized()) {
            LOG.debug("---- Worker " + this + " run loop reconnect");
            reconnect();
            enqueueNoopEvent();
            // Restart loop to check we connected.
            continue;
        }
        try {
            LOG.debug("---- Worker " + this + " run loop register functions");
            registerFunctions();
        } catch (IOException io) {
            LOG.warn("---- Worker " + this + " receieved IOException while" + " registering functions", io);
            session.closeSession();
            continue;
        }
        if (!isRunning() || !session.isInitialized())
            continue;
        event = eventList.poll();
        function = processSessionEvent(event);
        if (!isRunning() || !session.isInitialized())
            continue;
        // in the future, I expect to change this.
        if (function != null) {
            LOG.info("---- Worker " + this + " executing function");
            submitFunction(function);
            // Send another grab_job on the next loop
            enqueueNoopEvent();
        }
        if (!isRunning() || !session.isInitialized())
            continue;
        // Run IO, select waiting for ability to read and/or write
        // then read and/or write.
        int interestOps = SelectionKey.OP_READ;
        if (session.sessionHasDataToWrite()) {
            interestOps |= SelectionKey.OP_WRITE;
        }
        session.getSelectionKey().interestOps(interestOps);
        try {
            ioAvailable.select();
        } catch (IOException io) {
            LOG.warn("---- Worker " + this + " receieved IOException while" + " selecting for IO", io);
            session.closeSession();
            continue;
        }
        if (ioAvailable.selectedKeys().contains(session.getSelectionKey())) {
            LOG.debug("---- Worker " + this + " received input in run loop");
            if (!session.isInitialized()) {
                LOG.debug("---- Worker " + this + " session is no longer initialized");
                continue;
            }
            try {
                session.driveSessionIO();
            } catch (IOException io) {
                LOG.warn("---- Worker " + this + " received IOException while driving" + " IO on session " + session, io);
                session.closeSession();
                continue;
            }
        }
        LOG.debug("---- Worker " + this + " run loop finished driving session io");
    }
    shutDownWorker(true);
}
#method_after
public void work() {
    GearmanSessionEvent event = null;
    GearmanFunction function = null;
    LOG.info("---- Worker " + this + " starting work");
    if (!state.equals(State.IDLE)) {
        throw new IllegalStateException("Can not call work while worker " + "is running or shutting down");
    }
    state = State.RUNNING;
    // When we first start working we will already be initialized so must
    // enqueue a Noop event to trigger GRAB_JOB here.
    enqueueNoopEvent();
    while (isRunning()) {
        LOG.debug("---- Worker " + this + " top of run loop");
        if (!session.isInitialized()) {
            LOG.debug("---- Worker " + this + " run loop reconnect");
            reconnect();
            enqueueNoopEvent();
            // Restart loop to check we connected.
            continue;
        }
        try {
            LOG.debug("---- Worker " + this + " run loop register functions");
            registerFunctions();
        } catch (IOException io) {
            LOG.warn("---- Worker " + this + " receieved IOException while" + " registering functions", io);
            session.closeSession();
            continue;
        }
        if (!isRunning() || !session.isInitialized())
            continue;
        event = eventList.poll();
        function = processSessionEvent(event);
        if (!isRunning() || !session.isInitialized())
            continue;
        // in the future, I expect to change this.
        if (function != null) {
            LOG.info("---- Worker " + this + " executing function");
            submitFunction(function);
            // Send another grab_job on the next loop
            enqueueNoopEvent();
            // running.
            continue;
        }
        if (!isRunning() || !session.isInitialized())
            continue;
        // Run IO, select waiting for ability to read and/or write
        // then read and/or write.
        int interestOps = SelectionKey.OP_READ;
        if (session.sessionHasDataToWrite()) {
            interestOps |= SelectionKey.OP_WRITE;
        }
        session.getSelectionKey().interestOps(interestOps);
        try {
            ioAvailable.select();
        } catch (IOException io) {
            LOG.warn("---- Worker " + this + " receieved IOException while" + " selecting for IO", io);
            session.closeSession();
            continue;
        }
        if (ioAvailable.selectedKeys().contains(session.getSelectionKey())) {
            LOG.debug("---- Worker " + this + " received input in run loop");
            if (!session.isInitialized()) {
                LOG.debug("---- Worker " + this + " session is no longer initialized");
                continue;
            }
            try {
                session.driveSessionIO();
            } catch (IOException io) {
                LOG.warn("---- Worker " + this + " received IOException while driving" + " IO on session " + session, io);
                session.closeSession();
                continue;
            }
        }
        LOG.debug("---- Worker " + this + " run loop finished driving session io");
    }
    shutDownWorker(true);
}
#end_block

#method_before
public void metricQueriesBuildJoinClauseForTest1() {
    String expectedResult = " inner join MonMetrics.Dimensions dim0 on dim0.name = :dname0 and dim0" + ".value " + "= :dvalue0 and defdims.dimension_set_id = dim0.dimension_set_id inner join " + "MonMetrics.Dimensions dim1 on dim1.name = :dname1 and dim1.value = :dvalue1 and defdims" + ".dimension_set_id = dim1.dimension_set_id";
    Map<String, String> dimsMap = new HashMap<>();
    dimsMap.put("foo", "bar");
    dimsMap.put("biz", "baz");
    String s = MetricQueries.buildJoinClauseFor(dimsMap);
    assert (expectedResult.equals(s));
}
#method_after
public void metricQueriesBuildJoinClauseForTest1() {
    String expectedResult = " inner join MonMetrics.Dimensions dim0 on dim0.name = :dname0 and dim0" + ".value " + "= :dvalue0 and defdims.dimension_set_id = dim0.dimension_set_id inner join " + "MonMetrics.Dimensions dim1 on dim1.name = :dname1 and dim1.value = :dvalue1 and defdims" + ".dimension_set_id = dim1.dimension_set_id";
    Map<String, String> dimsMap = new HashMap<>();
    dimsMap.put("foo", "bar");
    dimsMap.put("biz", "baz");
    String s = MetricQueries.buildJoinClauseFor(dimsMap);
    assertEquals(expectedResult, s);
}
#end_block

#method_before
public void metricQueriesBuildJoinClauseForTest2() {
    String expectedResult = "";
    Map<String, String> dimsMap = new HashMap<>();
    assert (expectedResult.equals(MetricQueries.buildJoinClauseFor(dimsMap)));
}
#method_after
public void metricQueriesBuildJoinClauseForTest2() {
    String expectedResult = "";
    Map<String, String> dimsMap = new HashMap<>();
    assertEquals(expectedResult, MetricQueries.buildJoinClauseFor(dimsMap));
}
#end_block

#method_before
public void metricQueriesBuildJoinClauseForTest3() {
    String expectedResult = "";
    Map<String, String> dimsMap = null;
    assert (expectedResult.equals(MetricQueries.buildJoinClauseFor(dimsMap)));
}
#method_after
public void metricQueriesBuildJoinClauseForTest3() {
    String expectedResult = "";
    Map<String, String> dimsMap = null;
    assertEquals(expectedResult, MetricQueries.buildJoinClauseFor(dimsMap));
}
#end_block

#method_before
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, DateTime endTime, List<String> statisticsCols, int period, String offset, int limit, Boolean mergeMetricsFlag) throws MultipleMetricsException {
    List<Statistics> statisticsList = new ArrayList<>();
    List<String> statisticsColumns = createColumns(statisticsCols);
    try (Handle h = db.open()) {
        Map<byte[], Statistics> byteMap = findDefIds(h, tenantId, name, dimensions);
        if (byteMap.isEmpty()) {
            return statisticsList;
        }
        if (!Boolean.TRUE.equals(mergeMetricsFlag) && byteMap.keySet().size() > 1) {
            throw new MultipleMetricsException(name, dimensions);
        }
        List<List<Object>> statisticsListList = new ArrayList<>();
        for (byte[] bufferId : byteMap.keySet()) {
            Query<Map<String, Object>> query = h.createQuery(createQuery(period, startTime, endTime, offset, statisticsCols)).bind("definition_id", bufferId).bind("start_time", startTime).bind("end_time", endTime).bind("limit", limit + 1);
            if (offset != null && !offset.isEmpty()) {
                logger.debug("binding offset: {}", offset);
                query.bind("offset", new Timestamp(DateTime.parse(offset).getMillis()));
            }
            List<Map<String, Object>> rows = query.list();
            for (Map<String, Object> row : rows) {
                List<Object> statisticsRow = new ArrayList<>();
                Double sum = (Double) row.get("sum");
                Double average = (Double) row.get("avg");
                Double min = (Double) row.get("min");
                Double max = (Double) row.get("max");
                Long count = (Long) row.get("count");
                Timestamp time_stamp = (Timestamp) row.get("time_interval");
                if (time_stamp != null) {
                    statisticsRow.add(DATETIME_FORMATTER.print(time_stamp.getTime()));
                }
                if (average != null) {
                    statisticsRow.add(average);
                }
                if (count != null) {
                    statisticsRow.add(count);
                }
                if (max != null) {
                    statisticsRow.add(max);
                }
                if (min != null) {
                    statisticsRow.add(min);
                }
                if (sum != null) {
                    statisticsRow.add(sum);
                }
                statisticsListList.add(statisticsRow);
            }
        }
        if (Boolean.TRUE.equals(mergeMetricsFlag)) {
            Collections.sort(statisticsListList, new Comparator<List<Object>>() {

                @Override
                public int compare(List<Object> o1, List<Object> o2) {
                    String timestamp1 = (String) o1.get(0);
                    String timestamp2 = (String) o2.get(0);
                    return timestamp1.compareTo(timestamp2);
                }
            });
        }
        // Just use the first entry in the byteMap to get the def name and dimensions.
        Statistics statistics = byteMap.entrySet().iterator().next().getValue();
        statistics.setColumns(statisticsColumns);
        if (Boolean.TRUE.equals(mergeMetricsFlag)) {
            // Wipe out the dimensions.
            statistics.setDimensions(new HashMap<String, String>());
        }
        statistics.setStatistics(statisticsListList);
        statisticsList.add(statistics);
    }
    return statisticsList;
}
#method_after
@Override
public List<Statistics> find(String tenantId, String name, Map<String, String> dimensions, DateTime startTime, DateTime endTime, List<String> statisticsCols, int period, String offset, int limit, Boolean mergeMetricsFlag) throws MultipleMetricsException {
    List<Statistics> statisticsList = new ArrayList<>();
    // Sort the column names so that they match the order of the statistics in the results.
    List<String> statisticsColumns = createColumnsList(statisticsCols);
    try (Handle h = db.open()) {
        Map<byte[], Statistics> byteMap = findDefIds(h, tenantId, name, dimensions);
        if (byteMap.isEmpty()) {
            return statisticsList;
        }
        if (!Boolean.TRUE.equals(mergeMetricsFlag) && byteMap.keySet().size() > 1) {
            throw new MultipleMetricsException(name, dimensions);
        }
        List<List<Object>> statisticsListList = new ArrayList<>();
        String sql = createQuery(byteMap.keySet(), period, startTime, endTime, offset, statisticsCols);
        logger.debug("vertics sql: {}", sql);
        Query<Map<String, Object>> query = h.createQuery(sql).bind("start_time", startTime).bind("end_time", endTime).bind("limit", limit + 1);
        if (offset != null && !offset.isEmpty()) {
            logger.debug("binding offset: {}", offset);
            query.bind("offset", new Timestamp(DateTime.parse(offset).getMillis()));
        }
        List<Map<String, Object>> rows = query.list();
        for (Map<String, Object> row : rows) {
            List<Object> statisticsRow = parseRow(row);
            statisticsListList.add(statisticsRow);
        }
        // Just use the first entry in the byteMap to get the def name and dimensions.
        Statistics statistics = byteMap.entrySet().iterator().next().getValue();
        statistics.setColumns(statisticsColumns);
        if (Boolean.TRUE.equals(mergeMetricsFlag && byteMap.keySet().size() > 1)) {
            // Wipe out the dimensions.
            statistics.setDimensions(new HashMap<String, String>());
        }
        statistics.setStatistics(statisticsListList);
        statisticsList.add(statistics);
    }
    return statisticsList;
}
#end_block

#method_before
private String createQuery(int period, DateTime startTime, DateTime endTime, String offset, List<String> statistics) {
    StringBuilder sb = new StringBuilder();
    sb.append("SELECT " + getColumns(statistics));
    if (period >= 1) {
        sb.append(",MIN(time_stamp) as time_interval ");
        sb.append(" FROM (Select FLOOR((EXTRACT('epoch' from time_stamp) - ");
        sb.append(createOffset(period, startTime, endTime, offset));
        sb.append(" AS time_slice, time_stamp, value ");
    }
    sb.append(" FROM MonMetrics.Measurements ");
    sb.append("WHERE definition_dimensions_id = :definition_id ");
    sb.append(createWhereClause(startTime, endTime, offset));
    if (period >= 1) {
        sb.append(") as TimeSlices group by time_slice order by time_slice");
    }
    sb.append(" limit :limit");
    return sb.toString();
}
#method_after
private String createQuery(Set<byte[]> defDimIdSet, int period, DateTime startTime, DateTime endTime, String offset, List<String> statistics) {
    StringBuilder sb = new StringBuilder();
    sb.append("SELECT " + createColumnsStr(statistics));
    if (period >= 1) {
        sb.append("MIN(time_stamp) as time_interval ");
        sb.append("FROM (Select FLOOR((EXTRACT('epoch' from time_stamp) - ");
        sb.append(createOffsetStr(defDimIdSet, period, startTime, endTime, offset));
        sb.append(" AS time_slice, time_stamp, value ");
    }
    sb.append(" FROM MonMetrics.Measurements ");
    String inClause = createInClause(defDimIdSet);
    sb.append("WHERE to_hex(definition_dimensions_id) " + inClause);
    sb.append(createWhereClause(startTime, endTime, offset));
    if (period >= 1) {
        sb.append(") as TimeSlices group by time_slice order by time_slice");
    }
    sb.append(" limit :limit");
    return sb.toString();
}
#end_block

#method_before
private String createWhereClause(DateTime startTime, DateTime endTime, String offset) {
    String clause = "";
    if (startTime != null && endTime != null) {
        clause = "AND time_stamp >= :start_time AND time_stamp <= :end_time ";
    } else if (startTime != null) {
        clause = "AND time_stamp >= :start_time ";
    }
    if (offset != null && !offset.isEmpty()) {
        clause += " and time_stamp > :offset ";
    }
    return clause;
}
#method_after
private String createWhereClause(DateTime startTime, DateTime endTime, String offset) {
    String s = "";
    if (startTime != null && endTime != null) {
        s = "AND time_stamp >= :start_time AND time_stamp <= :end_time ";
    } else if (startTime != null) {
        s = "AND time_stamp >= :start_time ";
    }
    if (offset != null && !offset.isEmpty()) {
        s += " and time_stamp > :offset ";
    }
    return s;
}
#end_block

#method_before
public void start() {
    logger.info("[{}]: start", this.threadId);
    executorService = Executors.newFixedThreadPool(1);
    executorService.submit(kafkaConsumerRunnableBasic.setExecutorService(executorService));
}
#method_after
public void start() {
    logger.info("[{}]: start", this.threadId);
    ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(threadId + "-%d").setDaemon(true).build();
    executorService = Executors.newSingleThreadExecutor(threadFactory);
    executorService.submit(kafkaConsumerRunnableBasic.setExecutorService(executorService));
}
#end_block

#method_before
public void stop() {
    logger.info("[{}]: stop", this.threadId);
    this.stop = true;
    try {
        if (pipeline.shutdown()) {
            markRead();
        }
    } catch (Exception e) {
        logger.error("caught fatal exception while shutting down", e);
        this.executorService.shutdownNow();
    }
}
#method_after
public void stop() {
    logger.info("[{}]: stop", this.threadId);
    this.stop = true;
    try {
        if (pipeline.shutdown()) {
            markRead();
        }
    } catch (Exception e) {
        logger.error("caught fatal exception while shutting down", e);
    }
}
#end_block

#method_before
protected void publishEvent(final String msg) {
    try {
        if (pipeline.publishEvent(msg)) {
            markRead();
        }
    } catch (Exception e) {
        logger.error("caught fatal exception while publishing msg. Shutting entire persister down now!");
        this.executorService.shutdownNow();
        System.exit(-1);
    }
}
#method_after
protected void publishEvent(final String msg) {
    try {
        if (pipeline.publishEvent(msg)) {
            markRead();
        }
    } catch (Exception e) {
        logger.error("caught fatal exception while publishing msg. Shutting entire persister down now!");
        this.executorService.shutdownNow();
        LogManager.shutdown();
        System.exit(-1);
    }
}
#end_block

#method_before
@Override
public int process(String msg) throws IOException {
    MetricEnvelope[] metricEnvelopesArry = this.objectMapper.readValue(msg, MetricEnvelope[].class);
    for (final MetricEnvelope metricEnvelope : metricEnvelopesArry) {
        processEnvelope(metricEnvelope);
    }
    return metricEnvelopesArry.length;
}
#method_after
@Override
public int process(String msg) {
    MetricEnvelope[] metricEnvelopesArry;
    try {
        metricEnvelopesArry = this.objectMapper.readValue(msg, MetricEnvelope[].class);
    } catch (IOException e) {
        logger.error("[{}]: failed to deserialize message {}", this.threadId, msg, e);
        return 0;
    }
    for (final MetricEnvelope metricEnvelope : metricEnvelopesArry) {
        processEnvelope(metricEnvelope);
    }
    return metricEnvelopesArry.length;
}
#end_block

#method_before
private boolean isFlushTime() {
    logger.debug("[{}]: got heartbeat message, checking flush time. flush every {} seconds.", this.threadId, this.secondsBetweenFlushes);
    long now = System.currentTimeMillis();
    if (this.flushTimeMillis <= now) {
        logger.debug("[{}]: {} millis past flush time. flushing to repository now.", this.threadId, now - this.flushTimeMillis);
        return true;
    } else {
        logger.debug("[{}]: {} millis to next flush time. no need to flush at this time.", this.threadId, this.flushTimeMillis - now);
        return false;
    }
}
#method_after
private boolean isFlushTime() {
    logger.debug("[{}]: got heartbeat message, checking flush time. flush every {} seconds.", this.threadId, this.secondsBetweenFlushes);
    long now = System.currentTimeMillis();
    if (this.flushTimeMillis <= now) {
        logger.debug("[{}]: {} ms past flush time. flushing to repository now.", this.threadId, now - this.flushTimeMillis);
        return true;
    } else {
        logger.debug("[{}]: {} ms to next flush time. no need to flush at this time.", this.threadId, this.flushTimeMillis - now);
        return false;
    }
}
#end_block

#method_before
public int flush() throws Exception {
    logger.debug("[{}]: flushing", this.threadId);
    Timer.Context context = this.commitTimer.time();
    int msgFlushCnt = flushRepository();
    context.stop();
    this.commitMeter.mark();
    this.flushTimeMillis = System.currentTimeMillis() + this.millisBetweenFlushes;
    logger.debug("[{}]: flushed {} msg", this.threadId, msgFlushCnt);
    this.msgCount = 0;
    this.batchCount++;
    return msgFlushCnt;
}
#method_after
public int flush() throws Exception {
    logger.debug("[{}]: flushing", this.threadId);
    Timer.Context context = this.flushTimer.time();
    int msgFlushCnt = flushRepository();
    context.stop();
    this.flushMeter.mark();
    this.flushTimeMillis = System.currentTimeMillis() + this.millisBetweenFlushes;
    logger.debug("[{}]: flushed {} msg", this.threadId, msgFlushCnt);
    this.msgCount = 0;
    this.batchCount++;
    return msgFlushCnt;
}
#end_block

#method_before
@Override
public void addToBatch(MetricEnvelope metricEnvelope, String id) {
    Metric metric = metricEnvelope.metric;
    Map<String, Object> meta = metricEnvelope.meta;
    Definition definition = new Definition(metric.getName(), (String) meta.get("tenantId"), (String) meta.get("region"));
    Dimensions dimensions = new Dimensions(metric.getDimensions());
    Measurement measurement = new Measurement(metric.getTimestamp(), metric.getValue(), metric.getValueMeta());
    this.measurementBuffer.put(definition, dimensions, measurement);
    this.measurementMeter.mark();
}
#method_after
@Override
public void addToBatch(MetricEnvelope metricEnvelope, String id) {
    Metric metric = metricEnvelope.metric;
    Map<String, Object> meta = metricEnvelope.meta;
    Definition definition = new Definition(metric.getName(), (String) meta.get("tenantId"), (String) meta.get("region"));
    Dimensions dimensions = new Dimensions(metric.getDimensions());
    Measurement measurement = new Measurement(metric.getTimestamp(), metric.getValue(), metric.getValueMeta());
    this.measurementBuffer.put(definition, dimensions, measurement);
}
#end_block

#method_before
public void addToBatch(AlarmStateTransitionedEvent message, String id) {
    String metricsString = getSerializedString(message.metrics, id);
    String subAlarmsString = getSerializedString(message.subAlarms, id);
    String timeStamp = simpleDateFormat.format(new Date(message.timestamp * 1000));
    batch.add().bind("tenant_id", message.tenantId).bind("alarm_id", message.alarmId).bind("metrics", metricsString).bind("old_state", message.oldState.name()).bind("new_state", message.newState.name()).bind("sub_alarms", subAlarmsString).bind("reason", message.stateChangeReason).bind("reason_data", "{}").bind("time_stamp", timeStamp);
}
#method_after
public void addToBatch(AlarmStateTransitionedEvent message, String id) {
    String metricsString = getSerializedString(message.metrics, id);
    String subAlarmsString = getSerializedString(message.subAlarms, id);
    String timeStamp = simpleDateFormat.format(new Date(message.timestamp));
    batch.add().bind("tenant_id", message.tenantId).bind("alarm_id", message.alarmId).bind("metrics", metricsString).bind("old_state", message.oldState.name()).bind("new_state", message.newState.name()).bind("sub_alarms", subAlarmsString).bind("reason", message.stateChangeReason).bind("reason_data", "{}").bind("time_stamp", timeStamp);
}
#end_block

#method_before
private void commitBatch(String id) {
    long startTime = System.currentTimeMillis();
    Timer.Context context = commitTimer.time();
    batch.execute();
    handle.commit();
    handle.begin();
    context.stop();
    long endTime = System.currentTimeMillis();
    logger.debug("[{}]: commiting batch took {} millis", id, endTime - startTime);
}
#method_after
private void commitBatch(String id) {
    long startTime = System.currentTimeMillis();
    Timer.Context context = commitTimer.time();
    batch.execute();
    handle.commit();
    handle.begin();
    context.stop();
    long endTime = System.currentTimeMillis();
    logger.debug("[{}]: committing batch took {} ms", id, endTime - startTime);
}
#end_block

#method_before
@Override
protected int process(String msg) throws IOException {
    AlarmStateTransitionedEvent alarmStateTransitionedEvent = this.objectMapper.readValue(msg, AlarmStateTransitionedEvent.class);
    logger.debug("[{}]: [{}:{}] {}", this.threadId, this.getBatchCount(), this.getMsgCount(), alarmStateTransitionedEvent);
    this.alarmRepo.addToBatch(alarmStateTransitionedEvent, this.threadId);
    this.alarmStateTransitionCounter.inc();
    return 1;
}
#method_after
@Override
protected int process(String msg) {
    AlarmStateTransitionedEvent alarmStateTransitionedEvent;
    try {
        alarmStateTransitionedEvent = this.objectMapper.readValue(msg, AlarmStateTransitionedEvent.class);
    } catch (IOException e) {
        logger.error("[{}]: failed to deserialize message {}", this.threadId, msg, e);
        return 0;
    }
    logger.debug("[{}]: [{}:{}] {}", this.threadId, this.getBatchCount(), this.getMsgCount(), alarmStateTransitionedEvent);
    this.alarmRepo.addToBatch(alarmStateTransitionedEvent, this.threadId);
    this.alarmStateTransitionCounter.inc();
    return 1;
}
#end_block

#method_before
@Override
public void addToBatch(MetricEnvelope metricEnvelope, String id) {
    Metric metric = metricEnvelope.metric;
    Map<String, Object> meta = metricEnvelope.meta;
    String tenantId = getMeta(TENANT_ID, metric, meta, id);
    String region = getMeta(REGION, metric, meta, id);
    // Add the definition to the batch.
    StringBuilder definitionIdStringToHash = new StringBuilder(trunc(metric.getName(), MAX_COLUMN_LENGTH, id));
    definitionIdStringToHash.append(trunc(tenantId, MAX_COLUMN_LENGTH, id));
    definitionIdStringToHash.append(trunc(region, MAX_COLUMN_LENGTH, id));
    byte[] definitionIdSha1Hash = DigestUtils.sha(definitionIdStringToHash.toString());
    Sha1HashId definitionSha1HashId = new Sha1HashId((definitionIdSha1Hash));
    addDefinitionToBatch(definitionSha1HashId, trunc(metric.getName(), MAX_COLUMN_LENGTH, id), trunc(tenantId, MAX_COLUMN_LENGTH, id), trunc(region, MAX_COLUMN_LENGTH, id), id);
    definitionCounter.inc();
    // Calculate dimensions sha1 hash id.
    StringBuilder dimensionIdStringToHash = new StringBuilder();
    Map<String, String> preppedDimMap = prepDimensions(metric.getDimensions(), id);
    for (Map.Entry<String, String> entry : preppedDimMap.entrySet()) {
        dimensionIdStringToHash.append(entry.getKey());
        dimensionIdStringToHash.append(entry.getValue());
    }
    byte[] dimensionIdSha1Hash = DigestUtils.sha(dimensionIdStringToHash.toString());
    Sha1HashId dimensionsSha1HashId = new Sha1HashId(dimensionIdSha1Hash);
    // Add the dimension name/values to the batch.
    addDimensionsToBatch(dimensionsSha1HashId, preppedDimMap, id);
    dimensionCounter.inc();
    // Add the definition dimensions to the batch.
    StringBuilder definitionDimensionsIdStringToHash = new StringBuilder(definitionSha1HashId.toHexString());
    definitionDimensionsIdStringToHash.append(dimensionsSha1HashId.toHexString());
    byte[] definitionDimensionsIdSha1Hash = DigestUtils.sha(definitionDimensionsIdStringToHash.toString());
    Sha1HashId definitionDimensionsSha1HashId = new Sha1HashId(definitionDimensionsIdSha1Hash);
    this.addDefinitionDimensionToBatch(definitionDimensionsSha1HashId, definitionSha1HashId, dimensionsSha1HashId, id);
    definitionDimensionsCounter.inc();
    // Add the measurement to the batch.
    String timeStamp = simpleDateFormat.format(new Date(metric.getTimestamp()));
    double value = metric.getValue();
    addMetricToBatch(definitionDimensionsSha1HashId, timeStamp, value, metric.getValueMeta(), id);
    this.metricCounter.inc();
}
#method_after
@Override
public void addToBatch(MetricEnvelope metricEnvelope, String id) {
    Metric metric = metricEnvelope.metric;
    Map<String, Object> meta = metricEnvelope.meta;
    String tenantId = getMeta(TENANT_ID, metric, meta, id);
    String region = getMeta(REGION, metric, meta, id);
    // Add the definition to the batch.
    StringBuilder definitionIdStringToHash = new StringBuilder(trunc(metric.getName(), MAX_COLUMN_LENGTH, id));
    definitionIdStringToHash.append(trunc(tenantId, MAX_COLUMN_LENGTH, id));
    definitionIdStringToHash.append(trunc(region, MAX_COLUMN_LENGTH, id));
    byte[] definitionIdSha1Hash = DigestUtils.sha(definitionIdStringToHash.toString());
    Sha1HashId definitionSha1HashId = new Sha1HashId((definitionIdSha1Hash));
    addDefinitionToBatch(definitionSha1HashId, trunc(metric.getName(), MAX_COLUMN_LENGTH, id), trunc(tenantId, MAX_COLUMN_LENGTH, id), trunc(region, MAX_COLUMN_LENGTH, id), id);
    // Calculate dimensions sha1 hash id.
    StringBuilder dimensionIdStringToHash = new StringBuilder();
    Map<String, String> preppedDimMap = prepDimensions(metric.getDimensions(), id);
    for (Map.Entry<String, String> entry : preppedDimMap.entrySet()) {
        dimensionIdStringToHash.append(entry.getKey());
        dimensionIdStringToHash.append(entry.getValue());
    }
    byte[] dimensionIdSha1Hash = DigestUtils.sha(dimensionIdStringToHash.toString());
    Sha1HashId dimensionsSha1HashId = new Sha1HashId(dimensionIdSha1Hash);
    // Add the dimension name/values to the batch.
    addDimensionsToBatch(dimensionsSha1HashId, preppedDimMap, id);
    // Add the definition dimensions to the batch.
    StringBuilder definitionDimensionsIdStringToHash = new StringBuilder(definitionSha1HashId.toHexString());
    definitionDimensionsIdStringToHash.append(dimensionsSha1HashId.toHexString());
    byte[] definitionDimensionsIdSha1Hash = DigestUtils.sha(definitionDimensionsIdStringToHash.toString());
    Sha1HashId definitionDimensionsSha1HashId = new Sha1HashId(definitionDimensionsIdSha1Hash);
    this.addDefinitionDimensionToBatch(definitionDimensionsSha1HashId, definitionSha1HashId, dimensionsSha1HashId, id);
    // Add the measurement to the batch.
    String timeStamp = simpleDateFormat.format(new Date(metric.getTimestamp()));
    double value = metric.getValue();
    addMetricToBatch(definitionDimensionsSha1HashId, timeStamp, value, metric.getValueMeta(), id);
}
#end_block

#method_before
private String getMeta(String name, Metric metric, Map<String, Object> meta, String id) {
    if (meta.containsKey(name)) {
        return (String) meta.get(name);
    } else {
        logger.warn("[{}]: Failed to find {} in message envelope meta data. Metric message may be malformed. " + "Setting {} to empty string.", id);
        logger.warn("[{}]: metric: {}", id, metric.toString());
        logger.warn("[{}]: meta: {}", id, meta.toString());
        return "";
    }
}
#method_after
private String getMeta(String name, Metric metric, Map<String, Object> meta, String id) {
    if (meta.containsKey(name)) {
        return (String) meta.get(name);
    } else {
        logger.warn("[{}]: failed to find {} in message envelope meta data. metric message may be malformed. " + "setting {} to empty string.", id, name);
        logger.warn("[{}]: metric: {}", id, metric.toString());
        logger.warn("[{}]: meta: {}", id, meta.toString());
        return "";
    }
}
#end_block

#method_before
public void addMetricToBatch(Sha1HashId defDimsId, String timeStamp, double value, Map<String, String> valueMeta, String id) {
    String valueMetaString = getValueMetaString(valueMeta, id);
    logger.debug("[{}]: Adding metric to batch: defDimsId: {}, time: {}, value: {}, value meta {}", id, defDimsId.toHexString(), timeStamp, value, valueMetaString);
    metricsBatch.add().bind("definition_dimension_id", defDimsId.getSha1Hash()).bind("time_stamp", timeStamp).bind("value", value).bind("value_meta", valueMetaString);
    this.measurementCnt++;
    measurementMeter.mark();
}
#method_after
public void addMetricToBatch(Sha1HashId defDimsId, String timeStamp, double value, Map<String, String> valueMeta, String id) {
    String valueMetaString = getValueMetaString(valueMeta, id);
    logger.debug("[{}]: adding metric to batch: defDimsId: {}, time: {}, value: {}, value meta {}", id, defDimsId.toHexString(), timeStamp, value, valueMetaString);
    metricsBatch.add().bind("definition_dimension_id", defDimsId.getSha1Hash()).bind("time_stamp", timeStamp).bind("value", value).bind("value_meta", valueMetaString);
    this.measurementCnt++;
    measurementMeter.mark();
}
#end_block

#method_before
private void addDefinitionToBatch(Sha1HashId defId, String name, String tenantId, String region, String id) {
    if (definitionsIdCache.getIfPresent(defId) == null) {
        definitionCacheMissMeter.mark();
        if (!definitionIdSet.contains(defId)) {
            logger.debug("[{}]: Adding definition to batch: defId: {}, name: {}, tenantId: {}, region: {}", id, defId.toHexString(), name, tenantId, region);
            stagedDefinitionsBatch.add().bind("id", defId.getSha1Hash()).bind("name", name).bind("tenant_id", tenantId).bind("region", region);
            definitionIdSet.add(defId);
        }
    } else {
        definitionCacheHitMeter.mark();
    }
}
#method_after
private void addDefinitionToBatch(Sha1HashId defId, String name, String tenantId, String region, String id) {
    if (definitionsIdCache.getIfPresent(defId) == null) {
        definitionCacheMissMeter.mark();
        if (!definitionIdSet.contains(defId)) {
            logger.debug("[{}]: adding definition to batch: defId: {}, name: {}, tenantId: {}, region: {}", id, defId.toHexString(), name, tenantId, region);
            stagedDefinitionsBatch.add().bind("id", defId.getSha1Hash()).bind("name", name).bind("tenant_id", tenantId).bind("region", region);
            definitionIdSet.add(defId);
        }
    } else {
        definitionCacheHitMeter.mark();
    }
}
#end_block

#method_before
private void addDimensionsToBatch(Sha1HashId dimSetId, Map<String, String> dimMap, String id) {
    if (dimensionsIdCache.getIfPresent(dimSetId) == null) {
        dimensionCacheMissMeter.mark();
        if (!dimensionIdSet.contains(dimSetId)) {
            for (Map.Entry<String, String> entry : dimMap.entrySet()) {
                String name = entry.getKey();
                String value = entry.getValue();
                logger.debug("[{}]: Adding dimension to batch: dimSetId: {}, name: {}, value: {}", id, dimSetId.toHexString(), name, value);
                stagedDimensionsBatch.add().bind("dimension_set_id", dimSetId.getSha1Hash()).bind("name", name).bind("value", value);
            }
            dimensionIdSet.add(dimSetId);
        }
    } else {
        dimensionCacheHitMeter.mark();
    }
}
#method_after
private void addDimensionsToBatch(Sha1HashId dimSetId, Map<String, String> dimMap, String id) {
    if (dimensionsIdCache.getIfPresent(dimSetId) == null) {
        dimensionCacheMissMeter.mark();
        if (!dimensionIdSet.contains(dimSetId)) {
            for (Map.Entry<String, String> entry : dimMap.entrySet()) {
                String name = entry.getKey();
                String value = entry.getValue();
                logger.debug("[{}]: adding dimension to batch: dimSetId: {}, name: {}, value: {}", id, dimSetId.toHexString(), name, value);
                stagedDimensionsBatch.add().bind("dimension_set_id", dimSetId.getSha1Hash()).bind("name", name).bind("value", value);
            }
            dimensionIdSet.add(dimSetId);
        }
    } else {
        dimensionCacheHitMeter.mark();
    }
}
#end_block

#method_before
private void addDefinitionDimensionToBatch(Sha1HashId defDimsId, Sha1HashId defId, Sha1HashId dimId, String id) {
    if (definitionDimensionsIdCache.getIfPresent(defDimsId) == null) {
        definitionDimensionCacheMissMeter.mark();
        if (!definitionDimensionsIdSet.contains(defDimsId)) {
            logger.debug("[{}]: Adding definitionDimension to batch: defDimsId: {}, defId: {}, dimId: {}", defDimsId.toHexString(), defId, dimId, id);
            stagedDefinitionDimensionsBatch.add().bind("id", defDimsId.getSha1Hash()).bind("definition_id", defId.getSha1Hash()).bind("dimension_set_id", dimId.getSha1Hash());
            definitionDimensionsIdSet.add(defDimsId);
        }
    } else {
        definitionDimensionCacheHitMeter.mark();
    }
}
#method_after
private void addDefinitionDimensionToBatch(Sha1HashId defDimsId, Sha1HashId defId, Sha1HashId dimId, String id) {
    if (definitionDimensionsIdCache.getIfPresent(defDimsId) == null) {
        definitionDimensionCacheMissMeter.mark();
        if (!definitionDimensionsIdSet.contains(defDimsId)) {
            logger.debug("[{}]: adding definitionDimension to batch: defDimsId: {}, defId: {}, dimId: {}", defDimsId.toHexString(), defId, dimId, id);
            stagedDefinitionDimensionsBatch.add().bind("id", defDimsId.getSha1Hash()).bind("definition_id", defId.getSha1Hash()).bind("dimension_set_id", dimId.getSha1Hash());
            definitionDimensionsIdSet.add(defDimsId);
        }
    } else {
        definitionDimensionCacheHitMeter.mark();
    }
}
#end_block

#method_before
@Override
public int flush(String id) {
    try {
        long startTime = System.currentTimeMillis();
        Timer.Context context = flushTimer.time();
        executeBatches();
        writeRowsFromTempStagingTablesToPermTables();
        handle.commit();
        handle.begin();
        long endTime = System.currentTimeMillis();
        context.stop();
        logger.debug("[{}]: Writing measurements, definitions, and dimensions to database took {} millis", id, endTime - startTime);
        updateIdCaches();
        int flushCnt = this.measurementCnt;
        this.measurementCnt = 0;
        return flushCnt;
    } catch (Exception e) {
        logger.error("[{}]: Failed to write measurements, definitions, or dimensions to database", id, e);
        if (handle.isInTransaction()) {
            handle.rollback();
        }
        clearTempCaches();
        handle.begin();
        return this.measurementCnt = 0;
    }
}
#method_after
@Override
public int flush(String id) {
    try {
        long startTime = System.currentTimeMillis();
        Timer.Context context = commitTimer.time();
        executeBatches();
        writeRowsFromTempStagingTablesToPermTables();
        handle.commit();
        handle.begin();
        long endTime = System.currentTimeMillis();
        context.stop();
        logger.debug("[{}]: writing measurements, definitions, and dimensions to vertica took {} ms", id, endTime - startTime);
        updateIdCaches();
        int commitCnt = this.measurementCnt;
        this.measurementCnt = 0;
        return commitCnt;
    } catch (Exception e) {
        logger.error("[{}]: failed to write measurements, definitions, or dimensions to vertica", id, e);
        if (handle.isInTransaction()) {
            handle.rollback();
        }
        clearTempCaches();
        handle.begin();
        return this.measurementCnt = 0;
    }
}
#end_block

#method_before
private Map<String, String> prepDimensions(Map<String, String> dimMap, String id) {
    Map<String, String> newDimMap = new TreeMap<>();
    if (dimMap != null) {
        for (String dimName : dimMap.keySet()) {
            if (dimName != null && !dimName.isEmpty()) {
                String dimValue = dimMap.get(dimName);
                if (dimValue != null && !dimValue.isEmpty()) {
                    newDimMap.put(trunc(dimName, MAX_COLUMN_LENGTH, id), trunc(dimValue, MAX_COLUMN_LENGTH, id));
                    dimensionCounter.inc();
                }
            }
        }
    }
    return newDimMap;
}
#method_after
private Map<String, String> prepDimensions(Map<String, String> dimMap, String id) {
    Map<String, String> newDimMap = new TreeMap<>();
    if (dimMap != null) {
        for (String dimName : dimMap.keySet()) {
            if (dimName != null && !dimName.isEmpty()) {
                String dimValue = dimMap.get(dimName);
                if (dimValue != null && !dimValue.isEmpty()) {
                    newDimMap.put(trunc(dimName, MAX_COLUMN_LENGTH, id), trunc(dimValue, MAX_COLUMN_LENGTH, id));
                }
            }
        }
    }
    return newDimMap;
}
#end_block

#method_before
private String trunc(String s, int l, String id) {
    if (s == null) {
        return "";
    } else if (s.length() <= l) {
        return s;
    } else {
        String r = s.substring(0, l);
        logger.warn("[{}]: Input string exceeded max column length. Truncating input string {} to {} chars", id, s, l);
        logger.warn("[{}]: Resulting string {}", id, r);
        return r;
    }
}
#method_after
private String trunc(String s, int l, String id) {
    if (s == null) {
        return "";
    } else if (s.length() <= l) {
        return s;
    } else {
        String r = s.substring(0, l);
        logger.warn("[{}]: input string exceeded max column length. truncating input string {} to {} chars", id, s, l);
        logger.warn("[{}]: resulting string {}", id, r);
        return r;
    }
}
#end_block

#method_before
private boolean isBatchSize() {
    logger.debug("[{}]: checking batch size", this.threadId);
    if (this.msgCount >= this.batchSize) {
        logger.debug("[{}]: batch sized {} attained", this.threadId, this.batchSize);
        return true;
    } else {
        logger.debug("[{}]: batch size at now at {}, batch sized {} not attained", this.threadId, this.msgCount, this.batchSize);
        return false;
    }
}
#method_after
private boolean isBatchSize() {
    logger.debug("[{}]: checking batch size", this.threadId);
    if (this.msgCount >= this.batchSize) {
        logger.debug("[{}]: batch sized {} attained", this.threadId, this.batchSize);
        return true;
    } else {
        logger.debug("[{}]: batch size now at {}, batch size {} not attained", this.threadId, this.msgCount, this.batchSize);
        return false;
    }
}
#end_block

#method_before
@Override
protected void configure() {
    bind(PersisterConfig.class).toInstance(config);
    bind(Environment.class).toInstance(env);
    install(new FactoryModuleBuilder().implement(MetricHandler.class, MetricHandler.class).build(MetricHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionedEventHandler.class, AlarmStateTransitionedEventHandler.class).build(AlarmStateTransitionedEventHandlerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumerRunnableBasic.class, KafkaMetricsConsumerRunnableBasic.class).build(KafkaMetricsConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumerRunnableBasic.class, KafkaAlarmStateTransitionConsumerRunnableBasic.class).build(KafkaAlarmStateTransitionConsumerRunnableBasicFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaMetricsConsumer.class, KafkaMetricsConsumer.class).build(KafkaMetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricPipeline.class, MetricPipeline.class).build(MetricPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionPipeline.class, AlarmStateTransitionPipeline.class).build(AlarmStateTransitionPipelineFactory.class));
    install(new FactoryModuleBuilder().implement(AlarmStateTransitionConsumer.class, AlarmStateTransitionConsumer.class).build(AlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaAlarmStateTransitionConsumer.class, KafkaAlarmStateTransitionConsumer.class).build(KafkaAlarmStateTransitionConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(MetricsConsumer.class, MetricsConsumer.class).build(MetricsConsumerFactory.class));
    install(new FactoryModuleBuilder().implement(KafkaChannel.class, KafkaChannel.class).build(KafkaChannelFactory.class));
    if (config.getDatabaseConfiguration().getDatabaseType().equalsIgnoreCase(VERTICA)) {
        bind(DBI.class).toProvider(DBIProvider.class).in(Scopes.SINGLETON);
        bind(MetricRepo.class).to(VerticaMetricRepo.class);
        bind(AlarmRepo.class).to(VerticaAlarmRepo.class);
    } else if (config.getDatabaseConfiguration().getDatabaseType().equalsIgnoreCase(INFLUXDB)) {
        if (config.getInfluxDBConfiguration().getVersion() != null && !config.getInfluxDBConfiguration().getVersion().equalsIgnoreCase(INFLUXDB_V9)) {
            System.err.println("Found unsupported Influxdb version: " + config.getInfluxDBConfiguration().getVersion());
            System.err.println("Supported Influxdb versions are 'v9'");
            System.err.println("Check your config file");
            System.exit(1);
        }
        bind(InfluxV9RepoWriter.class).in(Singleton.class);
        bind(MetricRepo.class).to(InfluxV9MetricRepo.class);
        bind(AlarmRepo.class).to(InfluxV9AlarmRepo.class);
    } else {
        System.err.println("Found unknown database type: " + config.getDatabaseConfiguration().getDatabaseType());
        System.err.println("Supported databases are 'vertica' and 'influxdb'");
        System.err.println("Check your config file.");
        System.exit(1);
    }
}
#method_after
@Override
protected void configure() {
    bind(PersisterConfig.class).toInstance(config);
    bind(Environment.class).toInstance(env);
    install(new FactoryModuleBuilder().implement(new TypeLiteral<MetricHandler<MetricEnvelope[]>>() {
    }, new TypeLiteral<MetricHandler<MetricEnvelope[]>>() {
    }).build(new TypeLiteral<MetricHandlerFactory<MetricEnvelope[]>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<AlarmStateTransitionedEventHandler<AlarmStateTransitionedEvent>>() {
    }, new TypeLiteral<AlarmStateTransitionedEventHandler<AlarmStateTransitionedEvent>>() {
    }).build(new TypeLiteral<AlarmStateTransitionedEventHandlerFactory<AlarmStateTransitionedEvent>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<KafkaConsumerRunnableBasic<MetricEnvelope[]>>() {
    }, new TypeLiteral<KafkaConsumerRunnableBasic<MetricEnvelope[]>>() {
    }).build(new TypeLiteral<KafkaConsumerRunnableBasicFactory<MetricEnvelope[]>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<KafkaConsumerRunnableBasic<AlarmStateTransitionedEvent>>() {
    }, new TypeLiteral<KafkaConsumerRunnableBasic<AlarmStateTransitionedEvent>>() {
    }).build(new TypeLiteral<KafkaConsumerRunnableBasicFactory<AlarmStateTransitionedEvent>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<KafkaMetricsConsumer<MetricEnvelope[]>>() {
    }, new TypeLiteral<KafkaMetricsConsumer<MetricEnvelope[]>>() {
    }).build(new TypeLiteral<KafkaMetricsConsumerFactory<MetricEnvelope[]>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<ManagedPipeline<MetricEnvelope[]>>() {
    }, new TypeLiteral<ManagedPipeline<MetricEnvelope[]>>() {
    }).build(new TypeLiteral<ManagedPipelineFactory<MetricEnvelope[]>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<ManagedPipeline<AlarmStateTransitionedEvent>>() {
    }, new TypeLiteral<ManagedPipeline<AlarmStateTransitionedEvent>>() {
    }).build(new TypeLiteral<ManagedPipelineFactory<AlarmStateTransitionedEvent>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<Consumer<AlarmStateTransitionedEvent>>() {
    }, new TypeLiteral<Consumer<AlarmStateTransitionedEvent>>() {
    }).build(new TypeLiteral<ConsumerFactory<AlarmStateTransitionedEvent>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<KafkaAlarmStateTransitionConsumer<AlarmStateTransitionedEvent>>() {
    }, new TypeLiteral<KafkaAlarmStateTransitionConsumer<AlarmStateTransitionedEvent>>() {
    }).build(new TypeLiteral<KafkaAlarmStateTransitionConsumerFactory<AlarmStateTransitionedEvent>>() {
    }));
    install(new FactoryModuleBuilder().implement(new TypeLiteral<Consumer<MetricEnvelope[]>>() {
    }, new TypeLiteral<Consumer<MetricEnvelope[]>>() {
    }).build(new TypeLiteral<ConsumerFactory<MetricEnvelope[]>>() {
    }));
    install(new FactoryModuleBuilder().implement(KafkaChannel.class, KafkaChannel.class).build(KafkaChannelFactory.class));
    if (config.getDatabaseConfiguration().getDatabaseType().equalsIgnoreCase(VERTICA)) {
        bind(DBI.class).toProvider(DBIProvider.class).in(Scopes.SINGLETON);
        bind(MetricRepo.class).to(VerticaMetricRepo.class);
        bind(AlarmRepo.class).to(VerticaAlarmRepo.class);
    } else if (config.getDatabaseConfiguration().getDatabaseType().equalsIgnoreCase(INFLUXDB)) {
        if (config.getInfluxDBConfiguration().getVersion() != null && !config.getInfluxDBConfiguration().getVersion().equalsIgnoreCase(INFLUXDB_V9)) {
            System.err.println("Found unsupported Influxdb version: " + config.getInfluxDBConfiguration().getVersion());
            System.err.println("Supported Influxdb versions are 'v9'");
            System.err.println("Check your config file");
            System.exit(1);
        }
        bind(InfluxV9RepoWriter.class).in(Singleton.class);
        bind(MetricRepo.class).to(InfluxV9MetricRepo.class);
        bind(AlarmRepo.class).to(InfluxV9AlarmRepo.class);
    } else {
        System.err.println("Found unknown database type: " + config.getDatabaseConfiguration().getDatabaseType());
        System.err.println("Supported databases are 'vertica' and 'influxdb'");
        System.err.println("Check your config file.");
        System.exit(1);
    }
}
#end_block

#method_before
public AlarmDefinition create(String tenantId, String name, @Nullable String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, List<String> alarmActions, @Nullable List<String> okActions, @Nullable List<String> undeterminedActions) {
    // Assert no alarm exists by the name
    String alarmDefID = repo.exists(tenantId, name);
    if (alarmDefID != null) {
        throw new EntityExistsException("An alarm definition already exists for project / tenant: %s named: %s", tenantId, name);
    }
    DimensionValidation.validateMatchBy(matchBy);
    assertActionsExist(tenantId, alarmActions, okActions, undeterminedActions);
    Map<String, AlarmSubExpression> subAlarms = new HashMap<String, AlarmSubExpression>();
    for (AlarmSubExpression subExpression : alarmExpression.getSubExpressions()) subAlarms.put(UUID.randomUUID().toString(), subExpression);
    String alarmDefId = UUID.randomUUID().toString();
    AlarmDefinition alarm = null;
    try {
        LOG.debug("Creating alarm definition {} for tenant {}", name, tenantId);
        alarm = repo.create(tenantId, alarmDefId, name, description, severity, expression, subAlarms, matchBy, alarmActions, okActions, undeterminedActions);
        // Notify interested parties of new alarm
        String event = Serialization.toJson(new AlarmDefinitionCreatedEvent(tenantId, alarmDefId, name, description, expression, subAlarms, matchBy));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
        return alarm;
    } catch (Exception e) {
        if (alarm != null)
            try {
                repo.deleteById(tenantId, alarm.getId());
            } catch (Exception ignore) {
            }
        throw Exceptions.uncheck(e, "Error creating alarm definition for project / tenant %s", tenantId);
    }
}
#method_after
public AlarmDefinition create(String tenantId, String name, @Nullable String description, String severity, String expression, AlarmExpression alarmExpression, List<String> matchBy, List<String> alarmActions, @Nullable List<String> okActions, @Nullable List<String> undeterminedActions) {
    // Assert no alarm exists by the name
    String alarmDefID = repo.exists(tenantId, name);
    if (alarmDefID != null) {
        throw new EntityExistsException("An alarm definition already exists for project / tenant: %s named: %s", tenantId, name);
    }
    DimensionValidation.validateNames(matchBy);
    assertActionsExist(tenantId, alarmActions, okActions, undeterminedActions);
    Map<String, AlarmSubExpression> subAlarms = new HashMap<String, AlarmSubExpression>();
    for (AlarmSubExpression subExpression : alarmExpression.getSubExpressions()) subAlarms.put(UUID.randomUUID().toString(), subExpression);
    String alarmDefId = UUID.randomUUID().toString();
    AlarmDefinition alarm = null;
    try {
        LOG.debug("Creating alarm definition {} for tenant {}", name, tenantId);
        alarm = repo.create(tenantId, alarmDefId, name, description, severity, expression, subAlarms, matchBy, alarmActions, okActions, undeterminedActions);
        // Notify interested parties of new alarm
        String event = Serialization.toJson(new AlarmDefinitionCreatedEvent(tenantId, alarmDefId, name, description, expression, subAlarms, matchBy));
        producer.send(new KeyedMessage<>(config.eventsTopic, String.valueOf(eventCount++), event));
        return alarm;
    } catch (Exception e) {
        if (alarm != null)
            try {
                repo.deleteById(tenantId, alarm.getId());
            } catch (Exception ignore) {
            }
        throw Exceptions.uncheck(e, "Error creating alarm definition for project / tenant %s", tenantId);
    }
}
#end_block

#method_before
private InfluxPoint[] getInfluxPointArry() throws Exception {
    List<InfluxPoint> influxPointList = new LinkedList<>();
    for (Map.Entry<Definition, Map<Dimensions, List<Measurement>>> definitionMapEntry : this.measurementBuffer.entrySet()) {
        Definition definition = definitionMapEntry.getKey();
        Map<Dimensions, List<Measurement>> dimensionsMap = definitionMapEntry.getValue();
        for (Map.Entry<Dimensions, List<Measurement>> dimensionsMapEntry : dimensionsMap.entrySet()) {
            Dimensions dimensions = dimensionsMapEntry.getKey();
            List<Measurement> measurementList = dimensionsMapEntry.getValue();
            Map<String, String> tagMap = buildTagMap(definition, dimensions);
            for (Measurement measurement : measurementList) {
                InfluxPoint influxPoint = new InfluxPoint(definition.getName(), tagMap, measurement.getISOFormattedTimeString(), buildValueMap(measurement));
                influxPointList.add(influxPoint);
                this.measurementMeter.mark();
            }
        }
    }
    return influxPointList.toArray(new InfluxPoint[influxPointList.size()]);
}
#method_after
private InfluxPoint[] getInfluxPointArry() throws Exception {
    List<InfluxPoint> influxPointList = new LinkedList<>();
    for (Map.Entry<Definition, Map<Dimensions, List<Measurement>>> definitionMapEntry : this.measurementBuffer.entrySet()) {
        Definition definition = definitionMapEntry.getKey();
        Map<Dimensions, List<Measurement>> dimensionsMap = definitionMapEntry.getValue();
        for (Map.Entry<Dimensions, List<Measurement>> dimensionsMapEntry : dimensionsMap.entrySet()) {
            Dimensions dimensions = dimensionsMapEntry.getKey();
            List<Measurement> measurementList = dimensionsMapEntry.getValue();
            Map<String, String> tagMap = buildTagMap(definition, dimensions);
            for (Measurement measurement : measurementList) {
                InfluxPoint influxPoint = new InfluxPoint(definition.getName(), tagMap, measurement.getISOFormattedTimeString(), buildValueMap(measurement));
                influxPointList.add(influxPoint);
            }
        }
    }
    return influxPointList.toArray(new InfluxPoint[influxPointList.size()]);
}
#end_block

#method_before
protected void write(final InfluxPoint[] influxPointArry) throws Exception {
    HttpPost request = new HttpPost(this.influxUrl);
    request.addHeader("content-type", "application/json");
    request.addHeader("Authorization", this.baseAuthHeader);
    InfluxWrite influxWrite = new InfluxWrite(this.influxName, this.influxRetentionPolicy, influxPointArry, new HashMap());
    StringEntity params = new StringEntity(this.objectMapper.writeValueAsString(influxWrite));
    request.setEntity(params);
    try {
        logger.debug("Writing {} points to influxdb database {} at {}", influxPointArry.length, this.influxName, this.influxUrl);
        HttpResponse response = this.httpClient.execute(request);
        int rc = response.getStatusLine().getStatusCode();
        if (rc != HttpStatus.SC_OK) {
            HttpEntity entity = response.getEntity();
            String responseString = EntityUtils.toString(entity, "UTF-8");
            logger.error("Failed to write data to influx database {} at {}: {}", this.influxName, this.influxUrl, String.valueOf(rc));
            logger.error("Http response: {}", responseString);
            throw new Exception(rc + ":" + responseString);
        }
        logger.debug("Successfully wrote {} points to influx database {} at {}", influxPointArry.length, this.influxName, this.influxUrl);
    } finally {
        request.releaseConnection();
    }
}
#method_after
protected void write(final InfluxPoint[] influxPointArry) throws Exception {
    HttpPost request = new HttpPost(this.influxUrl);
    request.addHeader("Content-Type", "application/json");
    request.addHeader("Authorization", this.baseAuthHeader);
    InfluxWrite influxWrite = new InfluxWrite(this.influxName, this.influxRetentionPolicy, influxPointArry, new HashMap());
    String json = this.objectMapper.writeValueAsString(influxWrite);
    if (this.config.getInfluxDBConfiguration().getGzip()) {
        HttpEntity requestEntity = EntityBuilder.create().setText(json).setContentType(ContentType.APPLICATION_JSON).gzipCompress().build();
        request.setEntity(requestEntity);
        request.addHeader("Content-Encoding", "gzip");
    } else {
        StringEntity stringEntity = new StringEntity(json);
        request.setEntity(stringEntity);
    }
    try {
        logger.debug("Writing {} points to influxdb database {} at {}", influxPointArry.length, this.influxName, this.influxUrl);
        HttpResponse response = this.httpClient.execute(request);
        int rc = response.getStatusLine().getStatusCode();
        if (rc != HttpStatus.SC_OK) {
            HttpEntity responseEntity = response.getEntity();
            String responseString = EntityUtils.toString(responseEntity, "UTF-8");
            logger.error("Failed to write data to influx database {} at {}: {}", this.influxName, this.influxUrl, String.valueOf(rc));
            logger.error("Http response: {}", responseString);
            throw new Exception(rc + ":" + responseString);
        }
        logger.debug("Successfully wrote {} points to influx database {} at {}", influxPointArry.length, this.influxName, this.influxUrl);
    } finally {
        request.releaseConnection();
    }
}
#end_block

#method_before
@Override
public void addToBatch(MetricEnvelope metricEnvelope) {
    Metric metric = metricEnvelope.metric;
    Map<String, Object> meta = metricEnvelope.meta;
    logger.debug("metric: {}", metric);
    logger.debug("meta: {}", meta);
    String tenantId = "";
    if (meta.containsKey(TENANT_ID)) {
        tenantId = (String) meta.get(TENANT_ID);
    } else {
        logger.warn("Failed to find tenantId in message envelope meta data. Metric message may be malformed. Setting tenantId to empty string.");
        logger.warn("metric: {}", metric.toString());
        logger.warn("meta: {}", meta.toString());
    }
    String region = "";
    if (meta.containsKey(REGION)) {
        region = (String) meta.get(REGION);
    } else {
        logger.warn("Failed to find region in message envelope meta data. Metric message may be malformed. Setting region to empty string.");
        logger.warn("metric: {}", metric.toString());
        logger.warn("meta: {}", meta.toString());
    }
    // Add the definition to the batch.
    StringBuilder definitionIdStringToHash = new StringBuilder(trunc(metric.getName(), MAX_COLUMN_LENGTH));
    definitionIdStringToHash.append(trunc(tenantId, MAX_COLUMN_LENGTH));
    definitionIdStringToHash.append(trunc(region, MAX_COLUMN_LENGTH));
    byte[] definitionIdSha1Hash = DigestUtils.sha(definitionIdStringToHash.toString());
    Sha1HashId definitionSha1HashId = new Sha1HashId((definitionIdSha1Hash));
    this.addDefinitionToBatch(definitionSha1HashId, trunc(metric.getName(), MAX_COLUMN_LENGTH), trunc(tenantId, MAX_COLUMN_LENGTH), trunc(region, MAX_COLUMN_LENGTH));
    definitionCounter.inc();
    // Calculate dimensions sha1 hash id.
    StringBuilder dimensionIdStringToHash = new StringBuilder();
    Map<String, String> preppedDimMap = prepDimensions(metric.getDimensions());
    for (Map.Entry<String, String> entry : preppedDimMap.entrySet()) {
        dimensionIdStringToHash.append(entry.getKey());
        dimensionIdStringToHash.append(entry.getValue());
    }
    byte[] dimensionIdSha1Hash = DigestUtils.sha(dimensionIdStringToHash.toString());
    Sha1HashId dimensionsSha1HashId = new Sha1HashId(dimensionIdSha1Hash);
    // Add the dimension name/values to the batch.
    this.addDimensionsToBatch(dimensionsSha1HashId, preppedDimMap);
    // Add the definition dimensions to the batch.
    StringBuilder definitionDimensionsIdStringToHash = new StringBuilder(definitionSha1HashId.toHexString());
    definitionDimensionsIdStringToHash.append(dimensionsSha1HashId.toHexString());
    byte[] definitionDimensionsIdSha1Hash = DigestUtils.sha(definitionDimensionsIdStringToHash.toString());
    Sha1HashId definitionDimensionsSha1HashId = new Sha1HashId(definitionDimensionsIdSha1Hash);
    this.addDefinitionDimensionToBatch(definitionDimensionsSha1HashId, definitionSha1HashId, dimensionsSha1HashId);
    definitionDimensionsCounter.inc();
    // Add the measurement to the batch.
    String timeStamp = simpleDateFormat.format(new Date(metric.getTimestamp()));
    double value = metric.getValue();
    this.addMetricToBatch(definitionDimensionsSha1HashId, timeStamp, value, metric.getValueMeta());
}
#method_after
@Override
public void addToBatch(MetricEnvelope metricEnvelope) {
    Metric metric = metricEnvelope.metric;
    Map<String, Object> meta = metricEnvelope.meta;
    logger.debug("metric: {}", metric);
    logger.debug("meta: {}", meta);
    String tenantId = "";
    if (meta.containsKey(TENANT_ID)) {
        tenantId = (String) meta.get(TENANT_ID);
    } else {
        logger.warn("Failed to find tenantId in message envelope meta data. Metric message may be malformed" + ". Setting tenantId to empty string.");
        logger.warn("metric: {}", metric.toString());
        logger.warn("meta: {}", meta.toString());
    }
    String region = "";
    if (meta.containsKey(REGION)) {
        region = (String) meta.get(REGION);
    } else {
        logger.warn("Failed to find region in message envelope meta data. Metric message may be malformed. " + "Setting region to empty string.");
        logger.warn("metric: {}", metric.toString());
        logger.warn("meta: {}", meta.toString());
    }
    // Add the definition to the batch.
    StringBuilder definitionIdStringToHash = new StringBuilder(trunc(metric.getName(), MAX_COLUMN_LENGTH));
    definitionIdStringToHash.append(trunc(tenantId, MAX_COLUMN_LENGTH));
    definitionIdStringToHash.append(trunc(region, MAX_COLUMN_LENGTH));
    byte[] definitionIdSha1Hash = DigestUtils.sha(definitionIdStringToHash.toString());
    Sha1HashId definitionSha1HashId = new Sha1HashId((definitionIdSha1Hash));
    this.addDefinitionToBatch(definitionSha1HashId, trunc(metric.getName(), MAX_COLUMN_LENGTH), trunc(tenantId, MAX_COLUMN_LENGTH), trunc(region, MAX_COLUMN_LENGTH));
    definitionCounter.inc();
    // Calculate dimensions sha1 hash id.
    StringBuilder dimensionIdStringToHash = new StringBuilder();
    Map<String, String> preppedDimMap = prepDimensions(metric.getDimensions());
    for (Map.Entry<String, String> entry : preppedDimMap.entrySet()) {
        dimensionIdStringToHash.append(entry.getKey());
        dimensionIdStringToHash.append(entry.getValue());
    }
    byte[] dimensionIdSha1Hash = DigestUtils.sha(dimensionIdStringToHash.toString());
    Sha1HashId dimensionsSha1HashId = new Sha1HashId(dimensionIdSha1Hash);
    // Add the dimension name/values to the batch.
    this.addDimensionsToBatch(dimensionsSha1HashId, preppedDimMap);
    // Add the definition dimensions to the batch.
    StringBuilder definitionDimensionsIdStringToHash = new StringBuilder(definitionSha1HashId.toHexString());
    definitionDimensionsIdStringToHash.append(dimensionsSha1HashId.toHexString());
    byte[] definitionDimensionsIdSha1Hash = DigestUtils.sha(definitionDimensionsIdStringToHash.toString());
    Sha1HashId definitionDimensionsSha1HashId = new Sha1HashId(definitionDimensionsIdSha1Hash);
    this.addDefinitionDimensionToBatch(definitionDimensionsSha1HashId, definitionSha1HashId, dimensionsSha1HashId);
    definitionDimensionsCounter.inc();
    // Add the measurement to the batch.
    String timeStamp = simpleDateFormat.format(new Date(metric.getTimestamp()));
    double value = metric.getValue();
    this.addMetricToBatch(definitionDimensionsSha1HashId, timeStamp, value, metric.getValueMeta());
    this.metricCounter.inc();
}
#end_block

#method_before
private void checkHydrate(final String base) throws URISyntaxException {
    final UriInfo uriInfo = mock(UriInfo.class);
    final String alarmId = "b6a454b7-b557-426a-af51-657d0a7384d6";
    final URI uri = new URI(base);
    when(uriInfo.getBaseUri()).thenReturn(uri);
    final String alarmDefinitionId = "af72b3d8-51f3-4eee-8086-535b5e7a9dc8";
    final Alarm alarm = new Alarm(alarmId, alarmDefinitionId, "Test", "LOW", null, AlarmState.OK, null, null);
    alarm.setId("42");
    Links.hydrate(alarm.getAlarmDefinition(), uriInfo, AlarmDefinitionResource.ALARM_DEFINITIONS_PATH);
    assertEquals(alarm.getAlarmDefinition().getLinks().size(), 1);
    assertEquals(alarm.getAlarmDefinition().getLinks().get(0), new Link("self", base + // Have to cut the first / off of AlarmDefinitionResource.ALARM_DEFINITIONS_PATH
    AlarmDefinitionResource.ALARM_DEFINITIONS_PATH.substring(1) + "/" + alarmDefinitionId));
}
#method_after
private void checkHydrate(final String base) throws URISyntaxException {
    final UriInfo uriInfo = mock(UriInfo.class);
    final String alarmId = "b6a454b7-b557-426a-af51-657d0a7384d6";
    final URI uri = new URI(base);
    when(uriInfo.getBaseUri()).thenReturn(uri);
    final String alarmDefinitionId = "af72b3d8-51f3-4eee-8086-535b5e7a9dc8";
    final Alarm alarm = new Alarm(alarmId, alarmDefinitionId, "Test", "LOW", null, AlarmState.OK, DateTime.parse("2015-03-14T09:26:53"), DateTime.parse("2015-03-14T09:26:53"));
    alarm.setId("42");
    Links.hydrate(alarm.getAlarmDefinition(), uriInfo, AlarmDefinitionResource.ALARM_DEFINITIONS_PATH);
    assertEquals(alarm.getAlarmDefinition().getLinks().size(), 1);
    assertEquals(alarm.getAlarmDefinition().getLinks().get(0), new Link("self", base + // Have to cut the first / off of AlarmDefinitionResource.ALARM_DEFINITIONS_PATH
    AlarmDefinitionResource.ALARM_DEFINITIONS_PATH.substring(1) + "/" + alarmDefinitionId));
}
#end_block

#method_before
public static void main(String[] args) throws Exception {
    new MonApiApplication().run(args);
}
#method_after
public static void main(String[] args) throws Exception {
    /*
     * This should allow command line options to show the current version
     * java -jar monasca-api.jar --version
     * java -jar monasca-api.jar -version
     * java -jar monasca-api.jar version
     * Really anything with the word version in it will show the
     * version as long as there is only one argument
     * */
    if (args.length == 1 && args[0].toLowerCase().contains("version")) {
        showVersion();
        System.exit(0);
    }
    new MonApiApplication().run(args);
}
#end_block

#method_before
@Override
@SuppressWarnings("unchecked")
public void run(ApiConfig config, Environment environment) throws Exception {
    /**
     * Wire services
     */
    Injector.registerModules(new MonApiModule(environment, config));
    /**
     * Configure resources
     */
    environment.jersey().register(Injector.getInstance(VersionResource.class));
    environment.jersey().register(Injector.getInstance(AlarmDefinitionResource.class));
    environment.jersey().register(Injector.getInstance(AlarmResource.class));
    environment.jersey().register(Injector.getInstance(MetricResource.class));
    environment.jersey().register(Injector.getInstance(MeasurementResource.class));
    environment.jersey().register(Injector.getInstance(StatisticResource.class));
    environment.jersey().register(Injector.getInstance(NotificationMethodResource.class));
    /**
     * Configure providers
     */
    removeExceptionMappers(environment.jersey().getResourceConfig().getSingletons());
    environment.jersey().register(new EntityExistsExceptionMapper());
    environment.jersey().register(new EntityNotFoundExceptionMapper());
    environment.jersey().register(new IllegalArgumentExceptionMapper());
    environment.jersey().register(new InvalidEntityExceptionMapper());
    environment.jersey().register(new JsonProcessingExceptionMapper());
    environment.jersey().register(new JsonMappingExceptionManager());
    environment.jersey().register(new ConstraintViolationExceptionMapper());
    environment.jersey().register(new ThrowableExceptionMapper<Throwable>() {
    });
    /**
     * Configure Jackson
     */
    environment.getObjectMapper().setPropertyNamingStrategy(PropertyNamingStrategy.CAMEL_CASE_TO_LOWER_CASE_WITH_UNDERSCORES);
    environment.getObjectMapper().enable(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY);
    environment.getObjectMapper().disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS);
    SimpleModule module = new SimpleModule("SerializationModule");
    module.addSerializer(new SubAlarmExpressionSerializer());
    module.addSerializer(new TimestampSerializer());
    environment.getObjectMapper().registerModule(module);
    /**
     * Configure CORS filter
     */
    Dynamic corsFilter = environment.servlets().addFilter("cors", CrossOriginFilter.class);
    corsFilter.addMappingForUrlPatterns(null, true, "/*");
    corsFilter.setInitParameter("allowedOrigins", "*");
    corsFilter.setInitParameter("allowedHeaders", "X-Requested-With,Content-Type,Accept,Origin,X-Auth-Token");
    corsFilter.setInitParameter("allowedMethods", "OPTIONS,GET,HEAD");
    if (config.middleware.enabled) {
        ensureHasValue(config.middleware.serverVIP, "serverVIP", "enabled", "true");
        ensureHasValue(config.middleware.serverPort, "serverPort", "enabled", "true");
        ensureHasValue(config.middleware.adminAuthMethod, "adminAuthMethod", "enabled", "true");
        if ("password".equalsIgnoreCase(config.middleware.adminAuthMethod)) {
            ensureHasValue(config.middleware.adminUser, "adminUser", "adminAuthMethod", "password");
            ensureHasValue(config.middleware.adminPassword, "adminPassword", "adminAuthMethod", "password");
        } else if ("token".equalsIgnoreCase(config.middleware.adminAuthMethod)) {
            ensureHasValue(config.middleware.adminToken, "adminToken", "adminAuthMethod", "token");
        } else {
            throw new Exception(String.format("Invalid value '%s' for adminAuthMethod. Must be either password or token", config.middleware.adminAuthMethod));
        }
        if (config.middleware.defaultAuthorizedRoles == null || config.middleware.defaultAuthorizedRoles.isEmpty()) {
            ensureHasValue(null, "defaultAuthorizedRoles", "enabled", "true");
        }
        if (config.middleware.connSSLClientAuth) {
            ensureHasValue(config.middleware.keystore, "keystore", "connSSLClientAuth", "true");
            ensureHasValue(config.middleware.keystorePassword, "keystorePassword", "connSSLClientAuth", "true");
        }
        Map<String, String> authInitParams = new HashMap<String, String>();
        authInitParams.put("ServerVIP", config.middleware.serverVIP);
        authInitParams.put("ServerPort", config.middleware.serverPort);
        authInitParams.put(AuthConstants.USE_HTTPS, String.valueOf(config.middleware.useHttps));
        authInitParams.put("ConnTimeout", config.middleware.connTimeout);
        authInitParams.put("ConnSSLClientAuth", String.valueOf(config.middleware.connSSLClientAuth));
        authInitParams.put("ConnPoolMaxActive", config.middleware.connPoolMaxActive);
        authInitParams.put("ConnPoolMaxIdle", config.middleware.connPoolMaxActive);
        authInitParams.put("ConnPoolEvictPeriod", config.middleware.connPoolEvictPeriod);
        authInitParams.put("ConnPoolMinIdleTime", config.middleware.connPoolMinIdleTime);
        authInitParams.put("ConnRetryTimes", config.middleware.connRetryTimes);
        authInitParams.put("ConnRetryInterval", config.middleware.connRetryInterval);
        authInitParams.put("AdminToken", config.middleware.adminToken);
        authInitParams.put("TimeToCacheToken", config.middleware.timeToCacheToken);
        authInitParams.put("AdminAuthMethod", config.middleware.adminAuthMethod);
        authInitParams.put("AdminUser", config.middleware.adminUser);
        authInitParams.put("AdminPassword", config.middleware.adminPassword);
        authInitParams.put("MaxTokenCacheSize", config.middleware.maxTokenCacheSize);
        setIfNotNull(authInitParams, AuthConstants.TRUSTSTORE, config.middleware.truststore);
        setIfNotNull(authInitParams, AuthConstants.TRUSTSTORE_PASS, config.middleware.truststorePassword);
        setIfNotNull(authInitParams, AuthConstants.KEYSTORE, config.middleware.keystore);
        setIfNotNull(authInitParams, AuthConstants.KEYSTORE_PASS, config.middleware.keystorePassword);
        /**
         * Configure auth filters
         */
        Dynamic preAuthenticationFilter = environment.servlets().addFilter("pre-auth", new PreAuthenticationFilter());
        preAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        preAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        Dynamic tokenAuthFilter = environment.servlets().addFilter("token-auth", new TokenAuth());
        tokenAuthFilter.addMappingForUrlPatterns(null, true, "/");
        tokenAuthFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        tokenAuthFilter.setInitParameters(authInitParams);
        Dynamic postAuthenticationFilter = environment.servlets().addFilter("post-auth", new PostAuthenticationFilter(config.middleware.defaultAuthorizedRoles, config.middleware.agentAuthorizedRoles));
        postAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        postAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        environment.jersey().getResourceConfig().getContainerRequestFilters().add(new RoleAuthorizationFilter());
    } else {
        Dynamic mockAuthenticationFilter = environment.servlets().addFilter("mock-auth", new MockAuthenticationFilter());
        mockAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        mockAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
    }
}
#method_after
@Override
@SuppressWarnings("unchecked")
public void run(ApiConfig config, Environment environment) throws Exception {
    /**
     * Wire services
     */
    Injector.registerModules(new MonApiModule(environment, config));
    /**
     * Configure resources
     */
    environment.jersey().register(Injector.getInstance(VersionResource.class));
    environment.jersey().register(Injector.getInstance(AlarmDefinitionResource.class));
    environment.jersey().register(Injector.getInstance(AlarmResource.class));
    environment.jersey().register(Injector.getInstance(MetricResource.class));
    environment.jersey().register(Injector.getInstance(MeasurementResource.class));
    environment.jersey().register(Injector.getInstance(StatisticResource.class));
    environment.jersey().register(Injector.getInstance(NotificationMethodResource.class));
    /**
     * Configure providers
     */
    removeExceptionMappers(environment.jersey().getResourceConfig().getSingletons());
    environment.jersey().register(new EntityExistsExceptionMapper());
    environment.jersey().register(new EntityNotFoundExceptionMapper());
    environment.jersey().register(new IllegalArgumentExceptionMapper());
    environment.jersey().register(new InvalidEntityExceptionMapper());
    environment.jersey().register(new JsonProcessingExceptionMapper());
    environment.jersey().register(new JsonMappingExceptionManager());
    environment.jersey().register(new ConstraintViolationExceptionMapper());
    environment.jersey().register(new ThrowableExceptionMapper<Throwable>() {
    });
    /**
     * Configure Jackson
     */
    environment.getObjectMapper().setPropertyNamingStrategy(PropertyNamingStrategy.CAMEL_CASE_TO_LOWER_CASE_WITH_UNDERSCORES);
    environment.getObjectMapper().enable(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY);
    environment.getObjectMapper().disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS);
    SimpleModule module = new SimpleModule("SerializationModule");
    module.addSerializer(new SubAlarmExpressionSerializer());
    environment.getObjectMapper().registerModule(module);
    /**
     * Configure CORS filter
     */
    Dynamic corsFilter = environment.servlets().addFilter("cors", CrossOriginFilter.class);
    corsFilter.addMappingForUrlPatterns(null, true, "/*");
    corsFilter.setInitParameter("allowedOrigins", "*");
    corsFilter.setInitParameter("allowedHeaders", "X-Requested-With,Content-Type,Accept,Origin,X-Auth-Token");
    corsFilter.setInitParameter("allowedMethods", "OPTIONS,GET,HEAD");
    if (config.middleware.enabled) {
        ensureHasValue(config.middleware.serverVIP, "serverVIP", "enabled", "true");
        ensureHasValue(config.middleware.serverPort, "serverPort", "enabled", "true");
        ensureHasValue(config.middleware.adminAuthMethod, "adminAuthMethod", "enabled", "true");
        if ("password".equalsIgnoreCase(config.middleware.adminAuthMethod)) {
            ensureHasValue(config.middleware.adminUser, "adminUser", "adminAuthMethod", "password");
            ensureHasValue(config.middleware.adminPassword, "adminPassword", "adminAuthMethod", "password");
        } else if ("token".equalsIgnoreCase(config.middleware.adminAuthMethod)) {
            ensureHasValue(config.middleware.adminToken, "adminToken", "adminAuthMethod", "token");
        } else {
            throw new Exception(String.format("Invalid value '%s' for adminAuthMethod. Must be either password or token", config.middleware.adminAuthMethod));
        }
        if (config.middleware.defaultAuthorizedRoles == null || config.middleware.defaultAuthorizedRoles.isEmpty()) {
            ensureHasValue(null, "defaultAuthorizedRoles", "enabled", "true");
        }
        if (config.middleware.connSSLClientAuth) {
            ensureHasValue(config.middleware.keystore, "keystore", "connSSLClientAuth", "true");
            ensureHasValue(config.middleware.keystorePassword, "keystorePassword", "connSSLClientAuth", "true");
        }
        Map<String, String> authInitParams = new HashMap<String, String>();
        authInitParams.put("ServerVIP", config.middleware.serverVIP);
        authInitParams.put("ServerPort", config.middleware.serverPort);
        authInitParams.put(AuthConstants.USE_HTTPS, String.valueOf(config.middleware.useHttps));
        authInitParams.put("ConnTimeout", config.middleware.connTimeout);
        authInitParams.put("ConnSSLClientAuth", String.valueOf(config.middleware.connSSLClientAuth));
        authInitParams.put("ConnPoolMaxActive", config.middleware.connPoolMaxActive);
        authInitParams.put("ConnPoolMaxIdle", config.middleware.connPoolMaxActive);
        authInitParams.put("ConnPoolEvictPeriod", config.middleware.connPoolEvictPeriod);
        authInitParams.put("ConnPoolMinIdleTime", config.middleware.connPoolMinIdleTime);
        authInitParams.put("ConnRetryTimes", config.middleware.connRetryTimes);
        authInitParams.put("ConnRetryInterval", config.middleware.connRetryInterval);
        authInitParams.put("AdminToken", config.middleware.adminToken);
        authInitParams.put("TimeToCacheToken", config.middleware.timeToCacheToken);
        authInitParams.put("AdminAuthMethod", config.middleware.adminAuthMethod);
        authInitParams.put("AdminUser", config.middleware.adminUser);
        authInitParams.put("AdminPassword", config.middleware.adminPassword);
        authInitParams.put(AuthConstants.ADMIN_PROJECT_ID, config.middleware.adminProjectId);
        authInitParams.put(AuthConstants.ADMIN_PROJECT_NAME, config.middleware.adminProjectName);
        authInitParams.put("MaxTokenCacheSize", config.middleware.maxTokenCacheSize);
        setIfNotNull(authInitParams, AuthConstants.TRUSTSTORE, config.middleware.truststore);
        setIfNotNull(authInitParams, AuthConstants.TRUSTSTORE_PASS, config.middleware.truststorePassword);
        setIfNotNull(authInitParams, AuthConstants.KEYSTORE, config.middleware.keystore);
        setIfNotNull(authInitParams, AuthConstants.KEYSTORE_PASS, config.middleware.keystorePassword);
        /**
         * Configure auth filters
         */
        Dynamic preAuthenticationFilter = environment.servlets().addFilter("pre-auth", new PreAuthenticationFilter());
        preAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        preAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        Dynamic tokenAuthFilter = environment.servlets().addFilter("token-auth", new TokenAuth());
        tokenAuthFilter.addMappingForUrlPatterns(null, true, "/");
        tokenAuthFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        tokenAuthFilter.setInitParameters(authInitParams);
        Dynamic postAuthenticationFilter = environment.servlets().addFilter("post-auth", new PostAuthenticationFilter(config.middleware.defaultAuthorizedRoles, config.middleware.agentAuthorizedRoles));
        postAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        postAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
        environment.jersey().getResourceConfig().getContainerRequestFilters().add(new RoleAuthorizationFilter());
    } else {
        Dynamic mockAuthenticationFilter = environment.servlets().addFilter("mock-auth", new MockAuthenticationFilter());
        mockAuthenticationFilter.addMappingForUrlPatterns(null, true, "/");
        mockAuthenticationFilter.addMappingForUrlPatterns(null, true, "/v2.0/*");
    }
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object list(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("alarm_definition_id") String alarmDefId, @QueryParam("metric_name") String metricName, @QueryParam("metric_dimensions") String metricDimensionsStr, @QueryParam("state") AlarmState state, @QueryParam("state_updated_at") String stateUpdatedAtStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    Map<String, String> metricDimensions = Strings.isNullOrEmpty(metricDimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(metricName, metricDimensionsStr);
    DateTime stateUpdatedAt = Validation.parseAndValidateDate(stateUpdatedAtStr, "state_updated_at", false);
    final List<Alarm> alarms = repo.find(tenantId, alarmDefId, metricName, metricDimensions, state, stateUpdatedAt, offset, this.persistUtils.getLimit(limit), true);
    for (final Alarm alarm : alarms) {
        Links.hydrate(alarm.getAlarmDefinition(), uriInfo, AlarmDefinitionResource.ALARM_DEFINITIONS_PATH);
    }
    return Links.paginate(this.persistUtils.getLimit(limit), Links.hydrate(alarms, uriInfo), uriInfo);
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object list(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("alarm_definition_id") String alarmDefId, @QueryParam("metric_name") String metricName, @QueryParam("metric_dimensions") String metricDimensionsStr, @QueryParam("state") AlarmState state, @QueryParam("state_updated_start_time") String stateUpdatedStartStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    Map<String, String> metricDimensions = Strings.isNullOrEmpty(metricDimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(metricName, metricDimensionsStr, false);
    DateTime stateUpdatedStart = Validation.parseAndValidateDate(stateUpdatedStartStr, "state_updated_start_time", false);
    final List<Alarm> alarms = repo.find(tenantId, alarmDefId, metricName, metricDimensions, state, stateUpdatedStart, offset, this.persistUtils.getLimit(limit), true);
    for (final Alarm alarm : alarms) {
        Links.hydrate(alarm.getAlarmDefinition(), uriInfo, AlarmDefinitionResource.ALARM_DEFINITIONS_PATH);
    }
    return Links.paginate(this.persistUtils.getLimit(limit), Links.hydrate(alarms, uriInfo), uriInfo);
}
#end_block

#method_before
@BeforeClass
protected void setupClass() throws Exception {
    // This test won't work without the real mysql database so use mini-mon.
    // Warning, this will truncate your mini-mon database
    db = new DBI("jdbc:mysql://192.168.10.4/mon", "monapi", "password");
    handle = db.open();
    /*
    handle
        .execute(Resources.toString(getClass().getResource("alarm.sql"), Charset.defaultCharset()));
        */
    repo = new AlarmMySqlRepoImpl(db, new PersistUtils());
    alarmActions = new ArrayList<String>();
    alarmActions.add("29387234");
    alarmActions.add("77778687");
}
#method_after
@BeforeClass
protected void setupClass() throws Exception {
    // This test won't work without the real mysql database so use mini-mon.
    // Warning, this will truncate your mini-mon database
    db = new DBI("jdbc:mysql://192.168.10.4:3306/mon?connectTimeout=5000&autoReconnect=true&useLegacyDatetimeCode=false", "monapi", "password");
    handle = db.open();
    /*
    handle
        .execute(Resources.toString(getClass().getResource("alarm.sql"), Charset.defaultCharset()));
        */
    repo = new AlarmMySqlRepoImpl(db, new PersistUtils());
    alarmActions = new ArrayList<String>();
    alarmActions.add("29387234");
    alarmActions.add("77778687");
}
#end_block

#method_before
@BeforeMethod
protected void beforeMethod() {
    handle.execute("SET foreign_key_checks = 0;");
    handle.execute("truncate table alarm");
    handle.execute("truncate table sub_alarm");
    handle.execute("truncate table alarm_action");
    handle.execute("truncate table alarm_definition");
    handle.execute("truncate table alarm_metric");
    handle.execute("truncate table metric_definition");
    handle.execute("truncate table metric_definition_dimensions");
    handle.execute("truncate table metric_dimension");
    handle.execute("insert into alarm_definition (id, tenant_id, name, severity, expression, match_by, actions_enabled, created_at, updated_at, deleted_at) " + "values ('1', 'bob', '90% CPU', 'LOW', 'avg(cpu.idle_perc{flavor_id=777, image_id=888, device=1}) > 10', 'flavor_id,image_id', 1, NOW(), NOW(), NULL)");
    handle.execute("insert into alarm (id, alarm_definition_id, state, created_at, updated_at) values ('1', '1', 'OK', NOW(), NOW())");
    handle.execute("insert into alarm (id, alarm_definition_id, state, created_at, updated_at) values ('2', '1', 'UNDETERMINED', NOW(), NOW())");
    handle.execute("insert into alarm (id, alarm_definition_id, state, created_at, updated_at) values ('3', '1', 'ALARM', NOW(), NOW())");
    long subAlarmId = 42;
    for (int alarmId = 1; alarmId <= 3; alarmId++) {
        handle.execute("insert into sub_alarm (id, alarm_id, expression, created_at, updated_at) values ('" + String.valueOf(subAlarmId++) + "', '" + alarmId + "', 'avg(cpu.idle_perc{flavor_id=777, image_id=888, device=1}) > 10', NOW(), NOW())");
    }
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('1', 11)");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('1', 22)");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('2', 11)");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('3', 22)");
    handle.execute("insert into metric_definition (id, name, tenant_id, region) values (1, 'cpu.idle_perc', 'bob', 'west')");
    handle.execute("insert into metric_definition_dimensions (id, metric_definition_id, metric_dimension_set_id) values (11, 1, 1)");
    handle.execute("insert into metric_definition_dimensions (id, metric_definition_id, metric_dimension_set_id) values (22, 1, 2)");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (1, 'instance_id', '123')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (1, 'service', 'monitoring')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (2, 'flavor_id', '222')");
    alarm1 = new Alarm("1", "1", "90% CPU", "LOW", buildAlarmMetrics(buildMetricDefinition("cpu.idle_perc", "instance_id", "123", "service", "monitoring", null, null), buildMetricDefinition("cpu.idle_perc", "flavor_id", "222")), AlarmState.OK, null, null);
    alarm2 = new Alarm("2", "1", "90% CPU", "LOW", buildAlarmMetrics(buildMetricDefinition("cpu.idle_perc", "instance_id", "123", "service", "monitoring")), AlarmState.UNDETERMINED, null, null);
    alarm3 = new Alarm("3", "1", "90% CPU", "LOW", buildAlarmMetrics(buildMetricDefinition("cpu.idle_perc", "flavor_id", "222")), AlarmState.ALARM, null, null);
    handle.execute("insert into alarm_definition (id, tenant_id, name, severity, expression, match_by, actions_enabled, created_at, updated_at, deleted_at) " + "values ('234', 'bob', '50% CPU', 'LOW', 'avg(cpu.sys_mem{service=monitoring}) > 20 and avg(cpu.idle_perc{service=monitoring}) < 10', 'hostname,region', 1, NOW(), NOW(), NULL)");
    handle.execute("insert into alarm (id, alarm_definition_id, state, created_at, updated_at) values ('234111', '234', 'UNDETERMINED', NOW(), NOW())");
    handle.execute("insert into sub_alarm (id, alarm_id, expression, created_at, updated_at) values ('4343', '234111', 'avg(cpu.sys_mem{service=monitoring}) > 20', NOW(), NOW())");
    handle.execute("insert into sub_alarm (id, alarm_id, expression, created_at, updated_at) values ('4242', '234111', 'avg(cpu.idle_perc{service=monitoring}) < 10', NOW(), NOW())");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('234111', 31)");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('234111', 32)");
    handle.execute("insert into metric_definition (id, name, tenant_id, region) values (111, 'cpu.sys_mem', 'bob', 'west')");
    handle.execute("insert into metric_definition (id, name, tenant_id, region) values (112, 'cpu.idle_perc', 'bob', 'west')");
    handle.execute("insert into metric_definition_dimensions (id, metric_definition_id, metric_dimension_set_id) values (31, 111, 21)");
    handle.execute("insert into metric_definition_dimensions (id, metric_definition_id, metric_dimension_set_id) values (32, 112, 22)");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (21, 'service', 'monitoring')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (22, 'service', 'monitoring')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (21, 'hostname', 'roland')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (22, 'hostname', 'roland')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (21, 'region', 'colorado')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (22, 'region', 'colorado')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (22, 'extra', 'vivi')");
    compoundAlarm = new Alarm("234111", "234", "50% CPU", "LOW", buildAlarmMetrics(buildMetricDefinition("cpu.sys_mem", "service", "monitoring", "hostname", "roland", "region", "colorado"), buildMetricDefinition("cpu.idle_perc", "service", "monitoring", "hostname", "roland", "region", "colorado", "extra", "vivi")), AlarmState.UNDETERMINED, null, null);
}
#method_after
@BeforeMethod
protected void beforeMethod() {
    handle.execute("SET foreign_key_checks = 0;");
    handle.execute("truncate table alarm");
    handle.execute("truncate table sub_alarm");
    handle.execute("truncate table alarm_action");
    handle.execute("truncate table alarm_definition");
    handle.execute("truncate table alarm_metric");
    handle.execute("truncate table metric_definition");
    handle.execute("truncate table metric_definition_dimensions");
    handle.execute("truncate table metric_dimension");
    DateTime timestamp1 = ISO_8601_FORMATTER.parseDateTime("2015-03-14T09:26:53").withZoneRetainFields(DateTimeZone.forID("UTC"));
    DateTime timestamp2 = ISO_8601_FORMATTER.parseDateTime("2015-03-14T09:26:54").withZoneRetainFields(DateTimeZone.forID("UTC"));
    DateTime timestamp3 = ISO_8601_FORMATTER.parseDateTime("2015-03-14T09:26:55").withZoneRetainFields(DateTimeZone.forID("UTC"));
    handle.execute("insert into alarm_definition (id, tenant_id, name, severity, expression, match_by, actions_enabled, created_at, updated_at, deleted_at) " + "values ('1', 'bob', '90% CPU', 'LOW', 'avg(cpu.idle_perc{flavor_id=777, image_id=888, device=1}) > 10', 'flavor_id,image_id', 1, NOW(), NOW(), NULL)");
    handle.execute("insert into alarm (id, alarm_definition_id, state, created_at, updated_at) values ('1', '1', 'OK', '" + timestamp1.toString().replace('Z', ' ') + "', '" + timestamp1.toString().replace('Z', ' ') + "')");
    handle.execute("insert into alarm (id, alarm_definition_id, state, created_at, updated_at) values ('2', '1', 'UNDETERMINED', '" + timestamp2.toString().replace('Z', ' ') + "', '" + timestamp2.toString().replace('Z', ' ') + "')");
    handle.execute("insert into alarm (id, alarm_definition_id, state, created_at, updated_at) values ('3', '1', 'ALARM', '" + timestamp3.toString().replace('Z', ' ') + "', '" + timestamp3.toString().replace('Z', ' ') + "')");
    long subAlarmId = 42;
    for (int alarmId = 1; alarmId <= 3; alarmId++) {
        handle.execute("insert into sub_alarm (id, alarm_id, expression, created_at, updated_at) values ('" + String.valueOf(subAlarmId++) + "', '" + alarmId + "', 'avg(cpu.idle_perc{flavor_id=777, image_id=888, device=1}) > 10', NOW(), NOW())");
    }
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('1', 11)");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('1', 22)");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('2', 11)");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('3', 22)");
    handle.execute("insert into metric_definition (id, name, tenant_id, region) values (1, 'cpu.idle_perc', 'bob', 'west')");
    handle.execute("insert into metric_definition_dimensions (id, metric_definition_id, metric_dimension_set_id) values (11, 1, 1)");
    handle.execute("insert into metric_definition_dimensions (id, metric_definition_id, metric_dimension_set_id) values (22, 1, 2)");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (1, 'instance_id', '123')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (1, 'service', 'monitoring')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (2, 'flavor_id', '222')");
    alarm1 = new Alarm("1", "1", "90% CPU", "LOW", buildAlarmMetrics(buildMetricDefinition("cpu.idle_perc", "instance_id", "123", "service", "monitoring"), buildMetricDefinition("cpu.idle_perc", "flavor_id", "222")), AlarmState.OK, timestamp1, timestamp1);
    alarm2 = new Alarm("2", "1", "90% CPU", "LOW", buildAlarmMetrics(buildMetricDefinition("cpu.idle_perc", "instance_id", "123", "service", "monitoring")), AlarmState.UNDETERMINED, timestamp2, timestamp2);
    alarm3 = new Alarm("3", "1", "90% CPU", "LOW", buildAlarmMetrics(buildMetricDefinition("cpu.idle_perc", "flavor_id", "222")), AlarmState.ALARM, timestamp3, timestamp3);
    DateTime timestamp4 = ISO_8601_FORMATTER.parseDateTime("2015-03-15T09:26:53Z");
    handle.execute("insert into alarm_definition (id, tenant_id, name, severity, expression, match_by, actions_enabled, created_at, updated_at, deleted_at) " + "values ('234', 'bob', '50% CPU', 'LOW', 'avg(cpu.sys_mem{service=monitoring}) > 20 and avg(cpu.idle_perc{service=monitoring}) < 10', 'hostname,region', 1, NOW(), NOW(), NULL)");
    handle.execute("insert into alarm (id, alarm_definition_id, state, created_at, updated_at) values ('234111', '234', 'UNDETERMINED', '" + timestamp4.toString().replace('Z', ' ') + "', '" + timestamp4.toString().replace('Z', ' ') + "')");
    handle.execute("insert into sub_alarm (id, alarm_id, expression, created_at, updated_at) values ('4343', '234111', 'avg(cpu.sys_mem{service=monitoring}) > 20', NOW(), NOW())");
    handle.execute("insert into sub_alarm (id, alarm_id, expression, created_at, updated_at) values ('4242', '234111', 'avg(cpu.idle_perc{service=monitoring}) < 10', NOW(), NOW())");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('234111', 31)");
    handle.execute("insert into alarm_metric (alarm_id, metric_definition_dimensions_id) values ('234111', 32)");
    handle.execute("insert into metric_definition (id, name, tenant_id, region) values (111, 'cpu.sys_mem', 'bob', 'west')");
    handle.execute("insert into metric_definition (id, name, tenant_id, region) values (112, 'cpu.idle_perc', 'bob', 'west')");
    handle.execute("insert into metric_definition_dimensions (id, metric_definition_id, metric_dimension_set_id) values (31, 111, 21)");
    handle.execute("insert into metric_definition_dimensions (id, metric_definition_id, metric_dimension_set_id) values (32, 112, 22)");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (21, 'service', 'monitoring')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (22, 'service', 'monitoring')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (21, 'hostname', 'roland')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (22, 'hostname', 'roland')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (21, 'region', 'colorado')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (22, 'region', 'colorado')");
    handle.execute("insert into metric_dimension (dimension_set_id, name, value) values (22, 'extra', 'vivi')");
    compoundAlarm = new Alarm("234111", "234", "50% CPU", "LOW", buildAlarmMetrics(buildMetricDefinition("cpu.sys_mem", "service", "monitoring", "hostname", "roland", "region", "colorado"), buildMetricDefinition("cpu.idle_perc", "service", "monitoring", "hostname", "roland", "region", "colorado", "extra", "vivi")), AlarmState.UNDETERMINED, timestamp4, timestamp4);
}
#end_block

#method_before
@Test(groups = "database")
public void shouldFind() {
    checkList(repo.find("Not a tenant id", null, null, null, null, null, null, 1, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("flavor_id", "222").build(), null, null, null, 1, false), alarm1, alarm3);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").put("hostname", "roland").build(), null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, null, null, AlarmState.UNDETERMINED, null, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), null, null, null, 1, false), alarm1, alarm2);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", null, null, null, null, 1, false), alarm1, alarm2, alarm3);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, AlarmState.UNDETERMINED, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, AlarmState.UNDETERMINED, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, 1, false), alarm2);
}
#method_after
@Test(groups = "database")
public void shouldFind() {
    checkList(repo.find("Not a tenant id", null, null, null, null, null, null, 1, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", null, null, null, null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("flavor_id", "222").build(), null, null, null, 1, false), alarm1, alarm3);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").put("hostname", "roland").build(), null, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, null, null, AlarmState.UNDETERMINED, null, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), null, null, null, 1, false), alarm1, alarm2);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", null, null, null, null, 1, false), alarm1, alarm2, alarm3);
    checkList(repo.find(TENANT_ID, compoundAlarm.getAlarmDefinition().getId(), null, null, AlarmState.UNDETERMINED, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.sys_mem", null, AlarmState.UNDETERMINED, null, null, 1, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, 1, false), alarm2, compoundAlarm);
    checkList(repo.find(TENANT_ID, alarm1.getAlarmDefinition().getId(), "cpu.idle_perc", ImmutableMap.<String, String>builder().put("service", "monitoring").build(), AlarmState.UNDETERMINED, null, null, 1, false), alarm2);
    checkList(repo.find(TENANT_ID, null, null, null, null, DateTime.now(DateTimeZone.forID("UTC")), null, 0, false));
    checkList(repo.find(TENANT_ID, null, null, null, null, ISO_8601_FORMATTER.parseDateTime("2015-03-15T00:00:00Z"), null, 0, false), compoundAlarm);
    checkList(repo.find(TENANT_ID, null, null, null, null, ISO_8601_FORMATTER.parseDateTime("2015-03-14T00:00:00Z"), null, 1, false), alarm1, alarm2, alarm3, compoundAlarm);
}
#end_block

#method_before
@Test(groups = "database")
public void shouldUpdate() throws InterruptedException {
    final Alarm originalAlarm = repo.findById(TENANT_ID, ALARM_ID);
    final Date originalUpdatedAt = getAlarmUpdatedDate(ALARM_ID);
    assertEquals(originalAlarm.getState(), AlarmState.UNDETERMINED);
    Thread.sleep(1000);
    final Alarm updatedAlarm = repo.update(TENANT_ID, ALARM_ID, AlarmState.OK);
    final Date newUpdatedAt = getAlarmUpdatedDate(ALARM_ID);
    assertFalse(newUpdatedAt.equals(originalUpdatedAt), "updated_at did not change");
    assertEquals(updatedAlarm, originalAlarm);
    updatedAlarm.setState(AlarmState.OK);
    // Make sure it was updated in the DB
    assertEquals(repo.findById(TENANT_ID, ALARM_ID), updatedAlarm);
    Thread.sleep(1000);
    final Alarm unchangedAlarm = repo.update(TENANT_ID, ALARM_ID, AlarmState.OK);
    assertTrue(getAlarmUpdatedDate(ALARM_ID).equals(newUpdatedAt), "updated_at did change");
    assertEquals(unchangedAlarm, updatedAlarm);
}
#method_after
@Test(groups = "database")
public void shouldUpdate() throws InterruptedException {
    final Alarm originalAlarm = repo.findById(TENANT_ID, ALARM_ID);
    final DateTime originalUpdatedAt = getAlarmUpdatedDate(ALARM_ID);
    assertEquals(originalAlarm.getState(), AlarmState.UNDETERMINED);
    Thread.sleep(1000);
    final Alarm newAlarm = repo.update(TENANT_ID, ALARM_ID, AlarmState.OK);
    final DateTime newUpdatedAt = getAlarmUpdatedDate(ALARM_ID);
    assertNotEquals(newUpdatedAt.getMillis(), originalUpdatedAt.getMillis(), "updated_at did not change");
    assertEquals(newAlarm, originalAlarm);
    newAlarm.setState(AlarmState.OK);
    newAlarm.setStateUpdatedTimestamp(newUpdatedAt);
    // Make sure it was updated in the DB
    assertEquals(repo.findById(TENANT_ID, ALARM_ID), newAlarm);
    Thread.sleep(1000);
    final Alarm unchangedAlarm = repo.update(TENANT_ID, ALARM_ID, AlarmState.OK);
    assertTrue(getAlarmUpdatedDate(ALARM_ID).equals(newUpdatedAt), "updated_at did change");
    assertEquals(unchangedAlarm, newAlarm);
}
#end_block

#method_before
private Date getAlarmUpdatedDate(final String alarmId) {
    final List<Map<String, Object>> rows = handle.createQuery("select updated_at from alarm where id = :alarmId").bind("alarmId", alarmId).list();
    final Object updated_at = rows.get(0).get("updated_at");
    return (Date) updated_at;
}
#method_after
private DateTime getAlarmUpdatedDate(final String alarmId) {
    final List<Map<String, Object>> rows = handle.createQuery("select updated_at from alarm where id = :alarmId").bind("alarmId", alarmId).list();
    final Object updated_at = rows.get(0).get("updated_at");
    return (new DateTime(((Timestamp) updated_at).getTime(), DateTimeZone.forID("UTC")));
}
#end_block

#method_before
@Override
public int hashCode() {
    final int prime = 31;
    int result = super.hashCode();
    result = prime * result + ((alarmDefinition == null) ? 0 : alarmDefinition.hashCode());
    result = prime * result + ((links == null) ? 0 : links.hashCode());
    result = prime * result + ((metrics == null) ? 0 : metrics.hashCode());
    result = prime * result + ((state == null) ? 0 : state.hashCode());
    result = prime * result + ((stateUpdatedAt == null) ? 0 : stateUpdatedAt.hashCode());
    return result;
}
#method_after
@Override
public int hashCode() {
    final int prime = 31;
    int result = super.hashCode();
    result = prime * result + ((alarmDefinition == null) ? 0 : alarmDefinition.hashCode());
    result = prime * result + ((links == null) ? 0 : links.hashCode());
    result = prime * result + ((metrics == null) ? 0 : metrics.hashCode());
    result = prime * result + ((state == null) ? 0 : state.hashCode());
    result = prime * result + ((stateUpdatedTimestamp == null) ? 0 : stateUpdatedTimestamp.hashCode());
    result = prime * result + ((createdTimestamp == null) ? 0 : createdTimestamp.hashCode());
    return result;
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (!super.equals(obj))
        return false;
    if (getClass() != obj.getClass())
        return false;
    Alarm other = (Alarm) obj;
    if (alarmDefinition == null) {
        if (other.alarmDefinition != null)
            return false;
    } else if (!alarmDefinition.equals(other.alarmDefinition))
        return false;
    if (links == null) {
        if (other.links != null)
            return false;
    } else if (!links.equals(other.links))
        return false;
    if (metrics == null) {
        if (other.metrics != null)
            return false;
    } else if (!metrics.equals(other.metrics))
        return false;
    if (state != other.state)
        return false;
    if (!stateUpdatedAt.equals(other.stateUpdatedAt)) {
        return false;
    }
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (!super.equals(obj))
        return false;
    if (getClass() != obj.getClass())
        return false;
    Alarm other = (Alarm) obj;
    if (alarmDefinition == null) {
        if (other.alarmDefinition != null)
            return false;
    } else if (!alarmDefinition.equals(other.alarmDefinition))
        return false;
    if (links == null) {
        if (other.links != null)
            return false;
    } else if (!links.equals(other.links))
        return false;
    if (metrics == null) {
        if (other.metrics != null)
            return false;
    } else if (!metrics.equals(other.metrics))
        return false;
    if (state != other.state)
        return false;
    // Ignore timezones, only check milliseconds since epoch
    if (stateUpdatedTimestamp.getMillis() != other.stateUpdatedTimestamp.getMillis()) {
        return false;
    }
    if (createdTimestamp.getMillis() != other.createdTimestamp.getMillis()) {
        return false;
    }
    return true;
}
#end_block

#method_before
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, DateTime stateUpdatedAt, String offset, int limit, boolean enforceLimit) {
    try (Handle h = db.open()) {
        StringBuilder sbWhere = new StringBuilder();
        if (alarmDefId != null) {
            sbWhere.append("and ad.id = :alarmDefId ");
        }
        if (metricName != null) {
            sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id " + "inner join (select distinct id from metric_definition " + "            where name = :metricName) as md " + "on md.id = mdd.metric_definition_id ");
            buildJoinClauseFor(metricDimensions, sbWhere);
            sbWhere.append(")");
        }
        if (state != null) {
            sbWhere.append(" and a.state = :state");
        }
        if (stateUpdatedAt != null) {
            sbWhere.append(" and a.updated_at >= :stateUpdatedAt");
        }
        if (offset != null) {
            sbWhere.append(" and a.id > :offset");
        }
        String limitPart = "";
        if (enforceLimit) {
            limitPart = " limit :limit";
        }
        String sql = String.format(ALARM_SQL, sbWhere, limitPart);
        final Query<Map<String, Object>> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        if (stateUpdatedAt != null) {
            q.bind("stateUpdatedAt", stateUpdatedAt);
        }
        if (offset != null) {
            q.bind("offset", offset);
        }
        if (enforceLimit) {
            q.bind("limit", limit + 1);
        }
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        final long start = System.currentTimeMillis();
        final List<Map<String, Object>> rows = q.list();
        logger.debug("Query took {} milliseconds", System.currentTimeMillis() - start);
        final List<Alarm> alarms = createAlarms(tenantId, rows);
        return alarms;
    }
}
#method_after
@Override
public List<Alarm> find(String tenantId, String alarmDefId, String metricName, Map<String, String> metricDimensions, AlarmState state, DateTime stateUpdatedStart, String offset, int limit, boolean enforceLimit) {
    try (Handle h = db.open()) {
        StringBuilder sbWhere = new StringBuilder();
        if (alarmDefId != null) {
            sbWhere.append("and ad.id = :alarmDefId ");
        }
        if (metricName != null) {
            sbWhere.append(" and a.id in (select distinct a.id from alarm as a " + "inner join alarm_metric as am on am.alarm_id = a.id " + "inner join metric_definition_dimensions as mdd " + "  on mdd.id = am.metric_definition_dimensions_id " + "inner join (select distinct id from metric_definition " + "            where name = :metricName) as md " + "on md.id = mdd.metric_definition_id ");
            buildJoinClauseFor(metricDimensions, sbWhere);
            sbWhere.append(")");
        }
        if (state != null) {
            sbWhere.append(" and a.state = :state");
        }
        if (stateUpdatedStart != null) {
            sbWhere.append(" and a.updated_at >= :stateUpdatedStart");
        }
        if (offset != null) {
            sbWhere.append(" and a.id > :offset");
        }
        String limitPart = "";
        if (enforceLimit && limit > 0) {
            limitPart = " limit :limit";
        }
        String sql = String.format(ALARM_SQL, sbWhere, limitPart);
        final Query<Map<String, Object>> q = h.createQuery(sql).bind("tenantId", tenantId);
        if (alarmDefId != null) {
            q.bind("alarmDefId", alarmDefId);
        }
        if (metricName != null) {
            q.bind("metricName", metricName);
        }
        if (state != null) {
            q.bind("state", state.name());
        }
        if (stateUpdatedStart != null) {
            q.bind("stateUpdatedStart", stateUpdatedStart.toString().replace('Z', ' '));
        }
        if (offset != null) {
            q.bind("offset", offset);
        }
        if (enforceLimit && limit > 0) {
            q.bind("limit", limit + 1);
        }
        DimensionQueries.bindDimensionsToQuery(q, metricDimensions);
        final long start = System.currentTimeMillis();
        final List<Map<String, Object>> rows = q.list();
        logger.debug("Query took {} milliseconds", System.currentTimeMillis() - start);
        final List<Alarm> alarms = createAlarms(tenantId, rows);
        return alarms;
    }
}
#end_block

#method_before
private List<Alarm> createAlarms(String tenantId, List<Map<String, Object>> rows) {
    Alarm alarm = null;
    String previousAlarmId = null;
    final List<Alarm> alarms = new LinkedList<>();
    List<MetricDefinition> alarmedMetrics = null;
    for (final Map<String, Object> row : rows) {
        final String alarmId = (String) row.get("id");
        if (!alarmId.equals(previousAlarmId)) {
            alarmedMetrics = new ArrayList<>();
            alarm = new Alarm(alarmId, getString(row, "alarm_definition_id"), getString(row, "alarm_definition_name"), getString(row, "severity"), alarmedMetrics, AlarmState.valueOf(getString(row, "state")), (Timestamp) row.get("state_updated_at"), (Timestamp) row.get("created_at"));
            alarms.add(alarm);
        }
        previousAlarmId = alarmId;
        final Map<String, String> dimensionMap = new HashMap<>();
        // Not all Metrics have dimensions (at least theoretically)
        if (row.containsKey("metric_dimensions")) {
            final String dimensions = getString(row, "metric_dimensions");
            for (String dimension : dimensions.split(",")) {
                final String[] parsed_dimension = dimension.split("=");
                dimensionMap.put(parsed_dimension[0], parsed_dimension[1]);
            }
        }
        alarmedMetrics.add(new MetricDefinition(getString(row, "metric_name"), dimensionMap));
    }
    return alarms;
}
#method_after
private List<Alarm> createAlarms(String tenantId, List<Map<String, Object>> rows) {
    Alarm alarm = null;
    String previousAlarmId = null;
    final List<Alarm> alarms = new LinkedList<>();
    List<MetricDefinition> alarmedMetrics = null;
    for (final Map<String, Object> row : rows) {
        final String alarmId = (String) row.get("id");
        if (!alarmId.equals(previousAlarmId)) {
            alarmedMetrics = new ArrayList<>();
            alarm = new Alarm(alarmId, getString(row, "alarm_definition_id"), getString(row, "alarm_definition_name"), getString(row, "severity"), alarmedMetrics, AlarmState.valueOf(getString(row, "state")), new DateTime(((Timestamp) row.get("state_updated_timestamp")).getTime(), DateTimeZone.forID("UTC")), new DateTime(((Timestamp) row.get("created_timestamp")).getTime(), DateTimeZone.forID("UTC")));
            alarms.add(alarm);
        }
        previousAlarmId = alarmId;
        final Map<String, String> dimensionMap = new HashMap<>();
        // Not all Metrics have dimensions (at least theoretically)
        if (row.containsKey("metric_dimensions")) {
            final String dimensions = getString(row, "metric_dimensions");
            if (dimensions != null && !dimensions.isEmpty()) {
                for (String dimension : dimensions.split(",")) {
                    final String[] parsed_dimension = dimension.split("=");
                    if (parsed_dimension.length == 2) {
                        dimensionMap.put(parsed_dimension[0], parsed_dimension[1]);
                    } else {
                        logger.error("Failed to parse dimension. Dimension is malformed: {}", parsed_dimension);
                    }
                }
            }
        }
        alarmedMetrics.add(new MetricDefinition(getString(row, "metric_name"), dimensionMap));
    }
    return alarms;
}
#end_block

#method_before
public static void main(String[] args) throws Exception {
    /*
     * This should allow command line options to show the current version
     * java -jar monasca-api.jar --version
     * java -jar monasca-api.jar -version
     * java -jar monasca-api.jar version
     * Really anything with the word version in it will show the
     * version as long as there is only one argument
     * */
    if (args.length == 1 && args[0].toLowerCase().contains("version")) {
        showVersion();
        System.exit(1);
    }
    new MonApiApplication().run(args);
}
#method_after
public static void main(String[] args) throws Exception {
    /*
     * This should allow command line options to show the current version
     * java -jar monasca-api.jar --version
     * java -jar monasca-api.jar -version
     * java -jar monasca-api.jar version
     * Really anything with the word version in it will show the
     * version as long as there is only one argument
     * */
    if (args.length == 1 && args[0].toLowerCase().contains("version")) {
        showVersion();
        System.exit(0);
    }
    new MonApiApplication().run(args);
}
#end_block

#method_before
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset) {
    try (Handle h = db.open()) {
        // Build sql
        StringBuilder sbWhere = new StringBuilder();
        if (name != null)
            sbWhere.append(" and def.name = :name");
        String sql = String.format(FIND_BY_METRIC_DEF_SQL, MetricQueries.buildJoinClauseFor(dimensions), sbWhere);
        // Build query
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null)
            query.bind("name", name);
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        // Execute query
        List<Map<String, Object>> rows = query.list();
        // Build results
        List<MetricDefinition> metricDefs = new ArrayList<>(rows.size());
        byte[] currentId = null;
        Map<String, String> dims = null;
        for (Map<String, Object> row : rows) {
            byte[] defId = (byte[]) row.get("id");
            String metricName = (String) row.get("name");
            String dName = (String) row.get("dname");
            String dValue = (String) row.get("dvalue");
            if (defId == null || !Arrays.equals(currentId, defId)) {
                currentId = defId;
                dims = new HashMap<>();
                if (dName != null && dValue != null)
                    dims.put(dName, dValue);
                metricDefs.add(new MetricDefinition(metricName, dims));
            } else
                dims.put(dName, dValue);
        }
        return metricDefs;
    }
}
#method_after
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, int limit) {
    try (Handle h = db.open()) {
        // Build sql
        StringBuilder sbWhere = new StringBuilder();
        if (name != null)
            sbWhere.append(" and def.name = :name");
        String sql = String.format(FIND_BY_METRIC_DEF_SQL, MetricQueries.buildJoinClauseFor(dimensions), sbWhere);
        // Build query
        Query<Map<String, Object>> query = h.createQuery(sql).bind("tenantId", tenantId);
        if (name != null)
            query.bind("name", name);
        DimensionQueries.bindDimensionsToQuery(query, dimensions);
        // Execute query
        List<Map<String, Object>> rows = query.list();
        // Build results
        List<MetricDefinition> metricDefs = new ArrayList<>(rows.size());
        byte[] currentId = null;
        Map<String, String> dims = null;
        for (Map<String, Object> row : rows) {
            byte[] defId = (byte[]) row.get("id");
            String metricName = (String) row.get("name");
            String dName = (String) row.get("dname");
            String dValue = (String) row.get("dvalue");
            if (defId == null || !Arrays.equals(currentId, defId)) {
                currentId = defId;
                dims = new HashMap<>();
                if (dName != null && dValue != null)
                    dims.put(dName, dValue);
                metricDefs.add(new MetricDefinition(metricName, dims));
            } else
                dims.put(dName, dValue);
        }
        return metricDefs;
    }
}
#end_block

#method_before
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset) throws Exception {
    String serieNameRegex = buildSerieNameRegex(tenantId, config.region, name, dimensions);
    String query = String.format("list series /%1$s/", serieNameRegex);
    logger.debug("Query string: {}", query);
    List<Serie> result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.SECONDS);
    return buildMetricDefList(result, offset);
}
#method_after
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, int limit) throws Exception {
    // Limit is not implemented for Influxdb V8.
    String serieNameRegex = buildSerieNameRegex(tenantId, config.region, name, dimensions);
    String query = String.format("list series /%1$s/", serieNameRegex);
    logger.debug("Query string: {}", query);
    List<Serie> result = this.influxDB.Query(this.config.influxDB.getName(), query, TimeUnit.MILLISECONDS);
    return buildMetricDefList(result, offset);
}
#end_block

#method_before
private List<String> buildMetricNameList(List<Serie> result) throws Exception {
    Set<String> metricNameSet = new TreeSet<>();
    for (Serie serie : result) {
        for (Map<String, Object> point : serie.getRows()) {
            String encodedMetricName = (String) point.get("name");
            InfluxV8Utils.SerieNameDecoder serieNameDecoder;
            try {
                serieNameDecoder = new InfluxV8Utils.SerieNameDecoder(encodedMetricName);
            } catch (InfluxV8Utils.SerieNameDecodeException e) {
                logger.warn("Dropping series name that is not decodable: {}", point.get("name"), e);
                continue;
            }
            metricNameSet.add(serieNameDecoder.getMetricName());
        }
    }
    return new ArrayList<String>(metricNameSet);
}
#method_after
private List<MetricName> buildMetricNameList(List<Serie> result, String offset) throws Exception {
    // offset comes in as url encoded.
    String decodedOffset = null;
    if (offset != null) {
        decodedOffset = urlDecodeUTF8(offset);
    }
    Set<MetricName> metricNameSet = new TreeSet<>();
    for (Serie serie : result) {
        for (Map<String, Object> point : serie.getRows()) {
            String encodedMetricName = (String) point.get("name");
            if (offset != null) {
                if (encodedMetricName.compareTo(decodedOffset) <= 0) {
                    continue;
                }
            }
            InfluxV8Utils.SerieNameDecoder serieNameDecoder;
            try {
                serieNameDecoder = new InfluxV8Utils.SerieNameDecoder(encodedMetricName);
            } catch (InfluxV8Utils.SerieNameDecodeException e) {
                logger.warn("Dropping series name that is not decodable: {}", point.get("name"), e);
                continue;
            }
            MetricName metricName = new MetricName(urlEncodeUTF8(encodedMetricName), serieNameDecoder.getMetricName());
            metricNameSet.add(metricName);
            if (offset != null) {
                if (metricNameSet.size() >= Paged.LIMIT) {
                    return new ArrayList<>(metricNameSet);
                }
            }
        }
    }
    return new ArrayList<>(metricNameSet);
}
#end_block

#method_before
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset) throws Exception {
    String q = String.format("show series %1$s where %2$s %3$s %4$s", namePart(name), tenantIdPart(tenantId), regionPart(this.region), dimPart(dimensions));
    logger.debug("Metric definition query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<MetricDefinition> metricDefinitionList = metricDefinitionList(series);
    logger.debug("Found {} metric definitions matching query", metricDefinitionList.size());
    return metricDefinitionList;
}
#method_after
@Override
public List<MetricDefinition> find(String tenantId, String name, Map<String, String> dimensions, String offset, int limit) throws Exception {
    int startIndex = this.influxV9Utils.startIndex(offset);
    String q = String.format("show series %1$s " + "where %2$s %3$s %4$s %5$s %6$s", this.influxV9Utils.namePart(name, false), this.influxV9Utils.tenantIdPart(tenantId), this.influxV9Utils.regionPart(this.region), this.influxV9Utils.dimPart(dimensions), this.influxV9Utils.limitPart(limit), this.influxV9Utils.offsetPart(startIndex));
    logger.debug("Metric definition query: {}", q);
    String r = this.influxV9RepoReader.read(q);
    Series series = this.objectMapper.readValue(r, Series.class);
    List<MetricDefinition> metricDefinitionList = metricDefinitionList(series, startIndex);
    logger.debug("Found {} metric definitions matching query", metricDefinitionList.size());
    return metricDefinitionList;
}
#end_block

#method_before
private List<MetricDefinition> metricDefinitionList(Series series) {
    List<MetricDefinition> metricDefinitionList = new ArrayList<>();
    if (!series.isEmpty()) {
        for (Serie serie : series.getSeries()) {
            for (String[] values : serie.getValues()) {
                metricDefinitionList.add(new MetricDefinition(serie.getName(), dims(values, serie.getColumns())));
            }
        }
    }
    return metricDefinitionList;
}
#method_after
private List<MetricDefinition> metricDefinitionList(Series series, int startIndex) {
    List<MetricDefinition> metricDefinitionList = new ArrayList<>();
    if (!series.isEmpty()) {
        int index = startIndex;
        for (Serie serie : series.getSeries()) {
            for (String[] values : serie.getValues()) {
                MetricDefinition m = new MetricDefinition(serie.getName(), dims(values, serie.getColumns()));
                m.setId(String.valueOf(index++));
                metricDefinitionList.add(m);
            }
        }
    }
    return metricDefinitionList;
}
#end_block

#method_before
private Map<String, String> dims(String[] vals, String[] cols) {
    Map<String, String> dims = new HashMap<>();
    for (int i = 0; i < cols.length; ++i) {
        if (!vals[i].equalsIgnoreCase("null")) {
            dims.put(cols[i], vals[i]);
        }
    }
    return dims;
}
#method_after
private Map<String, String> dims(String[] vals, String[] cols) {
    Map<String, String> dims = new HashMap<>();
    for (int i = 0; i < cols.length; ++i) {
        if (!cols[i].equals("region") && !cols[i].equals("tenant_id") && !cols[i].equals("id")) {
            if (!vals[i].equalsIgnoreCase("null")) {
                dims.put(cols[i], vals[i]);
            }
        }
    }
    return dims;
}
#end_block

#method_before
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object getMetrics(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("offset") String offset) throws Exception {
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    return Links.paginate(offset, metricRepo.find(tenantId, name, dimensions, offset), uriInfo);
}
#method_after
@GET
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object getMetrics(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("name") String name, @QueryParam("dimensions") String dimensionsStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions(name, dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), metricRepo.find(tenantId, name, dimensions, offset, this.persistUtils.getLimit(limit)), uriInfo);
}
#end_block

#method_before
@GET
@Path("/names")
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object getMetricNames(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId) throws Exception {
    return metricRepo.listNames(tenantId);
}
#method_after
@GET
@Path("/names")
@Timed
@Produces(MediaType.APPLICATION_JSON)
public Object getMetricNames(@Context UriInfo uriInfo, @HeaderParam("X-Tenant-Id") String tenantId, @QueryParam("dimensions") String dimensionsStr, @QueryParam("offset") String offset, @QueryParam("limit") String limit) throws Exception {
    Map<String, String> dimensions = Strings.isNullOrEmpty(dimensionsStr) ? null : Validation.parseAndValidateNameAndDimensions("null", dimensionsStr);
    return Links.paginate(this.persistUtils.getLimit(limit), metricRepo.findNames(tenantId, dimensions, offset, this.persistUtils.getLimit(limit)), uriInfo);
}
#end_block

#method_before
@BeforeClass
protected void beforeClass() throws Exception {
    db = new DBI("jdbc:h2:mem:test;MODE=MySQL");
    handle = db.open();
    handle.execute(Resources.toString(getClass().getResource("notification_method.sql"), Charset.defaultCharset()));
    repo = new NotificationMethodMySqlRepoImpl(db);
}
#method_after
@BeforeClass
protected void beforeClass() throws Exception {
    db = new DBI("jdbc:h2:mem:test;MODE=MySQL");
    handle = db.open();
    handle.execute(Resources.toString(getClass().getResource("notification_method.sql"), Charset.defaultCharset()));
    repo = new NotificationMethodMySqlRepoImpl(db, new PersistUtils());
}
#end_block

#method_before
@BeforeMethod
protected void beforeMethod() {
    handle.execute("truncate table notification_method");
    handle.execute("insert into notification_method (id, tenant_id, name, type, address, created_at, updated_at) values ('123', '444', 'MyEmail', 'EMAIL', 'a@b', NOW(), NOW())");
}
#method_after
@BeforeMethod
protected void beforeMethod() {
    handle.execute("truncate table notification_method");
    handle.execute("insert into notification_method (id, tenant_id, name, type, address, created_at, updated_at) values ('123', '444', 'MyEmail', 'EMAIL', 'a@b', NOW(), NOW())");
    handle.execute("insert into notification_method (id, tenant_id, name, type, address, created_at, updated_at) values ('124', '444', 'OtherEmail', 'EMAIL', 'a@b', NOW(), NOW())");
}
#end_block

#method_before
public void shouldFind() {
    List<NotificationMethod> nms = repo.find("444", null);
    assertEquals(nms, Arrays.asList(new NotificationMethod("123", "MyEmail", NotificationMethodType.EMAIL, "a@b")));
}
#method_after
public void shouldFind() {
    List<NotificationMethod> nms = repo.find("444", null, 1);
    assertEquals(nms, Arrays.asList(new NotificationMethod("123", "MyEmail", NotificationMethodType.EMAIL, "a@b"), new NotificationMethod("124", "OtherEmail", NotificationMethodType.EMAIL, "a@b")));
}
#end_block

#method_before
@Override
public NotificationMethod create(String tenantId, String name, NotificationMethodType type, String address) {
    if (exists(tenantId, name, type, address) != null)
        throw new EntityExistsException("Notification method %s \"%s\" %s \"%s\" already exists.", tenantId, name, type, address);
    try (Handle h = db.open()) {
        String id = UUID.randomUUID().toString();
        h.insert("insert into notification_method (id, tenant_id, name, type, address, created_at, updated_at) values (?, ?, ?, ?, ?, NOW(), NOW())", id, tenantId, name, type.toString(), address);
        LOG.debug("Creating notification method {} for {}", name, tenantId);
        return new NotificationMethod(id, name, type, address);
    }
}
#method_after
@Override
public NotificationMethod create(String tenantId, String name, NotificationMethodType type, String address) {
    try (Handle h = db.open()) {
        h.begin();
        if (getNotificationIdForTenantIdAndName(h, tenantId, name) != null)
            throw new EntityExistsException("Notification method %s \"%s\" already exists.", tenantId, name);
        String id = UUID.randomUUID().toString();
        h.insert("insert into notification_method (id, tenant_id, name, type, address, created_at, updated_at) values (?, ?, ?, ?, ?, NOW(), NOW())", id, tenantId, name, type.toString(), address);
        LOG.debug("Creating notification method {} for {}", name, tenantId);
        h.commit();
        return new NotificationMethod(id, name, type, address);
    }
}
#end_block

#method_before
@Override
public List<NotificationMethod> find(String tenantId, String offset) {
    try (Handle h = db.open()) {
        if (offset != null) {
            return h.createQuery("select * from notification_method where tenant_id = :tenantId and id > :offset order by id asc limit :limit").bind("tenantId", tenantId).bind("offset", offset).bind("limit", Paged.LIMIT).map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
        } else {
            return h.createQuery("select * from notification_method where tenant_id = :tenantId").bind("tenantId", tenantId).map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
        }
    }
}
#method_after
@Override
public List<NotificationMethod> find(String tenantId, String offset, int limit) {
    try (Handle h = db.open()) {
        String rawQuery = "  SELECT nm.id, nm.tenant_id, nm.name, nm.type, nm.address, nm.created_at, nm.updated_at " + "FROM notification_method as nm " + "WHERE tenant_id = :tenantId %1$s order by nm.id asc limit :limit";
        String offsetPart = "";
        if (offset != null) {
            offsetPart = "and nm.id > :offset";
        }
        String query = String.format(rawQuery, offsetPart);
        Query<?> q = h.createQuery(query);
        q.bind("tenantId", tenantId);
        if (offset != null) {
            q.bind("offset", offset);
        }
        q.bind("limit", limit + 1);
        return (List<NotificationMethod>) q.map(new BeanMapper<NotificationMethod>(NotificationMethod.class)).list();
    }
}
#end_block

#method_before
@Override
public NotificationMethod update(String tenantId, String notificationMethodId, String name, NotificationMethodType type, String address) {
    String notificationID = exists(tenantId, name, type, address);
    if (notificationID != null && !notificationID.equalsIgnoreCase(notificationMethodId)) {
        throw new EntityExistsException("Notification method %s \"%s\" %s \"%s\" already exists.", tenantId, name, type, address);
    }
    try (Handle h = db.open()) {
        if (h.update("update notification_method set name = ?, type = ?, address = ? where tenant_id = ? and id = ?", name, type.name(), address, tenantId, notificationMethodId) == 0)
            throw new EntityNotFoundException("No notification method exists for %s", notificationMethodId);
        return new NotificationMethod(notificationMethodId, name, type, address);
    }
}
#method_after
@Override
public NotificationMethod update(String tenantId, String notificationMethodId, String name, NotificationMethodType type, String address) {
    try (Handle h = db.open()) {
        h.begin();
        String notificationID = getNotificationIdForTenantIdAndName(h, tenantId, name);
        if (notificationID != null && !notificationID.equalsIgnoreCase(notificationMethodId)) {
            throw new EntityExistsException("Notification method %s \"%s\" already exists.", tenantId, name);
        }
        if (h.update("update notification_method set name = ?, type = ?, address = ? where tenant_id = ? and id = ?", name, type.name(), address, tenantId, notificationMethodId) == 0)
            throw new EntityNotFoundException("No notification method exists for %s", notificationMethodId);
        h.commit();
        return new NotificationMethod(notificationMethodId, name, type, address);
    }
}
#end_block

#method_before
@Provides
Config stormConfig() {
    if (stormConfig == null) {
        stormConfig = new Config();
        stormConfig.setNumWorkers(config.numWorkerProcesses);
        stormConfig.setNumAckers(config.numAckerThreads);
        // Configure the StatsdMetricConsumer
        java.util.Map<Object, Object> statsdConfig = new java.util.HashMap<>();
        // note that you get default values if these are absent
        if (config.statsdConfig.getHost() != null)
            statsdConfig.put(StatsdMetricConsumer.STATSD_HOST, config.statsdConfig.getHost());
        if (config.statsdConfig.getPort() != null)
            statsdConfig.put(StatsdMetricConsumer.STATSD_PORT, config.statsdConfig.getPort());
        if (config.statsdConfig.getPrefix() != null)
            statsdConfig.put(StatsdMetricConsumer.STATSD_PREFIX, config.statsdConfig.getPrefix());
        if (config.statsdConfig.getDimensions() != null)
            statsdConfig.put(StatsdMetricConsumer.STATSD_DIMENSIONS, config.statsdConfig.getDimensions());
        stormConfig.registerMetricsConsumer(StatsdMetricConsumer.class, statsdConfig, 2);
    }
    return stormConfig;
}
#method_after
@Provides
Config stormConfig() {
    if (stormConfig == null) {
        stormConfig = new Config();
        stormConfig.setNumWorkers(config.numWorkerProcesses);
        stormConfig.setNumAckers(config.numAckerThreads);
        /* Configure the StatsdMetricConsumer */
        java.util.Map<Object, Object> statsdConfig = new java.util.HashMap<>();
        /*
       * Catch the case where the config file was not updated
       * in /etc/monasca/thresh-config.yml
       * note that you get default values if these are absent
       */
        if (config.statsdConfig.getHost() != null)
            statsdConfig.put(StatsdMetricConsumer.STATSD_HOST, config.statsdConfig.getHost());
        if (config.statsdConfig.getPort() != null)
            statsdConfig.put(StatsdMetricConsumer.STATSD_PORT, config.statsdConfig.getPort());
        if (config.statsdConfig.getPrefix() != null)
            statsdConfig.put(StatsdMetricConsumer.STATSD_PREFIX, config.statsdConfig.getPrefix());
        if (config.statsdConfig.getDimensions() != null)
            statsdConfig.put(StatsdMetricConsumer.STATSD_DIMENSIONS, config.statsdConfig.getDimensions());
        stormConfig.registerMetricsConsumer(StatsdMetricConsumer.class, statsdConfig, 2);
    }
    return stormConfig;
}
#end_block

#method_before
@Provides
StormTopology topology() {
    TopologyBuilder builder = new TopologyBuilder();
    // Receives metrics
    builder.setSpout("metrics-spout", Injector.getInstance(IRichSpout.class, "metrics"), config.metricSpoutThreads).setNumTasks(config.metricSpoutTasks);
    // Receives events
    builder.setSpout("event-spout", Injector.getInstance(IRichSpout.class, "event"), config.eventSpoutThreads).setNumTasks(config.eventSpoutTasks);
    // Event -> Events
    builder.setBolt("event-bolt", new EventProcessingBolt(config.database), config.eventBoltThreads).shuffleGrouping("event-spout").setNumTasks(config.eventBoltTasks);
    // Metrics / Event -> Filtering
    builder.setBolt("filtering-bolt", new MetricFilteringBolt(config.database), config.filteringBoltThreads).fieldsGrouping("metrics-spout", new Fields(MetricSpout.FIELDS[0])).allGrouping("event-bolt", EventProcessingBolt.METRIC_ALARM_EVENT_STREAM_ID).allGrouping("event-bolt", EventProcessingBolt.ALARM_DEFINITION_EVENT_STREAM_ID).setNumTasks(config.filteringBoltTasks);
    // Filtering /Event -> Alarm Creation
    builder.setBolt("alarm-creation-bolt", new AlarmCreationBolt(config.database), 1).fieldsGrouping("filtering-bolt", MetricFilteringBolt.NEW_METRIC_FOR_ALARM_DEFINITION_STREAM, new Fields(AlarmCreationBolt.ALARM_CREATION_FIELDS[3])).allGrouping("event-bolt", EventProcessingBolt.METRIC_SUB_ALARM_EVENT_STREAM_ID).allGrouping("event-bolt", EventProcessingBolt.ALARM_EVENT_STREAM_ID).allGrouping("event-bolt", EventProcessingBolt.ALARM_DEFINITION_EVENT_STREAM_ID).setNumTasks(// This has to be a single bolt right now because there is no
    1);
    // database protection for adding metrics and dimensions
    // Filtering / Event / Alarm Creation -> Aggregation
    builder.setBolt("aggregation-bolt", new MetricAggregationBolt(config.sporadicMetricNamespaces), config.aggregationBoltThreads).fieldsGrouping("filtering-bolt", new Fields(MetricFilteringBolt.FIELDS[0])).allGrouping("filtering-bolt", MetricAggregationBolt.METRIC_AGGREGATION_CONTROL_STREAM).fieldsGrouping("filtering-bolt", AlarmCreationBolt.ALARM_CREATION_STREAM, new Fields(AlarmCreationBolt.ALARM_CREATION_FIELDS[1])).allGrouping("event-bolt", EventProcessingBolt.METRIC_SUB_ALARM_EVENT_STREAM_ID).fieldsGrouping("event-bolt", EventProcessingBolt.METRIC_ALARM_EVENT_STREAM_ID, new Fields(EventProcessingBolt.METRIC_ALARM_EVENT_STREAM_FIELDS[1])).fieldsGrouping("alarm-creation-bolt", AlarmCreationBolt.ALARM_CREATION_STREAM, new Fields(AlarmCreationBolt.ALARM_CREATION_FIELDS[1])).setNumTasks(config.aggregationBoltTasks);
    // Alarm Creation / Event
    // Aggregation / Event -> Thresholding
    builder.setBolt("thresholding-bolt", new AlarmThresholdingBolt(config.database, config.kafkaProducerConfig), config.thresholdingBoltThreads).fieldsGrouping("aggregation-bolt", new Fields(MetricAggregationBolt.FIELDS[0])).fieldsGrouping("event-bolt", EventProcessingBolt.ALARM_EVENT_STREAM_ID, new Fields(EventProcessingBolt.ALARM_EVENT_STREAM_FIELDS[1])).allGrouping("event-bolt", EventProcessingBolt.ALARM_DEFINITION_EVENT_STREAM_ID).allGrouping("event-bolt", EventProcessingBolt.METRIC_SUB_ALARM_EVENT_STREAM_ID).setNumTasks(config.thresholdingBoltTasks);
    return builder.createTopology();
}
#method_after
@Provides
StormTopology topology() {
    TopologyBuilder builder = new TopologyBuilder();
    // Receives metrics
    builder.setSpout("metrics-spout", Injector.getInstance(IRichSpout.class, "metrics"), config.metricSpoutThreads).setNumTasks(config.metricSpoutTasks);
    // Receives events
    builder.setSpout("event-spout", Injector.getInstance(IRichSpout.class, "event"), config.eventSpoutThreads).setNumTasks(config.eventSpoutTasks);
    // Event -> Events
    builder.setBolt("event-bolt", new EventProcessingBolt(config.database), config.eventBoltThreads).shuffleGrouping("event-spout").setNumTasks(config.eventBoltTasks);
    // Metrics / Event -> Filtering
    builder.setBolt("filtering-bolt", new MetricFilteringBolt(config.database), config.filteringBoltThreads).fieldsGrouping("metrics-spout", new Fields(MetricSpout.FIELDS[0])).allGrouping("event-bolt", EventProcessingBolt.METRIC_ALARM_EVENT_STREAM_ID).allGrouping("event-bolt", EventProcessingBolt.ALARM_DEFINITION_EVENT_STREAM_ID).setNumTasks(config.filteringBoltTasks);
    // Filtering /Event -> Alarm Creation
    builder.setBolt("alarm-creation-bolt", new AlarmCreationBolt(config.database), config.alarmCreationBoltThreads).fieldsGrouping("filtering-bolt", MetricFilteringBolt.NEW_METRIC_FOR_ALARM_DEFINITION_STREAM, new Fields(AlarmCreationBolt.ALARM_CREATION_FIELDS[3])).allGrouping("event-bolt", EventProcessingBolt.METRIC_SUB_ALARM_EVENT_STREAM_ID).allGrouping("event-bolt", EventProcessingBolt.ALARM_EVENT_STREAM_ID).allGrouping("event-bolt", EventProcessingBolt.ALARM_DEFINITION_EVENT_STREAM_ID).setNumTasks(config.alarmCreationBoltTasks);
    // Filtering / Event / Alarm Creation -> Aggregation
    builder.setBolt("aggregation-bolt", new MetricAggregationBolt(config), config.aggregationBoltThreads).fieldsGrouping("filtering-bolt", new Fields(MetricFilteringBolt.FIELDS[0])).allGrouping("filtering-bolt", MetricAggregationBolt.METRIC_AGGREGATION_CONTROL_STREAM).fieldsGrouping("filtering-bolt", AlarmCreationBolt.ALARM_CREATION_STREAM, new Fields(AlarmCreationBolt.ALARM_CREATION_FIELDS[1])).allGrouping("event-bolt", EventProcessingBolt.METRIC_SUB_ALARM_EVENT_STREAM_ID).fieldsGrouping("event-bolt", EventProcessingBolt.METRIC_ALARM_EVENT_STREAM_ID, new Fields(EventProcessingBolt.METRIC_ALARM_EVENT_STREAM_FIELDS[1])).fieldsGrouping("alarm-creation-bolt", AlarmCreationBolt.ALARM_CREATION_STREAM, new Fields(AlarmCreationBolt.ALARM_CREATION_FIELDS[1])).setNumTasks(config.aggregationBoltTasks);
    // Alarm Creation / Event
    // Aggregation / Event -> Thresholding
    builder.setBolt("thresholding-bolt", new AlarmThresholdingBolt(config.database, config.kafkaProducerConfig), config.thresholdingBoltThreads).fieldsGrouping("aggregation-bolt", new Fields(MetricAggregationBolt.FIELDS[0])).fieldsGrouping("event-bolt", EventProcessingBolt.ALARM_EVENT_STREAM_ID, new Fields(EventProcessingBolt.ALARM_EVENT_STREAM_FIELDS[1])).allGrouping("event-bolt", EventProcessingBolt.ALARM_DEFINITION_EVENT_STREAM_ID).allGrouping("event-bolt", EventProcessingBolt.METRIC_SUB_ALARM_EVENT_STREAM_ID).setNumTasks(config.thresholdingBoltTasks);
    return builder.createTopology();
}
#end_block

#method_before
public String getDimensions() {
    return dimensions;
}
#method_after
public Map<String, String> getDimensions() {
    return dimensions;
}
#end_block

#method_before
public void setDimensions(String dimensions) {
    this.dimensions = dimensions;
}
#method_after
public void setDimensions(Map<String, String> dimensions) {
    this.dimensions = dimensions;
}
#end_block

#method_before
@SuppressWarnings("rawtypes")
@Override
public void prepare(Map stormConf, Object registrationArgument, TopologyContext context, IErrorReporter errorReporter) {
    logger = LoggerFactory.getLogger(Logging.categoryFor(getClass(), context));
    parseConfig(stormConf);
    if (registrationArgument instanceof Map) {
        parseConfig((Map) registrationArgument);
    }
    statsd = new NonBlockingStatsDClient(statsdPrefix + clean(topologyName), statsdHost, statsdPort);
    try {
        handler = statsdErrorHandler;
        udpclient = new NonBlockingUdpSender(statsdHost, statsdPort, Charset.defaultCharset(), handler);
    } catch (IOException e) {
        logger.error("{}", e);
    }
    logger.info("statsdPrefix ({}), topologyName ({}), clean(topologyName) ({})", new Object[] { statsdPrefix, topologyName, clean(topologyName) });
}
#method_after
@Override
public void prepare(Map stormConf, Object registrationArgument, TopologyContext context, IErrorReporter errorReporter) {
    logger = LoggerFactory.getLogger(Logging.categoryFor(getClass(), context));
    parseConfig(stormConf);
    if (registrationArgument instanceof Map) {
        parseConfig((Map<?, ?>) registrationArgument);
    }
    initClient();
    logger.info("statsdPrefix ({}), topologyName ({}), clean(topologyName) ({})", new Object[] { statsdPrefix, topologyName, clean(topologyName) });
}
#end_block

#method_before
void parseConfig(@SuppressWarnings("rawtypes") Map conf) {
    if (conf.containsKey(Config.TOPOLOGY_NAME)) {
        topologyName = (String) conf.get(Config.TOPOLOGY_NAME);
    }
    if (conf.containsKey(STATSD_HOST)) {
        statsdHost = (String) conf.get(STATSD_HOST);
    }
    if (conf.containsKey(STATSD_PORT)) {
        statsdPort = ((Number) conf.get(STATSD_PORT)).intValue();
    }
    if (conf.containsKey(STATSD_PREFIX)) {
        statsdPrefix = (String) conf.get(STATSD_PREFIX);
        if (!statsdPrefix.endsWith(".")) {
            statsdPrefix += ".";
        }
    }
    if (conf.containsKey(STATSD_DIMENSIONS)) {
        statsdDimensions = (String) conf.get(STATSD_DIMENSIONS);
        if (!isValidJSON(statsdDimensions)) {
            logger.error("Ignoring dimensions element invalid JSON ({})", new Object[] { statsdDimensions });
            // You get default dimensions
            statsdDimensions = monascaStatsdDimPrefix + defaultDimensions;
        } else {
            statsdDimensions = monascaStatsdDimPrefix + statsdDimensions;
        }
    }
}
#method_after
@SuppressWarnings("unchecked")
void parseConfig(Map<?, ?> conf) {
    if (conf.containsKey(Config.TOPOLOGY_NAME)) {
        topologyName = (String) conf.get(Config.TOPOLOGY_NAME);
    }
    if (conf.containsKey(STATSD_HOST)) {
        statsdHost = (String) conf.get(STATSD_HOST);
    }
    if (conf.containsKey(STATSD_PORT)) {
        statsdPort = ((Number) conf.get(STATSD_PORT)).intValue();
    }
    if (conf.containsKey(STATSD_PREFIX)) {
        statsdPrefix = (String) conf.get(STATSD_PREFIX);
        if (!statsdPrefix.endsWith(".")) {
            statsdPrefix += ".";
        }
    }
    if (conf.containsKey(STATSD_DIMENSIONS)) {
        statsdDimensions = mapToJsonStr((Map<String, String>) conf.get(STATSD_DIMENSIONS));
        if (!isValidJSON(statsdDimensions)) {
            logger.error("Ignoring dimensions element invalid JSON ({})", new Object[] { statsdDimensions });
            // You get default dimensions
            statsdDimensions = monascaStatsdDimPrefix + defaultDimensions;
        } else {
            statsdDimensions = monascaStatsdDimPrefix + statsdDimensions;
        }
    }
}
#end_block

#method_before
String clean(String s) {
    // storm metrics look pretty bad so cleanup is needed
    return s.replace('.', '_').replace('/', '_').replace(':', '_').replaceAll("__", "");
}
#method_after
String clean(String s) {
    /* storm metrics look pretty bad so cleanup is needed */
    return s.replace('.', '_').replace('/', '_').replace(':', '_').replaceAll("__", "");
}
#end_block

#method_before
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    Metric other = (Metric) obj;
    if (name == null) {
        if (other.name != null)
            return false;
    } else if (!name.equals(other.name))
        return false;
    if (value != other.value)
        return false;
    if (dimensions != other.dimensions)
        return false;
    return true;
}
#method_after
@Override
public boolean equals(Object obj) {
    if (this == obj)
        return true;
    if (obj == null)
        return false;
    if (getClass() != obj.getClass())
        return false;
    Metric other = (Metric) obj;
    if (name == null) {
        if (other.name != null)
            return false;
    } else if (!name.equals(other.name))
        return false;
    if (value != other.value)
        return false;
    if (!dimensions.equals(other.dimensions))
        return false;
    return true;
}
#end_block

#method_before
List<Metric> dataPointsToMetrics(TaskInfo taskInfo, Collection<DataPoint> dataPoints) {
    List<Metric> res = new LinkedList<>();
    StringBuilder sb = new StringBuilder().append(clean(taskInfo.srcComponentId)).append(".");
    int hdrLength = sb.length();
    for (DataPoint p : dataPoints) {
        sb.delete(hdrLength, sb.length());
        sb.append(clean(p.name));
        logger.debug("Storm StatsD metric p.name ({}) p.value ({})", new Object[] { p.name, p.value });
        if (p.value instanceof Number) {
            res.add(new Metric(sb.toString(), ((Number) p.value).intValue(), statsdDimensions));
        } else // there is a map of data points and it's not empty
        if (p.value instanceof Map && !(((Map<?, ?>) (p.value)).isEmpty())) {
            int hdrAndNameLength = sb.length();
            @SuppressWarnings("rawtypes")
            Map map = (Map) p.value;
            for (Object subName : map.keySet()) {
                Object subValue = map.get(subName);
                if (subValue instanceof Number) {
                    sb.delete(hdrAndNameLength, sb.length());
                    sb.append(".").append(clean(subName.toString()));
                    res.add(new Metric(sb.toString(), ((Number) subValue).intValue(), statsdDimensions));
                }
            }
        }
    }
    return res;
}
#method_after
private List<Metric> dataPointsToMetrics(TaskInfo taskInfo, Collection<DataPoint> dataPoints) {
    List<Metric> res = new LinkedList<>();
    StringBuilder sb = new StringBuilder().append(clean(taskInfo.srcComponentId)).append(".");
    int hdrLength = sb.length();
    for (DataPoint p : dataPoints) {
        sb.delete(hdrLength, sb.length());
        sb.append(clean(p.name));
        logger.debug("Storm StatsD metric p.name ({}) p.value ({})", new Object[] { p.name, p.value });
        if (p.value instanceof Number) {
            res.add(new Metric(sb.toString(), ((Number) p.value).doubleValue(), statsdDimensions));
        } else // There is a map of data points and it's not empty
        if (p.value instanceof Map && !(((Map<?, ?>) (p.value)).isEmpty())) {
            int hdrAndNameLength = sb.length();
            @SuppressWarnings("rawtypes")
            Map map = (Map) p.value;
            for (Object subName : map.keySet()) {
                Object subValue = map.get(subName);
                if (subValue instanceof Number) {
                    sb.delete(hdrAndNameLength, sb.length());
                    sb.append(".").append(clean(subName.toString()));
                    res.add(new Metric(sb.toString(), ((Number) subValue).doubleValue(), statsdDimensions));
                }
            }
        }
    }
    return res;
}
#end_block

#method_before
// Since the Java client doesn't support the Monasca metric type
public void report(String s, int number, String dimensions) {
    String statsdMessage = statsdPrefix + s + ":" + String.valueOf(number) + "|c" + statsdDimensions;
    LOG.debug("reporting: {}={}{}", s, number, dimensions);
    udpclient.send(statsdMessage);
}
#method_after
public void report(String s, Double number, String dimensions) {
    if (udpclient != null) {
        StringBuilder statsdMessage = new StringBuilder().append(statsdPrefix).append(s).append(":").append(String.valueOf(number)).append("|c").append(statsdDimensions);
        logger.debug("reporting: {}={}{}", s, number, dimensions);
        udpclient.send(statsdMessage.toString());
    } else {
        /* Try to setup the UDP client since it was null */
        initClient();
    }
}
#end_block

#method_before
@Override
public void cleanup() {
    statsd.stop();
    udpclient.stop();
}
#method_after
@Override
public void cleanup() {
    udpclient.stop();
}
#end_block

